# 

## Abstract

## Bibliography

Linear Model and Extensions

## Statistical properties of under-fitted OLS

and cov(ε) = σ 2 I n . However, we only fit the OLS of Y on X 2 with coefficient β2 and estimated variance σ2 . Show that

## Acronyms

I try hard to avoid using acronyms to reduce the unnecessary burden for reading. The following are standard and will be used repeatedly.

ANOVA [(Fisher's)](#) analysis of variance CLT central limit theorem CV cross-validation EHW Eicker-Huber-White (robust covariance matrix or standard error) FWL Frisch-Waugh-Lovell (theorem) GEE generalized estimating equation GLM generalized linear model HC heteroskedasticity-consistent (covariance matrix or standard error) IID independent and identically distributed LAD least absolute deviations lasso least absolute shrinkage and selection operator MLE maximum likelihood estimate OLS ordinary least squares RSS residual sum of squares WLS weighted least squares Symbols All vectors are column vectors as in R unless stated otherwise. Let the superscript " t " denote the transpose of a vector or matrix. a ∼ approximation in distribution R the set of all real numbers β regression coefficient ε error term H hat matrix H = X(X t X) -1 X t h ii leverage score: the (i, i)the element of the hat matrix H I n identity matrix of dimension n × n x i covariate vector for unit i X covariate matrix Y outcome vector y i outcome for unit i independence and conditional independence xv Useful R packages This book uses the following R packages and functions. package function or data use car hccm Eicker-Huber-White robust standard error linearHypothesis testing linear hypotheses in linear models foreign read.dta read stata data gee gee Generalized estimating equation HistData GaltonFamilies Galton's data on parents' and children's heights MASS lm.ridge ridge regression glm.nb Negative-Binomial regression glmnet cv.glmnet Lasso with cross-validation mlbench BostonHousing Boston housing data polr proportional odds logistic regression Matching lalonde LaLonde data nnet multinom Multinomial logistic regression quantreg rq quantile regression survival coxph Cox proportional hazards regression survdiff log rank test survfit Kaplan-Meier curve xvii

## Preface

The importance of studying the linear model A central task in statistics is to use data to build models to make inferences about the underlying data-generating processes or make predictions of future observations. Although real problems are very complex, the linear model can often serve as a good approximation to the true data-generating process. Sometimes, although the true data-generating process is nonlinear, the linear model can be a useful approximation if we properly transform the data based on prior knowledge. Even in highly nonlinear problems, the linear model can still be a useful first attempt in the data analysis process.

Moreover, the linear model has many elegant algebraic and geometric properties. Under the linear model, we can derive many explicit formulas to gain insights about various aspects of statistical modeling. In more complicated models, deriving explicit formulas may be impossible. Nevertheless, we can use the linear model to build intuition and make conjectures about more complicated models.

Pedagogically, the linear model serves as a building block in the whole statistical training. This book builds on my lecture notes for a master's level "Linear Model" course at UC Berkeley, taught over the past eight years. Most students are master's students in statistics. Some are undergraduate students with strong technical preparations. Some are Ph.D. students in statistics. Some are master's or Ph.D. students in other departments. This book requires the readers to have basic training in linear algebra, probability theory, and statistical inference.

## Recommendations for instructors

This book has twenty-seven chapters in the main text and four chapters as the appendices. As I mentioned before, this book grows out of my teaching of "Linear Model" at UC Berkeley. In different years, I taught the course in different ways, and this book is a union of my lecture notes over the past eight years. Below I make some recommendations for instructors based on my own teaching experience. Since UC Berkeley is on the semester system, instructors on the quarter system should make some adjustments to my recommendations below.  Preface my teaching assistants to review the appendices in the first two lab sessions and assigned homework problems from the appendices to remind the students to review the background materials. Then you can cover Chapters 2-24. You can omit Chapter 18 and some sections in other chapters due to their technical complications. If time permits, you can consider covering Chapter 25 due to the importance of the generalized estimating equation as well as its byproduct called the "cluster-robust standard error", which is important for many social science applications. Furthermore, you can consider covering Chapter 27 due to the importance of the Cox proportional hazards model.

## Part

I Introduction 1 Motivations for Statistical Models 1.1 Data and statistical models A wide range of problems in statistics and machine learning have the data structure as below: Unit outcome/response covariates/features/predictors i Y X 1 X 2 • • • X p 1 y 1 x 11 x 12 • • • x 1p 2 y 2 x 21 x 22 • • • x 2p . . . . . . . . . . . . . . . n y n x n1 x n2 • • • x np

For each unit i, we observe the outcome/response of interest, y i , as well as p covariates/features/predictors. We often use

$Y =      y 1 y 2$. . .

$y n     $to denote the n-dimensional outcome/response vector, and

$X =      x 11 x 12 • • • x 1p x 21 x 22 • • • x 2p . . . . . . . . . x n1 x n2 • • • x np     $to denote the n × p covariate/feature/predictor matrix, also called the design matrix. In most cases, the first column of X contains constants 1s.

Based on the data (X, Y ) , we can ask the following questions:

(Q1) Describe the relationship between X and Y , i.e., their association or correlation. For example, how is the patients' average height related to the children's average height? How is one's height related to one's weight? How are one's education and working experience related to one's income?

(Q2) Predict Y * based on new data X * . In particular, we want to use the current data (X, Y ) to train a predictor, and then use it to predict future Y * based on future X * . This is called supervised learning in the field of machine learning. For example, how do we predict whether an email is spam or not based on the frequencies of the most commonly occurring words and punctuation marks in the email? How do we predict cancer patients' survival time based on their clinical measures? (Q3) Estimate the causal effect of some components in X on Y . What if we change some components of X? How do we measure the impact of the hypothetical intervention of some components of X on Y ? This is a much harder question because most statistical tools are designed to infer association, not causation. For example, the U.S. Food and Drug Administration (FDA) approves drugs based on randomized controlled trials (RCT) because RCTs are most credible to infer causal effects of drugs on health outcomes. Economists are interested in evaluating the effect of a job training program on employment and wages. However, this is a notoriously difficult problem with only observational data.

The above descriptions are about generic X and Y , which can be many different types. We often use different statistical models to capture the features of different types of data. I give a brief overview of models that will appear in later parts of this book.

(T1) X and Y are univariate and continuous. In Francis Galton's[foot_0](#foot_0) classic example, X is the parents' average height and Y is the children's average height [(Galton, 1886)](#b120). Galton derived the following formula:

$y = ȳ + ρ σy σx (x -x)$which is equivalent to 

$y - ȳ σy = ρ x - x σx , (1$$(x i -x)(y i -ȳ).$This is the famous formula of "regression towards mediocrity" or "regression towards the mean". Galton first introduced the terminology "regression." Galton called regression because the relative deviation of the children's average height is smaller than that of the parents' average height if |ρ| < 1. We will derive the above Galtonian formula in Chapter 2. The name "regression" is widely used in statistics now. For instance, we sometimes use "linear regression" interchangeably with "linear model"; we also extend the name to "logistic regression" or "Cox regression" which will be discussed in later chapters of this book.

(T2) Y univariate and continuous, and X multivariate of mixed types. In the R package ElemStatLearn, the dataset prostate has an outcome of interest as the log of the prostatespecific antigenlpsa and some potential predictors including the log cancer volume

lcavol, the log prostate weight lweight, age age, etc. (T3) Y binary or indicator of two classes, and X multivariate of mixed types. For example, in the R package wooldridge, the dataset mroz contains an outcome of interest being the binary indicator for whether a woman was in the labor force in 1975, and some useful covariates are covariate name covariate meaning kidslt6 number of kids younger than six years old kidsge6 number of kids between six and eighteen years old age age educ years of education husage husband's age huseduc husband's years of education (T4) Y categorical without ordering. For example, the choice of housing type, single-family house, townhouse, or condominium, is a categorical variable.

(T5) Y categorical and ordered. For example, the final course evaluation at UC Berkeley can take value in {1, 2, 3, 4, 5, 6, 7}. These numbers have clear ordering but they are not the usual real numbers.

(T6) Y counts. For example, the number of times one went to the gym last week is a non-negative integer representing counts.

(T7) Y time-to-event outcome. For example, in medical trials, a major outcome of interest is the survival time; in labor economics, a major outcome of interest is the time to find the next job. The former is called survival analysis in biostatistics and the latter is called duration analysis in econometrics.

(T8) Y multivariate and correlated. In medical trials, the data are often longitudinal, meaning that the patient's outcomes are measured repeatedly over time. So each patient has a multivariate outcome. In field experiments of public health and development economics, the randomized interventions are often at the village level but the outcome data are collected at the household level. So within villages, the outcomes are correlated.

## Why linear models?

Why do we study linear models if most real problems may have nonlinear structures? There are important reasons.

(R1) Linear models are simple but non-trivial starting points for learning.

(R2) Linear models can provide insights because we can derive explicit formulas based on elegant algebra and geometry.

(R3) Linear models can handle nonlinearity by incorporating nonlinear terms, for example, X can contain the polynomials or nonlinear transformations of the original covariates. In statistics, "linear" often means linear in parameters, not necessarily in covariates.

(R4) Linear models can be good approximations to nonlinear data-generating processes.

(R5) Linear models are simpler than nonlinear models, but they do not necessarily perform worse than more complicated nonlinear models. We have finite data so we cannot fit arbitrarily complicated models.

If you are interested in nonlinear models, you can take another machine learning course.

Ordinary Least Squares (OLS) with a Univariate Covariate

## Univariate ordinary least squares

Figure [2](#fig_1).1 shows the scatterplot of Galton's dataset which can be found in the R package HistData as GaltonFamilies. In this dataset, father denotes the height of the father and mother denotes the height of the mother. The x-axis denotes the mid-parent height, calculated as (father + 1.08*mother)/2, and the y-axis denotes the height of a child. q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q fitted line: y=22.64+0.64x  With n data points (x i, y i ) n i=1 , our goal is to find the best linear fit of the data

$(x i, ŷi = α + βx i ) n i=1 .$What do we mean by the "best" fit? Gauss proposed to use the following criterion, called

## Linear Model and Extensions

the ordinary least squares (OLS) 1 :

(α, β) = arg min

$a,b n -1 n i=1 (y i -a -bx i ) 2 .$The OLS criterion is based on the squared "misfits" y i -a -bx i . Another intuitive criterion is based on the absolute values of those misfits, which is called the least absolute deviation (LAD). However, OLS is simpler because the objective function is smooth in (a, b). We will discuss LAD in Chapter 26.

How to solve the OLS minimization problem? The objective function is quadratic, and as a and b diverge, it diverges to infinity. So it must has a unique minimizer ( α, β) which satisfies the first-order condition:

$-2 n n i=1 (y i -α -βx i ) = 0, -2 n n i=1 x i (y i -α -βx i ) = 0.$These two equations are called the Normal Equations of OLS. The first equation implies ȳ = α + β x, [(2.1)](#) that is, the OLS line must go through the sample mean of the data (x, ȳ). The second equation implies

$xy = αx + βx 2 ,(2.2)$where xy is the sample mean of the x i y i 's, and x 2 is the sample mean of the x 2 i 's. Subtracting (2.1)×x from (2.2), we have

$xy -xȳ = β(x 2 -x2 )$=⇒ σxy = β σ2

x =⇒ β = σxy σ2

x .

So the OLS coefficient of x equals the sample covariance between x and y divided by the sample variance of x. From (2.1), we obtain that α = ȳ -β x.

Finally, the fitted line is

$y = α + βx = ȳ -β x + βx =⇒ y -ȳ = β(x -x)$=⇒ y -ȳ = σxy σ2

x (x -x) = ρxy σx σy σ2

x

$(x -x) =⇒ y - ȳ σy = ρxy x - x σx ,$which is the Galtonian formula mentioned in Chapter 1.

We can obtain the fitted line based on Galton's data using the R code below.

1 The idea of OLS is often attributed to Gauss and Legendre. Gauss used it in the process of discovering Ceres, and his work was published in 1809. Legendre's work appeared in 1805 but Gauss claimed that he had been using it since 1794 or 1795. [Stigler (1981)](#b212) reviews the history of OLS. 

## Final comments

We can write the sample mean as the solution to the OLS with only the intercept:

$ȳ = arg min µ n -1 n i=1 (y i -µ) 2 .$It is rare to fit OLS of y i on x i without the intercept: where x and y are the n-dimensional vectors containing all observations, and ⟨x, y⟩ = n i=1 x i y i denotes the inner product. Although not directly useful, this formula will be the building block for many discussions later.

## Homework problems

## Pairwise slopes

Given (x i , y i ) n i=1 with univariate x i and y i , show that Galton's slope equals β =

(i,j)

$w ij b ij ,$where the summation is over all pairs of observations (i, j), b ij = (y i -y j )/(x i -x j )

is the slope determined by two points (x i , y i ) and (x j , y j ), and

$w ij = (x i -x j ) 2 / (i ′ ,j ′ ) (x i ′ -x j ′ ) 2$is the weight proportional to the squared distance between x i and x j . In the above formulas, we define b ij = 0 if x i = x j . Remark: [Wu (1986)](#b244) and [Gelman and Park (2009)](#b124) used this formula. Problem 3.9 gives a more general result.

## Part II

## OLS and Statistical Inference

## OLS with Multiple Covariates

## The OLS formula

Recall that we have the outcome vector

$Y =      y 1 y 2$. . .

$y n     $and covariate matrix

$X =      x 11 x 12 • • • x 1p x 21 x 22 • • • x 2p . . . . . . . . . x n1 x n2 • • • x np      =      x t 1 x t 2 . . . x t n      = (X 1 , . . . , X p )$where x t i = (x i1 , . . . , x ip ) is the row vector consisting of the covariates of unit i, and X j = (x 1j , . . . , x nj ) t is the column vector of the j-th covariate for all units.

We want to find the best linear fit of the data (x i , ŷi ) n i=1 with ŷi = x t i β = β1 x i1 + • • • + βp x ip in the sense that

$β = arg min b n -1 n i=1 (y i -x t i b) 2 = arg min b n -1 ∥Y -Xb∥ 2 ,$where β is called the OLS coefficient, the ŷi 's are called the fitted values, and the y i -ŷi 's are called the residuals. The objective function is quadratic in b which diverges to infinity when b diverges to infinity. So it must have a unique minimizer β satisfying the first order condition -2 n n i=1

x i (y i -x t i β) = 0, which simplifies to n i=1

x i (y i -x t i β) = 0 ⇐⇒ X t (Y -X β) = 0.

(3.1)

The above equation (3.1) is called the Normal equation of the OLS, which implies the main theorem:

Linear Model and Extensions Theorem 3.1 The OLS coefficient equals

$β = n i=1 x i x t i -1 n i=1 x i y i = (X t X) -1 X t Y if X t X = n i=1$x i x t i is non-degenerate. The equivalence of the two forms of the OLS coefficient follows from

$X t X = (x 1 , . . . , x n )      x t 1 x t 2 . . . x t n      = n i=1 x i x t i ,$and

$X t Y = (x 1 , . . . , x n )      y 1 y 2 . . . y n      = n i=1$x i y i .

For different purposes, both forms can be useful. The non-degeneracy of X t X in Theorem 3.1 requires that for any non-zero vector α ∈ R p , we must have α t X t Xα = ∥Xα∥ 2 ̸ = 0 which is equivalent to Xα ̸ = 0, i.e., the columns of X are linearly independent[foot_1](#foot_1) . This effectively rules out redundant columns in the design matrix X. If X 1 can be represented by other columns X 1 = c 2 X 2 + • • • + c p X p for some (c 2 , . . . , c p ), then X t X is degenerate.

Throughout the book, we invoke the following condition unless stated otherwise.

Condition 3.1 The column vectors of X are linearly independent.

## The geometry of OLS

The OLS has very clear geometric interpretations. Figure [3](#fig_8).1 illustrate its geometry with n = 3 and p = 2. For any b = (b 1 , . . . , b p ) t ∈ R p and X = (X 1 , . . . , X p ) ∈ R n×p ,

$Xb = b 1 X 1 + • • • + b p X p$represents a linear combination of the column vectors of the design matrix X. So the OLS problem is to find the best linear combination of the column vectors of X to approximate the response vector Y . Recall that all linear combinations of the column vectors of X constitute FIGURE 3.1: The geometry of OLS the column space of X, denoted by C(X) 2 . So the OLS problem is to find the vector in C(X) that is the closest to Y . Geometrically, the vector must be the projection of Y onto C(X).

By projection, the residual vector ε = Y -X β must be orthogonal to C(X), or, equivalently, the residual vector is orthogonal to X 1 , . . . , X p . This geometric intuition implies that X t 1 ε = 0, . . . , X t p ε = 0,

$⇐⇒ X t ε =    X t 1 ε . . . X t p ε   = 0, ⇐⇒ X t (Y -X β) = 0,$which is essentially the Normal equation (3.1). The above argument gives a geometric derivation of the OLS formula in Theorem 3.1.

In Figure [3](#fig_8).1, since the triangle ABC is rectangular, the fitted vector Ŷ = X β is orthogonal to the residual vector ε, and moreover, the Pythagorean Theorem implies that ∥Y ∥ 2 = ∥X β∥ 2 + ∥ε∥ 2 .

In most applications, X contains a column of intercepts 1 n = (1, . . . , 1) t . In those cases, we have

$1 t n ε = 0 =⇒ n -1 n i=1 εi = 0,$2 Please review Chapter A for some basic linear algebra background.

so the residuals are automatically centered.

The following theorem states an algebraic fact that gives an alternative proof of the OLS formula. It is essentially the Pythagorean Theorem for the rectangular triangle BCD in Figure [3](#fig_8).1. Theorem 3.2 For any b ∈ R p , we have the following decomposition

$∥Y -Xb∥ 2 = ∥Y -X β∥ 2 + ∥X( β -b)∥ 2 ,$where implies that ∥Y -Xb∥ 2 ≥ ∥Y -X β∥ 2 with equality holding if and only if b = β.

Proof of Theorem 3.2: We have the following decomposition:

$∥Y -Xb∥ 2 = (Y -Xb) t (Y -Xb) = (Y -X β + X β -Xb) t (Y -X β + X β -Xb) = (Y -X β) t (Y -X β) + (X β -Xb) t (X β -Xb) +(Y -X β) t (X β -Xb) + (X β -Xb) t (Y -X β).$The first term equals ∥Y -X β∥ 2 and the second term equals ∥X( β -b)∥ 2 . We need to show the last two terms are zero. By symmetry of these two terms, we only need to show that the last term is zero. This is true by the Normal equation (3.1) of the OLS:

$(X β -Xb) t (Y -X β) = ( β -b) t X t (Y -X β) = 0. □$
## The projection matrix from OLS

The geometry in Section 3.2 also shows that Ŷ = X β is the solution to the following problem Ŷ = arg min v∈C(X) ∥Y -v∥ 2 .

Using Theorem 3.1, we have Ŷ = X β = HY , where H = X(X t X) -1 X t is an n × n matrix. It is called the hat matrix because it puts a hat on Y when multiplying Y . Algebraically, we can show that H is a projection matrix because

$H 2 = X(X t X) -1 X t X(X t X) -1 X t = X(X t X) -1 X t = H, and H t = X(X t X) -1 X t t = X(X t X) -1 X t = H.$Its rank equals its trace, so equals rank(H) = trace(H) = trace X(X t X) -1 X t = trace (X t X) -1 X t X = trace(I p ) = p.

The projection matrix H has the following geometric interpretations.

Proposition 3.1 The projection matrix H = X(X t X) -1 X t satisfies (G1) Hv = v ⇐⇒ v ∈ C(X);

(G2) Hw = 0 ⇐⇒ w ⊥ C(X).

Recall that C(X) is the column space of X. (G1) states that projecting any vector in C(X) onto C(X) does not change the vector, and (G2) states that projecting any vector orthogonal to C(X) onto C(X) results in a zero vector. Proof of Proposition 3.1: I first prove (G1). If v ∈ C(X), then v = Xb for some b, which implies that Hv = X(X t X) -1 X t Xb = Xb = v. Conversely, if v = Hv, then v = X(X t X) -1 X t v = Xu with u = (X t X) -1 X t v, which ensures that v ∈ C(X).

I then prove (G2). If w ⊥ C(X), then w is orthogonal to all column vectors of X. So X t j w = 0 (j = 1, . . . , p) =⇒ X t w = 0 =⇒ Hw = X(X t X) -1 X t w = 0.

Conversely, if Hw = X(X t X) -1 X t w = 0, then w t X(X t X) -1 X t w = 0. Because (X t X) -1 is positive definite, we have X t w = 0 ensuring that w ⊥ C(X).

□ Writing H = (h ij ) 1≤i,j≤n and ŷ = (ŷ 1 , . . . , ŷn ) t , we have another basic identity ŷi = n j=1 h ij y j = h ii y i + j̸ =i h ij y j .

It shows that the predicted value ŷi is a linear combination of all the outcomes. Moreover, if X contains a column of intercepts 1 n = (1, . . . , 1) t , then

$H1 n = 1 n =⇒ n j=1 h ij = 1 (i = 1, . . . , n),$which implies that ŷi is a weighted average of all the outcomes. Although the sum of the weights is one, some of them can be negative.

In general, the hat matrix has complex forms, but when the covariates are dummy variables, it has more explicit forms. I give two examples below.

Example 3.1 In a treatment-control experiment with m treated and n control units, the matrix X contains 1 and a dummy variable for the treatment:

$X = 1 m 1 m 1 n 0 n .$We can show that

$H = diag{m -1 1 m 1 t m , n -1 1 n 1 t n }.$Example 3.2 In an experiment with n j units receiving treatment level j (j = 1, . . . , J), the covariate matrix X contains J dummy variables for the treatment levels:

$X = diag{1 n1 , . . . , 1 n J }.$We can show that H = diag{n -1 1 1 n1 1 t n1 , . . . , n -1 J 1 n J 1 t n J }.

## Homework problems

## Univariate and multivariate OLS

Derive the univariate OLS based on the multivariate OLS formula with

$X =    1 x 1 . . . . . . 1 x n   $where the x i 's are scalars.

## OLS via vector and matrix calculus

Using vector and matrix calculus, show that the OLS estimator minimizes (Y -Xb) t (Y -Xb).

## OLS based on pseudo inverse

Show that β = X + Y . Remark: Recall the definition of the pseudo inverse in Chapter A.

## Invariance of OLS

Assume that X t X is non-degenerate and Γ is a p×p non-degenerate matrix. Define X = XΓ.

From the OLS fit of Y on X, we obtain the coefficient β, the fitted value Ŷ , and the residual ε; from the OLS fit of Y on X, we obtain the coefficient β, the fitted value Ỹ , and the residual ε.

Prove that β = Γ β, Ŷ = Ỹ , ε = ε.

Remark: From a linear algebra perspective, X and XΓ have the same column space if Γ is a non-degenerate matrix:

${Xb : b ∈ R p } = {XΓc : c ∈ R p }.$Consequently, there must be a unique projection of Y onto the common column space.

## Invariance of the hat matrix

Show that H does not change if we change X to XΓ where Γ ∈ R p×p is a non-degenerate matrix.

## Special hat matrices

Verify the formulas of the hat matrices in Examples 3.1 and 3.2.

## OLS with multiple responses

For each unit i = 1, . . . , n, we have multiple responses y i = (y i1 , . . . , y iq ) t ∈ R q and multiple covariates x i = (x i1 , . . . , x ip ) t ∈ R p . Define

$Y =    y 11 • • • y 1q . . . . . . y n1 • • • y nq    =    y t 1 . . . y t n    = (Y 1 , . . . , Y q ) ∈ R n×q and X =    x 11 • • • x 1p . . . . . . x n1 • • • x np    =    x t 1 . . . x t n    = (X 1 , . . . , X p ) ∈ R n×p$as the response and covariate matrices, respectively. Define the multiple OLS coefficient matrix as B = arg min

$B∈R p×q n i=1 ∥y i -B t x i ∥ 2$Show that B = ( B1 , . . . , Bq ) has column vectors

$B1 = (X t X) -1 X t Y 1 , . . . Bq = (X t X) -1 X t Y q .$Remark: This result tells us that the OLS fit with a vector outcome reduces to multiple separate OLS fits, or, the OLS fit of a matrix Y on a matrix X reduces to the column-wise OLS fits of Y on X.

## Full sample and subsample OLS coefficients

Partition the full sample into K subsamples:

$X =    X (1)$. . .

$X (K)    , Y =    Y (1)$. . .

$Y (K)    ,$where the kth sample consists of (X (k) , Y (k) ) with X (k) ∈ R n k ×p and Y (k) ∈ R n k being the covariate matrix and outcome vector. Note that n = K k=1 n k Let β be the OLS coefficient based on the full sample, and β(k) be the OLS coefficient based on the kth sample. Show that

$β = K k=1 W (k) β(k) ,$where the weight matrix equals

$W (k) = (X t X) -1 X t (k) X (k) .$3.9 Jacobi's theorem

The set {1, . . . , n} has n p size-p subsets. Each subset S defines a linear equation for b ∈ R p :

$Y S = X S b$where Y S ∈ R p is the subvector of Y and X S ∈ R p×p is the submatrix of X, corresponding to the units in S. Define the subset coefficient βS = X -1 S Y S if X S is invertible and βS = 0 otherwise. Show that the OLS coefficient equals a weighted average of these subset coefficients:

$β = S w S βS$where the summation is over all subsets and

$w S = | det(X S )| 2 S ′ | det(X S ′ )| 2 .$Remark: To prove this result, we can use Cramer's rule to express the OLS coefficient and use the Cauchy-Binet formula to expand the determinant of X t X. This result extends Problem 2.1. [Berman (1988)](#b59) attributed it to Jacobi. [Wu (1986)](#b244) used it in analyzing the statistical properties of OLS.

The Gauss-Markov Model and Theorem

## Gauss-Markov model

Without any stochastic assumptions, the OLS in Chapter 3 is purely algebraic. From now on, we want to discuss the statistical properties of β and associated quantities, so we need to invoke some statistical modeling assumptions. A simple starting point is the following Gauss-Markov model with a fixed design matrix X and unknown parameters (β, σ 2 ). where the design matrix X is fixed with linearly independent column vectors, and the random error term ε has the first two moments

$E(ε) = 0, cov(ε) = σ 2 I n .$The unknown parameters are (β, σ 2 ).

The Gauss-Markov model assumes that Y has mean Xβ and covariance matrix σ 2 I n . At the individual level, we can also write it as

$y i = x t i β + ε i , (i = 1, . . . , n)$where the error terms are uncorrelated with mean 0 and variance σ 2 .

The assumption that X is fixed is not essential, because we can condition on X even if we think X is random. The mean of each y i is linear in x i with the same β coefficient, which is a rather strong assumption. So is the homoskedasticity[foot_2](#foot_2) assumption that the error terms have the same variance σ 2 . The critiques on the assumptions aside, I will derive the properties of β under the Gauss-Markov model.

## Properties of the OLS estimator

I first derive the mean and covariance of β = (X t X) -1 X t Y .

Theorem 4.1 Under Assumption 4.1, we have

$E( β) = β, cov( β) = σ 2 (X t X) -1 .$
## Linear Model and Extensions

Proof of Theorem 4.1: Because E(Y ) = Xβ, we have

$E( β) = E (X t X) -1 X t Y = (X t X) -1 X t E(Y ) = (X t X) -1 X t Xβ = β.$Because cov(Y ) = σ 2 I n , we have

$cov( β) = cov (X t X) -1 X t Y = (X t X) -1 X t cov(Y )X(X t X) -1 = σ 2 (X t X) -1 X t X(X t X) -1 = σ 2 (X t X) -1 .$
## □

We can decompose the response vector as

$Y = Ŷ + ε,$where the fitted vector is Ŷ = X β = HY and the residual vector is ε = Y -Ŷ = (I n -H)Y. The two matrices H and I n -H are the keys, which have the following properties.

Lemma 4.1 Both H and I n -H are projection matrices. In particular, HX = X, (I n -H)X = 0, and they are orthogonal:

H(I n -H) = (I n -H)H = 0.

These follow from simple linear algebra, and I leave the proof as Problem 4.1. It states that H and I n -H are projection matrices onto the column space of X and its complement. Algebraically, Ŷ and ε are orthogonal by the OLS projection because Lemma 4.1 implies

$Ŷ t ε = Y t H t (I n -H)Y = Y t H(I n -H)Y = 0.$This is also coherent with the geometry in Figure [3](#fig_8).1.

Moreover, we can derive the mean and covariance matrix of Ŷ and ε.

Theorem 4.2 Under Assumption 4.1, we have

$E Ŷ ε = Xβ 0 and cov Ŷ ε = σ 2 H 0 0 I n -H .$So Ŷ and ε are uncorrelated.

Please do not be confused with the two statements above. First, Ŷ and ε are orthogonal. Second, Ŷ and ε are uncorrelated. They have different meanings. The first statement is an algebraic fact of the OLS procedure. It is about a relationship between two vectors Ŷ and ε which holds without assuming the Gauss-Markov model. The second statement is stochastic. It is about a relationship between two random vectors Ŷ and ε which requires the Gauss-Markov model assumption. Proof of Theorem 4.2: The conclusion follows from the simple fact that

$Ŷ ε = HY (I n -H)Y = H I n -H Y$is a linear transformation of Y . It has mean

$E Ŷ ε = H I n -H E(Y ) = H I n -H Xβ = HXβ (I n -H) Xβ = Xβ 0 ,$and covariance matrix

$cov Ŷ ε = H I n -H cov(Y ) H t (I n -H) t = σ 2 H I n -H H I n -H = σ 2 H 2 H(I n -H) (I n -H)H (I n -H) 2 = σ 2 H 0 0 I n -H ,$where the last step follows from Lemma 4.1. □ Assume the Gauss-Markov model. Although the original responses and error terms are uncorrelated between units with cov(ε i , ε j ) = 0 for i ̸ = j, the fitted values and the residuals are correlated with

$cov(ŷ i , ŷj ) = σ 2 h ij , cov(ε i , εj ) = σ 2 (1 -h ij )$for i ̸ = j based on Theorem 4.2.

## Variance estimation

Theorem 4.1 quantifies the uncertainty of β by its covariance matrix. However, it is not directly useful because σ 2 is still unknown. Our next task is to estimate σ 2 based on the observed data. It is the variance of each ε i , but the ε i 's are not observable either. Their empirical analogues are the residuals εi = y i -x t i β. It seems intuitive to estimate σ 2 by σ2 = rss/n where rss = n i=1 ε2 i is the residual sum of squares. However, Theorem 4.2 shows that εi has mean zero and variance σ 2 (1 -h ii ), which is not the same as the variance of original ε i . Consequently, rss has mean

$E(rss) = n i=1 σ 2 (1 -h ii ) = σ 2 {n -trace(H)} = σ 2 (n -p),$which implies the following theorem. i /(n -p).

Then E(σ 2 ) = σ 2 under Assumption 4.1.

Theorem 4.3 implies that σ2 is a biased estimator for σ 2 because E(σ 2 ) = σ 2 (n -p)/n. It underestimates σ 2 but with a large sample size n, the bias is small.

## Gauss-Markov Theorem

So far, we have focused on the OLS estimator. It is intuitive, but we have not answered the fundamental question yet. Why should we focus on it? Are there any other better estimators? Under the Gauss-Markov model, the answer is definite: we focus on the OLS estimator because it is optimal in the sense of having the smallest covariance matrix among all linear unbiased estimators. The following famous Gauss-Markov theorem quantifies this claim, which was named after Carl Friedrich Gauss and Andrey Markov 2 . It is for this reason that I call the corresponding model the Gauss-Markov model. The textbook by [Monahan (2008)](#b181) also uses this name.

Theorem 4.4 Under Assumption 4.1, the OLS estimator β for β is the best linear unbiased estimator (BLUE) in the sense that 3 cov( β) ⪰ cov( β) for any estimator β satisfying (C1) β = AY for some A ∈ R p×n not depending on Y ;

(C2) E( β) = β for any β. 2 David and Neyman (1938) used the name Markoff theorem. [Lehmann (1951)](#b162) appeared to first use the name Gauss-Markov theorem.

3 We write M 1 ≻ M 2 is M 1 -M 2 is positive semi-definite. See Chapter A for a review.

Before proving Theorem 4.4, we need to understand its meaning and immediate implications. We do not compare the OLS estimator with any arbitrary estimators. In fact, we restrict to the estimators that are linear and unbiased. Condition (C1) requires that β is a linear estimator. More precisely, it is a linear transformation of the response vector Y , where A can be any complex and possibly nonlinear function of X. Condition (C2) requires that β is an unbiased estimator for β, no matter what true value β takes.

Why do we restrict the estimator to be linear? The class of linear estimator is actually quite large because A can be any nonlinear function of X, and the only requirement is that the estimator is linear in Y . The unbiasedness is a natural requirement for many problems. However, in many modern applications with many covariates, some biased estimators can perform better than unbiased estimators if they have smaller variances. We will discuss these estimators in Part V of this book.

We compare the estimators based on their covariances, which are natural extensions of variances for scalar random variables. The conclusion cov( β) ⪰ cov( β) implies that for any vector c ∈ R p , we have

$c t cov( β)c ⪰ c t cov( β)c$which is equivalent to var(c t β) ≥ var(c t β), So any linear transformation of the OLS estimator has a variance smaller than or equal to the same linear transformation of any other estimator. In particular, if c = (0, . . . , 1, . . . , 0) t with only the jth coordinate being 1, then the above inequality implies that var( βj ) ≥ var( βj ), (j = 1, . . . , p).

So the OLS estimator has a smaller variance than other estimators for all coordinates. Now we prove the theorem. Proof of Theorem 4.4: We must verify that the OLS estimator itself satisfies (C1) and (C2). We have β = ÂY with Â = (X t X) -1 X t , and it is unbiased by Theorem 4.1.

First, the unbiasedness requirement implies that

$E( β) = β =⇒ E(AY ) = AE(Y ) = AXβ = β =⇒ AXβ = β$for any value of β. So AX = I p (4.1) must hold. In particular, the OLS estimator satisfies ÂX = (X t X) -1 X t X = I p . Second, we can decompose the covariance of β as

$cov( β) = cov( β + β -β) = cov( β) + cov( β -β) + cov( β, β -β) + cov( β -β, β).$The last two terms are in fact zero. By symmetry, we only need to show that the third term is zero:

$cov( β, β -β) = cov ÂY, (A -Â)Y = Âcov(Y )(A -Â) t = σ 2 Â(A -Â) t = σ 2 ( ÂA t -Â Ât ) = σ 2 (X t X) -1 X t A t -(X t X) -1 X t X(X t X) -1 = σ 2 (X t X) -1 I p -(X t X) -1$(by (4.1)) = 0. They hold only when β is BLUE. They do not hold when comparing two general estimators. Theorem 4.4 is elegant but abstract. It says that in some sense, we can just focus on the OLS estimator because it is the best one in terms of the covariance among all linear unbiased estimators. Then we do not need to consider other estimators. However, we have not mentioned any other estimators for β yet, which makes Theorem 4.4 not concrete enough. From the proof above, a linear unbiased estimator β = AY only needs to satisfy AX = I p , which imposes p 2 constraints on the p × n matrix A. Therefore, we have p(n -p) free parameters to choose from and have infinitely many linear unbiased estimators in general. A class of linear unbiased estimators discussed more thoroughly in Chapter 19, are the weighted least squares estimators

$β = (X t Σ -1 X) -1 X t Σ -1 Y,$where Σ is a positive definite matrix not depending on Y such that Σ and X t Σ -1 X are invertible. It is linear, and we can show that it is unbiased for β:

$E( β) = E (X t Σ -1 X) -1 X t Σ -1 Y = (X t Σ -1 X) -1 X t Σ -1 Xβ = β.$Different choices of Σ give different β, but Theorem 4.4 states that the OLS estimator with Σ = I n has the smallest covariance matrix under the Gauss-Markov model.

I will give an extension and some applications of the Gauss-Markov Theorem as homework problems.

## Homework problems

## Projection matrices

Prove Lemma 4.1.

## Univariate OLS and the optimal design

Assume the Gauss-Markov model y i = α + βx i + ε i (i = 1, . . . , n) with a scalar x i . Show that the variance of the OLS coefficient for x i equals var( β) = σ 2 n i=1 (x i -x) 2 .

Assume x i must be in the interval [[0,](#)[1]](#). We want to choose their values to minimize var( β). Assume that n is an even number. Find the minimizers x i 's.

Hint: You may find the following probability result useful. For a random variable ξ in the interval [0, 1], we have the following inequality

$var(ξ) = E(ξ 2 ) -{E(ξ)} 2 ≤ E(ξ) -{E(ξ)} 2 = E(ξ){1 -E(ξ)} ≤ 1/4.$The first inequality becomes an equality if and only if ξ = 0 or 1; the second inequality becomes an equality if and only if E(ξ) = 1/2.

## BLUE estimator for the mean

Assume that y i has mean µ and variance σ 2 , and y i (i = 1, . . . , n) are uncorrelated. A linear estimator of the mean µ has the form μ = n i=1 a i y i , which is unbiased as long as n i=1 a i = 1. So there are infinitely many linear unbiased estimators for µ. Find the BLUE for µ and prove why it is BLUE.

## Consequence of useless regressors

Partition the covariate matrix and parameter into

$X = (X 1 , X 2 ), β = β 1 β 2 ,$where X 1 ∈ R n×k , X 2 ∈ R n×l , β 1 ∈ R k and β 2 ∈ R l with k + l = p. Assume the Gauss-Markov model with β 2 = 0. Let β1 be the first k coordinates of β = (X t X) -1 X t Y and β1 = (X t 1 X 1 ) -1 X t 1 Y be the coefficient based on the partial OLS fit of Y on X 1 only. Show that cov( β1 ) ⪰ cov( β1 ).

## Simple average of subsample OLS coefficients

Inherit the setting of Problem 3.8. Define the simple average of the subsample OLS coefficients as β = K -1 K k=1 β(k) . Assume the Gauss-Markov model. Show that cov( β) ⪰ cov( β).

## Gauss-Markov theorem for prediction

Under Assumption 4.1, the OLS predictor Ŷ = X β for the mean Xβ is the best linear unbiased predictor in the sense that cov( Ỹ ) ⪰ cov( Ŷ ) for any predictor Ỹ satisfying (C1) Ỹ = HY for some H ∈ R n×n not depending on Y ;

(C2) E( Ỹ ) = Xβ for any β.

Prove this theorem.

## Nonlinear unbiased estimator under the Gauss-Markov model

Under Assumption 4.1, prove that if X t Q j X = 0, trace(Q j ) = 0, (j = 1, . . . , p)

Linear Model and Extensions

$then β = β +    Y t Q 1 Y . . . Y t Q p Y   $is unbiased for β. Remark: The above estimator β is a quadratic function of Y . It is a nonlinear unbiased estimator for β. It is not difficult to show the unbiasedness. More remarkably, [Koopmann (1982, Theorem 4.3)](#) showed that under Assumption 4.1, any unbiased estimator for β must have the form of β.

## Normal Linear Model: Inference and Prediction

Under the Gauss-Markov model, we have calculated the first two moments of the OLS estimator β = (X t X) -1 X t Y :

$E( β) = β, cov( β) = σ 2 (X t X) -1 ,$and have shown that σ2 = εt ε/(n -p) is unbiased for σ 2 , where ε = Y -X β is the residual vector. The Gauss-Markov theorem further ensures that the OLS estimator is BLUE. Although these results characterize the nice properties of the OLS estimator, they do not fully determine its distribution and are thus inadequate for statistical inference. This chapter will derive the joint distribution of ( β, σ2 ) under the Normal linear model with stronger distribution assumptions. where the design matrix X is fixed with linearly independent column vectors. The unknown parameters are (β, σ 2 ).

We can also write the Normal linear model as a linear function of covariates with error terms: Y = Xβ + ε or, equivalently,

$y i = x t i β + ε i , (i = 1, . . . , n),$where ε ∼ N(0, σ 2 I n ) or ε i iid ∼ N(0, σ 2 ), (i = 1, . . . , n).

Assumption 5.1 implies Assumption 4.1. Beyond the Gauss-Markov model, it further requires IID Normal error terms. Assumption 5.1 is extremely strong, but it is canonical in statistics. It allows us to derive elegant formulas and also justifies the outputs of the linear regression functions in many statistical packages. I will relax it in Chapter 6.

## Joint distribution of the OLS coefficient and variance estimator

We first state the main theorem on the joint distribution of ( β, σ2 ) via the joint distribution of ( β, ε).

## Linear Model and Extensions

Theorem 5.1 Under Assumption 5.1, we have

$β ε ∼ N β 0 , σ 2 (X t X) -1 0 0 I n -H ,$and σ2 /σ 2 ∼ χ 2 n-p /(n -p). So β ε, β σ2 .

Proof of Theorem 5.1: First,

$β ε = (X t X) -1 X t Y (I n -H)Y = (X t X) -1 X t I n -H Y$is a linear transformation of Y , so they are jointly Normal. We have verified their means and variances in Chapter 4, so we only need to show that their covariance is zero:

$cov( β, ε) = (X t X) -1 X t cov(Y )(I n -H) t = σ 2 (X t X) -1 X t (I n -H t ) = 0$which holds because (I n -H)X = 0 by Lemma 4.1. Second, because σ2 = rss/(n -p) = εt ε/(n -p) is a quadratic function of ε, it is independent of β. We only need to show that it is a scaled chi-squared distribution. This follows from Theorem B.10 in Chapter B due to the Normality of ε/σ with the projection matrix I n -H as its covariance matrix.

□ The second theorem is on the joint distribution of ( Ŷ , ε). We have shown their means and covariance matrix in the last chapter. Because they are linear transformations of Y , they are jointly Normal and independent.

Theorem 5.2 Under Assumption 5.1, we have

$Ŷ ε ∼ N Xβ 0 , σ 2 H 0 0 I n -H , so Ŷ ε.$Recall that we have shown that Y = Ŷ + ε with Ŷ ⊥ ε by the OLS properties, which is a pure linear algebra fact without assumptions. Theorem 4.2 ensures that Ŷ and ε are uncorrelated under Assumption 4.1. Now Theorem 5.2 further ensures that Ŷ ε under Assumption 5.1. The first result states that Ŷ and ε are orthogonal. The second result states that Ŷ and ε are uncorrelated. The third result states Ŷ and ε are independent. They hold under different assumptions.

## Pivotal quantities and statistical inference

## Scalar parameters

We first consider statistical inference for c t β, a one-dimensional linear function of β where c ∈ R p . For example, if c = e j ≡ (0, . . . , 1, . . . , 0) t with only the jth element being one, then c t β = β j is the jth element of β which measures the impact of x ij on y i on average. Standard software packages report statistical inference for each element of β. Sometimes we may also be interested in β j -β j ′ , the difference between the coefficients of two covariates, which corresponds to c = (0, . . . , 0, 1, 0, . . . , 0, -1, 0, . . . , 0) t = e j -e j ′ .

Theorem 5.1 implies that c t β ∼ N c t β, σ 2 c t (X t X) -1 c .

However, this is not directly useful because σ 2 is unknown. With σ 2 replaced by σ2 , the standardized distribution

$T c ≡ c t β -c t β σ2 c t (X t X) -1 c$does not follow N(0, 1) anymore. In fact, it is a t distribution as shown in Theorem 5.3 below.

Theorem 5.3 Under Assumption 5.1, for a fixed vector c ∈ R p , we have

$T c ∼ t n-p .$Proof of Theorem 5.3: From Theorem 5.1, the standardized distribution with the true

$σ 2 follows c t β -c t β σ 2 c t (X t X) -1 c ∼ N(0, 1), σ2 /σ 2 ∼ χ 2 n-p /(n -p)$, and they are independent. These facts imply that

$T c = c t β -c t β σ2 c t (X t X) -1 c = c t β -c t β σ 2 c t (X t X) -1 c σ2 σ 2 ∼ N(0, 1) χ 2 n-p /(n -p)$,

where N(0, 1) and χ 2 n-p denote independent standard Normal and χ 2 n-p random variables, respectively, with a little abuse of notation. Therefore, T c ∼ t n-p by the definition of the t distribution.

□ In Theorem 5.3, the left-hand side depends on the observed data and the unknown true parameters, but the right-hand side is a random variable depending on only the dimension (n, p) of X, but neither the data nor the true parameters. We call the quantity on the left-hand side a pivotal quantity. Based on the quantiles of the t n-p random variable, we can tie the data and the true parameter via the following probability statement

$pr c t β -c t β σ2 c t (X t X) -1 c ≤ t 1-α/2,n-p = 1 -α$for any 0 < α < 1, where t 1-α/2,n-p is the 1 -α/2 quantile of t n-p . When n -p is large (e.g. larger than 30), the 1 -α/2 quantile of t n-p is close to that of N(0, 1). In particular, t 97.5%,n-p ≈ 1.96, the 97.5% quantile of N(0, 1), which is the critical value for the 95% confidence interval. Define σ2 c t (X t X) -1 c ≡ ŝe c which is often called the (estimated) standard error of c t β. Using this definition, we can equivalently write the above probability statement as

$pr c t β -t 1-α/2,n-p ŝe c ≤ c t β ≤ c t β + t 1-α/2,n-p ŝe c = 1 -α.$We use

$c t β ± t 1-α/2,n-p ŝe c$as a 1 -α level confidence interval for c t β. By duality of confidence interval and hypothesis testing, we can also construct a level α test for c t β. More precisely, we reject the null hypothesis c t β = d if the above confidence interval does not cover d, for a fixed number d.

As an important case, c = e j so c t β = β j . Standard software packages, for example, R, report the point estimator βj , the standard error ŝe j = σ2 [(X t X) -1 ] jj , the t statistic T j = βj / ŝe j , and the two-sided p-value pr(|t n-p | ≥ |T j |) for testing whether β j equals zero or not. Section 5.4 below gives some examples.

## Vector parameters

We then consider statistical inference for Cβ, a multi-dimensional linear function of β where C ∈ R l×p . If l = 1, then it reduces to the one-dimensional case. If l > 1, then

$C =    c t 1 . . . c t l    =⇒ Cβ =    c t 1 β . . . c t l β    correspond to the joint value of l parameters c t 1 β, . . . , c t l β. Example 5.1 If C =      0 1 0 • • • 0 0 0 1 • • • 0 . . . . . . . . . • • • . . . 0 0 0 • • • 1      , then Cβ =    β 2 . . . β p    ,$contains all the coefficients except for the first one (the intercept in most cases). Most software packages report the test of the joint significance of (β 2 , . . . , β p ). Section 5.4 below gives some examples.

Example 5.2 Another leading application is to test whether β 2 = 0 in the following regression partitioned by X = (X 1 , X 2 ) where X 1 and X 2 are n × k and n × l matrices:

$Y = X 1 β 1 + X 2 β 2 + ε, with C = 0 l×k I l β 1 β 2 =⇒ Cβ = β 2 .$We will discuss this partitioned regression in more detail in Chapters 7 and 8. Now we will focus on the generic problem of inferring Cβ. To avoid degeneracy, we assume that C does not have redundant rows, quantified below.

Assumption 5.2 C has linearly independent rows.

$Theorem 5.1 implies that C β -Cβ ∼ N 0, σ 2 C(X t X) -1 C t$and therefore the standardized quadratic form has a chi-squared distribution

$(C β -Cβ) t σ 2 C(X t X) -1 C t -1 (C β -Cβ) ∼ χ 2 l .$The above chi-squared distribution follows from the property of the quadratic form of a Normal in Theorem B.10, where σ 2 C(X t X) -[foot_3](#foot_3) C t is a positive definite matrix 1 . Again this is not directly useful with unknown σ 2 . Replacing σ 2 with the unbiased estimator σ2 and using a scaling factor l, we can obtain a pivotal quantity that has an F distribution as summarized in Theorem 5.4 below.

Theorem 5.4 Under Assumptions 5.1 and 5.2, we have

$F C ≡ (C β -Cβ) t C(X t X) -1 C t -1 (C β -Cβ) lσ 2 ∼ F l,n-p .$Proof of Theorem 5.4: Similar to the proof of Theorem 5.3, we apply Theorem 5.1 to derive that

$F C = (C β -Cβ) t σ 2 C(X t X) -1 C t -1 (C β -Cβ)/l σ2 /σ 2 ∼ χ 2 l /l χ 2 n-p /(n -p)$, where χ 2 l and χ 2 n-p denote independent χ 2 l and χ 2 n-p random variables, respectively, with a little abuse of notation. Therefore, F C ∼ F l,n-p by the definition of the F distribution. □ Theorem 5.4 motivates the following confidence region for Cβ:

$r : (C β -r) t C(X t X) -1 C t -1 (C β -r) ≤ lσ 2 f 1-α,l,n-p ,$where f 1-α,l,n-p is the upper α quantile of the F l,n-p distribution. By duality of the confidence region and hypothesis testing, we can also construct a level α test for Cβ. Most statistical packages automatically report the p-value based on the F statistic in Example 5.1. As a final remark, the statistics in Theorems 5.3 and 5.4 are called the Wald-type statistics.

## Prediction based on pivotal quantities

Practitioners use OLS not only to infer β but also to predict future outcomes. For the pair of future data (x n+1 , y n+1 ), we observe only x n+1 and want to predict y n+1 based on (X, Y ) and x n+1 . Assume a stable relationship between y n+1 and x n+1 , that is,

$y n+1 ∼ N(x t n+1 β, σ 2 )$with the same (β, σ 2 ). First, we can predict the mean of y n+1 which is x t n+1 β. It is just a one-dimensional linear function of β, so the theory in Theorem 5.3 is directly applicable. A natural unbiased predictor is x t n+1 β with 1 -α level prediction interval

$x t n+1 β ± t 1-α/2,n-p ŝe xn+1 .$Second, we can predict y n+1 itself, which is a random variable. We can still use x t n+1 β as a natural unbiased predictor but need to modify the prediction interval. Because y n+1 β, we have

$y n+1 -x t n+1 β ∼ N 0, σ 2 + σ 2 x t n+1 (X t X) -1$x n+1 , and therefore

$y n+1 -x t n+1 β σ2 + σ2 x t n+1 (X t X) -1 x n+1 = y n+1 -x t n+1 β σ 2 + σ 2 x t n+1 (X t X) -1 x n+1 σ2 σ 2 ∼ N(0, 1) χ 2 n-p /(n -p)$,

where N(0, 1) and χ 2 n-p denote independent standard Normal and χ 2 n-p random variables, respectively, with a little abuse of notation. Therefore,

$y n+1 -x t n+1 β σ2 + σ2 x t n+1 (X t X) -1 x n+1 ∼ t n-p$is a pivotal quantity. Define the squared prediction error as

$pe 2 xn+1 = σ2 + σ2 x t n+1 (X t X) -1 x n+1 = σ2    1 + n -1 x t n+1 n -1 n i=1 x i x t i -1 x n+1    ,$which has two components. The first one has magnitude close to σ 2 which is of constant order. The second one has a magnitude decreasing in n if n -1 n i=1 x i x t i converges to a finite limit with large n. Therefore, the first component dominates the second one with large n, which results in the main difference between predicting the mean of y n+1 and predicting y n+1 itself. Using the notation pe xn+1 , we can construct the following 1 -α level prediction interval:

x t n+1 β ± t 1-α/2,n-p pe xn+1 .

## Examples and R implementation

Below I illustrate the theory in this chapter with two classic datasets.

## Univariate regression

Revisiting Galton's data, we have the following result:

> library ( " HistData " ) > galton _ fit = lm ( childHeight ~midparentHeight , + data = Galt onFamil ies ) > round ( summary ( galton _ fit )$ coef , 3 ) Estimate Std . Error t value Pr ( >| t |) ( Intercept ) 2 2 . 6 3 6 4 . 2 6 5 5 . 3 0 7 0 m id pa r en tH ei g ht 0 . 6 3 7 0 . 0 6 2 1 0 . 3 4 5 0

With the fitted line, we can predict childHeight at different values of midparentHeight. In the predict function, if we specify interval = "confidence", it gives the confidence intervals for the means of the new outcomes; if we specify interval = "prediction", it gives the prediction intervals for the new outcomes themselves.

$> new _ mph = seq ( 6 0 , 8 0 , by = 0 . 5 ) > new _ data = data . frame ( mi d pa re nt H ei gh t = new _ mph ) > new _ ci = predict ( galton _ fit , new _ data , + interval = " confidence " ) > new _ pi = predict ( galton _ fit , new _ data , + interval = " prediction " ) > round ( head ( cbind ( new _ ci , new _ pi )) , 3 )$fit lwr upr fit lwr upr 1 6 0 . 8 7 8 5 9 . 7 4 4 6 2 . 0 1 2 6 0 . 8 7 8 5 4 . 1 2 6 6 7 . 6 3 0 2 6 1 . 1 9 7 6 0 . 1 2 2 6 2 . 2 7 2 6 1 . 1 9 7 5 4 . 4 5 4 6 7 . 9 3 9 3 6 1 . 5 1 5 6 0 . 4 9 9 6 2 . 5 3 1 6 1 . 5 1 5 5 4 . 7 8 2 6 8 . 2 4 9 4 6 1 . 8 3 4 6 0 . 8 7 7 6 2 . 7 9 1 6 1 . 8 3 4 5 5 . 1 0 9 6 8 . 5 5 9 5 6 2 . 1 5 3 6 1 . 2 5 4 6 3 . 0 5 1 6 2 . 1 5 3 5 5 . 4 3 6 6 8 . 8 6 9 6 6 2 . 4 7 1 6 1 . 6 3 2 6 3 . 3 1 1 6 2 . 4 7 1 5 5 . 7 6 2 6 9 . 1 8 0 Figure 5.1 plots the fitted line as well as the confidence intervals and prediction intervals at level 95%. The file code5.4.1.R contains the R code.

## Multivariate regression

The R package Matching contains an experimental dataset lalonde from LaLonde [(1986)](#). Units were randomly assigned to the job training program, with treat being the treatment indicator. The outcome re78 is the real earnings in 1978, and other variables are pretreatment covariates. From the simple OLS, the treatment has a significant positive effect, whereas none of the covariates are predictive of the outcome.

> library ( " Matching " ) > data ( lalonde ) > lalonde _ fit = lm ( re 7 8 ~. , data = lalonde ) > summary ( lalonde _ fit ) Call : lm ( formula = re 7 8 ~. , data = lalonde )

## Residuals :

Min 1 Q Median 3 Q Max -9 6 1 2 -4 3 5 5 -1 5 7 2 3 0 5 4 5 3 1 1 9 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 60 70 80 Coefficients : Estimate Std . Error t value Pr ( >| t |) ( Intercept ) 2 . 5 6 7 e + 0 2 3 . 5 2 2 e + 0 3 0 . 0 7 3 0 . 9 4 1 9 3 age 5 . 3 5 7 e + 0 1 4 . 5 8 1 e + 0 1 1 . 1 7 0 0 . 2 4 2 8 4 educ 4 . 0 0 8 e + 0 2 2 . 2 8 8 e + 0 2 1 . 7 5 1 0 . 0 8 0 5 8 . black -2 . 0 3 7 e + 0 3 1 . 1 7 4 e + 0 3 -1 . 7 3 6 0 . 0 8 3 3 1 . hisp 4 . 2 5 8 e + 0 2 1 . 5 6 5 e + 0 3 0 . 2 7 2 0 . 7 8 5 6 2 married -1 . 4 6 3 e + 0 2 8 . 8 2 3 e + 0 2 -0 . 1 6 6 0 . 8 6 8 3 5 nodegr -1 . 5 1 8 e + 0 1 1 . 0 0 6 e + 0 3 -0 . 0 1 5 0 . 9 8 7 9 7 re 7 4 1 . 2 3 4e -0 1 8 . 7 8 4e -0 2 1 . 4 0 5 0 . 1 6 0 7 9 re 7 5 1 . 9 7 4e -0 2 1 . 5 0 3e -0 1 0 . 1 3 1 0 . 8 9 5 5 4 u 7 4

1 . 3 8 0 e + 0 3 1 . 1 8 8 e + 0 3 1 . 1 6 2 0 . 2 4 5 9 0 u 7 5 -1 . 0 7 1 e + 0 3 1 . 0 2 5 e + 0 3 -1 . 0 4 5 0 . 2 9 6 5 1 treat 1 . 6 7 1 e + 0 3 6 . 4 1 1 e + 0 2 2 . 6 0 6 0 . 0 0 9 4 8 **

Residual standard error : 6 5 1 7 on 4 3 3 degrees of freedom Multiple R -squared : 0 . 0 5 8 2 2 , Adjusted R -squared : 0 . 0 3 4 3 F -statistic : 2 . 4 3 3 on 1 1 and 4 3 3 DF , p -value : 0 . 0 0 5 9 7 4

The above result shows that none of the pretreatment covariates is significant. It is also of interest to test whether they are jointly significant. The result below shows that they are only marginally significant at the level 0.05 based on a joint test.

> library ( " car " ) > l i n e a r H y p o t he s i s ( lalonde _ fit , + c ( " age = 0 " , " educ = 0 " , " black = 0 " , + " hisp = 0 " , " married = 0 " , " nodegr = 0 " , + " re 7 4 = 0 " , " re 7 5 = 0 " , " u 7 4 = 0 " , + " u 7 5 = 0 " )) Linear hypothesis test Hypothesis : age = 0 educ = 0 black = 0 hisp = 0 married = 0 nodegr = 0 re 7 4 = 0 re 7 5 = 0 u 7 4 = 0 u 7 5 = 0 Model 1 : restricted model Model 2 : re 7 8 ~age + educ + black + hisp + married + nodegr + re 7 4 + re 7 5 + u 7 4 + u 7 5 + treat

Res . Df RSS Df Sum of Sq F Pr ( > F ) 1 4 4 3 1 . 9 1 7 8 e + 1 0 2 4 3 3 1 . 8 3 8 9 e + 1 0 1 0 7 8 8 7 9 9 0 2 3 1 . 8 5 7 4 0 . 0 4 9 2 9 * Below I create two pseudo datasets: one with all units assigned to the treatment, and the other with all units assigned to the control, fixing all the pretreatment covariates. The predicted outcomes are the counterfactual outcomes under the treatment and control. I further calculate their means and verify that their difference equals the OLS coefficient of

$treat. > new _ treat = lalonde > new _ treat $ treat = 1 > predict _ lalonde 1 = predict ( lalonde _ fit , new _ treat , + interval = " none " ) > new _ control = lalonde > new _ control $ treat = 0 > predict _ lalonde 0$= predict ( lalonde _ fit , new _ control , + interval = " none " ) > mean ( predict _ lalonde 1 ) [ 1 ] 6 2 7 6 . 9 1 > mean ( predict _ lalonde 0 ) [ 1 ] 4 6 0 6 . 2 0 1 > > mean ( predict _ lalonde 1 ) -mean ( predict _ lalonde 0 ) [ 1 ] 1 6 7 0 . 7 0 9

## Homework problems

## MLE

Under the Normal linear model, show that the maximum likelihood estimator (MLE) for β is the OLS estimator, but the MLE for σ 2 is σ2 = rss/n. Compare the mean squared errors of σ2 and σ2 for estimating σ 2 .

## MLE with Laplace errors

Assume that y i = x t i β + σε i where the ε i 's are i.i.d. Laplace distribution with density f (ε) = 2 -1 e -|ε| (i = 1, . . . , n). Find the MLEs of (β, σ 2 ).

Remark: We will revisit this problem in Chapter 26.

## Joint prediction

With multiple future data points (X n+1 , Y n+1 ) where X n+1 ∈ R l×p and Y n+1 ∈ R l , construct the joint predictors and prediction region for Y n+1 based on (X, Y ) and X n+1 . As a starting point, you can assume that l ≤ p and the rows of X n+1 are linearly independent. You can then consider the case in which the rows of X n+1 are not linearly independent.

Hint: Use Theorem B.10.

## Two-sample problem

$1. Assume that z 1 , . . . , z m iid ∼ N(µ 1 , σ 2 ) and w 1 , . . . , w n iid ∼ N(µ 2 , σ 2 )$, and test H 0 : µ 1 = µ 2 . Show that under H 0 , the t statistic with pooled variance estimator have the following distribution:

$t equal = z -w σ2 (m -1 + n -1 ) ∼ t m+n-2 , where σ2 = (m -1)S 2 z + (n -1)S 2 w /(m + n -2) with the sample means z = m -1 m i=1 z i , w = n -1 n i=1 w i ,$and the sample variances

$S 2 z = (m -1) -1 m i=1 (z i -z) 2 , S 2 w = (n -1) -1 n i=1 (w i -w) 2 .$Remark: The name "equal" is motivated by the "var.equal" parameter of the R function t.test.

2. We can write the above problem as testing hypothesis H 0 : β 1 = 0 in the linear regression

$Y = Xβ + ε with Y =           z 1 . . . z m w 1 . . . w n           , X =           1 1 . . . . . . 1 1 1 0 . . . . . . 1 0           , β = β 0 β 1 , ε =           ε 1 . . . ε m ε m+1 . . . ε m+n           .$Based on the Normal linear model, we can compute the t statistic. Show that it is identical to t equal .

## Analysis of Variance (ANOVA) with a multi-level treatment

Let x i be the indicator vector for J treatment levels in a completely randomized experiment, for example, x i = e j = (0, . . . , 1, . . . , 0) t with the jth element being one if unit i receives treatment level j (j = 1, . . . , J). Let y i be the outcome of unit i (i = 1, . . . , n). Let T j be the indices of units receiving treatment j, and let n j = |T j | be the sample size and ȳj = n -1 j i∈Tj y i be the sample mean of the outcomes under treatment j. Define ȳ = n -1 n i=1 y i as the grand mean. We can test whether the treatment has any effect on the outcome by testing the null hypothesis

$H 0 : β 1 = • • • = β J in the Normal linear model Y = Xβ + ε assuming ε ∼ N(0, σ 2 I n )$. This is a special case of testing Cβ = 0. Find C and show that the corresponding F statistic is identical to

$F = J j=1 n j (ȳ j -ȳ) 2 /(J -1) J j=1 i∈Tj (y i -ȳj ) 2 /(n -J) ∼ F J-1,n-J .$Remarks: (1) This is Fisher's F statistic. [(2)](#) In this linear model formulation, X does not contain a column of 1's. (3) The choice of C is not unique, but the final formula for F is. (4) You may use the Sherman-Morrison formula in Problem 1.3.

## Confidence interval for σ 2

Based on Theorem 5.1, construct a 1 -α level confidence interval for σ 2 .

## Relationship between t and F

Show that when C containing only one row c t , then T 2 c = F C , where T c is defined in Theorem 5.3 and F C is defined in Theorem 5.4.

## rss and t-statistic in univariate OLS

Focus on univariate OLS discussed in Chapter 2:

$y i = α + βx i + εi (i = 1, . . . , n). Show that rss equals n i=1 ε2 i = n i=1 (y i -ȳ) 2 (1 -ρ2 xy )$and under the homoskedasticity assumption, the t-statistic associated with β equals ρxy

$(1 -ρ2 xy )/(n -2)$.

## Equivalence of the t-statistics

With the data (x i , y i ) n i=1 where both x i and y i are scalars. Run OLS fit of y i on (1, x i ) to obtain t y|x , the t-statistic of the coefficient of x i , under the homoskedasticity assumption. Run OLS fit of x i on (1, y i ) to obtain t x|y , the t-statistic of the coefficient of y i , under the homoskedasticity assumption.

Show t y|x = t x|y . Remark: This is a numerical result that holds without any stochastic assumptions. I give an example below.

> library ( MASS ) > # simulate bivariate normal distribution > xy = mvrnorm ( n = 1 0 0 , mu = c ( 0 , 0 ) , + Sigma = matrix ( c ( 1 , 0 . 5 , 0 . 5 , 1 ) , ncol = 2 )) > xy = as . data . frame ( xy ) > colnames ( xy ) = c ( " x " , " y " ) > # # OLS > reg . y . x = lm ( y ~x , data = xy ) > reg . x . y = lm ( x ~y , data = xy ) > # # compare t statistics based on homoskedastic errors > summary ( reg . y . x )$ coef [[ 2 , 3 ]](#) [ 1 ] 4 . 4 7 0 3 3 1 > summary ( reg . x . y )$ coef [[ 2 , 3 ]](#) [ 1 ] 4 . 4 7 0 3 3 1

The equivalence of the t-statistics from the OLS fit of y on x and that of x on y demonstrates that based on OLS, the data do not contain any information about the direction of the relationship between x and y.

## An application

The R package sampleSelection [(Toomet and Henningsen, 2008)](#b223) describes the dataset RandHIE as follows: "The RAND Health Insurance Experiment was a comprehensive study of health care cost, utilization and outcome in the United States. It is the only randomized study of health insurance, and the only study which can give definitive evidence as to the causal effects of different health insurance plans." You can find more detailed information about other variables in this package. The main outcome of interest lnmeddol means the log of medical expenses. Use linear regression to investigate the relationship between the outcome and various important covariates.

Note that the solution to this problem is not unique, but you need to justify your choice of covariates and model, and need to interpret the results.

Asymptotic Inference in OLS: the Eicker-Huber-White (EHW) robust standard error

## Motivation

Standard software packages, for example, R, report the point estimator, standard error, and p-value for each coordinate of β based on the Normal linear model:

$Y = Xβ + ε ∼ N(Xβ, σ 2 I n ).$Statistical inference based on this model is finite-sample exact. However, the assumptions of this model are extremely strong: the functional form is linear, the error terms are additive with distributions not dependent on X, and the error terms are IID Normal with the same variance. If we do not believe some of these assumptions, can we still trust the associated statistical inference? Let us start with some simple numerical examples, with the R code in code6.1.R.

## Numerical examples

The first one is the ideal Normal linear model:

$> library ( car ) > n = 2 0 0 > x = runif (n , -2 , 2 ) > beta = 1 > xbeta = x * beta > Simu 1 = replicate ( 5 0 0 0 , + { y = xbeta + rnorm ( n ) + ols . fit = lm ( y ~x ) + c ( summary ( ols . fit )$ coef [ 2 , 1 : 2 ] , + sqrt ( hccm ( ols . fit )[ 2 , 2 ])) + })$In the above, we generate outcomes from a simple linear model

$y i = x i + ε i with ε i iid ∼ N(0, σ 2 = 1)$. Over 5000 replications of the data, we computed the OLS coefficient β of x i and reported two standard errors. One is the standard error discussed in Chapter 5 under the Normal linear model, which is also the default choice of the lm function of R. The other one, computed by the hccm function in the R package car, will be the main topic of this chapter. The (1, 1) the panel of Figure [6](#).1 shows the histogram of the estimator and reports the standard error (se0), as well as two estimated standard errors (se1 and se2). The distribution of β is symmetric and bell-shaped around the true parameter 1, and the estimated standard errors are close to the true one.

To investigate the impact of Normality, we change the error terms to be IID exponential with mean 1 and variance 1.

> Simu 2 = replicate ( 5 0 0 0 ,

$+ { y = xbeta + rexp ( n ) + ols . fit = lm ( y ~x ) + c ( summary ( ols . fit )$ coef [ 2 , 1 : 2 ] , + sqrt ( hccm ( ols . fit )[ 2 , 2 ])) + })$The (1, 2) panel of Figure [6](#).1 corresponds to this setting. With non-Normal errors, β is still symmetric and bell-shaped around the true parameter 1, and the estimated standard errors are close to the true one. So the Normality of the error terms does not seem to be a crucial assumption for the validity of the inference procedure under the Normal linear model.

We then generate errors from Normal with variance depending on x:

> Simu 3 = replicate ( 5 0 0 0 , + { y = xbeta + rnorm (n , 0 , abs ( x )) + ols . fit = lm ( y ~x ) + c ( summary ( ols . fit )$ coef [ 2 , 1 : 2 ] , + sqrt ( hccm ( ols . fit ) [[ 2 , 2 ]](#))) + })

The (2, 1) panel of Figure [6](#).1 corresponds to this setting. With heteroskedastic Normal errors, β is symmetric and bell-shaped around the true parameter 1, se2 is close to se0, but se1 underestimates se0. So the heteroskedasticity of the error terms does not change the Normality of the OLS estimator dramatically, although the statistical inference discussed in Chapter 5 can be invalid. Finally, we generate heteroskedastic non-Normal errors:

$> Simu 4 = replicate ( 5 0 0 0 , + { y = xbeta + runif (n , -x ^2 , x ^2 ) + ols . fit = lm ( y ~x ) + c ( summary ( ols . fit )$ coef [ 2 , 1 : 2 ] , + sqrt ( hccm ( ols . fit )[ 2 , 2 ])) + })$The (2, 2) panel of Figure [6](#).1 corresponds to this setting, which has a similar pattern as the (2, 1) panel. So the Normality of the error terms is not crucial, but the homoskedasticity is.

## Goal of this chapter

In this chapter, we will still impose the linearity assumption, but relax the distributional assumption on the error terms. We assume the following heteroskedastic linear model.

Assumption 6.1 (Heteroskedastic linear model) We have

$y i = x t i β + ε i ,$where the ε i 's are independent with mean zero and variance σ 2 i (i = 1, . . . , n). The design matrix X is fixed with linearly independent column vectors, and (β, σ 2 1 , . . . , σ 2 n ) are unknown parameters.

Because the error terms can have different variances, they are not IID in general under the heteroskedastic linear model. Their variances can be functions of the x i 's, and the variances σ 2 i are n free unknown numbers. Again, treating the x i 's as fixed is not essential, because we can condition on them if they are random. Without imposing Normality on the error terms, we cannot determine the finite sample exact distribution of the OLS estimator. This chapter will use the asymptotic analysis, assuming that the sample size n is large so that certain limiting theorems hold.

The asymptotic analysis later will show that if the error terms are homoskedastic, i.e., σ 2 i = σ 2 for all i = 1, . . . , n, we can still trust the statistical inference discussed in Chapter 5 based on the Normal linear model as long the central limit theorem (CLT) for the OLS estimator holds as n → ∞. If the error terms are heteroskedastic, i.e., their variances are different, we must adjust the standard error with the so-called Eicker-Huber-White heteroskedasticity robust standard error. I will give the technical details below. If you are unfamiliar with the asymptotic analysis, please first review the basics in Chapter C.

## Consistency of OLS

Under the heteroskedastic linear model, the OLS estimator β is still unbiased for β because the error terms have to mean zero. Moreover, we can show that it is consistent for β with large n and some regularity conditions. We start with a useful lemma. Lemma 6.1 Under Assumption 6.1, the OLS estimator has the representation β -β = B -1 n ξ n , where

$B n = n -1 n i=1 x i x t i , ξ n = n -1 n i=1 x i ε i Proof of Lemma 6.1: Since y i = x t i β + ε i , we have β = B -1 n n -1 n i=1 x i y i = B -1 n n -1 n i=1 x i (x t i β + ε i ) = B -1 n B n β + B -1 n n -1 n i=1 x i ε i = β + B -1 n n -1 n i=1 x i ε i . □ In the representation of Lemma 6.1, B n is fixed and ξ n is random. Since E(ξ n ) = 0, we know that E( β) = β, so the OLS estimator is unbiased. Moreover, cov(ξ n ) = cov n -1 n i=1 x i ε i = n -2 n i=1 σ 2 i x i x t i = M n /n,$where

$M n = n -1 n i=1 σ 2 i x i x t i .$So the covariance of the OLS estimator is

$cov( β) = n -1 B -1 n M n B -1 n .$It has a sandwich form, justifying the choice of notation B n for the "bread" and M n for the "meat." Intuitively, if B n and M n have finite limits, then the covariance of β shrinks to zero with large n, implying that β will concentrate near its mean β. This is the idea of consistency, formally stated below. Assumption 6.2 B n → B and M n → M where B and M are finite with B invertible. Theorem 6.1 Under Assumptions 6.1 and 6.2, we have β → β in probability.

Proof of Theorem 6.1: We only need to show that ξ n → 0 in probability. It has mean zero and covariance matrix M n /n, so it converges to zero in probability using Proposition C.4 in Chapter C. □

## Asymptotic Normality of the OLS estimator

Intuitively, ξ n is the sample average of some independent terms, and therefore, the classic Lindberg-Feller theorem guarantees that it enjoys a CLT under some regularity conditions. Consequently, β also enjoys a CLT with mean β and covariance matrix n -1 B -1 n M n B -1 n . The asymptotic results in this chapter require rather tedious regularity conditions. I give them for generality, and they hold automatically if we are willing to assume that the covariates and error terms are all bounded by a constant not depending on n. These general conditions are basically moment conditions required by the law of large numbers and CLT. You do not have to pay too much attention to the conditions when you first read this chapter.

The CLT relies on an additional condition on a higher-order moment

$d 2+δ,n = n -1 n i=1 ∥x i ∥ 2+δ E(ε 2+δ i ).$Theorem 6.2 Under Assumptions 6.1 and 6.2, if there exist a δ > 0 and C > 0 not depending on n such that d 2+δ,n ≤ C, then

$√ n( β -β) → N(0, B -1 M B -1 )$in distribution.

Proof of Theorem 6.2:

The key is to show the CLT for ξ n , and the CLT for β holds due to the Slutsky's Theorem; see Chapter C for a review. Define

$z n,i = n -1/2 x i ε i , (i = 1, . . . , n)$with mean zero and finite covariance, and we need to verify the two conditions required by the Lindeberg-Feller CLT stated as Proposition C.8 in Chapter C. First, the Lyapunov condition holds because

$n i=1 E ∥z n,i ∥ 2+δ = n i=1 E n -(2+δ)/2 ∥x i ∥ 2+δ ε 2+δ i = n -δ/2 × n -1 n i=1 ∥x i ∥ 2+δ E(ε 2+δ i ) = n -δ/2 × d 2+δ,n → 0. Second, n i=1 cov(z n,i ) = n -1 n i=1 σ 2 i x i x t i = M n → M. So the Lindberg-Feller CLT implies that n -1/2 n i=1 x i ε i = n i=1 z n,i → N(0, M ) in distri- bution. □$6.4 Eicker-Huber-White standard error

## Sandwich variance estimator

The CLT in Theorem 6.2 shows that

$β a ∼ N(β, n -1 B -1 M B -1 ),$where a ∼ denotes "approximation in distribution." However, the asymptotic covariance is unknown, and we need to use the data to construct a reasonable estimator for statistical inference. It is relatively easy to replace B with its sample analog B n , but

$Mn = n -1 n i=1 ε 2 i x i x t i$as the sample analog for M is not directly useful because the error terms are unknown either. It is natural to use ε2 i to replace ε 2 i to obtain the following estimator for M :

$Mn = n -1 n i=1 ε2 i x i x t i .$Although each ε2 i is a poor estimator for σ 2 i , the sample average Mn turns out to be well-behaved with large n and the regularity conditions below. Theorem 6.3 Under Assumptions 6.1 and 6.2, we have Mn → M in probability if

$n -1 n i=1 var(ε 2 i )x 2 ij1 x 2 ij2 , n -1 n i=1 x ij1 x ij2 x ij3 x ij4 , n -1 n i=1 σ 2 i x 2 ij1 x 2 ij2 x 2 ij3 (6.1)$are bounded from above by a constant C not depending on n for any j 1 , j 2 , j 3 , j 4 = 1, . . . , p.

Proof of Theorem 6.3: Assumption 6.2 ensures that β → β in probability by Theorem 6.1. Markov's inequality and the boundedness of the first term in (6.1) ensure that Mn -M n → 0 in probability. So we only need to show that Mn -Mn → 0 in probability. The (j 1 , j 2 )th element of their difference is

$( Mn -Mn ) j1,j2 = n -1 n i=1 ε2 i x i,j1 x i,j2 -n -1 n i=1 ε 2 i x i,j1 x i,j2 = n -1 n i=1 ε i + x t i β -x t i β 2 -ε 2 i x i,j1 x i,j2 = n -1 n i=1 x t i β -x t i β 2 + 2ε i x t i β -x t i β x i,j1 x i,j2 = (β -β) t n -1 n i=1 x i x t i x i,j1 x i,j2 (β -β) + 2(β -β) t n -1 n i=1 x i x i,j1 x i,j2 ε i .$It converges to zero in probability because the first term converges to zero due to the boundedness of the second term in (6.1), and the second term converges to zero in probability due to Markov's inequality and the boundedness of the third term in (6.1). □ The final variance estimator for β is

$Vehw = n -1 n -1 n i=1 x i x t i -1 n -1 n i=1 ε2 i x i x t i n -1 n i=1 x i x t i -1$, which is called the Eicker-Huber-White (EHW) heteroskedasticity robust covariance matrix. In matrix form, it equals Vehw = (X t X) -1 (X t ΩX)(X t X) -1 ,

where Ω = diag ε2 1 , . . . , ε2 n . [Eicker (1967)](#b96) first proposed to use Vehw . [White (1980a)](#b236) popularized it in economics which has been influential in empirical research. Related estimators appeared in many other contexts of statistics. [Cox (1961)](#b81) and [Huber (1967)](#b146) discussed the sandwich variance in the context of misspecified parametric models; see Section D. [2. Fuller (1975)](#) proposed a more general form of Vehw in the context of survey sampling. The square root of the diagonal terms of Vehw , denoted by ŝe ehw,j (j = 1, . . . , p), are called the heteroskedasticity-consistent standard errors, heteroskedasticity-robust standard errors, White standard errors, Huber-White standard errors, or Eicker-Huber-White standard errors, among many other names.

We can conduct statistical inference based on Normal approximations. For example, we can test linear hypotheses based on

$β a ∼ N(β, Vehw ),$and in particular, we can infer each element of the coefficient based on βj a ∼ N(β j , ŝe 2 ehw,j ).

## Other "HC" standard errors

Statistical inference based on the EHW standard error relaxes the parametric assumptions of the Normal linear model. However, its validity relies strongly on the asymptotic argument.

At finite samples, it can have poor behaviors. Since [White (1980a)](#b236) published his paper, several modifications of V appeared aiming for better finite-sample properties. I summarize them below. They all rely on the h ii 's, which are the diagonal elements of the projection matrix H and called the leverage scores. Define

$Vehw,k = n -1 n -1 n i=1 x i x t i -1 n -1 n i=1 ε2 i,k x i x t i n -1 n i=1 x i x t i -1 , where εi,k =                εi , (k = 0, HC0); εi n n-p , (k = 1, HC1); εi / √ 1 -h ii , (k = 2, HC2); εi /(1 -h ii ), (k = 3, HC3); εi /(1 -h ii ) min{2,nhii/(2p)} , (k = 4, HC4).$The HC1 correction is similar to the degrees of freedom correction in the OLS covariance estimator. The HC2 correction was motivated by the unbiasedness of covariance when the error terms have the same variance; see Problem 6.8 for more details. The HC3 correction was motivated by a method called jackknife which will be discussed in Chapter 11. This version appeared even early than [White (1980a)](#b236); see [Miller (1974)](#b179), [Hinkley (1977), and](#b138)[Reeds (1978)](#b198). I do not have a good intuition for the HC4 correction. See MacKinnon and [White (1985)](#b172), [Long and Ervin (2000)](#b169) and [Cribari-Neto (2004)](#b86) for reviews. Using simulation studies, [Long and Ervin (2000)](#b169) recommended HC3.

## Special case with homoskedasticity

As an important special case with σ 2 i = σ 2 for all i = 1, . . . , n, we have

$M n = σ 2 n -1 n i=1 x i x t i = σ 2 B n ,$which simplifies the covariance of β to cov( β) = σ 2 B -1 n /n, and the asymptotic Normality to √ n( β -β) → N(0, σ 2 B -1 ) in distribution. We have shown that under the Gauss-Markov model, σ2 = (n -p) -1 n i=1 ε2

i is unbiased for σ 2 . Moreover, σ2 is consistent for σ 2 under the same condition as Theorem 6.1, justifying the use of

$V = σ2 n i=1 x i x t i = σ2 (X t X) -1$as the covariance estimator. So under homoskedasticity, we can conduct statistical inference based on the following approximate Normality:

$β a ∼ N β, σ2 (X t X) -1 .$It is slightly different from the inference based on t and F distributions. But with large n, the difference is very small. I will end this section with a formal result on the consistency of σ2 .

Theorem 6.4 Under Assumptions 6.1 and 6.2, we have σ2 → σ 2 in probability if σ 2 i = σ 2 < ∞ for all i = 1, . . . , n and n -1 n i=1 var(ε 2 i ) is bounded above by a constant not depending on n.

Proof of Theorem 6.4: Using Markov's inequality, we can show that n -1 n i=1 ε 2 i → σ 2 in probability. In addition, n -1 n i=1 ε2 i has the same probability limit as σ2 . So we only need to show that n

$-1 n i=1 ε2 i -n -1 n i=1 ε 2 i → 0 in probability. Their difference is n -1 n i=1 ε2 i -n -1 n i=1 ε 2 i = n -1 n i=1 ε i + x t i β -x t i β 2 -ε 2 i = n -1 n i=1 x t i β -x t i β 2 + 2 x t i β -x t i β ε i = (β -β) t n -1 n i=1 x i x t i (β -β) + 2(β -β) t n -1 n i=1 x i ε i = -(β -β) t n -1 n i=1 x i x t i (β -β),$where the last step follows from Lemma 6.1. So the difference converges to zero in probability because β -β → 0 in probability by Theorem 6.1 and B n → B by Assumption 6.2. □

## Examples

I use three examples to compare various standard errors for the regression coefficients, with the R code in code6.5.R. The car package contains the hccm function that implements the EHW standard errors.

> library ( " car " )

## LaLonde experimental data

First, I revisit the lalonde data. In the following analysis, different standard errors give similar t-values. Only treat is significant, but all other pretreatment covariates are not.

> library ( " Matching " ) > data ( lalonde ) > ols . fit = lm ( re 7 8 ~. , data = lalonde ) > ols . fit . hc 0 = sqrt ( diag ( hccm ( ols . fit , type = " hc 0 " ))) > ols . fit . hc 1 = sqrt ( diag ( hccm ( ols . fit , type = " hc 1 " ))) > ols . fit . hc 2 = sqrt ( diag ( hccm ( ols . fit , type = " hc 2 " ))) > ols . fit . hc 3 = sqrt ( diag ( hccm ( ols . fit , type = " hc 3 " ))) > ols . fit . hc 4 = sqrt ( diag ( hccm ( ols . fit , type = " hc 4 " ))) > ols . fit . coef = summary ( ols . fit )$ coef > tvalues = ols . fit . coef [ , 1 ]/ + cbind ( ols . fit . coef [ , 2 ] , ols . fit . hc 0 , ols . fit . hc 1 , + ols . fit . hc 2 , ols . fit . hc 3 , ols . fit . hc 4 ) > colnames ( tvalues ) = c ( " ols " , " hc 0 " , " hc 1 " , " hc 2 " , " hc 3 " , " hc 4 " ) > round ( tvalues , 2 ) ols hc 0 hc 1 hc 2 hc 3 hc 4 ( Intercept ) 0 . 0 7 0 . 0 7 0 . 0 7 0 . 0 7 0 . 0 7 0 . 0 7 age 1 . 1 7 1 . 2 9 1 . 2 8 1 . 2 7 1 . 2 5 1 . 2 5 educ 1 . 7 5 2 . 0 3 2 . 0 0 1 . 9 9 1 . 9 4 1 . 9 2 black -1 . 7 4 -2 . 0 0 -1 . 9 7 -1 . 9 5 -1 . 9 1 -1 . 9 1 hisp 0 . 2 7 0 . 3 0 0 . 3 0 0 . 3 0 0 . 2 9 0 . 2 9 married -0 . 1 7 -0 . 1 7 -0 . 1 7 -0 . 1 7 -0 . 1 6 -0 . 1 6 nodegr -0 . 0 2 -0 . 0 1 -0 . 0 1 -0 . 0 1 -0 . 0 1 -0 . 0 1 re 7 4 1 . 4 0 0 . 9 8 0 . 9 6 0 . 9 2 0 . 8 7 0 . 7 7 re 7 5 0 . 1 3 0 . 1 4 0 . 1 4 0 . 1 3 0 . 1 3 0 . 1 2 u 7 4

1 . 1 6 0 . 8 9 0 . 8 8 0 . 8 7 0 . 8 5 0 . 8 3 u 7 5 -1 . 0 5 -0 . 7 6 -0 . 7 5 -0 . 7 5 -0 . 7 4 -0 . 7 4 treat 2 . 6 1 2 . 4 9 2 . 4 6 2 . 4 5 2 . 4 1 2 . 4 0 6.5.2 Data from [King and Roberts (2015)](#b156) The following example comes from [King and Roberts (2015)](#b156). The outcome variable is the multilateral aid flows, and the covariates include log population, log population squared, gross domestic product, former colony status, distance from the Western world, political freedom, military expenditures, arms imports, and the indicators for the years. Different robust standard errors are quite different for some coefficients.

> library ( foreign ) > dat = read . dta ( " isq . dta " ) > dat = na . omit ( dat [ , c ( " multish " , " lnpop " , " lnpopsq " , + " lngdp " , " lncolony " , " lndist " , + " freedom " , " militexp " , " arms " , + " year 8 3 " , " year 8 6 " , " year 8 9 " , " year 9 2 " )]) > ols . fit = lm ( multish ~lnpop + lnpopsq + lngdp + lncolony + + lndist + freedom + militexp + arms + + year 8 3 + year 8 6 + year 8 9 + year 9 2 , data = dat ) > ols . fit . hc 0 = sqrt ( diag ( hccm ( ols . fit , type = " hc 0 " ))) > ols . fit . hc 1 = sqrt ( diag ( hccm ( ols . fit , type = " hc 1 " ))) > ols . fit . hc 2 = sqrt ( diag ( hccm ( ols . fit , type = " hc 2 " ))) > ols . fit . hc 3 = sqrt ( diag ( hccm ( ols . fit , type = " hc 3 " ))) > ols . fit . hc 4 = sqrt ( diag ( hccm ( ols . fit , type = " hc 4 " ))) > ols . fit . coef = summary ( ols . fit )$ coef > tvalues = ols . fit . coef [ , 1 ]/ + cbind ( ols . fit . coef [ , 2 ] , ols . fit . hc 0 , ols . fit . hc 1 , + ols . fit . hc 2 , ols . fit . hc 3 , ols . fit . hc 4 ) > colnames ( tvalues ) = c ( " ols " , " hc 0 " , " hc 1 " , " hc 2 " , " hc 3 " , " hc 4 " ) > round ( tvalues , 2 ) ols hc 0 hc 1 hc 2 hc 3 hc 4 ( Intercept ) 7 . 4 0 4 . 6 0 4 . 5 4 4 . 4 3 4 . 2 7 4 . 1 4 lnpop -8 . 2 5 -4 . 4 6 -4 . 4 0 -4 . 3 0 -4 . 1 4 -4 . 0 1 lnpopsq 9 . 5 6 4 . 7 9 4 . 7 2 4 . 6 1 4 . 4 4 4 . 3 1 lngdp -6 . 3 9 -6 . 1 4 -6 . 0 6 -6 . 0 1 -5 . 8 8 -5 . 8 6 lncolony 4 . 7 0 4 . 7 5 4 . 6 9 4 . 6 4 4 . 5 3 4 . 4 7 lndist -0 . 1 4 -0 . 1 6 -0 . 1 6 -0 . 1 6 -0 . 1 5 -0 . 1 6 freedom 2 . 2 5 1 . 8 0 1 . 7 8 1 . 7 5 1 . 6 9 1 . 6 5 militexp 0 . 5 1 0 . 5 9 0 . 5 9 0 . 5 7 0 . 5 5 0 . 5 2 arms 1 . 3 4 1 . 1 7 1 . 1 5 1 . 1 0 1 . 0 3 0 . 9 1 year 8 3 1 . 0 5 0 . 8 5 0 . 8 4 0 . 8 3 0 . 8 0 0 . 7 9 year 8 6 0 . 3 5 0 . 4 0 0 . 3 9 0 . 3 9 0 . 3 8 0 . 3 8 year 8 9 0 . 7 0 0 . 8 1 0 . 8 0 0 . 8 0 0 . 7 8 0 . 7 9 year 9 2 0 . 3 1 0 . 4 0 0 . 4 0 0 . 4 0 0 . 3 9 0 . 4 0

However, if we apply the log transformation on the outcome, then all standard errors give similar t-values.

> ols . fit = lm ( log ( multish + 1 ) ~lnpop + lnpopsq + lngdp + lncolony + + lndist + freedom + militexp + arms + + year 8 3 + year 8 6 + year 8 9 + year 9 2 , data = dat ) > ols . fit . hc 0 = sqrt ( diag ( hccm ( ols . fit , type = " hc 0 " ))) > ols . fit . hc 1 = sqrt ( diag ( hccm ( ols . fit , type = " hc 1 " ))) > ols . fit . hc 2 = sqrt ( diag ( hccm ( ols . fit , type = " hc 2 " ))) > ols . fit . hc 3 = sqrt ( diag ( hccm ( ols . fit , type = " hc 3 " ))) > ols . fit . hc 4 = sqrt ( diag ( hccm ( ols . fit , type = " hc 4 " ))) > ols . fit . coef = summary ( ols . fit )$ coef > tvalues = ols . fit . coef [ , 1 ]/ + cbind ( ols . fit . coef [ , 2 ] , ols . fit . hc 0 , ols . fit . hc 1 , + ols . fit . hc 2 , ols . fit . hc 3 , ols . fit . hc 4 ) > colnames ( tvalues ) = c ( " ols " , " hc 0 " , " hc 1 " , " hc 2 " , " hc 3 " , " hc 4 " ) > round ( tvalues , 2 ) ols hc 0 hc 1 hc 2 hc 3 hc 4 ( Intercept ) 2 . 9 6 2 . 8 1 2 . 7 7 2 . 7 2 2 . 6 3 2 . 5 3 lnpop -2 . 8 7 -2 . 6 3 -2 . 6 0 -2 . 5 4 -2 . 4 5 -2 . 3 5 lnpopsq 4 . 2 1 3 . 7 2 3 . 6 7 3 . 5 9 3 . 4 6 3 . 3 2 lngdp -8 . 0 2 -7 . 4 9 -7 . 3 8 -7 . 3 8 -7 . 2 7 -7 . 3 3 lncolony 6 . 3 1 6 . 1 9 6 . 1 1 6 . 0 8 5 . 9 7 5 . 9 5 lndist -0 . 1 6 -0 . 1 4 -0 . 1 4 -0 . 1 4 -0 . 1 4 -0 . 1 4 freedom 1 . 4 7 1 . 5 3 1 . 5 1 1 . 5 0 1 . 4 7 1 . 4 6 militexp -0 . 3 2 -0 . 3 2 -0 . 3 1 -0 . 3 1 -0 . 3 0 -0 . 2 9 arms 1 . 2 7 1 . 1 2 1 . 1 0 1 . 0 5 0 . 9 8 0 . 8 6 year 8 3 0 . 1 0 0 . 1 0 0 . 1 0 0 . 1 0 0 . 1 0 0 . 1 0 year 8 6 -0 . 1 4 -0 . 1 4 -0 . 1 4 -0 . 1 4 -0 . 1 4 -0 . 1 4 year 8 9 0 . 4 6 0 . 4 5 0 . 4 4 0 . 4 4 0 . 4 4 0 . 4 4 year 9 2 0 . 0 3 0 . 0 3 0 . 0 3 0 . 0 3 0 . 0 3 0 . 0 3

In general, the difference between the OLS and EHW standard errors may be due to the heteroskedasticity or the poor approximation of the linear model. The above two analyses based on the original and transformed outcomes suggest that the linear approximation works better for the log-transformed outcome. We will discuss the issues of transformation and model misspecification later.

## Boston housing data

I also re-analyze the classic Boston housing data [(Harrison Jr and Rubinfeld, 1978)](#b133). The outcome variable is the median value of owner-occupied homes in US dollars 1000, and the covariates include per capita crime rate by town, the proportion of residential land zoned for lots over 25,000 square feet, the proportion of non-retail business acres per town, etc. You can find more details in the R package. In this example, different standard errors yield very different t-values.

> library ( " mlbench " ) > data ( BostonHousing ) > ols . fit = lm ( medv ~. , data = BostonHousing ) > summary ( ols . fit ) Call : lm ( formula = medv ~. , data = BostonHousing )

## Residuals :

Min 1 Q Median 3 Q Max -1 5 . 5 9 5 -2 . 7 3 0 -0 . 5 1 8 1 . 7 7 7 2 6 . 1 9 9

Coefficients : Estimate Std . Error t value Pr ( >| t |) ( Intercept ) 3 . 6 4 6 e + 0 1 5 . 1 0 3 e + 0 0 7 . 1 4 4 3 . 2 8e -1 2 *** crim -1 . 0 8 0e -0 1 3 . 2 8 6e -0 2 -3 . 2 8 7 0 . 0 0 1 0 8 7 ** zn 4 . 6 4 2e -0 2 1 . 3 7 3e -0 2 3 . 3 8 2 0 . 0 0 0 7 7 8 *** indus 2 . 0 5 6e -0 2 6 . 1 5 0e -0 2 0 . 3 3 4 0 . 7 3 8 2 8 8 chas 1 2 . 6 8 7 e + 0 0 8 . 6 1 6e -0 1 3 . 1 1 8 0 . 0 0 1 9 2 5 ** nox -1 . 7 7 7 e + 0 1 3 . 8 2 0 e + 0 0 -4 . 6 5 1 4 . 2 5e -0 6 *** rm 3 . 8 1 0 e + 0 0 4 . 1 7 9e -0 1 9 . 1 1 6 < 2e -1 6 *** age 6 . 9 2 2e -0 4 1 . 3 2 1e -0 2 0 . 0 5 2 0 . 9 5 8 2 2 9 dis -1 . 4 7 6 e + 0 0 1 . 9 9 5e -0 1 -7 . 3 9 8 6 . 0 1e -1 3 *** rad 3 . 0 6 0e -0 1 6 . 6 3 5e -0 2 4 . 6 1 3 5 . 0 7e -0 6 *** tax -1 . 2 3 3e -0 2 3 . 7 6 0e -0 3 -3 . 2 8 0 0 . 0 0 1 1 1 2 ** ptratio -9 . 5 2 7e -0 1 1 . 3 0 8e -0 1 -7 . 2 8 3 1 . 3 1e -1 2 *** b 9 . 3 1 2e -0 3 2 . 6 8 6e -0 3 3 . 4 6 7 0 . 0 0 0 5 7 3 *** lstat -5 . 2 4 8e -0 1 5 . 0 7 2e -0 2 -1 0 . 3 4 7 < 2e -1 6 *** Residual standard error : 4 . 7 4 5 on 4 9 2 degrees of freedom Multiple R -squared : 0 . 7 4 0 6 , Adjusted R -squared : 0 . 7 3 3 8 F -statistic : 1 0 8 . 1 on 1 3 and 4 9 2 DF , p -value : < 2 . 2e -1 6 > > ols . fit . hc 0 = sqrt ( diag ( hccm ( ols . fit , type = " hc 0 " ))) > ols . fit . hc 1 = sqrt ( diag ( hccm ( ols . fit , type = " hc 1 " ))) > ols . fit . hc 2 = sqrt ( diag ( hccm ( ols . fit , type = " hc 2 " ))) > ols . fit . hc 3 = sqrt ( diag ( hccm ( ols . fit , type = " hc 3 " ))) > ols . fit . hc 4 = sqrt ( diag ( hccm ( ols . fit , type = " hc 4 " ))) > ols . fit . coef = summary ( ols . fit )$ coef > tvalues = ols . fit . coef [ , 1 ]/ + cbind ( ols . fit . coef [ , 2 ] , ols . fit . hc 0 , ols . fit . hc 1 , + ols . fit . hc 2 , ols . fit . hc 3 , ols . fit . hc 4 ) > colnames ( tvalues ) = c ( " ols " , " hc 0 " , " hc 1 " , " hc 2 " , " hc 3 " , " hc 4 " ) > round ( tvalues , 2 ) ols hc 0 hc 1 hc 2 hc 3 hc 4 ( Intercept )

7 . 1 4 4 . 6 2 4 . 5 6 4 . 4 8 4 . 3 3 4 . 2 5 crim -3 . 2 9 -3 . 7 8 -3 . 7 3 -3 . 4 8 -3 . 1 7 -2 . 5 8 zn 3 . 3 8 3 . 4 2 3 . 3 7 3 . 3 5 3 . 2 7 3 . 2 8 indus 0 . 3 3 0 . 4 1 0 . 4 1 0 . 4 1 0 . 4 0 0 . 4 0 chas 1 3 . 1 2 2 . 1 1 2 . 0 8 2 . 0 5 2 . 0 0 2 . 0 0 nox -4 . 6 5 -4 . 7 6 -4 . 6 9 -4 . 6 4 -4 . 5 3 -4 . 5 2 rm 9 . 1 2 4 . 5 7 4 . 5 1 4 . 4 3 4 . 2 8 4 . 1 8 age 0 . 0 5 0 . 0 4 0 . 0 4 0 . 0 4 0 . 0 4 0 . 0 4 dis -7 . 4 0 -6 . 9 7 -6 . 8 7 -6 . 8 1 -6 . 6 6 -6 . 6 6 rad 4 . 6 1 5 . 0 5 4 . 9 8 4 . 9 1 4 . 7 6 4 . 6 5 tax -3 . 2 8 -4 . 6 5 -4 . 5 8 -4 . 5 4 -4 . 4 3 -4 . 4 2 ptratio -7 . 2 8 -8 . 2 3 -8 . 1 1 -8 . 0 6 -7 . 8 9 -7 . 9 3 b 3 . 4 7 3 . 5 3 3 . 4 8 3 . 4 4 3 . 3 4 3 . 3 0 lstat -1 0 . 3 5 -5 . 3 4 -5 . 2 7 -5 . 1 8 -5 . 0 1 -4 . 9 3

The log transformation of the outcome does not remove the discrepancy among the standard errors. In this example, heteroskedasticity seems an important problem.

> ols . fit = lm ( log ( medv ) ~. , data = BostonHousing ) > summary ( ols . fit ) Call : lm ( formula = log ( medv ) ~. , data = BostonHousing )

## Residuals :

Min 1 Q Median 3 Q Max -0 . 7 3 3 6 1 -0 . 0 9 7 4 7 -0 . 0 1 6 5 7 0 . 0 9 6 2 9 0 . 8 6 4 3 5 Coefficients :

Estimate Std . Error t value Pr ( >| t |) ( Intercept ) 4 . 1 0 2 0 4 2 3 0 . 2 0 4 2 7 2 6 2 0 . 0 8 1 < 2e -1 6 *** crim -0 . 0 1 0 2 7 1 5 0 . 0 0 1 3 1 5 5 -7 . 8 0 8 3 . 5 2e -1 4 *** zn 0 . 0 0 1 1 7 2 5 0 . 0 0 0 5 4 9 5 2 . 1 3 4 0 . 0 3 3 3 4 9 * indus 0 . 0 0 2 4 6 6 8 0 . 0 0 2 4 6 1 4 1 . 0 0 2 0 . 3 1 6 7 5 5 chas 1 0 . 1 0 0 8 8 7 6 0 . 0 3 4 4 8 5 9 2 . 9 2 5 0 . 0 0 3 5 9 8 ** nox -0 . 7 7 8 3 9 9 3 0 . 1 5 2 8 9 0 2 -5 . 0 9 1 5 . 0 7e -0 7 *** rm 0 . 0 9 0 8 3 3 1 0 . 0 1 6 7 2 8 0 5 . 4 3 0 8 . 8 7e -0 8 *** age 0 . 0 0 0 2 1 0 6 0 . 0 0 0 5 2 8 7 0 . 3 9 8 0 . 6 9 0 5 6 7 dis -0 . 0 4 9 0 8 7 3 0 . 0 0 7 9 8 3 4 -6 . 1 4 9 1 . 6 2e -0 9 *** rad 0 . 0 1 4 2 6 7 3 0 . 0 0 2 6 5 5 6 5 . 3 7 3 1 . 2 0e -0 7 *** tax -0 . 0 0 0 6 2 5 8 0 . 0 0 0 1 5 0 5 -4 . 1 5 7 3 . 8 0e -0 5 *** ptratio -0 . 0 3 8 2 7 1 5 0 . 0 0 5 2 3 6 5 -7 . 3 0 9 1 . 1 0e -1 2 *** b 0 . 0 0 0 4 1 3 6 0 . 0 0 0 1 0 7 5 3 . 8 4 7 0 . 0 0 0 1 3 5 *** lstat -0 . 0 2 9 0 3 5 5 0 . 0 0 2 0 2 9 9 -1 4 . 3 0 4 < 2e -1 6 *** Residual standard error : 0 . 1 8 9 9 on 4 9 2 degrees of freedom Multiple R -squared : 0 . 7 8 9 6 , Adjusted R -squared : 0 . 7 8 4 1 F -statistic : 1 4 2 . 1 on 1 3 and 4 9 2 DF , p -value : < 2 . 2e -1 6 > > ols . fit . hc 0 = sqrt ( diag ( hccm ( ols . fit , type = " hc 0 " ))) > ols . fit . hc 1 = sqrt ( diag ( hccm ( ols . fit , type = " hc 1 " ))) > ols . fit . hc 2 = sqrt ( diag ( hccm ( ols . fit , type = " hc 2 " ))) > ols . fit . hc 3 = sqrt ( diag ( hccm ( ols . fit , type = " hc 3 " ))) > ols . fit . hc 4 = sqrt ( diag ( hccm ( ols . fit , type = " hc 4 " ))) > ols . fit . coef = summary ( ols . fit )$ coef > tvalues = ols . fit . coef [ , 1 ]/ + cbind ( ols . fit . coef [ , 2 ] , ols . fit . hc 0 , ols . fit . hc 1 , + ols . fit . hc 2 , ols . fit . hc 3 , ols . fit . hc 4 ) > colnames ( tvalues ) = c ( " ols " , " hc 0 " , " hc 1 " , " hc 2 " , " hc 3 " , " hc 4 " ) > round ( tvalues , 2 ) ols hc 0 hc 1 hc 2 hc 3 hc 4 ( Intercept ) 2 0 . 0 8 1 4 . 2 9 1 4 . 0 9 1 3 . 8 6 1 3 . 4 3 1 3 . 1 3 crim -7 . 8 1 -5 . 3 1 -5 . 2 4 -4 . 8 5 -4 . 3 9 -3 . 5 6 zn 2 . 1 3 2 . 6 8 2 . 6 4 2 . 6 2 2 . 5 6 2 . 5 6 indus 1 . 0 0 1 . 4 6 1 . 4 4 1 . 4 3 1 . 4 0 1 . 4 1 chas 1 2 . 9 3 2 . 6 9 2 . 6 6 2 . 6 2 2 . 5 6 2 . 5 6 nox -5 . 0 9 -4 . 7 9 -4 . 7 2 -4 . 6 7 -4 . 5 6 -4 . 5 4 rm 5 . 4 3 3 . 3 1 3 . 2 6 3 . 2 0 3 . 1 0 3 . 0 2 age 0 . 4 0 0 . 3 3 0 . 3 2 0 . 3 2 0 . 3 1 0 . 3 1 dis -6 . 1 5 -6 . 1 2 -6 . 0 3 -5 . 9 8 -5 . 8 4 -5 . 8 2 rad 5 . 3 7 5 . 2 3 5 . 1 6 5 . 0 5 4 . 8 7 4 . 6 7 tax -4 . 1 6 -5 . 0 5 -4 . 9 8 -4 . 9 0 -4 . 7 6 -4 . 6 9 ptratio -7 . 3 1 -8 . 8 4 -8 . 7 2 -8 . 6 7 -8 . 5 1 -8 . 5 5 b 3 . 8 5 2 . 8 0 2 . 7 6 2 . 7 2 2 . 6 5 2 . 5 9 lstat -1 4 . 3 0 -7 . 8 6 -7 . 7 5 -7 . 6 3 -7 . 4 0 -7 . 2 8

## Final remarks

The beauty of the asymptotic analysis and the EHW standard error is that they hold under weak parametric assumptions on the error term. We do not need to modify the OLS estimator but only need to modify the covariance estimator. However, this framework has limitations. First, the proofs are based on limiting theorems that require the sample size to be infinity. We are often unsure whether the sample size is large enough for a particular application we have. Second, the EHW standard errors can be severely biased and have large variability. Finally, under the heteroskedastic linear model, the Gauss-Markov theorem does not hold, so the OLS can be inefficient. We will discuss possible improvements in Chapter 19. Finally, unlike Section 5.3, we cannot create any reasonable prediction intervals for a future observation y n+1 based on (X, Y, x n+1 ) since its variance σ 2 n+1 is fundamentally unknown without further assumptions.

## Homework problems

## Testing linear hypotheses under heteroskedasticity

Under the heteroskedastic linear model, how to test the hypotheses

$H 0 : c t β = 0, for c ∈ R p ,$and H 0 : Cβ = 0 for C ∈ R l×p with linearly independent rows? 6.2 Two-sample problem continued Continue Problem 5.4. 1. Assume that z 1 , . . . , z m are IID with mean µ 1 and variance σ 2 1 , and w 1 , . . . , w n are IID with mean µ 2 and variance σ 2 2 , and test H 0 : µ 1 = µ 2 . Show that under H 0 , the following t statistic has an asymptotically Normal distribution:

$t unequal = z -w S 2 z /m + S 2 w /n → N(0, 1)$in distribution.

Remark: The name "unequal" is motivated by the "var.equal" parameter of the R function t.test.

2. We can write the above problem as testing hypothesis H 0 : β 1 = 0 in the heteroskedastic linear regression. Based on the EHW standard error, we can compute the t statistic. Show that it is identical to t unequal with the HC2 correction.

## ANOVA with heteroskedasticity

This is an extension of Problem 5.5 in Chapter 5. Assume y i | i ∈ T j has mean β j and variance σ 2 j , which can be rewritten as a linear model without the Normality and homoskedasticity. In the process of solving Problem 5.5, you have derived the estimator of the covariance matrix of the OLS estimator under homoskedasticity. Find the HC0 and HC2 versions of the EHW covariance matrix. Which covariance matrices do you recommend, and why?

## Invariance of the EHW covariance estimator

If we transform X to X = XΓ where Γ is a p×p non-degenerate matrix, the OLS fit changes from

$Y = X β + ε to Y = X β + ε,$and the associated EHW covariance estimator changes from Vehw to Ṽehw . Show that

$V = Γ Ṽ Γ t ,$and the above result holds for HCj (j = 0, 1, 2, 3, 4). Show that the relationship also holds for the covariance estimator assuming homoskedasticity. Hint: You can use the results in Problems 3.4 and 3.5.

## Breakdown of the equivalence of the t-statistics based on the EHW standard error

This problem parallels Problem 5.9. With the data (x i , y i ) n i=1 where both x i and y i are scalars. Run OLS fit of y i on (1, x i ) to obtain t y|x , the t-statistic of the coefficient of x i , based on the EHW standard error. Run OLS fit of x i on (1, y i ) to obtain t x|y , the t-statistic of the coefficient of y i , based on the EHW standard error.

Give a counterexample with t y|x ̸ = t x|y .

6.6 Empirical comparison of the standard errors [Long and Ervin (2000)](#b169) reviewed and compared several commonly-used standard errors in OLS. Redo their simulation and replicate their Figures [1](#fig_95)[2](#fig_1)[3](#fig_8)[4](#fig_26). They specified more details of their covariate generating process in a technical report [(Long and Ervin, 1998)](#b168).

6.7 Robust standard error in practice [King and Roberts (2015)](#b156) gave three examples where the EHW standard errors differ from the OLS standard error. I have replicated one example in Section 6.5.

2. Replicate another one using linear regression although the original analysis used Poisson regression. You can find the datasets used by King and Roberts (2015) at Harvard Dataverse ([https://dataverse.harvard.edu/](https://dataverse.harvard.edu/)). 6.8 Unbiased sandwich variance estimator under the Gauss-Markov model Under the Gauss-Markov model with σ 2 i = σ 2 for i = 1, . . . , n, show that the HC0 version of Vehw is biased but the HC2 version of Ṽehw is unbiased for cov( β). se0=0.064 se1=0.064 se2=0.064 se0=0.093 se1=0.071 se2=0.094 se0=0.064 se1=0.064 se2=0.064 se0=0.088 se1=0.06 se2=0.089 Normal non-Normal homoskedastic heteroskedastic 0.6 0.8 1.0 1.2 1.4 0.6 0.8 1.0 1.2 1.4 0 2 4 6 0 1 2 3 4 β de nsity FIGURE 6.1: Simulation with 5000 replications: "se0" denotes the true standard error of β, "se1" denotes the estimated standard error based on the homoskedasticity assumption, and "se2" denotes the Eicker-Huber-White standard error allowing for heteroskedasticity. The density curves are Normal with mean 1 and standard deviation se0.

Part III

## Interpretation of OLS Based on Partial Regressions

The Frisch-Waugh-Lovell Theorem

## Long and short regressions

If we partition X and β into

$X = X 1 X 2 , β = β 1 β 2 ,$where

$X 1 ∈ R n×k , X 2 ∈ R n×l , β 1 ∈ R k and β 2 ∈ R l , then we can consider the long regression Y = X β + ε = X 1 X 2 β1 β2 + ε = X 1 β1 + X 2 β2 + ε,$and the short regression Y = X 2 β2 + ε, where β = β1 β2 and β2 are the OLS coefficients, and ε and ε are the residual vectors from the long and short regressions, respectively. These two regressions are of great interest in practice. For example, we can ask the following questions:

(Q1) if the true β 1 is zero, then what is the consequence of including X 1 in the long regression?

(Q2) if the true β 1 is not zero, then what is the consequence of omitting X 1 in the short regression?

(Q3) what is the difference between β2 and β2 ? Both of them are measures of the "impact" of X 2 on Y . Then why are they different? Does their difference give us any information about β 1 ?

Many problems in statistics are related to the long and short regressions. We will discuss some applications in Chapter 8 and give a related result in Chapter 9.

## FWL theorem for the regression coefficients

The following theorem helps to answer these questions.

Theorem 7.1 The OLS estimator for β 2 in the short regression is β2 = (X t 2 X 2 ) -1 X t 2 Y , and the OLS estimator for β 2 in the long regression has the following equivalent forms β2 = (X t X) -1 X t Y last l elements (7.1)

$= {X t 2 (I n -H 1 )X 2 } -1 X t 2 (I n -H 1 )Y where H 1 = X 1 (X t 1 X 1 ) -1 X t 1 (7.2) = ( Xt 2 X2 ) -1 Xt 2 Y where X2 = (I n -H 1 )X 2 (7.3) = ( Xt 2 X2 ) -1 Xt 2 Ỹ where Ỹ = (I n -H 1 )Y. (7.4)$This result is often called the Frisch-Waugh-Lovell (FWL) Theorem in econometrics [(Frisch and Waugh, 1933;](#b118)[Lovell, 1963)](#b170), although its equivalent forms were also known in classic statistics 1 .

Before proving Theorem 7.1, I will first discuss its meanings and interpretations. Equation (7.1) follows from the definition of the OLS coefficient. The matrix I n -H 1 in equation (7.2) is the projection matrix onto the space orthogonal to the column space of X 1 . Equation (7.3) states that β2 equals the OLS coefficient of Y on X2 = (I n -H 1 )X 2 , which is the residual matrix from the column-wise OLS fit of X 2 on X 1 2 . So β2 measures the "impact" of X 2 on Y after "adjusting" for the impact of X 1 , that is, it measures the partial or pure "impact" of X 2 on Y . Equation (7.4) is a slight modification of Equation (7.3), stating that β2 equals the OLS coefficient of Ỹ on X2 , where Ỹ = (I n -H 1 )Y is the residual vector from the OLS fit of Y on X 1 . From (7.3) and (7.4), it is not crucial to residualize Y , but it is crucial to residualize X 2 .

The forms (7.3) and (7.4) suggest the interpretation of β2 as the "impact" of X 2 on Y holding X 1 constant, or in an econometric term, the "impact" of X 2 on Y ceteris paribus. Marshall (1890) used the Latin phrase ceteris paribus. Its English meaning is "with other conditions remaining the same." However, the algebraic meaning of the FWL theorem is that the OLS coefficient of a variable equals the partial regression coefficient based on the residuals. Therefore, taking the Latin phase too seriously may be problematic because Theorem 7.1 is a pure algebraic result without any distributional assumptions. We cannot hold X 1 constant using pure linear algebra. Sometimes, we can manipulate the value of X 1 in an experimental setting, but this relies on the assumption of the data-collecting process.

There are many ways to prove Theorem 7.1. Below I first take a detour to give an unnecessarily complicated proof because some intermediate steps will be useful for later parts of the book. I will then give a simpler proof which requires a deep understanding of OLS as a linear projection.

The first proof relies on the following lemma.

Lemma 7.1 The inverse of X t X is

$(X t X) -1 = S 11 S 12 S 21 S 22 ,$where

$S 11 = (X t 1 X 1 ) -1 + (X t 1 X 1 ) -1 X t 1 X 2 ( Xt 2 X2 ) -1 X t 2 X 1 (X t 1 X 1 ) -1 , S 12 = -(X t 1 X 1 ) -1 X t 1 X 2 ( Xt 2 X2 ) -1 , S 21 = S t 12 , S 22 = ( Xt 2 X2 ) -1 .$1 Professor Alan Agresti gave me the reference of Yule (1907).

2 See Problem 3.7 for more details.

I leave the proof of Lemma 7.1 as Problem 7.1. With Lemma 7.1, we can easily prove Theorem 7.1. Proof of Theorem 7.1: (Version 1) The OLS coefficient is

$β1 β2 = (X t X) -1 X t Y = S 11 S 12 S 21 S 22 X t 1 Y X t 2 Y$.

Then using Lemma 7.1, we can simplify β2 as

$β2 = S 21 X t 1 Y + S 22 X t 2 Y = -( Xt 2 X2 ) -1 X t 2 X 1 (X t 1 X 1 ) -1 X t 1 Y + ( Xt 2 X2 ) -1 X t 2 Y = -( Xt 2 X2 ) -1 X t 2 H 1 Y + ( Xt 2 X2 ) -1 X t 2 Y = ( Xt 2 X2 ) -1 X t 2 (I n -H 1 )Y (7.5) = ( Xt 2 X2 ) -1 Xt 2 Y.$(7.6) Equation (7.5) is the form (7.2), and Equation (7.6) is the form (7.3). Because we also have

$X t 2 (I n -H 1 )Y = X t 2 (I n -H 1 ) 2 Y = Xt 2 Ỹ$, we can write β2 as β2 = ( Xt 2 X2 ) -1 Xt 2 Ỹ , giving the form (7.4).

□ The second proof does not invert the block matrix of X t X directly. Proof of Theorem 7.1: (Version 2) The OLS decomposition Y = X 1 β1 +X 2 β2 + ε satisfies

$X t ε = 0 =⇒ X 1 X 2 t ε = 0 =⇒ X t 1 ε = 0, X t 2 ε = 0.$Multiplying I n -H 1 on both sides of the OLS decomposition, we have

$(I n -H 1 )Y = (I n -H 1 )X 1 β1 + (I n -H 1 )X 2 β2 + (I n -H 1 )ε, which reduces to (I n -H 1 )Y = (I n -H 1 )X 2 β2 + ε because (I n -H 1 )X 1 = 0 and (I n -H 1 )ε = ε -H 1 ε = ε -X 1 (X t 1 X 1 ) -1 X t 1 ε = ε. Further multiplying X t$2 on both sides of the identity, we have

$X t 2 (I n -H 1 )Y = X t 2 (I n -H 1 )X 2 β2$because X t 2 ε = 0. The FWL theorem follows immediately. A subtle issue in this proof is to verify that X t 2 (I n -H 1 )X 2 is invertible. It is easy to show that matrix X t 2 (I n -H 1 )X 2 is positive semi-definite. To show it has rank l, we only need to show that

$u t 2 X t 2 (I n -H 1 )X 2 u 2 = 0 =⇒ u 2 = 0. We have u t 2 X t 2 (I n -H 1 )X 2 u 2 = ∥(I n -H 1 )X 2 u 2 ∥ 2 = 0, so (I n -H 1 )X 2 u 2 = 0, which further implies X 2 u 2 ∈ C(X 1 ) by Proposition 3.1. That is, X 2 u 2 = X 1 u 1 for some u 1 . So X 1 u 1 -X 2 u 2 = 0.$Since the columns of X are linearly independent, we must have u 1 = 0 and u 2 = 0. □ I will end this section with two byproducts of the FWL theorem. First, X2 is the residual matrix from the OLS fit of X 2 on X 1 . It is an n×l matrix with linearly independent columns as shown in the proof of Theorem 7.1 (Version 2) and induces a projection matrix

$H2 = X2 ( Xt 2 X2 ) -1 Xt 2 .$This projection matrix is closely related to the projection matrices induced by X and X 1 as shown in the following lemma.

Lemma 7. [2](#) We have

$H 1 H2 = H2 H 1 = 0, H = H 1 + H2 .$Lemma 7.2 is purely algebraic. I leave the proof as Problem 7.3. The first two identities imply that the column space of X2 is orthogonal to the column space of X 1 . The last identity H = H 1 + H2 has a clear geometric interpretation. For any vector v ∈ R n , we have Hv = H 1 v + H2 v, so the projection of v onto the column space of X equals the summation of the projection of v onto the column space of X 1 and the projection of v onto the column space of X2 . Importantly, H ̸ = H 1 + H 2 in general.

Second, we can obtain β2 from (7.3) or (7.4), which corresponds to the partial regression of Y on X2 or the partial regression of Ỹ on X2 . We can verify that the residual vector from the second partial regression equals the residual vector from the full regression.

Corollary 7.1 We have ε = ê, where ε is the residual vector from the OLS fit of Y on X and ê is the residual vector from the OLS fit of Ỹ on X2 , respectively.

It is important to note that this conclusion is only true if both Y and X 2 are residualized. The conclusion does not hold if we only residualize X 2 . See Problem 7. It suffices to show that I-H = (I-H2 )(I-H 1 ), or, equivalently, I-H = I-H 1 -H2 + H2 H 1 . This holds due to Lemma 7.2. □

## FWL theorem for the standard errors

Based on the OLS fit of Y on X, we have two estimated covariances for the second component β2 : V assuming homoskedasticity and Vehw allowing for heteroskedasticity.

The FWL theorem demonstrates that we can also obtain β2 from the OLS fit of Ỹ on X2 . Then based on this partial regression, we have two estimated covariances for β2 : Ṽ assuming homoskedasticity and Ṽehw allowing for heteroskedasticity.

The following theorem establishes their equivalence.

Theorem 7.2 (n -k -l) V = (n -l) Ṽ and Vehw = Ṽehw .

Theorem 7.1 is well known for a long time but Theorem 7.2 is less well known. [Lovell (1963)](#b170) hinted at the first identity in Theorem 7.2, and Ding (2021a) proved Theorem 7.2. Proof of Theorem 7.2: By Corollary 7.1, the full regression and partial regression have the same residual vector, denoted by ε. Therefore, Ωehw = Ωehw = diag{ε 2 } in the EHW covariance estimator.

Based on the full regression, define σ2 = ∥ε∥ 2 2 /(n -k -l). Then V equals the (2, 2)th block of σ2 (X t X) -1 , and Vehw equals the (2, 2)th block of (X t X) -1 X t Ωehw X(X t X) -1 . Based on the partial regression, define σ2 = ∥ε∥ 2 2 /(n -l). Then Ṽ = σ2 ( Xt 2 X2 ) -1 and Ṽehw = ( Xt 2 X2 ) -1 Xt 2 Ωehw X2 ( Xt 2 X2 ) -1 . Let σ2 = ∥ε∥ 2 /(n -k -l) and σ2 = ∥ε∥ 2 /(n -l) be the common variance estimators. They are identical up to the degrees of freedom correction. Under homoskedasticity, the covariance estimator for β2 is the (2, 2)th block of σ2 (X t X) -1 , that is, σ2 S 22 = σ2 ( Xt 2 X2 ) -1 by Lemma 7.1, which is identical to the covariance estimator for β2 up to the degrees of freedom correction.

The EHW covariance estimator from the full regression is the (2, 2) block of Â Ωehw Ât , where

$Â = (X t X) -1 X t = * -( Xt 2 X2 ) -1 X t 2 H 1 + ( Xt 2 X2 ) -1 X t 2 = * ( Xt 2 X2 ) -$1 Xt 2 by Lemma 7.1. I omit the * term because it does not affect the final calculation. Define Ã2 = ( Xt 2 X2 ) -1 Xt 2 , and then Vehw = Ã2 Ωehw Ãt 2 = Ã2 Ωehw Ãt 2 , which equals the EHW covariance estimator Ṽehw from the partial regression. □ 7.4 Gram-Schmidt orthogonalization, QR decomposition, and the computation of OLS

When the regressors are orthogonal, the coefficients from the long and short regressions are identical, which simplifies the calculation and theoretical discussion.

Corollary 7.2 If X t 1 X 2 = 0, i.e., the columns of X 1 and X 2 are orthogonal, then X2 = X 2 and β2 = β2 .

Proof of Corollary 7.2: We can directly prove Corollary 7.2 by verifying that X t X is block diagonal.

Alternatively, Corollary 7.2 follows directly from

$X2 = (I n -H 1 )X 2 = X 2 -X 1 (X t 1 X 1 ) -1 X t 1 X 2 = X 2 ,$and Theorem 7.1. □ This simple fact motivates us to orthogonalize the columns of the covariate matrix, which in turn gives the famous QR decomposition in linear algebra. Interestingly, the lm function in R uses the QR decomposition to compute the OLS estimator. To facilitate the discussion, I will use the notation βV2|V1 V 1 as the linear projection of the vector

$V 2 ∈ R n onto the vector V 1 ∈ R n , where βV2|V1 = V t 2 V 1 /V t 1 V 1 . This is from the univariate OLS of V 2 on V 1 .$With a slight abuse of notation, partition the covariate matrix into column vectors X = (X 1 , . . . , X p ). The goal is to find orthogonal vectors (U 1 , . . . , U p ) that generate the same column space as X. Start with

$X 1 = U 1 .$Regress X 2 on U 1 to obtain the fitted and residual vector

$X 2 = βX2|U1 U 1 + U 2 ;$by OLS, U 1 and U 2 must be orthogonal. Regress X 3 on (U 1 , U 2 ) to obtain the fitted and residual vector

$X 3 = βX3|U1 U 1 + βX3|U2 U 2 + U 3 ;$by Corollary 7.2, the OLS reduces to two univariate OLS and ensures that U 3 is orthogonal to both U 1 and U 2 . This justifies the notation βX3|U1 and βX3|U2 . Continue this procedure to the last column vector:

$X p = p-1 j=1 βXp|Uj U j + U p ;$by OLS, U p is orthogonal to all U j (j = 1, . . . , p -1). We further normalize the U vectors to have unit length:

$Q j = U j /∥U j ∥, (j = 1, . . . , p).$The whole process is called the Gram-Schmidt orthogonalization, which is essentially the sequential OLS fits. This process generates an n×p matrix with orthonormal column vectors

$Q = (Q 1 , . . . , Q p ).$More interestingly, the column vectors of X and Q can linearly represent each other because

$X = (X 1 , . . . , X p ) = (U 1 , . . . , U p )      1 βX2|U1 βX3|U1 • • • βXp|U1 0 1 βX3|U2 • • • βXp|U2 . . . . . . . . . • • • . . . 0 0 0 • • • 1      = Qdiag{∥U j ∥} p j=1      1 βX2|U1 βX3|U1 • • • βXp|U1 0 1 βX3|U2 • • • βXp|U2 . . . . . . . . . • • • . . . 0 0 0 • • • 1     $.

We can verify that the product of the second and the third matrix is an upper triangular matrix, denoted by R. By definition, the jth diagonal element of R equals ∥U j ∥, and the (j, j ′ )th element of R equals ∥U j ∥ βX j ′ |Uj for j ′ > j. Therefore, we can decompose X as

$X = QR$where Q is an n × p matrix with orthonormal columns and R is a p × p upper triangular matrix. This is called the QR decomposition of X.

Most software packages, for example, R, do not calculate the inverse of X t X directly. Instead, they first find the QR decomposition of X = QR. Since the Normal equation simplifies to

$X t X β = X t Y, R t Q t QR β = R t Q t Y, R β = Q t Y,$they then backsolve the linear equation since R is upper triangular.

In R, the qr function returns the QR decomposition of a matrix.

$> X = matrix ( rnorm ( 7 * 3 ) , 7 , 3 ) > X [ , 1 ] [ , 2 ] [ , 3 ] [1$,] -0 . 5 7 2 3 1 2 2 3 0 . 1 1 9 6 3 2 5 0 . 8 0 8 7 5 0 5 [ 2 ,] -1 . 7 6 0 9 0 2 2 5 1 . 0 6 2 7 6 3 1 1 . 8 1 7 0 3 6 1 [ 3 ,] -0 . 0 4 1 4 4 2 8 1 -0 . 2 9 0 4 7 4 9 -1 . 8 3 7 2 2 4 7 [ 4 ,] -0 . 3 7 6 2 7 8 2 1 0 . 4 4 7 6 9 3 2 -0 . 9 6 2 9 3 2 0 [ 5 ,] -1 . 4 0 8 4 8 0 2 7 0 . 2 7 3 5 4 0 8 -0 . 8 0 4 7 9 1 7 [ 6 ,] 1 . 8 4 8 7 8 5 1 8 0 . 7 2 9 0 0 0 5 1 . 2 6 8 8 9 2 9 [ 7 ,] 0 . 0 6 4 3 2 8 5 6 0 . 2 2 5 6 2 8 4 0 . 3 9 7 2 2 2 9 > qrX = qr ( X ) [1 9 1 0 0 8 7 8 -0 . 0 3 4 6 0 6 1 7 0 . 3 0 3 4 0 4 8 1  [ 2 ,] -0 . 5 8 7 6 9 9 8 1 -0 . 6 0 4 4 2 9 2 8 0 . 2 3 7 5 3 9 0 0  [ 3 ,] -0 . 0 1 3 8 3 1 5 1 0 . 2 1 1 9 1 9 9 1 -0 . 5 5 8 3 9 9 2 8  [ 4 ,] -0 . 1 2 5 5 8 2 5 7 -0 . 2 8 7 2 8 4 0 3 -0 . 6 2 8 6 4 7 5 0  [ 5 ,] -0 . 4 7 0 0 7 9 2 4 -0 . 0 7 0 2 0 0 7 6 -0 . 3 6 6 4 0 9 3 8  [ 6](#) ,] 0 . 6 1 7 0 3 0 6 7 -0 . 6 8 7 7 8 4 1 1 -0 . 0 9 9 9 9 8 5 9 [ 7 ,] 0 . 0 2 1 4 6 9 6 1 -0 . 1 6 7 4 8 2 4 6 0 . 0 1 6 0 5 4 9 3 > qr . R ( qrX )

$> qr . Q ( qrX ) [ , 1 ] [ , 2 ] [ , 3 ] [ 1 ,] -0 .$[ , 1 ] [ , 2 ] [ , 3 ] [ 1 ,] 2 . 9 9 6 2 6 1 -0 . 3 7 3 5 6 7 3 0 . 0 9 3 7 7 8 8 [ 2 ,] 0 . 0 0 0 0 0 0 -1 . 3 9 5 0 6 4 2 -2 . 1 2 1 7 2 2 3 [ 3 ,] 0 . 0 0 0 0 0 0 0 . 0 0 0 0 0 0 0 2 . 4 8 2 6 1 8 6

If we specify qr = TRUE in the lm function, it will also return the QR decomposition of the covariate matrix.

$> Y = rnorm ( 7 ) > lmfit = lm ( Y ~0 + X , qr = TRUE ) > qr . Q ( lmfit $ qr ) [ , 1 ] [ , 2 ] [ , 3 ] [1$,] -0 . 4 3 5 3 5 0 5 4 -0 . 2 5 6 7 9 8 2 3 -0 . 6 5 4 8 0 4 0 0 [ 2 ,] -0 . 4 7 0 9 1 2 7 5 -0 . 1 3 6 3 9 4 5 9 0 . 1 4 7 4 6 4 4 4 [ 3 ,] 0 . 6 6 4 9 4 5 3 2 0 . 0 7 7 2 5 4 3 5 -0 . 3 9 4 3 6 2 6 5 [ 4 ,] 0 . 2 1 1 3 6 3 4 7 -0 . 7 8 8 1 4 7 3 7 0 . 3 4 6 1 1 8 2 0 [ 5 ,] -0 . 0 4 4 9 3 3 5 6 -0 . 4 0 4 1 3 8 2 9 -0 . 0 1 1 5 6 2 7 3 [ 6 ,] -0 . 2 8 0 4 6 5 0 4 0 . 1 0 6 5 5 7 5 5 -0 . 1 6 1 9 3 2 5 1 [ 7 ,] 0 . 1 4 5 6 1 8 0 8 -0 . 3 3 7 0 8 2 1 9 -0 . 4 9 7 8 0 5 6 1 > qr . R ( lmfit $ qr ) X 1 X 2 X 3 1 3 . 1 9 0 0 3 5 -0 . 6 9 6 4 2 6 9 1 . 8 6 9 3 2 6 0 2 0 . 0 0 0 0 0 0 2 . 0 7 1 9 7 8 7 1 . 9 2 1 0 2 1 2 3 0 . 0 0 0 0 0 0 0 . 0 0 0 0 0 0 0 -0 . 9 2 6 1 9 2 1

The replicating R code is in code7.4.R[foot_4](#foot_4) .

## Homework problems

7.1 Inverse of a block matrix Prove Lemma 7.1 and the following alternative form:

$(X t X) -1 = Q 11 Q 12 Q 21 Q 22 ,$where

$H 2 = X 2 (X t 2 X 2 ) -1 X t 2 , X1 = (I n -H 2 )X 1 , and Q 11 = ( Xt 1 X1 ) -1 , Q 12 = -( Xt 1 X1 ) -1 X t 1 X 2 (X t 2 X 2 ) -1 , Q 21 = Q t 12 , Q 22 = (X t 2 X 2 ) -1 + (X t 2 X 2 ) -1 X t 2 X 1 ( Xt 1 X1 ) -1 X t 1 X 2 (X t 2 X 2 ) -1 .$Hint: Use the formula in Problem 1.3.

## Residuals in the FWL theorem

Give an example in which the residual vector from the partial regression of Y on X2 does not equal to the residual vector from the full regression.

## Projection matrices

Prove Lemma 7.2. Hint: Use Lemma 7.1.

## FWL theorem and leverage scores

Consider the partitioned regression Y = X 1 β1 + X 2 β2 + ε. To obtain the coefficient β2 , we can run two OLS fits:

(R1) regress X 2 on X 1 to obtain the residual X2 ;

(R2) regress Y on X2 to obtain the coefficient, which equals β2 by the FWL Theorem.

Although partial regression (R2) can recover the OLS coefficient, the leverage scores from (R2) are not the same as those from the long regression. Show that the summation of the corresponding leverage scores from (R1) and (R2) equals the leverage scores from the long regression.

Remark: The leverage scores are the diagonal elements of the hat matrix from OLS fits. Chapter 6 before mentioned them and Chapter 11 later will discuss them in more detail.

## Another invariance property of the OLS coefficient

Partition the covariate matrix as X = (X 1 , X 2 ) where X 1 ∈ R n×k and X 2 ∈ R n×l . Given any A ∈ R k×l , define X2 = X 2 -X 1 A. Fit two OLS:

$Y = β1 X 1 + β2 X 2 + ε and Y = β1 X 1 + β2 X2 + ε.$Show that β2 = β2 , ε = ε.

Hint: Use the result in Problem 3.4. Remark: Choose A = (X t 1 X 1 ) -1 X t 1 X 2 to be the coefficient matrix of the OLS fit of X 2 on X 1 . The above result ensures that β2 equals β2 from the OLS fit of Y on X 1 and (I n -H 1 )X 2 , which is coherent with the FWL theorem since X t 1 (I n -H 1 )X 2 = 0.

## Alternative formula for the EHW standard error

Consider the partition regression Y = X 1 β1 + X 2 β2 + ε with X 1 is an n × (p -1) matrix and X 2 is an n dimensional vector. So β2 is a scalar, and the (p, p)th element of Vehw equals ŝe 2 ehw,2 , the squared EHW standard error for β2 . Define

$X2 = (I n -H 1 )X 2 =    x12 . . . xn2    .$Prove that under Assumption 6.1, we have

$var( β2 ) = n i=1 w i σ 2 i , ŝe 2 ehw,2 = n i=1 w i ε2 i$where

$w i = x2 i2 ( n i=1 x2 i2 ) 2$. Remark: You can use Theorems 7.1 and 7.2 to prove the result. The original formula of the EHW covariance matrix has a complex form. However, using the FWL theorems, we can simplify each of the squared EHW standard errors as a weighted average of the squared residuals, or, equivalently, a simple quadratic form of the residual vector.

## A counterexample to the Gauss-Markov theorem

The Gauss-Markov theorem does not hold under the heteroskedastic linear model. This problem gives a counterexample in a simple linear model.

Assume y i = βx i + ε i without the intercept and with potentially different var

$(ε i ) = σ 2 i across i = 1, . . . ,$n. Consider two OLS estimators: the first OLS estimator does not contain the intercept β = n i=1 x i y i / n i=1 x 2 i ; the second OLS estimator contains the intercept β 2 even though the true linear model does not contain the intercept.

$= n i=1 (x i -x)y i / n i=1 (x i -x)$The Gauss-Markov theorem ensures that if σ 2 i = σ 2 for all i's, then the variance of β is smaller than or equal to the variance of β. However, it does not hold when σ 2 i 's vary. Give a counterexample in which the variance of β is larger than the variance of β.

7.8 QR decomposition of X and the computation of OLS Verify that the R matrix equals

$R =      Q t 1 X 1 Q t 1 X 2 • • • Q t 1 X p 0 Q t 2 X 2 • • • Q t 2 X p . . . . . . • • • . . . 0 0 • • • Q t p X p      .$Based on the QR decomposition of X, show that

$H = QQ t ,$and h ii equals the squared length of the i-th row of Q.

7.9 Uniqueness of the QR decomposition Show that if X has linearly independent column vectors, the QR decomposition must be unique. That is, if X = QR = Q 1 R 1 where Q and Q 1 have orthonormal columns and R and R 1 are upper triangular, then we must have

$Q = Q 1 , R = R 1 .$
## Applications of the Frisch-Waugh-Lovell Theorem

The FWL theorem has many applications, and I will highlight some of them in this chapter.

## Centering regressors

As a special case, partition the covariate matrix into X = (X 1 , X 2 ) with X 1 = 1 n . This is the usual case including the constant as the first regressor. The projection matrix

$H 1 = 1 n (1 t n 1 n ) -1 1 t n = n -1 1 n 1 t n =    n -1 • • • n -1 . . . . . . n -1 • • • n -1    ≡ A n$contains n -1 's as its elements, and

$C n = I n -n -1 1 n 1 t n$is the projection matrix orthogonal to 1 n . Multiplying any vector by A n is equivalent to obtaining the average of its components, and multiplying any vector by C n is equivalent to centering that vector, for example,

$A n Y =    ȳ . . . ȳ    = ȳ1 n ,and$$C n Y =    y 1 - ȳ . . . y n - ȳ    .$More generally, multiplying any matrix by A n is equivalent to averaging each column, and multiplying any matrix by C n is equivalent to centering each column of that matrix, for example,

$A n X 2 =    xt 2 . . . xt 2    = 1 n xt 2 ,$and

$C n X 2 =    x t 12 -xt 2 . . . x t n2 -xt 2    ,$where X 2 contains row vectors x t 12 , . . . , x t n2 with average x2 = n -1 n i=1 x i2 . The FWL theorem implies that the coefficient of X 2 in the OLS fit of Y on (1 n , X 2 ) equals the coefficient of C n X 2 in the OLS fit of C n Y on C n X 2 , that is, the OLS fit of the centered response vector on the column-wise centered X 2 . An immediate consequence is that if each column is centered in the design matrix, then to obtain the OLS coefficients, it does not matter whether to include the column 1 n or not.

The centering matrix C n has another property: its quadratic form equals the sample variance multiplied by n -1, for example,

$Y t C n Y = Y t C t n C n Y = (y 1 -ȳ, . . . , y n -ȳ)    y 1 - ȳ . . . y n - ȳ    = n i=1 (y i -ȳ) 2 = (n -1)σ 2 y ,$where σ2 y is the sample variance of the outcomes. For an n × p matrix X,

$X t C n X =    X t 1 . . . X t p    C n X 1 • • • X p =    X t 1 C n X 1 • • • X t 1 C n X p . . . . . . X t p C n X 1 • • • X t p C n X p    = (n -1)    σ11 • • • σ1p . . . . . . σp1 • • • σpp    , where σj1j2 = (n -1) -1 n i=1 (x ij1 -x•j1 )(x ij2 -x•j2 )$is the sample covariance between X j1 and X j2 . So (n -1) -1 X t C n X equals the sample covariance matrix of X. For these reason, I choose the notation C n with "C" for both "centering" and "covariance."

In another important special case, X 1 contains the dummies for a discrete variable, for example, the indicators for different treatment levels or groups. See Example 3.2 for the background. With k groups, X 1 can take the following two forms:

$X 1 =                     1 1 • • • 0 . . . . . . . . . 1 1 • • • 0 . . . . . . . . . 1 0 • • • 1 . . . . . . . . . 1 0 1 1 0 0 . . . . . . . . . 1 0 • • • 0                     n×k or X 1 =              1 • • • 0 . . . . . . 1 • • • 0 . . . . . . 0 • • • 1 . . . . . . 0 • • • 1              n×k , (8.1)$where the first form of X 1 contains 1 n and k -1 dummy variables, and the second form of X 1 contains k dummy variables. In both forms of X 1 , the observations are sorted according to the group indicators. If we regress Y on X 1 , the residual vector is

$Y -              ȳ[1] . . . ȳ[1] . . . ȳ[k] . . . ȳ[k]              ,(8.2)$where ȳ[1] , . . . , ȳ[k] are the averages of the outcomes within groups 1, . . . , k. Effectively, we center Y by group-specific means. Similarly, if we regress X 2 on X 1 , we center each column of X 2 by the group-specific means. Let Y c and X c 2 be the centered response vector and design matrix. The FWL theorem implies that the OLS coefficient of X 2 in the long regression is the OLS coefficient of X c 2 in the partial regression of Y c on X c 2 . When k is large, running the OLS with centered variables can reduce the computational cost.

## Partial correlation coefficient and Simpson's paradox

The sample Pearson correlation coefficient between n observations of two scalars (x i , y i )

$n i=1 ρyx = n i=1 (x i -x)(y i -ȳ) n i=1 (x i -x) 2 n i=1 (y i -ȳ) 2$measures the linear relationship between x and y. How do we measure the linear relationship between x and y after controlling for some other variables w ∈ R k-1 ? Intuitively, we can measure it with the sample Pearson correlation coefficient based on the residuals from the following two OLS fits: (R1) run OLS of Y on (1, W ) and obtain residual vector εy and residual sum of squares rss y ;

With εy and εx , we can define the sampling partial correlation coefficient between x and y given w as ρyx|w = n i=1 εx,i εy,i

$n i=1 ε2 x,i n i=1 ε2 y,i .$In the above definition, we do not center the residuals because they have zero sample means due to the inclusions of the intercept in the OLS fits (R1) and [(R2)](#). The sample partial correlation coefficient determines the coefficient of εx in the OLS fit of εy on εx :

$βyx|w = n i=1 εx,i εy,i n i=1 ε2 x,i = ρyx|w n i=1 ε2 y,i n i=1 ε2 x,i = ρyx|w σy|w σx|w ,(8.3)$where σ2 y|w = rss y /(n -k) and σ2 x|w = rss x /(n -k) are the variance estimators based on regressions (R1) and (R2) motivated by the Gauss-Markov model. Based on the FWL theorem, βyx|w equals the OLS coefficient of X in the long regression of Y on (1, X, W ). Therefore, (8.3) is the Galtonian formula for multiple regression, which is analogous to that for univariate regression (1.1).

To investigate the relationship between y and x, different researchers may run different regressions. One may run OLS of Y on (1, X, W ), and the other may run OLS of Y on (1, X, W ′ ), where W ′ is a subset of W . Let βyx|w be the coefficient of X in the first regression, and let βyx|w ′ be the coefficient of X in the second regression. Mathematically, it is possible that these two coefficients have different signs, which is called Simpson's paradox[foot_5](#foot_5) . It is a paradox because we expect both coefficients to measure the "impact" of X on Y . Because these two coefficients have the same signs as the partial correlation coefficients ρyx|w and ρyx|w ′ , Simpson's paradox is equivalent to ρyx|w ρyx|w ′ < 0.

To simplify the presentation, we discuss the special case with w ′ being an empty set. Simpson's paradox is then equivalent to ρyx|w ρyx < 0.

The following theorem gives an expression linking ρyx|w and ρyx .

$Theorem 8.1 For Y, X, W ∈ R n , we have ρyx|w = ρyx -ρyw ρxw 1 -ρ2 yw 1 -ρ2 xw .$Its proof is purely algebraic, so I leave it as Problem 8.6. Theorem 8.1 states that we can obtain the sample partial correlation coefficient based on the three pairwise correlation coefficients. Figure [8](#fig_13).1 illustrates the interplay among three variables. In particular, the correlation between x and y is due to two "pathways": the one acting through w and the one acting independent of w. The first path way is related to the product term ρyw ρxw , and the second pathway is related to ρyx|w . This gives some intuition for Theorem 8.1. We can observe Simpson's Paradox in the following simulation with the R code in code8.2.R.

> n = 1 0 0 0 > w = rbinom (n , 1 , 0 . 5 ) > x 1 = rnorm (n , -1 , 1 ) > x 0 = rnorm (n , 2 , 1 ) > x = ifelse (w , x 1 , x 0 ) > y = x + 6 * w + rnorm ( n ) > fit . xw = lm ( y ~x + w )$ coef > fit . x = lm ( y ~x )$ coef > fit . xw ( Intercept ) x w 0 . 0 5 6 5 5 4 4 2 0 . 9 7 9 6 9 9 0 7 5 . 9 2 5 1 7 0 7 2 > fit . x ( Intercept ) x 3 . 6 4 2 2 9 7 8 -0 . 3 7 4 3 3 6 8

Because w is binary, we can plot (x, y) in each group of w = 1 and w = 0 in Figure [8](#fig_13).2. In both groups, y and x are positively associated with positive regression coefficients; but in the pooled data, y and x are negatively associated with a negative regression coefficient. q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q -2.5 0.0 The two solid regression lines are fitted separately using the data from two groups, and the dash regression line is fitted using the pooled data.

## Hypothesis testing and analysis of variance

Partition X and β into

$X = X 1 X 2 , β = β 1 β 2 ,$where

$X 1 ∈ R n×k , X 2 ∈ R n×l , β 1 ∈ R k and β 2 ∈ R l .$We are often interested in testing [(8.4)](#) where ε ∼ N(0, σ 2 I n ). If H 0 holds, then X 2 is redundant and a short regression suffices:

$H 0 : β 2 = 0 in the long regression Y = Xβ + ε = X 1 β 1 + X 2 β 2 + ε,$$Y = X 1 β + ε. (8.5)$This is a special case of testing Cβ = 0 with

$C = 0 l×k I l×l .$As discussed before, we can use β2 ∼ N(0, σ 2 S 22 )

with S 22 = ( Xt 2 X2 ) -1 being the (2, 2)th block of (X t X) -1 by Lemma 7.1, to construct the Wald-type statistic for hypothesis testing:

$F Wald = βt 2 (S 22 ) -1 β2 lσ 2 = βt 2 Xt 2 X2 β2 lσ 2 ∼ F l,n-p .$Now I will discuss testing H 0 from an alternative perspective based on comparing the residual sum of squares in the long regression (8.4) and the short regression (8.5). This technique is called the analysis of variance (ANOVA), pioneered by R. A. Fisher in the design and analysis of experiments. Intuitively, if β 2 = 0, then the residual vectors from the long regression (8.4) and the short regression (8.5) should not be "too different." However, with the error term ε, these residuals are random, then the key is to quantify the magnitude of the difference. Define

$rss long = Y t (I n -H)Y and rss short = Y t (I n -H 1 )Y$as the residual sum of squares from the long and short regressions, respectively. By the definition of OLS, it must be true that

$rss long ≤ rss short and rss short -rss long = Y t (H -H 1 )Y ≥ 0. (8.6)$To understand the magnitude of the change in the residual sum of squares, we can standardize the above difference and define

$F anova = (rss short -rss long )/l rss long /(n -p) ,$In the definition of the above statistic, l and n -p are the degrees of freedom to make the mathematics more elegant, but they do not change the discussion fundamentally. The denominator of F anova is σ2 , so we can also write it as

$F anova = rss short -rss long lσ 2 . (8.7)$The following theorem states that these two perspectives yield an identical test statistic.

Theorem 8.2 Under Assumption 5.1, if β 2 = 0, then F anova ∼ F l,n-p . In fact, F anova = F Wald which is a numerical result without Assumption 5.1.

I divide the proof into two parts. The first part derives the exact distribution of F anova under the Normal linear model. It relies on the following lemma on the basic properties of the projection matrices. I relegate its proof to Problem 8.9. Lemma 8.1 We have

$HX 1 = X 1 , HX 2 = X 2 , HH 1 = H 1 , H 1 H = H 1$Moreover, H -H 1 is a projection matrix of rank p -k = l, I n -H is a projection matrix of rank n -p, and they are orthogonal:

$(H -H 1 )(I n -H) = 0. (8.8)$Proof of Theorem 8.2 (Part I):

The residual vector from the long regression is ε = (I n -H)Y = (I n -H)(Xβ + ε) = (I n -H)ε, so the residual sum of squares is

$rss long = εt ε = ε t (I n -H)ε;$since β 2 = 0, the residual vector from the short regression is ε = (

$I n -H 1 )Y = (I n - H 1 )(X 1 β 1 + ε) = (I n -H 1 )ε, so the residual sum of squares is rss short = εt ε = ε t (I n -H 1 )ε.$Let ε 0 = ε/σ ∼ N(0, I n ) be a standard Normal random vector, then we can write F anova as

$F anova = ε t (H -H 1 )ε/l ε t (I n -H)ε/(n -p) = ε t 0 (H -H 1 )ε 0 /l ε t 0 (I n -H)ε 0 /(n -p) = ∥(H -H 1 )ε 0 ∥ 2 /l ∥(I n -H)ε 0 ∥ 2 /(n -p)$.

(8.9) Therefore, we have the following joint Normality using the basic fact (8.8):

$(H -H 1 )ε 0 (I n -H)ε 0 = H -H 1 I n -H ε 0 ∼ N 0 0 , H -H 1 0 0 I n -H .$So (H -H 1 )ε 0 and (I n -H)ε 0 are Normal with mean zero and two projection matrices H -H 1 and I n -H as covariances, respectively, and moreover, they are independent. These imply that their squared lengths are chi-squared:

$∥(H -H 1 )ε 0 ∥ 2 ∼ χ 2 l , ∥(I n -H)ε 0 ∥ 2 ∼ χ 2 n-p ,$and they are independent. These facts, coupled with (8.9), imply that F anova ∼ F l,n-p . □

The second part demonstrates that F anova = F Wald without assuming the Normal linear model, which gives an indirect proof for the exact distribution of F anova under the Normal linear model. Proof of Theorem 8.2 (Part II): Using the FWL theorem that β2 = ( Xt [(8.10)](#) recalling that H2 = X2 ( Xt 2 X2 ) -1 Xt 2 is the projection matrix onto the column space of X2 . Therefore, F anova = F Wald follows from the basic identity H -H 1 = H2 ensured by Lemma 7.2.

$2 X2 ) -1 Xt 2 Y , we can rewrite F Wald as F Wald = Y t X2 ( Xt 2 X2 ) -1 Xt 2 X2 ( Xt 2 X2 ) -1 Xt 2 Y lσ 2 = Y t X2 ( Xt 2 X2 ) -1 Xt 2 Y lσ 2 = Y t H2 Y lσ 2 ,$□ We can use the anova function in R to compute the F statistic and the p-value. Below I revisit the lalonde data with the R code in code8.3.R. The result is identical as in Section 5.4.2.

> library ( " Matching " ) > data ( lalonde ) > lalonde _ full = lm ( re 7 8 ~. , data = lalonde ) > lalonde _ treat = lm ( re 7 8 ~treat , data = lalonde ) > anova ( lalonde _ treat , lalonde _ full ) Analysis of Variance

Table Model 1 : re 7 8 ~treat Model 2 : re 7 8 ~age + educ + black + hisp + married + nodegr + re + re 7 5 + u 7 4 + u 7 5 + treat Res . Df RSS Df Sum of Sq F Pr ( > F ) 1

4 4 3 1 . 9 1 7 8 e + 1 0 2 4 3 3 1 . 8 3 8 9 e + 1 0 1 0 7 8 8 7 9 9 0 2 3 1 . 8 5 7 4 0 . 0 4 9 2 9 *

In fact, we can conduct an analysis of variance in a sequence of models. For example, we can supplement the above analysis with a model containing only the intercept. The function anova works for a sequence of nested models with increasing complexities.

> lalonde 1 = lm ( re 7 8 ~1 , data = lalonde ) > anova ( lalonde 1 , lalonde _ treat , lalonde _ full ) Analysis of Variance

Table Model 1 : re 7 8 ~1 Model 2 : re 7 8 ~treat Model 3 : re 7 8 ~age + educ + black + hisp + married + nodegr + re + re 7 5 + u 7 4 + u 7 5 + treat Res . Df RSS Df Sum of Sq F Pr ( > F ) 1

4 4 4 1 . 9 5 2 6 e + 1 0 2 4 4 3 1 . 9 1 7 8 e + 1 0 1 3 4 8 0 1 3 4 5 6 8 . 1 9 4 6 0 . 0 0 4 4 0 5 ** 3 4 3 3 1 . 8 3 8 9 e + 1 0 1 0 7 8 8 7 9 9 0 2 3 1 . 8 5 7 4 0 . 0 4 9 2 8 6 *

Overall, the treatment variable is significantly related to the outcome but none of the pretreatment covariate is.

## Homework problems

## FWL with intercept

The following result is an immediate extension of Theorem 7.1.

Partition X and β into

$X = 1 n X 1 X 2 , β =   β 0 β 1 β 2   ,$where

$X 1 ∈ R n×k , X 2 ∈ R n×l , β 0 ∈ R, β 1 ∈ R k and β 2 ∈ R l . then we can consider the long regression Y = X β + ε = 1 n X 1 X 2   β0 β1 β2   + ε = 1 n β0 + X 1 β1 + X 2 β2 + ε,$and the short regression

$Y = 1 n β0 + X 2 β2 + ε, where β =   β0 β1 β2$  and β0 β2 are the OLS coefficients, and ε and ε are the residual vectors from the long and short regressions, respectively. Prove the following theorem.

Theorem 8.3 The OLS estimator for β 2 in the long regression equals the coefficient of X2 in the OLS fit of Y on (1 n , X2 ), where X2 is the residual matrix of the column-wise OLS fit of X 2 on (1 n , X 1 ), and also equals the coefficient of X2 in the OLS fit of Ỹ on (1 n , X2 ), where Ỹ is the residual vector of the OLS fit of Y on (1 n , X 1 ).

## General centering

Verify (8.2).

## Two-way centering of a matrix

Given X ∈ R n×p , show that all rows and columns of C n XC p have mean 0, where C n = I n -n -1 1 n 1 t n and C p = I p -p -1 1 p 1 t p .

## t-statistic in multivariate OLS

This problem extends Problem 5.8. Focus on multivariate OLS discussed in Chapter 5: y i = α + β1 x i1 + βt 2 x i2 + εi (i = 1, . . . , n), where x i1 is a scalar and x i2 can be a vector. Show that under homoskedasticity, the t-statistic associated with β1 equals ρyx1|x2

$(1 -ρ2 yx1|x2 )/(n -p) ,$where p is the total number of regressors and ρyx1|x2 is the sample partial correlation coefficient between y and x 1 given x 2 . Remark: [Frank (2000)](#b105) applied this formula to causal inference.

## Equivalence of the t-statistics in multivariate OLS

This problem extends Problems 5.9 and 6.5. With the data (x i1 , x i2 , y i ) n i=1 where both x i1 and y i are scalars and x i2 can be a vector. Run OLS fit of y i on (1, x i1 , x i2 ) to obtain t y|x1,x2 , the t-statistic of the coefficient of x i1 , under the homoskedasticity assumption. Run OLS fit of x i1 on (1, y i , x i2 ) to obtain t x1|y,x2 , the t-statistic of the coefficient of y i , under the homoskedasticity assumption.

Show t y|x1,x2 = t x1|y,x2 . Give a counterexample in which the numerical equivalence of the t-statistics breaks down based on the EHW standard error.

## Formula of the partial correlation coefficient

Prove Theorem 8.1.

## Examples of Simpson's Paradox

Give three numerical examples of (Y, X, W ) which causes Simpson's Paradox. Report the mean and covariance matrix for each example.

## Simpson's Paradox in reality

Find a real-life dataset with Simpson's Paradox.

## Basic properties of projection matrices

Prove Lemma 8.1.

## Correlation of the regression coefficients

1. Regress Y on (1 n , X 1 , X 2 ) where X 1 and X 2 are two n-vectors with positive sample Pearson correlation ρx1x2 > 0. Show that the corresponding OLS coefficients β1 and β2 are negatively correlated under the Gauss-Markov model.

2. Regress Y on (1 n , X 1 , X 2 , X 3 ) where X 1 and X 2 are two n-vectors and X 3 is an n × L dimensional matrix. If the partial correlation coefficient between X 1 and X 2 given X 3 is positive, then show that the corresponding OLS coefficients β1 and β2 are negatively correlated under the Gauss-Markov model.

## Inverse of sample covariance matrix and partial correlation coefficient

This is the sample version of Problem 2.7.

Based on X ∈ R n×p , we can compute the sample covariance matrix Σ. Denote its inverse by Σ-1 = (σ jk ) 1≤j,k≤p . Show that for any pair j ̸ = k, we have σjk = 0 ⇐⇒ ρxjx k |x \(j,k) = 0 where ρxjx k |x \(j,k) is the partial correlation coefficient of X j and X k given all other variables.

Cochran's Formula and Omitted-Variable Bias

## Cochran's formula

Consider an n × 1 vector Y , an n × k matrix X 1 , and an n × l matrix X 2 . Similar to the FWL theorem, we do not impose any statistical models. We can fit the following OLS:

$Y = X 1 β1 + X 2 β2 + ε, (9.1) Y = X 2 β2 + ε, (9.2) X 1 = X 2 δ + Û , (9.3)$where ε, ε are the residual vectors, and Û is the residual matrix. The last OLS fit means the OLS fit of each column of X 1 on X 2 , and therefore the corresponding residual Û is an n × k matrix.

Theorem 9.1 Under the OLS fits (9.1)-( [9](#).3), we have

$β2 = β2 + δ β1 .$This is a pure linear algebra fact similar to the FWL theorem. It is called Cochran's formula in statistics. Sir [David Cox (Cox, 2007)](#) attributed the formula to [Cochran (1938)](#b78) although Cochran himself attributed the formula to [Fisher (1925a)](#).

Cochran's formula may seem familiar to readers knowing the chain rule in calculus. In a deterministic world with scalar y, x 1 , x 2 , if

$y(x 1 , x 2 ) = x 1 β 1 + x 2 β 2 and x 1 (x 2 ) = x 2 δ, then dy dx 2 = ∂y ∂x 1 ∂x 1 ∂x 2 + ∂y ∂x 2 = δβ 1 + β 2 .$But the OLS decompositions above do not establish any deterministic relationships among Y and (X 1 , X 2 ).

In some sense, the formula is obvious. From the first and the third OLS fits, we have

$Y = X 1 β1 + X 2 β2 + ε = X 2 δ + Û β1 + X 2 β2 + ε = X 2 δ β1 + Û β1 + X 2 β2 + ε = X 2 δ β1 + β2 + Û β1 + ε . (9.4) FIGURE 9.1: A diagram for Cochran's formula$This suggests that β2 = β2 + δ β1 . The above derivation follows from simple algebraic manipulations and does not use any properties of the OLS. To prove Theorem 9.1, we need to verify that the last line is indeed the OLS fit of Y on X 2 . The proof is fact very simple.

Proof of Theorem 9.1: Based on the above discussion, we only need to show that (9.4) is the OLS fit of Y on X 2 , which is equivalent to show that Û β1 + ε is orthogonal to all columns of X 2 . This follows from

$X t 2 Û β1 + ε = X t 2 Û β1 + X t 2 ε = 0, because X t 2 Û = 0 based on the OLS fit in (9.$3) and X t 2 ε = 0 based on the OLS fit in (9.1).

## □

Figure [9](#fig_98).1 illustrates Theorem 9.1. Intuitively, β2 measures the total impact of X 2 on Y , which has two channels: β2 measures the impact acting directly and δ β1 measures the impact acting indirectly through X 1 .

Figure [9](#fig_98).1 shows the interplay among three variables. Theoretically, we can discuss a system of more than three variables which is called the path model. This more advanced topic is beyond the scope of this book. [Wright (1921](#b242)[Wright ( , 1934))](#b243)'s initial discussion of this approach was motivated by genetic studies. See [Freedman (2009)](#b111) for a textbook introduction.

## Omitted-variable bias

The proof of Theorem 9.1 is very simple. However, it is one of the most insightful formulas in statistics. Econometricians often call it the omitted-variable bias formula because it quantifies the bias of the OLS coefficient of X 2 in the short regression omitting possibly important variables in X 1 . If the OLS coefficient from the long regression is unbiased then the the OLS coefficient from the short regression has a biased term δ β1 , which equals the product of the coefficient of X 2 in the OLS fit of X 1 on X 2 and the coefficient of X 1 in the long regression.

Below I will discuss a canonical example of using OLS to estimate the treatment effect in observational studies. For unit i (i = 1, . . . , n), let y i be the outcome, z i be the binary treatment indicator (1 for the treatment group and 0 for the control group) and x i be the observed covariate vector. Practitioners often fit the following OLS:

$y i = β0 + β1 z i + βt$and interpret β1 as the treatment effect estimate. However, observational studies may suffer from unmeasured confounding, that is, the treatment and control units differ in unobserved but important ways. In the simplest case, the above OLS may have omitted a variable u i for each unit i, which is called a confounder. The oracle OLS is

$y i = β0 + β1 z i + βt 2 x i + β3 u i + εi$and the coefficient β1 is an unbiased estimator if the model with u i is correct. With X 1 containing the values of the u i 's and X 2 containing the values of the (1,

$z i , x t i )'s, Cochran's formula implies that   β0 β1 β2   =   β0 β1 β2   + β3   δ0 δ1 δ2  $where ( δ0 , δ1 , δt 2 ) t is the coefficient vector in the OLS fit of u i on (1, z i , x i ). Therefore, we can quantify the difference between the observed estimate β1 and oracle estimate β1 : β1 -β1 = β3 δ1 , which is sometimes called the confounding bias.

Using the basic properties of OLS, we can show that δ1 equals the difference in means of e i = u i -δt 2 x i across the treatment and control groups:

$δ1 = ē1 -ē0 ,$where the bar and subscript jointly denote the sample mean of a particular variable within a treatment group. So

$β -β1 = β3 (ē 1 -ē0 ). (9.5)$Moreover, we can obtain a more explicit formula for δ1 :

$δ1 = ū1 -ū0 -δt 2 (x 1 -x0 ). So β -β1 = β3 (ū 1 -ū0 ) -β3 δt 2 (x 1 -x0 ). (9.6)$Both (9.5) and (9.6) give some insights into the bias due to omitting an important covariate u. It is clear that the bias depends on β3 , which quantifies the relationship between u and y. The formula (9.5) shows that the bias also depends on the imbalance in means of u across the treatment and control groups, after adjusting for the observed covariates x, that is, the imbalance in means of the residual confounding. The formula (9.6) shows a more explicit formula of the bias. The above discussion is often called bias analysis in epidemiology or sensitivity analysis in statistics and econometrics.

## Homework problems

## Baron-Kenny method for mediation analysis

The Baron-Kenny method, popularized by [Baron and Kenny (1986)](#b54), is one of the most cited methods in social science. It concerns the interplay among three variables z, m, y controlling FIGURE 9.2: The graph for the Baron-Kenny method for some other variables x. Let Z, M, Y be n × 1 vectors representing the observed values of z, m, y, and let X be the n × p matrix representing the observations of x. The question of interest is to assess the "direct" and "indirect" effects of z on y, acting independently and through m, respectively. We do not need to define these notions precisely since we are only interested in the algebraic property below.

The Baron-Kenny method runs the OLS

$Y = β0 1 n + β1 Z + β2 M + X β3 + εY$and interprets β1 as the estimator of the "direct effect" of z on y. The "indirect effect" of z on y through m has two estimators. First, based on the OLS

$Y = β0 1 n + β1 Z + X β3 + εY ,$define the difference estimator as β1 -β1 . Second, based on the OLS

$M = γ0 1 n + γ1 Z + X γ2 + εM ,$define the product estimator as γ1 β2 . Figure [9](#fig_98).2 illustrates the OLS fits used in defining the estimators.

Prove that β1 -β1 = γ1 β2 that is, the difference estimator and product estimator are numerically identical.

## A special case of path analysis

Figure [9](#fig_98).3 represents the order of the variables X 1 , X 2 , X 3 , Y ∈ R n . Run the following OLS: Remark: The OLS coefficient of X 1 in the short regression of Y on (1 n , X 1 ) equals the summation of all the path coefficients from X 1 to Y as illustrated by Figure [9](#fig_98).3. This problem is a special case of the path model, but the conclusion holds in general.

$Y = β0 1 n + β1 X 1 + β2 X 2 + β3 X 3 + εY , X 3 = δ0 1 n + δ1 X 1 + δ2 X 2 + ε3 , X 2 = θ0 1 n + θ1 X 1 + ε2 , and Y = β0 1 n + β1 X 1 + εY .$
## EHW in long and short regressions

Theorem 9.1 gives Cochran's formula related to the coefficients from three OLS fits. This problem concerns the covariance estimation. There are at least two ways to estimate the covariance of β2 in the short regression (9.2). First, from the second OLS fit, the EHW covariance estimator is

$Ṽ2 = (X t 2 X 2 ) -1 X t 2 diag(ε 2 )X 2 (X t 2 X 2 ) -1 .$Second, Cochran's formula implies that β2 = ( δ, I l ) β1 β2 is a linear transformation of the coefficient from the long regression, which further justifies the EHW covariance estimator

$Ṽ ′ 2 = ( δ, I l )(X t X) -1 X t diag(ε 2 )X(X t X) -1 δt I l . Show that Ṽ ′ 2 = (X t 2 X 2 ) -1 X t 2 diag(ε 2 )X 2 (X t 2 X 2 ) -1 . Hint: Use the result in Problem 7.1. Remark: Based on Theorem 7.2, the EHW covariance estimator for β2 is V2 = ( Xt 2 X2 ) -1 Xt 2 diag(ε 2 ) X2 ( Xt 2 X2 ) -1 , where X2 = (I n -H 1 )X 2 .$Part IV

## Model Fitting, Checking, and Misspecification

## Multiple Correlation Coefficient

This chapter will introduce the R 2 , the multiple correlation coefficient, also called the coefficient of determination [(Wright, 1921)](#b242). It can achieve two goals: first, it extends the sample Pearson correlation coefficient between two scalars to a measure of correlation between a scalar outcome and a vector covariate; second, it measures how well multiple covariates can linearly represent an outcome.

10.1 Equivalent definitions of R 2

I start with the standard definition of R 2 between Y and X. Slightly different from other chapters, X excludes the column of 1's, so now X is an n × (p -1) matrix. Based on the OLS of Y on (1 n , X), we define

$R 2 = n i=1 (ŷ i -ȳ) 2 n i=1 (y i -ȳ) 2 .$We have discussed before that including 1 n in the OLS ensures

$n -1 n i=1 εi = 0, =⇒ n -1 n i=1 y i = n -1 n i=1 ŷi , =⇒ ȳ = ȳ,$i.e., the average of the fitted values equals the average of the original observed outcomes. So I use ȳ for both the means of outcomes and the fitted values. With scaling factor (n -1) -1 , the denominator of R 2 is the sample variance of the outcomes, and the numerator of R 2 is the sample variance of the fitted values. We can verify the following decomposition: Lemma 10.1 We have the following variance decomposition:

$n i=1 (y i -ȳ) 2 = n i=1 (ŷ i -ȳ) 2 + n i=1 (y i -ŷi ) 2 .$I leave the proof of Lemma 10.1 as Problem 10.1. Lemma 10.1 states that the total sum of squares n i=1 (y i -ȳ) 2 equals the regression sum of squares n i=1 (ŷ i -ȳ) 2 plus the residual sum of squares n i=1 (y i -ŷi ) 2 . From Lemma 10.1, R 2 must be lie within the interval [0, 1] which measures the proportion of the regression sum of squares in the total sum of squares. An immediate consequence of Lemma 10.1 is that

$rss = (1 -R 2 ) n i=1 (y i -ȳ) 2 .$We can also verify that R 2 is the squared sample Pearson correlation coefficient between Y and Ŷ .

$Theorem 10.1 We have R 2 = ρ2 y ŷ where ρyŷ = n i=1 (y i -ȳ)(ŷ i -ȳ) n i=1 (y i -ȳ) 2 n i=1 (ŷ i -ȳ) 2$.

(10.1) I leave the proof of Theorem 10.1 as Problem 10.2. It states that the multiple correlation coefficient equals the squared Pearson correlation coefficient between y i and ŷi . Although the sample Pearson correlation coefficient can be positive or negative, R 2 is always nonnegative. Geometrically, R 2 equals the squared cosine of the angle between the centered vectors Y -ȳ1 n and Ŷ -ȳ1 n ; see Chapter A.1.

In terms of long and short regressions, we can partition the design matrix into 1 n and X, then the OLS fit of the long regression is

$Y = 1 n β0 + X β + ε, (10.2)$and the OLS fit of the short regression is

$Y = 1 n β0 + ε, (10.3)$with β0 = ȳ. The total sum of squares is the residual sum of squares from the short regression so by Lemma 10.1, R 2 also equals

$R 2 = rss short -rss long rss short . (10.4)$10.2 R 2 and the F statistic

Under the Normal linear model

$Y = 1 n β 0 + Xβ + ε, ε ∼ N(0, σ 2 I n ),(10.5)$we can use the F statistic to test whether β = 0. This F statistic is a monotone function of R 2 . Most standard software packages report both F and R 2 . I first give a numeric result without assuming that model (10.5) is correct.

Theorem 10.2 We have

$F = n -p p -1 × R 2 1 -R 2 .$Proof of Theorem 10.2: Based on the long regression (10.2) and the short regression (10.3), we have (10.4) and

$F = (rss short -rss long )/(p -1)$rss long /(n -p) .

So the conclusion follows. □ I then give the exact distribution of R 2 under the Normal linear model.

Corollary 10.1 Under the Normal linear model (10.5), if β = 0, then

$R 2 ∼ Beta p -1 2 , n -p 2 .$Proof of Corollary 10.1: By definition, the F statistic can be represented as

$F = χ 2 p-1 /(p -1) χ 2 n-p /(n -p)$where χ 2 p-1 and χ 2 n-p denote independent χ 2 p-1 and χ 2 n-p random variables, respectively, with a little abuse of notation. Using Theorem 10.2, we have

$R 2 1 -R 2 = F × p -1 n -p = χ 2 p-1 χ 2 n-p which implies R 2 = χ 2 p-1 χ 2 p-1 + χ 2 n-p . Because χ 2 p-1 ∼ Gamma p-1 2 , 1 2 and χ 2 n-p ∼ Gamma n-p 2 , 1 2 by Proposition B.1, we have R 2 = Gamma p-1 2 , 1 2 Gamma p-1 2 , 1 2 + Gamma n-p 2 , 12$where Gamma p-1 2 , 1 2 and Gamma n-p 2 , 1 2 denote independent Gamma random variables, with a little abuse of notation. The R 2 follows the Beta distribution by the Beta-Gamma duality in Theorem B.1. □

## Numerical examples

Below I first use the LaLonde data to verify Theorems 10.1 and 10.2 numerically.

> library ( " Matching " ) > data ( lalonde ) > ols . fit = lm ( re 7 8 ~. , y = TRUE , data = lalonde ) > ols . summary = summary ( ols . fit ) > r 2 = ols . summary $ r . squared > all . equal ( r 2 , ( cor ( ols . fit $y , ols . fit

$$ fitted . values ))^2 , + check . names = FALSE ) [ 1 ] TRUE > > fstat = ols . summary $ fstatistic > all . equal ( fstat [ 1 ] , fstat [ 3 ]/ fstat [ 2 ]* r 2 /( 1 -r 2 ) , + check . names = FALSE ) [ 1 ] TRUE$I then use the data from [King and Roberts (2015)](#b156) to verify Theorems 10.1 and 10.2 numerically.

> library ( foreign ) > dat = read . dta ( " isq . dta " ) > dat = na . omit ( dat [ , c ( " multish " , " lnpop " , " lnpopsq " , + " lngdp " , " lncolony " , " lndist " , + " freedom " , " militexp " , " arms " , + " year 8 3 " , " year 8 6 " , " year 8 9 " , " year 9 2 " )]) > > ols . fit = lm ( log ( multish + 1 ) ~lnpop + lnpopsq + lngdp + lncolony + + lndist + freedom + militexp + arms + + year 8 3 + year 8 6 + year 8 9 + year 9 2 , + y = TRUE , data = dat ) > ols . summary = summary ( ols . fit ) > r 2 = ols . summary $ r . squared > all . equal ( r 2 , ( cor ( ols . fit $y , ols . fit $ fitted . values ))^2 , 

$+ check . names = FALSE ) [ 1 ] TRUE > > fstat = ols . summary $ fstatistic > all . equal ( fstat [ 1 ] , fstat [ 3 ]/ fstat [ 2 ]* r 2 /( 1 -r 2 ) , + check . names = FALSE ) [ 1 ] TRUE The R code is in code10.3.R.$
## Exact distribution of ρ

Assume the Normal linear model y i = α + βx i + ε i with a univariate x i with β = 0 and ε i 's IID N(0, σ 2 ). Find the exact distribution of ρxy .

## Partial R 2

The form (10.4) of R 2 is well defined in more general long and short regressions:

$Ŷ = 1 n β0 + X β + W γ + εY and Ŷ = 1 n β0 + W γ + εY$where X is an n × k matrix and W is an n × l matrix. Define the partial R 2 between Y and X given W as

$R 2 Y.X|W = rss(Y ∼ 1 n + W ) -rss(Y ∼ 1 n + X + W ) rss(Y ∼ 1 n + W )$which spells out the formulas of the long and short regressions. This is an intuitive measure of the multiple correlation between Y and X after controlling for W . The following properties make this intuition more explicit.

$1. The partial R 2 equals R 2 Y.X|W = R 2 Y.XW -R 2 Y.W 1 -R 2 Y.W$where R 2 Y.XW is the multiple correlation between Y and (X, W ), and R 2 Y.W is the multiple correlation between Y and W .

2. The partial R 2 equals the R 2 between εY and εX :

$R 2 Y.X|W = R 2 εY .ε X$where εX is the residual matrix from the OLS fit of X on (1 n , W ).

Prove the above two results. Do the following two results hold?

$R 2 Y.XW = R 2 Y.W + R 2 Y.X|W , R 2 Y.XW = R 2 Y.W |X + R 2 Y.X|W .$For each result, give a proof if it is correct, or give a counterexample if it is incorrect in general.

10.5 Omitted-variable bias in terms of the partial R 2

Revisit Section 9.2 on the following three OLS fits. The first one involves only the observed variables:

$y i = β0 + β1 z i + βt 2 x i + εi$, and the second and third ones involve the unobserved u:

$y i = β0 + β1 z i + βt 2 x i + β3 u i + εi , u i = δ0 + δ1 z i + δt 2 x i + vi .$The omitted-variable bias formula states that β -β1 = β3 δ1 . This formula is simple but may be difficult to interpret since u is unobserved and its scale is unclear to researchers. Prove that the formula has an alternative form:

$| β1 -β1 | 2 = R 2 Y.U |ZX × R 2 Z.U |X 1 -R 2 Z.U |X × rss(Y ∼ 1 n + Z + X) rss(Z ∼ 1 n + X) .$Remark: [Cinelli and Hazlett (2020)](#b77) suggested the partial R 2 parametrization for the omitted-variable bias formula. The formula has three factors: the first two factors depend on the unknown sensitivity parameters R 2 Y.U |ZX and R 2 Z.U |X , and the third factor equals the ratio of the two residual sums of squares based on the observed data.

## Leverage Scores and Leave-One-Out Formulas

## Leverage scores

We have seen the use of the hat matrix H = X(X t X) -[foot_7](#foot_7) X t in previous chapters. Because

$H =    x t 1 . . . x t n    (X t X) -1 x 1 • • • x n , its (i, j)th element equals h ij = x t i (X t X) -1 x j .$In this chapter, we will pay special attention to its diagonal elements

$h ii = x t i (X t X) -1 x i (i = 1, . . . , n)$often called the leverage scores, which play important roles in many discussions later.

First, because H is a projection matrix of rank p, we have

$n i=1 h ii = trace(H) = rank(H) = p which implies that n -1 n i=1 h ii = p/n,$i.e., the average of the leverage scores equals p/n and the maximum of the leverage scores must be larger than or equal to p/n, which is close to zero when p is small relative to n.

Second, because H = H 2 and H = H t , we have

$h ii = n j=1 h ij h ji = n j=1 h 2 ij = h 2 ii + j̸ =i h 2 ij ≥ h 2 ii which implies h ii ∈ [0, 1],$i.e., each leverage score is bounded between zero and one 1 . Third, because Ŷ = HY , we have

$ŷi = n j=1 h ij y j = h ii y i + j̸ =i h ij y j which implies that ∂ ŷi ∂y i = h ii .$So h ii measures the contribution of y i in the predicted value ŷi . In general, we do not want the contribution of y i in predicting itself to be too large, because this means we do not borrow enough information from other observations, making the prediction very noisy. This is also clear from the variance of the predicted value ŷi = x t i β under the Gauss-Markov model:

$2 var(ŷ i ) = x t i cov( β)x i = σ 2 x t i (X t X) -1 x i = σ 2 h ii .$So the variance of ŷi increases with h ii .

The final property of h ii is less obvious: it measures whether observation i is an outlier based on its covariate value, that is, whether x i is far from the center of the data. Partition the design matrix as

$X = 1 n X 2 with H 1 = n -1 1 n 1 t n . The covariates X 2 has sample mean x2 = n -1 n i=1 x i2 and sample covariance S = (n -1) -1 n i=1 (x i2 -x2 )(x i2 -x2 ) t = (n -1) -1 X t 2 (I n -H 1 )X 2 .$The sample Mahalanobis distance between x i2 and the center x2 is

$D 2 i = (x i2 -x2 ) t S -1 (x i2 -x2 ).$The following theorem shows that h ii is a monotone function of D[foot_8](#foot_8) i :

Theorem 11.1 We have

$h ii = 1 n + D 2 i n -1 , (11.1) so h ii ≥ 1/n.$Proof of Theorem 11.1: The definition of D 2 i implies that it is the (i, i)th element of the following matrix:

$   x 12 -x2 . . . x n2 -x2    t S -1 x 12 -x2 • • • x n2 -x2 =(I n -H 1 )X 2 (n -1) -1 X t 2 (I n -H 1 )X 2 -1 X t 2 (I n -H 1 ) =(n -1) X2 ( Xt 2 X2 ) -1 Xt 2 =(n -1) H2 =(n -1)(H -H 1 ), recalling that X2 = (I n -H 1 )X 2 , H2 = X2 ( Xt 2 X2 ) -1 Xt 2 , and H = H 1 + H2 by Lemma 7.2. Therefore, D 2 i = (n -1)(h ii -1/n)$which implies (11.1). □ These are the basic properties of the leverage scores. [Chatterjee and Hadi (1988)](#b72) provided an in-depth discussion of the properties of the leverage scores. We will see their roles frequently in later parts of the chapter.

Another advanced result on the leverage scores is due to [Huber (1973)](#b147). He proved that in the linear model with non-Normal IID ε i ∼ [0, σ 2 ] and σ 2 < ∞, all linear combinations of the OLS coefficient are asymptotically Normal if and only if the maximum leverage score converges to 0. This is a very elegant asymptotic result on the OLS coefficient. I give more details in Chapter C as an application of the Lindeberg-Feller CLT.

## Leave-one-out formulas

To measure the impact of the ith observation on the final OLS estimator, a natural approach is to delete the ith row from the full data

$X =    x t 1 . . . x t n    , Y =    y 1 . . . y n    ,$and check how much the OLS estimator changes. Let

$X [-i] =           x t 1 . . . x t i-1 x t i+1 . . . x t n           , Y [-i] =           y 1 . . . y i-1 y i+1 . . . y n          $denote the leave-i-out data, and define

$β[-i] = (X t [-i] X [-i] ) -1 X t [-i] Y [-i] (11.2)$as the corresponding OLS estimator. We can fit n OLS by deleting the ith row (i = 1, . . . , n). However, this is computationally intensive especially when n is large. The following theorem shows that we need only to fit OLS once.

Theorem 11.2 Recalling that β is the full data OLS, εi is the residual and h ii is the leverage score for the ith observation, we have

$β[-i] = β -(1 -h ii ) -1 (X t X) -1 x i εi if h ii ̸ = 1.$Proof of Theorem 11.2: From (11.2), we need to invert

$X t [-i] X [-i] = i ′ ̸ =i x i ′ x t i ′ = X t X -x i x t i and calculate X t [-i] Y [-i] = i ′ ̸ =i x i y i = X t Y -x i y i ,$which are the original X t X and X t Y without the contribution of the ith observation. Using the following Sherman-Morrison formula in Problem 1.3:

$(A + uv t ) -1 = A -1 -(1 + v t A -1 u) -1 A -1 uv t A -1 with A = X t X, u = x i , and v = -x i we can invert X t [-i] X [-i] as (X t [-i] X [-i] ) -1 = (X t X) -1 + 1 -x t i (X t X) -1 x i -1 (X t X) -1 x i x t i (X t X) -1 = (X t X) -1 + (1 -h ii ) -1 (X t X) -1 x i x t i (X t X) -1 . Therefore, β[-i] = (X t [-i] X [-i] ) -1 X t [-i] Y [-i] = (X t X) -1 + (1 -h ii ) -1 (X t X) -1 x i x t i (X t X) -1 (X t Y -x i y i ) = (X t X) -1 X t Y -(X t X) -1 x i y i + (1 -h ii ) -1 (X t X) -1 x i x t i (X t X) -1 X t Y -(1 -h ii ) -1 (X t X) -1 x i x t i (X t X) -1 x i y i = β -(X t X) -1 x i y i + (1 -h ii ) -1 (X t X) -1 x i x t i β -h ii (1 -h ii ) -1 (X t X) -1 x i y i = β -(1 -h ii ) -1 (X t X) -1 x i y i + (1 -h ii ) -1 (X t X) -1 x i ŷi = β -(1 -h ii ) -1 (X t X) -1 x i εi . □ With the leave-i-out OLS estimator β[-i] , we can define the predicted residual ε[-i] = y i -x t i β[-i] ,$which is different from the original residual εi . The predicted residual based on leave-one-out can better measure the performance of the prediction because it mimics the real problem of predicting a future observation. In contrast, the original residual based on the full data εi = y i -x t i β gives an overly optimistic measure of the performance of the prediction. This is related to the overfitting issue discussed later. Under the Gauss-Markov model, Theorem 4.2 implies that the original residual has mean zero and variance

$var(ε i ) = σ 2 (1 -h ii ),(11.3)$and we can show that the predicted residual has mean zero and variance

$var(ε [-i] ) = var(y i -x t i β[-i] ) = σ 2 + σ 2 x t i (X t [-i] X [-i] ) -1 x i . (11.4)$The following theorem further simplifies the predicted residual and its variance.

$Theorem 11.3 We have ε[-i] = εi /(1 -h ii ),$and under Assumption 4.1, we have

$var(ε [-i] ) = σ 2 /(1 -h ii ). (11.5)$Proof of Theorem 11.3: By definition and Theorem 11.2, we have

$ε[-i] = y i -x t i β[-i] = y i -x t i β -(1 -h ii ) -1 (X t X) -1 x i εi = y i -x t i β + (1 -h ii ) -1 x t i (X t X) -1 x i εi = εi + h ii (1 -h ii ) -1 εi = εi /(1 -h ii ). (11.6)$Combining (11.3) and (11.6), we can derive its variance formula. □ Comparing formulas (11.4) and (11.5), we obtain that

$1 + x t i (X t [-i] X [-i] ) -1 x i = (1 -h ii ) -1 = 1 -x t i (X t X) -1 x i -1 .$This is not an obvious linear algebra identity, but it follows immediately from the two ways of calculating the variance of the predicted residual.

## Applications of the leave-one-out formulas

## Gauss updating formula

Consider an online setting in which the data points come sequentially as illustrated by the figure below:

In this setting, we can update the OLS estimator step by step: based on the first n data points (x i , y i ) n i=1 , we calculate the OLS estimator β(n) , and with an additional data point (x n+1 , y n+1 ), we update the OLS estimator as β(n+1) . These two OLS estimators are closely related as shown in the following theorem.

Theorem 11.4 Let X (n) be the design matrix and Y (n) be the outcome vector for the first n observations. We have

$β(n+1) = β(n) + γ (n+1) ε[n+1] ,$where

$γ (n+1) = (X t (n+1) X (n+1) ) -1 x n+1 and ε[n+1] = y n+1 -x t n+1 β(n)$is the predicted residual of the (n + 1)the outcome based on the OLS of the first n observations. Proof of Theorem 11.4: This is the reverse form of the leave-one-out formula. We can view the first n + 1 data points as the full data, and β(n) as the OLS estimator leaving the (n + 1)the observation out. Applying Theorem 11.2, we have

$β(n) = β(n+1) -(X t (n+1) X (n+1) ) -1 x n+1 εn+1 1 -h n+1,n+1 = β(n+1) -γ (n+1) ε[n+1] ,$where εn+1 is the (n + 1)th residual based on the full data OLS, and the (n + 1)th predicted residual equals ε[n+1] = εn+1 /(1 -h n+1,n+1 ) based on Theorem 11.3. □ Theorem 11.4 shows that to obtain β(n+1) from β(n) , the adjustment depends on the predicted residual ε[n+1] . If we have a perfect prediction of the (n + 1)th observation based on β(n) , then we do not need to make any adjustment to obtain β(n+1) ; if the predicted residual is large, then we need to make a large adjustment.

Theorem 11.4 suggests an algorithm for sequentially computing the OLS estimators. But it gives a formula that involves inverting X t (n+1) X (n+1) at each step. Using the Sherman-Morrison formula in Problem 1.3 for updating the inverse of X t (n+1) X (n+1) based on the inverse of X t (n) X (n) , we have an even simpler algorithm below:

$(G1) Start with V (n) = (X t (n) X (n) ) -1 and β(n) . (G2) Update V (n+1) = V (n) -1 + x t n+1 V (n) x n+1 -1 V (n) x n+1 x t n+1 V (n) . (G3) Calculate γ (n+1) = V (n+1) x n+1 and ε[n+1] = y n+1 -x t n+1 β(n) . (G4) Update β(n+1) = β(n) + γ (n+1) ε[n+1] .$
## Outlier detection based on residuals

Under the Normal linear model Y = Xβ + ε with ε ∼ N(0, σ 2 I n ), we know some basic probabilistic properties of the residual vector:

$E(ε) = 0, var(ε) = σ 2 (I n -H).$At the same time, the residual vector is computable based on the data. So it is sensible to check whether these properties of the residual vector are plausible based on data, which in turn serves as modeling checking for the Normal linear model. The first quantity is the standardized residual

$standr i = εi σ2 (1 -h ii ) .$We may hope that it has mean 0 and variance 1. However, because of the dependence between εi and σ2 , it is not easy to quantify the exact distribution of standr i .

The second quantity is the studentized residual based on the predicted residual:

$studr i = ε[-i] σ2 [-i] /(1 -h ii ) = y i -x t i β[-i] σ2 [-i] /(1 -h ii )$, where β[-i] and σ2 [[-i]](#) are the estimates of the coefficient and variance based on the leave-iout OLS. Because (y i , β[-i] , σ2

[-i] ) are mutually independent under the Normal linear model, we can show that studr i ∼ t n-p-1 .

(11.7)

Because we know the distribution of studr i , we can compare it to the quantiles of the t distribution.

The third quantity is Cook's distance [(Cook, 1977)](#b79):

$cook i = ( β[-i] -β) t X t X( β[-i] -β)/(pσ 2 ) = (X β[-i] -X β) t (X β[-i] -X β)/(pσ 2 ),$where the first form measures the change of the OLS estimator and the second form measures the change in the predicted values based on leaving-i-out. It has a slightly different motivation, but eventually, it is related to the previous two residuals due to the leave-one-out formulas.

Theorem 11.5 Cook's distance is related to the standardized residual via:

$cook i = standr 2 i × h ii p(1 -h ii )$.

I leave the proof of Theorem 11.5 as Problem 11.4. I will end this subsubsection with two examples. The R code is in code11.3.2.R. The first one is simulated. I generate data from a univariate Normal linear model without outliers. I then use hatvalues, r.standard, r.student and cooks.distance to an lm.object to calculate the leverage scores, standardized residuals, studentized residuals, and Cook's distances. Their plots are in the first column of Figure [11](#fig_95).1.

$n = 1 0 0 x = seq ( 0 , 1 , length = n ) y = 1 + 3 * x + rnorm ( n ) lmmod = lm ( y ~x ) hatvalues ( lmmod ) rstandard ( lmmod ) rstudent ( lmmod ) cooks . distance ( lmmod )$If I add 8 to the outcome of the last observation, the plots change to the second column of Figure [11](#fig_95).1. If I add 8 to the 50th observation, the plots change to the last column of Figure [11](#fig_95).1. Both visually show the outliers. In this example, the three residual plots give qualitatively the same pattern, so the choice among them does not matter much. In general cases, we may prefer studr i because it has a known distribution under the Normal linear model.

The second one is a further analysis of the Lalonde data. Based on the plots in Figure [11](#fig_95).2, there are indeed some outliers in the data. It is worth investigating them more carefully.

Although the outliers detection methods above are very classic, they are rarely implemented in modern data analyses. They are simple and useful diagnostics. I recommend using them at least as a part of the exploratory data analysis.

## Jackknife

Jackknife is a general strategy for bias reduction and variance estimation proposed by [Quenouille (1949](#b195)[Quenouille ( , 1956) )](#b196) and popularized by [Tukey (1958)](#b227). Based on independent data (Z 1 , . . . , Z n ), how to estimate the variance of a general estimator θ(Z 1 , . . . , Z n ) of the parameter θ? Define θ[-i] as the estimator without observation i, and the pseudo-value as The jackknife point estimator is θj = n -1 n i=1 θi , and the jackknife variance estimator is

$θi = n θ -(n -1) θ[-i] .$$Vj = 1 n(n -1) n i=1 ( θi -θj )( θi -θj ) t .$We have already shown that the OLS coefficient is unbiased and derived several variance estimators for it. Here we focus on the jackknife in OLS using the leave-one-out formula for the coefficient. The pseudo-value is

$βi = n β -(n -1) β[-i] = n β -(n -1) β -(1 -h ii ) -1 (X t X) -1 x i εi = β + (n -1)(1 -h ii ) -1 (X t X) -1 x i εi .$The jackknife point estimator is It is a little unfortunate that the jackknife point estimator is not identical to the OLS estimator, which is BLUE under the Gauss-Markov model. We can show that E( βj ) = β and it is a linear estimator. So the Gauss-Markov theorem ensures that cov( βj ) ⪰ cov( β). Nevertheless, the difference between βj and β is quite small. I omit their difference in the following derivation. Assuming that βj ∼ = β, we can continue to calculate the approximate jackknife variance estimator:

$βj = β + n -1 n n -1 n i=1 x i x t i -1 n -1 n i=1 x i εi 1 -h ii .$$Vj ∼ = 1 n(n -1) n i=1 ( βi -β)( βi -β) t = n -1 n (X t X) -1 n i=1 εi 1 -h ii 2 x i x t i (X t X) -1 ,$which is almost identical to the HC3 form of the EHW covariance matrix introduced in Chapter 6.4. [2. Miller (1974)](#) first analyzed the jackknife in OLS but dismissed it immediately. [Hinkley (1977)](#b138) modified the original jackknife and proposed a version that is identical to HC1, and [Wu (1986)](#b244) proposed some further modifications and proposed a version that is identical to HC2. [Weber (1986)](#b234) made connections between EHW and jackknife standard errors. However, [Long and Ervin (2000)](#b169)'s finite-sample simulation seems to favor the original jackknife or HC3.

## Homework problems

11.1 Implementing the Gauss updating formula

Implement the algorithm in (G1)-( [G4](#)), and try it on simulated data.

## The distribution of the studentized residual

Prove (11.7).

## Leave-one-out coefficient

$Prove β = n i=1 w i β[-i] ,$and find the weights w i 's. Show they are positive and sum to one. Does β = n -1 n i=1 β[-i] hold in general?

## Cook's distance and the standardized residual

Prove Theorem 11.5.

## The relationship between the standardized and studentized residual

Prove the following results.

## (n

$-p -1)σ 2 [-i] = (n -p)σ 2 -ε2 i /(1 -h ii ). 2.$There is a monotone relationship between the standardized and studentized residual:

$studr i = standr i n -p -1 n -p -standr 2 i .$Remark: The right-hand side of the first result must be nonnegative so

$n k=1 ε2 k ≥ ε2 i /(1 -h ii )$, which implies the following inequality:

$h ii + ε2 i n k=1 ε2 k ≤ 1.$From this inequality, if h ii = 1 then εi = 0 which further implies that h ij = 0 for all j ̸ = i.

## Subset and full-data OLS coefficients

Leaving one observation out, we have the OLS coefficient formula in Theorem 11.2. Leave a subset of observations out, we have the OLS coefficient formula below. Partition the covariate matrix and outcome vector based on a subset S of {1, . . . , n}:

$X = X S X \S , Y = Y S Y \S .$Without using the observations in set S, we have the OLS coefficient

$β\S = (X t \S X \S ) -1 X \S Y \S .$The following formula facilitates the computation of many β\S 's simultaneously, which relies crucially on the subvector of the residuals εS = Y S -X S β\S and the submatrix of

$H H SS = X \S (X t X) -1 X t \S corresponding to the set S.$Theorem 11.6 Recalling that β is the full data OLS, we have

$β\S = β -(X t X) -1 X t \S (I -H SS ) -1 εS$where I is the identity matrix with the same dimension of H SS .

Prove Theorem 11.6.

## Bounding the leverage score

Theorem 11.1 shows n -1 ≤ h ii ≤ 1 for all i = 1, . . . , n. Prove the following stronger result:

$n -1 ≤ h ii ≤ s -1 i$where s i is the number of rows that are identical to x i or -x i .

11.8 More on the leverage score

$Show that det(X t [-i] X [-i] ) = (1 -h ii )det(X t X). Remark: If h ii = 1, then X t [-i] X [-i]$is degenerate with determinant 0. Therefore, if we delete an observation i with leverage score 1, the columns of the covariate matrix become linearly dependent.

## Population Ordinary Least Squares and Inference with a Misspecified Linear Model

Previous chapters assume fixed X and random Y . We can also view each observation (x i , y i ) as IID draws from a population and discuss population OLS. We will view the OLS from the level of random variables instead of data points. Besides its theoretical interest, the population OLS facilitates the discussion of the properties of misspecified linear models and motivates a prediction procedure without imposing any distributional assumptions.

## Population OLS

Assume that (x i , y i ) n i=1 are IID with the same distribution as (x, y), where x ∈ R p and y ∈ R. Below I will use (x, y) to denote a general observation, dropping the subscript i for simplicity. Let the joint distribution be F (x, y), and E(•), var(•), and cov(•) be the expectation, variance, and covariance under this joint distribution. We want to use x to predict y. The following theorem states that the conditional expectation E(y | x) is the best predictor in terms of the mean squared error.

Theorem 12.1 For any function m(x), we have the decomposition

$E {y -m(x)} 2 = E {E(y | x) -m(x)} 2 + E{var(y | x)}, (12.1)$provided the existence of the moments in (12.1). This implies

$E(y | x) = arg min m(•) E {y -m(x)} 2$with the minimum value equaling E{var(y | x)}, the expectation of the conditional variance of y given x.

Theorem 12.1 is well known in probability theory. I relegate its proof as Problem 12.1. We have finite data points, but the function E(y | x) lies in an infinite dimensional space. Nonparametric estimation of E(y | x) is generally a hard problem, especially with a multidimensional x. As a starting point, we often use a linear function of x to approximate E(y | x) and define the population OLS coefficient as

$β = arg min b∈R p L(b),where$$L(b) = E (y -x t b) 2 = E y 2 + b t xx t b -2yx t b = E(y 2 ) + b t E (xx t ) b -2E (yx t ) b is a quadratic function of b.$From the first-order condition, we have

$∂L(b) ∂b | b=β = 2E (xx t ) β -2E (xy) = 0 based on Proposition A.7, so β = {E (xx t )} -1 E (xy) ; (12.2)$from the second order condition, we have

$∂ 2 L(b) ∂b∂b t = 2E (xx t ) ≥ 0. So β is the unique minimizer of L(b).$The above derivation shows that x t β is the best linear predictor, and the following theorem states precisely that x t β is the best linear approximation to the possibly nonlinear conditional mean E(y | x).

Theorem 12.2 We have

$β = arg min b∈R p E {E(y | x) -x t b} 2 = {E (xx t )} -1 E {xE(y | x)} .$As a special case, when the covariate "x" in the above formulas contains 1 and a scalar x, the OLS coefficient has the following form.

Corollary 12.1 For scalar x and y, we have With β, we can define ε = y -x t β (12.3) as the population residual. Because we usually do not use the upper-case letter E for ε, this notation may cause confusions with previous discussion on OLS where ε denotes the vector of the error terms. Here ε is a scalar in (12.3). By the definition of β, we can verify

$(α, β) = arg min (a,b) E(y -a -bx) 2 ,$$E(xε) = E {x(y -x t β)} = E(xy) -E(xx t )β = 0. (12.4)$If we include 1 as a component of x, then E(ε) = 0, which, coupled with (12.4), implies cov(x, ε) = 0. So with an intercept in β, the mean of the population residual must be zero, and it is uncorrelated with other covariates by construction. We can also rewrite (12.3) as y = x t β + ε, (12.5) which holds by the definition of the population OLS coefficient and residual without any modeling assumption. We call (12.5) the population OLS decomposition.

## Population FWL theorem and Cochran's formula

To aid interpretation of the population OLS coefficient, we have the population FWL theorem.

Theorem 12.3 Consider the population OLS decomposition

$y = β 0 + β 1 x 1 + • • • + β p-1 x p-1 + ε (12.6)$and the partial population OLS decompositions:

$x k = γ 0 + γ 1 x 1 + • • • + no x k + • • • + γ p-1 x p-1 + xk , (12.7) y = δ 0 + δ 1 x 1 + • • • + no x k + • • • + δ p-1 x p-1 + ỹ, (12.8) ỹ = βk xk + ε, (12.9)$where xk is the residual from (12.7) and ỹ is the residual from (12.8). The coefficient β k from (12.6) equals cov(x k , ỹ)/var(x k ), the coefficient of xk from the population OLS of ỹ on xk in (12.9), which also equals cov(x k , y)/var(x k ), the coefficient of xk from the population OLS of y on xk . Moreover, ε from (12.6) equals ε from (12.9).

Similar to the proof of Theorem 7.1, we can invert the matrix E(xx t ) in (12.2) to prove Theorem 12.3 directly. Below I adopt an alternative proof which is a modification of the one given by [Angrist and Pischke (2008)](#b50). Proof of Theorem 12.3: Some basic identities of population OLS help to simplify the proof below. First, the OLS decomposition (12.7) ensures

$cov(x k , x k ) = cov(x k , γ 0 + γ 1 x 1 + • • • + no x k + γ p-1 x p-1 + xk ) = cov(x k , xk ) that is, cov(x k , x k ) = var(x k ). (12.10) It also ensures cov(x k , x l ) = 0, (l ̸ = k). (12.11)$Second, the OLS decompositions (12.6) and (12.8) ensure cov(x k , ε) = 0, (12.12) because xk is a linear combination of x by (12.7). It also ensures

$cov(x k , δ 0 + δ 1 x 1 + • • • + no x k + • • • + δ p-1 x p-1 ) = 0, which further implies cov(x k , y) = cov(x k , ỹ). (12.13)$Now I prove the equivalent forms of the coefficient β k . From (12.6), we have

$cov(x k , y) = cov (x k , β 0 + β 1 x 1 + • • • + β p-1 x p-1 + ε) = β k cov(x k , x k ) + l̸ =k β l cov(x k , x l ) + cov(x k , ε) = β k var(x k ),$by (12.10)-(12.12). Therefore,

$β k = cov(x k , y) var(x k ) ,$which also equals βk by (12.13).

Finally, I prove ε = ε. It suffices to show that ε = ỹ -βk xk satisfies

$E(ε) = 0, cov(x k , ε) = 0, cov(x l , ε) = 0 (l ̸ = k).$The first identity holds by (12.9). The second identity holds because

$cov(x k , ε) = cov(x k , ỹ) -βk cov(x k , xk ) = cov(x k , ỹ) -βk cov(x k , xk ) = 0$by (12.13), (12.10), and the population OLS of (12.9). The third identity holds because

$cov(x l , ε) = cov(x l , ỹ) -βk cov(x l , xk ) = 0$by (12.8) and (12.11). □ We also have a population version of Cochran's formula. Assume (y i , x i1 , x i2 ) n i=1 are IID, where y i is a scalar, x i1 has dimension k, and x i2 has dimension l. We have the following OLS decompositions of random variables [(12.14](#))

$y i = β t 1 x i1 + β t 2 x i2 + ε i ,$$y i = βt 2 x i2 + εi ,(12.15)$x i1 = δ t x i2 + u i .

(12.16) Equation (12.14) defines the population long regression, and Equation (12.15) defines the population short regression. In Equation (12.16), δ is a l × k matrix because it is the OLS decomposition of a vector on a vector. We can view (12.16) as OLS decomposition of each component of x i1 on x i2 . The following theorem states the population version of Cochran's formula.

Theorem 12.4 Based on (12.14)-(12.16), we have β2 = β 2 + δβ 1 .

I relegate the proof of Theorem 12.4 as Problem 12.5.

## Population R 2 and partial correlation coefficient

Exclude 1 from x and assume x ∈ R p-1 in this subsection. Assume that covariates and outcome are centered with mean zero and covariance matrix

$cov x y = Σ xx Σ xy Σ yx σ 2 y .$There are multiple equivalent definitions of R 2 . I start with the following definition

$R 2 = Σ yx Σ -1 xx Σ xy σ 2 y ,$and will give several equivalent definitions below. Let β be the population OLS coefficient of y on x, and ŷ = x t β be the best linear predictor.

Theorem 12.5 R 2 equals the ratio of the variance of the best linear predictor of y and the variance of y itself: R 2 = var(ŷ) var(y) .

Proof of Theorem 12.5: Because of the centering of x, we can verify that

$var(ŷ) = β t cov(x)β = cov(y, x)cov(x) -1 cov(x)cov(x) -1 cov(x, y)$= cov(y, x)cov(x) -1 cov(x, y).

## □

Theorem 12.6 R 2 equals the maximum value of the squared Pearson correlation coefficient between y and a linear combination of x:

$R 2 = max b∈R p-1 ρ 2 (y, x t b) = ρ 2 (y, ŷ).$Proof of Theorem 12.6: We have

$ρ 2 (y, x t b) = cov 2 (y, x t b) var(y)var(x t b) = b t Σ xy Σ yx b σ 2 y × b t Σ xx b . Define γ = Σ 1/2 xx b and b = Σ -1/2$xx γ such that b and γ have one-to-one mapping. So the maximum value of

$σ 2 y × ρ 2 (y, x t b) = γ t Σ -1/2 xx Σ xy Σ yx Σ -1/2 xx γ γ t γ , equals the maximum eigenvalue of Σ -1/2 xx Σ xy Σ yx Σ -1/2$xx , attained when γ equals the corresponding eigenvector; see Theorem A.4. This matrix is positive semi-definite and has rank one due to Proposition A.1, so it has exactly one non-zero eigenvalue which must equal its trace. So

$max b∈R p-1 ρ 2 (y, x t b) = σ -2 y trace(Σ -1/2 xx Σ xy Σ yx Σ -1/2 xx ) = σ -2 y trace(Σ xy Σ yx Σ -1/2 xx Σ -1/2 xx ) = σ -2 y trace(Σ yx Σ -1 xx Σ xy ) = σ -2 y Σ yx Σ -1 xx Σ xy = R 2 .$We can easily verify that R 2 = ρ 2 (y, ŷ).

□ We can also define population partial correlation and R 2 . For scalar y and x with another scalar or vector w, we can define the population OLS decomposition based on (1, w) as [(12.17)](#) where

$y = ŷ + ỹ, x = x + x,$$ỹ = {y -E(y)} -{w -E(w)} t β y , x = {x -E(x)} -{w -E(w)} t β x ,$with β y and β x being the coefficients of w in these population OLS. We then define the population partial correlation coefficient as

$ρ yx|w = ρ ỹ x.$If the marginal correlation and partial correlation have different signs, then we have Simpson's paradox at the population level. With a scalar w, we have a more explicit formula below.

Theorem 12.7 For scalar (y, x, w), we have

$ρ yx|w = ρ yx -ρ xw ρ yw 1 -ρ 2 xw 1 -ρ 2 yw .$I relegate the proof of Theorem 12.7 as Problem 12.7.

We can also extend the definition of ρ yx|w to the partial R 2 with a scalar y and possibly vector x and w. The population OLS decompositions (12.17) still hold in the general case. Then we can define the partial R 2 between y and x given w as the R 2 between ỹ and x:

$R 2 y.x|w = R 2 ỹ.x .$12.4 Inference for the population OLS

## Inference with the EHW standard errors

Based on the IID data (x i , y i ) n i=1 , we can easily obtain the moment estimator for the population OLS coefficient

$β = n -1 n i=1 x i x t i -1 n -1 n i=1 x i y i ,$and the residuals εi = y i -x t i β. Assume finite fourth moments of (x, y). We can use the law of large numbers to show that

$n -1 n i=1 x i x t i → E(xx t ), n -1 n i=1 x i y i → E(xy), so β → β in probability. We can use the CLT to show that n -1/2 n i=1 x i ε i → N(0, M ) in distribution, where M = E(ε 2 xx t ), so √ n( β -β) → N(0, V ) (12.18) in distribution, where V = B -1 M B -1 with B = E(xx t ).$The moment estimator for the asymptotic variance of β is again the Eicker-Huber-White robust covariance estimator:

$Vehw = n -1 n -1 n i=1 x i x t i -1 n -1 n i=1 ε2 i x i x t i n -1 n i=1 x i x t i -1 . (12.19)$Following the almost the same proof of Theorem 6.3, we can show that Vehw is consistent for the asymptotic covariance V . I summarize the formal results below, with the proof relegated as Problem 12.4.

Theorem 12.8 Assume that (x i , y i ) n i=1 iid ∼ (x, y) with E(∥x∥ 4 ) < ∞ and E(y 4 ) < ∞. We have (12.18) and n Vehw → V in probability.

So the EHW standard error is not only robust to the heteroskedasticity of the errors but also robust to the misspecification of the linear model [(Huber, 1967;](#b146)[White, 1980b;](#b237)[Angrist and Pischke, 2008;](#b50)[Buja et al., 2019a)](#). Of course, when the linear model is wrong, we need to modify the interpretation of β: it is the coefficient of x in the best linear prediction of y or the best linear approximation of the conditional mean function E(y | x).

12.5 To model or not to model? I start with a simple example. In the following calculation, I will use the fact that the kth moment of a Uniform(0, 1) random variable equals 1/(k + 1).

Example 12.1 Assume that x ∼ F (x), ε ∼ N(0, 1), x ε, and y = x 2 + ε.

$1. If x ∼ F 1 (x) is Uniform(-1, 1), then the best linear approximation is 1/3 + 0 • x. We can see this result from β(F 1 ) = cov(x, y) var(x) = cov(x, x 2 ) var(x) = E(x 3 ) var(x) = 0, and α(F 1 ) = E(y) = E(x 2 ) = 1/3. 2. If x ∼ F 2 (x)$is Uniform(0, 1), then the best linear approximation is -1/6 + x. We can see this result from

$β(F 2 ) = cov(x, y) var(x) = cov(x, x 2 ) var(x) = E(x 3 ) -E(x)E(x 2 ) E(x 2 ) -{E(x)} 2 = 1/4 -1/2 × 1/3 1/3 -(1/2) 2 = 1, and α(F 2 ) = E(y) -βE(x) = E(x 2 ) -E(x) = 1/3 -1/2 = -1/6 3. If x ∼ F 3 (x) is Uniform(-1, 0)$, then the best linear approximation is -1/6 -x. This result holds by symmetry.

Figure [12](#fig_110).1 shows the true conditional mean function x 2 and the best linear approximations. As highlighted in the notation above, the best linear approximation depends on the distribution of X.

From the above, we can see that the best linear approximation depends on the distribution of X. This complicates the interpretation of β from the population OLS decomposition [(Buja et al., 2019a)](#). More importantly, this can cause problems if we care about the external validity of statistical inference [(Sims, 2010, page 66)](#).

However, if we believe the following restricted mean model

$E(y | x) = x t β (12.20)$or, equivalently,

$y = x t β + ε, E(ε | x) = 0,$then the population OLS coefficient is the true parameter of interest:

${E(xx t )} -1 E(xy) = {E(xx t )} -1 E {xE(y | x)} = {E(xx t )} -1 E(xx t β) = β.$Moreover, the population OLS coefficient does not depend on the distribution of x. The above asymptotic inference applies to this model too. [Freedman (1981)](#b107) distinguished two types of OLS: the regression model and the correlation model, as shown in Figure [12](#fig_110).2. The left-hand side represents the regression model, or the restricted mean model (12.20). In the regression model, we first generate x and ε under some restrictions, for example, E(ε | x) = 0, and then generate the outcome based on y = x t β + ε, a linear function of x with error ε. In the correlation model, we start with a pair (x, y), then decompose y into the best linear predictor x t β and the leftover residual ε. The latter ensures E(εx) = 0, but the former requires E(ε | x) = 0. So the former imposes a stronger assumption since E(ε | x) = 0 implies

$E(εx) = E{E(εx | x)} = E{E(ε | x)x} = 0.$12.5.2 Anscombe's Quartet: the importance of graphical diagnostics [Anscombe (1973)](#b51) used four simple datasets to illustrate the importance of graphical diagnostics in linear regression. His datasets are in anscombe in the R package datasets: x1 and y1 constitute the first dataset, and so on.

> library ( datasets ) > # # Anscombe ' s Quartet > anscombe

x 1 x 2 x 3 x 4 y 1 y 2 y 3 y 4 1 1 0 1 0 1 0 8 8 . 0 4 9 . 1 4 7 . 4 6 6 . 5 8 2 8 8 8 8 6 . 9 5 8 . 1 4 6 . 7 7 5 . 7 6 3 1 3 1 3 1 3 8 7 . 5 8 8 . 7 4 1 2 . 7 4 7 . 7 1 4 9 9 9 8 8 . 8 1 8 . 7 7 7 . 1 1 8 . 8 4 5 1 1 1 1 1 1 8 8 . 3 3 9 . 2 6 7 . 8 1 8 . 4 7 6 1 4 1 4 1 4 8 9 . 9 6 8 . 1 0 8 . 8 4 7 . 0 4 regression model correlation model FIGURE 12.2: Freedman's classification of OLS 7 6 6 6 8 7 . 2 4 6 . 1 3 6 . 0 8 5 . 2 5 8 4 4 4 1 9 4 . 2 6 3 . 1 0 5 . 3 9 1 2 . 5 0 9 1 2 1 2 1 2 8 1 0 . 8 4 9 . 1 3 8 . 1 5 5 . 5 6 1 0 7 7 7 8 4 . 8 2 7 . 2 6 6 . 4 2 7 . 9 1 1 1 5 5 5 8 5 . 6 8 4 . 7 4 5 . 7 3 6 . 8 9

The four datasets have similar sample moments.

$> # # mean of x > c ( mean ( anscombe $ x 1 ) , + mean ( anscombe $ x 2 ) , + mean ( anscombe $ x 3 ) , + mean ( anscombe $ x 4 )) [ 1 ] 9 9 9 9 > # # variance of x > c ( var ( anscombe $ x 1 ) , + var ( anscombe $ x 2 ) , + var ( anscombe $ x 3 ) , + var ( anscombe $ x 4 )) [ 1 ] 1 1 1 1 1 1 1 1 > # # mean of y > c ( mean ( anscombe $ y 1 ) , +$mean ( anscombe $ y 2 ) , + mean ( anscombe $ y 3 ) , + mean ( anscombe $ y 4 )) [ 1 ] 7 . 5 0 0 9 0 9 7 . 5 0 0 9 0 9 7 . 5 0 0 0 0 0 7 . 5 0 0 9 0 9 > # # variance of y > c ( var ( anscombe $ y 1 ) , + var ( anscombe $ y 2 ) , + var ( anscombe $ y 3 ) , + var ( anscombe $ y 4 )) [ 1 ] 4 . 1 2 7 2 6 9 4 . 1 2 7 6 2 9 4 . 1 2 2 6 2 0 4 . 1 2 3 2 4 9

The results based on linear regression are almost identical.

> ols 1 = lm ( y 1 ~x 1 , data = anscombe ) > summary ( ols 1 ) Call : lm ( formula = y 1 ~x 1 , data = anscombe ) [9 2 1 2 7 -0 . 4 5 5 7 7 -0 . 0 4 1 3 6 0 . 7 0 9 4 1 1 . 8 3 8 8 2](#) Coefficients :

$Residuals : Min 1 Q Median 3 Q Max -1 .$Estimate Std . Error t value Pr ( >| t |) ( Intercept )

3 . 0 0 0 1 1 . 1 2 4 7 2 . 6 6 7 0 . 0 2 5 7 3 * x 1 0 . 5 0 0 1 0 . 1 1 7 9 4 . 2 4 1 0 . 0 0 2 1 7 ** Residual standard error : 1 . 2 3 7 on 9 degrees of freedom Multiple R -squared : 0 . 6 6 6 5 , Adjusted R -squared : 0 . 6 2 9 5 F -statistic : 1 7 . 9 9 on 1 and 9 DF , p -value : 0 . 0 0 2 1 7 > ols 2 = lm ( y 2 ~x 2 , data = anscombe ) > summary ( ols 2 ) Call : lm ( formula = y 2 ~x 2 , data = anscombe ) Residuals :

Min 1 Q Median 3 Q Max -1 . 9 0 0 9 -0 . 7 6 0 9 0 . 1 2 9 1 0 . 9 4 9 1 1 . Residual standard error : 1 . 2 3 6 on 9 degrees of freedom Multiple R -squared : 0 . 6 6 6 7 , Adjusted R -squared : 0 . 6 2 9 7 F -statistic :

1 8 on 1 and 9 DF , p -value : 0 . 0 0 2 1 6 5

However, the scatter plots of the datasets in Figure [12](#fig_110).3 reveal fundamental differences between the datasets. The first dataset seems ideal for linear regression. The second dataset shows a quadratic form of y versus x, and therefore, the linear model is misspecified. The third dataset shows a linear trend of y versus x, but an outlier has severely distorted the slope of the linear line. The fourth dataset is supported on only two values of x and thus may suffer from severe extrapolation.

## More on residual plots

Most standard statistical theory for inference assumes a correctly specified linear model (e.g., Gauss-Markov model, Normal linear model, or restricted mean model). However, the corresponding inferential procedures are often criticized since it is challenging to ensure that the model is correctly specified. Alternatively, we can argue that without assuming the linear model, the OLS estimator is consistent for the coefficient in the best linear approximation of the conditional mean function E(y | x), which is often a meaningful quantify even the linear model is misspecified. This can be misleading. Example 12.1 shows that the best linear approximation can be a bad approximation to a nonlinear conditional mean function, and it depends on the distribution of the covariates.

A classic statistical approach is to check whether the residual εi has any nonlinear trend with respect to the covariates. With only a few covariates, we can plot the residual against each covariate; with many covariates, we can plot the residual against the fitted value ŷi . Figure [12](#fig_110).4 gives four examples with the R code in code12.5.2.R. In these examples, the covariates are the same:

$n = 2 0 0 x 1 = rexp ( n ) x 2 = runif ( n )$The outcome models differ:

$1. Example a: y = x1 + x2 + rnorm(n) 2. Example b: y = x1 + x2 + rnorm(n, 0, x1+x2) 3. Example c: y = x1^2+ x2^2 + rnorm(n) 4. Example d: y = x1^2+ x2^2 + rnorm(n, 0, x1+x2)$In the last two examples, the residuals indeed show some nonlinear relationship with the covariates and the fitted value. This suggests that the linear function can be a poor approximation to the true conditional mean function. It requires the Normal linear model assumption. Chapter 6 relaxes the Normality assumption on the error term in statistical inference but does not discuss prediction. Under the heteroskedastic linear model assumption, we can predict the mean E(y n+1 ) = x t n+1 β by x t n+1 β with asymptotic standard error (x t n+1 Vehw x n+1 ) 1/2 , where Vehw is the EHW covariance matrix for the OLS coefficient. However, it is fundamentally challenging to predict y n+1 itself since the heteroskedastic linear model allows it to have a completely unknown variance σ 2 n+1 . Under the population OLS formulation, it seems even more challenging to predict the future outcome since we do not even assume that the linear model is correctly specified. In particular, x t n+1 β does not have the same mean as y n+1 in general. Perhaps surprisingly, we can construct a prediction interval for y n+1 based on x n+1 and (X, Y ) using an idea called conformal prediction [(Vovk et al., 2005;](#b233)[Lei et al., 2018)](#b163). It leverages the exchangeability[foot_9](#foot_9) of the data points (x 1 , y 1 ), . . . , (x n , y n ), (x n+1 , y n+1 ).

Pretending that we know the value y n+1 = y * , we can fit OLS using n + 1 data points and obtain residuals εi (y * ) = y i -x t i β(y * ), (i = 1, . . . , n + 1) where we emphasize the dependence of the OLS coefficient and residuals on the unknown y * . The absolute values of the residuals |ε i (y * )|'s are also exchangeable, so the rank of

$|ε n+1 (y * )|, denoted by Rn+1 (y * ) = 1 + n i=1 1{|ε i (y * )| ≤ |ε n+1 (y * )|},$must have a uniform distribution over {1, 2, . . . , n, n + 1}, a known distribution not depending on anything else. It is a pivotal quantity satisfying pr Rn+1 (y * ) ≤ ⌈(1 -α)(n + 1)⌉ ≥ 1 -α.

(12.21)

Equivalently, this is a statement linking the unknown quantity y * and observed data, so it gives a confidence set for y * at level 1 -α. In practice, we can use a grid search to solve for the inequality (12.21) involving y * . Below we evaluate the leave-one-out prediction with the Boston housing data. 

$= qr . Q ( X . QR ) X . R = qr . R ( X . QR ) Gram . inv = solve ( t ( X . R )%*% X . R ) hatmat = X . Q %*% t ( X . Q ) resmat = diag ( n ) -hatmat leverage = diag ( hatmat ) Resvec$= ols . fit . full $ residuals cvt = qt ( 0 . 9 7 5 , df = n -p -1 ) cvr = ceiling ( 0 . 9 5 *( n + 1 ))

loo . pred = matrix ( 0 , n , 5 ) loo . cov = matrix ( 0 , n , 2 )

$for ( i in 1 : n ) { beta . i = beta -Gram . inv %*% X [i , ]* Resvec [ i ]/( 1 -leverage [ i ]) e . sigma . i = sqrt ( e . sigma ^2 *( n -p ) - ( Resvec [ i ])^2 /( 1 -leverage [ i ]))/ sqrt ( n -p -1 ) pred . i = sum ( X [i , ]* beta . i ) lower . i = pred . i -cvt * e . sigma . i / sqrt ( 1 -leverage [ i ]) upper . i = pred . i + cvt * e . sigma . i / sqrt ( 1 -leverage [ i ]) loo . pred [i , 1 : 3 ] = c ( pred .i , lower .i , upper . i ) loo . cov [i , 1 ] = findInterval ( Y [ i ] , c ( lower .i , upper . i )) grid . r = sapply ( grid .y , FUN = function ( y ){ Res = Resvec + resmat [ , i ]*( y -Y [ i ])$q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q first 20 middle 20 last 20 In the above code, I use the QR decomposition to compute X t X and H. Moreover, the calculations of lower.i, upper.i, and Res use some tricks to avoid fitting n OLS. I relegate the justification of them in Problem 12.10.

The variable loo.pred has five columns corresponding to the point predictors, lower and upper intervals based on the Normal linear model and conformal prediction.

> colnames ( loo . pred ) = c ( " point " , " G . l " , " G . u " , " c . l " , " c . u " ) > head ( loo . pred ) point G . l G . u c . l c . u [ 1 ,] 6 . 6 3 3 5 1 4 -2 . 9 4 1 5 3 2 1 6 . 2 0 8 5 5 9 -3 . 5 1 6 . 7 [ 2 ,] 8 . 8 0 6 6 4 1 -1 . 3 4 9 3 6 7 1 8 . 9 6 2 6 4 9 -2 . 6 2 0 . 1 [ 3 ,] 1 2 . 0 4 4 1 5 4 2 . 6 0 8 2 9 0 2 1 . 4 8 0 0 1 8 2 . 2 2 1 . 8 [ 4 ,] 1 1 . 0 2 5 2 5 3 1 . 5 6 5 1 5 2 2 0 . 4 8 5 3 5 5 1 . 2 2 1 . 0 [ 5 ,] -5 . 1 8 1 1 5 4 -1 4 . 8 1 9 0 4 1 4 . 4 5 6 7 3 3 -1 5 . 0 4 . 9 [ 6 ,] 8 . 3 2 4 1 1 4 -1 . 3 8 2 9 1 0 1 8 . 0 3 1 1 3 8 -2 . 0 1 8 . 8

Figure [12](#fig_110).5 plots the observed outcomes and the prediction intervals for the 20 observations with the outcomes at the bottom, middle, and top. The Normal and conformal intervals are almost indistinguishable. For the observations with the highest outcome, the predictions are quite poor. Surprisingly, the overall coverage rates across observations are close to 95% for both methods.

> apply ( loo . cov == 1 , 2 , mean ) [ 1 ] 0 . 9 4 8 6 1 6 6 0 . 9 5 2 5 6 9 2 Figure 12.6 compares the lengths of the two prediction intervals. Although the conformal prediction intervals are slightly wider than the Normal prediction interval, the differences are rather small, with the ratio of the length above 0.96.

The R code is in code12.6.R. q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0.85

0.90 0.95 1.00 0 100 200 300 400 500 observation i Normal over conformal ratio of the lengths of leave-one-out prediction intervals FIGURE 12.6: Boston housing data 12.7 Homework problems 12.1 Conditional mean Prove Theorem 12.1. 12.2 Best linear approximation Prove Theorem 12.2. Hint: It is similar to Problem 8.6. 12.3 Univariate population OLS Prove Corollary 12.1. 12.4 Asymptotics for the population OLS Prove Theorem 12.8. 12.5 Population Cochran's formula Prove Theorem 12.4. 12.6 Canonical correlation analysis (CCA) Assume that (x, y), where x ∈ R p and y ∈ R k , has the joint non-degenerate covariance matrix: Σ xx Σ xy Σ yx Σ yy . 1. Find the best linear combinations (α, β) that give the maximum Pearson correlation coefficient: (α, β) = arg max a∈R k ,b∈R p ρ(y t a, x t b).

Note that you need to detail the steps in calculating (α, β) based on the covariance matrix above.

2. Define the maximum value as cc(x, y). Show that cc(x, y) ≥ 0 and cc(x, y) = 0 if x y.

Remark: The maximum value cc(x, y) is called the canonical correlation between x and y. We can also define partial canonical correlation between x and y given w.

## Population partial correlation coefficient

Prove Theorem 12.7.

## Independence and correlation

With scalar random variables x and y, show that if x y, then ρ yx = 0. With another random variable w, if x y | w, does ρ yx|w = 0 hold? If so, give a proof; otherwise, give a counterexample.

12.9 Best linear approximation of a cubic curve Assume that x ∼ N(0, 1), ε ∼ N(0, σ 2 ), x ε, and y = x 3 + ε. Find the best linear approximation of E(y | x) based on x.

## Leave-one-out formula in conformal prediction

Justify the calculations of lower.i, upper.i, and Res in Section 12.6.

## Conformal prediction for multiple outcomes

Assuming exchangeability of (x 1 , y 1 ), . . . , (x n , y n ), (x n+1 , y n+1 ), . . . , (x n+k , y n+k ).

Propose a method to construct joint conformal prediction regions for (y n+1 , . . . , y n+k ) based on (X, Y ) and (x n+1 , . . . , x n+k ).

12.12 Cox's theorem [Cox (1960)](#b80) considered the data-generating process x 1 -→ x 2 -→ y under the following linear models: for i = 1, . . . , n, we have

$x i2 = α 0 + α 1 x i1 + η i and y i = β 0 + β 1 x i2 + ε i$where η i has mean 0 and variance σ 2 η , ε i has mean 0 and variance σ 2 ε , and the η i s and ε i s are independent. The linear model implies

$y i = (β 0 + β 1 α 0 ) + (β 1 α 1 )x i1 + (ε i + β 1 η i )$where ε i + β 1 η i s are independent with mean 0 and variance σ 2 ε + β 2 1 σ 2 η . Therefore, we have two ways to estimate β 1 α 1 :

1. the first estimator is γ1 , the OLS estimator of the y i 's on the x i1 's with the intercept; 2. the second estimator is α1 β1 , the product of the OLS estimator of the x i2 's on the x i1 's with the intercept and that of the y i 's on the x i2 's with the intercept. [Cox (1960)](#b80) proved the following theorem.

Theorem 12.9 Let X 1 = (x 11 , . . . , x n1 ). We have var( α1 β1 | X 1 ) ≤ var(γ 1 | X 1 ), and more precisely,

$var(γ 1 | X 1 ) = σ 2 ε + β 2 1 σ 2 η n i=1 (x i1 -x1 ) 2 and var( α1 β1 | X 1 ) = σ 2 ε E(ρ 2 12 | X 1 ) + β 2 1 σ 2 η n i=1 (x i1 -x1 ) 2$where ρ12 ∈ [-1, 1] is the sample Pearson correlation coefficient between the x i1 's and the x i2 's.

Prove Theorem 12.9. Remark: If we further assume that the error terms are Normal, then α1 β1 is the maximum likelihood estimator for α 1 β 1 . Therefore, the asymptotic optimality theory for the maximum likelihood estimator justifies the superiority of α1 β1 over γ1 . Cox's theorem provides a stronger finite-sample result without assuming the Normality of the error terms.

12.13 Measurement error and Frisch's bounds 1. Given scalar random variables x and y, we can obtain the population OLS coefficient (α, β) of y on (1, x). However, x and y may be measured with errors, that is, we observe x * = x + u and y * = y + v, where u and v are mean zero error terms satisfying u v and (u, v) (x, y). We can obtain the population OLS coefficient (α * , β * ) of y * on (1, x * ) and the population OLS coefficient (a * , b * ) of x * on (1, y * ).

Prove that if β = 0 then

$β * = b * = 0; if β ̸ = 0 then |β * | ≤ |β| ≤ 1/|b * |.$2. Given scalar random variables x, y and a random vector w, we can obtain the population OLS coefficient (α, β, γ) of y on (1, x, w). When x and y are measured with error as above with mean zero errors satisfying u v and (u, v) (x, y, w), we can obtain the population OLS coefficient (α * , β * , γ * ) of y on (1, x * , w), and the population OLS coefficient (a * , b * , c * ) of x * on (1, y * , w).

Prove that the same result holds as in the first part of the problem.

Remark: Tamer (2010) reviewed Frisch (1934)'s upper and lower bounds for the univariate OLS coefficient based on the two OLS coefficients of the observables. The second part of the problem extends the result to the multivariate OLS with a covariate subject to measurement error. The lower bound is well documented in most books on measurement errors, but the upper bound is much less well known.

## A three-way decomposition

The main text focuses on the two-way decomposition of the outcome: y = x t β + ε, where β is the population OLS coefficient and ε is the population OLS residual. However, x t β is only the best linear approximation to the true conditional mean function µ(x) ≡ E(y | x). This suggests the following three-way decomposition of the outcome:

$y = x t β + {µ(x) -x t β} + {y -µ(x)},$which must hold without any assumptions. Introduce the notation for the linear term ŷ = x t β, the notation for the approximation error:

$δ = µ(x) -x t β,$and the notation for the "ideal residual":

$e = y -µ(x).$Then we can decompose the outcome as

$y = ŷ + δ + e$and the population OLS residual as

$ε = {µ(x) -x t β} + {y -µ(x)} = δ + e. 1. Prove that E(ŷe | x) = 0, E(δe | x) = 0 and E(ŷe) = 0, E(δe) = 0, E(ŷδ) = 0. Further, prove that E(ε 2 ) = E(δ 2 ) + E(e 2 ).$2. Introduce an intermediate quantity between the population OLS coefficient β and the OLS coefficient β:

$β = n -1 n i=1 x i x t i -1 n -1 n i=1 x i µ(x i ) . Equation (12.18) states that √ n( β -β) → N(0, B -1 M B -1 ) in distribution, where B = E(xx t ) and M = E(ε 2 xx t ). Prove that cov( β -β) = cov( β -β) + cov( β -β),$and moreover,

$√ n( β -β) → N(0, B -1 M 1 B -1 ), √ n( β -β) → N(0, B -1 M 2 B -1 )$in distribution, where

$M 1 = E(e 2 xx t ), M 2 = E(δ 2 xx t ) Verify that M = M 1 + M 2 .$Remark: To prove the result, you may find the law of total covariance formula in (B.4) helpful. We can also write M 1 as M 1 = E{var(y | x)xx t }. So the meat matrix M has two sources of uncertainty, one is from the conditional variance of y given x, and the other is from the approximation error.

## Part V

Overfitting, Regularization, and Model Selection

## Perils of Overfitting

Previous chapters assume that the covariate matrix X is given and the linear model, correctly specified or not, is also given. Although including useless covariates in the linear model results in less precise estimators, this problem is not severe when the total number of covariates is small compared to the sample size. In many modern applications, however, the number of covariates can be large compared to the sample size. Sometimes, it can be a nonignorable fraction of the sample size; sometimes, it can even be larger than the sample size. For instance, modern DNA sequencing technology often generates covariates of millions of dimensions, which is much larger than the usual sample size under study. In these applications, the theory in previous chapters is inadequate. This chapter introduces an important notation in statistics: overfitting.

13.1 David Freedman's simulation [Freedman (1983)](#b108) used a simple simulation to illustrate the problem with a large number of covariates. He simulated data from the following Normal linear model Y = Xβ + ε with ε ∼ N(0, σ 2 I n ) and β = (µ, 0, . . . , 0) t . He then computed the sample R 2 . Since the covariates do not explain any variability of the outcome at all in the true model, we would expect R 2 to be extremely small over repeated sampling. However, he showed, via both simulation and theory, that R 2 is surprisingly large when p is large compared to n. Figure [13](#fig_95).1 shows the results from Freedman's simulation setting with n = 100 and p = 50, over 1000 replications. The R code is in code13.1.R. The (1, 1)th subfigure shows the histogram of the R 2 , which centers around 0.5. This can be easily explained by the exact distribution of R 2 proved in Corollary 10.1:

$R 2 ∼ Beta p -1 2 , n -p 2 ,$with the density shown in the (1, 1)th and (1, 2)th subfigure of Figure [13](#fig_95).1. The beta distribution above has mean

$E(R 2 ) = p-1 2 p-1 2 + n-p 2 = p -1 n -1 and variance var(R 2 ) = p-1 2 × n-p 2 p-1 2 + n-p 2 2 p-1 2 + n-p 2 + 1 = 2(p -1)(n -p) (n -1) 2 (n + 1) .$When p/n → 0, we have E(R 2 ) → 0, var(R 2 ) → 0, so Markov's inequality implies that R 2 → 0 in probability. However, when p/n → γ ∈ (0, 1), we have E(R 2 ) → γ, var(R 2 ) → 0, so Markov's inequality implies that R 2 → γ in probability. This means that when p has the same order as n, the sample R 2 is close to the ratio p/n even though there is no association between the covariates and the outcome in the true data-generating process. In Freedman's simulation, γ = 0.5 so R 2 is close to 0.5. The (1, 2)th subfigure shows the histogram of the R 2 based on a model selection first step by dropping all covariates with p-values larger than 0.25. The R 2 in the (1, 2)th subfigure are slightly smaller but still centered around 0.37. The joint F test based on the selected model does not generate uniform p-values in the (2, 2)th subfigure, in contrast to the uniform p-values in the (2, 1)th subfigure. With a model selection step, statistical inference becomes much more complicated. This is a topic called selective inference which is beyond the scope of this book.

The above simulation and calculation give an important warning: we cannot overinterpret the sample R 2 since it can be too optimistic about model fitting. In many empirical research, R 2 is at most 0.1 with a large number of covariates, making us wonder whether those researchers are just chasing the noise rather than the signal. So we do not trust R 2 as a model-fitting measure with a large number of covariates. In general, R 2 cannot avoid overfitting, and we must modify it in model selection.

## Variance inflation factor

The following theorem quantifies the potential problem of including too many covariates in OLS.

Theorem 13.1 Consider a fixed design matrix X. Let βj be the coefficient of X j of the OLS fit of Y on (1 n , X 1 , . . . , X q ) with q ≤ p. Under the model y i = f (x i ) + ε i with an unknown f (•) and the ε i 's uncorrelated with mean zero and variance σ 2 , the variance of βj equals

$var( βj ) = σ 2 n i=1 (x ij -xj ) 2 × 1 1 -R 2 j ,$where R 2 j is the sample R 2 from the OLS fit of X j on 1 n and all other covariates.

Theorem 13.1 does not even assume that the true mean function is linear. It states that the variance of βj has two multiplicative components. If we run a short regression of Y on 1 n and X j = (x 1j , . . . , x nj ) t , the coefficient equals

$βj = n i=1 (x ij -xj )y i n i=1 (x ij -xj ) 2 where xj = n -1 n i=1 x ij . It has variance var( βj ) = var n i=1 (x ij -xj )y i n i=1 (x ij -xj ) 2 = n i=1 (x ij -xj ) 2 σ 2 { n i=1 (x ij -xj ) 2 } 2 = σ 2 n i=1 (x ij -xj ) 2 .$So the first component is the variance of the OLS coefficient in the short regression. The second component 1/(1 -R 2 j ) is called the variance inflation factor (VIF). The VIF indeed inflates the variance of βj , and the more covariates are added into the long regression, the larger the variance inflation factor is. In R, the car package provides the function vif to compute the VIF for each covariate.

The proof of Theorem 13.1 below is based on the FWL theorem. Proof of Theorem 13.1: Let Xj = (x 1j , . . . , xnj ) t be the residual vector from the OLS fit of X j on 1 n and other covariates, which have a sample mean of zero. The FWL theorem implies that

$βj = n i=1 xij y i n i=1 x2 ij which has variance var( βj ) = n i=1 x2 ij σ 2 n i=1 x2 ij 2 = σ 2 n i=1 x2 ij . (13.1) Because n i=1$x2 ij is the residual sum of squares from the OLS of X j on 1 n and other covariates, it is related to R 2 j via

$R 2 j = 1 - n i=1 x2 ij n i=1 (x ij -xj ) 2 or, equivalently, n i=1 x2 ij = (1 -R 2 j ) n i=1 (x ij -xj ) 2 . (13.2)$Combining (13.1) and (13.2) gives Theorem 13.1. □

## Bias-variance trade-off

Theorem 13.1 characterizes the variance of the OLS coefficient, but it does not characterize its bias. In general, a more complex model is closer to the true mean function f (x i ), and can then reduce the bias of approximating the mean function. However, Theorem 13.1 implies that a more complex model results in larger variances of the OLS coefficients. So we face a bias-variance trade-off. Consider a simple case where the true data-generating process is linear:

$y i = β 1 + β 2 x i1 + • • • + β s-1 x is + ε i . (13.3)$Ideally, we want to use the model (13.3) with exactly s covariates. In practice, we may not know which covariates to include in the OLS. If we underfit the data using a short regression with q < s:

$y i = β1 + β2 x i1 + • • • + βq-1 x iq + εi , (i = 1, . . . , n) (13.4)$then the OLS coefficients are biased. If we increase the complexity of the model to overfit the data using a long regression with p > s:

$y i = β1 + β2 x i1 + • • • + βp-1 x ip + εi , (i = 1, . . . , n) (13.5)$then the OLS coefficients are unbiased. Theorem 13.1, however, shows that the OLS coefficients from the under-fitted model (13.4) have smaller variances than those from the overfitted model (13.5).

Example 13.1 In general, we have a sequence of models with increasing complexity. For simplicity, we consider nested models containing 1 n and covariates

${X 1 } ⊂ {X 1 , X 2 } ⊂ • • • ⊂ {X 1 , . . . , X p }$in the following simulation setting. The true linear model is y i = x t i β + N(0, 1) with p = 40 but only the first 10 covariates have non-zero coefficients 1 and all other covariates have coefficients 0. We generate two datasets: both have sample size n = 200, all covariates have IID N(0, 1) entries, and the error terms are IID. We use the first dataset to fit the OLS and thus call it the training dataset. We use the second dataset to assess the performance of the fitted OLS from the training dataset, and thus call it the testing dataset[foot_10](#foot_10) . Figure [13](#fig_95).2 plots the residual sum of squares against the number of covariates in the training and testing datasets. By definition of OLS, the residual sum of squares decreases with the number of covariates in the training dataset, but it first decreases and then increases in the testing dataset with minimum value attained at 10, the number of covariates in the true data generating process.

The following example has a nonlinear true mean function but still uses OLS with polynomials of covariates to approximate the truth[foot_11](#foot_11) .

Example 13.2 The true nonlinear model is y i = sin(2πx i ) + N(0, 1) with the x i 's equally spaced in [0, 1] and the error terms are IID. The training and testing datasets both have sample sizes n = 200. Figure [13](#fig_95).3 plots the residual sum of squares against the order of the polynomial in the OLS fit

$y i = p-1 j=0 β j x j i + ε i .$By the definition of OLS, the residual sum of squares decreases with the order of polynomials in the training dataset, but it achieves the minimum near p = 5 in the testing dataset. We can show that the residual sum of squares decreases to zero with p = n in the training dataset; see Problem 13.7. However, it is larger than that under p = 5 in the testing dataset.

The R code for Figures 13.2 and 13.3 is in code13.3.R.

## Model selection criteria

With a large number of covariates X 1 , . . . , X p , we want to select a model that has the best performance for prediction. In total, we have 2 p possible models. Which one is the best? What is the criterion for the best model? Practitioners often use the linear model for multiple purposes. A dominant criterion is the prediction performance of the linear model in a new dataset [(Yu and Kumbier, 2020)](#b245). However, we do not have the new dataset yet in the statistical modeling stage. So we need to find criteria that are good proxies for the prediction performance.

13.4.1 RSS, R 2 and adjusted R 2

The first obvious criterion is the rss, which, however, is not a good criterion because it favors the largest model. The sample R 2 has the same problem of favoring the largest model. Most model selection criteria are in some sense modifications of rss or R 2 . The adjusted R 2 takes into account the complexity of the model:

$R2 = 1 - n -1 n -p (1 -R 2 ) = 1 - n i=1 (y i -ŷi ) 2 /(n -p) n i=1 (y i -ȳ) 2 /(n -1) = 1 - σ2 σ2 y .$So based on R2 , the best model has the smallest σ2 , the estimator for the variance of the error term in the Gauss-Markov model. The following theorem shows that R2 is closely related to the F statistic in testing two nested Normal linear models.

Theorem 13.2 Consider the setting of Chapter 8.3. Test two nested Normal linear models:

$Y = X 1 β 1 + ε versus Y = X 1 β 1 + X 2 β 2 + ε,$or, equivalently, test β 2 = 0. We can use the standard F statistic defined in Chapter 8.3, and we can also compare the adjusted R 2 's from these two models: R2 1 and R2 2 . They are related via

$F > 1 ⇐⇒ R2 1 < R2 2 .$I leave the proof of the theorem as Problem 13.3. From Theorem 13.2, R2 does not necessarily favor the largest model. However, R2 still favors unnecessarily large models compared with the usual hypothesis testing based on the Normal linear model because the mean of F is approximately 1, but the upper quantile of F is much larger than 1 (for example, the 95% quantile of F 1,n-p is larger than 3.8, and the 95% quantile of F 2,n-p is larger than 2.9).

## Information criteria

Taking into account the model complexity, we can find the model with the smallest aic or bic, defined as aic = n log rss n + 2p and bic = n log rss n + p log n, with full names "Akaike's information criterion " and "Bayes information criterion." aic and bic are both monotone functions of the rss penalized by the number of parameters p in the model. The penalty in bic is larger so it favors smaller models than aic. Shao (1997)'s results suggested that bic can consistently select the true model if the linear model is correctly specified, but aic can select the model that minimizes the prediction error if the linear model is misspecified. In most statistical practice, the linear model assumption cannot be justified, so we recommend using aic.

## Cross-validation (CV)

The first choice is the leave-one-out cross-validation based on the predicted residual:

$press = n i=1 ε2 [-i] = n i=1 ε2 i (1 -h ii ) 2 (13.6)$which is called the predicted residual error sum of squares (PRESS) statistic.

Because the average value of h ii is n -1 n i=1 h ii = p/n, we can approximate PRESS by the generalized cross-validation (GCV) criterion:

$gcv = n i=1 ε2 i (1 -p/n) 2 = rss × 1 - p n -2 . When p/n ≈ 0, we have 3 log gcv = log rss -2 log 1 - p n ≈ log rss + 2p n = aic/n + log n,$so GCV is approximately equivalent to AIC with small p/n. With large p/n, they may have large differences. GCV is not crucial for OLS because it is easy to compute PRESS. However, it is much more useful in other models where we need to fit the data n times to compute PRESS. For a general model without simple leave-one-out formulas, it is computationally intensive to obtain PRESS. The K-fold cross-validation (K-CV) is computationally more attractive. The best model has the smallest K-CV, computed as follows:

1. randomly shuffle the observations; 2. split the data into K folds; 3. for each fold k, use all other folds as the training data, and compute the predicted errors on fold k (k = 1, . . . , K);

4. aggregate the prediction errors across K folds, denoted by K-CV.

When K = 3, we split the data into 3 folds. Run OLS to obtain a fitted function with folds 2, 3 and use it to predict on fold 1, yielding prediction error r 1 ; run OLS with folds 1, 3 and predict on fold 2, yielding prediction error r 2 ; run OLS with folds 1, 2 and predict on fold 2, yielding prediction error r 3 . The total prediction error is r = r 1 + r 2 + r 3 . We want to select a model that minimizes r. Usually, practitioners choose K = 5 or 10, but this can depend on the computational resource.

## Best subset and forward/backward selection

Given a model selection criterion, we can select the best model.

For a small p, we can enumerate all 2 p models. The function regsubsets in the R package leaps implements this[foot_13](#foot_13) . Figure [13](#fig_95).4 shows the results of the best subset selection in two applications, with the code in code13.4.4.R.

For large p, we can use forward or backward regressions. Forward regression starts with a model with only the intercept. In step one, it finds the best covariate among the p candidates based on the prespecified criterion. In step two, it keeps this covariate in the model and finds the next best covariate among the remaining p -1 candidates. It proceeds by adding the next best covariate one by one.

The backward regression does the opposite. It starts with the full model with all p covariates. In step one, it drops the worst covariate among the p candidates based on the prespecified criterion. In step two, it drops the next worst covariate among the remaining p -1 candidates. It proceeds by dropping the next worst covariate one by one.

Both methods generate a sequence of models, and select the best one based on the prespecified criterion. Forward regression works in the case with p ≥ n but it stops at step n -1; backward regression works only in the case with p < n. The functions step or stepAIC in the MASS package implement these.

## Homework problems

## Inflation and deflation of the estimated variance

This problem extends Theorem 13.1 to the estimated variance.

The covariate matrix X has columns 1 n , X 1 , . . . , X p . Compare the coefficient of X 1 in the following long and short regressions:

$Y = β0 1 n + β1 X 1 + • • • + βp X p + ε, and Y = β0 1 n + β1 X 1 + • • • + βq X q + ε,$where q < p. Under the condition in Theorem 13.1,

$var( β1 ) var( β1 ) = 1 -R 2 X1.X2•••Xq 1 -R 2 X1.X2•••Xp ≥ 1, recalling that R 2 U.$V denotes the R 2 of U on V . Now we compare the corresponding estimated variances v ar( β1 ) and ṽ ar( β1 ) based on homoskedasticity.

$1. Show that v ar( β1 ) ṽ ar( β1 ) = 1 -R 2 Y.X1•••Xp 1 -R 2 Y.X1•••Xq × 1 -R 2 X1.X2•••Xq 1 -R 2 X1.X2•••Xp × n -q -1 n -p -1 .$2. Using the definition of the partial R 2 in Problem 10.4, show that v ar( β1 ) ṽ ar

$( β1 ) = 1 -R 2 Y.Xq+1•••Xp|X1•••Xq 1 -R 2 X1.Xq+1•••Xp|X2...Xq × n -q -1 n -p -1 .$Remark: The first result shows that the ratio of the estimated variances has three factors: the first one corresponds to the R 2 's of the outcome on the covariates, the second one is identical to the one for the ratio of the true variances, and the third one corresponds to the degrees of freedom correction. The first factor deflates the estimated variance since the R 2 increases with more covariates included in the regression, and the second and the third factors inflate the estimated variance. Overall, whether adding more covariates inflate or deflate the estimated variance depends on the interplay of the three factors. The answer is not definite as Theorem 13.1.

The variance inflation result in Theorem 13.1 sometimes causes confusion. It only concerns the variance. When we view some covariates as random, then the bias term can also contribute to the variance of the OLS estimator. In this case, we should interpret Theorem 13.1 with caution. See Ding (2021b) for a related discussion.

## Inflation and deflation of the variance under heteroskedasticity

Relax the condition in Theorem 13.1 as var(ε i ) = σ 2 i with possibly different variances. Give a counterexample in which the variance decreases with more covariates included in the regression.

## Equivalence of F and R2

Prove Theorem 13.2. (1 -h ii ) -1 is unbiased for σ 2 under the Gauss-Markov model in Assumption 4.1, recalling press in (13.6) and the leverage score h ii of unit i.

Remark: Theorem 4.3 shows that σ2 = rss/(n -p) is unbiased for σ 2 under the Gauss-Markov model. rss is the "in-sample" residual sum of squares, whereas press is the "leaveone-out" residual sum of squares. The estimator σ2 is standard, whereas σ2 press appeared in [Shen et al. (2023)](#b209).

## Simulation with misspecified linear models

Replicate the simulation in Example 13.1 with correlated covariates and an outcome model with quadratic terms of covariates.

## Best subset selection in lalonde data

Produce the figure similar to the ones in Figure [13](#fig_95).4 based on the lalonde data in the Matching package. Report the selected model based on aic, bic, press, and gcv.

## Perfect polynomial

Prove that given distinct x i (i = 1, . . . , n) within [0, 1] and any y i (i = 1, . . . , n), we can always find an nth order polynomial

$p n (x) = n-1 j=0 b j x j such that p n (x i ) = y i , (i = 1, . . . , n).$Hint: Use the formula in (A.4). The first column corresponds to the full model without testing, and the second column corresponds to the selected model with testing.

q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0 10 20 30 40 q training data testing data FIGURE 13.2: Training and testing errors: linear mean function q q q q q q q q q q q q q q q q q q q q q 5 10 15 20 0.9 

## Ridge Regression

## Introduction to ridge regression

The OLS estimator has many nice properties. For example, Chapter 4 shows that it is BLUE under the Gauss-Markov model, and Chapter 5 shows that it follows a Normal distribution that allows for finite-sample exact inference under the Normal linear model. However, it can have the following problems that motivate ridge regression in this chapter.

The first motivation is quite straightforward. From the formula

$β = (X t X) -1 X t Y,$if the columns of X are highly correlated, then X t X will be nearly singular; more extremely, if the number of covariates is larger than the sample size, then X t X has a rank smaller than or equal to n and thus is not invertible. So numerically, the OLS estimator can be unstable due to inverting X t X. Because X t X must be positive semi-definite, its smallest eigenvalue determines whether it is invertible or not. [Hoerl and Kennard (1970)](#b141) proposed the following ridge estimator as a modification of OLS:

$βridge (λ) = (X t X + λI p ) -1 X t Y, (λ > 0) (14.1)$which involves a positive tuning parameter λ. Because the smallest eigenvalue of X t X +λI p is larger than or equal to λ > 0, the ridge estimator is always well defined. Now I turn to the second equivalent motivation. The OLS estimator minimizes the residual sum of squares

$rss(b 0 , b 1 , . . . , b p ) = n i=1 (y i -b 0 -b 1 x i1 -• • • -b p x ip ) 2 .$From Theorem 13.1 on the variance inflation factor, the variances of the OLS estimators increase with additional covariates included in the regression, leading to unnecessarily large estimators by chance. To avoid large OLS coefficients, we can penalize the residual sum of squares criterion with the squared length of the coefficients[foot_14](#foot_14) and use βridge (λ) = arg min b0,b1,...,bp

$   rss(b 0 , b 1 , . . . , b p ) + λ p j=1 b 2 j    . (14.2)$Again in (14.2), λ is a tuning parameter that ranges from zero to infinity. We first discuss the ridge estimator with a fixed λ and then discuss how to choose it. When λ = 0, it reduces to OLS; when λ = ∞, all coefficients must be zero except that βridge

$0 (∞) = ȳ.$With λ ∈ (0, ∞), the ridge coefficients are generally smaller than the OLS coefficients, and the penalty shrinks the OLS coefficients toward zero. So the parameter λ controls the magnitudes of the coefficients or the "complexity" of the model. In (14.2), we only penalize the slope parameters not the intercept.

As a dual problem in optimization, we can also define the ridge estimator as Definitions (14.2) and ( [14](#).3) are equivalent because for a given λ, we can always find a t such that the solutions from (14.2) and ( [14](#).3) are identical. In fact, the corresponding t and

$λ satisfy t = ∥ βridge (λ)∥ 2 .$However, the ridge estimator has an obvious problem: it is not invariant to linear transformations of X. In particular, it is not equivalent under different scaling of the covariates. Intuitively, the b j 's depend on the scale of X j 's, but the penalty term p j=1 b[foot_15](#foot_15) j puts equal weight on each coefficient. A convention in practice is to standardize the covariates before applying the ridge estimator 2 . Condition 14.1 (standardization) The covariates satisfy

$n -1 n i=1 x ij = 0, n -1 n i=1 x 2 ij = 1, (j = 1, . . . , p)$and the outcome satisfy ȳ = 0.

With all covariates centered at zero, the ridge estimator for the intercept, given any values of the slopes and the tuning parameter λ, equals βridge 0 = ȳ. So if we center the outcomes at mean zero, then we can drop the intercept in the ridge estimators defined in (14.2) and (14.3).

For descriptive simplicity, we will assume Condition 14.1 and call it standardization from now on. This allows us to drop the intercept. Using the matrix form, the ridge estimator minimizes

$(Y -Xb) t (Y -Xb) + λb t b,$which is a quadratic function of b. From the first order condition, we have

$-2X t Y -X βridge (λ) + 2λ βridge (λ) = 0 =⇒ βridge (λ) = (X t X + λI p ) -1 X t Y,$which coincides with the definition in (14.1). We also have the second-order condition

$2X t X + 2λI p ≻ 0, (λ > 0)$which verifies that the ridge estimator is indeed the minimizer. The predicted vector is

$Ŷ ridge (λ) = X βridge (λ) = X(X t X + λI p ) -1 X t Y = H(λ)Y,$where H(λ) = X(X t X + λI p ) -1 X t is the hat matrix for ridge regression. When λ = 0, it reduces to the hat matrix for the OLS; when λ > 0, it is not a projection matrix because {H(λ)} 2 ̸ = H(λ).

14.2 Ridge regression via the SVD of X I will focus on the case with n ≥ p and relegate the discussion of the case with n ≤ p to Problem 14.11. To facilitate the presentation, I will use the SVD decomposition of the covariate matrix:

$X = U DV t$where U ∈ R n×p has orthonormal columns such that U t U = I p , V ∈ R p×p is an orthogonal matrix with V V t = V t V = I p , and D ∈ R p×p is a diagonal matrix consisting of the singular values. Figure [14](#fig_95).1 illustrates this important decomposition.

$orthonormal columns diagonal orthogonal FIGURE 14.1: SVD of X$The SVD of X implies the eigen-decomposition of X t X:

$X t X = V D 2 V t$with eigen-vectors V j being the column vectors of V and eigen-values d 2 j being the squared singular values. The following lemma is crucial for simplifying the theory and computation.

Lemma 14.1 The ridge coefficient equals

$βridge (λ) = V diag d j d 2 j + λ U t Y,$where the diagonal matrix is p × p.

Proof of Lemma 14.1: The ridge coefficient equals

$βridge (λ) = (X t X + λI p ) -1 X t Y = (V DU t U DV t + λI p ) -1 V DU t Y = V (D 2 + λI p ) -1 V t V DU t Y = V (D 2 + λI p ) -1 DU t Y = V diag d j d 2 j + λ U t Y. □$
## Statistical properties

The Gauss-Markov theorem shows that the OLS estimator is BLUE under the Gauss-Markov model: Y = Xβ + ε, where ε has mean zero and covariance σ 2 I n . Then in what sense, can ridge regression improve OLS? I will discuss the statistical properties of the ridge estimator under the Gauss-Markov model.

Based on Lemma 14.1, we can calculate the mean of the ridge estimator:

$E{ βridge (λ)} = V diag d j d 2 j + λ U t Xβ = V diag d j d 2 j + λ U t U DV t β = V diag d 2 j d 2 j + λ V t β$which does not equal β in general. So the ridge estimator is biased. We can also calculate the covariance matrix of the ridge estimator:

$cov{ βridge (λ)} = σ 2 V diag d j d 2 j + λ U t U diag d j d 2 j + λ V t = σ 2 V diag d 2 j (d 2 j + λ) 2 V t .$The mean squared error (MSE) is a measure capturing the bias-variance trade-off:

$mse(λ) = E βridge (λ) -β t βridge (λ) -β .$Using Theorem B.8 on the expectation of a quadratic form, we have

$mse(λ) = [E{ βridge (λ)} -β] t [E{ βridge (λ)} -β] C1 + trace[cov{ βridge (λ)}] C2 .$The following theorem simplifies C 1 and C 2 .

Theorem 14.1 Under Assumption 4.1, the ridge estimator satisfies [(14.4)](#) where γ = V t β = (γ 1 , . . . , γ p ) t has the jth coordinate γ j = V t j β, and

$C 1 = λ 2 p j=1 γ 2 j (d 2 j + λ) 2 ,$$C 2 = σ 2 p j=1 d 2 j (d 2 j + λ) 2 .$(14.5)

Proof of Theorem 14.1: First, we have

$C 1 = β t V diag d 2 j d 2 j + λ V t -I p 2 β = β t V diag λ 2 (d 2 j + λ) 2 V t β = γ t diag λ 2 (d 2 j + λ) 2 γ = λ 2 p j=1 γ 2 j (d 2 j + λ) 2 .$Second, we have

$C 2 = σ 2 trace V diag d 2 j (d 2 j + λ) 2 V t = σ 2 trace diag d 2 j (d 2 j + λ) 2 = σ 2 p j=1 d 2 j (d 2 j + λ) 2 .$□ Theorem 14.1 shows the bias-variance trade-off for the ridge estimator. The MSE is

$mse(λ) = C 1 + C 2 = λ 2 p j=1 γ 2 j (d 2 j + λ) 2 + σ 2 p j=1 d 2 j (d 2 j + λ) 2 .$When λ = 0, the ridge estimator reduces to the OLS estimator: the bias is zero and the variance σ 2 p j=1 d -2 j dominates. When λ = ∞, the ridge estimator reduces to zero: the bias p j=1 γ 2 j dominates and the variance is zero. As we increase λ from zero, the bias increases and the variance decreases. So we face a bias-variance trade-off.

## Selection of the tuning parameter

## Based on parameter estimation

For parameter estimation, we want to choose the λ that minimizes the MSE. So the optimal λ must satisfy the following first-order condition:

$∂mse(λ) ∂λ = 2 p j=1 γ 2 j λ d 2 j + λ d 2 j + λ -λ (d 2 j + λ) 2 -2σ 2 p j=1 d 2 j (d 2 j + λ) 3 = 0 which is equivalent to λ p j=1 γ 2 j d 2 j (d 2 j + λ) 3 = σ 2 p j=1 d 2 j (d 2 j + λ) 3 .$(14.6) However, (14.6) is not directly useful because we do not know γ and σ 2 . Three methods below try to solve (14.6) approximately. [Dempster et al. (1977)](#b90) used OLS to construct an unbiased estimator σ2 and γ = V t β, and then solve λ from [Hoerl et al. (1975)](#b140) assumed that X t X = I p . Then d 2 j = 1 (j = 1, . . . , p) and γ = β, and solve λ from

$λ p j=1 γ2 j d 2 j (d 2 j + λ) 3 = σ2 p j=1 d 2 j (d 2 j + λ) 3 , which is a nonlinear equation of λ.$$λ p j=1 β2 j (1 + λ) 3 = σ2 p j=1 1 (1 + λ) 3 , resulting in λ hkb = pσ 2 /∥ β∥ 2 .$Lawless (1976) used

$λ lw = pσ 2 / βt D 2 β$to weight the β j 's based on the eigenvalues of X t X. But all these methods require estimating (β, σ 2 ). If the initial OLS estimator is not reliable, then these estimates of λ are unlikely to be reliable. None of these methods work for the case with p > n.

## Based on prediction

For prediction, we need slightly different criteria. Without estimating (β, σ 2 ), we can use the leave-one-out cross-validation. The leave-one-out formula for the ridge below is similar to that for OLS. Theorem 14.2 Define β(λ) = (X t X + λI p ) -1 X t Y as the ridge coefficient (dropping the superscript "ridge" for simplicity), ε(λ) = Y -X β(λ) as the residual vector using the full data, and h ii (λ) = x t i (X t X + λI p ) -1 x i as the (i, i)th diagonal element of H(λ) = X(X t X + λI p ) -1 X t . Define β[-i] (λ) as the ridge coefficient without observation i, and

$ε[-i] (λ) = y i -x t i β[-i] (λ)$as the predicted residual. The leave-one-out formulas for ridge regression are

$β[-i] (λ) = β(λ) -{1 -h ii (λ)} -1 (X t X + λI p ) -1 x i εi (λ) and ε[-i] (λ) = εi (λ)/{1 -h ii (λ)}.$I leave the proof of Theorem 14.2 as Problem 14.5. By Theorem 14.2, the PRESS statistic for ridge is [Golub et al. (1979)](#b126) proposed the GCV criterion to simplify the calculation of the PRESS statistic by replacing h ii (λ) with their average value n -1 trace{H(λ)}:

$press(λ) = n i=1 ε[-i] (λ) 2 = n i=1 {ε i (λ)} 2 {1 -h ii (λ)} 2 .$$gcv(λ) = n i=1 {ε i (λ)} 2 [1 -n -1 trace {H(λ)}] 2 .$In the R package MASS, the function lm.ridge implements the ridge regression, kHKB and kLW report two estimators for λ, and GCV contains the GCV values for a sequence of λ.

## Computation of ridge regression

Lemma 14.1 gives the ridge coefficients. So the predicted vector equals

$Ŷ (λ) = X βridge (λ) = U DV t V diag d j d 2 j + λ U t Y = U Ddiag d j d 2 j + λ U t Y = U diag d 2 j d 2 j + λ U t Y,$and the hat matrix equals

$H(λ) = U diag d 2 j d 2 j + λ U t .$These formulas allow us to compute the ridge coefficient and predictor vector for many values of λ without inverting each X t X + λI p . We have similar formulas for the case with n < p; see Problem 14.11.

A subtle point is due to the standardization of the covariates of the outcome. In R, the lm.ridge function first computes the ridge coefficient based on the standardized covariates and outcome, and then transforms them back to the original scale. Let x1 , . . . , xp , ȳ be the means of the covariates and outcome, and let sd j = {n -1 n i=1 (x ij -xi ) 2 } 1/2 be the standard deviation of the covariates which are report as scales in the output of lm.ridge. From the ridge coefficients { βridge 

$(λ)(x i1 -x1 )/sd 1 + • • • + βridge p (λ)(x ip -xp )/sd p or, equivalently, ŷi (λ) = αridge (λ) + βridge 1 (λ)/sd 1 × x i1 + • • • + βridge p (λ)/sd p × x ip where αridge (λ) = ȳ -βridge 1 (λ)x 1 /sd 1 -• • • -βridge p (λ)x p /sd p .$
## Numerical examples

We can use the following numerical example to illustrate the bias-variance trade-off in selecting λ in the ridge. The code is in code14.5.R.

## Uncorrelated covariates

I first simulate data from a Normal linear model with uncorrelated covariates.

library ( MASS ) n = 2 0 0 p = 1 0 0 beta = rep ( 1 / sqrt ( p ) , p ) sig = 1 / 2 X = matrix ( rnorm ( n * p ) , n , p ) X = scale ( X ) X = X * sqrt ( n /( n -1 )) Y = as . vector ( X %*% beta + rnorm (n , 0 , sig ))

The following code calculates the theoretical bias, variance, and mean squared error, reported in the (1, 1)th panel of Figure [14](#fig_95).2. eigenxx = eigen ( t ( X )%*% X ) xis = eigenxx $ values gammas = t ( eigenxx $ vectors )%*% beta lambda . seq = seq ( 0 , 7 0 , 0 . 0 1 ) bias 2 . seq = lambda . seq var . seq = lambda . seq mse . seq = lambda . seq for ( i in 1 : length ( lambda . seq )) { ll = lambda . seq [ i ] bias 2 . seq [ i ] = ll ^2 * sum ( gammas ^2 /( xis + ll )^2 ) var . seq [ i ] = sig ^2 * sum ( xis /( xis + ll )^2 ) mse . seq [ i ] = bias 2 . seq [ i ] + var . seq [ i ] }

y . min = min ( bias 2 . seq , var . seq , mse . seq ) y . max = max ( bias 2 . seq , var . seq , mse . seq ) par ( mfrow = c ( 2 , 2 )) plot ( bias 2 . seq ~lambda . seq , type = " l " , ylim = c ( y . min , y . max ) , xlab = expression ( lambda ) , main = " " , ylab = " bias -variance ␣ tradeoff " , lty = 2 , bty = " n " ) lines ( var . seq ~lambda . seq , lty = 3 ) lines ( mse . seq ~lambda . seq , lwd = 3 , lty = 1 ) abline ( v = lambda . seq [ which . min ( mse . seq )] , lty = 1 , col = " grey " ) legend ( " topright " , c ( " bias " , " variance " , " mse " ) , lty = c ( 2 , 3 , 1 ) , lwd = c ( 1 , 1 , 4 ) , bty = " n " )

The (1, 1)th panel also reported the λ's based on different approaches.

ridge . fit = lm . ridge ( Y ~X , lambda = lambda . seq ) abline ( v = lambda . seq [ which . min ( ridge . fit $ GCV )] , lty = 2 , col = " grey " ) abline ( v = ridge . fit $ kHKB , lty = 3 , col = " grey " ) abline ( v = ridge . fit $ kLW , lty = 4 , col = " grey " ) legend ( " bottomright " , c ( " MSE " , " GCV " , " HKB " , " LW " ) , lty = 1 : 4 , col = " grey " , bty = " n " )

I also calculate the prediction error of the ridge estimator in the testing dataset, which follows the same data-generating process as the training dataset. The (1, 2)th panel of Figure [14](#fig_95).2 shows its relationship with λ. Overall, GCV, HKB, and LW are similar, but the λ selected by the MSE criterion is the worst for prediction.

X . new = matrix ( rnorm ( n * p ) , n , p ) X . new = scale ( X . new ) X . new = X . new * matrix ( sqrt ( n /( n -1 )) , n , p ) Y . new = as . vector ( X . new %*% beta + rnorm (n , 0 , sig )) predict . error = Y . new -X . new %*% ridge . fit $ coef predict . mse = apply ( predict . error ^2 , 2 , mean ) plot ( predict . mse ~lambda . seq , type = " l " , xlab = expression ( lambda ) , ylab = " predicted ␣ MSE " , bty = " n " ) abline ( v = lambda . seq [ which . min ( mse . seq )] , lty = 1 , col = " grey " ) abline ( v = lambda . seq [ which . min ( ridge . fit $ GCV )] , lty = 2 , col = " grey " ) abline ( v = ridge . fit $ kHKB , lty = 3 , col = " grey " ) abline ( v = ridge . fit $ kLW , lty = 4 , col = " grey " ) legend ( " bottomright " , c ( " MSE " , " GCV " , " HKB " , " LW " ) , lty = 1 : 4 , col = " grey " , bty = " n " ) mtext ( " independent ␣ covariates " , side = 1 , line = -5 8 , outer = TRUE , font . main = 1 , cex = 1 . 5 )

## Correlated covariates

I then simulate data from a Normal linear model with correlated covariates.

n = 2 0 0 p = 1 0 0 beta = rep ( 1 / sqrt ( p ) , p ) sig = 1 / 2 # # correlated Normals X = matrix ( rnorm ( n * p ) , n , p ) + rnorm (n , 0 , 0 . 5 ) # # standardize the covariates X = scale ( X ) X = X * matrix ( sqrt ( n /( n -1 )) , n , p ) Y = as . vector ( X %*% beta + rnorm (n , 0 , sig ))

The second row of Figure [14](#fig_95).2 shows the bias-variance trade-off. Overall, GCV works the best for selecting λ for prediction.

## Further commments on OLS, ridge, and PCA

The SVD of X is closely related to the principal component analysis (PCA), so is OLS and ridge regression. Assume that the columns of X are centered, so X t X = V D 2 V t is proportional to the sample covariance matrix of X. Assume d 1 ≥ d 2 ≥ • • • . PCA tries to find linear combinations of the covariate x i that contain maximal information. For a vector v ∈ R p , the linear combination v t x i has sample variance proportional to

$Q(v) = v t X t Xv.$If we multiply v by a constant c, the above sample variance will change by the factor c 2 . So a meaningful criterion is to maximize Q(v) such that ∥v∥ = 1. This is exactly the setting of Theorem A.3. The maximum value equals d 2 1 which is achieved by V 1 , the first column of V . We call

$XV 1 =    x t 1 V 1 . . . x t n V 1   $the first principal component of X. Similar to Theorem A.3, we can further maximize Q(v) such that ∥v∥ = 1 and v ⊥ V 1 , yielding the maximum value d 2 2 which is achieved by V 2 . We call XV 2 the second principal component of X. By induction, we can define all the p principal components, stacked in the following n × p matrix:

$(XV 1 , . . . , XV p ) = XV = U DV t V = U D.$So U D in the SVD decomposition contains the principal components of X. Since D is a diagonal matrix that only changes the scales of the columns of U , we also call U = (U 1 , . . . , U p ) the principal components of X. They are orthogonal since U t U = I p . Section 14.5 shows that the ridge estimator yields the predicted value

$Ŷ (λ) = U diag d 2 j d 2 j + λ U t Y = p j=1 d 2 j d 2 j + λ ⟨U j , Y ⟩U j$where ⟨U j , Y ⟩ = U t j Y denotes the inner product of vectors U j and Y. As a special case with λ = 0, the OLS estimator yields the predicted value

$Ŷ = U U t Y = p j=1 ⟨U j , Y ⟩U j ,$which is identical to the predicted value based on OLS of Y on the principal components U . Moreover, the principal components in U are orthogonal and have unit length, so the OLS fit of Y on U is equivalent to the component-wise OLS of Y on U j with coefficient ⟨U j , Y ⟩ (j = 1, . . . , p). So the predicted value based OLS equals a linear combination of the principal components with coefficients ⟨U j , Y ⟩; the predicted value based on ridge also equals a linear combination of the principal components but the coefficients are shrunk by the factors d 2 j /(d 2 j + λ). When the columns of X are not linearly independent, for example, p > n, we cannot run OLS of Y on X or OLS of Y on U , but we can still run ridge. Motivated by the formulas above, another approach is to run OLS of Y on the first p * principal components Ũ = (U 1 , . . . , U p * ) with p * < p. This is called the principal component regression (PCR). The predicted value is

$Ŷ (p * ) = ( Ũ t Ũ ) -1 Ũ t Y = p * j=1 ⟨U j , Y ⟩U j ,$which truncates the summation in the formula of Ŷ based on OLS. Compared to the predicted values of OLS and ridge, Ŷ (p * ) effectively imposes zero weights on the principal components corresponding to small singular values. It depends on a tuning parameter p * similar to λ in the ridge. Since p * must be a positive integer and λ can be any positive real value, PCR is a discrete procedure while ridge is a continuous procedure.

## Homework problems

14.1 Ridge coefficient as a posterior mode under a Normal prior Assume fixed X, σ 2 and τ 2 . Show that if Remark: This result ensures that the ridge estimator must have a smaller MSE than OLS in a neighborhood of λ = 0, which is coherent with the pattern in Figure [14](#fig_95).2.

$Y | β ∼ N(Xβ, σ 2 I n ), β ∼ N(0, τ 2 I p ),$
## Ridge and OLS

Show that if X has linearly independent columns, then

$βridge (λ) = (X t X + λI p ) -1 X t X β = V diag d 2 j d 2 j + λ V t β$where β is the OLS coefficient.

## Ridge as OLS with augmented data

Show that βridge (λ) equals the OLS coefficient of Ỹ on X with augmented data

$Ỹ = Y 0 p , X = X √ λI p ,$where Ỹ is an n + p dimensional vector and X is an (n + p) × p matrix. Remark: The columns of X must be linearly independent, so the inverse of Xt X always exists. This is a theoretical result of the ridge regression. It should not be used for computation especially when p is large.

## Leave-one-out formulas for ridge

Prove Theorem 14.2.

Hint: You can use the result in Problem 14.4 and apply the leave-one-out formulas for OLS in Theorems 11.2 and 11.3.

## Generalized ridge regression

Covariates have different importance, so it is reasonable to use different weights in the penalty term. Find the explicit formula for the ridge regression with general quadratic penalty:

arg min b∈R p {(Y -Xb) t (Y -Xb) + λb t Qb} where Q is a p × p positive definite matrix.

## Degrees of freedom of ridge regression

For a predictor Ŷ for Y , define the degrees of freedom of the predictor as

$n i=1 cov(y i , ŷi )/σ 2 .$Calculate the degrees of freedom of ridge regression in terms of the eigenvalues of X t X.

14.8 Extending the simulation in Figure [14](#fig_95).2

Re-run the simulation that generates Figure [14](#fig_95).2, and report the λ selected by [Dempster et al. (1977)](#b90)'s method, PRESS, and K-fold CV. Extend the simulation to the case with p > n.

## Unification of OLS, ridge, and PCR

We can unify the predicted values of the OLS, ridge, and PCR as

$Ŷ = p j=1 s j ⟨U j , Y ⟩U j ,$where

$s j =        1, OLS, d 2 j d 2 j +λ , ridge, 1(j ≤ p * ), PCR.$Based on the unified formula, show that under Assumption 4.1, we have

$E( Ŷ ) = p j=1 s j d j γ j U j$with the γ j 's defined in Theorem 14.1, and

$cov( Ŷ ) = σ 2 p j=1 s 2 j U j U t j .$
## An equivalent form of ridge coefficient

Show that the ridge coefficient has two equivalent forms: for λ > 0,

$(X t X + λI p ) -1 X t Y = X t (XX t + λI n ) -1 Y.$Remark: This formula has several interesting implications. First, the left-hand side involves inverting a p × p matrix, and it is more useful when p < n; the right-hand side involves inverting an n × n matrix, so it is more useful when p > n. Second, from the form on the right-hand side, we can see that the ridge estimator lies in C(X t ), the row space of X. That is, the ridge estimator can be written as X t δ, where δ

$= (XX t + λI n ) -1 Y ∈ R p .$This always holds but is particularly interesting in the case with p > n when the row space of X is not the entire R p . Third, if p > n and XX t is invertible, then we can let λ go to zero on the right-hand side, yielding βridge (0) = X t (XX t ) -1 Y which is the minimum norm estimator; see Problem 18.7. Using the definition of the pseudoinverse in Chapter A, we can further show that βridge (0) = X + Y.

## Computation of ridge with n < p

When n < p, X has singular value decomposition X = U DV t , where D ∈ R n×n is a diagonal matrix containing the singular values, U ∈ R n×n is an orthogonal matrix with U U t = U t U = I n , and V ∈ R p×n has orthonormal columns with V t V = I n . Show that the ridge coefficient, the predicted value, and the hat matrix have the same form as the case with n > p. The only subtle difference is that the diagonal matrices have dimension n × n.

Remark: The above result also ensures that Theorem 14.1 holds when p > n if we modify the summation as from j = 1 to n.

## Recommended reading

To celebrate the 50th anniversary of [Hoerl and Kennard (1970)](#b141)'s paper in Technometrics, the editor invited Roger W. Hoerl, the son of Art Hoerl, to review the historical aspects of the original paper, and Trevor Hastie to review the essential role of the idea of ridge regression in data science. See [Hoerl (2020)](#b142) and [Hastie (2020)](#b134).

## Lasso

## Introduction to the lasso

Ridge regression works well for prediction, but it may be difficult to interpret many small but non-zero coefficients. [Tibshirani (1996)](#b217) proposed to use the lasso, the acronym for the Least Absolute Shrinkage and Selection Operator, to achieve the ambitious goal of simultaneously estimating parameters and selecting important variables in the linear regression. By changing the penalty term in the ridge regression, the lasso automatically estimates some parameters as zero, dropping them out of the model and thus selecting the remaining variables as important predictors. [Tibshirani (1996)](#b217) defined the lasso as βlasso (t) = arg min The two forms of lasso are equivalent in the sense that for a given λ in (15.2), there exists a t such that the solution for (15.1) is identical to the solution for [(15.2)](#). In particular, t = p j=1 βlasso j (λ). Technically, the minimizer of the lasso problem may not be unique especially when p > n, so the right-hand sides of the optimization problems should be a set. Fortunately, even though the minimizer may not be unique, the resulting predictor is always unique. [Tibshirani (2013)](#b219) clarifies this issue.

Both forms of the lasso are useful. We will use the form (15.2) for computation and use the form (15.1) for geometric intuition. Similar to the ridge estimator, the lasso is not invariant to the linear transformation of X. We proceed after standardizing the covariates and outcome as Condition 14.1. For the same reason as the ridge, we can drop the intercept after standardization.

## Comparing the lasso and ridge: a geometric perspective

The ridge and lasso are very similar: both minimize a penalized version of the residual sum of squares. They differ in the penalty term: ridge uses an L 2 penalty, i.e., the L 2 norm of the contour plot [FIGURE 15](#).1: Lasso with a sparse solution coefficient ∥b∥ 2 = p j=1 b 2 j , and lasso uses an L 1 penalty, i.e., the L 1 norm of the coefficient ∥b∥ 1 = p j=1 |b j |. Compared to the ridge, the lasso can give sparse solutions due to the non-smooth penalty term. That is, estimators of some coefficients are exactly zero.

Focus on the form (15.1). We can gain insights from the contour plot of the residual sum of squares as a function of b. With a well-defined OLS estimator β, Theorem 3.2 ensures

$(Y -Xb) t (Y -Xb) = (Y -X β) t (Y -X β) + (b -β) t X t X(b -β),$which equals a constant term plus a quadratic function centered at the OLS coefficient. Without any penalty, the minimizer is of course the OLS coefficient. With the L 1 penalty, the OLS coefficient may not be in the region defined by p j=1 |b j | ≤ t. If this happens, the intersection of the contour plot of (Y -Xb) t (Y -Xb) and the border of the restriction region p j=1 |b j | ≤ t can be at some axis. For example, Figure [15](#fig_40).1 shows a case with p = 2, and the lasso estimator hits the x-axis, resulting in a zero coefficient for the second coordinate. However, this does not mean that lasso always generates sparse solutions because sometimes the intersection of the contour plot of (Y -Xb) t (Y -Xb) and the border of the restriction region is at an edge of the region. For example, Figure [15](#fig_40).2 shows a case with a non-sparse lasso solution.

In contrast, the restriction region of the ridge is a circle, so the ridge solution does not hit any axis unless the original OLS coefficient is zero. Figure [15](#fig_40).3 shows the general ridge estimator. 

## Computing the lasso via coordinate descent

Many efficient algorithms can solve the lasso problem. The glmnet package in R uses the coordinate descent algorithm based on the form (15.2) [(Friedman et al., 2007](#b114)[(Friedman et al., , 2010))](#b116). I will first review a lemma which is the stepstone for the algorithm.

## The soft-thresholding lemma

Let sign(x) denote the sign of a real number x, which equals 1, 0, -1 if x > 0, x = 0, x < 0, respectively. Let (x) + = max(x, 0) denote the positive part of a real number x.

Lemma 15.1 Given b 0 and λ ≥ 0, we have 

$arg min b∈R 1 2 (b -b 0 ) 2 + λ|b| = sign(b 0 ) (|b 0 | -λ) + =      b 0 -λ, if b 0 ≥ λ, 0 if -λ ≤ b 0 ≤ λ, b 0 + λ if b 0 ≤ -λ.$
## Coordinate descent for the lasso

For a given λ > 0, we can use the following algorithm:

1. Standardize the data as Condition 14.1. So we need to solve a lasso problem without the intercept. For simplicity of derivation, we change the scale of the residual sum of squares without essentially changing the problem: min b1,...,bp

$1 2n n i=1 (y i -b 1 x i1 -• • • -b p x ip ) 2 + λ p j=1 |b j |.$Initialize β.

2. Update βj given all other coefficients. Define the partial residual as r ij = y i -

$k̸ =j βk x ik . Updating βj is equivalent to minimizing 1 2n n i=1 (r ij -b j x ij ) 2 + λ|b j |. Define βj,0 = n i=1 x ij r ij n i=1 x 2 ij = n -1 n i=1 x ij r ij -4 -2 0 2 4 -3 -2 -1 0 1 2 3 λ = 2 b 0 S(b 0 , λ) FIGURE 15.4: Soft-thresholding$as the OLS coefficient of the r ij 's on the x ij 's, so

$1 2n n i=1 (r ij -b j x ij ) 2 = 1 2n n i=1 (r ij -βj,0 x ij ) 2 + 1 2n n i=1 x 2 ij (b j -βj,0 ) 2 = constant + 1 2 (b j -βj,0 ) 2 .$Then updating βj is equivalent to minimizing 1 2 (b j -βj,0 ) 2 + λ|b j |. Lemma 15.1 implies βj = S( βj,0 , λ).

## Iterate until convergence.

Does the algorithm always converge? The theory of [Tseng (2001)](#b225) ensures it converges, but this is beyond the scope of this book. We can start with a large λ and all zero coefficients. We then gradually decrease λ, and for each λ, we apply the above algorithm. We finally select λ via K-fold cross-validation. Since we gradually decrease λ, the initial values from the last step are very close to the minimizer and the algorithm converges fairly fast.

## Example: comparing OLS, ridge and lasso

In the Boston housing data, the OLS, ridge, and lasso have similar performance in out-ofsample prediction. Lasso and ridge have similar coefficients. See Figure [15](#fig_40).5(a). But if we artificially add 200 columns of covariates of pure noise N(0, 1), then the ridge and lasso perform much better. Lasso can automatically shrink many coefficients to zero. See [Figure 15.5(b)](#). q q q q q q q q q q q q q q q q q q q q q q q q q ridge lasso 5 10 5 10 -15

-10 -5 0 indices of covariates coefficients (a) original data q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qqq q qq q q q q q q qq q q qqqqqqqqqq qq qq q q q qqq q qqq q qq q q qq q qq q q q qq q qq q q qqqq q qq q q q qqq q qq q qqq q q q q q q q q q qqq q qq q qq q q qq q qqq q q q q q qq q q qqqqqqq q qq q qqqqqq q qq q q q qqqq q q q qqqqqq q q q qqqqqqqqqqqqqqqq q q q qqqqqqq q q q q q q q qq q qqq q qq q q qq ridge lasso   Figure [15](#fig_40).7 compares the constraints corresponding to the ridge, lasso, and elastic net.

$A A B B E E J J L L (a) 0 < q < 1 A A B B E E J J L L (b) q = 1 A A B B E E J J L L (c) q = 2$Because the constraint of the elastic net is not smooth, it encourages sparse solutions in the same way as the lasso. Due to the ridge penalty, the elastic net can deal with the collinearity of the covariates better than the lasso. [Friedman et al. (2007)](#b114) proposed to use the coordinate descent algorithm to solve for the elastic net estimator, and [Friedman et al. (2009)](#b115) implemented it in an R package called Show that if β(1) and β(2) are two solutions, then α β(1) + (1 -α) β( [2](#formula_9)) is also a solution for any 0 ≤ α ≤ 1. Show that X β(1) = X β(2) must hold. Hint: The function ∥ • ∥ 2 is strongly convex. That is, for any v 1 , v 2 and 0 < α < 1, we have

$∥αv 1 + (1 -α)v 2 ∥ 2 ≤ α∥v 1 ∥ 2 + (1 -α)∥v 2 ∥ 2$and the inequality holds when v 1 ̸ = v 2 . The function ∥ • ∥ is convex. That is, for any v 1 , v 2 and 0 < α < 1, we have

$∥αv 1 + (1 -α)v 2 ∥ 1 ≤ α∥v 1 ∥ 1 + (1 -α)∥v 2 ∥ 1 .$
## The soft-thresholding lemma

Prove Lemma 15.1.

## Penalized OLS with an orthogonal design matrix

Consider the special case with standardized and orthogonal design matrix:

$X t 1 n = 0, X t X = I p .$For a fixed λ ≥ 0, find the explicit formulas of the jth coordinates of the following estimators in terms of the corresponding jth coordinate of the OLS estimator βj and λ (j = 1, . . . , p):

$βridge (λ) = arg min b∈R p ∥Y -Xb∥ 2 + λ∥b∥ 2 , βlasso (λ) = arg min b∈R p ∥Y -Xb∥ 2 + λ∥b∥ 1 , βenet (λ) = arg min b∈R p ∥Y -Xb∥ 2 + λ(α∥b∥ 2 + (1 -α)∥b∥ 1 ) , βsubset (λ) = arg min b∈R p ∥Y -Xb∥ 2 + λ∥b∥ 0 ,$where

$∥b∥ 2 = p j=1 b 2 j , ∥b∥ 1 = p j=1 |b j |, ∥b∥ 0 = p j=1$1(b j ̸ = 0).

## Standardization in the elastic net

For fixed λ and α, show that the intercept in βenet (λ, α) equals zero under the standardization in Condition 14.1.

## Coordinate descent for the elastic net

Give the detailed coordinate descent algorithm for the elastic net.

## Reducing elastic net to lasso

Consider the following form of the elastic net:

$arg min b∈R p ∥Y -Xb∥ 2 + λ{α∥b∥ 2 + (1 -α)∥b∥ 1 }.$Show that it reduces to the following lasso:

$arg min b∈R p ∥ Ỹ -Xb∥ 2 + λ∥b∥ 1 , where Ỹ = Y 0 p , X = X √ λαI p , λ = λ(1 -α).$Hint: Use the result in Problem 14.4.

## Reducing lasso to iterative ridge

Based on the simple result min

$ac=b (a 2 + c 2 )/2 = |b|,$for scalars a, b, c, Hoff (2017) rewrote the lasso problem

$min b∈R p {∥Y -Xb∥ 2 + λ∥b∥ 1 } as min u,v∈R p {∥Y -X(u • v)∥ 2 + λ(∥u∥ 2 + ∥v∥ 2 )/2}$where • denotes the component-wise product of vectors. Hoff (2017, Lemma 1) showed that a local minimum of the new problem must be a local minimum of the lasso problem.

Show that the new problem can be solved based on the following iterative ridge regressions:

Part VI

## Transformation and Weighting

## Transformations in OLS

Transforming the outcome and covariates is fundamental in linear models. Whenever we specify a linear model y i = x t i β + ε i , we implicitly have transformed the original y and x, or at least we have chosen the scales of them. [Carroll and Ruppert (1988)](#b71) is a textbook on this topic. This chapter discusses some important special cases.

## Transformation of the outcome

Although we can view y i = x t i β + ε i , (i = 1, . . . , n) as a linear projection that works for any type of outcome y i ∈ R, the linear model works the best for continuous outcomes and especially for Normally distributed outcomes. Sometimes, the linear model can be a poor approximation of the original outcome but may perform well for certain transformations of the outcome.

## Log transformation

With positive, especially heavy-tailed outcomes, a standard transformation is the log transformation. So we fit a linear model

$log y i = x t i β + ε i , (i = 1, . . . , n).$The interpretation of the coefficients changes a little bit. Because

$∂ logŷ i ∂x ij = ∂ ŷi ŷi ∂x ij = βj ,$we can interpret βj in the following way: ceteris paribus, if x ij increases by one unit, then the proportional increase in the average outcome is βj . In economics, βj is the semi-elasticity of y on x j in the model with log transformation on the outcome. Sometimes, we may apply the log transformation on both the outcome and a certain covariate:

log

$y i = β 1 x i1 + • • • + β j log x ij + • • • + ε i , (i = 1, . . . , n).$The jth fitted coefficient becomes

$∂ logŷ i ∂ log x ij = ∂ ŷi ŷi ∂x ij x ij = βj ,$so ceteris paribus, if x ij increases by 1%, then the average outcome will increase by βj %.

In economics, βj is the x j -elasticity of y in the model with log transformation on both the outcome and x j . The log transformation only works for positive variables. For a nonnegative outcome, we can modify the log transformation to log(y i + 1).

## Box-Cox transformation

Power transformation is another important class. The Box-Cox transformation unifies the log transformation and the power transformation:

$g λ (y) = y λ -1 λ , λ ̸ = 0, log y, λ = 0. L'Hôpital's rule implies that lim λ→0 y λ -1 λ = lim λ→0 dy λ /dλ 1 = lim λ→0 y λ log y = log y,$so as a function of λ, g λ (y) is continuous at λ = 0. The log transformation is a limiting version of the power transformation. Can we choose λ based on data? [Box and Cox (1964)](#b64) proposed a strategy based on the maximum likelihood under the Normal linear model:

$Y λ =    y λ1 . . . y λn    =    g λ (y 1 ) . . . g λ (y 1 )    ∼ N(Xβ, σ 2 I n ).$The density function of Y λ is

$f (Y λ ) = (2πσ 2 ) -n/2 exp - 1 2σ 2 (Y λ -Xβ) t (Y λ -Xβ) .$The Jacobian of the transformation from

$Y to Y λ is det ∂Y λ ∂Y = det      y λ-1 1 y λ-1 2 . . . y λ-1 n      = n i=1 y λ-1 i , so the density function of Y is f (Y ) = (2πσ 2 ) -n/2 exp - 1 2σ 2 (Y λ -Xβ) t (Y λ -Xβ) n i=1 y λ-1 i .$If we treat the density function of Y as a function of (β, σ 2 , λ), then it is the likelihood function, defined as L(β, σ 2 , λ). Given (σ 2 , λ), maximizing the likelihood function is equivalent to minimizing (Y λ -Xβ) t (Y λ -Xβ), i.e., we can run OLS of Y λ on X to obtain

$β(λ) = (X t X) -1 X t Y λ .$Given λ, maximizing the likelihood function is equivalent to first obtaining β(λ) and then obtaining σ2 (λ

$) = n -1 Y λ (I n -H)Y λ .$The final step is to maximize the profile likelihood as a function of λ:

$L( β(λ), σ2 (λ), λ) = 2πσ 2 (λ) -n/2 exp - nσ 2 (λ) 2σ 2 (λ) n i=1 y λ-1 i .$Dropping some constants, the log profile likelihood function of λ is

$l p (λ) = - n 2 log σ2 (λ) + (λ -1) n i=1 log y i .$The boxcox function in the R package MASS plots l p (λ), finds it maximizer λ, and construct a 95% confidence interval [ λl , λU ] based on the following asymptotic pivotal quantity

$2 l p ( λ) -l p (λ) a ∼ χ 2 1 ,$which holds by Wilks' Theorem. In practice, we often use the λ values within [ λl , λU ] that have more scientific meanings. I use two datasets to illustrate the Box-Cox transformation, with the R code in code16.1.2.R. For the jobs data, λ = 2 seems a plausible value. library ( MASS ) library ( mediation ) par ( mfrow = c ( 1 , 3 )) jobslm = lm ( job _ seek ~treat + econ _ hard + depress 1 + sex + age + occp + marital + nonwhite + educ + income , data = jobs ) boxcox ( jobslm , lambda = seq ( 1 . 5 , 3 , 0 . 1 ) , plotit = TRUE ) jobslm 2 = lm ( I ( job _ seek ^2 ) ~treat + econ _ hard + depress 1 + sex + age + occp + marital + nonwhite + educ + income , data = jobs ) hist ( jobslm $ residuals , xlab = " residual " , ylab = " " , main = " job _ seek " , font . main = 1 ) hist ( jobslm 2 $ residuals , , xlab = " residual " , ylab = " " , main = " job _ seek ^2 " , font . main = 1 ) Linear approximations may not be adequate, so we can consider a polynomial specification. With one-dimensional x, we can use

$y i = β 1 + β 2 x i + β 3 x 2 i • • • + β p x p-1 i + ε i .$In economics, it is almost the default choice to include the quadratic term of working experience in the log wage equation. I give an example below using the data from [Angrist et al. (2006)](#b49). The quadratic term of exper is significant.

> library ( foreign ) > census 0 0 = read . dta ( " census 0 0 . dta " ) > head ( census 0 0 ) age educ logwk perwt exper exper 2 black 1 4 8 1 2 6 . 6 7 0 5 7 6 1 . 0 8 5 0 0 2 1 3 0 9 0 0 0 2 4 2 1 3 6 . 7 8 3 9 0 5 0 . 9 6 6 6 3 8 3 2 3 5 2 9 0 3 4 9 1 3 6 . 7 6 2 3 8 3 1 . 2 1 3 2 2 9 7 3 0 9 0 0 0 4 4 4 1 3 6 . 3 0 2 8 5 1 0 . 4 8 3 3 1 9 1 2 5 6 2 5 0 5 4 5 1 6 6 . 0 4 3 3 8 6 0 . 9 6 6 6 3 8 3 2 3 5 2 9 0 6 4 3 1 3 5 . 0 6 1 1 3 8 1 . 0 8 5 0 0 2 1 2 4 5 7 6 0 > > census 0 0 ols 1 = lm ( logwk ~educ + exper + black , + data = census 0 0 ) > census 0 0 ols 2 = lm ( logwk ~educ + exper + I ( exper ^2 ) + black , + data = census 0 0 ) > round ( summary ( census 0 0 ols 1 )$ coef , 4 ) Estimate Std . Error t value Pr ( >| t |) ( Intercept ) 4 . 8 9 1 8 0 . 0 3 1 5 1 5 5 . 0 5 4 0 0 . 0 0 0 0 educ 0 . 1 1 5 2 0 . 0 0 1 2 9 9 . 1 4 7 2 0 . 0 0 0 0 exper 0 . 0 0 0 2 0 . 0 0 0 8 0 . 2 2 9 4 0 . 8 1 8 5 black -0 . 2 4 6 6 0 . 0 0 8 5 -2 9 . 1 6 7 4 0 . 0 0 0 0 > round ( summary ( census 0 0 ols 2 )$ coef , 4 ) Estimate Std . Error t value Pr ( >| t |) ( Intercept ) 5 . 0 7 7 7 0 . 0 8 8 7 5 7 . 2 2 5 4 0 . 0 0 0 0 educ 0 . 1 1 4 8 0 . 0 0 1 2 9 7 . 6 5 0 6 0 . 0 0 0 0 exper -0 . 0 1 4 8 0 . 0 0 6 7 -2 . 2 0 1 3 0 . 0 2 7 7 I ( exper ^2 ) 0 . 0 0 0 3 0 . 0 0 0 1 2 . 2 4 2 5 0 . 0 2 4 9 black -0 . 2 4 6 7 0 . 0 0 8 5 -2 9 . 1 7 3 2 0 . 0 0 0 0

We can also include polynomial terms of more than one covariate, for example,

$(1, x 1i , . . . , x d i1 , x i2 , . . . , x l i2 ) or (1, x 1i , . . . , x d i1 , x i2 , . . . , x l i2 , x i1 x i2 , . . . , x d i1 x l i2$). We can also approximate the conditional mean function of the outcome by a linear combination of some basis functions:

$y i = f (x i ) + ε i ∼ = J j=1 β j S j (x i ) + ε i ,$where the S j (x i )'s are basis functions. The gam function in the mgcv package uses this strategy including the automatic procedure of choosing the number of basis functions J. The following example has a sine function as the truth, and the basis expansion approximation yields reasonable performance with sample size n = 1000. Figure [16](#fig_95).3 plots both the true and estimated curves. library ( mgcv ) n = 1 0 0 0 dat = data . frame ( x <-seq ( 0 , 1 , length . out = n ) , true <-sin ( x * 1 0 ) , y <-true + rnorm ( n )) np . fit = gam ( y ~s ( x ) , data = dat ) plot ( y ~x , data = dat , bty = " n " , pch = 1 9 , cex = 0 . 1 , col = " grey " ) lines ( true ~x , col = " grey " ) lines ( np . fit $ fitted . values ~x , lty = 2 ) legend ( " bottomright " , c ( " true " , " estimated " ) , lty = 1 : 2 , col = c ( " grey " , " black " ) , bty = " n " )

The generalized additive model is an extension of the multivariate case:

$y i = f 1 (x i1 ) + • • • + f p (x ip ) + ε i ∼ = J1 j=1 β 1j S j (x i1 ) + • • • + Jp j=1 β pj S j (x ip ) + ε i .$The gam function in the mgcv package implements this strategy. Again I use the dataset from [Angrist et al. (2006)](#b49) to illustrate the procedure with nonlinearity in educ and exper shown in Figure [16](#fig_95).4. q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0.0 0. The R code in this section is in code16.2.1.R. See [Wood (2017)](#b239) for more details about the generalized additive model.

## Regression discontinuity and regression kink

The left panel of Figure [16](#fig_95).5 shows an example of regression discontinuity, where the linear functions before and after a cutoff point can differ with a possible jump. A simple way to capture the two regimes of linear regression is to fit the following model:

$y i = β 1 + β 2 x i + β 3 1 (x i > c) + β 4 x i 1 (x i > c) + ε i . So y i = β 1 + β 2 x i + ε i x i ≤ c, (β 1 + β 3 ) + (β 2 + β 4 ) x i + ε i , x i > c.$Testing the discontinuity at c is equivalent to testing

$(β 1 + β 3 ) + (β 2 + β 4 ) c = β 1 + β 2 c ⇐⇒ β 3 + β 4 c = 0.$If we center the covariates at c, then 

$y i = β 1 + β 2 (x i -c) + β 3 1 (x i > c) + β 4 (x i -c)1 (x i > c) + ε i$$y i = β 1 + β 2 (x i -c) + ε i x i ≤ c, (β 1 + β 3 ) + (β 2 + β 4 ) (x i -c) + ε i , x i > c.$So testing the discontinuity at c is equivalent to testing β 3 = 0.

The right panel of Figure [16](#fig_95).5 shows an example of regression kink, where the linear functions before and after a cutoff point can differ but the whole regression line is continuous. A simple way to capture the two regimes of linear regression is to fit the following model:

$y i = β 1 + β 2 R c (x i ) + β 3 (x i -c) + ε i using R c (x) = max(0, x -c) = 0, x ≤ c, x -c, x > c.$So

$y i = β 1 + β 3 (x i -c) + ε i , x i ≤ c, β 1 + (β 2 + β 3 ) (x i -c) + ε i , x i > c.$This ensures that the mean function is continuous at c with both left and right limits equaling β 1 . Testing the kink is equivalent to testing β 2 = 0.

These regressions have many applications in economics, but I omit the economic background. Readers can find more discussions in [Angrist and Pischke (2008)](#b50) and [Card et al. (2015)](#b69).

## Interaction

Interaction is an important notion in applied statistics. It measures the interplay of two or more variables acting simultaneously on an outcome. Epidemiologists find that cigarette smoking and alcohol consumption both increase the risks of many cancers. Then they want to measure how cigarette smoking and alcohol consumption jointly increase the risks. That is, does cigarette smoking increase the risks of cancers more in the presence of alcohol consumption than in the absence of it? Political scientists are interested in measuring the interplay of different get-out-to-vote interventions on voting behavior. This chapter will review many aspects of interaction in the context of linear regression. [Cox (1984)](#b83) and Berrington de González and [Cox (2007)](#b84) reviewed interaction from a statistical perspective. VanderWeele (2015) offers a textbook discussion on interaction with a focus on applications in epidemiology.

## Two binary covariates interact

Let's start with the simplest yet nontrivial example with two binary covariates x i1 , x i2 ∈ {0, 1}. We can fit the OLS:

$y i = β0 + β1 x i1 + β2 x i2 + β12 x i1 x i2 + εi . (17.1)$We can express the coefficients in terms of the means of the outcomes within four combinations of the covariates. The following proposition is an algebraic result.

Proposition 17.1 From (17.1), we have

$β0 = ȳ00 , β1 = ȳ10 -ȳ00 , β2 = ȳ01 -ȳ00 , β12 = (ȳ 11 -ȳ10 ) -(ȳ 01 -ȳ00 ),$where ȳf1f2 is the average value of the y i 's with x i1 = f 1 and

$x i2 = f 2 .$The proof of Proposition 17.1 is pure algebraic which is relegated to Problem 17.1. The proposition generalizes to OLS with more than two binary covariates. See [Zhao and Ding (2022)](#b250) for more details.

Practitioners also interpret the coefficient of the product term of two continuous variables as an interaction. The coefficient β12 equals the difference between ȳ11 -ȳ10 , the effect of x i2 on y i holding x i1 at level 1, and ȳ01 -ȳ00 , the effect of x i2 on y i holding x i1 at level 0. It also equals β12 = (ȳ 11 -ȳ01 ) -(ȳ 10 -ȳ00 ), that is, the difference between ȳ11 -ȳ01 , the effect of x i1 on y i holding x i2 at level 1, and ȳ10 -ȳ00 , the effect of x i1 on y i holding x i2 at level 0. The formula shows the symmetry of x i1 and x i2 in defining interaction. 17.2 A binary covariate interacts with a general covariate

## Treatment effect heterogeneity

In many studies, we are interested in the effect of a binary treatment z i on an outcome y i , adjusting for some background covariates x i . The covariates can play many roles in this problem. They may affect the treatment, enter the outcome model, and modify the effect of the treatment on the outcome. We can formulate the problem in terms of linear regression:

$y i = β 0 + β 1 z i + β t 2 x i + β t 3 x i z i + ε i ,(17.2)$where

$E(ε i | z i , x i ) = 0. So E(y i | z i = 1, x i ) = β 0 + β 1 + (β 2 + β 3 ) t x i and E(y i | z i = 0, x i ) = β 0 + β t 2 x i , which implies that E(y i | z i = 1, x i ) -E(y i | z i = 0, x i ) = β 1 + β t 3 x i .$The conditional average treatment effect is thus a linear function of the covariates. As long as β 3 ̸ = 0, we have treatment effect heterogeneity, which is also called effect modification.

A statistical test for β 3 = 0 is straightforward based on OLS and EHW standard error. Note that (17.2) includes the interaction of the treatment and all covariates. With prior knowledge, we may believe that the treatment effect varies with respect to a subset of covariates, or, equivalently, we may set some components of β 3 to be zero. [Johnson and Neyman (1936)](#b150) proposed a technique to identify the region of covariates in which the conditional average treatment β 1 + β t 3 x is zero. For a given x, we can test the null hypothesis that β 1 + β t 3 x = 0, which is a linear combination of the regression coefficients of (17.2). If we fail to reject the null hypothesis, then this x belongs to the region of zero conditional average effect. See [Rogosa (1981)](#b200) for more discussions.

## Johnson-Neyman technique

## Blinder-Oaxaca decomposition

The linear regression (17.2) also applies to descriptive statistics when z i is a binary indicator for subgroups. For example, z i can be a binary indicator for age, racial, or gender groups, y i can be the log wage, and x i can be a vector of explanatory variables such as education, experience, industry, and occupation. Sometimes, it is more insightful to write (17.2) in terms of two possibly non-parallel linear regressions:

$y i = γ 0 + θ t 0 x i + ε i , E(ε i | z i = 0, x i ) = 0 (17.3)$for the group with z i = 0, and

$y i = γ 1 + θ t 1 x i + ε i , E(ε i | z i = 1, x i ) = 0 (17.4)$for the group with z i = 1. Regressions (17.3) and (17.4) are just a reparametrization of (17.2) with

$γ 0 = β 0 , θ 0 = β 2 , γ 1 = β 0 + β 1 , θ 1 = β 2 + β 3 .$Based on (17.3) and (17.4), we can decompose the difference in the outcome means as

$E(y i | z i = 1) -E(y i | z i = 0) = {γ 1 + θ t 1 E(x i | z i = 1)} -{γ 0 + θ t 0 E(x i | z i = 0)} = θ t 0 {E(x i | z i = 1) -E(x i | z i = 0)} +(θ 1 -θ 0 ) t E(x i | z i = 0) + γ 1 -γ 0 +(θ 1 -θ 0 ) t {E(x i | z i = 1) -E(x i | z i = 0)}.$The decomposition has three components: the first component

$E = θ t 0 {E(x i | z i = 1) -E(x i | z i = 0)} = β t 2 {E(x i | z i = 1) -E(x i | z i = 0$)} measures the endowment effect since it is due to the difference in the covariates; the second component

$C = (θ 1 -θ 0 ) t E(x i | z i = 0) + γ 1 -γ 0 = β t 3 E(x i | z i = 0) + β 1 measures the difference in coefficients; the third component I = (θ 1 -θ 0 ) t {E(x i | z i = 1) -E(x i | z i = 0)} = β t 3 {E(x i | z i = 1) -E(x i | z i = 0$)} measures the interaction between the endowment and coefficients. This is called the Blinder-Oaxaca decomposition. [Jann (2008)](#b149) reviews other forms of the decomposition, extending the original forms in [Blinder (1973)](#b62) and [Oaxaca (1973)](#b185).

Estimation and testing for E, C, and I are straightforward. Based on the OLS of (17.2) and the sample means x1 and x0 of the covariates, we have point estimators

$Ê = βt 2 (x 1 -x0 ), Ĉ = βt 3 x0 + β1 , Î = βt 3 (x 1 -x0 ).$Given the covariates, they are just linear transformations of the OLS coefficients. Statistical inference is thus straightforward.

## Chow test

Chow (1960) proposed to test whether the two regressions (17.3) and (17.4) are identical. Under the null hypothesis that γ 0 = γ 1 and θ 0 = θ 1 , he proposed an F test assuming homoskedasticity, which is called the Chow test in econometrics. In fact, this is just a special case of the standard F test for the null hypothesis that β 1 = 0 and β 3 = 0 in (17.2). Moreover, based on the OLS in (17.2), we can also derive the robust test based on the EHW covariance estimator. [Chow (1960)](#b74) discussed a subtle case in which one group has a small size rending the OLS fit underdetermined. I relegate the details to Problem 17.3. Note that under this null hypothesis, C = I = 0, so the difference in the outcome means is purely due to the difference in the covariate means.

## Difficulties of intereaction

## Removable interaction

The significance of the interaction term differs with y and log(y).

> n = 1 0 0 0 > x 1 = rnorm ( n ) > x 2 = rnorm ( n ) > y = exp ( x 1 + x 2 + rnorm ( n )) > ols . fit = lm ( log ( y ) ~x 1 * x 2 ) > summary ( ols . fit ) Call : lm ( formula = log ( y ) ~x 1 * x 2 )

## Residuals :

Min 1 Q Median 3 Q Max -3 . 7 3 7 3 -0 . 6 8 2 2 -0 . 0 1 1 1 0 . 7 0 8 4 3 . 1 0 3 9 Coefficients :

Estimate Std . Error t value Pr ( >| t |) ( Intercept ) 0 . 0 0 3 2 1 4 0 . 0 3 1 2 8 6 0 . 1 0 3 0 . 9 1 8 x 1 1 . 0 5 6 8 0 1 0 . 0 3 0 6 4 9 3 4 . 4 8 0 <2e -1 6 *** x 2 1 . 0 0 9 4 0 4 0 . 0 3 0 7 7 8 3 2 . 7 9 7 <2e -1 6 *** x 1 : x 2 -0 . 0 1 7 5 2 8 0 . 0 3 0 5 2 6 -0 . 5 7 4 0 . 5 6 6 > ols . fit = lm ( y ~x 1 * x 2 ) > summary ( ols . fit )

Call : lm ( formula = y ~x 1 * x 2 )

Residuals : Min 1 Q Median 3 Q Max -3 5 . 9 5 -5 . 1 7 -0 . 9 7 2 . 3 4 5 1 3 . 3 5

Coefficients : Estimate Std . Error t value Pr ( >| t |) ( Intercept ) 5 . 2 8 4 2 0 . 6 6 8 6 7 . 9 0 3 7 . 1 7e -1 5 *** x 1 6 . 7 5 6 5 0 . 6 5 5 0 1 0 . 3 1 5 < 2e -1 6 *** x 2 4 . 9 5 4 8 0 . 6 5 7 7 7 . 5 3 3 1 . 1 1e -1 3 *** x 1 : x 2 7 . 3 8 1 0 0 . 6 5 2 4 1 1 . 3 1 4 < 2e -1 6 ***

## Main effect in the presence of interaction

In the OLS fit below, we observe significant main effects.

> # # data from " [https :// stats](https://stats) . idre . ucla . edu / stat / data / hsbdemo . dta " > hsbdemo = read . table ( " hsbdemo . txt " ) > ols . fit = lm ( read ~math + socst , data = hsbdemo ) > summary ( ols . fit ) Call : lm ( formula = read ~math + socst , data = hsbdemo ) Residuals : Min 1 Q Median 3 Q Max -1 8 . 8 7 2 9 -4 . 8 9 8 7 -0 . 6 2 8 6 5 . 2 3 8 0 2 3 . 6 9 9 3 Coefficients : Estimate Std . Error t value Pr ( >| t |) ( Intercept ) 7 . 1 4 6 5 4

3 . 0 4 0 6 6 2 . 3 5 0 0 . 0 1 9 7 * math 0 . 5 0 3 8 4 0 . 0 6 3 3 7 7 . 9 5 1 1 . 4 1e -1 3 *** socst 0 . 3 5 4 1 4 0 . 0 5 5 3 0 6 . 4 0 4 1 . 0 8e -0 9 ***

Then we add the interaction term into the OLS, and suddenly we have significant interaction but not significant main effects. Min 1 Q Median 3 Q Max -1 8 . 6 0 7 1 -4 . 9 2 2 8 -0 . 7 1 9 5 4 . 5 9 1 2 2 1 . 8 5 9 2

Coefficients : Estimate Std . Error t value Pr ( >| t |) ( Intercept ) 3 7 . 8 4 2 7 1 5 1 4 . 5 4 5 2 1 0 2 . 6 0 2 0 . 0 0 9 9 8 ** math -0 . 1 1 0 5 1 2 0 . 2 9 1 6 3 4 -0 . 3 7 9 0 . 7 0 5 1 4 socst -0 . 2 2 0 0 4 4 0 . 2 7 1 7 5 4 -0 . 8 1 0 0 . 4 1 9 0 8 math : socst 0 . 0 1 1 2 8 1 0 . 0 0 5 2 2 9 2 . 1 5 7 0 . 0 3 2 2 1 * However, if we center the covariates, the main effects are significant again. Residuals : Min 1 Q Median 3 Q Max -1 8 . 6 0 7 1 -4 . 9 2 2 8 -0 . 7 1 9 5 4 . 5 9 1 2 2 1 . 8 5 9 2

Coefficients : Estimate Std . Error t value Pr ( >| t |) ( Intercept ) 5 1 . 6 1 5 3 2 7 0 . 5 6 8 6 8 5 9 0 . 7 6 3 < 2e -1 6 *** math . c 0 . 4 8 0 6 5 4 0 . 0 6 3 7 0 1 7 . 5 4 5 1 . 6 5e -1 2 *** socst . c 0 . 3 7 3 8 2 9 0 . 0 5 5 5 4 6 6 . 7 3 0 1 . 8 2e -1 0 *** math . c : socst . c 0 . 0 1 1 2 8 1 0 . 0 0 5 2 2 9 2 . 1 5 7 0 . 0 3 2 2 *

Based on the linear model with interaction

$E(y i | x i1 , x i2 ) = β 0 + β 1 x i1 + β 2 x i2 + β 12 x i1 x i2 ,$better definitions of the main effects are

$n -1 n i=1 ∂E(y i | x i1 , x i2 ) ∂x i1 = n -1 n i=1 (β 1 + β 12 x i2 ) = β 1 + β 12 x2 and n -1 n i=1 ∂E(y i | x i1 , x i2 ) ∂x i2 = n -1 n i=1 (β 2 + β 12 x i1 ) = β 2 + β 12 x1 ,$which are called the average partial or marginal effects. So when the covariates are centered, we can interpret β 1 and β 2 as the main effects. In contrast, the interpretation of the interaction term does not depend on the centering of the covariates because

$∂ 2 E(y i | x i1 , x i2 ) ∂x i1 ∂x i2 = β 12 .$The R code in this section is in code17.3.R.

## Power

Usually, statistical tests for interaction do not have enough power. Proposition 17.1 provides a simple explanation.

The variance of the interaction equals var( β12 ) = σ 2 11 n 11 + σ 2 10 n 10 + σ 2 01 n 01 + σ 2 00 n 00 , where σ 2 f1f2 = var(y i

$| x i1 = f 1 , x i2 = f 2 )$. Therefore, its variance is driven by the smallest value of n 11 , n 10 , n 01 , n 00 . Even when the total sample size is large, one of the subgroup sample sizes can be small, resulting in a large variance of the estimator of the interaction.

## Homework problems

## Interaction and difference-in-difference

Prove Proposition 17.1. Moreover, simplify the HC0 and HC2 versions of the EHW standard errors of the coefficients in terms of n f1f2 and σ2 f1f2 , where n f1f2 is the sample size and σ2 f1f2 is the sample variance of the outcomes for units with x i1 = f 1 and x i2 = f 2 . Hint: You can prove the proposition by inverting the 4 × 4 matrix X t X. However, this method is a little too tedious. Moreover, this proof does not generalize to OLS with K > 2 binary covariates. So it is better to find alternative proofs. For the EHW standard errors, you can use the result in Problems 6.3 and 6.4.

## Two OLS

Given data (x i , z i , y i ) n i=1 where x i denotes the covariates, z i denotes the binary group indicator, and y i denotes the outcome. We can fit two separate OLS: ŷi = γ1 + x t i β1 and ŷi = γ0 + x t i β0 with data in group 1 and group 0, respectively. We can also fit a joint OLS using the pooled data: ŷi = α0 + αz z i + x t i αx + z i x t i αzx .

1. Find ( α0 , αz , αx , αzx ) in terms of (γ 1 , β1 , γ0 , β0 ).

2. Show that the fitted values ŷi 's are the same from the separate and the pooled OLS for all units i = 1, . . . , n.

3. Show that the leverage scores h ii 's are the same from the separate and the pooled OLS.

## Chow test when one group size is too small

Assume (17.3) and (17.4) with homoskedastic Normal error terms. Let n 1 and n 0 denote the sample sizes of groups with z i = 1 and z i = 0. Consider the case with n 0 larger than the number of covariates but n 1 smaller than the number of covariates. So we can fit OLS and estimate the variance based on (17.3), but we cannot do so based on (17.4). The statistical test discussed in the main paper does not apply. [Chow (1960)](#b74) proposed the following test based on prediction. Let γ0 and θ0 be the coefficients, and σ 2 0 be the variance estimate based on OLS with units z i = 0. Under the null hypothesis that γ 0 = γ 1 and θ 0 = θ 1 , predict the outcomes of the units z i = 1: ŷi = γ0 + θt 0 x i with the prediction error d i = y i -ŷi following a multivariate Normal distribution. Propose an F test based on d i with z i = 1. Hint: It is more convenient to use the matrix form of OLS.

## Invariance of the interaction

In Section 17.3.2, the point estimate and standard error of the coefficient of the interaction term remain the same no matter whether we center the covariates or not. This result holds in general. This problem quantifies this phenomenon. With scalars x i1 , x i2 , y i (i = 1, . . . , n), we can fit the OLS

$y i = β0 + β1 x i1 + β2 x i2 + β12 x i1 x i2 + εi .$Under any location transformations of the covariates

$x ′ i1 = x i1 -c 1 , x ′ i2 = x i2 -c 2 , we can fit the OLS y i = β0 + β1 x ′ i1 + β2 x ′ i2 + β12 x ′ i1 x ′ i2 + εi .$1. Express β0 , β1 , β2 , β12 in terms of β0 , β1 , β2 , β12 . Verify that β12 = β12 .

2. Show that the EHW standard errors for β12 and β12 are identical.

Hint: Use the results in Problems 3.4 and 6.4.

## Restricted OLS

Assume that in the standard linear model Y = Xβ + ε, the parameter has restriction

$Cβ = r (18.1)$where C is an l × p matrix and r is a l dimensional vector. Assume that C has linearly independent row vectors; otherwise, some restrictions are redundant. We can use the restricted OLS: βr = arg min b∈R p ∥Y -Xb∥ 2 under the restriction Cb = r.

I first give some examples of linear models with restricted parameters, then derive the algebraic properties of the restricted OLS estimator βr , and finally discuss statistical inference with restricted OLS.

## Examples

Example 18.1 (Short regression) Partition X into X 1 and X 2 with k and l columns, respectively, with p = k + l. The short regression of Y on X 1 yields OLS coefficient β1 . So ( βt 1 , 0 t l ) = βr with C = (0 l×k , I l×l ), r = 0 l .

Example 18.2 (Testing linear hypothesis) Consider testing the linear hypothesis Cβ = r in the linear model. We have discussed in Chapter 5 the Wald test based on the OLS estimator and its estimated covariance matrix under the Normal linear model. An alternative strategy is to test the hypothesis based on comparing the residual sum of squares under the OLS and restricted OLS. Therefore, we need to compute both β and βr .

Example 18.3 (One-way analysis of variance) If x i contains the intercept and Q 1 dummy variables of a discrete regressor of Q 1 levels, (f i1 , . . . , f iQ1 ) t , then we must impose a restriction on the parameter in the linear model

$y i = α + Q1 j=1 β j f ij + ε i .$A canonical choice is β Q1 = 0, which is equivalent to dropping the last dummy variable due to its redundancy. Another canonical choice is Q1 j=1 β j = 0. This restriction keeps the symmetry of the regressors in the linear model and changes the interpretation of β j as the deviation from the "effect" of level j with respect to the average "effect." Both are special cases of restricted OLS.

Example 18.4 (Two-way analysis of variance) With two factors of levels Q 1 and Q 2 , respectively, the regressor x i contains the Q 1 dummy variables of the first factor, (f i1 , . . . , f iQ1 ) t , the Q 2 dummies of the second factor, (g i1 , . . . , g iQ2 ) t , and the Q 1 Q 2 dummy variables of the interaction terms, (f i1 g i1 , . . . , f iQ1 g iQ2 ) t . We must impose restrictions on the parameters in the linear model

$y i = α + Q1 j=1 β j f ij + Q2 k=1 γ k g ik + Q1 j=1 Q2 k=1 δ jk f ij g ik + ε i .$Similar to the discussion in Example 18.3, two canonical choices of restrictions are

$β Q1 = 0, γ Q2 = 0, δ Q1,k = δ j,Q2 = 0, (j = 1, . . . , Q 1 ; k = 1, . . . , Q 2 ). and Q1 j=1 β j = 0, Q2 k=1 γ k = 0, Q1 j=1 δ jk = Q2 k=1 δ jk = 0, (j = 1, . . . , Q 1 ; k = 1, . . . , Q 2 ).$
## Algebraic properties

I first give an explicit formula of the restricted OLS [(Theil, 1971;](#b216)[Rao, 1973)](#b197). For simplicity, the following theorem assumes that X t X is invertible. This condition may not hold in general; see Examples 18.3 and 18.4. [Greene and Seaks (1991)](#b129) discussed the results without this assumption; see Problem 18.8 for more details.

$Theorem 18.1 If X t X is invertible, then βr = β -(X t X) -1 C t {C(X t X) -1 C t } -1 (C β -r),$where β is the unrestricted OLS coefficient.

Proof of Theorem 18.1: The Lagrangian for the restricted optimization problem is

$(Y -Xb) t (Y -Xb) -2λ t (Cb -r). So the first order condition is 2X t (Y -Xb) -2C t λ = 0 which implies X t Xb = X t Y -C t λ. (18.2) Solve the linear system in (18.2) to obtain b = (X t X) -1 (X t Y -C t λ).$Using the linear restriction Cb = r, we have

$C(X t X) -1 (X t Y -C t λ) = r which implies that λ = {C(X t X) -1 C t } -1 (C β -r). So the restricted OLS coefficient is βr = (X t X) -1 (X t Y -C t λ) = β -(X t X) -1 C t λ = β -(X t X) -1 C t {C(X t X) -1 C t } -1 (C β -r).$Since the objective function is convex and the restrictions are linear, the solution from the first-order condition is indeed the minimizer. □ In the special case with r = 0, Theorem 18.1 has a simpler form.

Corollary 18.1 Under the restriction (18.1) with r = 0, we have

$βr = M r β, where M r = I p -(X t X) -1 C t {C(X t X) -1 C t } -1 C.$Moreover, M r satisfies the following properties

$M r (X t X) -1 C t = 0, CM r = 0, {I p -C t (CC t ) -1 C}M r = M r .$The M r matrix plays central roles below.

The following result is also an immediate corollary of Theorem 18.1.

Corollary 18.2 Under the restriction (18.1), we have

$βr -β = M r ( β -β).$I leave the proofs of Corollaries 18.1 and 18.2 as Problem 18.1.

## Statistical inference

I first focus on the Gauss-Markov model with the restriction (18.1). As direct consequences of Corollary 18.2, we can show that the restricted OLS estimator is unbiased for β, and obtain its covariance matrix below.

Corollary 18.3 Assume the Gauss-Markov model with the restriction (18.1). We have

$E( βr ) = β, cov( βr ) = σ 2 M r (X t X) -1 M t r .$Moreover, under the Normal linear model with the restriction (18.1), we can derive the exact distribution of the restricted OLS estimator and propose an unbiased estimator for σ 2 .

Theorem 18.2 Assume the Normal linear model with the restriction (18.1). We have

$βr ∼ N(β, σ 2 M r (X t X) -1 M t r ). An unbiased estimator for σ 2 is σ2 r = εt r εr /(n -p + l),$where εr = Y -X βr . Moreover, βr σ2 r .

Based on the results in Theorem 18.2, we can derive the t and F statistics for finitesample inference of β based on the estimator βr and the estimated covariance matrix σ2 r M r (X t X) -1 M t r .

Corollary 18.3 and Theorem 18.2 extend the results for the OLS estimator. I leave their proofs as Problem 18.3.

I then discuss statistical inference under the heteroskedastic linear model with the restriction (18.1). Corollary 18.2 implies that cov

$( βr ) = M r (X t X) -1 X t diag{σ 2 1 , . . . , σ 2 n }X(X t X) -1 M t r .$Therefore, the EHW-type estimated covariance matrix is

$Vehw,r = M r (X t X) -1 X t diag{ε 2 i,r , . . . , ε2 n,r }X(X t X) -1 M t r .$where the εi,r 's are the residuals from the restricted OLS.

## Final remarks

This chapter follows [Theil (1971)](#b216) and [Rao (1973)](#b197). [Tarpey (2000)](#b215) contains additional algebraic results on restricted OLS.

## Homework problems

18.1 Algebraic details of restricted OLS Prove Corollaries 18.1 and 18.2.

## Invariance of restricted OLS

Consider an N × 1 vector Y and two N × p matrices, X and X ′ , that satisfy X ′ = XΓ for some nonsingular p × p matrix Γ. The restricted OLS fits of Y = X βr + εr subject to C βr = r, Y = X βr + εr subject to C βr = r, with X = XΓ and C = CΓ yield ( βr , εr , Vehw,r ) and ( βr , εr , Ṽehw,r ) as the coefficient vectors, residuals, and robust covariances. Prove that they must satisfy βr = Γ βr , εr = εr , Vehw,r = Γ Ṽehw,r Γ t .

## Moments and distribution of restricted OLS

Prove Corollary 18.3 and Theorem 18.2.

## Minimum normal estimator as restricted OLS

An application of the formula of βr is the minimum norm estimator for under-determined linear equations. When X has more columns than rows, Y = Xβ can have infinitely many solutions, but we may only be interested in the solution with the minimum norm. Assume p ≥ n and the rows of X are linearly independent. Show that the solution to

$min b ∥b∥ 2 such that Y = Xb is βm = X t (XX t ) -1 Y.$18.8 Restricted OLS with degenerate design matrix [Greene and Seaks (1991)](#b129) pointed out that restricted OLS does not require that X t X be invertible, although the proof of Theorem 18.1 does. Modify the proof to show that the restricted OLS and the Lagrange multiplier satisfy

$βr λ = W -1 X t Y r as long as W = X t X C t C 0 is invertible.$Derive the statistical results in parallel with Section 18.3. Remark: If X has full column rank p, then W must be invertible. Even if X does not have full column rank, W can still be invertible. See Problem 18.9 below for more details.

## Restricted OLS with degenerate design matrix: more algebra

This problem provides more algebraic details for Problem 18.8. Prove Lemma 18.1 below.

$Lemma 18.1 Consider W = X t X C t C 0$where X t X may not be invertible and C has full row rank.

The matrix W is invertible if and only if X C has full column rank p. Remark: When X has full column rank p, then X C must have full column rank p, which ensures that W is invertible by Lemma 18.1. I made the comment in Problem 18.8. The invertibility of W plays an important role in other applications. See Benzi et al. (2005) and Bai and Bai (2013) for more general results.

## Weighted Least Squares

## Generalized least squares

We can extend the Gauss-Markov model to allow for a general covariance structure of the error term. The following model is due to [Aitkin (1936)](#b47).

$Assumption 19.1 (Generalized Gauss-Markov model) We have Y = Xβ + ε, E(ε) = 0, cov(ε) = σ 2 Σ. (19.1)$where X is a fixed matrix with linearly independent columns. The unknown parameters are β and σ 2 . The Σ is a known positive definite matrix.

Two leading cases of generalized least squares are

$Σ = diag w -1 1 , . . . , w -1 n ,(19.2)$which corresponds to a diagonal covariance matrix, and

$Σ = diag {Σ 1 , . . . , Σ K } (19.3)$which corresponds to a block diagonal covariance matrix where Σ k is n k ×n k and

$K k=1 n k = n.$Under model (19.1), we can still use the OLS estimator β = (X t X) -1 X t Y . It is unbiased

$E( β) = (X t X) -1 X t E(Y ) = (X t X) -1 X t Xβ = β. It has covariance matrix cov( β) = cov (X t X) -1 X t Y = (X t X) -1 X t cov(Y )X(X t X) -1 = σ 2 (X t X) -1 X t ΣX(X t X) -1 . (19.4)$The OLS estimator is BLUE under the Gauss-Markov model, but it is not under the generalized Gauss-Markov model. Then what is the BLUE? We can transform (19.1) into the Gauss-Markov model by standardizing the error term:

$Σ -1/2 Y = Σ -1/2 Xβ + Σ -1/2 ε. Define Y * = Σ -1/2 Y, X * = Σ -1/2 X and ε * = Σ -1/2 ε. The model (19.1) reduces to Y * = Xβ + ε * , E(ε * ) = 0, cov(ε * ) = σ 2 I n ,$which is the Gauss-Markov model for the transformed variables Y * and X * . The Gauss-Markov theorem ensures that the BLUE is

$βΣ = (X t * X * ) -1 X t * Y * = (X t Σ -1 X) -1 X t Σ -1 Y.$It is unbiased because

$E( βΣ ) = (X t Σ -1 X) -1 X t Σ -1 E(Y ) = (X t Σ -1 X) -1 X t Σ -1 Xβ = β. It has covariance matrix cov( βΣ ) = cov (X t Σ -1 X) -1 X t Σ -1 Y = (X t Σ -1 X) -1 X t Σ -1 cov(Y )Σ -1 X(X t Σ -1 X) -1 = σ 2 (X t Σ -1 X) -1 X t Σ -1 ΣΣ -1 X(X t Σ -1 X) -1 = σ 2 (X t Σ -1 X) -1 . (19.5)$In particular, cov( βΣ ) is smaller than or equal to cov( β) in the matrix sense[foot_17](#foot_17) . So based on (19.4) and (19.5), we have the following pure linear algebra inequality:

Corollary 19.1 If X has linear independent columns and Σ is invertible, then

$(X t Σ -1 X) -1 ⪯ (X t X) -1 X t ΣX(X t X) -1 .$Problem 19.1 gives a more general result.

## Weighted least squares

This chapter focuses on the first covariance structure in (19.2) and Chapter 25 will discuss the second in (19.3). The Σ in (19.2) results in the weighted least squares (WLS) estimator

$βw = βΣ = (X t Σ -1 X) -1 X t Σ -1 Y = n i=1 w i x i x t i -1 n i=1 w i x i y i .$From the derivation above, we can also write the WLS estimator as

$βw = arg min b∈R p (Y -Xb) t Σ -1 (Y -Xb) = arg min b∈R p n i=1 w i (y i -x t i b) 2 = arg min b∈R p (Y * -X * b) t (Y * -X * b) = arg min b∈R p n i=1 (y * i -x t * i b) 2 ,$where y * i = w 1/2 i y i and x * i = w 1/2 i x i . So WLS is equivalent to the OLS with transformed variables, with the weights inversely proportional to the variances of the errors. By this equivalence, WLS inherits many properties of OLS. See the problems in Section 19.5 for more details.

Analogous to OLS, we can derive finite-sample exact inference based on the generalized Normal linear model:

$y i = x t i β + ε i , ε i ∼ N(0, σ 2 /w i ), or, equivalently, y * i = x t * i β + ε * i , ε * i ∼ N(0, σ 2$). The lm function with weights reports the standard error, t-statistic, and p-value based on this model. This assumes that the weights fully capture the heteroskedasticity, which is unrealistic in many problems.

In addition, we can derive asymptotic inference based on the following heteroskedastic model y i = x t i β + ε i where the ε i 's are independent with mean zero and variances σ 2 i (i = 1, . . . , n). It is possible that w i ̸ = 1/σ 2 i , i.e., the variances used to construct the WLS estimator can be misspecified. Even though there is no guarantee that βw is BLUE, it is still unbiased. From the decomposition βw =

$n i=1 w i x i x t i -1 n i=1 w i x i y i = n i=1 w i x i x t i -1 n i=1 w i x i (x t i β + ε i ) = β + n -1 n i=1 w i x i x t i -1 n -1 n i=1 w i x i ε i ,$we can apply the law of large numbers to show that βw is consistent for β and apply the CLT to show that βw

$a ∼ N (β, V w ) ,$where

$V w = n -1 n -1 n i=1 w i x i x t i -1 n -1 n i=1 w 2 i σ 2 i x i x t i n -1 n i=1 w i x i x t i -1$.

The EHW robust covariance generalizes to

$Vehw,w = n -1 n -1 n i=1 w i x i x t i -1 n -1 n i=1 w 2 i ε2 w,i x i x t i n -1 n i=1 w i x i x t i -1$, where εw,i = y i -x t i βw is the residual from the WLS. Note that in the sandwich covariance, w i appears in the "bread" but w 2 i appears in the "meat." This formula appeared in Magee (1998) and [Romano and Wolf (2017)](#b201). The function hccm in the R package car can compute various EHW covariance estimators based on WLS. To save space in the examples below, I report only the standard errors based on the generalized Normal linear model and leave the calculations of the EHW covariances as a homework problem.

## WLS motivated by heteroskedasticity

## Feasible generalized least squares

Assume that ε has mean zero and covariance diag σ 2 1 , . . . , σ 2 n . If the σ 2 i 's are known, we can simply apply the WLS above; if they are unknown, we need to estimate them first. This gives the following feasible generalized least squares estimator (FGLS):

1. Run OLS of y i on x i to obtain the residuals εi . Then obtain the squared residuals ε2 i .

2. Run OLS of log(ε 2 i ) on x i to obtain the fitted values and exponentiate them to obtain (σ 2 i ) n i=1 ;

3. Run WLS of y i on x i with weights σ-2

$i to obtain βfgls = n i=1 σ-2 i x i x t i -1 n i=1 σ-2 i x i y i .$In Step 2, we can change the model based on our understanding of heteroskedasticity. Here I use the Boston housing data to compare the OLS and FGLS, with R code in code18.3.1.R. > library ( mlbench ) > data ( BostonHousing ) > ols . fit = lm ( medv ~. , data = BostonHousing ) > dat . res = BostonHousing > dat . res $ medv = log (( ols . fit $ residuals )^2 ) > t . res . ols = lm ( medv ~. , data = dat . res ) > w . fgls = exp ( -t . res . ols $ fitted . values ) > fgls . fit = lm ( medv ~. , weights = w . fgls , data = BostonHousing ) > ols . fgls = cbind ( summary ( ols . fit )$ coef [ , 1 : 3 ] , + summary ( fgls . fit )$ coef [ , 1 : 3 ]) > round ( ols . fgls , 3 ) Estimate Std . Error t value Estimate Std . Error t value ( Intercept ) 3 6 . 4 5 9 5 . 1 0 3 7 . 1 4 4 9 . 4 9 9 4 . 0 2 . 3 4 crim -0 . 1 0 8 0 . 0 3 3 -3 . 2 8 7 -0 . 0 8 1 0 . 0 -1 . 8 2 zn 0 . 0 4 6 0 . 0 1 4 3 . 3 8 2 0 . 0 3 0 0 . 0 2 . 6 7 indus 0 . 0 2 1 0 . 0 6 1 0 . 3 3 4 -0 . 0 3 5 0 . 0 -0 . 9 2 chas 1 2 . 6 8 7 0 . 8 6 2 3 . 1 1 8 1 . 4 6 2 1 . 1 1 . 3 1 nox -1 7 . 7 6 7 3 . 8 2 0 -4 . 6 5 1 -7 . 1 6 1 2 . 7 -2 . 5 7 rm

3 . 8 1 0 0 . 4 1 8 9 . 1 1 6 5 . 6 7 5 0 . 3 1 5 . 5 9 age 0 . 0 0 1 0 . 0 1 3 0 . 0 5 2 -0 . 0 4 4 0 . 0 -5 . 5 0 dis -1 . 4 7 6 0 . 1 9 9 -7 . 3 9 8 -0 . 9 2 7 0 . 1 -6 . 6 8 rad 0 . 3 0 6 0 . 0 6 6 4 . 6 1 3 0 . 1 7 0 0 . 0 3 . 3 1 tax -0 . 0 1 2 0 . 0 0 4 -3 . 2 8 0 -0 . 0 1 0 0 . 0 -4 . 1 4 ptratio -0 . 9 5 3 0 . 1 3 1 -7 . 2 8 3 -0 . 7 0 0 0 . 0 -7 . 4 5 b 0 . 0 0 9 0 . 0 0 3 3 . 4 6 7 0 . 0 1 4 0 . 0 6 . 5 4 lstat -0 . 5 2 5 0 . 0 5 1 -1 0 . 3 4 7 -0 . 1 5 8 0 . 0 -4 . 3 8

Unfortunately, the coefficients, including the point estimates and standard errors, from OLS and FGLS are quite different for several covariates. This suggests that the linear model may be misspecified. Otherwise, both estimators are consistent for the same true coefficient, and they should not be so different even in the presence of randomness.

The above FGLS estimator is close to [Wooldridge (2012, Chapter 8)](#). [Romano and Wolf (2017)](#b201) propose to regress log(max(δ 2 , ε2 i )) on log |x i1 |, . . . , log |x ip | to estimate the individual variances. Their modification has two features: first, they truncate the small residuals by a pre-specified positive number δ 2 ; second, their regressors are the logs of the absolute values of the original covariates. [Romano and Wolf (2017)](#b201) highlighted the efficiency gain from the  [2019](#)) proposed some improved versions of the FGLS estimator even if the variance function is misspecified. However, it is unusual for practitioners to use FGLS even though it can be more efficient than OLS. There are several reasons. First, the EHW standard errors are convenient for correcting the standard error of OLS under heteroskedasticity. Second, the efficiency gain is usually small, and it is even possible that the FGLS is less efficient than OLS when the variance function is misspecified. Third, the linear model is very likely to be misspecified, and if so, OLS and FGLS estimate different parameters. The OLS has the interpretations as the best linear predictor and the best linear approximation of the conditional mean, but the FGLS has more complicated interpretations when the linear model is wrong. Based on these reasons, we need to carefully justify the choice of FGLS over OLS in real data analyses.

## Aggregated data and ecological regression

In some case, (y i , x i ) come from aggregated data, for example, y i can be the average test score and x i can be the average parents' income of students within classroom i. If we believe that the student-level test score and parents' income follow a homoskedastic linear model, then the model based on the classroom average must be heteroskedastic, with the variance inversely proportional to the classroom size. In this case, a natural choice of weight is w i = n i , the classroom size. Below I use the lavoteall dataset from the R package ei. It contains the the fraction of black registered voters x, the fraction of voter turnout t, and the total number of people n in each Louisiana precinct. Figure [19](#fig_50).1 is the scatterplot. In this example, OLS and WLS give similar results although n varies a lot across precincts.

> library ( " ei " ) > data ( lavoteall ) > ols . fit = lm ( t ~x , data = lavoteall ) > wls . fit = lm ( t ~x , weights = n , data = lavoteall ) > compare = cbind ( summary ( ols . fit )$ coef [ , 1 : 3 ] , + summary ( wls . fit )$ coef [ , 1 : 3 ]) > round ( compare , 3 ) Estimate Std . Error t value Estimate Std . Error t value ( Intercept ) 0 . 7 1 1 0 . 0 0 2 4 0 8 . 2 1 1 0 . 7 0 6 0 . 0 0 2 4 2 1 . 6 6 2 x -0 . 0 8 3 0 . 0 0 4 -1 9 . 9 5 3 -0 . 0 8 0 0 . 0 0 4 -1 9 . 9 3 8

In the above, we can interpret the coefficient of x as the precinct-level relationship between the fraction of black registered voters and the fraction voting. Political scientists are interested in using aggregated data to infer individual voting behavior. Hypothetically, the precinct i has individual data {x ij , y ij : j = 1, . . . , n i } where x ij and y ij are the binary racial and voting status of individual (i, j) (i = 1, . . . , n; j = 1, . . . , n i ). However, we only observe the aggregated data {x i• , ȳi• , n i : i = 1, . . . , n}, where

$xi• = n -1 i ni j=1 x ij , ȳi• = n -1 i ni j=1 y ij$are the fraction of black registered voters and the fraction voting, respectively. Can we infer the individual voting behavior based on the aggregated data? In general, this is almost impossible. Under some assumptions, we can make progress. Goodman's ecological regression below is one possibility.

Assume that for precinct i = 1, . . . , n, we have

$y ij | x ij = 1 iid ∼ Bernoulli(p i1 ), y ij | x ij = 0 iid ∼ Bernoulli(p i0 ), (j = 1, . . . , n i ).$This is the individual-level model, where the p i1 's and p i0 's measure the association between race and voting. We further assume that they are random and independent of the x ij 's, with means

$E(p i1 ) = p 1 , E(p i0 ) = p 0 . (19.6)$Then we can decompose the aggregated outcome variable as

$ȳi• = n -1 i ni j=1 y ij = n -1 i ni j=1 {x ij y ij + (1 -x ij )y ij } = n -1 i ni j=1 {x ij p 1 + (1 -x ij )p 0 } + ε i = p 1 xi• + p 0 (1 -xi• ) + ε i ,$where

$ε i = n -1 i ni j=1 {x ij (y ij -p 1 ) + (1 -x ij )(y ij -p 0 )}.$So we have a linear relationship between the aggregated outcome and covariate ȳi

$• = p 1 xi• + p 0 (1 -xi• ) + ε i , where E(ε i | xi• ) = 0.$Goodman [(1953)](#) suggested to use the OLS of ȳi• on {x i• , (1 -xi• )} to estimate (p 1 , p 0 ), and [Goodman (1959)](#b128) suggested to use the corresponding WLS with weight n i since the variance of ε i has the magnitude n -1 i . Moreover, the variance of ε i has a rather complicated form of heteroskedasticity, so we should use the EHW standard error for inference. This is called Goodman's regression or ecological regression. The R code in code18.3.2.R implements ecological regression based on the lavoteall data.

> ols . fit = lm ( t ~0 + x + I ( 1 -x ) , data = lavoteall ) > wls . fit = lm ( t ~0 + x + I ( 1 -x ) , weights = n , data = lavoteall ) > compare = cbind ( summary ( ols . fit )$ coef [ , 1 : 3 ] , + summary ( wls . fit )$ coef [ , 1 : 3 ]) > round ( compare , 3 ) Estimate Std . Error t value Estimate Std . Error t value x 0 . 6 2 8 0 . 0 0 3 1 8 8 . 2 9 2 0 . 6 2 6 0 . 0 0 3 1 9 4 . 4 9 3 I ( 1 -x ) 0 . 7 1 1 0 . 0 0 2 4 0 8 . 2 1 1 0 . 7 0 6 0 . 0 0 2 4 2 1 . 6 6 2

The assumption in (19.6) is crucial, which can be too strong when the precinct level p i1 's and p i0 's vary in systematic but unobserved ways. When the assumption is violated, it is possible that the ecological regression yields the opposite result compared to the individual regression. This is called the ecological fallacy.

Another obvious problem of ecological regression is that the estimated coefficients may lie outside of the interval [0, 1]. Problem 19.18 gives an example. [Gelman et al. (2001)](#b125) gave an alternative set of assumptions justifying the ecological regression. [King (1997)](#b155) proposed some extensions. [Robinson (1950)](#b199) warned that the ecological correlation might not inform individual correlation. [Freedman et al. (1991)](#b113) warned that the assumptions underlying the ecological regression might not be plausible in practice.

## WLS with other motivations

WLS can be used in other settings unrelated to heteroskedasticity. I review two examples below.

## Local linear regression

Calculus tells us that locally we can approximate any smooth function f (x) by a linear function even though the original function can be highly nonlinear:

$f (x) ≈ f (x 0 ) + f ′ (x 0 )(x -x 0 )$when x is near x 0 . The left panel of Figure [19](#fig_50).2 shows that in the neighborhood of x 0 = 0.4, even a sine function can be well approximated by a line. Based on data (x i , y i ) n i=1 , if we want to predict the mean value of y given x = x 0 , then we can predict based on a line with the local data points close to x 0 . It is also reasonable to down weight the points that are far from x 0 , which motivates the following WLS:

$( α, β) = arg min a,b n i=1 w i {y i -a -b(x i -x 0 )} 2 with w i = K {(x i -x 0 )/$h} where K(•) is called the kernel function and h is called the bandwidth parameter. With the fitted line ŷ(x) = α + β(x -x 0 ), the predicted value at x = x 0 is the intercept α.

Technically, K(•) can be any density function, and two canonical choices are the standard Normal density and the Epanechikov kernel K(t) = 0.75(1 -t 2 )1(|t| ≤ 1). The choice of q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0.0 0.2 0.4 0.6 0.8 1.0 -2 -1 0 1 2 local linear approximation x y q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0.0 0.2 0.4 0.6 0.8 1.0 With large bandwidth, we have a poor linear approximation, leading to bias; with small bandwidth, we have few data points, leading to large variance. In practice, we face a biasvariance trade-off. In practice, we can either use cross-validation or other criterion to select h.

In general, we can approximate a smooth function by a polynomial:

$f (x) ≈ K k=0 f (k) (x 0 ) k! (x -x 0 ) k$when x is near x 0 . So we can even fit a polynomial function locally, which is called local polynomial regression [(Fan and Gijbels, 1996)](#b97). In the R package Kernsmooth, the function locpoly fits local polynomial regression, and the function dpill selects h based on [Ruppert et al. (1995)](#b205). The default specification of locpoly is the local linear regression.

> library ( " KernSmooth " ) > n = 5 0 0 > x = seq ( 0 , 1 , length . out = n ) > fx = sin ( 8 * x ) > y = fx + rnorm (n , 0 , 0 . 5 ) > plot ( y ~x , pch = 1 9 , cex = 0 . 2 , col = " grey " , bty = " n " , + main = " local ␣ linear ␣ fit " , font . main = 1 ) > lines ( fx ~x , lwd = 2 , col = " grey " ) > h = dpill (x , y ) > locp . fit = locpoly (x , y , bandwidth = h ) > lines ( locp . fit , lty = 2 )

## Regression with survey data

Most discussions in this book are based on i.i.d. samples, or, at least, the sample represents the population of interest. Sometimes, researchers over sample some units and under sample some other units from a population of interest. x i y i . However, we do not have all the data points in the large population, but sample each data point independently with probability

## sampling with probability

$π i = pr(I i = 1 | x i , y i ),$where I i is a binary indicator for being included in the sample. Conditioning on

$X N = (x i ) N i=1 and Y N = (y i ) N i=1$, βideal is a fixed number, and an estimator is the following WLS estimator

$β1/π = N i=1 I i π i x i x t i -1 N i=1 I i π i x i y i = n i=1 π -1 i x i x t i -1 n i=1 π -1 i x i y i ,$with weights inversely proportional to the sampling probability. This inverse probability weighting estimator is reasonable because

$E N i=1 I i π i x i x t i | X N , Y N = N i=1 x i x t i , E N i=1 I i π i x i y i | X N , Y N = N i=1$x i y i .

The inverse probability weighting estimators are called the Horvitz-Thompson estimators [(Horvitz and Thompson, 1952)](#b144), which are the cornerstones of survey sampling. Below I use the dataset census00.dta to illustrate the use of sampling weight, which is the perwt variable. The R code is in code18.4.2.R. > library ( foreign ) > census 0 0 = read . dta ( " census 0 0 . dta " ) > ols . fit = lm ( logwk ~age + educ + exper + exper 2 + black , + data = census 0 0 ) > wls . fit = lm ( logwk ~age + educ + exper + exper 2 + black , + weights = perwt , data = census 0 0 ) > compare = cbind ( summary ( ols . fit )$ coef

[ , 1 : 3 ] , + summary ( wls . fit )$ coef [ , 1 : 3 ]) > round ( compare , 4 ) Estimate Std . Error t value Estimate Std . Error t value ( Intercept ) 5 . 1 6 6 7 0 . 1 2 8 2 4 0 . 3 5 . 0 7 4 0 0 . 1 2 6 8 4 0 . 0 age -0 . 0 1 4 8 0 . 0 0 6 7 -2 . 2 -0 . 0 0 8 4 0 . 0 0 6 7 -1 . 3 educ 0 . 1 2 9 6 0 . 0 0 6 6 1 9 . 7 0 . 1 2 2 8 0 . 0 0 6 5 1 8 . 8 exper 2 0 . 0 0 0 3 0 . 0 0 0 1 2 . 2 0 . 0 0 0 2 0 . 0 0 0 1 1 . 3 black -0 . 2 4 6 7 0 . 0 0 8 5 -2 9 . 2 -0 . 2 5 7 4 0 . 0 0 8 0 -3 2 . 0 19.5 Homework problems 19.1 A linear algebra fact related to WLS This problem extends Corollary 19.1. Show that

$(X t Σ -1 X) -1 ⪯ (X t ΩX) -1 X t ΩΣΩX(X t ΩX) -1 .$When Ω = Σ -1 , the equality holds.

## Generalized least squares with a block diagonal covariance

Partition X and Y into

$X =    X 1 . . . X K    , Y =    Y 1 . . . Y K    corresponding to Σ in (19.3) such that X k ∈ R n k ×p and Y k ∈ R n k . Show that the generalized least squares estimator is βΣ = K k=1 X t k Σ -1 k X k -1 K k=1 X t k Σ -1 k Y k .$
## Univariate WLS

Prove the following Galtonian formula for the univariate WLS: 

## Difference-in-means with weights

With a binary covariate x i , show that the coefficient of x i in the WLS of y i on (1, x i ) with weights w i (i = 1, . . . , n) equals ȳw,1 -ȳw,0 , where

$ȳw,1 = n i=1 w i x i y i n i=1 w i x i , ȳw,0 = n i=1 w i (1 -x i )y i n i=1 w i (1 -x i )$are the weighted averages of the outcome under treatment and control, respectively.

Hint: You can use the result in Problem 19.3.

## Asymptotic Normality of WLS and robust covariance estimator

Under the heteroskedastic linear model, show that βw is consistent and asymptotically Normal, and show that n Vw is consistent for the asymptotic covariance of √ n( βw -β). Specify the regularity conditions.

## WLS in ANOVA

This problem extends Problems 5.5 and 6.3.

For units i = 1, . . . , n, assume y i denotes the outcome, x i denotes the p-vector with entries as the dummy variables for a discrete covariate with p levels, w i > 0 denotes a weight, and π i > 0 denotes another weight that is a function of x i only (for example, π i = n j /n if x i = e j ). Run the following regressions:

• WLS of y i on x i with weight w i for i = 1, . . . , n to obtain the coefficient vector β and EHW covariance matrix V ;

• WLS of y i on x i with weight w i π i for i = 1, . . . , n to obtain the coefficient vector β′ and EHW covariance matrix V ′ .

Show that β = β′ with the jth entry βj = β′ j = i:xi=ej w i y i i:xi=ej w i , and moreover, V = V ′ are diagonal with the (j, j)th entry

$Vjj = V ′ jj = i:xi=ej w 2 i (y i -βj ) 2 ( i:xi=ej w i ) 2$.

## An infeasible generalized least squares estimator

Can we skip Step 2 in Section 19.3.1 and directly apply the following WLS estimator:

$βigls = n i=1 ε-2 i x i x t i -1 n i=1 ε-2 i x i y i$with εi = y i -x t i β. If so, give a theoretical justification; if not, give a counterexample. Evaluate the finite-sample properties of βigls using simulated data.

## FWL theorem in WLS

This problem is an extension of Theorem 7.1.

Consider the WLS with an n × 1 vector Y , an n × k matrix X 1 , an n × l matrix X 2 , and weights w i 's. Show that βw,2 in the long WLS fit Y = X 1 βw,1 + X 2 βw,2 + εw equals the coefficient of Xw,2 in the WLS fit of Ỹw on Xw,2 , where Xw,2 are the residual vectors from the column-wise WLS of X 2 on X 1 , and Ỹw is the residual vector from the WLS of Y on X 1 .

## The sample version of Cochran's formula in WLS

This problem is an extension of Theorem 9.1.

Consider the WLS with an n × 1 vector Y , an n × k matrix X 1 , an n × l matrix X 2 , and weights w i 's. We can fit the following WLS:

$Y = X 1 βw,1 + X 2 βw,2 + εw , Y = X 2 βw,2 + εw , X 1 = X 2 δw + Ûw ,$where εw , εw , Ûw are the residuals. The last WLS fit means the WLS fit of each column of X 1 on X 2 . Similar to Theorem 9.1, we have βw,2 = βw,2 + δw βw,1 .

Prove this result.

## EHW robust covariance estimator in WLS

We have shown in Section 19.1 that the coefficients from WLS are identical to those from OLS with transformed variables. Further show that the corresponding HC0 version of EHW covariance estimators are also identical.

## Invariance of covariance estimators in WLS

Problem 6.4 states the invariance of covariance estimators in OLS. Show that the same result holds for covariance estimators in WLS.

## Ridge with weights

Define the ridge regression with weights w i 's, and derive the the formula for the ridge coefficient.

## Coordinate descent algorithm in lasso with weights

Define the lasso with weights w i 's, and give the coordinate descent algorithm for solving the weighted lasso problem.

## General leave-one-out formula via WLS

With data (X, Y ), we can define β[-i] (w) as the WLS estimator of Y on X with weights w i ′ = 1(i ′ ̸ = i) + w1(i ′ = i) for i ′ = 1, . . . , n, where 0 ≤ w ≤ 1. It reduces to the OLS estimator β when w = 1 and the leave-one-out OLS estimator β[-i] when w = 0.

Show the general formula

$β[-i] (w) = β - 1 -w 1 -(1 -w)h ii (X t X) -1 x i εi$recalling that h ii is the leverage score and εi is the residual of observation i. Remark: Based on the above formula, we can compute the derivative of β[-i] (w) with respect to w: [Pregibon (1981)](#b193) reviewed related formulas for OLS. [Broderick et al. (2020)](#b65) discussed related formulas for general statistical models.

$∂ β[-i] (w) ∂w = 1 {1 -(1 -w)h ii } 2 (X t X) -1 x i εi , which reduces to ∂ β[-i] (0) ∂w = 1 (1 -h ii ) 2 (X t X) -1 x i εi at w = 0 and ∂ β[-i] (1) ∂w = (X t X) -1 x i εi at w = 1.$
## Hat matrix and leverage score in WLS

Based on the WLS estimator βw = (X t W X) -1 X t W Y with W = diag(w 1 , . . . , w n ), we have the predicted vector

$Ŷw = X βw = X(X t W X) -1 X t W Y.$This motivates the definition of the hat matrix

$H w = X(X t W X) -1 X t W such that Ŷw = H w Y .$First, show the following basic properties of the hat matrix:

$W H w = H t w W, X t W (I n -H w ) = 0.$Second, prove an extended version of Theorem 11.1: with

$x i = (1, x t i2 ) t , the (i, i)th diagonal element of H w satisfies h w,ii = w i n i ′ =1 w i ′ (1 + D 2 w,i ) where D 2 w,i = (x i2 -xw,2 ) t S -1 w (x i2 -xw,2 ) with xw,2 = n i=1 w i x i2 / n i=1 w i being the weighted average of x i2 's and S w = n i=1 w i (x i2 -xw,2 )(x i2 -xw,2 ) t / n i=1 w i being the corresponding sample covariance ma- trix.$Remark: [Li and Valliant (2009)](#b164) presented the basic properties of H w for WLS in the context of survey data.

## Leave-one-out formula for WLS

Use the notation in Problem 19.15. Let βw be the WLS estimator of Y on X with weights w i 's. Let βw[-i] be the WLS estimator without using the ith observation. Show that

$βw[-i] = βw - w i 1 -h w,ii (X t W X) -1 x i εw,i .$19.17 EHW standard errors in WLS

Report the EHW standard errors in the examples in Sections 19.3.1, 19.3.2, and 19.4.2.

## Another example of ecological inference

The fultongen dataset in the ri package contains aggregated data from 289 precincts in Fulton County, Georgia. The variable t represents the fraction voting in 1994 and x the fraction in 1992. The variable n represents the total number of people. Run ecological regression similar to Section 19.3.2.

## Part VII

Generalized Linear Models 20

## Logistic Regression for Binary Outcomes

Many applications have binary outcomes y i ∈ {0, 1}. This chapter discusses statistical models of binary outcomes, focusing on the logistic regression.

## Regression with binary outcomes

## Linear probability model

For simplicity, we can still use the linear model for a binary outcome. It is also called the linear probability model:

$y i = x t i β + ε i , E(ε i | x i ) = 0$because the conditional probability of y i given x i is a linear function of x i :

$pr(y i = 1 | x i ) = E(y i | x i ) = x t i β.$An advantage of this linear model is that the interpretation of the coefficient remains the same as linear models for general outcomes:

$∂pr(y i = 1 | x i ) ∂x ij = β j ,$that is, β j measures the partial impact of x ij on the probability of y i .

A minor technical issue is that the linear probability model implies heteroskedasticity because var(

$y i | x i ) = x t i β(1 -x t i β)$. Therefore, we must use the EHW covariance based on OLS. We can also use FGLS to improve efficiency over OLS.

A more severe problem with the linear probability model is its plausibility in general. We may not believe that a linear model is the correct model for a binary outcome because the probability pr(y i = 1 | x i ) on the left-hand side is bounded between zero and one, but the linear combination x t i β on the right-hand side can be unbounded for general covariates and coefficient. Nevertheless, the OLS decomposition y i = x t i β + ε i works for any y i ∈ R, so it is applicable for binary y i . Sometimes, practitioners feel that the linear model is not natural for binary outcomes because the predicted value can be outside the range of [0, 1]. Therefore, it is more reasonable to build a model that automatically accommodates the binary feature of the outcome.

## General link functions

A linear combination of general covariates may be outside the range of [0, 1], but we can find a monotone transformation to force it to lie within the interval [[0,](#)[1]](#). This motivates us to consider the following model:

$pr(y i = 1 | x i ) = g(x t i β),$where g(•) : R → [0, 1] is a monotone function, and its inverse is often called the link function. Mathematically, the distribution function of any continuous random variable is a monotone function that maps from R to [0, 1]. So we have infinitely many choices for g(•). Four canonical choices "logit", "probit", "cauchit", and "cloglog" are below which are the standard options in R:

$name functional form logit g(z) = e z 1+e z probit g(z) = Φ(z) cauchit g(z) = 1 π arctan(z) + 1 2 cloglog g(z) = 1 -exp(-e z )$The above g(z)'s correspond to different distribution functions. The g(z) for the logit model[foot_18](#foot_18) is the distribution function of the standard logistic distribution with density

$g ′ (z) = e z (1 + e z ) 2 = g(z) {1 -g(z)} .$(20.1)

The g(z) for the probit model[foot_19](#foot_19) is the distribution function of a standard Normal distribution. The g(z) for the cauchit model is the distribution function of the standard Cauchy distribution with density

$g ′ (z) = 1 π(1 + z 2 )$.

The g(z) for the cloglog model is the distribution function of the standard log-Weilbull distribution with density g ′ (z) = exp(z -e z ).

I will give more motivations for the first three link functions in Section 20.7.1 and for the fourth link function in Problem 22.4. Figure [20](#fig_52).1 shows the distributions and densities of the corresponding link functions. The distribution functions are quite similar for all links, but the density for cloglog is asymmetric although all other three densities are symmetric. This chapter will focus on the logit model, and extensions to other models are conceptually straightforward. We can also write the logit model as

$pr(y i = 1 | x i ) ≡ π(x i , β) = e x t i β 1 + e x t i β , (20.2)$for the conditional probability of y i given x i , or, equivalently, logit {pr(

$y i = 1 | x i )} ≡ log pr(y i = 1 | x i ) 1 -pr(y i = 1 | x i ) = x t i β,$for the log of the odds of y i given x i , with the logit function logit(π) = log π 1 -π . Because y i is a binary random variable, its probability completely determines its distribution. So we can also write the logit model as

$y i | x i ∼ Bernoulli e x t i β 1 + e x t i β$.

Each coefficient β j measures the impact of x ij on the log odds of the outcome:

$∂ ∂x ij logit{pr(y i = 1 | x i )} = β j .$Epidemiologists also call β j the conditional log odds ratio because

$β j = logit {pr(y i = 1 | . . . , x ij + 1, . . .)} -logit {pr(y i = 1 | . . . , x ij , . . .)} = log pr(y i = 1 | . . . , x ij + 1) 1 -pr(y i = 1 | . . . , x ij + 1) -log pr(y i = 1 | . . . , x ij , . . .) 1 -pr(y i = 1 | . . . , x ij , . . .) = log pr(y i = 1 | . . . , x ij + 1, . . .) 1 -pr(y i = 1 | . . . , x ij + 1, . . .) pr(y i = 1 | . . . , x ij , . . .) 1 -pr(y i = 1 | . . . , x ij , . . .) ,$that is, the change of the log odds of y i if we increase x ij by a unit holding other covariates unchanged. Qualitatively, if β j > 0, then larger values of x ij lead to larger probability of y i = 1; if β j < 0, then larger values of x ij lead to smaller probability of y i = 1.

## Maximum likelihood estimator of the logistic model

Because we have specified a fully parametric model for y i given x i , we can estimate β using the maximum likelihood. With independent observations, the likelihood function for general binary outcomes is[foot_20](#foot_20)

$L(β) = n i=1 f (y i | x i ) = n i=1 {π(x i , β) if y i = 1 or 1 -π(x i , β) if y i = 0} = n i=1 {π(x i , β)} yi {1 -π(x i , β)} 1-yi .$Under the logit form [(20.2)](#), the likelihood function simplifies to

$L(β) = n i=1 π(x i , β) 1 -π(x i , β) yi {1 -π(x i , β)} = n i=1 e x t i β yi 1 1 + e x t i β = n i=1 e yix t i β 1 + e x t i β .$The log-likelihood function is

$log L(β) = n i=1 y i x t i β -log(1 + e x t i β ) ,$the score function is

$∂ log L(β) ∂β = n i=1 x i y i - x i e x t i β 1 + e x t i β = n i=1 x i y i - e x t i β 1 + e x t i β = n i=1 x i {y i -g(x t i β)} = n i=1 x i {y i -π(x i , β)} ,$and the Hessian matrix

$∂ 2 log L(β) ∂β∂β t = ∂ 2 log L(β) ∂β j ∂β j ′ 1≤j,j ′ ≤p = - n i=1 x i ∂g(x t i β) ∂β t (20.1) = - n i=1 x i x t i g(x t i β) {1 -g(x t i β)} = - n i=1 π(x i , β) {1 -π(x i , β)} x i x t i .$For any α ∈ R p , we have

$α t ∂ 2 log L(β) ∂β∂β t α = - n i=1 π(x i , β) {1 -π(x i , β)} (α t x i ) 2 ≤ 0$so the Hessian matrix is negative semi-definite. If it is negative definite, then the likelihood function has a unique maximizer.

The maximum likelihood estimate (MLE) must satisfy the following score or Normal equation:

$n i=1 x i y i -π(x i , β) = n i=1 x i y i - e x t i β 1 + e x t i β = 0.$If we view π(x i , β) as the fitted probability for y i , then y i -π(x i , β) is the residual, and the score equation is similar to that of OLS. Moreover, if x i contains 1, then

$n i=1 y i -π(x i , β) = 0 =⇒ n -1 n i=1 y i = n -1 n i=1 π(x i , β),$that is the average of the outcomes equals the average of their fitted values. However, the score equation is nonlinear, and in general, there is no explicit formula for the MLE. We usually use Newton's method to solve for the MLE based on the linearization of the score equation. Starting from the old value β old , we can approximate the score equation by a linear equation:

$0 = ∂ log L(β) ∂β ∼ = ∂ log L(β old ) ∂β + ∂ 2 log L(β old ) ∂β∂β t (β -β old ),$and then update

$β new = β old - ∂ 2 log L(β old ) ∂β∂β t -1 ∂ log L(β old )$∂β .

Using the matrix form, we can gain more insight from Newton's method. Recall that

$Y =    y 1 . . . y n    , X =    x t 1 . . . x t n    ,$and define

$Π old =    π(x 1 , β old ) . . . π(x n , β old )    , W old = diag π(x i , β old ) 1 -π(x i , β old ) n i=1 .$Then

$∂ log L(β old ) ∂β = X t (Y -Π old ), ∂ 2 log L(β old ) ∂β∂β t = -X t W old X,$and Newton's method simplifies to

$β new = β old + (X t W old X) -1 X t (Y -Π old ) = (X t W old X) -1 X t W old Xβ old + X t (Y -Π old ) = (X t W old X) -1 X t W old Z old ,$where

$Z old = Xβ old + (W old ) -1 (Y -Π old ).$So we can obtain β new based on the WLS fit of Z old on X with weights W old , the diagonal elements of which are the conditional variances of the y i 's given the x i 's at β old . The glm function in R uses the Fisher scoring algorithm, which is identical to Newton's method for the logit model 4 . Sometimes, it is also called the iteratively reweighted least squares algorithm.

## Statistics with the logit model

## Inference

Based on the general theory of MLE, β is consistent for β and is asymptotically Normal. Approximately, we can conduct statistical inference based on

$β a ∼ N    β, - ∂ 2 log L( β) ∂β∂β t -1    = N β, (X t Ŵ X) -1 , where Ŵ = diag π(x i , β) 1 -π(x i , β) n i=1$.

Based on this, the glm function reports the point estimate, standard error, z-value, and pvalue for each coordinate of β. It is almost identical to the output of the lm function, except that the interpretation of the coefficient becomes the conditional log odds ratio. I use the data from [Hirano et al. (2000)](#b139) to illustrate logistic regression, where the main interest is the effect of the encouragement of receiving the flu shot via email on the binary indicator of flu-related hospitalization. We can fit a logistic regression using the glm function in R with family = binomial(link = logit). 4 The Fisher scoring algorithm uses a slightly different approximation:

$0 = ∂ log L(β) ∂β ∼ = ∂ log L(β old ) ∂β + E ∂ 2 log L(β old ) ∂β∂β t | X (β -β old ),$with the expected Fisher information instead of the observed Fisher information. For other link functions, the Fisher scoring algorithm is different from Newton's method. age -0 . 0 0 7 9 8 6 0 . 0 0 5 5 6 9 -1 . 4 3 4 0 . 1 5 1 5 4 copd 0 . 3 3 7 0 3 7 0 . 1 5 3 9 3 9 2 . 1 8 9 0 . 0 2 8 5 7 * dm 0 . 4 5 4 3 4 2 0 . 1 4 3 5 9 3 3 . 1 6 4 0 . 0 0 1 5 6 ** heartd 0 . 6 7 6 1 9 0 0 . 1 5 3 3 8 4 4 . 4 0 8 1 . 0 4e -0 5 *** race -0 . 2 4 2 9 4 9 0 . 1 4 3 0 1 3 -1 . 6 9 9 0 . 0 8 9 3 6 . renal 1 . 5 1 9 5 0 5 0 . 3 6 5 9 7 3 4 . 1 5 2 3 . 3 0e -0 5 *** sex -0 . 2 1 2 0 9 5 0 . 1 4 4 4 7 7 -1 . 4 6 8 0 . 1 4 2 1 0 liverd 0 . 0 9 8 9 5 7 1 . 0 8 4 6 4 4 0 . 0 9 1 0 . 9 2 7 3 1 ( Dispersion parameter for binomial family taken to be 1 )

Null deviance : 1 6 6 7 . 9 on 2 8 6 0 degrees of freedom Residual deviance : 1 5 9 8 . 4 on 2 8 5 1 degrees of freedom AIC : 1 6 1 8 . 4

## Number of Fisher Scoring iterations : 5

Three subtle issues arise in the above code. First, flu = within(flu, rm(receive)) drops receive, which is the indicator of whether a patient received the flu shot or not. The reason is that assign is randomly assigned but receive is subject to selection bias, that is, patients receiving the flu shot can be quite different from patients not receiving the flu shot.

Second, the Null deviance and Residual deviance are defined as -2 log L( β) and -2 log L( β), respectively, where β is the MLE assuming that all coefficients except the intercept are zero, and β is the MLE without any restrictions. They are not of independent interest, but their difference is: Wilks' theorem ensures that

${-2 log L( β)} -{-2 log L( β)} = 2 log L( β) L( β) a ∼ χ 2 p-1 .$So we can test whether the coefficients of the covariates are all zero, which is analogous to the joint F test in linear models.

> pchisq ( assign . logit $ null . deviance -assign . logit $ deviance , + df = assign . logit $ df . null -assign . logit $ df . residual , + lower . tail = FALSE ) [ 1 ] 1 . 9 1 2 9 5 2e -1 1 Third, the AIC is defined as -2 log L( β) + 2p, where p is the number of parameters in the logit model. This is also the general formula of AIC for other parametric models.

## Prediction

The logit model is often used for prediction or classification since the outcome is binary. With the MLE β, we can predict the probability of being one as πn+1 = g(x t n+1 β) for a unit with covariate value x n+1 , and we can easily dichotomize the fitted probability to predict the outcome itself by ŷn+1 = 1(π n+1 ≥ c), for example, with c = 0.5.

We can even quantify the uncertainty in the fitted probability based on a linear approximation (i.e., the delta method). Based on

$πn+1 = g(x t n+1 β) ∼ = g(x t n+1 β) + g ′ (x t n+1 β)x t n+1 ( β -β) = g(x t n+1 β) + g(x t n+1 β){1 -g(x t n+1 β)}x t n+1 ( β -β),$we can approximate the asymptotic variance of πn+1 by

$[g(x t n+1 β){1 -g(x t n+1 β)}] 2 x t n+1 (X t Ŵ X) -1 x n+1 .$We can use the predict function in R to calculate the predicted values based on a glm object in the same way as the linear model. If we specify type="response", then we obtain the fitted probabilities; if we specify se.fit = TRUE, then we also obtain the standard errors of the fitted probabilities. In the following, I predict the probabilities of flu-related hospitalization if a patient receives the email encouragement or not, fixing other covariates at their empirical means.

> emp . mean = apply ( flu , 2 , mean ) > data . ave = rbind ( emp . mean , emp . mean ) > data . ave [ 1 , 1 ] = 1 > data . ave [[ 2 , 1 ]](#) = 0 > data . ave = data . frame ( data . ave ) > predict ( assign . logit , newdata = data . ave , + type = " response " , se . fit = TRUE ) $ fit emp . mean emp . mean . 1 0 . 0 6 9 8 1 8 2 8 0 . 0 8 3 7 8 8 1 8 $ se . fit emp . mean emp . mean . 1 0 . 0 0 6 6 8 9 6 6 5 0 . 0 0 7 5 2 6 3 0 7

## More on interpretations of the coefficients

Many practitioners find the coefficients in the logit model difficult to interpret. Another measure of the impact of the covariate on the outcome is the average marginal effect or average partial effect. For a continuous covariate x ij , the average marginal effect is defined as

$ame j = n -1 n i=1 ∂pr(y i = 1 | x i ) ∂x ij = n -1 n i=1 g ′ (x t i β)β j ,$which reduces to the following form for the logit model

$ame j = β j × n -1 n i=1 π(x i , β) {1 -π(x i , β)} .$For a binary covariate x ij , the average marginal effect is defined as

$ame j = n -1 n i=1 {pr(y i = 1 | . . . , x ij = 1, . . .) -pr(y i = 1 | . . . , x ij = 0, . . .)}$The margins function in the margins package can compute the average marginal effects and the corresponding standard errors. In particular, the average marginal effect of assign is not significant as shown below. The R code in this section is in code19.3.R.

> library ( " margins " ) > ape = margins ( assign . logit ) > summary ( ape ) factor AME SE z p lower upper age -0 . 0 0 0 6 0 . 0 0 0 4 -1 . 4 3 2 2 0 . 1 5 2 1 -0 . 0 0 1 4 0 . 0 0 0 2 assign -0 . 0 1 5 0 0 . 0 1 0 3 -1 . 4 4 8 0 0 . 1 4 7 6 -0 . 0 3 5 2 0 . 0 0 5 3 copd 0 . 0 2 5 5 0 . 0 1 1 7 2 . 1 8 3 0 0 . 0 2 9 0 0 . 0 0 2 6 0 . 0 4 8 5 dm 0 . 0 3 4 4 0 . 0 1 0 9 3 . 1 4 6 5 0 . 0 0 1 7 0 . 0 1 3 0 0 . 0 5 5 9 heartd 0 . 0 5 1 2 0 . 0 1 1 8 4 . 3 4 4 1 0 . 0 0 0 0 0 . 0 2 8 1 0 . 0 7 4 3 liverd 0 . 0 [0 7 5 0 . 0 8 2 2 0 . 0 9 1 2 0 . 9 2 7 3 -0 . 1 5 3 6 0 . 1 6 8 6](#) race -0 . 0 1 8 4 0 . 0 1 0 9 -1 . 6 9 5 8 0 . 0 8 9 9 -0 . 0 3 9 7 0 . 0 0 2 9 renal 0 . [1 1 5 1 0 . 0 2 7 8 4 . 1 4 6 1 0 . 0 0 0 0 0 . 0 6 0 7 0 . 1 6 9 6](#) sex -0 . 0 1 6 1 0 . 0 1 1 0 -1 . 4 6 6 0 0 . 1 4 2 6 -0 . 0 3 7 6 0 . 0 0 5 4

The interaction term is much more complicated. Contradictory suggestions are given across fields. Consider the following model

$pr(y i = 1 | x i1 , x i2 ) = g(β 0 + β 1 x i1 + β 2 x i2 + β 12 x i1 x i2 ).$If the link is logit, then epidemiologists interpret e β12 as the interaction between x i1 and x i2 on the odds ratio scale. Consider a simple case with binary x i1 and x i2 . Given x i2 = 1, the odds ratio of x i1 on y i equals e β1+β12 ; given x i2 = 0, the odds ratio of x i1 on y i equals e β1 . Therefore, the ratio of the two odds ratio equals e β12 . When we measure effects on the odds ratio scale, the logistic model is a natural choice. The interaction term in the logistic model indeed measures the interaction of x i1 and x i2 . [Ai and Norton (2003)](#b46) gave a different suggestion. Define

$z i = β 0 + β 1 x i1 + β 2 x i2 + β 12 x i1 x i2 .$We have two ways to define the interaction effect: first,

$n -1 n i=1 ∂pr(y i = 1 | x i1 , x i2 ) ∂(x i1 x i2 ) = n -1 n i=1 g ′ (z i )β 12 .$second,

$n -1 n i=1 ∂ 2 pr(y i = 1 | x i1 , x i2 ) ∂x i1 ∂x i2 = n -1 n i=1 ∂ ∂x i2 ∂pr(y i = 1 | x i1 , x i2 ) ∂x i1 = n -1 n i=1 ∂ ∂x i2 {g ′ (z i )(β 1 + β 12 x i2 )} = n -1 n i=1 {g ′′ (z i )(β 2 + β 12 x i1 )(β 1 + β 12 x i2 ) + g ′ (z i )β 12 } ;$Although the first one is more straightforward based on the definition of the average partial effect, the second one is more reasonable based on the natural definition of interaction based on the mixed derivative. Note that even if β 12 = 0, the second definition of interaction does not necessarily equal 0 since

$n -1 n i=1 ∂ 2 pr(y i = 1 | x i1 , x i2 ) ∂x i1 ∂x i2 = n -1 n i=1 g ′′ (z i )β 1 β 2 .$This is due to the nonlinearity of the link function. The second definition quantifies interaction based on the probability itself while the parameters in the logistic model measure the odds ratio. This combination of model and parameter does not seem a natural choice.

## Does the link function matter?

First, I generate data from a simple one-dimensional logistic model.

> n = 1 0 0 > x = rnorm (n , 0 , 3 ) > prob = 1 /( 1 + exp ( -1 + x )) > y = rbinom (n , 1 , prob )

Then I fit the data with the linear probability model and binary models with four link functions.

## > lpmfit

= lm ( y ~x ) > probitfit = glm ( y ~x , family = binomial ( link = " probit " )) Warning message : glm . fit : fitted probabilities numerically 0 or 1 occurred > logitfit = glm ( y ~x , family = binomial ( link = " logit " )) > cloglogfit = glm ( y ~x , family = binomial ( link = " cloglog " )) Warning message : glm . fit : fitted probabilities numerically 0 or 1 occurred > cauchitfit = glm ( y ~x , family = binomial ( link = " cauchit " ))

The coefficients are quite different because the coefficients measure the association between x and y on difference scales. These parameters are not directly comparable.

$> betacoef = c ( lpmfit $ coef [ 2 ] , + probitfit $ coef [ 2 ] , + logitfit $ coef [ 2 ] , + cloglogfit $ coef [ 2 ] , + cauchitfit $ coef [ 2 ]$) > names ( betacoef ) = c ( " lpm " , " probit " , " logit " , " cloglog " , " cauchit " ) > round [( betacoef , 2 )](#) lpm probit logit cloglog cauchit -0 . 1 0 -0 . 8 3 -1 . 4 7 -1 . 0 7 -2 . 0 9

However, if we care only about the prediction, then these five models give very similar results.

> table (y , lpmfit $ fitted . values > 0 . 5 ) y FALSE TRUE 0 3 1 9 1 5 5 5 > table (y , probitfit $ fitted . values > 0 . 5 ) y FALSE TRUE 0 3 1 9 1 5 5 5 > table (y , logitfit $ fitted . values > 0 . 5 ) y FALSE TRUE 0 3 1 9 1 5 5 5 > table (y , cloglogfit $ fitted . values > 0 . 5 ) y FALSE TRUE 0 3 4 6 1 7 5 3 > table (y , cauchitfit $ fitted . values > 0 . 5 ) y FALSE TRUE 0 3 4 6 1 7 5 3

q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq lpm probit logit cloglog cauchit 0.00 0.25 0.50 0.75 1.000.00 0.25 0.50 0.75 1.000.00 0.25 0.50 0.75 1.000.00 0.25 0.50 0.75 1.000.00 0. An interesting fact is that the coefficients from the logit model approximately equal those from the probit model multiplied by 1.7, a constant that minimizes max y |g logit (by)g probit (y)|. We can easily compute this constant numerically: Based on the above calculation, the maximum difference is approximately 0.009. Therefore, the logit and probit link functions are extremely close up to the scaling factor 1.7. However, min b max y |g logit (by) -g * (y)| is much larger for the link functions of cauchit and cloglog. 

## Extensions of the logistic regression

$where ℓ i (β) = y i (β 0 + β 1 x i1 + • • • + β p x ip ) -log(1 + e β0+β1xi1+•••+βpxip )$is the log-likelihood function. When α = 1, it gives the ridge analog of the logistic regression; when α = 0, it gives the lasso analog; when α ∈ (0, 1), it gives the elastic net analog. The R package glmpath uses the coordinate descent algorithm based on a quadratic approximation of the log-likelihood function. We can select the tuning parameter λ based on cross-validation.

## Case-control study

A nice property of the logit model is that it works not only for the cohort study with data from conditional distribution y i | x i but also for the case-control study with data from the conditional distribution x i | y i . The former is a prospective study while the latter is a retrospective study. Below, I will explain the basic idea in [Prentice and Pyke (1979)](#b194).

Assume that (x i , y i , s i ) IID with

$pr(y i = 1 | x i ) = e β0+x t i β 1 + e β0+x t i β (20.3) and pr(s i = 1 | x i , y i ) = pr(s i = 1 | y i ) = p 1 , if y i = 1, p 0 , if y i = 0. (20.4)$But we only have data with s i = 1 with p 1 and p 0 often unknown. Fortunately, conditioning on s i = 1, we have the following result.

Theorem 20.1 Under (20.3) and (20.4), we have

$pr(y i = 1 | x i , s i = 1) = e δ+β0+x t i β$1 + e δ+β0+x t i β , where δ = log(p 1 /p 0 ).

## Proof of Theorem 20.1: We have pr(y

$i = 1 | x i , s i = 1) = pr(y i = 1 | x i )pr(s i = 1 | x i , y i = 1) pr(y i = 1 | x i )pr(s i = 1 | x i , y i = 1) + pr(y i = 0 | x i )pr(s i = 1 | x i , y i = 0)$by Bayes' formula. Under the logit model, we have

$pr(y i = 1 | x i , s i = 1) = e β 0 +x t i β 1+e β 0 +x t i β p 1 e β 0 +x t i β 1+e β 0 +x t i β p 1 + 1 1+e β 0 +x t i β p 0 = e β0+x t i β p 1 e β0+x t i β p 1 + p 0 = e β0+x t i β p 1 /p 0 e β0+x t i β p 1 /p 0 + 1 = e δ+β0+x t i β 1 + e δ+β0+x t i β .$□ Theorem 20.1 ensures that conditioning on s i = 1, the model of y i given x i is still logit with the intercept changing from β 0 to β 0 + log(p 1 /p 0 ). Although we cannot consistently estimate the intercept without knowing (p 1 , p 0 ), we can still estimate all the slopes. [Kagan (2001)](#b151) showed that the logistic link is the only one that enjoys this property. [Samarani et al. (2019)](#b206) hypothesized that variation in the inherited activating Killer-cell Immunoglobulin-like Receptor genes in humans is associated with their innate susceptibility/resistance to developing Crohn's disease. They used a case-control study from three cities [(Manitoba, Montreal, and Ottawa)](#) in Canada to investigate the potential association. 

$y * i = x t i β + ε i$and -ε i has distribution function g(•) and is independent of x i . From this latent linear model, we can verify that pr(

$y i = 1 | x i ) = pr(y * i ≥ 0 | x i ) = pr(x t i β + ε i ≥ 0 | x i ) = pr(-ε i ≤ x t i β | x i ) = g(x t i β).$So the g(•) function can be interpreted as the distribution function of the error term in the latent linear model. This latent variable formulation provides another way to interpret the coefficients in the models for binary data. It is a powerful way to generate models for more complex data. We will see another example in the next chapter.

## Inverse model

Assume that y i ∼ Bernoulli(q), (20.5) and [(20.6)](#) where x i does not contain 1. This is called the linear discriminant model. We can verify that y i | x i follows a logit model as shown in the theorem below. 

$x i | y i = 1 ∼ N(µ 1 , Σ), x i | y i = 0 ∼ N(µ 0 , Σ),$$logit{pr(y i = 1 | x i )} = α + x t i β,$where

$α = log q 1 -q - 1 2 µ t 1 Σ -1 µ 1 -µ t 0 Σ -1 µ 0 , β = Σ -1 (µ 1 -µ 0 ) . (20.7)$Proof of Theorem 20.2: Using Bayes' formula, we have

$pr(y i = 1 | x i ) = pr(y i = 1, x i ) pr(x i ) = pr(y i = 1)pr(x i | y i = 1) pr(y i = 1)pr(x i | y i = 1) + pr(y i = 0)pr(x i | y i = 0) = e ∆ 1 + e ∆ , where ∆ = log pr(y i = 1)pr(x i | y i = 1) pr(y i = 0)pr(x i | y i = 0) = log q {(2π) p det(Σ)} -1/2 exp -(x i -µ 1 ) t Σ -1 (x i -µ 1 )/2 (1 -q) {(2π) p det(Σ)} -1/2 exp {-(x i -µ 0 ) t Σ -1 (x i -µ 0 )/2} = log q exp -(x i -µ 1 ) t Σ -1 (x i -µ 1 )/2 (1 -q) exp {-(x i -µ 0 ) t Σ -1 (x i -µ 0 )/2} = log q exp --2x t i Σ -1 µ 1 + µ t 1 Σ -1 µ 1 /2 (1 -q) exp {-(-2x t i Σ -1 µ 0 + µ t 0 Σ -1 µ 0 ) /2} = log q 1 -q - 1 2 µ t 1 Σ -1 µ 1 -µ t 0 Σ -1 µ 0 + x t i Σ -1 (µ 1 -µ 0 ) .$So y i | x i follows a logistic model with α and β given in [(21.12)](#). □ We can easily obtain the moment estimators for the unknown parameters under (21.10) and [(21.11)](#). Let n 1 = n i=1 y i and n 0 = n -n 1 . The moment estimator for q is q = n 1 /n, the sample mean of the y i 's. The moment estimators for µ 1 and µ 0 are

$μ1 = n -1 1 n i=1 y i x i , μ0 = n -1 0 n i=1 (1 -y i )x i ,$the sample means of the x i 's for units with y i = 1 and y i = 0, respectively. The moment estimator for Σ is

$Σ = n i=1 y i (x i -μ1 )(x i -μ1 ) t + n i=1 (1 -y i )(x i -μ0 )(x i -μ0 ) t /(n -2),$the pooled covariance matrix, after centering the x i 's by the y-specific means. Based on Theorem 20.2, we can obtain estimates α and β by replacing the true parameters with their moment estimators. This gives us another way to fit the logistic model. [Efron (1975)](#b95) compared the above moment estimator and the MLE under the logistic model. Since the linear discriminant model imposes stronger assumptions, the estimator based on Theorem 20.2 is more efficient. In contrast, the MLE of the logistic model is more robust because it does not impose the Normality assumption on x i .

## Homework problems

## Invariance of logistic regression

This problem extends Problem 3.4.

Assume xi = x i Γ with an invertible Γ. Run logistic regression of y i 's on x i 's to obtain the coefficient β and fitted probabilities πi 's. Run another logistic regression of y i 's on xi 's to obtain the coefficient β and fitted probabilities πi 's.

Show that β = Γ β and πi = πi for all i's.

## Two logistic regressions

This is an extension of Problem 17.2.

## Linear Model and Extensions

Given data (x i , z i , y i ) n i=1 where x i denotes the covariates, z i ∈ {1, 0} denotes the binary group indicator, and y i denotes the binary outcome. We can fit two separate logistic regressions:

logit{pr

$(y i = 1 | z i = 1, x i )} = γ 1 + x t i β 1 and logit{pr(y i = 1 | z i = 0, x i )} = γ 0 + x t i β 0$with the treated and control data, respectively. We can also fit a joint logistic regression using the pooled data:

$logit{pr(y i = 1 | z i , x i )} = α 0 + α z z i + x t i α x + z i x t i α zx .$Let the parameters with hats denote the MLEs, for example, γ1 is the MLE for γ 1 . Find (α 0 , αz , αx , αzx ) in terms of (γ 1 , β1 , γ0 , β0 ).

## Likelihood for Probit model

Write down the likelihood function for the Probit model, and derive the steps for Newton's method and Fisher scoring for computing the MLE. How do we estimate the asymptotic covariance matrix of the MLE?

## Logit and general exponential family

Efron [(1975)](#) pointed out an extension of Theorem 20.2. Show that under (21.10) and

$f (x i | y i = y) = g(θ y , η)h(x i , η) exp(x t i θ y ), (y = 0, 1)$with parameters (θ 1 , θ 0 , η), we have

$logit{pr(y i = 1 | x i )} = α + x t i β.$Find the formulas of α and β in terms of (θ 1 , θ 0 , η).

Hint: As a sanity check, you can compare this problem with Theorem 20.2.

## Empirical comparison of logistic regression and linear discriminant analysis

Compare the performance of logistic regression and linear discriminant analysis in terms of prediction accuracy. You should simulate at least three cases: (1) the model for linear discriminant analysis is correct;

(2) the model for linear discriminant analysis is incorrect but the model for logistic regression is correct;

(3) the model for logistic regression is incorrect.

## Quadratic discriminant analysis

Assume that y i ∼ Bernoulli(q), and

$x i | y i = 1 ∼ N(µ 1 , Σ 1 ), x i | y i = 0 ∼ N(µ 0 , Σ 0 ),$where

$x i ∈ R p does not contain 1. Prove that logit{pr(y i = 1 | x i )} = α + x t i β + x t i Λx i , where α = log q 1 -q - 1 2 log det(Σ 1 ) det(Σ 0 ) - 1 2 (µ t 1 Σ -1 1 µ 1 -µ t 0 Σ -1 0 µ 0 ), β = Σ -1 1 µ 1 -Σ -1 0 µ 0 , Λ = - 1 2 (Σ -1 1 -Σ -1 0 ).$Remark: This problem extends the linear discriminant model in Section 20.7.2 to the quadratic discriminant model by allowing for heteroskedasticity in the conditional Normality of x given y. It implies the logistic model with the linear, quadratic, and interaction terms of the basic covariates.

## Logit and other links

Compute the minimizer and minimum value of max y |g logit (by) -g * (y)| for * = cauchit and cloglog.

## Data analysis

Reanalyze the data in Section 20.6.2, stratifying the analysis based on center. Do the results vary significantly across centers?

## R 2 in logistic regression

Recall that R 2 in the linear model measures the linear dependence of the outcome on the covariates. However, the definition of R 2 is not obvious in the logistic model. The glm function in R does not return any R 2 for the logistic regression.

Recall the following equivalent definitions of R 2 in the linear model

$R 2 = n i=1 (ŷ i -ȳ) 2 n i=1 (y i -ȳ) 2 = 1 - n i=1 (y i -ŷi ) 2 n i=1 (y i -ȳ) 2 = ρ2 y ŷ = ( n i=1 (y i -ȳ)(ŷ i -ȳ)) 2 n i=1 (y i -ȳ) 2 n i=1 (ŷ i -ȳ) 2 .$The fitted values are πi = π(x i , β) in the logistic model, which have mean ȳ with the intercept included in the model. Analogously, we can define R 2 in the logistic model as

$R 2 model = ss m ss t , R 2 residual = 1 - ss r ss t , R 2 correlation = ρ2 y π = C 2 y π ss t ss m ,$where

$ss t = n i=1 (y i -ȳ) 2 , ss m = n i=1 (π i -ȳ) 2 , ss r = n i=1 (y i -πi ) 2 , C y π = n i=1 (y i -ȳ)(π i -ȳ).$These three definitions are not equivalent in general. In particular, R 2 model differs from R 2 residual since ss t = ss m + ss r + 2C επ where

$C επ = n i=1 (y i -πi )(π i -ȳ). 1. Prove that R 2 model ≥ 0, R 2 correlation ≥ 0 with equality holding if πi = ȳ for all i. Prove that R 2 model ≤ 1, R 2 residual ≤ 1, R 2$correlation ≤ 1 with equality holding if y i = πi for all i. Note that R 2 residual may be negative. Give an example.

## Define

$π1 = n i=1 y i πi n i=1 y i , π0 = n i=1 (1 -y i )π i n i=1$(1 -y i ) as the average of the fitted values for units with y i = 1 and y i = 0, respectively. Define

$D = π1 -π0 . Prove that D = (R 2 model + R 2 residual )/2 = R 2 model R 2 correlation .$Further prove that D ≥ 0 with equality holding if πi = ȳ for all i, and D ≤ 1 with equality holding if y i = πi for all i.

3. [McFadden (1974)](#b178) defined the following R 2 :

$R 2 mcfadden = 1 - log L( β) log L( β)$recalling that β is the MLE assuming that all coefficients except the intercept are zero, and β is the MLE without any restrictions. This R 2 must lie within [0, 1]. Verify that under the Normal linear model, the above formula does not reduce to the usual R 2 .

4. [Cox and Snell (1989)](#b85) defined the following R 2 :

$R 2 CS = 1 - L( β) L( β) 2/n$.

Verify that under the Normal linear model, the above formula reduces to the usual R 2 .

Remark: [Tjur (2009)](#b222) gave an excellent discussion of R 2 model , R 2 residual , R 2 correlation and D. [Nagelkerke (1991)](#b182) pointed out that the upper bound of this R 2 CS is 1 -(L( β)) 2/n < 1 and proposed to modify it as

$R 2 nagelkerke = 1 -L( β) L( β) 2/n 1 -L( β) 2/n$to ensure that its upper bound is 1. However, this modification seems purely ad hoc. Although D is an appealing definition of R 2 for the logistic model, it does not generalize to other models. Overall, I feel that R 2 correlation is a better definition that easily generalizes to other models. [Zhang (2017)](#b249) defined an R 2 based on the variance function of the outcome for generalized linear models including the binary logistic model. [Hu et al. (2006)](#b145) studied the asymptotic properties of some of the R 2 s above.

## Logistic Regressions for Categorical Outcomes

Categorical outcomes are common in empirical research. The first type of categorical outcome is nominal. For example, the outcome denotes the preference for fruits (apple, orange, and pear) or transportation services (Uber, Lyft, or BART). The second type of categorical outcome is ordinal. For example, the outcome denotes the course evaluation at Berkeley (1, 2, . . . , 7) or Amazon review (1 to 5 stars). This chapter discusses statistical modeling strategies for categorical outcomes, including two classes of models corresponding to the nominal and ordinal outcomes, respectively.

## Multinomial distribution

A categorical random variable y taking values in {1, . . . , K} with probabilities pr(y

$= k) = π k (k = 1, . . . , K) is often called a multinomial distribution, denoted by y ∼ Multinomial {1; (π 1 , . . . , π K )} ,(21.1)$where K k=1 π k = 1. We can calculate the mean and covariance matrix of y: Proposition 21.1 If y is the Multinomial random variable in [(21.1)](#), then (1(y = 1), . . . , 1(y = K -1)) has mean (π 1 , . . . , π K-1 ) and covariance matrix

$     π 1 (1 -π 1 ) -π 1 π 2 • • • -π 1 π K-1 -π 1 π 2 π 2 (1 -π 2 ) • • • -π 2 π K-1 . . . . . . . . . . . . -π 1 π K-1 -π 2 π K-1 • • • π K-1 (1 -π K-1 )      . (21.2)$As a byproduct, we know that the matrix in (21.2) is positive semi-definite.

Proof of Proposition 21.1: Without loss of generality, I will calculate the (1, 1)th and the (1, 2)th element of the matrix. First, 1(y = 1) is Bernoulli with probability π 1 , so the (1, 1)th element equals var(1(y = 1)) = π 1 (1 -π 1 ). Similarly, the (2, 2)th element equals var(1

$(y = 2)) = π 2 (1 -π 2 ). Second, 1(y = 1)+1(y = 2) is Bernoulli with probability π 1 +π 2 , so var(1(y = 1)+1(y = 2)) = (π 1 + π 2 )(1 -π 1 -π 2 ). Therefore, the (1, 2)-th element equals cov(1(y = 1), 1(y = 2)) = {var(1(y = 1) + 1(y = 2)) -var(1(y = 1)) -var(1(y = 2))} /2 = -π 1 π 2 .$
## □

With independent samples of (x i , y i ) n i=1 , we want to model y i based on covariates x i[foot_21](#foot_21) :

$y i | x i ∼ Multinomial [1; {π 1 (x i ), . . . , π K (x i )}] ,$where K k=1 π k (x i ) = 1 for all x i . We can write the probability mass function of pr(y i | x i ) as yi=k) .

$π yi (x i ) = K k=1 {π k (x i ) if y i = k} = K k=1 {π k (x i )} 1($Here π k (x i ) is a general function of x i . The remaining parts of this chapter will discuss the canonical choices of π k (x i ) for nominal and ordinal outcomes.

## Multinomial logistic model for nominal outcomes

## Modeling

Viewing category K as the reference level, we can model the ratio of the probabilities of categories k and K as log

$π k (x i ) π K (x i ) = x t i β k (k = 1, . . . , K -1)$which implies that

$π k (x i ) = π K (x i )e x t i β k (k = 1, . . . , K -1).$Due to the normalization, we have

$K k=1 π k (x i ) = 1 =⇒ K k=1 π K (x i )e x t i β k = 1 =⇒ π K (x i ) K k=1 e x t i β k = 1 =⇒ π K (x i ) = 1 K k=1 e x t i β k =⇒ π k (x i ) = e x t i β k K l=1 e x t i β l (k = 1, . . . , K -1).$A more compact form is

$π k (x i ) = π k (x i , β) = e x t i β k K l=1 e x t i β l , (k = 1, . . . , K) (21.3)$where β = (β 1 , . . . , β K-1 ) denotes the parameter with β K = 0 for the reference category.

From the ratio form of ( [21](#formula_618).3), we can only identify β k -β K for all k = 1, . . . , K. So for convenience, we impose the restriction β K = 0. Model [(21.](#)3) is called the multinomial logistic regression model.

Similar to the binary logistic regression model, we can interpret the coefficients as the conditional log odds ratio compared to the reference level:

$β k,j = log π k (. . . , x ij + 1, . . .) π K (. . . , x ij + 1, . . .) -log π k (. . . , x ij , . . .) π K (. . . , x ij , . . .) = log π k (. . . , x ij + 1, . . .) π K (. . . , x ij + 1, . . .)$π k (. . . , x ij , . . .) π K (. . . , x ij , . . .) .

## MLE

The likelihood function for the multinomial logistic model is

$L(β) = n i=1 K k=1 {π k (x i )} 1(yi=k) = n i=1 K k=1 e x t i β k K l=1 e x t i β l 1(yi=k) = n i=1 K k=1 e 1(yi=k)x t i β k K k=1 e x t i β k ,$and the log-likelihood function is

$log L(β) = n i=1 K k=1 1(y i = k)x t i β k -log K k=1 e x t i β k .$The score function is

$∂ log L(β) ∂β =     ∂ log L(β) ∂β1$. . .

$∂ log L(β) ∂β K-1     ∈ R p(K-1)$with

$∂ log L(β) ∂β k = n i=1 x i 1(y i = k) - x i e x t i β k K l=1 e x t i β l = n i=1 x i 1(y i = k) - e x t i β k K l=1 e x t i β l = n i=1 x i {1(y i = k) -π k (x i , β)} ∈ R p , (k = 1, . . . , K -1).$The Hessian matrix is

$∂ 2 log L(β) ∂β∂β t =         ∂ 2 log L(β) ∂β1∂β t 1 ∂ 2 log L(β) ∂β1∂β t 2 • • • ∂ 2 log L(β) ∂β1∂β t K-1 ∂ 2 log L(β) ∂β2∂β t 1 ∂ 2 log L(β) ∂β2∂β t 2 • • • ∂ 2 log L(β) ∂β2∂β t K-1 . . . . . . . . . ∂ 2 log L(β) ∂β K-1 ∂β t 1 ∂ 2 log L(β) ∂β K-1 ∂β t 2 • • • ∂ 2 log L(β) ∂β K-1 ∂β t K-1         ∈ R p(K-1)×p(K-1) (21.4) with the (k, k)th block ∂ 2 log L(β) ∂β k ∂β t k = - n i=1 x i ∂ ∂β t k e x t i β k K l=1 e x t i β l = - n i=1 x i x t i e x t i β k K l=1 e x t i β l -e x t i β k e x t i β k ( K l=1 e x t i β l ) 2 = - n i=1 π k (x i , β) {1 -π k (x i , β)} x i x t i ∈ R p×p (k = 1, . . . , K -1)$and the (k, l)th block

$∂ 2 log L(β) ∂β k ∂β t l = - n i=1 x i ∂ ∂β t l e x t i β k K l=1 e x t i β l = - n i=1 x i x t i -e x t i β k e x t i β l ( K l=1 e x t i β l ) 2 = n i=1 π k (x i , β)π l (x i , β)x i x t i ∈ R p×p (k ̸ = l : k, l = 1, . . . , K -1).$We can verify that the Hessian matrix is negative semi-definite based on Proposition 21.1, which is left as Problem 21.2.

In R, the function multinom in the nnet package uses Newton's method to fit the multinomial logistic model. We can make inference about the parameters based on the asymptotic Normality of the MLE. Based on a new observation with covariate x n+1 , we can make prediction based on the fitted probabilities π k (x n+1 , β), and furthermore classify it into K categories based on ŷn+1 = arg max 1≤k≤K π k (x n+1 , β).

## A latent variable representation for the multinomial logistic regression

We can view the multinomial logistic regression [(21.](#)3) as an extension of the binary logistic regression model. The binary logistic regression has a latent variable representation as shown in Section 20.7.1. The multinomial logistic regression also has a latent variable representation below.

$Assume        U i1 = x t i β 1 + ε i1 , . . . U iK = x t i β K + ε iK$, where ε i1 , . . . , ε iK are IID standard Gumbel random variables 2 . Using the language of economics, (U i1 , . . . , U iK ) are the utilities associated with the choices (1, . . . , K). So unit i chooses k if k has the highest utility:

$y i = k if U ik > U il for all l ̸ = k.$2 See Section B.1.3 for a review.

We can show that this latent variable model implies [(21.3)](#). This follows from the lemma below, which is due to [McFadden (1974)](#b178) 3 . When K = 2, it also gives another latent variable representation for the binary logistic regression, which is different from the one in Section 20.7.1.

$Lemma 21.1 Assume        U 1 = V 1 + ε 1 , . . . U K = V K + ε K ,$where ε 1 , . . . , ε K are IID standard Gumbel. Define

$y = arg max 1≤l≤K U l$as the index corresponding to the maximum of the U k 's. We have

$pr(y = k) = e V k K l=1 e V l .$Proof of Lemma 21.1: Recall that the standard Gumbel random variable has CDF F (z) = exp(-e -z ) and density f (z) = exp(-e -z )e -z . The event "y = k" is equivalent to the event "U k > U l for all l ̸ = k", so

$pr(y = k) =pr(U k > U l , l = 1, . . . , k -1, k + 1, . . . , K) =pr(V k + ε k > V l + ε l , l = 1, . . . , k -1, k + 1, . . . , K) = ∞ -∞ pr(V k + z > V l + ε l , l = 1, . . . , k -1, k + 1, . . . , K)f (z)dz$where the last line follows from conditioning on ε k . By independence of the ε's, we have

$pr(y = k) = ∞ -∞ l̸ =k pr(ε l < V k -V l + z)f (z)dz = ∞ -∞ l̸ =k exp(-e -V k +V l -z ) exp(-e -z )e -z dz = ∞ -∞ exp   - l̸ =k e -V k +V l e -z   exp(-e -z )e -z dz.$Changing of variables t = e -z with dz = -1/tdt, we obtain

$pr(y = k) = ∞ 0 exp   -t l̸ =k e -V k +V l   exp(-t)dt = ∞ 0 exp(-tC k )dt where C k = 1 + l̸ =k e -V k +V l .$3 Daniel McFadden shared the 2000 Nobel Memorial Prize in Economic Sciences with James Heckman.

The integral simplifies to 1/C k due to the density of the exponential distribution. Therefore,

$pr(y = k) = 1 1 + l̸ =k e -V k +V l = e V k e V k + l̸ =k e V l = e V k K l=1 e V l$. □ This lemma is remarkable. It extends to more complex utility functions. I will use it again in Section 21.6.

## Proportional odds model for ordinal outcomes

For ordinal outcomes, we can still use the multinomial logistic model, but by doing this, we discard the ordering information in the outcome. Consider a simple case with a scalar x i , the multinomial logistic model does not rule out the possibility that β k < 0 and β k+1 > 0, which implies that x i increases the probability of category k + 1 but decreases the probability of category k. However, the outcome must first reach level k and then increase to level k + 1. If this happens, it may be hard to interpret the model.

Motivated by the latent linear representation of the binary logistic model, we imagine that the ordinal outcome arises from discretizing a continuous latent variable:

$y * i = x t i β + ε i , pr(ε i ≤ z | x i ) = g(z)(21.5)$and

$y i = k, if α k-1 < y * i ≤ α k , (k = 1, . . . , K)(21.6)$where

$-∞ = α 0 < α 1 < • • • < α K-1 < α K = ∞.$Figure [21](#fig_110).1 illustrates the data generating process with K = 4. The unknown parameters are (β, α 1 , . . . , α K-1 ). The distribution of the latent error term g(•) is known, for example, it can be logistic or Normal. The former results in the proportional odds logistic model, and the latter results in the ordered Probit model. Based on the latent linear model, we can compute

$pr(y i ≤ k | x i ) = pr(y * i ≤ α k | x i ) = pr(x t i β + ε i ≤ α k | x i ) = pr(ε i ≤ α k -x t i β | x i ) = g(α k -x t i β).$I will focus on the proportional odds logistic model in the main text and defer the details for the ordered Probit model to Problem 21.5. With this model, we have

$pr(y i ≤ k | x i ) = e α k -x t i β 1 + e α k -x t i β ordinal outcome y * = x T β + ε density of the latent variable 1 2 3 4 α 0 = -∞ α 1 α 2 α 3 α 4 = ∞ FIGURE 21.1: Latent variable representation of the ordinal outcome or logit{pr(y i ≤ k | x i )} = log pr(y i ≤ k | x i ) pr(y i > k | x i ) = α k -x t i β.(21.7)$The model has the "proportional odds" property because

$pr(y i ≤ k | . . . , x ij + 1, . . .) pr(y i > k | . . . , x ij + 1, . . .) pr(y i ≤ k | . . . , x ij , . . .) pr(y i > k | . . . , x ij , . . .) = e -βj$which is a positive constant not depending on k. The sign of x t i β is negative due to the latent variable representation. Some textbooks and software packages use a positive sign, but the function polr in package MASS of R uses [(21.7)](#).

The proportional odds model implies a quite complicated form of the probability for each category:

$pr(y i = k | x i ) = e α k -x t i β 1 + e α k -x t i β - e α k-1 -x t i β 1 + e α k-1 -x t i β , (k = 1, . . . , K).$So the likelihood function is

$L(β, α 1 , . . . , α K-1 ) = n i=1 K k=1 {pr(y i = k | x i )} 1(yi=k) = n i=1 K k=1 e α k -x t i β 1 + e α k -x t i β - e α k-1 -x t i β 1 + e α k-1 -x t i β 1(yi=k)$.

The log-likelihood function is concave [(Pratt, 1981;](#b192)[Burridge, 1981)](#b68), and it is strictly concave in most cases. The function polr in the R package MASS computes the MLE of the proportional odds model using the BFGS algorithm. It uses the explicit formulas of the gradient of the log-likelihood function, and computes the Hessian matrix numerically. I relegate the formulas of the gradient as a homework problem. For more details of the Hessian matrix, see [Agresti (2010)](#b44), which is a textbook discussion on modeling ordinal data.

## A case study

I use a small observational dataset from the Karolinska Institute in Stockholm, Sweden to illustrate the application of logistic regressions. [Rubin (2008)](#b203) used this dataset to investigate whether it is better for cardia cancer patients to be treated in a large or small-volume hospital, where volume is defined by the number of patients with cardia cancer treated in recent years. I use the following variables: highdiag indicating whether a patient was diagnosed at a high volume hospital, hightreat indicating whether a patient was treated at a high volume hospital, age representing the age, rural indicating whether the patient was from a rural area, and survival representing the years of survival after diagnosis with three categories ("1", "2-4", "5+"). The R code is in code20.5.R. karolinska = read . table ( " karolinska . txt " , header = TRUE ) karolinska = karolinska [ , c ( " highdiag " , " hightreat " , " age " , " rural " , " male " , " survival " )]

## Binary logistic for the treatment

We have two choices of the treatment: highdiag and hightreat. The logistic fit of highdiag on covariates is shown below.

> diagglm = glm ( highdiag ~age + rural + male , + data = karolinska , + family = binomial ( link = " logit " )) > summary ( diagglm ) Call : glm ( formula = highdiag ~age + rural + male , family = binomial ( link = " logit " ) , data = karolinska )

Deviance Residuals : Min 1 Q Median 3 Q Max -2 . 0 6 1 4 7 -0 . 9 8 6 4 5 -0 . 0 5 7 5 9 1 . 0 1 3 9 1 1 . 7 5 6 9 6

Coefficients : Estimate Std . Error z value Pr ( >| z |) ( Intercept ) 3 . 4 6 6 0 4 1 . 1 4 5 4 5 3 . 0 2 6 0 . 0 0 2 4 7 9 ** age -0 . 0 3 1 2 4 0 . 0 1 4 8 1 -2 . 1 1 0 0 . 0 3 4 8 5 4 * rural -1 . 2 6 3 2 2 0 . 3 4 5 3 0 -3 . 6 5 8 0 . 0 0 0 2 5 4 *** male -0 . 9 7 5 2 4 0 . 4 1 3 0 3 -2 . 3 6 1 0 . 0 1 8 2 1 6 *

The logistic fit of hightreat is shown below.

> treatglm = glm ( hightreat ~age + rural + male , + data = karolinska , + family = binomial ( link = " logit " )) > summary ( treatglm ) Call : glm ( formula = hightreat ~age + rural + male , family = binomial ( link = " logit " ) , data = karolinska ) [2 9 1 2 -0 . 9 9 7 8  0 . 5 3 8 7  0 . 8 4 0 8  1 . 4 8 1 0](#) Coefficients : Estimate Std . Error z value Pr ( >| z |) ( Intercept ) 6 . 4 4 6 8 3 1 . 4 9 5 4 4 4 . 3 1 1 1 . 6 3e -0 5 *** age -0 . 0 6 2 9 7 0 . 0 1 8 9 0 -3 . 3 3 2 0 . 0 0 0 8 6 2 *** rural -1 . 2 8 7 7 7 0 . 3 9 5 7 2 -3 . 2 5 4 0 . 0 0 1 1 3 7 ** male -0 . 7 4 8 5 6 0 . 4 5 2 8 5 -1 . 6 5 3 0 . 0 9 8 3 2 9 .

$Deviance Residuals : Min 1 Q Median 3 Q Max -2 .$Both treatments are associated with the covariates. hightreat is more strongly associated with age. [Rubin (2008)](#b203) argued that highdiag is more random than hightreat, and may have weaker association with other hidden covariates. For each model below, I fit the data twice corresponding to two choices of treatment. Overall, we should trust the results with highdiag more based on Rubin (2008)'s argument.

## Binary logistic for the outcome

I first fit binary logistic models for the dichotomized outcome indicating whether the patient survived longer than a year after diagnosis.

> karolinska $ loneyear = ( karolinska $ survival != " 1 " ) > loneyearglm = glm ( loneyear ~highdiag + age + rural + male , + data = karolinska , + family = binomial ( link = " logit " )) > summary ( loneyearglm ) Call : glm ( formula = loneyear ~highdiag + age + rural + male , family = binomial ( link = " logit " ) , data = karolinska )

Deviance Residuals : Min 1 Q Median 3 Q Max -1 . 1 7 5 5 -0 . 9 9 3 6 -0 . 7 7 3 9 1 . 3 0 2 4 1 . 8 5 5 7

Coefficients : Estimate Std . Error z value Pr ( >| z |) ( Intercept ) -1 . 2 2 9 1 9 1 . 1 5 5 4 5 -1 . 0 6 4 0 . 2 8 7 4 highdiag 0 . 1 3 6 8 4 0 . 3 6 5 8 6 0 . 3 7 4 0 . 7 0 8 4 age -0 . 0 0 3 8 9 0 . 0 1 4 1 1 -0 . 2 7 6 0 . 7 8 2 9 rural 0 . 3 3 3 6 0 0 . 3 5 7 9 8 0 . 9 3 2 0 . 3 5 1 4 male 0 . 8 6 7 0 6 0 . 4 4 0 3 4 1 . 9 6 9 0 . 0 4 8 9 * highdiag is not significant in the above regression.

> loneyearglm = glm ( loneyear ~hightreat + age + rural + male , + data = karolinska , + family = binomial ( link = " logit " )) > summary ( loneyearglm ) Call : glm ( formula = loneyear ~hightreat + age + rural + male , family = binomial ( link = " logit " ) , data = karolinska )

Deviance Residuals : Min 1 Q Median 3 Q Max -1 . 3 7 6 7 -0 . 9 6 8 3 -0 . 6 7 8 4 1 . 0 8 1 3 2 . 0 8 3 3

Coefficients : Estimate Std . Error z value Pr ( >| z |) ( Intercept ) -3 . 3 5 3 9 7 7 1 . 3 1 7 9 4 2 -2 . 5 4 5 0 . 0 1 0 9 3 * hightreat 1 . 4 1 7 4 5 8 0 . 4 5 5 6 0 3 3 . 1 1 1 0 . 0 0 1 8 6 ** age 0 . 0 0 8 7 2 5 0 . 0 1 4 8 4 0 0 . 5 8 8 0 . 5 5 6 5 5 rural 0 . 6 3 3 2 7 8 0 . 3 6 8 5 2 5 1 . 7 1 8 0 . 0 8 5 7 2 . male 1 . 0 7 9 9 7 3 0 . 4 5 2 1 9 1 2 . 3 8 8 0 . 0 1 6 9 3 * hightreat is significant in the above regression.

## Multinomial logistic for the outcome

I then fit multinomial logistic models for the outcome with three categories.

> library ( nnet ) > yearmultinom = multinom ( survival ~highdiag + age + rural + male , + data = karolinska ) # weights : 1 8 ( 1 0 variable ) initial value 1 7 3 . 5 8 0 7 4 2 iter 1 0 value 1 3 4 . 3 3 1 9 9 2 final value 1 3 4 . 1 3 0 8 1 5 converged > summary ( yearmultinom ) Call : multinom ( formula = survival ~highdiag + age + rural + male , data = karolinska )

Coefficients :

( Intercept ) highdiag age rural male 2 -4 -1 . 0 7 5 8 1 8 -0 . 0 6 9 7 3 1 8 7 -0 . 0 0 4 6 2 4 0 3 0 0 . 1 7 4 4 2 5 6 0 . 5 0 2 8 7 8 6 5 + -4 . 1 8 0 4 1 6 0 . 6 4 0 3 6 2 8 9 -0 . 0 0 1 8 4 6 4 5 3 0 . 7 3 6 5 1 1 1 2 . 1 6 2 8 7 1 7

Std . Errors : ( Intercept ) highdiag age rural male 2 -4 1 . 2 8 6 9 8 7 0 . 4 1 1 3 0 0 6 0 . 0 1 5 9 6 3 7 7 0 . 4 0 1 4 7 1 8 0 . 4 7 1 6 8 3 1 5 + 2 . 0 0 3 5 8 1 0 . 5 8 1 6 3 6 5 0 . 0 2 1 4 8 9 3 6 0 . 5 7 4 1 0 1 7 1 . 0 7 4 1 2 3 9

Residual Deviance : 2 6 8 . 2 6 1 6 AIC : 2 8 8 . 2 6 1 6 > predict ( yearmultinom , type = " probs " )[ 1 : 5 , ] 1 2 -4 5 + 1 0 . 5 9 5 0 6 3 1 0 . 2 6 4 7 0 4 7 0 . 1 4 0 2 3 2 2 2 2 0 . 5 9 4 1 8 0 2 0 . 2 6 5 5 3 6 9 0 . 1 4 0 2 8 2 9 3 3 0 . 8 0 8 1 3 7 6 0 . 1 7 1 8 9 6 3 0 . 0 1 9 9 6 6 1 3 4 0 . 5 9 5 0 6 3 1 0 . 2 6 4 7 0 4 7 0 . 1 4 0 2 3 2 2 2 5 0 . 6 3 6 6 9 2 9 0 . 2 2 6 0 0 8 6 0 . 1 3 7 2 9 8 4 9 highdiag is not significant above. The predict function gives the fitted probabilities for all categories of the outcome.

> yearmultinom = multinom ( survival ~hightreat + age + rural + male , + data = karolinska ) # weights : 1 8 ( 1 0 variable ) initial value 1 7 3 . 5 8 0 7 4 2 iter 1 0 value 1 2 9 . 5 4 8 6 4 2 final value 1 2 9 . 2 8 3 7 3 9 converged > summary ( yearmultinom ) Call : multinom ( formula = survival ~hightreat + age + rural + male , data = karolinska )

Coefficients :

( Intercept ) hightreat age rural male polr ( formula = survival ~hightreat + age + rural + male , data = karolinska , Hess = TRUE )

Coefficients : Value Std . Error t value hightreat 1 . 3 9 9 5 3 8 0 . 4 4 5 1 8 3 . 1 4 3 8 age 0 . 0 0 8 0 3 2 0 . 0 1 4 3 8 0 . 5 5 8 4 rural 0 . [6 3 8 8 6 2  0 . 3 5 4 5 0 1 . 8 0 2 2  male  1 . 1 2 2 6 9 8  0 . 4 4 3 7 7 2 . 5 2 9 9](#) Intercepts : Value Std . Error t value 1 | 2 -4 3 . 3 2 7 3 1 . 2 7 5 2 2 . 6 0 9 2 2 -4 | 5 + 4 . 9 2 5 8 1 . 3 1 0 6 3 . 7 5 8 3

Residual Deviance : 2 6 0 . 2 8 3 1 AIC : 2 7 2 . 2 8 3 1 hightreat is significant above.

## Discrete choice models

## Model

The covariates in model ( [21](#formula_618).3) depend only on individuals. [McFadden (1974)](#b178) extends it to allow for choice-specific covariates z ik . His formulation is based on the latent utility representation:

$       U i1 = z t i1 θ + ε i1 , . . . U iK = z t iK θ + ε iK , where ε i1 , . . . , ε iK are IID standard Gumbel. Unit i chooses k if k has the highest utility. Lemma 21.1 implies that π k (z i ) = π k (z i , θ) = e z t ik θ K l=1 e z t il θ , (k = 1, . . . , K).(21.8)$Model [(21.8)](#) seems rather similar to model [(21.3)](#). However, there are many subtle differences. First, a component of z ik may vary only with choice k, for example, it can represent the price of choice k. Partition z ik into three types of covariates: x i that only vary cross individuals, c k that only vary across choices, and w ik that vary across both individuals and choices. Model [(21.8](#)) reduces to

$π k (z i ) = e x t i θx+c t k θc+w t ik θw K l=1 e x t i θx+c t l θc+w t il θw = e c t k θc+w t ik θw K l=1 e c t l θc+w t il θw$, that is, the individual-specific covariates drop out. Therefore, z ik in model [(21.8)](#) does not contain covariates that vary only with individuals. In particular, z ik in model [(21.8)](#) does not contain the constant, but in contrast, the x i in model [(21.](#)3) ususally contains the intercept by default.

Second, if we want to use individual-specific covariates in the model, they must have choice-specific coefficients. So a more general model unifying [(21.3](#)) and (21.8) is

$π k (x i , w ik , θ, β) = e w t ik θ+x t i β k K l=1 e w t il θ+x t i β l , (k = 1, . . . , K). (21.9)$Equivalently, we can create pseudo covariates z ik as the original w ik together with interaction of x i and the dummy for choice k. For example, if K = 3 and x i contain the intercept and a scalar individual-specific covariate, then (z i1 , z i2 , z i3 ) are

$  z i1 z i2 z i3   =   w i1 1 0 x i 0 w i2 0 1 0 x i w i3 0 0 0 0   ,$where K = 3 is the reference level. So with augmented covariates, the discrete choice model [(21.8](#)) is strictly more general than the multinomial logistic model [(21.3](#)). In the special case with K = 2, model [(21.8](#)) reduces to

$pr(y i = 1 | x i ) = e x t i β 1 + e x t i β$where

$x i = z i1 -z i2 .$Based on the model specification [(21.8)](#), the log likelihood function is

$log L(θ) = n i=1 K k=1 1(y i = k) z t ik θ -log K l=1 e z t il θ .$So the score function is

$∂ ∂θ log L(θ) = n i=1 K k=1 1(y i = k) z ik - K l=1 e z t il θ z il K l=1 e z t il θ = n i=1 K k=1 1(y i = k){z ik -E(z ik ; θ)},$where E(•; θ) is the average value of {z i1 , . . . , z iK } over the probability mass function

$p k (θ) = e z t ik θ / K l=1 e z t il θ .$The Hessian matrix is

$∂ 2 ∂θ∂θ t log L(θ) = - n i=1 K k=1 1(y i = k) K l=1 e z t il θ z il z t il K l=1 e z t il θ - K l=1 e z t il θ z il K l=1 e z t il θ z t il ( K l=1 e z t il θ ) 2 = - n i=1 K k=1 1(y i = k)cov(z ik ; θ),$where cov(•; θ) is the covariance matrix of {z i1 , . . . , z iK } over the probability mass function defined above. From these formulas, we can easily compute the MLE using Newton's method and obtain its asymptotic distribution based on the inverse of the Fisher information matrix.

## Example

The R package mlogit provides a function mlogit to fit the general discrete logistic model [(Croissant, 2020)](#b87). Here I use an example from this package to illustrate the model fitting of mlogit. The R code is in code20.6.R.

> library ( " nnet " ) > library ( " mlogit " ) > data ( " Fishing " ) > head ( Fishing ) mode price . beach price . pier price . boat price . charter 1 charter 1 5 7 . 9 3 0 1 5 7 . 9 3 0 1 5 7 . 9 3 0 1 8 2 . 9 3 0 2 charter 1 5 . 1 1 4 1 5 . 1 1 4 1 0 . 5 3 4 3 4 . 5 3 4 3 boat 1 6 1 . 8 7 4 1 6 1 . 8 7 4 2 4 . 3 3 4 5 9 . 3 3 4 4 pier 1 5 . 1 3 4 1 5 . 1 3 4 5 5 . 9 3 0 8 4 . 9 3 0 5 boat 1 0 6 . 9 3 0 1 0 6 . 9 3 0 4 1 . 5 1 4 7 1 . 0 1 4 6 charter 1 9 2 . 4 7 4 1 9 2 . 4 7 4 2 8 . 9 3 4 6 3 . 9 3 4 catch . beach catch . pier catch . boat catch . charter income 1 0 . 0 6 7 8 0 . 0 5 0 3 0 . 2 6 0 1 0 . 5 3 9 1 7 0 8 3 . 3 3 2 2 0 . 1 0 4 9 0 . 0 4 5 1 0 . 1 5 7 4 0 . 4 6 7 1 1 2 5 0 . 0 0 0 3 0 . 5 3 3 3 0 . 4 5 2 2 0 . 2 4 1 3 1 . 0 2 6 6 3 7 5 0 . 0 0 0 4 0 . 0 6 7 8 0 . 0 7 8 9 0 . 1 6 4 3 0 . 5 3 9 1 2 0 8 3 . 3 3 3 5 0 . 0 6 7 8 0 . 0 5 0 3 0 . 1 0 8 2 0 . 3 2 4 0 4 5 8 3 . 3 3 2 6 0 . 5 3 3 3 0 . 4 5 2 2 0 . 1 6 6 5 0 . 3 9 7 5 4 5 8 3 . 3 3 2

The dataset Fishing is in the "wide" format, where mode denotes the choice of four modes of fishing (beach, pier, boat and charter), price and catch denote the price and catching rates which are choice-specific, income is individual-specific. We need to first transform the dataset into "long" format.

> Fish = dfidx ( Fishing , + varying = 2 : 9 , + shape = " wide " , + choice = " mode " ) > head ( Fish ) ~~~~~~f irst 1 0 observations out of 4 7 2 8 ~~~~~~m ode income price catch idx 1 FALSE 7 0 8 3 . 3 3 2 1 5 7 . 9 3 0 0 . 0 6 7 8 1 : each 2 FALSE 7 0 8 3 . 3 3 2 1 5 7 . 9 3 0 0 . 2 6 0 1 1 : boat 3 TRUE 7 0 8 3 . 3 3 2 1 8 2 . 9 3 0 0 . 5 3 9 1 1 : rter 4 FALSE 7 0 8 3 . 3 3 2 1 5 7 . 9 3 0 0 . 0 5 0 3 1 : pier 5 FALSE 1 2 5 0 . 0 0 0 1 5 . 1 1 4 0 . 1 0 4 9 2 : each 6 FALSE 1 2 5 0 . 0 0 0 1 0 . 5 3 4 0 . 1 5 7 4 2 : boat 7 TRUE 1 2 5 0 . 0 0 0 3 4 . 5 3 4 0 . 4 6 7 1 2 : rter 8 FALSE 1 2 5 0 . 0 0 0 1 5 . 1 1 4 0 . 0 4 5 1 2 : pier 9 FALSE 3 7 5 0 . 0 0 0 1 6 1 . 8 7 4 0 . 5 3 3 3 3 : each 1 0 TRUE 3 7 5 0 . 0 0 0 2 4 . 3 3 4 0 . 2 4 1 3 3 : boat Using only choice-specific covariates, we have the following fitted model: > summary ( mlogit ( mode ~0 + price + catch , data = Fish ))

Call : mlogit ( formula = mode ~0 + price + catch , data = Fish , method = " nr " ) Frequencies of alternatives : choice beach boat charter pier 0 . 1 1 3 3 7 0 . 3 5 3 6 4 0 . 3 8 2 4 0 0 . 1 5 0 5 9 nr method 6 iterations , 0 h : 0 m : 0 s g '( -H )^-1 g ␣ = ␣ 0 . 0 0 0 1 7 9

## Homework problems

## Inverse model for the multinomial logit model

The following result extends Theorem 20.2.

Assume that y i ∼ Multinomial(1; q 1 , . . . , q K ), (21.10) and

$x i | y i = k ∼ N(µ k , Σ),(21.11)$where x i does not contain 1. We can verify that y i | x i follows a multinomial logit model as shown in the theorem below.

Theorem 21.1 Under (21.10) and (21.11), we have

$pr(y i = k | x i ) = e α k +x t i β k K l=1 e α l +x t i β l$, where

$α k = log q k - 1 2 µ t k Σ -1 µ k , β k = Σ -1 µ k .$Prove Theorem 21.1.

## Hessian matrix in the multinomial logit model

Prove that the Hessian matrix [(21.4](#)) of the log-likelihood function of the multinomial logit model is negative semi-definite. Hint: Use Proposition 21.1.

## Iteratively reweighted least squares algorithm for the multinomial logit model

Similar to the binary logistic model, Newton's method for computing the MLE for the multinomial logit model can be written as iteratively reweighted least squares. Give the details.

## Score function of the proportional odds model

Derive the explicit formulas of the score function of the proportional odds model.

## Ordered Probit regression

If we choose ε i | x i ∼ N(0, 1) in [(21.5)](#), then the corresponding model is called the ordered Probit regression. Write down the likelihood function and derive the score function for this model. Remark: You can use the function polr in R to fit this model with the specification method = "probit".

## Case-control study and multinomial logistic model

This problem extends Theorem 20.1. Assume that pr(

$y i = k | x i ) = e α k +x t i β k K l=1 e α l +x t i β l and pr(s i = 1 | y i = k, x i ) = pr(s i = 1 | y i = k) = p k for k = 1, . . . , K. Show that pr(y i = k | x i , s i = 1) = e αk +x t i β k K l=1 e αl +x t i β l with αk = α k + log p k for k = 1, . . . , K.$
## Regression Models for Count Outcomes

A random variable for counts can take values in {0, 1, 2, . . .}. This type of variable is common in applied statistics. For example, it can represent how many times you visit the gym every week, how many lectures you have missed in the linear model course, how many traffic accidents happened in certain areas during certain periods, etc. This chapter focuses on statistical modeling of those outcomes given covariates. Hilbe ( [2014](#)) is a textbook focusing on count outcome regressions.

## Some random variables for counts

I first review four canonical choices of random variables for modeling count data. 

## Poisson

$k ∼ Poisson(λ k ) for k = 1, . . . , K, then y 1 + • • • + y K ∼ Poisson(λ), and (y 1 , . . . , y K ) | y 1 + • • • + y K = n ∼ Multinomial (n, (λ 1 /λ, . . . , λ K /λ))$,

$where λ = λ 1 + • • • + λ K . Conversely, if S ∼ Poisson(λ) with λ = λ 1 + • • • + λ K , and (y 1 , . . . , y K ) | S = n ∼ Multinomial (n, (λ 1 /λ, . . . , λ K /λ)), then y 1 , . . . , y K are mutually independent with y k ∼ Poisson(λ k ) for k = 1, . . . , K.$Where does the Poisson random variable come from? One way to generate Poisson is through independent Bernoulli random variables. I will review Le Cam (1960)'s theorem below without giving a proof. Theorem 22.1 Suppose X i 's are independent Bernoulli random variables with probabilities

$p i 's (i = 1, . . . , n). Define λ n = n i=1 p i and S n = n i=1 X i . Then ∞ k=0 pr(S n = k) -e -λn λ k n k! ≤ 2 n i=1 p 2 i .$As a special case, if p i = λ/n, then Theorem 22.1 implies

$∞ k=0 pr(S n = k) -e -λ λ k k! ≤ 2 n i=1 (λ/n) 2 = λ 2 /n → 0.$So the sum of IID Bernoulli random variables is approximately Poisson if the probability has order 1/n. This is called the law of rare events, or Poisson limit theorem, or Le Cam's theorem. By Theorem 22.1, we can use Poisson as a model for the sum of many rare events.

## Negative-Binomial

The Poisson distribution restricts that the mean must be the same as the variance. It cannot capture the feature of overdispersed data with variance larger than the mean. The Negative-Binomial is an extension of the Poisson that allows for overdispersion. Here the definition of the Negative-Binomial below is different from its standard definition, but it is more natural as an extension of the Poisson 1 . Define y as the Negative-Binomial random variable, denoted by NB(µ, θ) with µ > 0 and θ > 0, if

$y | λ ∼ Poisson(λ), λ ∼ Gamma(θ, θ/µ).(22.1)$So the Negative-Binomial is the Poisson with a random Gamma intensity, that is, the Negative-Binomial is a scale mixture of the Poisson. If θ → ∞, then λ is a point mass at µ and the Negative-Binomial reduces to Poisson(µ). We can verify that it has the following probability mass function.

Proposition 22.3 The Negative-Binomial random variable defined in ( [22](#formula_671).1) has the probability mass function

$pr(y = k) = Γ(k + θ) Γ(k + 1)Γ(θ) θ µ + θ θ µ µ + θ k , (k = 0, 1, 2, . . .). Proof of Proposition 22.3: We have pr(y = k) = ∞ 0 pr(y = k | λ)f (λ)dλ = ∞ 0 e -λ λ k k! (θ/µ) θ Γ(θ) λ θ-1 e -(θ/µ)λ dλ = (θ/µ) θ k!Γ(θ) ∞ 0 λ k+θ-1 e -(1+θ/µ)λ dλ.$The function in the integral is the density function of Gamma(k + θ, 1 + θ/µ) without the normalizing constant (1 + θ/µ) k+θ Γ(k + θ) .

1 With IID Bernoulli(p) trials, the Negative-Binomial distribution, denoted by y ∼ NB ′ (r, p), is the number of success before the rth failure. Its probability mass function is

$pr(y = k) = k + r -1 k (1 -p) r p k , (k = 0, 1, 2, . . .)$If p = µ/(µ + θ) and r = θ then these two definitions coincide. This definition is more restrictive because r must be an integer.

## So pr(y

$= k) = (θ/µ) θ k!Γ(θ) (1 + θ/µ) k+θ Γ(k + θ) = Γ(k + θ) k!Γ(θ) θ µ + θ θ µ µ + θ k .$
## □

We can derive the mean and variance of the Negative-Binomial. [Proposition 22.4](#) The Negative-Binomial random variable defined in [(22.1)](#) has moments

$E(y) = µ, var(y) = µ + µ 2 θ > E(y).$Proof of Proposition 22.4: Recall Proposition B.2 that a Gamma(α, β) random variable has mean α/β and variance α/β 2 . We have

$E(y) = E {E(y | λ)} = E(λ) = θ θ/µ = µ,and$$var(y) = E {var(y | λ)} + var {E(y | λ)} = E(λ) + var(λ) = θ θ/µ + θ (θ/µ) 2 = µ + µ 2 θ .$□ So the dispersion parameter θ controls the variance of the Negative-Binomial. With the same mean, the Negative-Binomial has a larger variance than Poisson. Figure [22](#fig_61).1 further compares the log probability mass functions of the Negative Binomial and Poisson. It shows that the Negative Binomial has a slightly higher probability at zero but much heavier tails than the Poisson.

## Zero-inflated count distributions

Many count distributions have larger masses at zero compared to Poisson and Negative-Binomial. Therefore, it is also important to have more general distributions capturing this feature of empirical data. We can simply add an additional zero component to the Poisson or the Negative-Binomial.

A zero-inflated Poisson random variable y is a mixture of two components: a point mass at zero and a Poisson(λ) random variable, with probabilities p and 1 -p, respectively. So y has the probability mass function

$pr(y = k) = p + (1 -p)e -λ , if k = 0, (1 -p)e -λ λ k k! , if k = 1, 2, . . . .$and moments below:

$Proposition 22.5 E(y) = (1 -p)λ and var(y) = (1 -p)λ(1 + pλ).$A zero-inflated Negative-Binomial random variable y is a mixture of two components:  

## Regression models for counts

To model a count outcome y i given x i , we can still use OLS. However, a problem with OLS is that the predicted value can be negative. This can be easily fixed by running OLS of log(y i + 1) given x i . However, this still does not reflect the fact that y i is a count outcome. For example, these two OLS fits cannot easily make a prediction for the probabilities pr(y i ≥ 1 | x i ) or pr(y i > 3 | x i ). A more direct approach is to model the conditional distribution of y i given x i using the distributions reviewed in Section 22.1.

## Poisson regression

Poisson regression assumes

$y i | x i ∼ Poisson(λ i ), λ i = λ(x i , β) = e x t i β .$So the mean and variance of

$y i | x i are E(y i | x i ) = var(y i | x i ) = e x t i β .$Because log E(y i | x i ) = x t i β, this model is sometimes called the log-linear model, with the coefficient β j interpreted as the conditional log mean ratio:

$log E(y i | . . . , x ij + 1, . . .) E(y i | . . . , x ij , . . .) = β j .$The likelihood function for independent Poisson random variables is

$L(β) = n i=1 e -λi λ yi i y i ! ∝ n i=1 e -λi λ yi i ,$and omitting the constants, we can write the log-likelihood function as

$log L(β) = n i=1 (-λ i + y i log λ i ) = n i=1 -e x t i β + y i x t i β .$The score function is

$∂ log L(β) ∂β = n i=1 -x i e x t i β + x i y i = n i=1 x i y i -e x t i β = n i=1 x i {y i -λ(x i , β)} ,$and the Hessian matrix is

$∂ 2 log L(β) ∂β∂β t = - n i=1 x i ∂ ∂β t e x t i β = - n i=1 e x t i β x i x t i$which is negative semi-definite. When the Hessian is negative definite, the MLE is unique. It must satisfy that

$n i=1 x i y i -e x t i β = n i=1 x i y i -λ(x i , β) = 0.$We can solve this nonlinear equation using Newton's method:

$β new = β old - ∂ 2 log L(β old ) ∂β∂β t -1 ∂ log L(β old ) ∂β = β old -(X t W old X) -1 X t (Y -Λ old ),$where

$X =    x t 1 . . . x t n    , Y =    y 1 . . . y n    and Λ old =    exp(x t 1 β old ) . . . exp(x t n β old )    , W old = diag exp(x t i β old ) n i=1 .$Similar to the derivation for the logit model, we can simplify Newton's method to

$β new = (X t W old X) -1 X t W old Z old ,$where

$Z old = Xβ old + (W old ) -1 (Y -Λ old ).$So we have an iterative reweighted least squares algorithm. In R, we can use the glm function with "family = poisson(link = "log")" to fit the Poisson regression, which uses Newton's method.

Statistical inference based on

$β a ∼ N    β, - ∂ 2 log L( β) ∂β∂β t -1    = N β, (X t Ŵ X) -1 , where Ŵ = diag exp(x t i β) n i=1$.

After obtaining the MLE, we can predict the mean E(y i | x i ) by λi = e x t i β . Because Poisson regression is a fully parametrized model, we can also predict any other probability quantities involving y i | x i . For example, we can predict pr(y i = 0 | x i ) by e -λi , and pr(y i ≥ 3 | x i ) by 1 -e -λi (1 + λi + λ2 i /2).

## Negative-Binomial regression

Negative-Binomial regression assumes

$y i | x i ∼ NB(µ i , θ), µ i = e x t i β ,$so it has conditional mean and variance:

$E(y i | x i ) = e x t i β , var(y i | x i ) = e x t i β (1 + e x t i β /θ).$It is also a log-linear model.

The log-likelihood function for Negative-Binomial regression is log

$L(β, θ) = n i=1 l i (β, θ) with l i (β, θ) = log Γ(y i + θ) -log Γ(y i + 1) -log Γ(θ) +θ log θ µ i + θ + y i log µ i µ i + θ ,$where µ i = e x t i β has partial derivative ∂µ i /∂β = µ i x i . We can use Newton's algorithm or Fisher scoring algorithm to compute the MLE ( β, θ) which requires deriving the first and second derivatives of log L(β, θ) with respect to (β, θ). I will derive some important components and relegate other details to Problem 22.1. First,

$∂ log L(β, θ) ∂β = n i=1 (1 + µ i /θ) -1 (y i -µ i )x i .$The corresponding first-order condition can be viewed as the estimating equation of Poisson regression with weights (1 + µ i /θ) -1 . Second,

$∂ 2 log L(β, θ) ∂β∂θ = n i=1 µ i (µ i + θ) 2 (y i -µ i )x i .$We can verify

$E ∂ 2 log L(β, θ) ∂β∂θ | X = 0$since each term inside the summation has conditional expectation zero. This implies that the Fisher information matrix is diagonal, so β and θ are asymptotically independent. The glm.nb in the MASS package iterate between β and θ: given θ, update β based on Fisher scoring; given β, update θ based on Newton's algorithm. It reports standard errors based on the inverse of the Fisher information matrix. [2](#foot_22)

## Zero-inflated regressions

The zero-inflated Poisson regression assumes that

$y i | x i ∼ 0, with probability p i , Poisson(λ i ), with probability 1 -p i ,$where

$p i = e x t i γ 1 + e x t i γ , λ i = e x t i β .$The zero-inflated Negative-Binomial regression assumes that

$y i | x i ∼ 0, with probability p i , NB(µ i , θ), with probability 1 -p i ,$where

$p i = e x t i γ 1 + e x t i γ , µ i = e x t i β .$To avoid over-parametrization, we can also restrict some coefficients to be zero. The zeroinfl function in the R package pscl can fit the zero-inflated Poisson and Negative-Binomial regressions.

## A case study

I will use the dataset from [Royer et al. (2015)](#b202) to illustrate the regressions for count outcomes. The R code is in code21.3.R. From the regression formula below, we are interested in the effect of two treatments incentive_commit and incentive on the number of visits to the gym, controlling for two pretreatment covariates target and member_gym_pre. > library ( " foreign " ) > library ( " MASS " ) > gym 1 = read . dta ( " gym _ treatment _ exp _ weekly . dta " ) > f . reg = weekly _ visit ~incentive _ commit + incentive + + target + member _ gym _ pre 

$> AIClm = 1 : lweekids > for ( i in 1 : lweekids ) + { + gymweek = gym 1 [ which ( gym 1 $ incentive _ week == weekids [ i ]) , ] + regweek = lm ( f . reg , data = gymweek ) + regweekcoef = summary ( regweek )$ coef + + c o e f i n c e n t i v e c o m m i t [ i ] = regweekcoef [ 2 , 1 ] + coefincentive [ i ] = regweekcoef [ 3 , 1 ] + s e i n c e n t i v e c o m m i t [ i ] = regweekcoef [ 2 , 2 ] + seincentive [ i ] = regweekcoef [ 3 , 2 ] + + AIClm [ i ] = AIC ( regweek ) + }$By changing the line with lm by regweek = glm ( f . reg , family = poisson ( link = " log " ) , data = gymweek ) and regweek = glm . nb ( f . reg , data = gymweek ) we obtain the corresponding results from Poisson and Negative-Binomial regressions. Figure [22](#fig_61).2 compares the regression coefficients with the associated confidence intervals over time. Three regressions give very similar patterns: incentive_commit has both short-term and longterm effects, but incentive only has short-term effects.

The left panel of Figure [22](#fig_61).3 shows that variances are larger than the means for outcomes from all weeks, and the right panel of Figure [22](#fig_61).3 shows the point estimates and confidence intervals of θ from Negative-Binomial regressions. Overall, overdispersion seems an important feature of the data.  

## Zero-inflated regressions

$[ i ] = regweekcoef $ count [ 2 , 1 ] + coefincentive [ i ] = regweekcoef $ count [ 3 , 1 ] + s e i n c e n t i v e c o m m i t [ i ] = regweekcoef $ count [ 2 , 2 ] + seincentive [ i ] = regweekcoef $ count [ 3 , 2 ] + + c o e f i n c e n t i v e c o m m i t 0 [ i ] = regweekcoef $ zero [ 2 , 1 ] + coefincentive 0 [ i ] = regweekcoef $ zero [ 3 , 1 ] + s e i n c e n t i v e c o m m i t 0 [ i ] = regweekcoef $ zero [ 2 , 2 ] + seincentive 0 [ i ] = regweekcoef $ zero [ 3 , 2 ] + + AIC 0 poisson [ i ] = AIC ( regweek ) + }$Replacing the line with zeroinfl by regweek = zeroinfl ( f . reg , dist = " negbin " , data = gymweek ) we can fit the corresponding zero-inflated Negative-Binomial regressions. but have effects on the zero components. This suggests that the treatments affect the outcome mainly by changing the workers' behavior of whether to go to the gym. Another interesting result is the large θ's from the zero-inflated Negative-Binomial regression:

> quantile ( gymtheta , probs = c ( 0 . 0 1 , 0 . 2 5 , 0 . 5 , 0 . 7 5 , 0 . 9 9 )) 1 % 2 5 % 5 0 % 7 5 % 9 9 % 1 2 . 3 1 3 . 1 1 3 . 7 1 4 . 4 1 5 . 7

Once the zero-inflated feature has been modeled, it is not crucial to account for the overdispersion. It is reasonable because the maximum outcome is five, ruling out heavytailedness. This is further corroborated by the following comparison of the AICs from five regression models. Figure [22](#fig_61).6 shows that zero-inflated Poisson regressions have the smallest AICs, beating the zero-inflated Negative-Binomial regressions, which are more flexible but have more parameters to estimate. 

## Overdispersion and zero-inflation

Show that for a zero-inflated Poisson, if p ≤ 1/2 then E(y) < var(y) always holds. What is the condition for E(y) < var(y) when p > 1/2?

## Poisson latent variable and the binary regression model with the cloglog link

Assume that y * i | x i ∼ Poisson(e x t i β ), and define y i = 1(y * i > 0) as the indicator that y * i is not zero. Show that y i | x i follows a cloglog model, that is,

$pr(y i = 1 | x i ) = g(x t i β),$where g(z) = 1 -exp(-e z ).

Remark: The cloglog model for binary outcome arises naturally from a latent Poisson model. It was only briefly mentioned in Chapter 20.

## Likelihood for the zero-inflated Poisson regression

Write down the likelihood function for the Zero-inflated Poisson model, and derive the steps for Newton's method.

## Likelihood for the Zero-inflated Negative-Binomial regression

Write down the likelihood function for the Zero-inflated Negative-Binomial model, and derive the steps for Newton's method.

## Generalized Linear Models: A Unification

This chapter unifies Chapters 20-22 under the formulation of the generalized linear model (GLM) by [Nelder and Wedderburn (1972)](#b183).

## Generalized Linear Models

So far we have discussed the following models for independent observations (y i , x i ) n i=1 .

Example 23.1 The Normal linear model for continuous outcomes assumes

$y i | x i ∼ N(µ i , σ 2 ), with µ i = x t i β.(23.1)$Example 23.2 The logistic model for binary outcomes assumes

$y i | x i ∼ Bernoulli(µ i ), with µ i = e x t i β 1 + e x t i β .(23.2)$Example 23.3 The Poisson model for count outcomes assumes

$y i | x i ∼ Poisson(µ i ), with µ i = e x t i β . (23.3) Example 23.4$The Negative-Binomial model for overdispersed count outcomes assumes

$y i | x i ∼ NB(µ i , δ), with µ i = e x t i β .(23.4)$We use δ for the dispersion parameter to avoid confusion because θ means something else below (Chapter 22 uses θ for the dispersion parameter).

In the above models, µ i denotes the conditional mean. This chapter will unify Examples 23.1-23.4 as GLMs.

## Exponential family

Consider a general conditional probability density or mass function:

$f (y i | x i ; θ i , ϕ) = exp y i θ i -b(θ i ) a(ϕ) + c(y i , ϕ) ,(23.5)$where (θ i , ϕ) are unknown parameters, and {a(•), b(•), c(•, •)} are known functions. The above conditional density [(23.5](#)) is called the natural exponential family with dispersion, where θ i is the natural parameter depending on x i , and ϕ is the dispersion parameter. Sometimes, a(ϕ) = 1 and c(y i , ϕ) = c(y i ), simplifying the conditional density to a natural exponential family.  have a unified structure as [(23.5)](#), as detailed below.

The logistic and Poisson models are simpler without the dispersion parameter. The Normal linear model has a dispersion parameter for the variance. The Negative-Binomial model is more complex: without fixing δ it does not belong to the exponential family with dispersion.

The exponential family [(23.5](#)) has nice properties derived from the classic Bartlett's identities [(Bartlett, 1953)](#b55). I first review Bartlett's identities: Lemma 23.1 Given a probability density or mass function f (y | θ) indexed by a scalar parameter θ, if we can change the order of expectation and differentiation, then

$E ∂ log f (y | θ) ∂θ = 0 and E ∂ log f (y | θ) ∂θ 2 = E - ∂ 2 log f (y | θ) ∂θ 2 .$This lemma is well-known in classic statistical theory for likelihood, and I give a simple proof below. 

$∂ ∂θ e ℓ(y|θ) ∂ ∂θ ℓ(y | θ)dy = 0 =⇒ e ℓ(y|θ) ∂ ∂θ ℓ(y | θ) 2 + e ℓ(y|θ) ∂ 2 ∂θ 2 ℓ(y | θ) dy = 0,$which implies Bartlett's second identity. □ Lemma 23.1 implies the moments of the exponential family [(23.5)](#).

Theorem 23.1 The first two moments of [(23.5)](#) are

$E(y i | x i ; θ i , ϕ) ≡ µ i = b ′ (θ i ) and var(y i | x i ; θ i , ϕ) ≡ σ 2 i = b ′′ (θ i )a(ϕ).$Proof of Theorem 23.1: The first two derivatives of the log conditional density are

$∂ log f (y i | x i ; θ i , ϕ) ∂θ i = y i -b ′ (θ i ) a(ϕ) , ∂ 2 log f (y i | x i ; θ i , ϕ) ∂θ 2 i = - b ′′ (θ i ) a(ϕ) . Lemma 23.1 implies that E y i -b ′ (θ i ) a(ϕ) = 0, E y i -b ′ (θ i ) a(ϕ) 2 = b ′′ (θ i ) a(ϕ) ,$which further imply the first two moments of y i given x i . □

## Generalized linear model

Section 23.1.1 is general, allowing the mean parameter µ i to depend on x i in an arbitrary way. This flexibility does not immediately generate a useful statistical procedure. To borrow information across observations, we assume that the relationship between y i and x i remain "stable" and can be captured by a fixed parameter β. A simple starting point is to use x t i β to approximate µ i , which, however, works naturally only for outcomes taking values in a wide range of (-∞, ∞). For general outcome variables, we can link its mean and the linear combination of covariates by µ i = µ(x t i β), where µ(•) is a known function and β is an unknown parameter. The inverse of µ(•) is called the link function. This is called a GLM, which has the following components:

(C1) the conditional distribution [(23.5](#)) as an exponential family with dispersion;

(C2) the conditional mean µ i = b ′ (θ i ) and variance σ 2 i = b ′′ (θ i )a(ϕ) in Theorem 23.1;

(C3) the function linking the conditional mean and covariates µ i = µ(x t i β). Models (23.1)- [(23.4](#)) are the classical examples. Figure [23](#fig_110).1 illustrates the relationship among key quantities in a GLM. In particular,  

$θ i = (b ′ ) -1 (µ i ) = (b ′ ) -1 {µ(x t i β)}($
## MLE for GLM

The contribution of unit i to the log-likelihood function is

$ℓ i = log f (y i | x i ; β, ϕ) = y i θ i -b(θ i ) a(ϕ) + c(y i , ϕ).$The contribution of unit i to the score function is

$∂ℓ i ∂β = ∂ℓ i ∂θ i ∂θ i ∂µ i ∂µ i ∂β ,$where

$∂ℓ i ∂θ i = y i -b ′ (θ i ) a(ϕ) , ∂θ i ∂µ i = 1 b ′′ (θ i ) = a(ϕ) σ 2 i follow from Theorem 23.1. So ∂ℓ i ∂β = y i -b ′ (θ i ) σ 2 i ∂µ i ∂β = y i -µ i σ 2 i ∂µ i ∂β ,$leading to the following score equation for the MLE:

$n i=1 y i -µ i σ 2 i ∂µ i ∂β = 0,(23.7)$or, more explicitly,

$n i=1 y i -µ(x t i β) σ 2 i µ ′ (x t i β)x i = 0$The general relationship [(23.6](#)) between θ i and β is quite complicated.

$A natural choice of µ(•) is to cancel (b ′ ) -1 in (23.6) so that µ(•) = b ′ (•) =⇒ θ i = x t i β.$This link function µ(•) is called the canonical link or the natural link, which leads to further simplifications:

$µ i = b ′ (x t i β) =⇒ ∂µ i ∂β = b ′′ (x t i β)x i = b ′′ (θ i )x i = σ 2 i a(ϕ) x i ,and$$∂ℓ i ∂β = y i -µ i σ 2 i σ 2 i a(ϕ) x i = a(ϕ) -1 x i (y i -µ i ) =⇒ a(ϕ) -1 n i=1 x i (y i -µ i ) = 0 =⇒ n i=1 x i (y i -µ i ) = 0. (23.8)$We have shown that the MLEs of models (23.1)-( [23](#formula_706).3) all satisfy [(23.8)](#). However, the MLE of [(23.4)](#) does not because it does not use the natural link function resulting in µ(

$•) ̸ = b ′ (•): µ( * ) = e * , b ′ ( * ) = δ e * 1 -e * .$Using Bartlett's second identity in Lemma 23.1, we can write the expected Fisher information conditional on covariates as

$n i=1 E ∂ℓ i ∂β ∂ℓ i ∂β t | x i = n i=1 E y i -µ i σ 2 i 2 ∂µ i ∂β ∂µ i ∂β t | x i = n i=1 1 σ 2 i ∂µ i ∂β ∂µ i ∂β t = n i=1 1 σ 2 i {µ ′ (x t i β)} 2 x i x t i = X t W X,$where

$W = diag 1 σ 2 i {µ ′ (x t i β)} 2 n i=1$.

With the canonical link, it further simplifies to

$n i=1 E ∂ℓ i ∂β ∂ℓ i ∂β t | x i = n i=1 E y i -µ i a(ϕ) 2 x i x t i | x i = {a(ϕ)} -2 n i=1 σ 2 i x i x t i .$We can obtain the estimated covariance matrix by replacing the unknown parameters with their estimates. Now we review the estimated covariance matrices of the classical GLMs with canonical links. [Example 23.1 (continued)](#) In the Normal linear model, V = σ2 (X t X) -1 with σ 2 estimated further by the residual sum of squares. [Example 23.2 (continued)](#) In the binary logistic model, V = (X t Ŵ X) -1 with Ŵ = diag{π i (1 -πi )} n i=1 , where πi = e x t i β /(1 + e x t i β ).

Example [23.3 (continued)](#) In the Poisson model, V = (X t Ŵ X) -1 with Ŵ = diag{ λi } n i=1 , where λi = e x t i β .

I relegate the derivation of the formula for the Negative-Binomial regression as Problem 23.2. It is a purely theoretical exercise since δ is usually unknown in practice.

## Other GLMs

The glm function in R allows for the specification of the family parameters, with the corresponding canonical link functions shown below:

binomial ( link = " logit " ) gaussian ( link = " identity " ) Gamma ( link = " inverse " ) inverse . gaussian ( link = " 1 / mu ^2 " ) poisson ( link = " log " ) quasi ( link = " identity " , variance = " constant " ) quasibinomial ( link = " logit " ) quasipoisson ( link = " log " ) 3 correspond to the second, the first, and the fifth choices above. Below I will briefly discuss the third choice for the Gamma regression and omit the discussion of other choices. See the help file of the glm function and [McCullagh and Nelder (1989)](#b176) for more details.

The Gamma(α, β) random variable is positive with mean α/β and variance α/β 2 . For convenience in modeling, we use a reparametrization Gamma ′ (µ, ν) where

$µ ν = α/β α ⇐⇒ α β = ν ν/µ .$So its mean equals µ and its variance equals µ 2 /ν which is quadratic in µ. A feature of the Gamma random variable is that its coefficient of variation equals 1/ √ ν which does not depend on the mean. So Gamma ′ (µ, ν) is a parametrization based on the mean and the coefficient of variation [(McCullagh and Nelder, 1989](#b176)).[foot_23](#foot_23) Gamma ′ (µ, ν) has density

$f (y) = β α Γ(α) y α-1 e -βy = (ν/µ) ν Γ(ν) y ν-1 e -(ν/µ)y ,$and we can verify that it belongs to the exponential family with dispersion. Gamma regression assumes

$y i | x i ∼ Gamma ′ (µ i , ν)$where µ i = e x t i β . So it is also a log-linear model. This does not correspond to the canonical link. Instead, we should specify Gamma(link = "log") to fit the log-linear Gamma regression model.

The log-likelihood function is

$log L(β, ν) = n i=1 - νy i e x t i β + (ν -1) log y i + ν log ν -νx t i β -log Γ(ν) . Then ∂ log L(β, ν) ∂β = n i=1 (νy i e -x t i β x i -νx i ) = ν n i=1 e -x t i β (y i -e x t i β )x i and ∂ 2 log L(β, ν) ∂β∂ν = n i=1 e -x t i β (y i -e x t i β )x i .$So the MLE for β solves the following estimating equation

$n i=1 e -x t i β (y i -e x t i β )x i = 0.$Moreover, ∂ 2 log L(β, ν)/∂β∂ν has expectation zero so the Fisher information matrix is diagonal. In fact, it is identically zero when evaluated at β since it is identical to the estimating equation. I end this subsection with a comment on the estimating equation of β. It is similar to the Poisson score equation except for the additional weight e -x t i β . For positive outcomes, it is also conventional to fit OLS of log y i on x i , resulting in the following estimating equation [Firth (1988)](#b98) compared Gamma and log-Normal regressions based on efficiency. However, these two models are not entirely comparable: Gamma regression assumes that the log of the conditional mean of y i given x i is linear in x i , whereas log-Normal regression assumes that the conditional mean of log y i given x i is linear in x i . By Jensen's inequality, log E(

$n i=1 (log y i -x t i β)x i = 0.$$y i | x i ) ≥ E(log y i | x i ). See Problem 23.$3 for more discussions of Gamma regression.

## Homework problems

## MLE in GLMs with binary regressors

The MLEs for β in Models ( [23](#formula_706).1)-( [23](#formula_706).3) do not have explicit formulas in general. But in the special case with the covariate x i containing 1 and a binary covariate z i ∈ {0, 1}, their MLEs do have simple formulas. Find them in terms of sample means of the outcomes. Then find the variance estimators of β.

## Negative-Binomial covariance matrices

Assume that δ is known. Derive the estimated asymptotic covariance matrices of the MLE in the Negative-Binomial regression with µ i = e x t i β .

## Gamma regression

Verify that Gamma ′ (µ, ν) belongs to the natural exponential family with dispersion. Derive the first-and second-order derivatives of the log-likelihood function and Newton's algorithm for computing the MLE. Derive the estimated asymptotic covariance matrices of the MLE. Show that if

$y i | x i ∼ Gamma ′ (µ i , ν) with µ i = e x t i β , then E(log y i | x i ) = ψ(ν) -log(ν) + x t i β and var(log y i | x i ) = ψ ′ (ν)$where ψ(ν) = d log Γ(ν)/dν is the digamma function and ψ ′ (ν) is the trigamma function.

Remark: Use Proposition B.3 to calculate the moments. The above conditional mean function ensures that the OLS estimator of log y i on x i is consistent for all components of β except for the intercept.

From Generalized Linear Models to Restricted Mean Models: the Sandwich Covariance Matrix This chapter discusses the consequence of misspecified GLMs, extending the EHW covariance estimator to its analogs under the GLMs. It serves as a stepping stone to the next chapter on the generalized estimating equation.

## Restricted mean model

The logistic, Poisson, and Negative-Binomial models are extensions of the Normal linear model. All of them are fully parametric models. However, we have also discussed OLS as a restricted mean model E(y i | x i ) = x t i β without imposing any additional assumptions (e.g., the variance) on the conditional distribution. The restricted mean model is a semiparametric model. Then a natural question is: what are the analogs of the restricted mean model for the binary and count models?

Binary outcome is too special because the conditional mean determines the distribution. So if we assume that the conditional mean is µ i = e x t i β /(1 + e x t i β ), then conditional distribution must be Bernoulli(µ i ). Consequently, misspecification of the conditional mean function implies misspecification of the whole conditional distribution.

For other outcomes, the conditional mean cannot determine the conditional distribution. If we assume E(y i | x i ) = µ(x t i β), we can verify that

$E n i=1 y i -µ(x t i β) σ2 (x i , β) ∂µ(x t i β) ∂β = E E n i=1 y i -µ(x t i β) σ2 (x i , β) ∂µ(x t i β) ∂β | x i = 0$for any σ2 that can be a function of x i , the true parameter β, and maybe ϕ. So we can estimate β by solving the estimating equation:

$n i=1 y i -µ(x t i β) σ2 (x i , β) ∂µ(x t i β) ∂β = 0. (24.1) If σ2 (x i , β) = σ 2 (x i ) = var(y i | x i$), then the above estimating equation is the score equation derived from the GLM of an exponential family. If not, (24.1) is not a score function but it is still a valid estimating equation. In the latter case, σ2 (x i , β) is a "working" variance. This has important implications for the practical data analysis. First, we can interpret the MLE from a GLM more broadly: it is also valid under a restricted mean model even if the conditional distribution is misspecified. Second, we can construct more general estimators beyond the MLEs from GLMs. However, we must address the issue of variance estimation since the inference based on the Fisher information matrix no longer works in general.

## Sandwich covariance matrix

To simplify the notation, we assume (x i , y i ) n i=1 are IID draws although we usually view the covariates as fixed. This additional assumption is innocuous as the final inferential procedures are identical.

$Theorem 24.1 Assume (x i , y i ) n i=1 are IID with E(y i | x i ) = µ(x t i β). We have √ n β -β → N(0, B -1 M B -1 ) with B = E 1 σ2 (x, β) ∂µ(x t β) ∂β ∂µ(x t β) ∂β t ,(24.2)$$M = E σ 2 (x) {σ 2 (x, β)} 2 ∂µ(x t β) ∂β ∂µ(x t β) ∂β t . (24.3)$Proof of Theorem 24.1: Applying Theorem D.1 to

$w = (x, y), m(w, β) = y -µ(x t b) σ2 (x, β) ∂µ(x t β) ∂β ,$we can derive the asymptotic distribution of the β. The bread matrix equals

$B = -E ∂m(w, β) ∂β t = -E ∂ ∂β t y -µ(x t β) σ2 (x, β) ∂µ(x t β) ∂β = -E y -µ(x t β) σ2 (x, β) ∂ 2 µ(x t β) ∂β∂β t + ∂µ(x t β) ∂β ∂ ∂β t y -µ(x t β) σ2 (x, β) (24.4) = -E ∂µ(x t β) ∂β ∂ ∂β t y -µ(x t β) σ2 (x, β) = E ∂µ(x t β) ∂β ∂µ(x t β) ∂β t /σ 2 (x, β) + ∂µ(x t β) ∂β ∂ σ2 (x, β) ∂β t y -µ(x t β) {σ 2 (x, β)} 2 (24.5) = E ∂µ(x t β) ∂β ∂µ(x t β) ∂β t /σ 2 (x, β) ,$where the first term of [(24.4](#)) and the second term of (24.5) are both zero under the restricted mean model. The meat matrix equals

$M = E {m(w, β)m(w, β) t } = E y -µ(x t β) σ2 (x) 2 ∂µ(x t β) ∂β ∂µ(x t β) ∂β t = E σ 2 (x) {σ 2 (x)} 2 ∂µ(x t β) ∂β ∂µ(x t β) ∂β t .$□ We can estimate the asymptotic variance by replacing B and M by their sample analogs. With β and the residual εi = y i -µ(x t i β), we can conduct statistical inference based on the following Normal approximation:

$β a ∼ N(β, V ), with V ≡ n -1 B-1 M B-1 , where B = n -1 n i=1 1 σ2 (x i , β) ∂µ(x t i β) ∂β ∂µ(x t i β) ∂β t , M = n -1 n i=1 ε2 i σ4 (x i , β) ∂µ(x t i β) ∂β ∂µ(x t i β) ∂β t .$As a special case, when the GLM is correctly specified with σ 2 (x) = σ2 (x, β), then B = M and the asymptotic variance reduces to the inverse of the Fisher information matrix discussed in Section 23.2.

Example 24.1 (continued) In a working Normal linear model, σ2 (x i , β) = σ2 is constant and ∂µ(x t i β)/∂β = x i . So

$B = n -1 n i=1 1 σ2 x i x t i , M = n -1 n i=1 ε2 i (σ 2 ) 2 x i x t i with residual εi = y i -x t i β, recovering the EHW variance estimator V = n i=1 x i x t i -1 n i=1 ε2 i x i x t i n i=1 x i x t i -1$.

Example 24.2 (continued) In a working binary logistic model, σ2 (x i , β) = π(x i , β){1π(x i , β)} and ∂µ(x t i β)/∂β = π(x i , β){1 -π(x i , β)}x i , where π(x i , β) = µ(x t i β) = e x t i β /(1 + e x t i β ). So

$B = n -1 n i=1 πi (1 -πi )x i x t i , M = n -1 n i=1 ε2 i x i x t i$with fitted mean πi = e x t i β /(1 + e x t i β ) and residual εi = y i -πi , yielding a new covariance estimator

$V = n i=1 πi (1 -πi )x i x t i -1 n i=1 ε2 i x i x t i n i=1 πi (1 -πi )x i x t i -1 . Example 24.3 (continued) In a working Poisson model, σ2 (x i , β) = λ(x i , β) and ∂µ(x t i β)/∂β = λ(x i , β)x i , where λ(x i , β) = µ(x t i β) = e x t i β . So B = n -1 n i=1 λi x i x t i , M = n -1 n i=1 ε2 i x i x t i$with fitted mean λi = e x t i β and residual εi = y i -λi , yielding a new covariance estimator

$V = n i=1 λi x i x t i -1 n i=1 ε2 i x i x t i n i=1 λi x i x t i -1$.

Again, I relegate the derivation of the formulas for the Negative-Binomial regression as a homework problem. The R package sandwich implements the above covariance matrices [(Zeileis, 2006)](#b247).

## Applications of the sandwich standard errors

## Linear regression

In R, several functions can compute the EHW standard error: the hccm function in the car package, and the vcovHC and sandwich functions in the sandwich package. The first two are special functions for OLS, and the third one works for general models. Below, we use these functions to compute various types of standard errors.

> library ( " car " ) > library ( " lmtest " ) > library ( " sandwich " ) > library ( " mlbench " ) > > # # linear regression > data ( " BostonHousing " ) > lm . boston = lm ( medv ~. , data = BostonHousing ) > hccm 0 = hccm ( lm . boston , type = " hc 0 " ) > sandwich 0 = sandwich ( lm . boston , adjust = FALSE ) > vcovHC 0 = vcovHC ( lm . boston , type = " HC 0 " ) > > hccm 1 = hccm ( lm . boston , type = " hc 1 " ) > sandwich 1 = sandwich ( lm . boston , adjust = TRUE ) > vcovHC 1 = vcovHC ( lm . boston , type = " HC 1 " ) > > hccm 3 = hccm ( lm . boston , type = " hc 3 " ) > vcovHC 3 = vcovHC ( lm . boston , type = " HC 3 " ) > > dat . reg = data . frame ( hccm 0 = diag ( hccm 0 )^( 0 . 5 ) , + sandwich 0 = diag ( sandwich 0 )^( 0 . 5 ) , + vcovHC 0 = diag ( vcovHC 0 )^( 0 . 5 ) , + + hccm 1 = diag ( hccm 1 )^( 0 . 5 ) , + sandwich 1 = diag ( sandwich 1 )^( 0 . 5 ) , + vcovHC 1 = diag ( vcovHC 1 )^( 0 . 5 ) , + + hccm 3 = diag ( hccm 3 )^( 0 . 5 ) , + vcovHC 3 = diag ( vcovHC 3 )^( 0 . 5 )) > round ( dat . reg [ -1 , ] , 2 ) hccm 0 sandwich 0 vcovHC 0 hccm 1 sandwich 1 vcovHC 1 hccm 3 vcovHC 3 crim 0 . 0 3 0 . 0 3 0 . 0 3 0 . 0 3 0 . 0 3 0 . 0 3 0 . 0 3 0 . 0 3 zn 0 . 0 1 0 . 0 1 0 . 0 1 0 . 0 1 0 . 0 1 0 . 0 1 0 . 0 1 0 . 0 1 indus 0 . 0 5 0 . 0 5 0 . 0 5 0 . 0 5 0 . 0 5 0 . 0 5 0 . 0 5 0 . 0 5 chas 1 1 .

2 8 1 . 2 8 1 . 2 8 1 . 2 9 1 . 2 9 1 . 2 9 1 . 3 5 1 . 3 5 nox 3 . 7 3 3 . 7 3 3 . 7 3 3 . 7 9 3 . 7 9 3 . 7 9 3 . 9 2 3 . 9 2 rm 0 . 8 3 0 . 8 3 0 . 8 3 0 . 8 4 0 . 8 4 0 . 8 4 0 . 8 9 0 . 8 9 age 0 . 0 2 0 . 0 2 0 . 0 2 0 . 0 2 0 . 0 2 0 . 0 2 0 . 0 2 0 . 0 2 dis 0 . 2 1 0 . 2 1 0 . 2 1 0 . 2 1 0 . 2 1 0 . 2 1 0 . 2 2 0 . The sandwich function can compute HC0 and HC1, corresponding to adjusting for the degrees of freedom or not; hccm and vcovHC can compute other HC standard errors.

Estimate Std . Error z value Pr ( >| z |) ( Intercept ) -6 . 6 7 6 4 1 2 . 4 6 0 3 5 -2 . 7 1 3 6 0 . 0 0 6 6 5 6 ** x 1 . 1 0 8 3 2 0 . 3 9 6 7 2 2 . 7 9 3 7 0 . 0 0 5 2 1 1 **

24.3.3 Poisson regression 24.3.3.1 A correctly specified Poisson regression I first generate data from a correctly specified Poisson regression. The two types of standard errors are very close. Because the true model is the Negative-Binomial regression, we can use the correct model to fit the data. Theoretically, the MLE is the most efficient estimator. However, in this particular dataset, the robust standard error from Poisson regression is no larger than the one from Negative-Binomial regression. Moreover, the robust standard errors from the Poisson and Negative-Binomial regressions are very close. reasonable if the parameter of interest is the risk ratio instead of the odds ratio. Importantly, since the Poisson model is a wrong model, we must use the sandwich covariance estimator. [24.3.5](#) How robust are the robust standard errors? Section 24.1 discusses the restricted mean model as an extension of the GLM, allowing for misspecification of the GLM while still preserving the conditional mean. We can extend the discussion to other parametric models. [Huber (1967)](#b146) started the literature on the statistical properties of the MLE in a misspecified model, and [White (1982)](#b238) addressed detailed inferential problems. [Buja et al. (2019b)](#b67) reviewed this topic recently.

The discussion in Section 24.1 is useful when the conditional mean is correctly specified. However, if we think the GLM is severely misspecified with a wrong conditional mean, then the robust sandwich standard errors are unlikely to be helpful because the MLE converges to a wrong parameter in the first place [(Freedman, 2006)](#b109).

## Homework problems

## MLE in GLMs with binary regressors

Continue with Problem 23.1. Find the variance estimators of β without assuming the models are correct.

## Negative-Binomial covariance matrices

Continue with Problem 23.2. Derive the estimated asymptotic covariance matrices of the MLE without assuming the Negative-Binomial model is correct.

## Robust standard errors in the Karolinska data

Report the robust standard errors in the case study of Section 21.5 in Chapter 21. For some models, the function coeftest(*, vcov = sandwich) does work. Alternatively, you can use the nonparametric bootstrap to obtain the robust standard errors.

## Robust standard errors in the gym data

Report the robust standard errors in the case study of Section 22.3 in Chapter 22.

## Generalized Estimating Equation for Correlated Multivariate Data

Previous chapters dealt with cross-sectional data, that is, we observe n units at a particular time point, collecting various covariates and outcomes. In addition, we assume that these units are independent, and sometimes, we even assume they are IID draws. Many applications have correlated data. Two canonical examples are (E1) repeated measurements of the same set of units over time, which are often called longitudinal data in biostatistics [(Fitzmaurice et al., 2012)](#b103) or panel data in econometrics [(Wooldridge, 2010)](#b240); and

(E2) clustered observations belonging to classrooms, villages, etc, which are common in cluster-randomized experiments in education [(Schochet, 2013)](#b207) and public health [(Turner et al., 2017a,b)](#).

Many excellent textbooks cover this topic intensively. This chapter focuses on a simple yet powerful strategy, which is a natural extension of the GLM discussed in the last chapter.

It was initially proposed in [Liang and Zeger (1986)](#b165), the most cited paper published in Biometrika in the past one hundred years [(Titterington, 2013)](#b221). For simplicity, we will use the term "longitudinal data" for general correlated data.

## Examples of correlated data

## Longitudinal data

We have used the data from [Royer et al. (2015)](#b202) in Chapter 22. Each worker's number of gym visits was repeatedly measured over more than 100 weeks. It is a standard longitudinal dataset. In Chapter 22, we conducted analysis for each week separately, and in this chapter, we will accommodate the longitudinal structure of the data.

25.1.2 Clustered data: a neuroscience experiment [Moen et al. (2016)](#b180) examined the effects of Pten knockdown and fatty acid delivery on soma size of neurons in the brain of a mouse. The useful variables for our analysis are the id of mouse mouseid, the fatty acid level fa, the Pten knockdown indicator pten, the outcome somasize, the number of neurons numpten and numctrl under Pten knockdown or not.

> Pten = read . csv ( " P t e n A n a l y s i s D a t a . csv " ) [[ , -( 7 : 9 )](#)] > head ( Pten ) mouseid fa pten somasize numctrl numpten 1 0 0 0 8 3 . 8 3 7 3 0 4 4 2 0 0 0 6 9 . 9 8 4 3 0 4 4 3 0 0 0 8 2 . 1 2 8 3 0 4 4 4 0 0 8 6 . 4 4 6 3 0 4 4 5 0 0 7 4 . 0 3 2 3 0 4 4 6 0 0 7 1 . 6 9 3 3 0 4 4

The three-way table below shows the treatment combinations for 14 mice, from which we can see that the Pten knockdown indicator varies within mice, but the fatty acid level varies only between mice.

> table ( Pten $ mouseid , Pten $ fa , Pten $ pten ) , , = 0 0 1 0 3 0 0 1 5 8 0 2 1 8 0 3 2 0 4 5 6 0 5 0 3 9 6 0 3 3 7 0 5 8 8 0 6 0 9 0 0 1 0 0 0 1 1 0 0 1 2 0 0 1 3 0 0 , , = 1 0 1 0 4 4 0 1 6 8 0 2 3 3 0 3 1 1 0 4 7 6 0 5 0 5 5 6 0 5 5 7 0 7 5 8 0 9 2 9 0 0 1 0 0 0 1 1 0 0 1 2 0 0 1 3 0 0

## Clustered data: a public health intervention

Poor sanitation leads to morbidity and mortality in developing countries. In 2012, [Guiteras et al. (2015)](#b131) conducted a cluster-randomized experiment in rural Bangladesh to evaluate the effectiveness of different policies on the use of hygienic latrines. To illustrate our theory, we use a subset of their original data and exclude the households not eligible for subsidies or with missing outcomes, resulting in 10125 households in total. The median, mean, and maximum of village size are 83, 119, and 500, respectively. We choose the outcome y it as the binary indicator for whether the household (i, t) had access to a hygienic latrine or not, measured in June 2013, and covariate x it as the baseline access rate to hygienic latrines in the community that household (i, t) belonged to, measured in January 2012 before the experiment.

The useful variables below are z, x, y, and vid, which denote the binary treatment indicator, covariate x it , the outcome, and the village id vid, > hygaccess = read . csv ( " hygaccess . csv " ) > hygaccess = hygaccess [ , c ( " r 4 _ hyg _ access " , " treat _ cat _ 1 " , + " bl _ c _ hyg _ access " , " vid " , " eligible " )] > hygaccess = hygaccess [ which ( hygaccess $ eligible == " Eligible " & + hygaccess $ r 4 _ hyg _ access != " Missing " ) ,] > hygaccess $ y = ifelse ( hygaccess $ r 4 _ hyg _ access == " Yes " , 1 , 0 ) > hygaccess $ z = hygaccess $ treat _ cat _ 1 > hygaccess $ x = hygaccess $ bl _ c _ hyg _ access

## Marginal model and the generalized estimating equation

We will extend the restricted mean model to deal with longitudinal data, where we observe outcome y it and covariate x it for each unit i = 1, . . . , n at time t = 1, . . . , n i . The n i 's can vary across units. When n i = 1 for all units, we drop the time index and model the conditional mean as E(y i | x i ) = µ(x t i β), and use the following estimating equation to estimate the parameter β:

$n i=1 y i -µ(x t i β) σ2 (x i , β) ∂µ(x t i β) ∂β = 0. (25.1)$In (25.1), σ2 (x i , β) is a working variance function usually motivated by a GLM, but it can be misspecified. With an n i × 1 vector outcome and an n i × p covariate matrix

$Y i =    y i1 . . . y ini    , X i =    x t i1 . . . x t ini    , (i = 1, . . . , n) (25.2)$we can extend the restricted mean model to [25.6)](#) where [(25.](#)3) and [(25.6) are definitions, and (25.4](#)) and [(25.5](#)) are the two key assumptions. Assumption [(25.4](#)) requires that the conditional mean of y it depends only on x it but not on any other x is with s ̸ = t. Assumption [(25.5)](#) requires that the relationship between x it and y it is stable across units and time points with the function µ(•) and the parameter β not varying with respect to i or t. The model assumptions in [(25.4](#)) and [(25.5](#)) are really strong, and I defer the critiques to the end of this chapter. Nevertheless, the marginal model attracts practitioners for (A1) its similarity to GLM and the restricted mean model, and

$E(Y i | X i ) ≡    E(y i1 | X i ) . . . E(y ini | X i )    (25.3) =    E(y i1 | x i1 ) . . . E(y ini | x ini )    (25.4) =    µ(x t i1 β) . . . µ(x t ini β)    (25.5) ≡ µ(X i , β),($(A2) its simplicity of requiring only specification of the marginal conditional means, not the whole joint distribution.

The advantage (A1) facilitates the interpretation of the coefficient, and the advantage (A2) is crucial because of the lack of familiar multivariate distributions in statistics except for the multivariate Normal. The generalized estimating equation (GEE) for β is the vector form of (25.1):

$n i=1 ∂µ(X i , β) ∂β p×ni Ṽ -1 (X i , β) ni×ni {Y i -µ(X i , β)} ni×1 = 0 p×1 ,(25.7)$where [(25.7](#)) has a similar form as (25.1) with three terms organized to match the dimension so that matrix multiplications are well-defined:

(GEE1) the last term

$Y i -µ(X i , β) =    y i1 -µ(x t i1 β) . . . y ini -µ(x t ini β)   $represents the residual vector, (GEE2) the second term is the inverse of Ṽ (X i , β), a working covariance matrix of the conditional distribution of Y i given X i which may be misspecified:

$Ṽ (X i , β) ̸ = V (X i ) ≡ cov(Y i | X i ).$It is relatively easy to specify the working variance σ2 (x it , β) for each marginal component, for example, based on the marginal GLM. So the key is to specify the

$n i × n i dimensional correlation matrix R i to obtain Ṽ (X i , β) = diag{σ(x it , β)} ni i=1 R i diag{σ(x it , β)} ni i=1 .$We assume that the R i 's are given now, and will discuss how to choose them in a later section.

(GEE3) the first term is the partial derivative of an n i × 1 vector µ(X i , β) = (µ(x t i1 β), . . . , µ(x t ini β)) t with respect to a p × 1 vector β = (β 1 , . . . , β p ) t :

$∂µ(X i , β) ∂β = ∂µ(x t i1 β) ∂β , . . . , ∂µ(x t ini β) ∂β =      ∂µ(x t i1 β) ∂β1 • • • ∂µ(x t in i β) ∂β1 . . . . . . ∂µ(x t i1 β) ∂βp • • • ∂µ(x t in i β) ∂βp     $, which is a p × n i matrix, denoted by D i (β).

## Statistical inference with GEE

## Computation using the Gauss-Newton method

We can use Newton's method to solve the GEE [(25.7)](#). However, calculating the derivative of the left-hand side of (25.7) involves calculating the second order derivative of µ(X i , β) with respect to β. A simpler alternative without calculating the second-order derivative is the Gauss-Newton method based on the following approximation:

$0 = n i=1 ∂µ(X i , β) ∂β Ṽ -1 (X i ) {Y i -µ(X i , β)} ∼ = n i=1 D i (β old ) Ṽ -1 (X i , β old ) Y i -µ(X i , β old ) -D t i (β old )(β -β old ) = n i=1 D i (β old ) Ṽ -1 (X i , β old ) Y i -µ(X i , β old ) - n i=1 D i (β old ) Ṽ -1 (X i , β old )D t i (β old )(β -β old ).$So given β old , we update it as

$β new = β old + n i=1 D i (β old ) Ṽ -1 (X i , β old )D t i (β old ) -1 × n i=1 D i (β old ) Ṽ -1 (X i , β old ) Y i -µ(X i , β old ) . (25.8)$
## Asymptotic inference

The asymptotic distribution of β follows from Theorem D.2. Similar to the proof of Theorem 24.1, we can verify that √ n( β -β) → N(0, B -1 M B -1 ) in distribution where

$B = E n -1 n i=1 D i (β) Ṽ -1 (X i , β)D t i (β) , M = E n -1 n i=1 D i (β) Ṽ -1 (X i , β)V (X i ) Ṽ -1 (X i , β)D t i (β) .$After obtaining β and the residual vector εi = Y i -µ(X i , β) for unit i (i = 1, . . . , n), we can conduct asymptotic inference based on the Normal approximation

$β a ∼ N(β, n -1 B-1 M B-1 ),$where

$B = n -1 n i=1 D i ( β) Ṽ -1 (X i , β)D t i ( β), M = n -1 n i=1 D i ( β) Ṽ -1 (X i , β)ε i εt i Ṽ -1 (X i , β)D t i ( β).$This covariance estimator proposed by [Liang and Zeger (1986)](#b165), is robust to the misspecification of the marginal variances and the correlation structure as long as the conditional mean of Y i given X i is correctly specified.

## Implementation: choice of the working covariance matrix

We have not discussed the choice of the working correlation matrix R i . Different choices do not affect the consistency but affect the efficiency of β. A simple starting point is the independent working correlation matrix R i = I ni . Under this correlation matrix, the GEE reduces to

$n i=1 ∂µ(x t i1 β) ∂β , . . . , ∂µ(x t ini β) ∂β    σ-2 (x i1 , β) . . . σ-2 (x ini , β)    ×    y i1 -µ(x t i1 β) . . . y ini -µ(x t ini β)    = 0,$or, more compactly,

$n i=1 ni t=1 y it -µ(x t it β) σ2 (x it , β) ∂µ(x t it β) ∂β = 0. (25.9)$This is simply the estimating equation of a restricted mean model treating all data points (i, t) as independent observations. This implies that the point estimate assuming the independent working correlation matrix is still consistent, although we must change the standard error as in Section 25.3.2. With this simple starting point, we have a consistent yet inefficient estimate of β, and then we can compute the residuals. The correlation among the residuals contains information about the true covariance matrix. With small and equal n i 's, we can estimate the conditional covariance without imposing any structure based on the residuals. Using the estimated covariance matrix, we can update the GEE estimate to improve efficiency. This leads to a two-step procedure.

An important intermediate case is motivated by the exchangeability of the data points within the same unit i, so the working covariance matrix is Ṽ (X

$i , β) = diag{σ(x it )} ni i=1 R i (ρ)diag{σ(x it )} ni i=1 , where R i (ρ) =      1 ρ • • • ρ ρ 1 • • • ρ . . . . . . . . . ρ ρ • • • 1     $.

We can estimate ρ based on the residuals from the first step.

The above three choices of the working covariance matrix are called "independent", "unstructured", and "exchangeable" in the "corstr" parameter of the function gee in the gee package in R. This function also contains other choices proposed by [Liang and Zeger (1986)](#b165).

A carefully chosen working covariance matrix can lead to efficiency gain compared to the simple independent covariance matrix. An efficient estimator requires a correctly specified working covariance matrix. This is often an infeasible goal, and what is more, the conditional covariance cov(Y i | X i ) is a nuisance parameter if the conditional mean is the main parameter of interest. In practice, the independent working covariance suffices in many applications despite its potential efficiency loss. This is similar to the use of OLS in the presence of heteroskedasticity in linear models. [Section 25.4](#) focuses on the independent working covariance, which is common in econometrics. [Section 25.6](#) gives further justifications for this simple strategy.

## A special case: cluster-robust standard error

Importantly, [Liang and Zeger (1986)](#b165)'s standard error treats each cluster i as an independent contributor to the uncertainty. In econometrics, this is called the cluster-robust standard error. Alternatively, we can use the bootstrap by resampling the clusters to approximate the asymptotic covariance matrix. I will discuss linear and logistic regressions in this section, and leave the technical details of Poisson regression as a homework problem.

Stack the Y i 's and X i 's in [(25.2)](#) together to obtain

$Y =    Y 1 . . . Y n    , X =    X 1 . . . X n    ,$which are the N dimensional outcome vector and N × p covariate matrix, where N = n i=1 n i .

## OLS

An important special case is the marginal linear model with an independent working covariance matrix and homoskedasticity, resulting in the following estimating equation:

$n i=1 ni t=1$x it (y it -x t it β) = 0.

So the point estimator is just the pooled OLS using all data points:

$β = n i=1 ni t=1 x it x t it -1 n i=1 ni t=1 x it y it = n i=1 X t i X i -1 n i=1 X t i Y i = (X t X) -1 X t Y.$The three forms of β above are identical: the first one is based on N observations, the second one is based on n independent units, and the last one is based on the matrix form with the pooled data. Although the point estimate is identical to the case with independent data points, we must adjust for the standard error according to Section 25.3.2. From

$D i (β) = (x i1 , . . . , x ini ) = X t i , we can verify that ĉ ov( β) = n i=1 X t i X i -1 n i=1 X t i εi εt i X i n i=1 X t i X i -1$, where εi = Y i -X i β = (ε i1 , . . . , εini ) t is the residual vector of unit i. This is called the (Liang-Zeger) cluster-robust covariance matrix in econometrics. The square roots of the diagonal terms are called the cluster-robust standard errors. The cluster-robust covariance matrix is often much larger than the (Eicker-Huber-White) heteroskedasticity-robust covariance matrix assuming independence of observations (i, t):

$ĉ ov ehw ( β) = n i=1 ni t=1 x it x t it -1 n i=1 ni t=1 ε2 it x it x t it n i=1 ni t=1 x it x t it -1$.

Note that

$X t X = n i=1 X t i X i = n i=1 ni t=1$x it x t it , so the bread matrices in ĉ ov( β) and ĉ ov ehw ( β) are identical. The only difference is due to the meat matrices:

$n i=1 X t i εi εt i X i = n i=1 ni t=1 εit x it ni t=1 εit x it t ̸ = n i=1 ni t=1 ε2 it x it x t it in general.$
## Logistic regression

For binary outcomes, we can use the marginal logistic model with an independent working covariance matrix, resulting in the following estimating equation:

$n i=1 ni t=1$x it {y it -π(x it , β)} = 0

where π(x it , β) = e x t it β /(1 + e x t it β ). So the point estimator is the pooled logistic regression using all data points, but we must adjust for the standard error according to Section 25.3.2. From

$D i (β) = (π(x i1 , β){1 -π(x i1 , β)}x i1 , . . . , π(x ini , β){1 -π(x ini , β)}x ini ) = X t i Ṽ (X i , β), with Ṽ (X i , β) = diag{π(x it , β){1 -π(x it , β)}} ni t=1 , we can verify that B = n -1 n i=1 X t i Vi X i , M = n -1 n i=1 X t i εi εt i X i ,$where εi = (ε i1 , . . . , εini ) t with residual εit = y it -e xit β /(1 + e xit β ), and Vi = diag{π(x it , β){1 -π(x it , β)}} ni t=1 . So the cluster-robust covariance estimator for logistic regression is

$ĉ ov( β) = n i=1 X t i Vi X i -1 n i=1 X t i εi εt i X i n i=1 X t i Vi X i -1$.

I leave the cluster-robust covariance estimator for Poisson regression to Problem 25.5.

zLPP + Subsidy + Supply 0 . 7 3 8 9 0 . 0 5 5 7 8 1 3 . 2 4 6 0 . 1 3 6 1 5 . 4 zSupply Only 0 . 3 6 1 4 0 . 0 7 5 1 4 4 . 8 1 0 0 . 2 4 2 6 1 . 4 x 2 . 0 4 8 8 0 . 0 8 2 0 9 2 4 . 9 5 7 0 . 2 1 5 8 9 . 4 > > hygaccess . gee = gee ( y ~z + x , id = vid , + family = binomial ( link = logit ) , + corstr = " exchangeable " , + data = hygaccess ) > summary ( hygaccess . gee )$ coef Estimate Naive S . E . Naive z Robust S . E . Robust z ( Intercept ) -1 . 7 9 7 6 0 . 1 3 2 4 -1 3 . 5 7 5 0 . 1 5 4 1 -1 1 . 6 zLPP Only 0 . 3 0 3 8 0 . 1 7 8 1 1 . 7 0 5 0 . 1 9 4 6 1 . 5 zLPP + Subsidy 0 . 7 2 2 7 0 . 1 3 1 6 5 . 4 9 1 0 . 1 2 7 1 5 . 6 zLPP + Subsidy + Supply 0 . 8 5 4 7 0 . 1 3 2 7 6 . 4 4 1 0 . 1 2 4 7 6 . 8 zSupply Only 0 . 3 2 3 6 0 . 1 9 1 1 1 . 6 9 3 0 . 2 3 9 8 1 . 3 x 1 . 9 4 9 7 0 . 1 1 2 8 1 7 . 2 8 6 0 . 1 9 4 7 1 0 . 0

Covariate adjustment improves efficiency and makes the choice of the working covariance matrix less important.

## Longitudinal data

The regression formula f.reg will remain the same although other parameters may vary.

> library ( " gee " ) > library ( " foreign " ) > gym 1 = read . dta ( " gym _ treatment _ exp _ weekly . dta " ) > f . reg = weekly _ visit ~incentive _ commit + incentive + target + member _ gym _ pre Using all data, we find a significant effect of incentive_commit but an insignificant effect of incentive.

normal . gee = gee ( f . reg , id = id , + family = gaussian , + corstr = " independence " , + data = gym 1 ) > normal . gee = summary ( normal . gee )$ coef > normal . gee Estimate Naive S . E . Naive z Robust S . E . Robust z ( Intercept ) -0 . 6 9 0 0 5 0 . 0 1 1 1 3 6 -6 1 . 9 6 8 0 . 0 8 6 7 2 -7 . 9 5 7 2 incentive _ commit 0 . 1 5 6 6 6 0 . 0 0 8 3 5 8 1 8 . 7 4 5 0 . 0 6 3 7 6 2 . 4 5 6 9 incentive 0 . 0 1 0 2 2 0 . 0 0 8 2 7 5 1 . 2 3 5 0 . 0 5 9 1 0 0 . 1 7 2 9 target 0 . 6 2 6 6 6 0 . 0 0 7 4 6 5 8 3 . 9 4 9 0 . 0 6 7 7 3 9 . 2 5 2 7 member _ gym _ pre 1 . 1 4 9 1 9 0 . 0 0 7 0 7 7 1 6 2 . 3 7 5 0 . 0 6 2 5 2 1 8 . 3 8 0 1

However, this pooled analysis can be misleading because we have seen from the analysis before that the treatments have no effects in the pre-experimental periods and smaller effects in the long term. A pooled analysis can dilute the short-term effects, missing the treatment effect heterogeneity across time. This can be fixed by the following subgroup analysis based on time.

> normal . gee 1 = gee ( f . reg , id = id , + subset = ( incentive _ week < 0 ) , + family = gaussian , + corstr = " independence " , + data = gym 1 ) > normal . gee 1 = summary ( normal . gee 1 )$ coef > normal . gee 1 Estimate Naive S . E . Naive z Robust S . E . Robust z ( Intercept ) -0 . 8 7 9 3 7 4 0 . 0 4 2 3 0 -2 0 . 7 8 6 8 0 . 0 8 7 3 9 -1 0 . 0 6 2 incentive _ commit -0 . 0 0 4 2 4 1 0 . 0 3 1 7 5 -0 . 1 3 3 6 0 . 0 6 2 4 3 -0 . 0 6 7 incentive -0 . 0 7 3 8 8 4 0 . 0 3 1 4 4 -2 . 3 5 0 2 0 . 0 6 2 2 3 -1 . 1 8 7 target 0 . 7 4 2 6 7 5 0 . 0 2 8 3 6 2 6 . 1 8 8 7 0 . 0 6 7 0 1 1 1 . 0 8 3 member _ gym _ pre 1 . 6 0 1 5 6 9 0 . 0 2 6 8 9 5 9 . 5 6 6 4 0 . 0 6 6 0 0 2 4 . 7 6 3 > > > normal . gee 2 = gee ( f . reg , id = id , + subset = ( incentive _ week > 0 & incentive _ week < 1 ) , + family = gaussian , + corstr = " independence " , + data = gym 1 ) > normal . gee 2 = summary ( normal . gee 2 )$ coef > normal . gee 2 Estimate Naive S . E . Naive z Robust S . E . Robust z ( Intercept ) -0 . 7 9 2 5 0 . 0 3 2 7 5 -2 4 . 1 9 4 0 . 0 8 9 8 2 -8 . 8 incentive _ commit 0 . 3 6 6 2 0 . 0 2 4 5 8 1 4 . 8 9 8 0 . 0 6 8 9 5 5 . 3 incentive 0 . 1 7 4 4 0 . 0 2 4 3 4 7 . 1 6 6 0 . 0 6 4 5 7 2 . 7 target 0 . 6 7 3 5 0 . 0 2 1 9 6 3 0 . 6 7 4 0 . 0 7 1 5 9 9 . 4 member _ gym _ pre 1 . 4 1 3 8 0 . 0 2 0 8 2 6 7 . 9 1 4 0 . 0 6 7 2 7 2 1 . 0 > > normal . gee 3 = gee ( f . reg , id = id , + subset = ( incentive _ week >= 1 5 ) , + family = gaussian , + corstr = " independence " , + data = gym 1 ) > normal . gee 3 = summary ( normal . gee 3 )$ coef > normal . gee 3 Estimate Naive S . E . Naive z Robust S . E . Robust z ( Intercept ) -0 . 6 6 1 5 0 0 0 . 0 1 2 2 2 2 -5 4 . 1 3 0 . 0 9 0 2 8 -7 . 3 3 incentive _ commit 0 . 1 3 4 7 8 9 0 . 0 0 9 1 7 3 1 4 . 6 9 0 . 0 6 6 7 6 2 . 0 9 incentive -0 . 0 0 9 7 1 6 0 . 0 0 9 0 8 2 -1 . 0 7 0 . 0 6 1 4 2 -0 . 1 2 target 0 . 6 1 1 6 3 5 0 . 0 0 8 1 9 3 7 4 . 6 6 0 . 0 7 0 4 2 8 . 6 0 member _ gym _ pre 1 . 0 7 7 8 7 4 0 . 0 0 7 7 6 8 1 3 8 . 7 7 0 . 0 6 4 9 4 1 6 . 5 7

Changing the family parameter to poisson(link = log), we can fit a marginal log-linear model with independent Poisson covariance. Figure [25](#fig_73).1 shows the point estimates and confidence intervals based on the regressions above. The confidence intervals based on the cluster-robust standard errors are much wider than those based on the EHW standard errors. Without dealing with clustering, the confidence intervals are too narrow and give wrong inference.

## Critiques on the key assumptions

Consider the simple case with n i = 2 for all i below. [25.6.1 Assumption (25.4)](#) Assumption [(25.4)](#)

$requires E(y it | X i ) = E(y it | x it ),$which holds automatically if x it = x i is time-invariant. With time-varying covariates, it effectively rules out the dynamics between x and y. Assumption [(25.4](#)) holds in the following data-generating process:

$x i1 / /$x i2 y i1 y i2 q q q q q q q q q q q q q q q q pooled before short long  

## Normal

$x i1 / / " " x i2 y i1 y i2 or x i1 / / x i2 y i1 < < y i2 or x i1 / / " " x i2 y i1 < < y i2$With more complex data generating processes, Assumption [(25.4)](#) does not hold in general: [Liang and Zeger (1986)](#b165) assumed fixed covariates, ruling out the dynamics of x. [Pepe and Anderson (1994)](#b189) pointed out the importance of Assumption [(25.4)](#) in GEE with random time-varying covariates. [Pepe and Anderson (1994)](#b189) also showed that with an independent working covariance matrix, we can drop Assumption [(25.4)](#) as long as the marginal conditional mean is correctly specified. That is, if

$x i1 / / " " x i2 y i1 < < / / y i2$$E(y it | x it ) = µ(x t it β), then E n i=1 ni t=1 y it -µ(x t it β) σ2 (x it , β) ∂µ(x t it β) ∂β = n i=1 ni t=1 E E{y it -µ(x t it β) | x it } σ2 (x it , β) ∂µ(x t it β) ∂β = 0.$This gives another justification for the use of the independent working covariance matrix even though it can result in efficiency loss when Assumption [(25.4](#)) holds. [25.6.2 Assumption (25.5)](#) Assumption [(25.5](#)) requires a "stable" relationship between x and y across clusters and time points:

E(y it | x it ) = µ(x t it β) where µ and β do not depend on i or t. For clustered data, we can justify this assumption by the exchangeability of the units within clusters. However, it is much harder to interpret or justify it for longitudinal data with complex outcome dynamics.

We consider linear structural equations with a scalar time-invariant covariate. Without direct dependence of y i2 on y i1 , the data generating process

$y i1 = α 1 + βx i + ε i1 , y i2 = α 2 + βx i + ε i2 , corresponding to the graph x i } } ! ! y i1 y i2 has conditional expectations E(y it | x i ) = α t + βx i if E(ε it | x i ) = 0. (25.10)$However, with direct dependence of y i2 on y i1 , the data generating process [25](#formula_748).10) holds. The stability assumption requires

$y i1 = α 1 + βx i + ε i1 , y i2 = α 2 + γy i1 + δx i + ε i2 , corresponding to the graph x i } } ! ! y i1 / / y i2 has conditional expectations E(y i1 | x i ) = α 1 + βx i but E(y i2 | x i ) = α 2 + γ(α 1 + βx i ) + δx i = (α 2 + γα 1 ) + (δ + βγ)x i if ($$α 1 = α 2 + γα 1 , β = βγ + δ,$which are strange restrictions on the model parameters.

With time-varying covariates, this issue becomes even more subtle because Assumption [(25.4](#)) is unlikely to hold in the first place.

## Explanation and prediction

Liang and Zeger (1986)'s marginal model is more useful if the goal is to explain the relationship between x and y, in particular, a component of x it represents the time-invariant treatment and y it represents the time-varying outcomes. If the goal is prediction, then the marginal model can be problematic. For instance, if we observe the covariate value for a 25.4 Cluster-robust standard error in ANOVA This problem extends Problems 5.5, 6.3 and 19.6.

Inherit the setting from Problem 19.6. If the units are clustered by a factor c i ∈ {1, . . . , M } for i = 1, . . . , n, we can obtain the cluster-robust covariances Vlz and V ′ lz from the two WLS fits. Show that Vlz = V ′ lz .

## Cluster-robust standard error for Poisson regression

Similar to Sections 25.4.1 and 25.4.2, derive the cluster-robust covariance matrix for Poisson regression:

$ĉ ov( β) = n i=1 X t i Vi X i -1 n i=1 X t i εi εt i X i n i=1 X t i Vi X i -1$, where εi = Y i -µ(X i , β) and Vi = diag{e x t it β } ni t=1 .

## Data analysis

Re-analyze the data from [Royer et al. (2015)](#b202) using the exchangeable working covariance matrix. Compare the corresponding results with Figure [25](#fig_73).1.

## Part VIII

Beyond Modeling the Conditional Mean

## Quantile Regression

26.1 From the mean to the quantile For a random variable y, we can define its mean as

$E(y) = arg min µ∈R E (y -µ) 2 .$With IID data (y i ) n i=1 , we can compute the sample mean

$ȳ = n -1 n i=1 y i = arg min µ∈R n -1 n i=1 (y i -µ) 2 ,$which satisfies the CLT:

$√ n(ȳ -E(y)) → N(0, var(y))$in distribution if the variance var(y) is finite.  However, the mean can miss important information about y. How about other features of the outcome y? Quantiles can characterize the distribution of y. For a random variable y, we can define its distribution function as F (c) = pr(y ≤ c) and its τ th quantile as

$F -1 (τ ) = inf {q : F (q) ≥ τ } . ρτ(u) u τ = 1 3 ρτ(u) u τ = 1 2 ρτ(u) u τ = 2 3 FIGURE 26.2: Check function$This defines a quantile function F -1 : [0, 1] → R. If the distribution function is strictly monotone, then the quantile function reduces to the inverse of the distribution function, and the τ -th quantile solves τ = pr(y ≤ q) as an equation of q. See Figure [26](#fig_110).1. For simplicity, this chapter focuses on the case with a monotone distribution function. The definition above formulates the mean as the minimizer of an objective function. Similarly, we can define quantiles in an equivalent way below.

Proposition 26.1 With a monotone distribution function and positive density at the τ th quantile, we have

$F -1 (τ ) = arg min q∈R E {ρ τ (y -q)} ,$where

$ρ τ (u) = u {τ -1(u < 0)} = uτ, if u ≥ 0, -u(1 -τ ), if u < 0,$is the check function (the name comes from its shape; see Figure [26.](#fig_110)2). In particular, the median of y is median(y) = F -1 (0.5) = arg min q∈R E {|y -q|} .

Proof of Proposition 26.1: To simplify the proof, we further assume that y has density function f (•). We will use Leibniz's integral rule:

$d dx b(x) a(x) f (x, t)dt = f (x, b(x))b ′ (x) -f (x, a(x))a ′ (x) + b(x) a(x)$∂f (x, t) ∂x dt.

We can write

$E {ρ τ (y -q)} = q -∞ (τ -1)(c -q)f (c)dc + ∞ q τ (c -q)f (c)dc.$To minimize it over q, we can solve the first-order condition

$∂E {ρ τ (y -q)} ∂q = (1 -τ ) q -∞ f (c)dc -τ ∞ q f (c)dc = 0.$So (1 -τ )pr(y ≤ q) -τ {1 -pr(y ≤ q)} = 0 which implies that τ = pr(y ≤ q), so the τ th quantile satisfies the first-order condition. The second-order condition ensures it is the minimizer:

$∂ 2 E {ρ τ (y -q)} ∂q 2 q=F -1 (τ )$= f F -1 (τ ) > 0 by Leibniz's integral rule again. □ The empirical distribution function is F (c) = n -1 n i=1 1(y i ≤ c), which is a step function, increasing but not strictly monotone. With Proposition 26.1, we can easily define the sample quantile as

$F -1 (τ ) = arg min q∈R n -1 n i=1 ρ τ (y i -q),$which may not be unique even though the population quantile is. We can view F -1 (τ ) as a set containing all minimizers, and with large samples the values in the set do not differ much. Similar to the sample mean, the sample quantile also satisfies a CLT.

Theorem 26.1 Assume (y i ) n i=1 iid ∼ y with distribution function F (•) that is strictly increasing and density function f (•) that is positive at the τ th quantile. The sample quantile is consistent for the true quantile and is asymptotically Normal:

$√ n F -1 (τ ) -F -1 (τ ) → N 0, τ (1 -τ ) [f {F -1 (τ )}] 2 in distribution.$In particular, the sample median satisfies

$√ n F -1 (0.5) -median(y) → N 0, 1 4 [f {median(y)}] 2 in distribution.$Proof of Theorem 26.1: Based on the first order condition in Proposition 26.1, the population quantile solves E{m τ (y -q)} = 0, and the sample quantile solves

$n -1 n i=1 m τ (y i -q) = 0,$where the check function has a partial derivative with respect to u except for the point 0:

$m τ (u) = (τ -1)1(u < 0) + τ 1(u > 0) = τ -1(u < 0).$By Theorem D.1, we only need to find the bread and meat matrices, which are scalars now:

$B = ∂E{m τ (y -q)} ∂q q=F -1 (τ ) = ∂E{τ -1(y ≤ q)} ∂q q=F -1 (τ ) = - ∂F (q) ∂q q=F -1 (τ ) = -f {F -1 (τ )},$and

$M = E {m τ (y -q)} 2 q=F -1 (τ ) = E {τ -1(y ≤ q)} 2 q=F -1 (τ ) = E τ 2 + 1(y ≤ q) -2 • 1(y ≤ q)τ q=F -1 (τ ) = τ 2 + τ -2τ 2 = τ (1 -τ ).$Therefore, √ n{ F -1 (τ ) -F -1 (τ )} converges to Normal with mean zero and variance

$M/B 2 = τ (1 -τ )/[f {F -1 (τ )}] 2 .$□ To conduct statistical inference for the quantile F -1 (τ ), we need to estimate the density of y at the τ th quantile to obtain the estimated standard error of F -1 (τ ). Alternatively, we can use the bootstrap to obtain the estimated standard error. We will discuss the inference of quantiles in R in Section 26.4.

## From the conditional mean to conditional quantile

With an explanatory variable x for outcome y, we can define the conditional mean as

$E(y | x) = arg min m(•) E {y -m(x)} 2 .$We can use a linear function x t β to approximate the conditional mean with the population OLS coefficient

$β = arg min b∈R p E (y -x t b) 2 = {E(xx t )} -1 E(xy),$and the sample OLS coefficient

$β = n -1 n i=1 x i x t i -1 n -1 n i=1$x i y i .

We have discussed the statistical properties of β in previous chapters. Motivated by Proposition 26.1, we can define the conditional quantile function as

$F -1 (τ | x) = arg min q(•) E [ρ τ {y -q(x)}] .$We can use a linear function x t β(τ ) to approximate the conditional quantile function with

$β(τ ) = arg min b∈R p E {ρ τ (y -x t b)}$called the τ th population regression quantile, and

$β(τ ) = arg min b∈R p n -1 n i=1 ρ τ (y i -x t i b) (26.1)$called the τ th sample regression quantile. As a special case, when τ = 0.5, we have the regression median:

$β(0.5) = arg min b∈R p n -1 n i=1 |y i -x t i b|,$which is also called the least absolute deviations (LAD). [Koenker and Bassett Jr (1978)](#b157) started the literature under a correctly specified conditional quantile model:

$F -1 (τ | x) = x t β(τ ).$The interpretation of the j-th coordinate of the coefficient, β j (τ ), is the partial influence of x ij on the τ th conditional quantile of y i given x i . [Angrist et al. (2006)](#b49) discussed quantile regression under misspecification, viewing it as the best linear approximation to the true conditional quantile function. This chapter will focus on the statistical properties of the sample regression quantiles following [Angrist et al. (2006)](#b49)'s discussion of statistical inference allowing for the misspecification of the quantile regression model. Before that, we first comment on the population regression quantiles based on some generative models. Below assume that the v i 's are IID independent of the covariates x i 's, with mean zero and distribution g(c) = pr(v i ≤ c).

Example 26.1 Under the linear model y i = x t i β + σv i , we can verify that

$E(y i | x i ) = x t i β and F -1 (τ | x i ) = x t i β + σg -1 (τ ).$Therefore, with the first regressor being 1, we have β 1 (τ ) = β 1 + σg -1 (τ ), β j (τ ) = β j , (j = 2, . . . , p).

In this case, both the true conditional mean and quantile functions are linear, and the population regression quantiles are constant across τ except for the intercept.

Example 26.2 Under a heteroskedastic linear model y i = x t i β + (x t i γ)v i with x t i γ > 0 for all x i 's, we can verify that

$E(y i | x i ) = x t i β and F -1 (τ | x i ) = x t i β + x t i γg -1 (τ ). Therefore, β(τ ) = β + γg -1 (τ ).$In this case, both the true conditional quantile functions are linear, and all coordinates of the population regression quantiles vary with τ .

Example 26.3 Under the transformed linear model log y i = x t i β + σv i , we can verify that

$E(y i | x i ) = exp(x t i β)M v (σ),$where M v (t) = E(e tv ) is the moment generating function of v, and

$F -1 (τ | x i ) = exp x t i β + σg -1 (τ ) .$In this case, both the true conditional mean and quantile functions are log-linear in covariates.

## Sample regression quantiles

## Computation

The regression quantiles (26.1) do not have explicit formulas in general, and we need to solve the optimization problem numerically. Motivated by the piece-wise linear feature of the check function, we decompose y i -x t i β into the difference between its positive part and negative part:

$y i -x t i β = u i -v i , where u i = max(y i -x t i β, 0), v i = -min(y i -x t i β, 0). So the objective function simplifies to the summation of ρ τ (y i -x t i β) = τ u i + (1 -τ )v i ,$which is simply a linear function of the u i 's and v i 's. Of course, these u i 's and v i 's are not arbitrary because they must satisfy the constraints by the data. Using the notation

$Y =    y 1 . . . y n    , X =    x t 1 . . . x t n    , u =    u 1 . . . u n    , v =    v 1 . . . v n    ,$finding the τ th regression quantile is equivalent to a linear programming problem with linear objective function and linear constraints:

$min b,u,v τ 1 t n u + (1 -τ )1 t n v, s.t. Y = Xb + u -v,$and u i ≥ 0, v i ≥ 0 (i = 1, . . . , n).

The function rq in the R package quantreg computes the regression quantiles with various choices of methods.

## Asymptotic inference

Similar to the sample quantiles, the regression quantiles are also consistent for the population regression quantiles and asymptotically Normal. So we can conduct asymptotic inference based on the results in the following theorem [(Angrist et al., 2006)](#b49).

Theorem 26.2 Assume (y i , x i ) n i=1 iid ∼ (y, x). Under some regularity conditions, we have

$√ n β(τ ) -β(τ ) → N(0, B -1 M B -1 )$in distribution, where

$B = E f y|x {x t β(τ )} xx t , M = E {τ -1 (y -x t β(τ ) ≤ 0)} 2 xx t ,$with f y|x (•) denoting the conditional density of y given x.

Proof of Theorem 26.2: The population regression quantile solves

$E {m τ (y -x t b)x} = 0,$and the sample regression quantile solves

$n -1 n i=1 m τ (y i -x t i b)x i = 0.$By Theorem D.1, we only need to calculate the explicit forms of B and M . Let F y|x (•) and f y|x (•) be the conditional distribution and density functions. We have

$E {m τ (y -x t b)x} = E [{τ -1(y -x t b ≤ 0)} x] = E τ -F y|x (x t b) x , so ∂E {m τ (y -x t b)x} ∂b t = -E f y|x (x t b)xx t .$This implies the formula of B. The formula of M follows from

$M = E m 2 τ (y -x t β(τ ))xx t = E {τ -1(y -x t β(τ ) ≤ 0)} 2 xx t .$□ Based on Theorem 26.2, we can estimate the asymptotic covariance matrix of β(τ ) by n

$-1 B-1 M B-1 , where M = n -1 n i=1 τ -1 y i -x t i β(τ ) ≤ 0 2 x i x t i and B = (2nh) -1 n i=1 1 |y i -x t i β(τ )| ≤ h x i x t i$for a carefully chosen h. Powell (1991)'s theory suggests to use h satisfying condition h = O(n -1/3 ), but the theory is not so helpful since it only suggests the order of h. The quantreg package in R chooses a specific h that satisfies this condition. In finite samples, the bootstrap often gives a better estimation of the asymptotic covariance matrix.

## Numerical examples

## Sample quantiles

We can use the quantile function to obtain the sample quantiles. However, it does not report standard errors. Instead, we can use the rq function to compute sample quantiles by regressing the outcome on constant 1. These two functions may return different sample quantiles when they are not unique. The difference is often small with large sample sizes. I use the following simulation to compare various methods for standard error estimation. The first data-generating process has a standard Normal outcome.

Exponential(1) Normal(0,1) 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0.05 0.10 0.15 0.20 quantiles standard errors method boot iid ker true FIGURE 26.3: Standard errors for sample quantiles library ( quantreg ) mc = 2 0 0 0 n = 2 0 0 taus = ( 1 : 9 )/ 1 0 get . se = function ( x ){ x $ coef [ 1 ,2 ]} q . normal = replicate ( mc ,{ y = rnorm ( n ) qy = rq ( y ~1 , tau = taus ) se . iid = summary ( qy , se = " iid " ) se . ker = summary ( qy , se = " ker " ) se . boot = summary ( qy , se = " boot " ) qy = qy $ coef se . iid = sapply ( se . iid , get . se ) se . ker = sapply ( se . ker , get . se ) se . boot = sapply ( se . boot , get . se ) c ( qy , se . iid , se . ker , se . boot ) })

In the above, se = "iid", se = "ker", and se = "boot" correspond to the standard errors by [Koenker and Bassett Jr (1978)](#b157), [Powell (1991)](#b191), and the bootstrap. I also run the same simulation but replace the Normal outcome with Exponential: y = rexp(n). Figure [26](#fig_110).3 compares the estimated standard errors with the true asymptotic standard error in Theorem 26.1. Bootstrap works the best, and the one involving kernel estimation of the density seems biased.

## OLS versus LAD

I will use simulation to compare OLS and LAD. In rq, the default value is tau=0.5, fitting the LAD. The first data-generating process is a Normal linear model:

$x = rnorm ( n ) simu . normal = replicate ( mc , { y = 1 + x + rnorm ( n ) c ( lm ( y ~x )$ coef [ 2 ] , rq ( y ~x )$ coef [ 2 ]) })$The second data generating process replaces the error term to a Laplace distribution[foot_24](#foot_24) : simu . laplace = replicate ( mc , { y = 1 + x + rexp ( n ) -rexp ( n ) c ( lm ( y ~x )$ coef [[ 2 ]](#) , rq ( y ~x )$ coef [[ 2 ]](#)) })

OLS is the MLE under a Normal linear model, and LAD is the MLE under a linear model with independent Laplace errors.

The third data-generating process replaces the error term with standard Exponential:

simu . exp = replicate ( mc , { y = 1 + x + rexp ( n ) c ( lm ( y ~x )$ coef [[ 2 ]](#) , rq ( y ~x )$ coef [[ 2 ]](#)) })

The fourth data generating process has y i = 1 + e i x i with e i IID Exponential, so

$E(y i | x i ) = 1 + x i , var(y i | x i ) = x 2 i ,$which is a heteroskedastic linear model, and

$F -1 (0.5 | x i ) = 1 + median(e i )x i = 1 + (log 2)x i ,$which is a linear quantile model. The coefficients are different in the conditional mean and quantile functions.

x = abs ( x ) simu . x = replicate ( mc , { y = 1 + rexp ( n )* x c ( lm ( y ~x )$ coef [[ 2 ]](#) , rq ( y ~x )$ coef [[ 2 ]](#)) })

Figure [26](#fig_110).4 compares OLS and LAD under the above four data-generating processes. With Normal errors, OLS is more efficient; with Laplace errors, LAD is more efficient. This confirms the theory of MLE. With Exponential errors, LAD is also more efficient than OLS. Under the fourth data-generating process, LAD is more efficient than OLS. In general, however, OLS and LAD target the conditional mean and conditional median, respectively. Since the parameters differ in general, the comparison of the standard errors is not very meaningful. Both OLS and LAD give useful information about the data.

## Application

## Parents' and children's heights

I revisit Galton's data introduced in Chapter 2. The following code gives the coefficients for quantiles 0.1 to 0.9. 26.5.2 U.S. wage structure [Angrist et al. (2006)](#b49) used quantile regression to study the U.S. wage structure. They used census data in [1980, 1990, and 2000](#) to fit quantile regressions on log weekly wage on education and other variables. The following code gives the coefficients for quantile regressions with τ equaling 0.1 to 0.9. I repeated the regressions with data from three years. Due to the large sample size, I use the m-of-n bootstrap with m = 500. > census 9 0 = read . dta ( " census 9 0 . dta " ) > census 0 0 = read . dta ( " census 0 0 . dta " ) > f . reg = logwk ~educ + exper + exper 2 + black > > m . boot = 5 0 0 > rq 8 0 = rq ( f . reg , data = census 8 0 , tau = taus ) > rqlist 8 0 = summary ( rq 8 0 , se = " boot " , + bsmethod = " xy " , mofn = m . boot ) > rq 9 0 = rq ( f . reg , data = census 9 0 , tau = taus ) > rqlist 9 0 = summary ( rq 9 0 , se = " boot " , + bsmethod = " xy " , mofn = m . boot ) > rq 0 0 = rq ( f . reg , data = census 0 0 , tau = taus ) > rqlist 0 0 = summary ( rq 0 0 , se = " boot " , + bsmethod = " xy " , mofn = m . boot )

Figure [26](#fig_110).6 shows the coefficient of educ across years and across quantiles. In 1980, the coefficients are nearly constant across quantiles, showing no evidence of heterogeneity in the return of education. Compared with 1980, the return of education in 1990 increases across all quantiles, but it increases more at the upper quantiles. Compared with 1990, the return of education in 2000 decreases at the lower quantiles and increases at the upper quantiles, showing more dramatic heterogeneity across quantiles.

The original data used by [Angrist et al. (2006)](#b49) contain weights due to sampling. Ideally, we should use the weights in the quantile regression. Like lm, the rq function also allows for specifying weights.

The R code in this section is in code24.5.R.

0.08 0.10 0.12 0.14 0.16 0.25 0.50 0.75 τ coefficient of educ year 00 90 80 quantile regressions FIGURE 26.6: Angrist et al. (2006)'s data 26.6 Extensions With clustered data, we must use the cluster-robust standard error which can be approximated by the clustered bootstrap with the rq function. I use Hagemann (2017)'s example below where the students are clustered in classrooms. See code24.6.R. > star = read . csv ( " star . csv " ) > star . rq = rq ( pscore ~small + regaide + black + + girl + poor + tblack + texp + + tmasters + factor ( fe ) , + data = star ) > res = summary ( star . rq , se = " boot " )$ coef [ 2 : 9 , ] > res . clus = summary ( star . rq , se = " boot " , + cluster = star $ classid )$ coef [ 2 : 9 , ] > round ( res , 3 ) Value Std . Error t value Pr ( >| t |) small

6 . 5 0 0 1 . 1 2 2 5 . 7 9 5 0 . 0 0 0 regaide 0 . 2 9 4 1 . 0 7 1 0 . 2 7 4 0 . 7 8 4 black -1 0 . 3 3 4 1 . 6 5 7 -6 . 2 3 7 0 . 0 0 0 girl 5 . 0 7 3 0 . 8 7 8 5 . 7 7 7 0 . 0 0 0 poor -1 4 . 3 4 4 1 . 0 2 4 -1 4 . 0 1 1 0 . 0 0 0 tblack -0 . 1 9 7 1 . 7 5 1 -0 . 1 1 3 0 . 9 1 0 texp 0 . 4 1 3 0 . 0 9 8 4 . 2 3 1 0 . 0 0 0 tmasters -0 . 5 3 0 1 . 0 6 8 -0 . 4 9 7 0 . 6 1 9 > round ( res . clus , 3 ) Value Std . Error t value Pr ( >| t |) small 6 . 5 0 0 1 . 6 6 2 3 . 9 1 2 0 . 0 0 0 regaide 0 . 2 9 4 1 . 6 2 7 0 . 1 8 1 0 . 8 5 7 black -1 0 . 3 3 4 1 . 8 4 9 -5 . 5 8 8 0 . 0 0 0 girl 5 . 0 7 3 0 . 8 1 9 6 . 1 9 5 0 . 0 0 0 poor -1 4 . 3 4 4 1 . 1 5 2 -1 2 . 4 5 5 0 . 0 0 0 tblack -0 . 1 9 7 3 . 1 1 3 -0 . 0 6 3 0 . 9 4 9 texp 0 . 4 1 3 0 . 1 6 8 2 . 4 6 5 0 . 0 1 4 tmasters -0 . 5 3 0 1 . 4 4 4 -0 . 3 6 7 0 . 7 1 3

With high dimensional covariates, we can use regularized quantile regression. For instance, the rq function can implement the lasso version with method = "lasso" and a prespecified lambda. It does not implement the ridge version.

## Homework problems

26.1 Quantile regression with a binary regressor For i = 1, . . . , n, the first 1/3 observations have x i = 1 and the last 2/3 observations have x i = 0; y i | x i = 1 follows an Exponential(1), and y i | x i = 0 follows an Exponential [(2)](#). Find

$(α, β) = arg min (a,b) n i=1 ρ 1/2 (y i -a -bx i ).$and the joint asymptotic distribution.

## Conditional quantile function in bivariate Normal

Show that if (x, y) follows a bivariate Normal, the conditional quantile function of y given x is linear in x with the same slope across all τ .

## Quantile range and variance

A symmetric random variable y satisfies y ∼ -y. Define the 1 -α quantile range of a symmetric random variable y as the interval of its α/2 and 1 -α/2 quantiles. Given two symmetric random variables y 1 and y 2 , show that if the 1 -α quantile range of y 1 is wider than that of y 2 for all α, then var(y 1 ) ≥ var(y 2 ). Does the converse of the statement hold? If so, give a proof; if not, give a counterexample.

## Interquartile range and estimation

The interquartile range of a random variable y equals the difference between its 75% and 25% quantiles. Based on IID data (y i ) n i=1 , write a function to estimate the interquartile range and the corresponding standard error using the bootstrap. Use simulated data to evaluate the finite sample properties of the point estimate (e.g., bias and variance) and the 95% confidence interval (e.g. coverage rate and length).

Find the asymptotic distribution of the estimator for the interquartile range.

## Joint asymptotic distribution of the sample median and the mean

Assume that y 1 , . . . , y n ∼ y are IID. Find the joint asymptotic distribution of the sample mean ȳ and median m. Hint: The mean µ and median m satisfy the estimating equation with w(y, µ, m) = y -µ 0.5 -1(y -m ≤ 0) .

## Weighted quantile regression and application

Many real data contain weights due to sampling. For example, in [Angrist et al. (2006)](#b49)'s data, perwt is the sampling weight. Define the weighted quantile regression problem theoretically and re-analyze [Angrist et al. (2006)](#b49)'s data with weights. Note that similar to lm and glm, the quantile regression function rq also has a parameter weights.

Modeling Time-to-Event Outcomes

## Examples

Time-to-event data are common in biomedical and social sciences. Statistical analysis of time-to-event data is called survival analysis in biostatistics and duration analysis in econometrics. The former name comes from biomedical applications where the outcome denotes the survival time or the time to the recurrence of the disease of interest. The latter name comes from the economic applications where the outcome denotes the weeks unemployed or days until the next arrest after being released from incarceration. See [Kalbfleisch and Prentice (2011)](#b152) for biomedical applications and [Heckman and Singer (1984)](#b135) for economic applications. [Freedman (2008)](#b110) gave a concise and critical introduction to survival analysis.

## Survival analysis

The Combined Pharmacotherapies and Behavioral Interventions study evaluated the efficacy of medication, behavioral therapy, and their combination for the treatment of alcohol dependence [(Anton et al., 2006)](#b52). Between January 2001 and January 2004, n = 1224 recently alcohol-abstinent volunteers were randomized to receive medical management with 16 weeks of naltrexone (100mg daily) or placebo, with or without a combined behavioral intervention. It was a 2 × 2 factorial experiment. The outcome of interest is the time to the first day of heavy drinking and other endpoints. I adopt the data from [Lin et al. (2016)](#b166).

> COMBINE = read . table ( " combine _ data . txt " , header = TRUE )[ , -1 ] > head ( COMBINE ) AGE GENDER T 0 _ PDA NALTREXONE THERAPY site relapse futime 1 3 1 male 3 . 3 3 3 3 3 3 1 0 site _ 0 0 1 1 2 2 4 1 female 1 6 . 6 6 6 6 6 7 1 1 site _ 0 1 8 3 4 4 male 7 3 . 3 3 3 3 3 3 0 1 site _ 0 1 2 0 4 6 5 male 1 0 . 0 0 0 0 0 0 1 0 site _ 0 0 1 1 2 5 3 9 male 0 . 0 0 0 0 0 0 0 1 site _ 0 1 4 6 5 6 male 1 3 . 3 3 3 3 3 3 0 0 site _ 0 1 1 NALTREXONE and THERAPY are two treatment indicators. futime is the follow-up time, which is censored if relapse equals 0. For those censored observations, futime equals 112, so it is administrative censoring. Figure [27](#fig_110).1 shows the histograms of futime in four treatment groups. A large number of patients have censored outcomes. Other variables are covariates.

## Duration analysis

Carpenter (2002) asked the question: Why does the U.S. Food and Drug Administration approve some drugs more quickly than others? With data about 450 drugs reviewed from 1977 to 2000, he studied the dependence of review times on various covariates, including political influence, wealth of the richest organization representing the disease, media cover- > fda <-read . dta ( " fda . dta " ) > names ( fda )

[ 1 ] " acttime " " censor " " hcomm " " hfloor " " scomm " [ 6 ] " sfloor " " prespart " " demhsmaj " " demsnmaj " " orderent " [ 1 1 ] " stafcder " " prevgenx " " lethal " " deathrt 1 " " hosp 0 1 " [ 1 6 ] " hospdisc " " hhosleng " " acutediz " " orphdum " " mandiz 0 1 " [ 2 1 ] " femdiz 0 1 " " peddiz 0 1 " " natreg " " natregsq " " wpnoavg 3 " [ 2 6 ] " vandavg 3 " " condavg 3 " " _ st " " _ d " " _ t " [ 3 1 ] " _ t 0 " " caseid "

An obvious feature of time-to-event data is that the outcome is non-negative. This can be easily dealt with by the log transformation. However, the outcomes may be censored, resulting in inadequate tail information. With right censoring, modeling the mean involves extrapolation in the right tail.

## Time-to-event data

Let T ≥ 0 denote the outcome of interest. We can characterize a non-negative continuous T using its density f (t), distribution function F (t), survival function S(t) = 1 -F (t) = pr(T > t), and hazard function

$λ(t) = lim ∆t↓0 pr(t ≤ T < t + ∆t | T ≥ t)/∆t.$Within a small time interval [t, t + ∆t], we have approximation pr(t ≤ T < t + ∆t | T ≥ t) ∼ = λ(t)∆t, so the hazard function denotes the death rate within a small interval conditioning on surviving up to time t. Both the survival and hazard functions are commonly used to describe a positive random variable. First, the survival function has a simple relationship with the expectation.

Proposition 27.1 For a non-negative random variable T ,

$E(T ) = ∞ 0 S(t)dt.$Proposition 27.1 holds for both continuous and discrete non-negative random variables. It states that the expectation of a nonnegative random variable equals the area under the survival function. It does not require the existence of the density function of T . Proof of Proposition 27.1: Fubini's theorem allows us to swap the expectation and integral below:

$E(T ) = E T 0 dt = E ∞ 0 1(T > t)dt = ∞ 0 E{1(T > t)}dt = ∞ 0 pr(T > t)dt = ∞ 0 S(t)dt.$□ Second, the survival and hazard functions can determine each other in the following way.

Proposition 27.2 For a non-negative continuous random variable T ,

$λ(t) = f (t) S(t) = - d dt log S(t), S(t) = exp - t 0 λ(s)ds .$Proof of Proposition 27.2: By definition,

$λ(t) = lim ∆t↓0 pr(t ≤ T < t + ∆t) ∆t 1 pr(T ≥ t) = lim ∆t↓0 F (t + ∆t) -F (t) ∆t 1 S(t) = f (t) S($
## t) .

We can further write the above equation as Example 27.2 (Gamma) The Gamma(α, β) random variable T has density f (t) = β α t α-1 e -βt /Γ(α). When α = 1, it reduces to Exponential(β) with a constant hazard function. In general, the survival function and hazard function do not have simple forms, but we can use dgamma and pgamma to compute them numerically. The left panel of Figure [27](#fig_110).2 plots the hazard functions of Gamma(α, β). When α < 1, the hazard function is decreasing; when α > 1, the hazard function is increasing. q q q q q q FIGURE 27.3: Discrete survival function with masses (0.1, 0.05, 0.15, 0.2, 0.3, 0.2) at [(1,](#)[2,](#)[3,](#)[4,](#)[5,](#)[6)](#) where Z ∼ Exponential(1). We can verify that T has the density function

$λ(t) = f (t) S(t) = - dS(t)/dt S(t) = - d dt log S(t),$$f (t) = a b t b a-1 exp - t b a , survivalfunction$$S(t) = exp - t b a ,$and hazard function

$λ(t) = a b t b a-1$.

So when a = 1, Weibull reduces to Exponential with constant hazard function. When a > 1, the hazard function increases; when a < 1, the hazard function decreases.

We can characterize a positive discrete random variable T ∈ {t 1 , t 2 , . . .} by its probability mass function f

$(t k ) = pr(T = t k ), distribution function F (t) = k:t k ≤t f (t k ), survival function S(t) = k:t k >t f (t k ),$
## and discrete hazard function

$λ k = pr(T = t k | T ≥ t k ) = f (t k ) S(t k -) ,$where S(t k -) denotes the left limit of the function S(t) at t k . Figure [27](#fig_110).3 shows an example of a survival function for a discrete random variable, which shows that S(t) is a step function and right-continuous with left limits. The discrete hazard and survival functions have the following connection which will be useful for the next section. (1 -λ k ).

Note that S(t) is a step function decreasing at each t k because λ k is probability and thus bounded between zero and one. Proof of Proposition 27.3: By definition,

$1 -λ k = 1 -pr(T = t k | T ≥ t k ) = pr(T > t k | T ≥ t k )$is the probability of surviving longer than t k conditional on surviving at least as long as t k . We can verify Proposition 27.3 within each interval of the t k 's. For example, if t < t 1 , then

$S(t) = pr(T > t) = 1; if t 1 ≤ t < t 2 , then S(t) = pr(T > t 1 ) = pr(T > t 1 , T ≥ t 1 ) = pr(T > t 1 | T ≥ t 1 )pr(T ≥ t 1 ) = 1 -λ 1 ; if t 2 ≤ t < t 3 , then S(t) = pr(T > t 2 ) = pr(T > t 2 , T ≥ t 2 ) = pr(T > t 2 | T ≥ t 2 )pr(T ≥ t 2 ) = (1 -λ 2 )(1 -λ 1 ).$We can also verify other values of S(t) by induction. □

## Kaplan-Meier survival curve

Without censoring, estimating the CDF or the survival function is rather straightforward. With IID data (T 1 , . . . , T n ), we can estimate the CDF by F (t) = n -1 n i=1 1(T i ≤ t) and the survival function by Ŝ(t) = 1 -F (t).

Figure [27](#fig_110).4 shows the common data structure with censoring in survival analysis:

(S1) t 1 , . . . , t K are the death times, and d 1 , . . . , d K are the corresponding number of deaths;

(S2) r 1 , . . . , r K are the number of patients at risk, that is, r 1 patients are not dead or censored right before time t 1 , and so on;

(S3) c 1 , . . . , c K are the number of censored patients within interval [t 1 , t 2 ), . . . , [d K , ∞). [Kaplan and Meier (1958)](#b153) proposed the following simple estimator for the survival function.

Definition 27.1 (Kaplan-Meier curve) First estimate the discrete hazard function at the failure times {t 1 , . . . , t K } as λk = d k /r k (k = 1, . . . , K) and then estimate the survival function as

$Ŝ(t) = k:t k ≤t (1 -λk ).$The Ŝ(t) in Definition 27.1 is also called the product-limit estimator of the survival function due to its mathematical form.

At each failure time t k , we view d k as the result of r k Bernoulli trials with probability λ k . So λk = d k /r k has variance λ k (1 -λ k )/r k which can be estimated by v ar( λk ) = λk (1 -λk )/r k .

We can estimate the variance of the survival function using the delta method. We can approximate the variance of log Ŝ

$(t) = k:t k ≤t log(1 -λk ) ∼ = k:t k ≤t log(1 -λ k ) - k:t k ≤t (1 -λ k ) -1 ( λk -λ k ) by v ar log Ŝ(t) = k:t k ≤t (1 -λ k ) -2 v ar( λk ) = k:t k ≤t (1 -λk ) -2 λk (1 -λk )/r k = k:t k ≤t d k r k (r k -d k ) ,$which is called Greenwood's formula [(Greenwood, 1926)](#b130). A hidden assumption above is the independence of the λk 's. This assumption cannot be justified due to the dependence of the events. However, a deeper theory of counting processes shows that Greenwood's formula is valid even without the independence [(Fleming and Harrington, 2011)](#b104).

Based on Greenwood's formula, we can construct a confidence interval for log S(t):

$log Ŝ(t) ± z α v ar log Ŝ(t) ,$which implies a confidence interval for S(t). However, this interval can be outside of range [0, 1] because the log transformation log S(t) is in the range of (-∞, 0) but the Normal approximation is in the range (-∞, ∞). A better transformation is log-log:

$v(t) = log {-log S(t)} , v(t) = log -log Ŝ(t) .$Using Taylor expansion, we can approximate the variance of

$v(t) ∼ = log {-log S(t)} - 1 log S(t) log Ŝ(t) -log S(t) by v ar log Ŝ(t) log Ŝ(t) 2 .$Based on this formula and Greenwood's formula above, we can construct a confidence interval for v(t):

log -log Ŝ(t) ± z α v ar log Ŝ(t) / log Ŝ(t), which implies another confidence interval for S(t). In the R package survival, the function survfit can fit the Kaplan-Meier curve, where the specifications conf.type = "log" and conf.type = "log-log" return confidence intervals based on the log and log-log transformations, respectively. Figure [27](#fig_110).5 plots four curves based on the combination of NALTREXONE and THERAPY using the data of [Lin et al. (2016)](#b166). I do not show the confidence intervals due to the large overlap. The above discussion on the Kaplan-Meier curve is rather heuristic. More fundamentally, what is the underlying censoring mechanism that ensures the possibility that the distribution of the survival time can be recovered by the observed data? It turns out that we have implicitly assumed that the survival time and the censoring time are independent. Homework problem 27.1 gives a theoretical statement.

## Cox model for time-to-event outcome

Another important problem is to model the dependence of the survival time T on covariates x. The major challenge is that the survival time is often censored. Let C i be the censoring time of unit i, and we can only observe the minimum value of the survival time and the censoring time. So the observed data are (x i , y i , δ i ) n i=1 , where

$y i = min(T i , C i ), δ i = 1(T i ≤ C i )$are the event time and the censoring indicator, respectively. A key assumption is that the censoring mechanism is noninformative:

$Assumption 27.1 (noninformative censoring) T i C i | x i .$We can start with parametric models.

Example 27.5 Assume T

$i | x i ∼ Log-Normal(x t i β, σ 2 ). Equivalently, log T i = x t i β + ε i ,$where the ε i 's are IID N(0, σ 2 ) independent of the x i 's. This is a Normal linear model on log T i .

Example 27.6 Assume that T

$i | x i ∼ Weibull(a, b = e x t i β ).$Based on the definition of the Weibull distribution in Example 27.1, we have

$log T i = x t i β + ε i$where the ε i 's are IID a -1 log Exponential(1), independent of the x i 's.

The R package survival contains the function survreg to fit parametric survival models including the choices of dist = "lognormal", dist = "weibull", etc. However, these parametric models are not commonly used in practice. The parametric forms can be too strong, and due to right censoring, the inference can be driven by extrapolation to the right tail. [Cox (1972)](#b82) proposed to model the conditional hazard function

## Cox model and its interpretation

$λ(t | x) = lim ∆t↓0 pr(t ≤ T < t + ∆t | T ≥ t, x)/∆t = f (t | x) S(t | x) .$His celebrated proportional hazards model has the following form.

Assumption 27.2 (Cox proportional hazards model) Assume the conditional hazard ratio function has the form

$λ(t | x) = λ 0 (t) exp(x t β),(27.2)$where β is an unknown parameter and λ 0 (•) is an unknown function.

Exponential(1) Gamma(2,2) lnorm(0,1) 0 2 4 6 0 2 4 6 0 2 4 6 0.00 0.25 0.50 0.75 1.00 t survival functions power 0 0.5 2 Under proportional hazards assumption FIGURE 27.6: Proportional hazards assumption with different baseline survival functions, where the power equals γ = exp(β).

where the product is over K time points with failures, x k is the covariate value of the failure at time t k , and R(t k ) contains the indices of the units at risk at time t k , i.e., the units not censored or failed right before the time t k . [Freedman (2008)](#b110) gives a heuristic explanation of the partial likelihood based on the following results which extends Proposition B.7 on the Exponential distribution.

Theorem 27.1 If T 1 , . . . , T n are independent with hazard function λ i (t) (i = 1, . . . , n), then their minimum value T = min 1≤i≤n T i has hazard function

$n i=1 λ i (t). Moreover, if λ i (t) = c i λ(t), then pr(T i = T ) = c i n i ′ =1 c i ′ . Proof of Theorem 27.1: The survival function of T is pr(T > t) = pr(T 1 > t, . . . , T n > t) = n i=1 S i (t),$so Proposition 27.2 implies that its hazard function is

$- d dt log pr(T > t) = n i=1 - d dt log S i (t) = n i=1 λ i (t).$So the first conclusion follows.

As a byproduct of the above proof, the density of T is

$n i=1 λ i (t) n i=1 S i (t) based in Proposition 27.2. It must have integral one; with λ i (t) = c i λ(t), this implies n i=1 c i ∞ 0 λ(t) n i=1 S i (t)dt = 1. (27.3) Therefore, we have pr(T i = T ) = pr{T i ≤ T i ′ for all i ′ ̸ = i} = ∞ 0 i ′ ̸ =i S i ′ (t)f i (t)dt = ∞ 0 n i ′ =1 S i ′ (t)λ i (t)dt = c i ∞ 0 λ(t) n i ′ =1 S i ′ (t)dt = c i n i=1 c i ′ ,$where the last equality holds due to (27.3). □ Theorem 27.1 explains each of the K components in the partial likelihood. At time t k , the units in R(t k ) are all at risk, and unit k fails, assuming no ties. The probability that unit k has the smallest failure time among units in R

$(t k ) is exp(x t k β) l∈R(t k ) exp(x t l β)$from Theorem 27.1. The product in the partial likelihood is based on the independence of the events at the K failure times, which is more difficult to justify. A rigorous justification relies on the deeper theory of counting processes [(Fleming and Harrington, 2011)](#b104) or semiparametric statistics [(Tsiatis, 2007)](#b226). The log-likelihood function is

$log L(β) = K k=1    x t k β -log l∈R(t k ) exp(x t l β)    ,$and the score function is

$∂ log L(β) ∂β = K k=1 x k - l∈R(t k ) exp(x t l β)x l l∈R(t k ) exp(x t l β) . Define π β (l | R k ) = exp(x t l β)/ l∈R(t k ) exp(x t l β), (l ∈ R(t k ))$which sum to one, so they induce a probability measure leading to expectation

$E β (• | R k ) and covariance cov β (• | R k ).$With this notation, the score function simplifies to

$∂ log L(β) ∂β = K k=1 {x k -E β (x | R k )} , where E β (x | R k ) = l∈R(t k ) π l (β | R k )$x l ; the Hessian matrix simplifies to

$∂ 2 log L(β) ∂β∂β t = - K k=1 cov β (x | R k ) ⪯ 0, where cov β (x | R k ) = l∈R(t k ) exp(x t l β)x l x t l l∈R(t k ) exp(x t l β) -l∈R(t k ) exp(x t l β)x l l∈R(t k ) exp(x t l β)x t l    l∈R(t k ) exp(x t l β)    2 = l∈R(t k ) π β (l | R k )x l x t l - l∈R(t k ) π β (l | R k )x l l∈R(t k ) π β (l | R k )x t l .$The coxph function in the R package survival uses Newton's method to compute the maximizer β of the partial likelihood function, and uses the inverse of the observed Fisher information to approximate its asymptotic variance. [Lin and Wei (1989)](#b167) proposed a sandwich covariance estimator to allow for the misspecification of the Cox model. The coxph function with robust = TRUE reports the corresponding robust standard errors.

## Examples

Using [Lin et al. (2016)](#b166)'s data, we have the following results. -0 . 2 4 9 7 1 9 0 . 7 7 9 0 2 0 0 . 0 9 7 6 9 0 -2 . 5 5 6 0 . 0 1 0 5 8 * THERAPY -0 . 1 6 7 0 5 0 0 . 8 4 6 1 5 8 0 . 0 9 6 1 0 2 -1 . 7 3 8 0 . 0 8 2 1 7 . AGE -0 . 0 1 5 5 4 0 0 . 9 8 4 5 8 0 0 . 0 0 3 5 5 9 -4 . 3 6 6 1 . 2 7e -0 5 *** GENDERmale -0 . 1 4 0 6 2 1 0 . 8 6 8 8 1 8 0 . 0 7 5 3 6 8 -1 . 8 6 6 0 . 0 6 2 0 7 . T 0 _ PDA 0 . 0 0 2 5 5 0 1 . 0 0 2 5 5 3 0 . 0 0 1 3 6 8 1 . 8 6 3 0 . 0 6 2 4 2 . sitesite _ 1 -0 . 0 9 1 8 5 3 0 . 9 1 2 2 3 9 0 . 1 6 7 2 6 1 -0 . 5 4 9 0 . 5 8 2 9 0 sitesite _ 1 0 -0 . 2 2 7 1 8 5 0 . 7 9 6 7 7 4 0 . 1 7 5 4 2 7 -1 . 2 9 5 0 . 1 9 5 3 1 sitesite _ 2 0 . 1 2 1 2 3 6 1 . 1 2 8 8 9 2 0 . 1 6 0 0 5 2 0 . 7 5 7 0 . 4 4 8 7 6 sitesite _ 3 -0 . 0 8 4 4 8 3 0 . 9 1 8 9 8 7 0 . 1 6 1 1 2 1 -0 . 5 2 4 0 . 6 0 0 0 4 sitesite _ 4 -0 . 4 7 1 6 1 2 0 . 6 2 3 9 9 6 0 . 1 7 5 2 0 3 -2 . 6 9 2 0 . 0 0 7 1 1 ** sitesite _ 5 -0 . 1 2 8 2 8 6 0 . 8 7 9 6 0 2 0 . 1 6 1 7 8 2 -0 . 7 9 3 0 . 4 2 7 8 0 sitesite _ 6 -0 . 2 4 0 5 6 3 0 . 7 8 6 1 8 5 0 . 1 6 1 9 5 8 -1 . 4 8 5 0 . 1 3 7 4 5 sitesite _ 7 0 . 3 7 2 0 0 4 1 . 4 5 0 6 3 9 0 . 1 5 7 6 1 6 2 . 3 6 0 0 . 0 1 8 2 7 * sitesite _ 8 0 . 0 6 7 7 0 0 1 . 0 7 0 0 4 5 0 . 1 6 0 8 7 6 0 . 4 2 1 0 . 6 7 3 8 8 sitesite _ 9 0 . 2 6 7 3 7 3 1 . 3 0 6 5 2 8 0 . 1 5 4 9 1 1 1 . 7 2 6 0 . 0 8 4 3 5 . NALTREXONE : THERAPY 0 . 3 3 7 5 3 9 1 . 4 0 1 4 9 5 0 . 1 3 7 4 4 1 2 . 4 5 6 0 . 0 1 4 0 5 * NALTREXONE has a significant negative log hazard ratio, but THERAPY has a nonsignificant negative log hazard ratio. More interestingly, their interaction NALTREXONE:THERAPY has a significant positive log hazard ratio. This suggests that combining NALTREXONE and THERAPY is worse than using NALTREXONE alone to delay the first time of heavy drinking and other endpoints. This is also coherent with the survival curves in Figure [27](#fig_110).5, in which the best Kaplan-Meier curve corresponds to NALTREXONE=1, THERAPY=0.

Using Keele (2010)'s data, we have the following results:

> cox . fit <-coxph ( Surv ( acttime , censor ) + hcomm + hfloor + scomm + sfloor + + prespart + demhsmaj + demsnmaj + + prevgenx + lethal + + deathrt 1 + acutediz + hosp 0 1 + + hospdisc + hhosleng + + mandiz 0 1 + femdiz 0 1 + peddiz 0 1 + orphdum + + natreg + I ( natreg ^2 ) + vandavg 3 + wpnoavg 3 + + condavg 3 + orderent + stafcder , + data = fda ) > summary ( cox . fit ) Call : coxph ( formula = Surv ( acttime , censor ) ~hcomm + hfloor + scomm + sfloor + prespart + demhsmaj + demsnmaj + prevgenx + lethal + deathrt 1 + acutediz + hosp 0 1 + hospdisc + hhosleng + mandiz 0 1 + femdiz 0 1 + peddiz 0 1 + orphdum + natreg + I ( natreg ^2 ) + vandavg 3 + wpnoavg 3 + condavg 3 + orderent + stafcder , data = fda ) n = 4 0 8 , number of events = 2 6 2 coef exp ( coef ) se ( coef ) z Pr ( >| z |) hcomm

3 . 6 4 2e -0 1 1 . 4 3 9 e + 0 0 2 . 9 5 1 e + 0 0 0 . 1 2 3 0 . 9 0 1 7 7 5 hfloor 7 . 9 4 4 e + 0 0 2 . 8 1 9 e + 0 3 8 . 1 7 3 e + 0 0 0 . 9 7 2 0 . 3 3 1 0 7 1 scomm 4 . 7 1 6e -0 1 1 . 6 0 3 e + 0 0 1 . 8 9 8 e + 0 0 0 . 2 4 8 0 . 8 0 3 7 7 1 sfloor 2 . 6 0 4 e + 0 0 1 . 3 5 2 e + 0 1 2 . 3 7 0 e + 0 0 1 . 0 9 9 0 . 2 7 1 8 7 7 prespart 8 . 0 3 8e -0 1 2 . 2 3 4 e + 0 0 3 . 0 4 2e -0 1 2 . 6 4 3 0 . 0 0 8 2 2 6 ** demhsmaj 1 . 3 6 3 e + 0 0 3 . 9 0 9 e + 0 0 1 . 9 1 7 e + 0 0 0 . 7 1 1 0 . 4 7 6 8 9 0 demsnmaj 1 . 2 1 7 e + 0 0 3 . 3 7 7 e + 0 0 5 . 6 0 6e -0 1 2 . 1 7 1 0 . 0 2 9 9 4 0 * prevgenx -9 . 9 1 5e -0 4 9 . 9 9 0e -0 1 7 . 7 7 9e -0 4 -1 . 2 7 5 0 . 2 0 2 4 5 9 lethal 7 . 8 7 2e -0 2 1 . 0 8 2 e + 0 0 2 . 3 7 8e -0 1 0 . 3 3 1 0 . 7 4 0 6 0 5 deathrt 1 6 . 5 3 7e -0 1 1 . 9 2 3 e + 0 0 2 . 4 3 5e -0 1 2 . 6 8 5 0 . 0 0 7 2 5 3 ** acutediz 1 . 9 9 4e -0 1 1 . 2 2 1 e + 0 0 2 . 2 6 2e -0 1 0 . 8 8 2 0 . 3 7 7 8 9 6 hosp 0 1 4 . 2 8 0e -0 2 1 . 0 4 4 e + 0 0 2 . 4 9 5e -0 1 0 . 1 7 2 0 . 8 6 3 7 6 8 hospdisc -1 . 2 3 8e -0 6 1 . 0 0 0 e + 0 0 5 . 2 7 8e -0 7 -2 . 3 4 5 0 . 0 1 9 0 0 2 * hhosleng -1 . 2 7 3e -0 2 9 . 8 7 4e -0 1 1 . 9 8 8e -0 2 -0 . 6 4 0 0 . 5 2 1 8 9 1 mandiz 0 1 -1 . 1 7 7e -0 1 8 . 8 8 9e -0 1 3 . 8 0 0e -0 1 -0 . 3 1 0 0 . 7 5 6 7 1 1 femdiz 0 1 9 . 0 3 2e -0 1 2 . 4 6 8 e + 0 0 3 . 4 9 7e -0 1 2 . 5 8 3 0 . 0 0 9 7 9 9 ** peddiz 0 1 -3 . 4 0 1e -0 2 9 . 6 6 6e -0 1 5 . 1 1 2e -0 1 -0 . 0 6 7 0 . 9 4 6 9 6 8 orphdum 5 . 5 4 0e -0 1 1 . 7 4 0 e + 0 0 2 . 1 0 9e -0 1 2 . 6 2 6 0 . 0 0 8 6 3 0 ** natreg -2 . 2 2 1e -0 2 9 . 7 8 0e -0 1 8 . 2 8 2e -0 3 -2 . 6 8 2 0 . 0 0 7 3 1 8 ** I ( natreg ^2 ) 1 . 0 2 9e -0 4 1 . 0 0 0 e + 0 0 4 . 5 6 7e -0 5 2 . 2 5 3 0 . 0 2 4 2 7 6 * vandavg 3 -2 . 0 1 4e -0 2 9 . 8 0 1e -0 1 1 . 5 3 6e -0 2 -1 . 3 1 1 0 . 1 8 9 8 0 2 wpnoavg 3 5 . 2 2 0e -0 3 1 . 0 0 5 e + 0 0 1 . 4 2 6e -0 3 3 . 6 6 0 0 . 0 0 0 2 5 2 *** condavg 3 9 . 6 2 8e -0 3 1 . 0 1 0 e + 0 0 2 . 2 7 1e -0 2 0 . 4 2 4 0 . 6 7 1 6 3 7 orderent -1 . 8 1 0e -0 2 9 . 8 2 1e -0 1 8 . 1 4 7e -0 3 -2 . 2 2 2 0 . 0 2 6 2 9 6 * stafcder 8 . 0 1 3e -0 4 1 . 0 0 1 e + 0 0 7 . 9 8 6e -0 4 1 . 0 0 3 0 . 3 1 5 7 1 9

## Log-rank test as a score test from Cox model

A standard problem in clinical trials is to compare the survival times under treatment and control. Assume no ties in the failure times, and let x denote the binary indicator for treatment. Under the proportional hazards assumption, the control group has hazard λ 0 (t), and the treatment group has hazard λ 1 (t) = λ 0 (t)e β . We are interested in testing the null hypothesis

$β = 0 ⇐⇒ λ 1 (t) = λ 0 (t) ⇐⇒ S 1 (t) = S 0 (t).$Under the null hypothesis, the score function reduces to

$∂ log L(β) ∂β β=0 = K k=1 {x k -E β=0 (x | R k )} = K k=1 x k - r k1 r k , because E β=0 (x | R k ) = l∈R(t k ) x l l∈R(t k ) 1 = r k1 r k$equaling the ratio of the number of treated units at risk r k1 over the number of units at risk r k , at time t k . The Fisher information at the null is

$- ∂ 2 log L(0) ∂β∂β t = K k=1 cov β=0 (x | R k ) = K k=1 r k1 r k 1 - r k1 r k .$The score test for classical parametric models relies on

$∂ log L(0) ∂β a ∼ N 0, - ∂ 2 log L(0) ∂β∂β t ,$which follows from Bartlett's identity and the CLT. Applying this fact to Cox's model, we have

$LR = K k=1 x k -r k1 r k K k=1 r k1 r k 1 -r k1 r k a ∼ N(0, 1).$So we reject the null at level α if |LR| is larger than the upper 1 -α/2 quantile of standard Normal. This is almost identical to the log-rank test without ties. Allowing for ties, [Mantel (1966)](#b174) proposed a more general form of the log-rank test[foot_26](#foot_26) . The survdiff function in the survival package implements various tests including the logrank test as a special case. Below, I use the gehan dataset in the MASS package to illustrate the log rank test. The data were from a matched-pair experiment of 42 leukaemia patients [(Gehan, 1965)](#b122). Treated units received the drug 6-mercaptopurine, and the rest are controls. For illustration purposes, I ignore the pair indicators.

## > library ( MASS ) > head ( gehan ) pair time cens treat

$1 1 1 1 control 2 1 1 0 1 6 -MP 3 2 2 2 1 control 4 2 7 1 6 -MP 5 3 3 1 control 6 3 3 2 0 6 -MP > survdiff ( Surv ( time , cens ) ~treat , + data = gehan ) Call : survdiff ( formula = Surv ( time , cens ) ~treat , data = gehan ) N Observed Expected (O -E )^2 / E (O -E )^2 / V treat = 6 -MP 2 1 9 1 9 . 3 5 . 4 6 1 6 . 8 treat = control 2 1$2 1 1 0 . 7 9 . 7 7 1 6 . 8

Chisq = 1 6 . 8 on 1 degrees of freedom , p = 4e -0 5

The treatment was quite effective, yielding an extremely small p-value even with moderate sample size. It is also clear from the Kaplan-Meier curves in Figure [27](#fig_110).7 and the results from fitting the Cox proportional hazards model. 4 . 8 1 6 9 0 . 4 1 2 4 3 . 8 1 2 0 . 0 0 0 1 3 8 *** exp ( coef ) exp ( -coef ) lower . 9 5 upper . 9 5 treatcontrol 4 . 8 1 7 0 . 2 0 7 6 2 . 1 4 7 1 0 . 8 1 Concordance = 0 . 6 9 ( se = 0 . 0 4 1 ) Likelihood ratio test = 1 6 . 3 5 on 1 df , p = 5e -0 5 Wald test = 1 4 . 5 3 on 1 df , p = 1e -0 4 Score ( logrank ) test = 1 7 . 2 5 on 1 df , p = 3e -0 5

The log-rank test is a standard tool in survival analysis. However, what it delivers is just a special case of the Cox proportional hazards model. The p-value from the log-rank test is close to the p-value from the score test of the Cox proportional hazards model with only a binary treatment indicator. The latter can also adjust for other pretreatment covariates.

## Extensions

## Stratified Cox model

Many randomized trials are stratified. The Combined Pharmacotherapies and Behavioral Interventions study reviewed at the beginning of this chapter is an example with site indicating the strata. The previous analysis includes the dummy variables of site in the Cox model. An alternative more flexible model is to allow for different baseline hazard functions across strata. Technically, assume

$λ s (t | x) = λ s (t) exp(β t x)$for strata s = 1, . . . , S, where β is an unknown parameter and {λ 1 (•), . . . , λ S (•)} are unknown functions. Therefore, within each stratum s, the proportional hazards assumption holds; across strata, the proportional hazard assumption may not hold. Within stratum s, we can obtain the partial likelihood L s (β); by independence of the data across strata, we can obtain the joint partial likelihood Based on the standard procedure, we can obtain the MLE and conduct inference based on the large-sample theory. The coxph function can naturally allow for stratification with the + strata() in the regression formula. -0 . 2 5 2 2 3 9 0 . 7 7 7 0 5 9 0 . 0 9 7 7 8 8 0 . 0 9 6 4 3 7 -2 . 6 1 6 0 . 0 0 8 9 1 THERAPY -0 . 1 7 3 4 5 6 0 . 8 4 0 7 5 4 0 . 0 9 6 2 5 8 0 . 0 9 5 9 5 8 -1 . 8 0 8 0 . 0 7 0 6 6 AGE -0 . 0 1 5 1 0 4 0 . 9 8 5 0 1 0 0 . 0 0 3 5 5 4 0 . 0 0 3 5 1 2 -4 . 3 0 1 1 . 7e -0 5 GENDERmale -0 . 1 3 9 8 3 7 0 . 8 6 9 5 0 0 0 . 0 7 5 3 8 8 0 . 0 7 6 5 8 0 -1 . 8 2 6 0 . 0 6 7 8 5 T 0 _ PDA 0 . 0 0 2 7 4 7 1 . 0 0 2 7 5 1 0 . 0 0 1 3 6 9 0 . 0 0 1 3 5 0 2 . 0 3 5 0 . 0 4 1 8 2 NALTREXONE : THERAPY 0 . 3 3 5 6 7 1 1 . 3 9 8 8 7 9 0 . 1 3 7 6 7 6 0 . 1 3 6 8 9 0 2 . 4 5 2 0 . 0 1 4 2 0 NALTREXONE ** THERAPY . AGE *** GENDERmale . T 0 _ PDA * NALTREXONE : THERAPY * exp ( coef ) exp ( -coef ) lower . 9 5 upper . 9 5 NALTREXONE 0 . 7 7 7 1 1 . 2 8 6 9 0 . 6 4 3 2 0 . 9 3 8 7 THERAPY 0 . 8 4 0 8 1 . 1 8 9 4 0 . 6 9 6 6 1 . 0 1 4 7 AGE 0 . 9 8 5 0 1 . 0 1 5 2 0 . 9 7 8 3 0 . 9 9 1 8 GENDERmale 0 . 8 6 9 5 1 . 1 5 0 1 0 . 7 4 8 3 1 . 0 1 0 3 T 0 _ PDA 1 . 0 0 2 8 0 . 9 9 7 3 1 . 0 0 0 1 1 . 0 0 5 4 NALTREXONE : THERAPY 1 . 3 9 8 9 0 . 7 1 4 9 1 . 0 6 9 7 1 . 8 2 9 4 treat *** adult agedx exp ( coef ) exp ( -coef ) lower . 9 5 upper . 9 5 treat 0 . 4 5 7 7 2 . 1 8 4 7 0 . 3 4 2 3 0 . 6 1 2 2 adult 0 . 8 7 2 0 1 . 1 4 6 8 0 . 4 8 8 9 1 . 5 5 5 3 agedx 1 . 0 0 7 9 0 . 9 9 2 2 0 . 9 8 7 8 1 . 0 2 8 4 Concordance = 0 . 5 9 6 ( se = 0 . 0 2 3 ) Likelihood ratio test = 2 3 . 1 3 on 3 df , p = 4e -0 5 Wald test = 2 8 . 5 5 on 3 df , p = 3e -0 6 Score ( logrank ) test = 2 3 . 0 1 on 3 df , p = 4e -0 5 , Robust = 2 6 . 5 5 p = 7e -0 6 ( Note : the likelihood ratio and score tests assume independence of observations within a cluster , the Wald and robust score tests do not ).

## Penalized Cox model

The glmnet package implements the penalized version of the Cox model.

## Critiques on survival analysis

The Kaplan-Meier curve and the Cox proportional hazards model are standard tools for analyzing medical data with censored survival times. They are among the most commonly used methods in medical journals. [Kaplan and Meier (1958)](#b153) and [Cox (1972)](#b82) are two of the most cited papers in statistics. [Freedman (2008)](#b110) criticized these two standard tools. Both rely on the critical assumption of noninformative censoring that censoring and survival time are independent or conditionally independent given covariates. When censoring is due to administrative constraints, this may be a plausible assumption. The data from [Lin et al. (2016)](#b166) is a convincing example of noninformative censoring. However, many other studies have more complex censoring mechanisms, for example, one may drop out of the study, and another may be killed by an irrelevant cause. The Cox model relies on an additional assumption of proportional hazards. This particular functional form facilitates the interpretation of the coefficients as log conditional hazard ratios if the model is correctly specified. However, its interpretation becomes obscure when the model is mis-specified. Two survival curves based on [Lin et al. (2016)](#b166) 's data cross each other, which makes the proportional hazards assumption dubious. [Hernán (2010)](#b136) offered a more fundamental critique on hazard-based survival analysis. For example, in a randomized treatment-control experiment, the hazard ratio at time t is the ratio of the instantaneous probability of death conditioning on the event that the patients have survived up to time t: lim ∆t↓0 pr(t ≤ T < t + ∆t | x = 1, T ≥ t)/∆t lim ∆t↓0 pr(t ≤ T < t + ∆t | x = 0, T ≥ t)/∆t This ratio is difficult to interpret because patients who have survived up to time t can be quite different in treatment and control groups, especially when the treatment is effective. Even though patients are randomly assigned at the baseline, the survivors up to time t are not. [Hernán (2010)](#b136) suggested focusing on the comparison of the survival functions.

## Linearly independent vectors and rank

We call a set of vectors A 1 , . . . , A m ∈ R n linearly independent if

$x 1 A 1 + • • • + x m A m = 0 must imply x 1 = • • • = x m =$0. We call A j1 , . . . , A j k maximally linearly independent if adding another vector makes them linearly dependent. Define k as the rank of of {A 1 , . . . , A m } and also define k as the rank of the matrix A = (A 1 , . . . , A m ).

A set of vectors may have different subsets of vectors that are maximally linearly independent. But the rank k is unique. We can also define the rank of a matrix in terms of its row vectors. A remarkable theorem in linear algebra is that it does not matter whether we define the rank of a matrix in terms of its column vectors or row vectors.

From the matrix product formulas (A.1) and (A.2), we have the following result.

Proposition A.1 rank(AB) ≤ min{rank(A), rank(B)}.

The rank decomposition of a matrix decomposes A into the product of two matrices of full ranks. □ Proposition A.1 ensures that rank(B) ≥ k and rank(C) ≥ k so they must both have rank k. The decomposition in Proposition A.2 is not unique since the choice of the maximally linearly independent column vectors of A is not unique.

## Some special matrices

An n×n matrix A is symmetric if A t = A. An n×n diagonal matrix A has zero off-diagonal elements, denoted by A = diag{a 11 , . . . , a nn }. Diagonal matrices are symmetric.

An n × n matrix is orthogonal if A t A = AA t = I n . The column vectors of an orthogonal matrix are orthonormal; so are its row vectors. If A is orthogonal, then ∥Ax∥ = ∥x∥ for any vector x ∈ R n . That is, multiplying a vector by an orthogonal matrix does not change the length of the vector. Geometrically, an orthogonal matrix corresponds to rotation.

An n × n matrix A is upper triangular if a ij = 0 for i > j and lower triangular if a ij = 0 for i < j. An n × n matrix A can be factorized as

$A = LU$where L is a lower triangular matrix and U is an upper triangular matrix. This is called the LU decomposition of a matrix.

## Determinant

The original definition of the determinant of a square matrix A = (a ij ) has a very complex form, which will not be used in this book.

The determinant of a 2 × 2 matrix has a simple form: if both A and B are invertible.

## Eigenvalues and eigenvectors

For an n × n matrix A, if there exists a pair of n-dimensional vector x and a scalar λ such that Ax = λx, then we call λ an eigenvalue and x the associated eigenvector of A. From the definition, eigenvalue and eigenvector always come in pairs. The following eigen-decomposition theorem is important for real symmetric matrices.

Theorem A.1 If A is an n × n symmetric matrix, then there exists an orthogonal matrix P such that P t AP = diag{λ 1 , . . . , λ n },

where the λ's are the n eigenvalues of A, and the column vectors of P = (γ 1 , • • • , γ n ) are the corresponding eigenvectors.

If we write the eigendecomposition as AP = P diag{λ 1 , . . . , λ n } or, equivalently,

$A(γ 1 , • • • , γ n ) = (λ 1 γ 1 , • • • , λ n γ n ),$then (λ i , γ i ) must be a pair of eigenvalue and eigenvector. Moreover, the eigendecomposition in Theorem A.1 is unique up to the permutation of the columns of P and the corresponding λ i 's.

Corollary A.1 If P t AP = diag{λ 1 , . . . , λ n }, then

$A = P diag{λ 1 , . . . , λ n }P t , A k = A • A • • • A = P diag{λ k 1 , . . . , λ k n }P t ;$if the eigenvalues of A are nonzero, then

A -1 = P diag{1/λ 1 , . . . , 1/λ n }P t .

The eigen-decomposition is also useful for defining the square root of an n×n symmetric matrix. In particular, if the eigenvalues of A are nonnegative, then we can define A 1/2 = P diag{ λ 1 , . . . , λ n }P t By definition, A 1/2 is a symmetric matrix satisfying A 1/2 A 1/2 = A. There are other definitions of the square root of a symmetric matrix, but we adopt this form in this book.

From Theorem A.1, we can write A as

$A = P diag{λ 1 , . . . , λ n }P t = (γ 1 , • • • , γ n )diag{λ 1 , . . . , λ n }    γ t 1 . . . γ t n    = n i=1 λ i γ i γ t i .$For an n × n symmetric matrix A, its rank equals the number of non-zero eigenvalues and its determinant equals the product of all eigenvalues. The matrix A is of full rank if all its eigenvalues are non-zero, which implies that its rank equals n and its determinant is non-zero.

## Quadratic form

For an n × n symmetric matrix A = (a ij ) and an n-dimensional vector x, we can define the quadratic form as

$x t Ax = ⟨x, Ax⟩ = n i=1 n j=1 a ij x i x j .$We always consider a symmetric matrix in the quadratic form without loss of generality. Otherwise, we can symmetrize A as Ã = (A + A t )/2 without changing the value of the quadratic form because

x t Ax = x t Ãx.

We call A positive semi-definite, denoted by A ⪰ 0, if x t Ax ≥ 0 for all x; we call A positive definite, denoted by A ≻ 0, if x t Ax > 0 for all nonzero x.

We can also define the partial order between matrices. We call A ⪰ B if and only if A -B ⪰ 0, and we call A ≻ B if and only if A -B ≻ 0. This is important in statistics because we often compare the efficiency of estimators based on their variances or covariance matrices. Given two unbiased estimators θ1 and θ2 for a scalar parameter θ, we say that θ1 is more efficient than θ2 if var( θ2 ) ≥ var( θ1 ). In the vector case, we say that θ1 is more efficient than θ2 if cov( θ2 ) ⪰ cov( θ1 ), which is equivalent to var(ℓ t θ2 ) ≥ var(ℓ t θ1 ) for any linear combination of the estimators.

The eigenvalues of a symmetric matrix determine whether it is positive semi-definite or positive definite.

Theorem A.2 For a symmetric matrix A, it is positive semi-definite if and only if all its eigenvalues are nonnegative, and it is positive definite if and only if all its eigenvalues are positive.

An important result is the relationship between the eigenvalues and the extreme values of the quadratic form. Assume that the eigenvalues are rearranged in decreasing order such that λ 1 ≥ • • • ≥ λ n . For a unit vector x, we have that has length ∥α∥ 2 = ∥x∥ 2 = 1. Then the maximum value of x t Ax is λ 1 which is achieved at α 1 = 1 and α 2 = • • • = α n = 0 (for example, if x = γ 1 , then α 1 = 1 and α 2 = • • • = α n = 0). For a unit vector x that is orthogonal to γ 1 , we have that

$x t Ax = n i=2 λ i α 2 i$where α = P t x has unit length with α 1 = 0. The maximum value of x t Ax is λ 2 which is achieved at α 2 = 1 and α 1 = α 3 = • • • = α n = 0, for example, x = γ 2 . By induction, we have the following theorem.

Theorem A.3 Suppose that an n × n symmetric matrix has eigen-decomposition n i=1 λ i γ i γ t i where λ 1 ≥ • • • ≥ λ n .

1. The optimization problem max x

x t Ax such that ∥x∥ = 1 has maximum λ 1 which can be achieved by γ 1 .

2. The optimization problem max x

x t Ax such that ∥x∥ = 1, x ⊥ γ 1 has maximum λ 2 which can be achieved by γ 2 .

3. The optimization problem max x

x t Ax such that ∥x∥ = 1, x ⊥ γ 1 , . . . , x ⊥ γ k has maximum λ k+1 which can be achieved by γ k+1 (k = 1, . . . , n -1).

Theorem A.3 implies the following theorem on the Rayleigh quotient r(x) = x t Ax/x t x (x ∈ R n ). with the maximizer and minimizer being the eigenvectors corresponding to the maximum and minimum eigenvalues, respectively.

An immediate consequence of Theorem A.4 is that the diagonal elements of A are bounded by the smallest and largest eigenvalues of A. This follows by taking x = (0, . . . , 1, . . . , 0) t where only the ith element equals 1.

## Trace

The trace of an n × n matrix A = (a ij ) is the sum of all its diagonal elements, denoted by trace(A) = n i=1 a ii .

The trace operator has two important properties that can sometimes help to simplify calculations.

Proposition A.5 trace(AB) = trace(BA) as long as AB and BA are both square matrices.

We can verify Proposition A.5 by definition. It states that AB and BA have the same trace although AB differs from BA in general. In fact, it is particularly useful if the dimension of BA is much lower than the dimension of AB. For example, if both A = (a 1 , . . . , a n ) 

## □

## Projection matrix

An n × n matrix H is a projection matrix if it is symmetric and H 2 = H. The eigenvalues of H must be either 1 or 0. To see this, we assume that Hx = λx for some nonzero vector x, and use two ways to calculate H 2 x: H 2 x = Hx = λx, H 2 x = H(Hx) = H(λx) = λHx = λ 2 x.

So (λ -λ 2 )x = 0 which implies that λ -λ 2 = 0, i.e., λ = 0 or 1. So the trace of H equals its rank: trace(H) = rank(H).

Why is this a reasonable definition of a "projection matrix"? Or, why must a projection matrix satisfy H t = H and H 2 = H? First, it is reasonable to require that Hx 1 = x 1 for any x 1 ∈ C(H), the column space of H. Since x 1 = Hα for some α, we indeed have Hx 1 = H(Hα) = H 2 α = Hα = x 1 because of the property H 2 = H. Second, it is reasonable to require that x 1 ⊥ x 2 for any vector x 1 = Hα ∈ C(H) and x 2 such that Hx 2 = 0. So we need α t H t x 2 = 0 which is true if H = H t . Therefore, the two conditions are natural for the definition of a projection matrix.

More interestingly, a project matrix has a more explicit form as stated below.

Theorem A.5 If an n × p matrix X has p linearly independent columns, then H = X(X t X) -1 X t is a projection matrix. Conversely, if an n × n matrix H is a projection matrix with rank p, then H = X(X t X) -1 X t for some n × p matrix X with linearly independent columns.

It is relatively easy to verify the first part of Theorem A.5; see Chapter 3. The second part of Theorem A.5 follows from the eigen-decomposition of H, with the first p eigen-vectors being the column vectors of X.

## Cholesky decomposition

An n × n positive semi-definite matrix A can be decomposed as A = LL t where L is an n × n lower triangular matrix with non-negative diagonal elements. If A is positive definite, the decomposition is unique. In general, it is not. Take an arbitrary orthogonal matrix Q, we have A = LQQ t L t = CC t where C = LQ. So we can decompose a positive semi-definite matrix A as A = CC t , but this decomposition is not unique.

## Singular value decomposition (SVD)

Any n × m matrix A can be decomposed as

$A = U DV t$where U is n × n orthogonal matrix, V is m × m orthogonal matrix, and D is n × m matrix with all zeros for the non-diagonal elements. For a tall matrix with n ≥ m, the diagonal matrix D has many zeros, so we can also write . . .

$V t r    = r k=1 d k U k V t k .$The SVD implies that

$AA t = U DD t U t , A t A = V D t DV t ,$which are the eigen decompositions of AA t and A t A. This ensures that AA t and A t A have the same non-zero eigenvalues. An application of the SVD is to define the pseudoinverse of any matrix. Define D + as the pseudoinverse of D with the non-zero elements inverted but the zero elements intact at zero. Define

$A + = V D + U t = r k=1 d -1 k V k U t k$as the pseudoinverse of A. The definition holds even if A is not a square matrix. We can verify that AA + A = A, A + AA + = A + .

If A is a square nondegenerate matrix, then A + = A -1 equals the standard definition of the inverse. In the special case with a symmetric A, its SVD is identical to its eigen decomposition. If A is not invertible, its pseudoinverse equals A + = P diag(λ -1 1 , . . . , λ -1 k , 0, . . . , 0)P t if rank(A) = k < n and λ 1 , λ 1 , . . . , λ k are the nonzero eigen-values.

Another application of the SVD is the polar decomposition for any square matrix A. Since A = U DV t = U DU t U V t with orthogonal U and V , we have A = (AA t ) 1/2 Γ, where (AA t ) 1/2 = U DU t and Γ = U V t is an orthogonal matrix. The above formulas become more powerful in conjunction with the chain rule. For example, for any differentiable function h(z) mapping from R to R with derivative h ′ (z), we have ∂h(a t x) ∂x = h ′ (a t x)a, ∂h(x t Ax) ∂x = 2h ′ (x t Ax)Ax.

## A.2 Vector calculus

For any differentiable function h(z) mapping from R q to R with gradient ∂h(z)/∂z, we have ∂h(B t x) ∂x = ∂h(B t 1 x, . . . , B t q x) ∂x = q j=1 ∂h(B t 1 x, . . . , B t q x) ∂z j B j = B ∂h(B t x) ∂z .

Moreover, we can also define the Hessian matrix of a function f (x) mapping from R p to R:

$∂ 2 f (x) ∂x∂x t = ∂ 2 f (x) ∂x i ∂x j 1≤i,j≤p = ∂ ∂x t ∂f (x) ∂x .$A.3 Homework problems

## Triangle inequality of the inner product

With three unit vectors u, v, w ∈ R n , prove that 1 -⟨u, w⟩ ≤ 1 -⟨u, v⟩ + 1 -⟨v, w⟩.

Remark: The result is a direct consequence of the standard triangle inequality but it has an interesting implication. If ⟨u, v⟩ ≥ 1 -ϵ and ⟨v, w⟩ ≥ 1 -ϵ, then ⟨u, w⟩ ≥ 1 -4ϵ. This implied inequality is mostly interesting when ϵ is small. It states that when u and v are highly corrected and v and w are highly correlated, then u and w must also be highly correlated. Note that we can find counterexamples for the following relationship: ⟨u, v⟩ > 0, ⟨v, w⟩ > 0 but ⟨u, w⟩ = 0.

## Van der Corput inequality

Assume that v, u 1 , . . . , u m ∈ R n have unit length. Prove that Remark: This result is not too difficult to prove but it says something fundamentally interesting. If v is correlated with many vectors u 1 , . . . , u m , then at least some vectors in u 1 , . . . , u m must be also correlated.

## Inverse of a block matrix

Prove that

$A B C D -1 = A -1 + A -1 B(D -CA -1 B) -1 CA -1 -A -1 B(D -CA -1 B) -1 -(D -CA -1 B) -1 CA -1 (D -CA -1 B) -1 = (A -BD -1 C) -1 -(A -BD -1 C) -1 BD -1 -D -1 C(A -BD -1 C) -1 D -1 + D -1 C(A -BD -1 C) -1 BD -1 ,$provided that all the inverses of the matrices exist. The two forms of the inverse imply the Woodbury formula:

$(A -BD -1 C) -1 = A -1 + A -1 B(D -CA -1 B) -1 CA -1 ,$which further implies the Sherman-Morrison formula:

$(A + uv t ) -1 = A -1 -(1 + v t A -1 u) -1 A -1 uv t A -1 ,$where A is an invertible square matrix, and u and v are two column vectors.

## Matrix determinant lemma

Prove that given the identity matrix I n and two n-vectors u and v, we have det(I n + uv t ) = 1 + v t u.

Further show that if I n is replaced by an n × n invertible matrix A, we have det(A + uv t ) = (1 + v t A -1 u) • det(A).

## Decomposition of a positive semi-definite matrix

Show that if A is positive semi-definite, then there exists a matrix C such that A = CC t .

## Trace of the product of two matrices

Prove that A and B are two n × n positive semi-definite matrices, then trace(AB) ≥ 0.

Hint: Use the eigen-decomposition of A = n i=1 λ i γ i γ t i . Remark: In fact, a stronger result holds. If two n × n symmetric matrices A and B have eigenvalues

$λ 1 ≥ • • • ≥ λ n , µ 1 ≥ • • • ≥ µ n respectively, then n i=1 λ i µ n+1-i ≤ trace(AB) ≤ n i=1 λ i µ i .$The result is due to Von Neumann [(1937)](#) and [Ruhe (1970)](#b204). See also [Chen and Li (2019, Lemma 4.12)](#).

## Vector calculus

What is the formula for ∂x t Ax/∂x if A is not symmetric in Proposition A.7?

## B

## Random Variables

Let "IID" denote "independent and identically distributed", "

iid ∼" denote a sequence of random variables that are IID with some common distribution, and " " denote independence between random variables. Define Euler's Gamma function as

$Γ(z) = ∞ 0 x z-1 e -x dx, (z > 0),$which is a natural extension of the factorial since Γ(n) = (n -1)!. Further define the digamma function as ψ(z) = d log Γ(z)/dz and the trigamma function as ψ ′ (z). In R, we can use gamma ( z ) lgamma ( z ) digamma ( z ) trigamma ( z )

to compute Γ(z), log Γ(z), ψ(z), and ψ ′ (z).

B.1 Some important univariate random variables B.1.1 Normal, χ 2 , t and F

The standard Normal random variable Z ∼ N(0, 1) has density f (z) = (2π) -1/2 exp -z 2 /2 .

A Normal random variable X has mean µ and variance σ 2 , denoted by N(µ, σ 2 ), if X = µ + σZ. We can show that X has density f (x) = (2π) 1/2 exp -(x -µ) 2 /(2σ 2 ) .

A chi-squared random variable with degrees of freedom n, denoted by Q n ∼ χ 2 n , can be represented as

$Q n = n i=1 Z 2 i ,$where Z i iid ∼ N(0, 1). We can show that its density is f n (q) = q n/2 exp(-q/2) 2 n/2 Γ(n/2) , (q > 0). (B.1)

We can verify that the above density (B.1) is well-defined even if we change the integer n to be an arbitrary positive real number ν, and call the corresponding random variable Q ν a chi-squared random variable with degrees of freedom ν, denoted by Q ν ∼ χ 2 ν .

## Linear Model and Extensions

A t random variable with degrees of freedom ν can be represented as

$t ν = Z Q ν /ν$where Z ∼ N(0, 1), Q ν ∼ χ 2 ν , and Z Q ν . An F random variable with degrees of freedom (r, s) can be represented as

$F = Q r /r Q s /s$where Q r ∼ χ 2 r , Q s ∼ χ 2 s , and Q r Q s .

## B.1.2 Beta-Gamma duality

The Gamma(α, β) random variable with parameters α, β > 0 has density

$f (x) = β α Γ(α)$x α-1 e -βx , (x > 0). (B.

2)

The Beta(α, β) random variable with parameters α, β > 0 has density

$f (x) = Γ(α + β) Γ(α)Γ(β)$x α-1 (1 -x) β-1 , (0 < x < 1).

These two random variables are closely related as shown in the following theorem. Another simple but useful fact is that χ 2 is a special Gamma random variable. Comparing the densities in (B.1) and (B.2), we obtain the following result.

Proposition B.1 χ 2 n ∼ Gamma(n/2, 1/2). We can also calculate the moments of the Gamma and Beta distributions. .

Second, we have pr(X i = X) = pr(X i < X j for all j ̸ = i) Therefore, the density of y at c ≥ 0 is λ 2 e λc , and the density of y at c ≤ 0 is λ 2 e -λc , which can be unified as λ 2 e λ|c| . □ If X 0 is the standard exponential random variable, then we define the Gumbel(µ, β) random variable as Y = µ -β log X 0 .

The standard Gumbel distribution has µ = 0 and β = 1, with CDF F (y) = exp(-e -y ), y ∈ R and density f (y) = exp(-e -y )e -y , y ∈ R.

By definition and Proposition B.7, we can verify that the maximum of IID Gumbels is also Gumbel. 

## B.2 Multivariate distributions

A random vector (X 1 , . . . , X n ) t is a vector consisting of n random variables. If all components are continuous, we can define its joint density f X1•••Xn (x 1 , . . . , x n ). 

## Independence

Random variables (X 1 , . . . , X n ) are mutually independent if

$f X1•••Xn (x 1 , . . . , x n ) = f X1 (x 1 ) • • • f Xn (x n ).$Note that in this definition, each of (X 1 , . . . , X n ) can be vectors. We have the following rules under independence. For a scalar random variable, cov(Y ) = var(Y ).

Proposition B.12 For A ∈ R r×n , Y ∈ R n and C ∈ R r , we have cov(AY + C) = Acov(Y )A t .

Using Proposition B.12, we can verify that for any n-dimensional random vector, cov(Y ) ⪰ 0 because for all x ∈ R n , we have 

## B.3 Multivariate Normal and its properties

I use a generative definition of the multivariate Normal random vector. First, Z is a standard Normal random vector if Z = (Z 1 , . . . , Z n ) t has components Z i iid ∼ N(0, 1). Given a mean vector µ and a positive semi-definite covariance matrix Σ, define a Normal random vector Y ∼ N(µ, Σ) with mean µ and covariance Σ if Y can be represented as

$Y = µ + AZ, (B.3)$where A satisfies Σ = AA t . We can verify that cov(Y ) = Σ, so indeed Σ is its covariance matrix. If Σ ≻ 0, then we can verify that Y has density f Y (y) = (2π) -n/2 {det(Σ)} -1/2 exp -(y -µ) t Σ -1 (y -µ)/2 . (B.4)

We can easily verify the following result by calculating the density.

Proposition B.14 If Z ∼ N(0, I n ) and Γ is an orthogonal matrix, then ΓZ ∼ N(0, I n ).

I do not define multivariate Normal based on the density (B.4) because it is only well defined with a positive definite Σ. I do not define multivariate Normal based on the characteristic function because it is more advanced than the level of this book. Definition (B.3) does not require Σ to be positive definite and is more elementary. However, it has a subtle issue of uniqueness. Although the decomposition Σ = AA t is not unique, the resulting distribution Y = µ + AZ is. We can verify this using the Polar decomposition. Because A = Σ 1/2 Γ where Γ is an orthogonal matrix, we have Y = µ + Σ 1/2 ΓZ = µ + Σ 1/2 Z where Z = ΓZ is a standard Normal random vector by Proposition B.14. Importantly, although the definition (B.3) can be general, we usually use the following representation

$Y = µ + Σ 1/2 Z. Theorem B.5 Assume that Y 1 Y 2 ∼ N µ 1 µ 2 , Σ 11 Σ 12 Σ 21 Σ 22 .$Then Y 1 Y 2 if and only if Σ 12 = 0. □ An obvious corollary of Proposition B.15 is that if X 1 ∼ N(µ 1 , σ 2 1 ) and X 2 ∼ N(µ 2 , σ 2 2 ) are independent, then X 1 +X 2 ∼ N(µ 1 +µ 2 , σ 2 1 +σ 2 2 ). So the summation of two independent Normals is also Normal. Remarkably, the reverse of the result is also true.

Theorem B.6 (Levy-Cramer) If X 1 X 2 and X 1 + X 2 is Normal, then both X 1 and X 2 must be Normal.

The statement of Theorem B.6 is extremely simple. But its proof is non-trivial and beyond the scope of this book. See [Benhamou et al. (2018)](#b56) for a proof.

$Theorem B.7 Assume Y 1 Y 2 ∼ N µ 1 µ 2 , Σ 11 Σ 12 Σ 21 Σ 22 .$1. The marginal distributions are Normal:

$Y 1 ∼ N (µ 1 , Σ 11 ) , Y 2 ∼ N (µ 2 , Σ 22 ) .$2. If Σ 22 ≻ 0, then the conditional distribution is Normal:

$Y 1 | Y 2 = y 2 ∼ N µ 1 + Σ 12 Σ -1 22 (y 2 -µ 2 ), Σ 11 -Σ 12 Σ -1 22 Σ 21 ;$Y 2 is independent of the residual

$Y 1 -Σ 12 Σ -1 22 (Y 2 -µ 2 ) ∼ N µ 1 , Σ 11 -Σ 12 Σ -1 22 Σ 21 .$I review some other results of the multivariate Normal below.

Proposition B.16 Assume Y ∼ N(µ, σ 2 I n ). If AB t = 0, then AY BY .

Proposition B.17 Assume

$Y 1 Y 2 ∼ N µ 1 µ 2 , σ 2 1 ρσ 1 σ 2 ρσ 1 σ 2 σ 2 2 ,$where ρ is the correlation coefficient defined as

$ρ = cov(Y 1 , Y 2 )$var(Y 1 )var(Y 2 ) .

Then the conditional distribution is

$Y 1 | Y 2 = y 2 ∼ N µ 1 + ρ σ 1 σ 2 (y 2 -µ 2 ), σ 2 1 (1 -ρ 2 ) .$
## B.4 Quadratic forms of random vectors

Given a random vector Y and a symmetric matrix A, we can define the quadratic form Y t AY , which is a random variable playing an important role in statistics. The first theorem is about its mean. 

## □

The variance of the quadratic form is much more complicated for a general random vector. For the multivariate Normal random vector, we have the following formula. 2. If Y ∼ N(0, I n ) and H is a projection matrix of rank K, then

$Y t HY ∼ χ 2 K .$3. If Y ∼ N(0, H) where H is a projection matrix of rank K, then

$Y t Y ∼ χ 2 K .$Proof of Theorem B.10:

1. I only prove the general result with rank(Σ) = k ≤ n. By definition, Y = µ + Σ 1/2 Z where Z is a standard Normal random vector, then

$(Y -µ) t Σ + (Y -µ) = Z t Σ 1/2 Σ + Σ 1/2 Z = k i=1 Z 2 i ∼ χ 2 k .$2. Using the eigendecomposition of the projection matrix H = P diag {1, . . . , 1, 0, . . . , 0} P t with K 1's in the diagonal matrix, we have Y t HY = Y t P diag {1, . . . , 1, 0, . . . , 0} P t Y = Z t diag {1, . . . , 1, 0, . . . , 0} Z, where Z = (Z 1 , . . . , Z n ) t = P t Y ∼ N(0, P t P ) = N(0, I n ) is a standard Normal random vector. So

$Y t HY = K i=1 Z 2 i ∼ χ 2 K .$3. Writing Y = H 1/2 Z where Z is a standard Normal random vector, we have

$Y t Y = Z t H 1/2 H 1/2 Z = Z t HZ ∼ χ 2 K$using the second result.

## □

B.5 Homework problems

## Beta-Gamma duality

Prove Theorem B.1. Hint: Calculate the joint density of (X + Y, X/(X + Y )).

## Gamma and Beta moments

Prove Propositions B.2-B.5.

## Maximums of Gumbels

Prove Proposition B.9.

## Independence and uncorrelatedness in the multivariate Normal

Prove Theorem B.5.

## Transformation of bivariate Normal

Prove that if (Y 1 , Y 2 ) t follows a bivariate Normal distribution

$Y 1 Y 2 ∼ N 0 0 , 1 ρ ρ 1 , then Y 1 + Y 2 Y 1 -Y 2 .$Remark: This result holds for arbitrary ρ. Condition (LF2) often holds by proper standardization, and the key is to verify Condition (LF1). Condition (LF1) is general but it looks cumbersome. In many cases, we impose a stronger moment condition that is easier to verify: (LF1') kn i=1 E∥Z ni ∥ 2+δ → 0 for some δ > 0. We can show that (LF1') implies that (LF1):

$kn i=1 E ∥Z ni ∥ 2 1 {∥Z ni ∥ > c} = kn i=1 E ∥Z ni ∥ 2+δ ∥Z ni ∥ -δ 1 ∥Z ni ∥ δ > c δ ≤ kn i=1 E∥Z ni ∥ 2+δ c -δ → 0.$Condition (LF1') is called the Lyapunov condition. A beautiful application of the Lindeberg-Feller CLT is the proof of Huber (1973)'s theorem on OLS mentioned in Chapter 11. I first review the theorem and then give a proof.

Theorem C.1 Assume Y = Xβ + ε where the covariates are fixed and error terms ε = (ε 1 , . . . , ε n ) t are IID non-Normal with mean zero and finite variance σ 2 . Recall the OLS estimator β = (X t X) -1 X t Y . Any linear combination of β is asymptotically Normal if and only if max

$1≤i≤n h ii → 0,$where h ii is the ith diagonal element of the hat matrix H = X(X t X) -1 X t .

In the main text, h ii is called the leverage score of unit i. The maximum leverage score κ = max 1≤i≤n h ii plays an important role in analyzing the properties of OLS. Theorem C.1 assumes that the errors are not Normal because the asymptotic Normality under Normal errors is a trivial result (See Chapter 5). It is slightly different from the asymptotic analysis in Chapter 6. Theorem C.1 only concerns any linear combination of the OLS estimator alone, but the results in Chapter 6 allow for the joint inference of several linear combinations of the OLS estimator. An implicit assumption of Chapter 6 is that the dimension p of the covariate matrix is fixed, but Theorem C.1 allows for a diverging p. The leverage score condition implicitly restricts the dimension and moments of the covariates. Another interesting feature of Theorem C.1 is that the statement is coordinate-free, that is, it holds up to a non-singular transformation of the covariates (See also Problems 3.4 and 3.5). The proof of sufficiency follows [Huber (1973)](#b147) closely, and the proof of necessity was suggested by Professor Peter Bickel. Proof of Theorem C.1: I first simplify the notation without essential loss of generality. By the invariance of the OLS estimator and the hat matrix in Problems 3.4 and 3.5, we can also assume X t X = I p . So β -β = (X t X) -1 X t ε = X t ε and the hat matrix H = X(X t X) -1 X t = XX t has diagonal elements h ii = x t i x i = ∥x i ∥ 2 and non-diagonal elements h ij = x t i x j . We can also assume σ 2 = 1.

Consider a fixed vector a ∈ R p and assume ∥a∥ 2 = 1. We have a t β -a t β = a t X t ε ≡ s t ε, where s = Xa ⇐⇒ s i = x t i a (i = 1, . . . , n) satisfies ∥s∥ 2 = a t X t Xa = ∥a∥ 2 = 1 and s 2 i = (x t i a) 2 ≤ ∥x i ∥ 2 ∥a∥ 2 = ∥x i ∥ 2 = h ii by the Cauchy-Schwarz inequality.

I first prove the sufficiency. The key term a t β -a t β is a linear combination of the IID errors, and it has mean 0 and variance var(s t ε) = ∥s∥ 2 = 1. We only need to verify Condition (LF1) to establish the CLT. It holds because for any fixed c > 0, we have 

$n i=1 E s 2 i ε 2 i 1 {|s i ε i | > c} ≤ n i=1 s 2 i max 1≤i≤n E ε 2 i 1 {|s i ε i | > c} (C.4) = max 1≤i≤n E ε 2 i 1 {|s i ε i | > c} (C.5) ≤ E ε 2 i 1 κ 1/2 |ε i | > c (C.$$* = x t i * β = x t i * X t ε = n j=1 x t i * x j ε j = h i * i * ε i * + j̸ =i * h i * j ε j .$If ŷi * is asymptotically Normal, then both h i * i * ε i * and j̸ =i * h i * j ε j must have Normal limiting distributions by Theorem B.6. Therefore, h i * i * must converge to zero because ε i * has a non-Normal distribution. So max 1≤i≤n h ii must converge to zero. □

## C.3 Tools for proving convergence in probability and distribution

The first tool is the continuous mapping theorem:

Proposition C.9 Let f : R K → R L be continuous except on a set O with pr(Z ∈ O) = 0. Then Z n → Z implies f (Z n ) → f (Z) in probability (and in distribution).

The second tool is Slutsky's Theorem:

Proposition C.10 Let Z n and W n be random vectors. If Z n → Z in distribution, and W n → c in probability (or in distribution) for a constant c, then

$1. Z n + W n → Z + c in distribution; 2. W n Z n → cZ in distribution; 3. W -1 n Z n → c -1 Z in distribution if c ̸ = 0.$The third tool is the delta method. I will present a special case below for asymptotically Normal random vectors. Heuristically, it states that if T n is asymptotically Normal, then any function of T n is also asymptotically Normal. This is true because any function is a locally linear function by the first-order Taylor expansion.

Proposition C.11 Let f (z) be a function from R p to R q , and ∂f (z)/∂z ∈ R p×q be the partial derivative matrix. If √ n(Z n -θ) → N(µ, Σ) in distribution, then where m(•, •) is a vector function with the same dimension as b, and W = {w i } n i=1 are the observed data. Let β denote the solution which is an estimator of β. Under mild regularity conditions, β is consistent and asymptotically Normal[foot_27](#foot_27) . This is the classical theory of M-estimation. I will review it below. See [Stefanski and Boos (2002)](#b211) for a reader-friendly introduction that contains many interesting and important examples. The proofs below are not rigorous. See [Newey and McFadden (1994)](#b184) for the rigorous ones. Proof of Theorem D.1: I give a "physics" proof. When I use approximations, I mean the error terms are of higher orders under some regularity conditions. The consistency follows from swapping the order of "solving equation" and "taking the limit based on the law of large numbers": For non-IID data, the above covariance estimator can be conservative unless E{m(w i , β)} = 0 for all i = 1, . . . , n.

$√ n{f (Z n ) -f (θ)} → N ∂f (θ) ∂z t µ, ∂f($
## D.1 M-estimation

## D.2 Maximum likelihood estimator

As an important application of Theorem D.1, we can derive the asymptotic properties of the maximum likelihood estimator (MLE) θ under IID sampling from a parametric model  [Fisher (1925b)](#b101) pioneered the asymptotic theory of the MLE under correctly specified models.

If the model is incorrect, I(θ) can be different from J(θ) but the sandwich covariance B -1 M B -1 still holds. So a covariance matrix estimator for the MLE under misspecification is I n ( θ) -1 J n ( θ)I n ( θ) -1 .

Huber (1967) studied the asymptotic theory of the MLE under correctly specified models. He focused on the case with IID observations and pioneered the sandwich covariance formula. Perhaps a more important question is what is the parameter if the model is misspecified.

The population analog of the MLE is the minimizer of

$-E{log f (y | θ)},$where the expectation is over true but unknown distribution y ∼ g(y). We can rewrite the population objective function as -g(y) log f (y | θ)dy = g(y) log g(y) f (y | θ) dy -g(y) log g(y)dy.

The first term is called the Kullback-Leibler divergence or relative entropy of g(y) and f (y | θ), whereas the second term is called the entropy of g(y). The first term depends on θ whereas the second term does not. Therefore, the targeted parameter of the MLE is the minimizer of the Kullback-Leibler divergence. By Gibbs' inequality, the Kullback-Leibler divergence is non-negative in general and is 0 if g(y) = f (y | θ). Therefore, if the model is correct, then the true θ indeed minimizes the Kullback-Leibler divergence with minimum value 0

Example D.4 Assume that y 1 , . . . , y n iid ∼ N(µ, 1). The log-likelihood contributed by unit i is log f

$(y i | µ) = - 1 2 log(2π) - 1 2 (y i -µ) 2 , so ∂ log f (y i | µ) ∂µ = y i -µ, ∂ 2 log f (y i | µ) ∂µ 2 = -1.$The MLE is μ = ȳ. If the model is correctly specified, we can use

$I n (μ) -1 = n -1 or J n (μ) -1 = 1/ n i=1 (y i -μ) 2$to estimate the variance of μ. If the model is misspecified, we can use

$I n (μ) -1 J n (μ)I n (μ) -1 = n i=1 (y i -μ) 2 /n 2$to estimate the variance of μ. The sandwich variance estimator seems the best overall. The Normal model can be totally wrong but it is still meaningful to estimate the mean parameter µ = E(y). The MLE is just the sample moment estimator which has variance var(y)/n. Since the sample variance s 2 = n i=1 (y i -μ) 2 /(n -1) is unbiased for var(y), a natural unbiased estimator for var(μ) is s 2 /n, which is close to the sandwich variance estimator.

The above discussion extends to the case with independent but non-IID data. The covariance estimators still apply by replacing each f by f i within the summation. Note that the sandwich covariance estimator is conservative in general. [White (1982)](#b238) pioneered the asymptotic analysis of the MLE with misspecified models in econometrics but made a mistake for the M term. [Chow (1984)](#b75)

$l i = - 1 2 log(2πσ 2 ) - 1 2σ 2 (y i -x t i β) 2 , (i = 1, . . . , n)$with the simplification l i = log f (y i | x i , β, σ 2 ). So the first-order derivatives are

$∂l i ∂β = 1 σ 2 x i (y i -x t i β), ∂l i ∂σ 2 = - 1 2σ 2 + 1 2(σ 2 ) 2 (y i -x t i β) 2 ;$the second derivative is

$∂ 2 l i ∂β 2 = - 1 σ 2 x i x t i , ∂ 2 l i ∂(σ 2 ) 2 = 1 2(σ 2 ) 2 - 1 (σ 2 ) 3 (y i -x t i β) 2 and ∂ 2 l i ∂β∂σ 2 = - 1 (σ 2 ) 2 x i (y i -x t i β).$The MLE of β is the OLS estimator β and the MLE of σ 2 is σ2 = n i=1 ε2 i /n, where εi = y i -x t i β is the residual. We have

$I n ( β, σ2 ) = diag 1 σ2 n i=1 x i x t i , n 2(σ 2 ) 2 ,$and

$J n ( β, σ2 ) = 1 (σ 2 ) 2 n i=1 ε2 i x i x t i * * * ,$where the * terms do not matter for the later calculations. If the Normal linear model is correctly specified, we can use the (1, 1)th block of I n ( β, σ2 ) -1 as the covariance estimator for β, which equals σ2 n i=1

x i x t i -1

.

If the Normal linear model is misspecified, we can use the (1, 1)th block of I n ( β, σ2 ) -1 J n ( β, σ2 )I n ( β, σ2 ) -1 as the covariance estimator for β, which equals Show that E( Ṽ ) = var(x) when x 1 , . . . , x n are independent with the same mean µ, and E( Ṽ ) ≥ var(x) when x i ∼ [µ i , σ 2 i ] are independent.

![Version 1: a basic linear model course assuming minimal technical preparations If you want to teach a basic linear model course without assuming strong technical preparations from the students, you can start with the appendices by reviewing basic linear algebra, probability theory, and statistical inference. Then you can cover Chapters 2-17. If time permits, you can consider covering Chapter 20 due to the importance of the logistic model for binary data.]()

![an advanced linear model course assuming strong technical preparations If you want to teach an advanced linear model course assuming strong technical preparations from the students, you can start with the main text directly. When I did this, I asked xix xx]()

![variances, and ρ = σxy /(σ x σxy ) is the sample Pearson correlation coefficient with the sample covariance σxy = (n -1) -1 n i=1]()

![FIGURE 2.1: Galton's dataset]()

![library ( " HistData " ) > xx = Galton Families $ m id pa re n tH ei gh t > yy = Galton Families $ childHeight > > center _ x = mean ( xx ) > center _ y = mean ( yy ) > sd _ x = sd ( xx ) > sd _ y = sd ( yy ) > rho _ xy = cor ( xx , yy ) > > beta _ fit = rho _ xy * sd _ y / sd _ x > alpha _ fit = center _ y -beta _ fit * center _]()

![Assumption 4.1 (Gauss-Markov model) We have Y = Xβ + ε]()

![Defineσ2 = rss/(n -p) = n i=1 ε2]()

![The above covariance decomposition simplifies tocov( β) = cov( β) + cov( ββ), which implies cov( β) -cov( β) = cov( ββ) ⪰ 0.□ In the process of the proof, we have shown two stronger results cov( ββ, β) = 0 and cov( ββ) = cov( β) -cov( β).]()

![Assumption 5.1 (Normal linear model) We haveY ∼ N(Xβ, σ 2 I n ),]()

![FIGURE 5.1: Prediction in Galton's regression]()

![Proof of Corollary 7.1: We have ε = (I -H)Y and ê = (I -H2 ) Ỹ = (I -H2 )(I -H 1 )Y.]()

![FIGURE 8.1: Correlations among three variables]()

![FIGURE 8.2: An Example of Simpson's Paradox. The two solid regression lines are fitted separately using the data from two groups, and the dash regression line is fitted using the pooled data.]()

![FIGURE 9.3: A path model]()

![R 2 and the sample Pearson correlation coefficient Prove Theorem 10.1.]()

![FIGURE 11.1: Outlier detections]()

![FIGURE 11.2: Outlier detections in the LaLonde data]()

![var(y) is the population Pearson correlation coefficient. So Corollary 12.1 gives the population version of the Galtonian formula. I leave the proofs of Theorem 12.2 and Corollary 12.1 as Problems 12.2 and 12.3.]()

![FIGURE 12.1: Best linear approximations correspond to three different distributions of x.]()

![standard error : 1 . 2 3 7 on 9 degrees of freedom Multiple R -squared : 0 . 6 6 6 2 , Adjusted R -squared : 0 . 6 2 9 2 F -statistic : 1 7 . 9 7 on 1 and 9 DF , p -value : 0 . 0 0 2 1 7 9 > ols 3 = lm ( y 3 ~x 3 , data = anscombe ) > summary ( ols 3 ) Call : lm ( formula = y 3 ~x 3 , data = anscombe ) : 1 7 . 9 7 on 1 and 9 DF , p -value : 0 . 0 0 2 1 7 6 > ols 4 = lm ( y 4 ~x 4 , data = anscombe ) > summary ( ols 4 ) Call : lm ( formula = y 4 ~x 4 , data = anscombe )]()

![FIGURE 12.3: Anscombe's Quartet: scatter plots]()

![FIGURE 12.4: Residual plots]()

![library ( " mlbench " ) data ( BostonHousing ) attach ( BostonHousing ) n = dim ( BostonHousing )[ 1 ] p = dim ( BostonHousing )[ 2 ] -1 ymin = min ( medv ) ymax = max ( medv ) grid . y = seq ( ymin -3 0 , ymax + 3 0 , 0 . 1 ) BostonHousing = BostonHousing [ order ( medv ) , ] detach ( BostonHousing ) ols . fit . full = lm ( medv ~. , data = BostonHousing , x = TRUE , y = TRUE , qr = TRUE ) beta = ols . fit . full $ coef e . sigma = summary ( ols . fit . full )$ sigma X = ols . fit . full $ x Y = ols . fit . full $ y X . QR = ols . fit . full $ qr X . Q]()

![FIGURE 12.5: Leave-one-out prediction intervals based on the Boston housing data]()

![Using press to construct an unbiased estimator for σ 2]()

![FIGURE 13.1: Freedman's simulation. The first row shows the histograms of the R 2 s, and the second row shows the histograms of the p-values in testing that all coefficients are 0. The first column corresponds to the full model without testing, and the second column corresponds to the selected model with testing.]()

![FIGURE 13.3: Training and testing errors: nonlinear mean function]()

![βridge (t) = arg min b0,b1,...,bp rss(b 0 , b 1 , . . . , b p )]()

![} based on the standardized variables, we can obtain the predicted values based on the original variables as ŷi (λ) -ȳ = βridge 1]()

![FIGURE 14.2: Bias-variance trade-off in ridge regression]()

![then the mode of the posterior distribution of β | Y equals βridge (σ 2 /τ 2 ): βridge (σ 2 /τ 2 ) = arg max β f (β | Y ) where f (β | Y ) is the posterior density of β given Y . 14.2 Derivative of the MSE Show that ∂mse(λ) ∂λ λ=0 < 0.]()

![FIGURE 15.2: Lasso with a non-sparse solution]()

![The solution inLemma 15.1 is a function of b 0 and λ, and we will use the notationS(b 0 , λ) = sign(b 0 ) (|b 0 | -λ) +from now on, where S denotes the soft-thresholding operator. For a given λ > 0, it is a function of b 0 illustrated by Figure15.4. The proof of Lemma 15.1 is to solve the optimization problem. It is tricky since we cannot naively solve the first-order condition due to the nonsmoothness of |b| at 0. Nevertheless, it is only a one-dimensional optimization problem, and I relegate the proof as Problem 15.2.]()

![training and testing data > set . seed ( 2 3 0 ) > nsample = dim ( BostonHousing )[ 1 ] > trainindex = sample ( 1 : nsample , floor ( nsample * 0 . 9 )) > > xmatrix = model . matrix ( medv ~. , data = BostonHousing )[ , -1 ] > yvector = BostonHousing $ medv > dat = data . frame ( yvector , xmatrix ) > > # # linear regression > bostonlm = lm ( yvector ~. , data = dat [ trainindex , ]) > predicterror = dat $ yvector [ -trainindex ] -+ predict ( bostonlm , dat [ -trainindex , ]) > mse . ols = sum ( predicterror ^2 )/ length ( predicterror ) > > # # ridge regression > lambdas = seq ( 0 , 5 , 0 . 0 1 ) > lm 0 = lm . ridge ( yvector ~. , data = dat [ trainindex , ] , + lambda = lambdas ) > coefridge = coef ( lm 0 )[ which . min ( lm 0 $ GCV ) , ] > p r e d i c t e r r o r r i d g e = dat $ yvector [ -trainindex ] -+ cbind ( 1 , xmatrix [ -trainindex , ])%*% coefridge > mse . ridge = sum ( p r e d i c t e r r o r r i d g e ^2 )/ length ( p r e d i c t e r r o r r i d g e ) > > # # lasso > cvboston = cv . glmnet ( x = xmatrix [ trainindex , ] , y = yvector [ trainindex ]) > coeflasso = coef ( cvboston , s = " lambda . min " ) > p r e d i c t e r r o r l a s s o = dat $ yvector [ -trainindex ] -+ cbind ( 1 , xmatrix [ -trainindex , ])%*% coeflasso > mse . lasso = sum ( p r e d i c t e r r o r l a s s o ^2 )/ length ( p r e d i c t e r r o r l a s s o ) > > c ( mse . ols , mse . ridge , mse . lasso )]()

![> # # adding more noisy covariates > n . noise = 2 0 0 > xnoise = matrix ( rnorm ( nsample * n . noise ) , nsample , n . noise ) > xmatrix = cbind ( xmatrix , xnoise ) > dat = data . frame ( yvector , xmatrix ) > > # # linear regression > bostonlm = lm ( yvector ~. , data = dat [ trainindex , ]) > predicterror = dat $ yvector [ -trainindex ] -+ predict ( bostonlm , dat [ -trainindex , ]) > mse . ols = sum ( predicterror ^2 )/ length ( predicterror ) > > # # ridge regression > lambdas = seq ( 1 0 0 , 1 5 0 , 0 . 0 1 ) > lm 0 = lm . ridge ( yvector ~. , data = dat [ trainindex , ] , + lambda = lambdas ) > coefridge = coef ( lm 0 )[ which . min ( lm 0 $ GCV ) , ] > p r e d i c t e r r o r r i d g e = dat $ yvector [ -trainindex ] -+ cbind ( 1 , xmatrix [ -trainindex , ])%*% coefridge > mse . ridge = sum ( p r e d i c t e r r o r r i d g e ^2 )/ length ( p r e d i c t e r r o r r i d g e ) > > q]()

![FIGURE 15.5: Comparing ridge and lasso]()

![FIGURE 15.6: Shrinkage estimators]()

![Figure 15.6 shows the constraints corresponding to different values of q.Zou andHastie (2005) proposed the elastic net which combines the penalties of the lasso and ridge:]()

![FIGURE 15.7: Comparing the ridge, lasso, and elastic net]()

![FIGURE 16.1: Box-Cox transformation in the jobs data]()

![FIGURE 16.3: Nonparametric regression using the basis expansion]()

![FIGURE 16.4: Generalized additive model]()

![ols . fit = lm ( read ~math * socst , data = hsbdemo ) > summary ( ols . fit ) Call : lm ( formula = read ~math * socst , data = hsbdemo ) Residuals :]()

![hsbdemo $ math . c = hsbdemo $ math -mean ( hsbdemo $ math ) > hsbdemo $ socst . c = hsbdemo $ socst -mean ( hsbdemo $ socst ) > ols . fit = lm ( read ~math . c * socst .c , data = hsbdemo ) > summary ( ols . fit ) Call : lm ( formula = read ~math . c * socst .c , data = hsbdemo )]()

![FIGURE 19.1: Fulton data]()

![FIGURE 19.2: Local linear regression]()

![FIGURE 19.3: Survey sampling]()

![(x i -xw )(y i -ȳw ) n i=1 w i (x i -xw ) αw = ȳw -βw xw where xw = n i=1 w i x i / n i=1 w i and ȳw = n i=1 w i y i / n i=1 w i are the weighted means of the covariate and outcome.]()

![FIGURE 20.1: Distributions and densities corresponding to the link functions]()

![flu = read . table ( " fludata . txt " , header = TRUE ) > flu = within ( flu , rm ( receive )) > assign . logit = glm ( outcome ~. , + family = binomial ( link = logit ) , + data = flu ) > summary ( assign . logit ) Call : glm ( formula = outcome ~. , family = binomial ( link = logit ) , data = flu )]()

![FIGURE 20.2: Comparing the fitted probabilities from different link functions]()

![20.6.1 Penalized logistic regressionSimilar to the high dimensional linear model, we can also extend the logit model to a penalized version. Since the objective function for the original logit model is the log-likelihood, we can minimize the following penalized log-likelihood function:1 -α)|β j |},]()

![dat = read . csv ( " samarani . csv " ) > pool . glm = glm ( case _ comb ~ds 1 + ds 2 + ds 3 + ds 4 _ a + + ds 4 _ b + ds 5 + ds 1 _ 3 + center , + family = binomial ( link = logit ) , + data = dat ) > summary ( pool . glm ) Call : glm ( formula = case _ comb ~ds 1 + ds 2 + ds 3 + ds 4 _ a + ds 4 _ b + ds 5 + ds 1 _ 3 + center , family = binomial ( link = logit ) , data = dat )]()

![Under(21.10) and (21.11), we have]()

![random variable y is Poisson(λ) if its probability mass function is pr(y = k) = e -λ λ k k! , (k = 0, 1, 2, . . .) which sums to 1 by the Taylor expansion formula e λ = ∞ k=0 λ k k! . The Poisson(λ) random variable has the following properties: Proposition 22.1 If y ∼ Poisson(λ), then E(y) = var(y) = λ. Proposition 22.2 If y 1 , . . . , y K are mutually independent with y]()

![FIGURE 22.1: Comparing the log probabilities of Poisson and Negative-Binomial with the same mean]()

![FIGURE 22.2: Linear, Poisson,]()

![Figure 22.4 plots the histograms of the outcomes from four weeks before and four weeks after the experiment. Eight histograms all show severe zero inflation because most workers]()

![FIGURE 22.4: Zero-inflation of the data]()

![]()

![FIGURE 22.6: Comparing AICs from five regression models]()

![Proof of Lemma 23.1: Define ℓ(y | θ) = log f (y | θ) as the log likelihood function, so e ℓ(y|θ) is the density satisfying e ℓ(y|θ) dy = f (y | θ)dy = 1 by the definition of a probability density function (we can replace the integral by summation for a probability mass function). Differentiate the above identity to obtain | θ)dy = 0, which implies Bartlett's first identity. Differentiate it twice to obtain]()

![23.6) depends on x i and β, with (b ′ ) -1 indicating the inverse function of b ′ (•).]()

![FIGURE 23.1: Quantities in a GLM]()

![rpois (n , lambda . x ) > pois . pois = glm ( y ~x , family = poisson ( link = log )) > summary ( pois . pois ) Coefficients : Estimate Std . Error z value Pr ( >| z |) ( Intercept ) -0 . 0 Negative-Binomial regression model I then generate data from a Negative-Binomial regression model. The conditional mean function is still E(y i | x i ) = e x t i β , so we can still use Poisson regression as a working model. The robust standard error doubles the classical standard error. > library ( MASS ) > theta = 0 . 2 > y = rnegbin (n , mu = lambda .x , theta = theta ) > nb . pois = glm ( y ~x , family = poisson ( link = log )) > summary ( nb . pois ) Coefficients : Estimate Std . Error z value Pr ( >| z |]()

![on EHW and LZ standard errors]()

![FIGURE 25.1: GEE analysis of the gym data]()

![FIGURE 26.1: CDF and quantile]()

![Figure26.5 shows the quantile regression lines, which are almost parallel with different intercepts. In Galton's data, x and y are very close to a bivariate Normal distribution. Theoretically, we can verify that with bivariate Normal (x, y), the conditional quantile function F -1 (τ | x) is linear in x with the same slope. See Problem 26.2.]()

![FIGURE 26.5: Galton's data]()

![FIGURE 27.1: Histograms of the time to event in the data fromLin et al. (2016)]()

![FIGURE 27.2: Left: Gamma(α, β = 2) hazard functions; Right: Log-Normal(µ, σ 2 ) hazard functions]()

![Example 27.3 (Log-Normal) The Log-Normal random variable T ∼Log-Normal(µ, σ 2 ) equals exponential of N(µ, σ 2 ). The right panel of Figure 27.2 plots the hazard functions with four different parameter combinations. Example 27.4 (Weibull) The Weibull distribution has many different parametrizations. Here I follow the R function dweibull, which has a shape parameter a > 0 and scale parameter b > 0. The Weibull(a, b) random variable T can be generated by T = bZ 1/a (27.1) which is equivalent to log T = log b + a -1 log Z,]()

![FIGURE 27.4: Data structure for the Kaplan-Meier curve]()

![FIGURE 27.5: Lin et al. (2016)'s data]()

![> cox . fit <-coxph ( Surv ( futime , relapse ) ~NALTREXONE * THERAPY + + AGE + GENDER + T 0 _ PDA + site , + data = COMBINE ) > summary ( cox . fit ) Call : coxph ( formula = Surv ( futime , relapse ) ~NALTREXONE * THERAPY + AGE + GENDER + T 0 _ PDA + site , data = COMBINE )]()

![FIGURE 27.7: Kaplan-Meier curves with 95% confidence intervals based on Gehan (1965)'s data]()

![cox . fit <-coxph ( Surv ( futime , relapse ) ~NALTREXONE * THERAPY + + AGE + GENDER + T 0 _ PDA + strata ( site ) = Surv ( futime , relapse ) ~NALTREXONE * THERAPY + AGE + GENDER + T 0 _ PDA + strata ( site ) , data = COMBINE , robust = TRUE ) n = 1 2 2 6 , number of events = 8 5 6 coef exp ( coef ) se ( coef ) robust se z Pr ( >| z |) NALTREXONE]()

![If an n × m matrix has rank k, then A = BC for some n × k matrix B and k × m matrix C.Proof of Proposition A.2: Let A j1 , . . . , A j k be the maximally linearly independent column vectors of A. Stack them into an n × k matrix B = (A j1 , . . . , A j k ). They can linearly represent all column vectors of A:A = (c 11 A j1 + • • • + c k1 A j k , . . . , c 1m A j1 + • • • + c km A j k ) = (BC 1 , . . . , BC m ) = BC,where C = (C 1 , . . . , C m ) is an k × m matrix with column vectors]()

![the determinant are more useful. I will review two.Proposition A.3 For two square matrices A and B, we havedet(AB) = det(A)det(B) = det(BA).Proposition A.4 For two square matrices A ∈ R m×m and B ∈ R n×n , we have Let I n be the n × n identity matrix. An n × n matrix A is invertible/nonsingular if there exists an n × n matrix B such that AB = BA = I n . We call B the inverse of A, denoted by A -1 . If A is an orthogonal matrix, then A t = A -1 .A square matrix is invertible if and only if det(A) ̸ = 0. The inverse of a 2 ×-1 = B -1 A -1]()

![Rayleigh quotient and eigenvalues) The maximum and minimum eigenvalues of an n × n symmetric matrix A equalsλ max (A) = max x̸ =0 r(x), λ min (A) = min x̸ =0 r(x)]()

![t and B = (b 1 , . . . , b n ) are vectors, then trace(AB) = trace(BA) = ⟨B t , A⟩ = n i=1 a i b i . Proposition A.6 The trace of an n × n symmetric matrix A equals the sum of its eigenvalues: trace(A) = n i=1 λ i . Proof of Proposition A.6: It follows from the eigen-decomposition and Proposition A.5. Let Λ = diag{λ 1 , . . . , λ n }, and we have trace(A) = trace(P ΛP t ) = trace(ΛP t P ) = trace(Λ) = n i=1 λ i .]()

![= U DV twhere U is n × m matrix with orthonormal columns (U t U = I m ), V is m × m orthogonal matrix, and D is m × m diagonal matrix. Similarly, for a wide matrix with n ≤ m, we can writeA = U DV t where U is n × n orthogonal matrix, V is m × n matrix with orthonormal columns (V t V = I n ),and D is n × n diagonal matrix. If D has only r ≤ min(m, n) nonzero elements, then we can further simplify the decomposition as A = U DV twhere U is n × r matrix with orthonormal columns (U t U = I r ), V is m × r matrix with orthonormal columns (V t V = I r ), and D is r × r diagonal matrix. With more explicit forms ofU = (U 1 , . . . , U r ), D = diag(d 1 , . . . , d r ), V = (V 1 , . . . , V r ),we can write A as A = (U 1 , . . . , U r )]()

![If f (x) is a function from R p to R, then we use the notation ∂f (x) ∂x ≡]()

![If X ∼ Gamma(α, θ), Y ∼ Gamma(β, θ) and X Y , then 1. X + Y ∼ Gamma(α + β, θ), 2. X/(X + Y ) ∼ Beta(α, β), 3. X + Y X/(X + Y ).]()

![If X ∼ Gamma(α, β), then If X ∼ Gamma(α, β), then E(log X) = ψ(α) -log β, var(log X) = ψ ′ (α). Proposition B.4 If X ∼ Beta(α, β), then E(X) = α α + β , var(X) = αβ (α + β)(α + β + 1)]()

![The difference between two IID exponential random variables follows the Laplace distribution Proposition B.8 If y 1 and y 2 are two IID Exponential(λ), then y = y 1 -y 2 has densityλ 2 exp(-λ|c|), -∞ < c < ∞which is the density of a Laplace distribution with mean 0 and variance 2/λ 2 .Proof of Proposition B.8: Both y 1 and y 2 have density f (c) = λe -λc and CDF F (c) = 1 -e -λc . The CDF of y = y 1 -y 2 at c ≥ 0 ispr(y 1 -y 2 ≤ c) = ∞ 0 pr(y 2 ≥ z -c)λe -λz dz = ∞ -λ(z-c) λe -λz dz = λe λc ∞ 0 e -2λz dz = λe λc /(2λ) = e λc /2.By symmetry, y 1 -y 2 ∼ y 2 -y 1 , so the CDF at c ≤ 0 is pr(y 1 -y 2 ≤ c) = 1 -pr(y 1 -y 2 ≤ -c)= 1 -e -λc /2.]()

![If Y 1 , . . . , Y n are IID Gumbel(µ, β), thenmax 1≤i≤n Y i ∼ Gumbel(µ + β log n, β). If Y 1 , . . . , Y n are independent Gumbel(µ i , 1), then max 1≤i≤n Y i ∼ Gumbel log n i=1e µi , 1 .I leave the proof as Problem 2.3.]()

![For a random vector X Y with X and Y possibly being vectors, if it has joint density f XY (x, y), then we can obtain the marginal distribution of X f X (x) = f XY (x, y)dy and define the conditional densityf Y |X (y | x) = f XY (x, y) f X (x) if f X (x) ̸ = 0.Based on the conditional density, we can define the conditional expectation of any function of Y asE {g(Y ) | X = x} = g(y)f Y |X (y | x)dyand the conditional variance asvar {g(Y ) | X = x} = E {g(Y )} 2 | X = x -[E {g(Y ) | X = x}]In the above definitions, the conditional mean and variance are both deterministic functions of x. We can replace x by the random variable X to define E {g(Y ) | X} and var {g(Y ) | X}, which are functions of the random variable X and are thus random variables. Below are two important laws of conditional expectation and variance. Theorem B.2 (Law of total expectation) We have E(Y ) = E {E(Y | X)} . Theorem B.3 (Law of total variance or analysis of variance) We have var(Y ) = E {var(Y | X)} + var {E(Y | X)} .]()

![If X Y , then h(X) g(Y ) for any functions h(•) and g(•).Proposition B.11 If X Y , then f XY (x, y) = f X (x)f Y (y), f Y |X (y | x) = f Y (y), E {g(Y ) | X} = E {g(Y )} , E {g(Y )h(X)} = E {g(Y )} E {h(X)} .Expectations of random vectors or random matricesFor a random matrix W = (W ij ), we define E(W ) = (E(W ij )). For constant matrices A and C, we can verify thatE(AW + C) = AE(W ) + C, E(AW C) = AE(W )C.Covariance between two random vectorsIf W ∈ R r and Y ∈ R s , then their covariance cov(W, Y ) = E {W -E(W )} {Y -E(Y )} t is an r × s matrix. As a special case, cov(Y ) = cov(Y, Y ) = E {Y -E(Y )} {Y -E(Y )} t = E(Y Y t ) -E(Y )E(Y ) t .]()

![t cov(Y )x = cov(x t Y ) = var(x t Y ) ≥ 0.Proposition B.13 For two random vectors W and Y , we havecov(AW + C, BY + D) = Acov(W, Y )B t and cov(AW + BY ) = Acov(W )A t + Bcov(Y )B t + Acov(W, Y )B t + Bcov(Y, W )A t .Similar to Theorem B.3, we have the following decomposition of the covariance. Theorem B.4 (Law of total covariance) We have cov (Y, W ) = E {cov (Y, W | X)} + cov {E(Y | X), E(W | X)} .]()

![leave the proof of Theorem B.5 as Problem 2.4.Proposition B.15 If Y ∼ N(µ,Σ), then BY + C ∼ N(Bµ + C, BΣB t ), that is, any linear transformation of a Normal random vector is also a Normal random vector.Proof of Proposition B.15: By definition, Y = µ + Σ 1/2 Z where Z is the standard Normal random vector, we haveBY + c = B(µ + Σ 1/2 Z) + C = Bµ + C + BΣ 1/2 Z ∼ N(Bµ + C, BΣ 1/2 Σ 1/2t B t )∼ N(Bµ + C, BΣB t ).]()

![If Y has mean µ and covariance Σ, thenE(Y t AY ) = trace(AΣ) + µ t Aµ.Proof of Theorem B.8: The proof relies on the following two basic facts.• E(Y Y t ) = cov(Y ) + E(Y )E(Y t ) = Σ + µµ t . • For an n × n symmetric random matrix W = (w ij ), we have E {trace(W )} = trace {E(W )} because E ( n i=1 w ii ) = n i=1 E(w ii ). The conclusion follows from E(Y t AY ) = E {trace(Y t AY )} = E {trace(AY Y t )} = trace {E(AY Y t )} = trace AE(Y Y T ) = trace {A(Σ + µµ t )} = trace AΣ + Aµµ T= trace(AΣ) + trace(µ t Aµ) = trace(AΣ) + µ t Aµ.]()

![If Y ∼ N(µ, Σ), then var(Y t AY ) = 2trace(AΣAΣ) + 4µ t AΣAµ.I relegate the proof as Problem 2.10. From its definition, χ 2 n is the summation of the squares of n IID standard Normal random variables. It is closely related to quadratic forms of multivariate Normals.Theorem B.10 1. If Y ∼ N(µ, Σ) is an n-dimensional random vector with Σ ≻ 0, then (Y -µ) t Σ -1 (Y -µ) ∼ χ 2 n . If rank(Σ) = k ≤ n, then (Y -µ) t Σ + (Y -µ) ∼ χ 2 k .]()

![ni ) → Σ. Then kn i=1 {Z ni -E(Z ni )} → N(0, Σ) in distribution.]()

![follows from the property of max, (C.5) follows from the fact ∥s∥ 2 = 1, (C.6) follows from the fact that |s i | ≤ |h ii | ≤ κ 1/2 , and (C.7) follows from κ → 0 and the dominant convergence theorem in Proposition C.5.I then prove the necessity. Pick one i * from arg max 1≤i≤n h ii . Consider a special linear combination of the OLS estimator: ŷi * = x t i * β, which is the fitted value of the i * th observation and has the form ŷi]()

![θ) ∂z t Σ ∂f (θ) ∂z in distribution.Proof of Proposition C.11: I will give an informal proof. Using Taylor expansion, we have√ n{f (Z n ) -f (θ)} ∼ = ∂f (θ) ∂z t √ n(Z n -θ), which is a linear transformation of √ n(Z n -θ). Because √ n(Z n -θ) → N(µ, Σ) in distribution, we have √ n{f (Z n ) -f (θ)} → ∂f (θ) ∂z t N(µ, Σ) = N ∂f (θ) ∂z t µ, ∂f (θ) ∂z t Σ ∂f (θ) ∂z in distribution. □ Proposition C.11 above is more useful when ∂f (θ)/∂z ̸ = 0. Otherwise, we need to invoke higher-order Taylor expansion to obtain a more accurate asymptotic approximation. D M-Estimation and MLE A wide range of statistics estimation problems can be formulated as an estimating equation: m(W, b) = n -1 n i=1 m(w i , b) = 0]()

![start with the simple case with IID data. Theorem D.1 Assume that W = {w i } n i=1 are IID with the same distribution as w. The true parameter β ∈ R p is the unique solution of E {m(w, b)} = 0, and the estimator β ∈ R p is the solution of m(W, b) = 0.Under some regularity conditions,√ n β -β → N(0, B -1 M B -t )in distribution, where B = -∂E {m(w, β)} ∂b t , M = E{m(w, β)m(w, β) t }.]()

![The asymptotic Normality follows from three steps. First, from the Taylor expansion0 = m(W, β) ∼ = m(W, β) + ∂ m(W, i , β) → N(0, M ) in distribution.Finally, Slutsky's theorem implies the result. □ The above result also holds with independent but non-IID data. Theorem D.2 Assume that {w i } n i=1 are independent observations. The true parameter β ∈ R p is the unique solution to E { m(W, b)} = 0, and the estimator β ∈ R p is the solution to m(W, b) = 0, Under some regularity conditions, √ n β -β → N(0, B -1 M B -t )]()

!['s first identity. Under regularity conditions, √ n( θ -θ) converges in distribution to Normal with mean zero and covariance B -1 M B -1 , whereB = -∂ ∂θ t E ∂ log f (y | θ) ∂θ = E -∂ 2 log f (y | θ) ∂θ∂θ tis called the Fisher information matrix, denoted by I(θ), andM = E ∂ log f (y | θ) ∂θ ∂ log f (y | θ) ∂θ tis sometimes also called the Fisher information matrix, denoted by J(θ).If the model is correct, Bartlett's second identity ensures thatI(θ) = J(θ), (D.2)and therefore √ n( θ -θ) converges in distribution to Normal with mean zero and covariance I(θ) -1 = J(θ) -1 . So a covariance matrix estimator for the MLE is I n ( θ) -1 or J n ( θ) -1 , whereI n ( θ) = -n i=1 log f (y i | θ) ∂θ∂θ t and J n ( θ) = n i=1 log f (y i | θ) ∂θ ∂ log f (y i | θ) ∂θ t .]()

![corrected his error, and Abadie et al. (2014) developed a more general theory. A leading application is the MLE under a misspecified Normal linear model. The EHW robust covariance arises naturally in this case. Example D.5 The Normal linear model has individual log-likelihood:]()

![]()

Who was Francis Galton? He was Charles Darwin's nephew and was famous for his pioneer work in statistics and for devising a method for classifying fingerprints that proved useful in forensic science. He also invented the term eugenics, a field that causes a lot of controversies nowadays.

This book uses different notions of "independence" which can be confusing sometimes. In linear algebra, a set of vectors is linearly independent if any nonzero linear combination of them is not zero; see Chapter A. In probability theory, two random variables are independent if their joint density factorizes into the product of the marginal distributions; see Chapter B.

In this book, I do not spell it as homoscedasticity since "k" better indicates the meaning of variance.[McCulloch (1985)](#b177) gave a convincing argument. See also[Paloyo (2014)](#b188).

Because X has linearly independent columns, X t X is a non-degenerate and thus positive definite matrix. Since u t C(X t X) -1 C t u ≥ 0, to show that C(X t X) -1 C t is non-degenerate, we only need to show thatu t C(X t X) -1 C t u = 0 =⇒ u = 0. From u t C(X t X) -1 C t u = 0, we know C t u = u 1 c 1 + • • • u l c l = 0.Since the rows of C are linearly independent, we must have u = 0.

The "0 +" in the above code forces the OLS to exclude the constant term.

The usual form of Simpson's paradox is in terms of a

× 2 × 2 table with all binary variables. Here we focus on the continuous version.

This also follows from Theorem A.4 since the eigenvalues of H are 0 and 1.

We have already proved a more general result on the covariance matrix of Ŷ in Theorem 4.2.

Exchangeability is a technical term in probability and statistics. Random elements z 1 , . . . , zn are exchangeable if (z π(1) , . . . , z π(n) ) have the same distribution as (z 1 , . . . , zn), where π(1), . . . , π(n) is a permutation of the integers 1, . . . , n. In other words, a set of random elements are exchangeable if their joint distribution does not change under re-ordering. IID random elements are exchangeable.

Splitting a dataset into the training and testing datasets is a standard tool to assess the out-of-sample performance of proposed methods. It is important in statistics and machine learning.

A celebrated theorem due to Weierstrass states that on a bounded interval, any continuous function can be approximated arbitrarily well by a polynomial function. Here is the mathematical statement of Weierstrass's theorem. Suppose f is a continuous function defined on the interval[a, b]. For every ε > 0, there exists a polynomial p such that for all x ∈ [a, b], we have |f (x) -p(x)| < ε.

The approximation is due to the Taylor expansion log(1+ x) = x -x 2 /2 + x 3 /3 -• • • ≈ x.

Note that this function uses a definition of bic that differs from the above definition by a constant, but this does not change the model selection result.

This is also called the Tikhonov regularization[(Tikhonov, 1943)](#b220). See[Bickel and Li (2006)](#b61) for a review of the idea of regularization in statistics.

I choose this standardization because it is the default choice in the function lm.ridge in the R package MASS. In practical data analysis, the covariates may have concrete meanings. In those cases, you may not want to scale the covariates in the way as Condition 14.1. However, the discussion below does not rely on the choice of scaling although it requires centering the covariates and outcome.

> library ( " mlbench " )

The matrix X t Σ -1 X is positive definite and thus invertible, because (1) for any α ∈ R p , Σ -1 ⪰ 0 =⇒ α t X t ΣXα ≥ 0, and (2) α t X t ΣXα = 0 ⇐⇒ Xα = 0 ⇐⇒ α = 0 since X has linearly independent columns.

[Berkson (1944)](#b58) was an early use of the logit model.

[Bliss (1934)](#b63) was an early use of the probit model.

The notation can be confusing because β denotes both the true parameter and the dummy variable for the likelihood function.

An alternative strategy is to model 1(y i = k) | x i for each k. The advantage of this strategy is that it reduces to binary logistic models. The disadvantage of this strategy is that it does not model the whole distribution of y i and can lose efficiency in estimation.

The command rnbreg in Stata uses the Berndt-Hall-Hall-Hausman (BHHH) algorithm by default, which may give slightly different numbers compared with R. The BHHH algorithm is similar to Newton's algorithm but avoids calculating the Hessian matrix.

The coefficient of variation of a random variable A equals var(A)/E(A).

Note that the difference between two independent Exponentials has the same distribution as Laplace. See Proposition B.8.

> library ( foreign ) > census 8 0 = read . dta ( " census 8 0 . dta " )

[Peto and Peto (1972)](#b190) popularized the name log-rank test.

There are counterexamples in which β is inconsistent; see[Freedman and Diaconis (1982)](#b112). The examples in this book are all regular.

