# Semantic Entropy Probes: Robust and Cheap Hallucination Detection in LLMs

## Abstract

## 

We propose semantic entropy probes (SEPs), a cheap and reliable method for uncertainty quantification in Large Language Models (LLMs). Hallucinations, which are plausible-sounding but factually incorrect and arbitrary model generations, present a major challenge to the practical adoption of LLMs. Recent work by Farquhar  et al. [21]  proposes semantic entropy (SE), which can detect hallucinations by estimating uncertainty in the space semantic meaning for a set of model generations. However, the 5-to-10-fold increase in computation cost associated with SE computation hinders practical adoption. To address this, we propose SEPs, which directly approximate SE from the hidden states of a single generation. SEPs are simple to train and do not require sampling multiple model generations at test time, reducing the overhead of semantic uncertainty quantification to almost zero. We show that SEPs retain high performance for hallucination detection and generalize better to out-of-distribution data than previous probing methods that directly predict model accuracy. Our results across models and tasks suggest that model hidden states capture SE, and our ablation studies give further insights into the token positions and model layers for which this is the case.

## Introduction

Large Language Models (LLMs) have demonstrated impressive capabilities across a wide variety of natural language processing tasks [[70,](#b69)[71,](#b70)[54,](#b53)[68,](#b67)[8]](#b7). They are increasingly deployed in realworld settings, including in high-stakes domains such as medicine, journalism, or legal services [[65,](#b64)[76,](#b75)[53,](#b52)[63]](#b62). It is therefore paramount that we can trust the outputs of LLMs. Unfortunately, LLMs have a tendency to hallucinate. Originally defined as "content that is nonsensical or unfaithful to the provided source" [[45,](#b44)[23,](#b22)[27]](#b26), the term is now used to refer to nonfactual, arbitrary content generated by LLMs. For example, when asked to generate biographies, even capable LLMs such as GPT-4 will often fabricate facts entirely [[48,](#b47)[69,](#b68)[21]](#b20). While this may be acceptable in low-stakes use cases, hallucinations can cause significant harm when factuality is critical. The reliable detection or mitigation of hallucinations is a key challenge to ensure the safe deployment of LLM-based systems.

Various approaches have been proposed to address hallucinations in LLMs (see [Section 2)](#). An effective strategy for detecting hallucinations is to sample multiple responses for a given prompt and check if the different samples convey the same meaning [[21,](#b20)[31,](#b30)[30,](#b29)[16,](#b15)[13,](#b12)[11,](#b10)[19,](#b18)[43,](#b42)[48]](#b47). The core idea is that if the model knows the answer, it will consistently provide the same answer. If the model is hallucinating, its responses may vary across generations. For example, given the prompt "What is the capital of France?", an LLM that "knows" the answer will consistently output (Paris, Paris, Paris), while an LLM that "does not know" the answer may output (Naples, Rome, Berlin), indicating a hallucination.

One explanation for why this works is that LLMs have calibrated uncertainty [[30,](#b29)[54]](#b53), i.e., "language models (mostly) know what they know" [[30]](#b29). When an LLM is certain about an answer, it consistently provides the correct response. Conversely, when uncertain, it generates arbitrary answers. This suggests that we can leverage model uncertainty to detect hallucinations. However, we cannot use tokenlevel probabilities to estimate uncertainty directly because different sequences of tokens may convey the same meaning. For the example, the answers "Paris", "It's Paris", and "The capital of France is Paris" all mean the same. To address this, Farquhar et al. [[21]](#b20) propose semantic entropy (SE), which clusters generations into sets of equivalent meaning and then estimates uncertainty in semantic space.

BioASQ TriviaQA NQ Open SQuAD 0.5 0.6 0.7 0.8 AUC (Acc) Generalization Setting, Short Generation Semantic Entropy Probe Accuracy Probe Log Likelihood Naive Entropy P(True) Semantic Entropy

Figure [1](#): Semantic entropy probes (SEPs) outperform accuracy probes for hallucination detection with Llama-2-7B, although there is a gap to 10x costlier baselines. (See Sec. 5.)

A major limitation of SE and other sampling-based approaches is that they require multiple model generations for each input query, typically between 5 and 10. This results in a 5-to-10-fold higher cost compared to naive generation without SE, presenting a major hurdle to the practical adoption of these methods. Computationally cheaper methods for reliable hallucination detection in LLMs are needed.

The hidden states of LLMs are a promising avenue to better understand, predict, and steer a wide range of LLM behaviors [[79,](#b78)[26,](#b25)[67]](#b66). In particular, a recent line of work learns to predict the truthfulness of model responses by training a simple linear probe on the hidden states of LLMs. Linear probes are computationally efficient, both to train and when used at inference. However, existing approaches are usually supervised [[61,](#b60)[35,](#b34)[4,](#b3)[44]](#b43) and therefore require a labeled training dataset assigning accuracy to statements or model generations. And while unsupervised approaches exist [[9]](#b8), their validity has been questioned [[20]](#b19). In this paper, we argue that supervising probes via SE is preferable to accuracy labels for robust prediction of truthfulness.

We propose Semantic Entropy Probes (SEPs), linear probes that capture semantic uncertainty from the hidden states of LLMs, presenting a cost-effective and reliable hallucination detection method. SEPs combine the advantages of probing and sampling-based hallucination detection. Like other probing approaches, SEPs are easy to train, cheap to deploy, and can be applied to the hidden states of a single model generation. Similar to sampling-based hallucination detection, SEPs capture the semantic uncertainty of the model. Furthermore, they address some of the shortcomings of previous approaches. Contrary to sampling-based hallucination detection, SEPs act directly on a single model hidden state and do not require generating multiple samples at test time. And unlike previous probing methods, SEPs are trained to predict semantic entropy [[31]](#b30) rather than model accuracy, which can be computed without access to ground truth accuracy labels that can be expensive to curate.

We find that SEP predictions are effective proxies for truthfulness. In fact, SEPs generalize better to new tasks than probes trained directly to predict accuracy, setting a new state-of-the-art for cost-efficient hallucination detection, cf. Fig. [1](#). Our results additionally provides insights into the inner workings of LLMs, strongly suggesting that model hidden states directly capture the model's uncertainty over semantic meanings. Through ablation studies, we show that this holds across a variety of models, tasks, layers, and token positions.

In summary, our core contributions are:

• We propose Semantic Entropy Probes (SEPs), linear probes trained on the hidden states of LLMs to capture semantic entropy (Section 4). • We demonstrate that semantic entropy is encoded in the hidden states of a single model generation and can be successfully extracted using probes (Section 6). • We perform ablation studies to study SEP performance across models, tasks, layers, and token positions. Our results strongly suggest internal model states across layers and tokens implicitly capture semantic uncertainty, even before generating any tokens. (Section 6) • We show that SEPs can be used to predict hallucinations and that they generalize better than probes directly trained for accuracy as suggested by previous work, establishing a new state-of-the-art for cost-efficient hallucination detection (Section 7, Fig. [1](#)).

## Related Work

LLM Hallucinations. We refer to Rawte et al. [[60]](#b59), Zhang et al. [[78]](#b77) for extensive surveys on hallucinations in LLMs and here briefly review the most relevant related work to this paper. Early work on hallucinations in language models typically refers to issues in summarization tasks where models "hallucinate" content that is not faithful to the provided source text [[45,](#b44)[14,](#b13)[17,](#b16)[10,](#b9)[75,](#b74)[42,](#b41)[51]](#b50). Around the same time, research emerged that showed LLMs themselves could store and retrieve factual knowledge [[58]](#b57), leading to the currently popular closed-book setting, where LLMs are queried without any additional context [[62]](#b61). Since then, a large variety of work has focused on detecting hallucinations in LLMs for general natural language generation tasks. These can typically be classified into one of two directions: sampling-based and retrieval-based approaches.

Sampling-Based Hallucination Detection. For sampling-based approaches, a variety of methods have been proposed that sample multiple model completions for a given query and then quantify the semantic difference between the model generations [[31,](#b30)[30,](#b29)[16,](#b15)[13,](#b12)[11,](#b10)[19]](#b18). For this paper, Farquhar et al. [[21]](#b20) is particularly relevant, as we use their semantic entropy measure to supervise our hidden state probes (we summarize their method in Section 3). A different line of work does not directly re-sample answers for the same query, but instead asks follow-up questions to uncover inconsistencies in the original answer [[15,](#b14)[2]](#b1). Recent work has also proposed to detect hallucinations in scenarios where models generate entire paragraphs of text by decomposing the paragraph into individual facts or sentences, and then validating the uncertainty of those individual facts separately [[39,](#b38)[49,](#b48)[43,](#b42)[15]](#b14).

Retrieval-Based Methods. A different strategy to mitigate hallucinations is to rely on external knowledge bases, e.g. web search, to verify the factuality of model responses [[22,](#b21)[77,](#b76)[57,](#b56)[18,](#b17)[24,](#b23)[36,](#b35)[74,](#b73)[66]](#b65). An advantage of such approaches is that they do not rely on good model uncertainties and can be used directly to fix errors in model generations. However, retrieval-based approaches can add significant cost and latency. Further, they may be less effective for domains such as reasoning, where LLMs are also prone to produce unfaithful and misleading generations [[73,](#b72)[33]](#b32). Thus, retrieval-and uncertainty-based methods are orthogonal and can be combined for maximum effect.

Sampling and Finetuning Strategies. A number of different strategies exist to reduce, rather than detect, the number of hallucinations that LLMs generate. Previous work has proposed simple adaptations to LLM sampling schemes [[34,](#b33)[12,](#b11)[64]](#b63), preference optimization targeting factuality [[69]](#b68), or finetuning to align "verbal" uncertainties of LLMs with model accuracy [[47,](#b46)[37,](#b36)[5]](#b4).

Understanding Hidden States. Recent work suggests that simple operations on LLM hidden states can qualitatively change model behavior [[79,](#b78)[67,](#b66)[61]](#b60) manipulate knowledge [[26]](#b25), or reveal deceitful intent [[40]](#b39). Probes can be a valuable tool to better understand the internal representations of neural networks like LLMs [[3,](#b2)[6]](#b5). Previous work has shown that hidden state probes can predict LLM outputs one or multiple tokens ahead with high accuracy [[7,](#b6)[55]](#b54). Relevant to our paper is recent work that suggests there is a "truthfulness" direction in latent space that predicts correctness of statements and generations [[44,](#b43)[4,](#b3)[9,](#b8)[35,](#b34)[4]](#b3). Our work extends this -we are also interested in predicting if the model is hallucinating nonfactual responses, however, rather than directly supervising probes with accuracy labels, we argue that capturing semantic entropy is key for generalization performance.

## Semantic Entropy

Measuring uncertainty in free-form natural language generation tasks is challenging. The uncertainties over tokens output by the language model can be misleading because they conflate semantic uncertainty, uncertainty over the meaning of the generation, with lexical and syntactic uncertainty, uncertainty over how to phrase the answer (see the example in Section 1). To address this, Farquhar et al. [[21]](#b20) propose semantic entropy, which aggregates token-level uncertainties across clusters of semantic equivalence. [2](#foot_0) Semantic entropy is important in the context of this paper because we use it as the supervisory signal to train our hidden state SEP probes.

Semantic entropy is calculated in three steps: (1) for a given query x, sample model completions from the LLM, (2) aggregate the generations into clusters (C 1 , . . . , C K ) of equivalent semantic meaning, (3) calculate semantic entropy, H SE , by aggregating uncertainties within each cluster.

Step ( [1](#formula_0)) is trivial, and we detail steps ( [2](#formula_1)) and (3) below.

Semantic Clustering. To determine if two generations convey the same meaning, Farquhar et al. [[21]](#b20) use natural language inference (NLI) models, such as DeBERTa [[25]](#b24), to predict entailment between the generations. Concretely, two generations s a and s b are identical in meaning if s a entails s b and s b entails s a , i.e. they entail each other bi-directionally. Farquhar et al. [[21]](#b20) then propose a greedy algorithm to cluster generations semantically: for each sample s a , we either add it to an existing cluster C k if bi-directional entailment holds between s a and a sample s b ∈ C k , or add it to a new cluster if the semantic meaning of s a is distinct from all existing clusters. After processing all generations, we obtain a clustering of the generations by semantic meaning.

Semantic Entropy. Given an input context x, the joint probability of a generation s consisting of tokens (t 1 , . . . , t n ) is given by the product of conditional token probabilities in the sequence,

$p(s | x) = n i=1 p(t i | t 1:i-1 , x).(1)$The probability of the semantic cluster C is then the aggregate probability of all possible generations s which belong to that cluster,

$p(C | x) = s∈C p(s | x).(2)$The uncertainty associated with the distribution over semantic clusters is the semantic entropy,

$H[C | x] = E p(C|x) [-log p(C | x)].(3)$Estimating SE in Practice. In practice, we cannot compute the above exactly. The expectations with respect to p(s|x) and p(C|x) are intractable, as the number of possible token sequences grows exponentially with sequence length. Instead, Farquhar et al. [[21]](#b20) sample N generations (s 1 , . . . , s N ) at non-zero temperature from the LLM (typically and also in this paper N = 10). They then treat (C 1 , . . . , C K ) as Monte Carlo samples from the true distribution over semantic clusters p(C|x), and approximate semantic entropy as

$H[C | x] ≈ - 1 K K k=1 log p(C k |x).(4)$We here use an additional approximation, employing the discrete variant of SE that yields good performance without access to token probabilities, making it compatible with black-box models [[21]](#b20).

For the discrete SE variant, we estimate cluster probabilities p(C|x) as the fraction of generations in that cluster, p(  [4](#formula_3)) for generations of different lengths [[41,](#b40)[50,](#b49)[31,](#b30)[21]](#b20).

$C k |x) = N j=1 1[s j ∈ C k ]/K,$
## Semantic Entropy Probes

Although semantic entropy is effective at detecting hallucinations, its high computational cost may limit its use to only the most critical scenarios. In this section, we propose Semantic Entropy Probes (SEPs), a novel method for cost-efficient and reliable uncertainty quantification in LLMs. SEPs are linear probes trained on the hidden states of LLMs to capture semantic entropy [[31]](#b30). However, unlike semantic entropy and other sampling-based approaches, SEPs act on the hidden states of a single model generation and do not require sampling multiple responses from the model at test time. Thus, SEPs solve a key practical issue of semantic uncertainty quantification by almost completely eliminating the computational overhead of semantic uncertainty estimation at test time. We further argue that SEPs are advantageous to probes trained to directly predict model accuracy. Our intuition for this is that semantic entropy is an inherent property of the model that should be encoded in the hidden states and thus should be easier to extract than truthfulness, which relies on potentially noisy external information. We discuss this further in Section 8.

Training SEPs. SEPs are constructed as linear logistic regression models, trained on the hidden states of LLMs to predict semantic entropy. We create a dataset of (h l p (x), H SE (x)) pairs, where x is an input query, h l p (x) ∈ R d is the model hidden state at token position p and layer l, d is the hidden state dimension, and H SE (x) ∈ R is the semantic entropy. That is, given an input query x, we first generate a high-likelihood model response via greedy sampling and store the hidden state at a particular layer and token position, h l p (x). We then sample N = 10 responses from the model at high temperature (T = 1) and compute semantic entropy, H SE (x), as detailed in the previous section.

For inputs, we rely on questions from popular QA datasets (see Section 5 for details), although we do not need the ground-truth labels provided by these datasets and could alternatively compute semantic entropy for any unlabeled set of suitable LLM inputs.

Binarization. Semantic entropy scores are real numbers. However, for the purposes of this paper, we convert them into binary labels, indicating whether semantic entropy is high or low, and then train a logistic regression classifier to predict these labels. Our motivation for doing so is two-fold. For one, we ultimately want to use our probes for predicting binary model correctness, so we eventually need to construct a binary classifier regardless. Additionally, we would like to compare the performance of SEP probes and accuracy probes. This is easier if both probes target binary classification problems. We note that the logistic regression classifier returns probabilities, such that we can always recover fine-grained signals even after transforming the problem into binary classification.

More formally, we compute

$HSE (x) = 1[H SE (x) > γ ⋆ ],$where γ ⋆ is a threshold that optimally partitions the raw SE scores into high and low values according to the following objective:

$γ ⋆ = arg min γ j∈SElow (H SE (x j ) -Ĥlow ) 2 + j∈SEhigh (H SE (x j ) -Ĥhigh ) 2 ,(5)$where

$SE low = {j : H SE (x j ) < γ}, SE high = {j : H SE (x j ) ≥ γ}, Ĥlow = 1 |SE low | j∈SElow H SE (x j ), Ĥhigh = 1 |SE high | j∈SEhigh H SE (x j ).$This procedure is inspired by splitting objectives used in regression trees [[38]](#b37) and we have found it to perform well in practice compared to alternatives such as soft labelling, cf. Appendix B.

In summary, given a input dataset of queries, {x j } Q j=1 , we compute a training set of hidden statebinarized semantic entropy pairs, {(h l p (x j ), HSE (x j ))} Q j=1 , and use this to train a linear classifier, which is our semantic entropy probe (SEP). At test time, SEPs predict the probability that a model generation for a given input query x has high semantic entropy.

Probing Locations. We collect hidden states, h l p (x), across all layers, l, of the LLM to investigate which layers best capture semantic entropy. We consider two different token positions, p. Firstly, we consider the hidden state at the last token of the input x, i.e. the token before generating (TBG) the model response. Secondly, we consider the last token of the model response, which is the token before the end-of-sequence token, i.e. the second last token (SLT). We refer to these scenarios as TBG and SLT. The TBG experiments allow us to study to what extent LLM hidden states capture semantic entropy before generating a response. The TBG setup potentially allows us to quantify the semantic uncertainty given an input in a single forward pass -without generating any novel tokensfurther reducing the cost of our approach over sampling-based alternatives. In practice, this may be useful to quickly determine if a model will answer a particular input query with high certainty.

## Experiment Setup

We investigate and evaluate Semantic Entropy Probes (SEPs) across a range of models and datasets. First, we show that we can accurately predict semantic entropy from the hidden states of LLMs (Section 6). We then explore how SEP predictions vary across different tasks, models, tokens indices, and layers. Second, we demonstrate that SEPs are a cheap and reliable method for hallucination detection (Section 7), which generalizes better to novel tasks than accuracy probes, although they cannot match the performance of much more expensive sampling-based methods in our experiments.

Tasks. We evaluate SEPs on four datasets: TriviaQA [[29]](#b28), SQuAD [[59]](#b58), BioASQ [[72]](#b71), and NQ Open [[32]](#b31). We use the input queries of these tasks to derive training sets for SEPs and evaluate the performance of each method on the validation/test sets, creating splits if needed. We consider a short-and a long-form setting: Short-form answers are generated by few-shot prompting the LLM to answer "as briefly as possible" and long-form answers are generated by prompting for a "single brief but complete sentence", leading to an approximately six-fold increase in the number of generated tokens [[21]](#b20). Following Farquhar et al. [[21]](#b20), we assess model accuracy via the SQuAD F1 score for short-form generations, and we use use GPT-4 [[54]](#b53) to compare model answers to ground truth labels for long-form answers. We provide prompt templates in Appendix B.1.

Models. We evaluate SEPs on five different models. For short generations, we generate hidden states and answers with Llama-2 7B and 70B [[71]](#b70), Mistral 7B [[28]](#b27), and Phi-3 Mini [[1]](#b0), and use DeBERTa-Large [[25]](#b24) as the entailment model for calculating semantic entropy [[31]](#b30). For long generations, we use Llama-2-70B [[71]](#b70) or Llama-3-70B [[46]](#b45) and use GPT-3.5 [[8]](#b7) to predict entailment.

Baselines. We compare SEPs against the ground truth semantic entropy, accuracy probes supervised with model correctness labels, naive entropy, log likelihood, and the p(True) method of Kadavath et al. [[30]](#b29). For naive entropy, following Farquhar et al. [[21]](#b20), we compute the length-normalized average log token probabilities across the same number of generations as for SE. For log likelihood, we use the length-normalized log likelihood of a single model generation. The p(True) method works by constructing a custom few-shot prompt that contains a number of examples -each consisting of a training set input, a corresponding low-temperature model answer, high-temperature model samples, and a model correctness score. Essentially, p(True) treats sampling-based truthfulness detection as an in-context learning task, where the few-shot prompt teaches the model that model answers with high semantic variety are likely incorrect. We refer to Kadavath et al. [[30]](#b29) for more details.

Linear Probe. For both SEPs and our accuracy probe baseline, we use the logistic regression model from scikit-learn [[56]](#b55) with default hyperparameters for L 2 regularization and the LBFGS optimizer.

Evaluation. We evaluate SEPs both in terms of their ability to capture semantic entropy as well as their ability to predict model hallucinations. In both cases, we compute the area under the receiver operating characteristic curve (AUROC), with gold labels given by binarized SE or model accuracy.

## LLM Hidden States Implicitly Capture Semantic Entropy

This section investigates whether LLM hidden states encode semantic entropy. We study SEPs across different tasks, models, and layers, and compare them to accuracy probes in-and out-of-distribution.

Hidden States Capture Semantic Entropy. Figure [2](#fig_1) shows that SEPs are consistently able to capture semantic entropy across different models and tasks. Here, probes are trained on hidden states of the second-last-token for the short-form generation setting. In general, we observe that AUROC values increase for later layers in the model, reaching values between 0.7 and 0.95 depending on the scenario.

Semantic Entropy Can Be Predicted Before Generating. Next, we investigate if semantic entropy can be predicted before even generating the output. Similar to before, Fig. [3](#fig_2) shows AUROC values for predicting binarized semantic entropy from the SEP probes. Perhaps surprisingly (although in line with related work, cf. Section 2), we find that SEPs can capture semantic entropy even before generation. SEPs consistently achieve good AUROC values, with performance slightly below the SLT experiments in Fig. [2](#fig_1). The TBG variant provides even larger cost savings than SEPs already do, as it allows us to quantify uncertainty before generating any novel tokens, i.e. with a single forward pass through the model. This could be useful in practice, for example, to refrain from answering queries for which semantic uncertainty is high.   AUROC values for Llama-2-7B on BioASQ, in both Fig. [2](#fig_1) and Fig. [3](#fig_2), reach very high values, even for early layers. We investigated this and believe it is likely related to the particularities of BioASQ. Concretely, it is the only of our tasks to contain a significant number of yes-no questions, which are generally associated with lower semantic entropy as the possible number of semantic meanings in outcome space is limited. For a model with relatively low accuracy such as Llama-2-7B, simply identifying whether or not the given input is a yes-no question, will lead to high AUROC values.

SEPs Capture Semantic Uncertainty for Long Generations. While experiments with short generations are popular even in the recent literature [[31,](#b30)[30,](#b29)[16,](#b15)[13,](#b12)[11]](#b10), this scenario is increasingly disconnected from popular use cases of LLMs as conversational chatbots. In recognition of this, we also study our probes in a long-form generation setting, which increases the average length of model responses from ~15 characters in the short-length scenario to about ~100 characters.

Figure [4](#fig_3) shows that, even in the long-form setting, SEPs are able to capture semantic entropy well in both the second-last-token and token-before-generation scenarios for Llama-2-70B and Llama-3-70B. Compared to the short-form generation scenario, we now observe more often that AUROC values peak for intermediate layers. This makes sense as hidden states closer to the final layer will likely be preoccupied with predicting the next token. In the long-form setting, the next token is more often unrelated to the semantic uncertainty of the overall answer, and instead concerned with syntax or lexis. Counterfactual Context Addition Experiment. To confirm that SEPs capture SE rather than relying on spurious correlations, we perform a counterfactual intervention experiment for Llama-2-7B on TriviaQA. For each input question of TriviaQA, the dataset contains a "context", from which the ground truth answer can easily be predicted. We usually exclude this context, because including it makes the task too easy. However, for the purpose of this experiment, we add the context and study how this affects SEP predictions.

Figure [5](#fig_4) shows a kernel density estimate of the distribution over the predicted probability for high semantic entropy, p(high SE), for 

## NQ Open SQuAD

Semantic Entropy Probe Accuracy Probe

Figure [6](#): SEPs predict model hallucinations better than accuracy probes when generalizing to unseen tasks. In-distribution, accuracy probes perform better. Short generation setting with Llama-2-7B, SEPs trained on the second-last-token (SLT). For the generalization setting, probes are trained on all tasks except the one that we evaluate on.

Llama-2-7B on the TriviaQA dataset with context (blue) and without context (orange) in the short generation setting using the SLT. Without context, the distribution for p(high SE) from the SEP is concentrated around 0.9. However, as soon as we provide the context, p(high SE) decreases, as shown by the shift in distribution. As the task becomes much easier -accuracy increases from 26% to 78% -the model becomes more certain -ground truth SE decreases from 1.84 to 0.50. This indicates SEPs accurately capture model behavior for the context addition experiment, with predictions for p(high SE) following ground truth SE behavior, despite never being trained on inputs with context.

## SEPs Are Cheap and Reliable Hallucination Detectors

In this section, we explore the use of SEPs to predict hallucinations, comparing them to accuracy probes and other baselines. Crucially, we also evaluate probes in a challenging generalization setting, testing them on tasks that they were not trained for. This setup is much more realistic than evaluating probes in-distribution, as, for most deployment scenarios, inputs will rarely match the training distribution exactly. The generalization setting does not affect semantic entropy, naive entropy, or log likelihood, which do not rely on training data. While p(True) does rely on a few samples for prompt construction, we find its performance is usually unaffected by the task origin of the prompt data.

Tab. 1: ∆AUROC (x100) of SEPs and acc. probes over tasks in-distribution. Avg ± std error, (S)hortand (L)ong-form gens.

Model SEP -Acc Pr.

$Mistral-7B (S) 2.8 ± 1.4 Phi-3-3.8B (S) 2.1 ± 0.8 Llama-2-7B (S) -0.5 ± 2.6 Llama-2-70B (S) 1.3 ± 0.7 Llama-2-70B (L) -1.9 ± 7.5 Llama-3-70B (L) -2.0 ± 2.1$Figure [6](#) shows both in-distribution and generalization performance of SEPs and accuracy probes across different layers for Llama-2-7B in a short-form generation setting trained on the SLT. In-distribution, accuracy probes outperform SEPs across most layers and tasks, with the exception of NQ Open. In Table [1](#), we compare the average difference in AUROC between SEPs and accuracy probes for predicting model hallucinations, taking a representative set of high-performing layers for both probe types (see Appendix B). We find that SEPs and accuracy probes perform similarly on in-distribution data across models. We report unaggregated results in Fig. A.8. The performance of SEPs here is commendable: SEPs are trained without any ground truth answers or accuracy labels, and yet, can capture truthfulness. To the best of our knowledge, SEPs may be the best unsupervised method for hallucination detection even in-distribution, given problems of other unsupervised methods for truthfulness prediction [[20]](#b19).

However, when evaluating probe generalization to new tasks, SEPs show their true strength. We evaluate probes in a leave-one-out fashion -evaluating on all datasets except one, which we train on. As shown in Fig. [6](#) (right), SEPs consistently outperform accuracy probes across various layers and tasks for short-form generations in the generalization setting. For BioASQ, the difference is particularly large. SEPs clearly generalize better to unseen tasks than accuracy probes. In Table [2](#) and Fig. [7](#), we report results for more models, taking a representative set of high-performing layers for both probe types, and Fig. A.7 shows results for Mistral-7B across layers. We again find that SEPs

$B i o A S Q T r i v i a Q A N Q O p e n S Q u A D 0.5 0.6 0.7 0.8 0.9 AUC (Acc) Llama-2-7B B i o A S Q T r i v i a Q A N Q O p e n S Q u A D Mistral-7B B i o A S Q T r i v i a Q A N Q O p e n S Q u A D Phi-3-3.8B B i o A S Q T r i v i a Q A N Q O p e n S Q u A D Llama-2-70B$Semantic Entropy Probe Accuracy Probe Log Likelihood Naive Entropy P(True) Semantic Entropy Semantic Entropy Probe Accuracy Probe Log Likelihood Naive Entropy P(True) Semantic Entropy Semantic Entropy Probe Accuracy Probe Log Likelihood Naive Entropy P(True) Semantic Entropy Semantic Entropy Probe Accuracy Probe Log Likelihood Naive Entropy P(True) Semantic Entropy Figure 7: SEPs generalize better to new tasks than accuracy probes across models and tasks. They approach, but do not match, the performance of other, 10x costlier baselines (hatched bars). Short generation setting, SLT, performance for a selection of representative layers. generalize better than accuracy probes to novel tasks. We additionally compare to the sampling-based semantic entropy, naive entropy, and p(True) methods. While SEPs cannot match the performance of Tab. 2: ∆AUROC (x100) of SEPs over acc. probes for task generalization. Avg ± std error, (S)hortand (L)ong-form gens. Model SEP -Acc Pr.

Mistral-7B (S) 10.5 ± 3.5 Phi-3-3.8B (S)

9.9 ± 2.9 Llama-2-7B (S) 7.7 ± 1.3 Llama-2-70B (S)

7.9 ± 3.0 Llama-2-70B (L)

2.2 ± 0.4 Llama-3-70B (L)

6.2 ± 1.9

these methods, it is important to note the significantly higher cost these baselines incur, requiring 10 additional model generations, whereas SEPs and accuracy probes operate on single generations.

We further evaluate SEPs for long-form generations. As shown in Fig. [8](#), SEPs outperform accuracy probes for Llama-2-70B and Llama-3-70B in the generalization setting. We also provide in-distribution results for long generations with both models in Figs. A.9 and A.10. Both results confirm the trend discussed above. Overall, our results clearly suggest that SEPs are the best choice for cost-effective uncertainty quantification in LLMs, especially if the distribution of the query data is unknown.

8 Discussion, Future Work, and Conclusions.

Discussion. Our experiments show that SEPs generalize better than accuracy probes -in terms of detecting hallucinations -to inputs from unseen tasks. One potential explanation for this is that semantic uncertainty is a better probing target than truthfulness, because semantic uncertainty is a more model-internal characteristic that can be better predicted from model hidden states. Model correctness labels required for accuracy probing on the other hand are external and can be noisy, which may make them more difficult to predict from hidden states. We can find evidence for this by comparing the in-distribution AUROC values for SEPs (for predicting binarized SE) with the AUROC

BioASQ TriviaQA NQ Open SQuAD 0.6 0.7 0.8 AUC (Acc) Llama-2-70B Generalization BioASQ TriviaQA NQ Open SQuAD Llama-3-70B Generalization Semantic Entropy Probe Accuracy Probe Log Likelihood Naive Entropy P(True) Semantic Entropy Figure 8: Semantic entropy probes outperform accuracy probes for hallucination detection in the long-form generation generalization setting with both Llama-2-70B and Llama-3-70B. of the accuracy probes for predicting accuracy in Figs. A.5 and A.6. AUROC values -which are insensitive to class frequencies and a good proxy for task difficulty -for predicting SE are significantly higher, indicating that semantic entropy is indeed better captured by model hidden states than accuracy.

Another possible explanation for the gap in OOD generalization could be that accuracy probes capture model correctness in a way that is specific to the training dataset. For example, the probe may latch on to discriminative features for model correctness that relate to the task at hand but do not generalize, such as identifying a knowledge domain where accuracy is high or low, but which rarely occurs outside the training data. Conversely, semantic probes may capture more inherent model statese.g., uncertainty from failure to gather relevant facts or attributes for the query. The literature on mechanistic interpretability [[52]](#b51) supports the idea that such information is likely contained in model hidden states. We believe that concretizing these links is a fruitful area for future research.

Future Work. We believe it should be possible to further close the performance gap between sampling-based approaches, such as semantic entropy, and SEPs. One avenue to achieve this could be to increase the scale of the training datasets used to train SEPs. In this work, we relied on established QA tasks to train SEPs to allow for easy comparison to accuracy probes. However, future work could explore training SEPs on unlabelled data, such as inputs generated from another LLM or natural language texts used for general model training or finetuning. This could massively increase in the amount of training data for SEPs, which should improve probe accuracy, and also allow us to explore other more complex probing techniques that require more training data.

Conclusions. We have introduced semantic entropy probes (SEPs): linear probes trained on the hidden states of LLMs to predict semantic entropy, an effective measure of uncertainty for freeform LLM generations [[21]](#b20). We find that the hidden states of LLMs implicitly capture semantic entropy across a wide range of scenarios. SEPs are able to predict semantic entropy consistently, and, importantly, they detect model hallucinations more effectively than probes trained directly for accuracy prediction when testing on novel inputs from a different distribution than the training setdespite not requiring any ground truth model correctness labels. Semantic uncertainty probing, both in terms of model interpretability and practical applications, is an exciting avenue for further research.

1 5 9 13 17 21 25 29 33 Layer 0.50 0.55 0.60 0.65 0.70 0.75 AUC (SE) Hidden State Residual Stream MLP Output Figure A.11: Probing different model components for SEPs. The hidden states are more predictive than residual streams and MLP outputs. TriviaQA, Llama-2-7B, in-distribution, short-form generations, SLT. 1 17 33 49 65 81 Layer 0.5 0.6 0.7 0.8 0.9 AUC BioASQ TriviaQA NQ Open SQuAD Best Split Even Split Figure A.12: Comparing binarization methods for semantic entropy. Our "best split" procedure slightly outperforms the "even split" strategy, although SEPs do not appear overly sensitive to the binarization procedure. Long-form generations for Llama-2-70B, SLT, in-distribution. 0.0 0.5 1.0 1.5 2.0 2.5

Semantic Entropy Binarization Threshold 

## B Experiment Details

Here we provide additional details to reproduce the experiments of the main paper.

## B.1 Prompt Templates

We use the following prompt templates across experiments.

For long-form generations, we use the following prompt template:

Answer the following question in a single brief but complete sentence. Question: [query question] Answer:

For short-form generations, we adjust the instruction and additionally provide 5 demonstration examples with short ground truth answers, to elicit a short answer from the model:

Answer the following question as briefly as possible. 

## B.2 Semantic Entropy Calculation

We compute semantic entropy with N = 10 generations sampled at temperature T = 1.0 and using default values of top-p (p = 0.9) and top-K (K = 50).

For short-form generations, we predict entailment using DeBERTa-Large [[25]](#b24) and assess model accuracy via the SQuAD F1 score.

For long-form generations, we predict entailment with GPT-3.5 [[8]](#b7) and the following prompt: To assess the correctness of long-form generations, we prompt GPT-4 [[54]](#b53) or GPT-4o [3](#foot_1)

## B.5 Evaluation

To evaluate the performance of the probes in the generalization setting, we employ the following leave-one-out procedure for the aggregate results reported in the barplots and tables.

First, each probe is trained on a single dataset. Then, the trained probes are evaluated on all other datasets in terms of AUROC of detecting hallucinations, excluding the dataset used for training. We then report the mean across all probes evaluated on that specific dataset. This allows us to assess the generalization capability of the probes by measuring their performance on datasets that were not used during the training phase. This scenario is important in practice, as the distribution of the query data will rarely be known.

## C Compute Resources

We make use of an internal cluster with 24 Nvidia A100 80GB GPUs. We further use GPT 3.5, 4, and 4o via the OpenAI API.

For experiments requiring the use of Llama 70B models, we require 2 A100s to do inference and calculate the hidden states. The smaller models require only a slice of an A100 80GB. However, once the training data for the semantic entropy probes has been created, a CPU-only computing resource is sufficient to fit the logistic regression models.

Based on tracked finished runs, we estimate ~300 GPU-hours plus ~310 CPU-hours to obtain the results in the paper.

![and then compute semantic entropy as the entropy of the resulting categorical distribution, H SE (x) := -K k=1 p(C k |x) log p(C k |x). Discrete SE further avoids problems when estimating Eq. (]()

![Figure2: Semantic Entropy Probes (SEPs) achieve high fidelity for predicting semantic entropy. Across datasets and models, SEPs are consistently able to capture semantic entropy from hidden states of mid-to-late layers. Short generation scenario with probes trained on second-last token (SLT).]()

![Figure3: Semantic entropy can be predicted from the hidden states of the last input token, without generating any novel tokens. Short generations with SEPs trained on the token before generating (TBG).]()

![Figure 4: SEPs successfully capture semantic entropy in Llama-2-70B and Llama-3-70B for long generations across layers and for both SLT and TBG token positions.]()

![Fig. 5: SEPs capture drop in SE due to added context.]()

![Figure A.13: MSE of the best-split objective Eq. (5) for different binarization thresholds γ for models in either short-form generation or (L)ong-form generation settings (SLT).]()

![Question: [example question 1] Answer: [example answer 1] ... Question: [example question 5] Answer: [example answer 5] Question: [query question] Answer: Finally, for the counterfactual context addition experiment, we prepend the context, prior to the question: Context: [query context] Question: [query question] Answer:]()

![Here are two possible answers: Possible Answer 1: [model generation a] Possible Answer 2: [model generation b] Does Possible Answer 1 semantically entail Possible Answer 2? Respond with entailment, contradiction, or neutral.]()

![as followsWe are assessing the quality of answers to the following question: [query question] The expected answer is: [ground truth label]. The proposed answer is: [model generation]. Within the context of the question, does the proposed answer mean the same as the expected answer? Respond only with yes or no. Response: A) True B) False The possible answer is: A For p(True), we obtain the probability of model truthfulness by measuring the token probability of A at the end of the prompt.]()

Farquhar et al.[[21]](#b20) is a journal version of the original semantic entropy paper by Kuhn et al.[[31]](#b30).

We use GPT-4 to evaluate Llama-2-70B but switched to GPT-4o for our more recent experiments on Llama-3-70B given the difference in cost between the two GPT models.

