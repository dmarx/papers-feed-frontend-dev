# Modular Duality in Deep Learning

## Abstract

## 

An old idea in optimization theory says that since the gradient is a dual vector it may not be subtracted from the weights without first being mapped to the primal space where the weights reside. We take this idea seriously in this paper and construct such a duality map for general neural networks. Our map, which we call modular dualization, forms a unifying theoretical basis for training algorithms that are a) fast and b) scalable. Modular dualization involves first assigning operator norms to layers based on the semantics of each layer, and then using these layerwise norms to recursively induce a duality map on the weight space of the full neural architecture. We conclude by deriving GPU-friendly algorithms for dualizing Embed, Linear and Conv2D layers-the latter two methods are based on a rectangular Newton-Schulz iteration [(Kovarik, 1970;](#b24)[Björck & Bowie, 1971)](#b4). A variant of our methods was used to set speed records for training NanoGPT. Overall, we hope that our theory of modular duality will yield a next generation of fast and scalable optimizers for general neural architectures.

## Introduction

In this paper, we pursue a rigorous and first-principles theoretical framework for designing neural network training algorithms. We hope that building such a framework will facilitate the design of a next generation of fast and scalable optimizers that are automatically tailored to different neural architectures.

While gradient descent is the workhorse of modern machine learning, the most vanilla form of the algorithm does not, in our view, pass a basic type check. For a gradient update to type check, we insist that the gradient must be passed through a duality map before being multiplied by a learning rate and applied to the weights: weight ´LR ˚weight.grad type check: failed! (1)

weight ´LR ˚dualizepweight.gradq type check: passed!

Why? The reason is that the loss function may not be equally smooth in all directions in weight space, and there is no reason for the sizes of different components of the raw gradient vector to respect this heterogeneity.

In other words, the geometry of the loss function may be non-isotropic. Insisting on a type check should force the user to become cognizant of this issue and to find a suitable duality map. A good duality map should adjust the size and direction of the gradient to respect the smoothness structure of the loss function.

Duality maps on vector spaces are commonplace in physics and applied math. Examples include the musical isomorphism in differential geometry [(Grosse, 2022)](#b17), raising and lowering indices in general relativity [(Carroll, 2019)](#b9) and the bra-ket notation in quantum mechanics [(Sakurai & Napolitano, 2020)](#b31). Duality maps are also central to several optimization theories including mirror descent [(Nemirovsky & Yudin, 1983)](#b29), natural gradient descent [(Amari, 2016)](#b0) and steepest descent on a normed space [(Boyd & Vandenberghe, 2004)](#b5). Despite the efforts of some prescient papers [(Carlson et al., 2015b;](#)[Flynn, 2017)](#b14), the latter kind of duality map involving normed vector spaces is yet to puncture the deep learning mainstream.

We believe that duality is a key theoretical concept that will help in building performant large-scale machine learning systems. To support this belief, we show in this paper that two important and seemingly disparate methods in contemporary optimization research may be seen as approximations to a single duality map. These methods are maximal update parameterization [(Yang & Hu, 2021)](#b37), which is aimed at scalable training, and Shampoo [(Shi et al., 2023)](#b32), which is targeted at fast training. We show in Section 4.1 that both methods emerge as partial approximations to a single duality map induced by the RMS-RMS operator norm.

The main contribution of this paper is to describe a procedure for constructing duality maps for general neural architectures. The procedure, which we call modular dualization, works in three steps:

Step 1: Operator norms are assigned to individual layers based on the input-output semantics of each layer;

Step 2: Based on these operator norms, duality maps are constructed for individual layers;

Step 3: Given the layerwise duality maps and the structure of the neural architecture, a single duality map is recursively induced on the full weight space of the architecture.

To instantiate this procedure for a rich family of neural architectures-including convolutional networks and transformers-we write down duality maps for Linear, Embed and Conv2D layers. We also provide GPU-friendly algorithms for computing these duality maps. Overall, we hope that modular dualization will help in the principled design of the machine learning systems of the future.

## Related Work

This paper constructs a duality map for general neural architectures. Our approach is based on assigning operator norms to individual network layers and using these layerwise norms to recursively induce a duality map on the full neural architecture. The most closely related prior work is a series of papers on spectral descent [(Carlson et al., 2015a;](#)[b;](#)[2016)](#b0) and a paper on duality structure gradient descent [(Flynn, 2017)](#b14).

Spectral descent has been applied to restricted Boltzmann machines [(Carlson et al., 2015a)](#) and discrete graphical models [(Carlson et al., 2016)](#b7), but let us focus on the more closely related paper on spectral descent for deep learning [(Carlson et al., 2015b)](#). In that paper, the authors propose assigning the Schatten-8 norm (a.k.a. spectral norm) to individual linear layers. This assignment is based on the observation that neural networks admit natural majorization bounds in the Schatten-8 norm. The authors call the corresponding duality map for linear layers the "#-operator"-a name presumably inspired by the musical isomorphism [(Grosse, 2022)](#b17). The authors propose a cheap approximation to the #-operator based on sketching [(Martinsson & Tropp, 2020)](#b28), and they also propose a way to mix RMSprop-style pre-conditioning information [(Tieleman & Hinton, 2012)](#b35) into the weight updates. In contrast to our work, the authors only derive duality maps for single linear layers, and these maps are then heuristically extended to all-layer updates. Nonetheless, the authors achieve substantial wall clock speedups using variants of spectral descent to train small networks. Now, let us turn our attention to duality structure gradient descent [(Flynn, 2017)](#b14), which constructs a duality map on the full weight space of the neural architecture based on identifying a Finsler structure [(Deimling, 1985)](#b11) inherent to neural networks. Similar to modular dualization, Flynn (2017)'s duality map works by assigning duality maps to each layer and then inducing a duality map on the full weight space. The substantial difference to our approach is that Flynn (2017) leverages a weighted sum (L 1 combination) of layerwise norms to construct his full duality map. This leads to optimization methods that only update a single layer at each iteration, and the methods need to be heuristically extended to achieve all-layer updates. In contrast, we leverage the modular norm [(Large et al., 2024)](#b26), which takes a weighted max (L 8 combination) of layerwise norms. In turn, our duality map leads directly to more conventional all-layer optimizers.

Another important difference between our work on modular duality and prior work on duality structure gradient descent is that we fully "modularize" our theory-meaning that our construction is explicitly recursive-and as such it is easy to code up into a software package. In this regard, we are inspired by a line of work that attempts to build optimization algorithms that automatically adapt to the structure of general computation graphs. The earliest work we know of in this category is the PhD thesis of Grant ( [2004](#)) on disciplined convex programming, which aims to infer the convexity properties of general functions by breaking them up into subexpressions and applying composition theorems from convex analysis. More recent progress in this vein includes work on universal majorization-minimization algorithms [(Streeter & Dillon, 2022;](#b34)[Streeter, 2023)](#b33) and related papers on automatic majorization [(Tran et al., 2015;](#b36)[Bernstein et al., 2023)](#b3).

## Theoretical Preliminaries

In this section, we introduce duality maps, a means of constructing duality maps based on norms, and finally a norm called the modular norm that is well-suited to describe the geometry of general neural architectures.

## Duality Maps

Given a vector space V, we say that a function f : V Ñ R is a linear functional on V if f is linear. We define the dual space V ˚to be the set of linear functionals on the vector space V. The dual space is itself a vector space provided that addition is defined pointwise pf `gqpxq :" f pxq `gpxq and scalar multiplication is defined pointwise pαf qpxq :" αf pxq for any scalar α. By duality map we simply mean any function that sends members of the dual vector space V ˚to the primal vector space V. The function need not be an involution.

Let L : W Ñ R denote the loss of a differentiable machine learning model with weight space W " R n . The Taylor expansion of the loss at weight setting w P W is given by: Lpw `∆wq " Lpwq `∇w Lpwq J ∆w `higher-order terms.

(3)

Observe that, in the first-order term, the gradient ∇ w Lpwq is acting as a linear functional: it is pairing with the weight vector ∆w P W in a linear way to produce a real number. As such, we shall say that the gradient belongs to the dual weight space: ∇ w Lpwq P W ˚. We shall forbid ourselves from directly subtracting a member of the dual weight space W ˚from the weight space W. If we would like to conduct a gradient descent update, then we had better find a duality map to send the gradient back to the primal space W.

This restriction may seem absurd! After all, here the weight space W and its dual W ˚are both just R n . However, insisting upon this type check serves to remind us that the curvature of the loss function may be highly heterogeneous. The next section will show one way to construct duality maps to account for this.

## Steepest Descent on a Normed Space

Suppose that we have found a norm }¨} : W Ñ R and a sharpness parameter λ ą 0 that serve as a good model of the higher-order terms in the Taylor expansion of the loss function given in Equation (3):

$Lpw `∆wq « Lpwq `∇w Lpwq J ∆w `λ 2 ¨}∆w} 2 . (4$$)$In other words, the norm provides a good characterization of the heterogeneity in curvature of the loss function.

Then it makes sense to solve for a weight update ∆w by minimizing the right-hand side of Equation ( [4](#formula_1)). We will show that the minimizer can be expressed in terms of a dual norm and a duality map: Definition 1 (Dual norm). Given a norm }¨} : R n Ñ R, the dual norm }¨} : of a vector g P R n is given by: }g} : :" max

$tPR n :}t}"1 g J t.$(5)

Definition 2 (Duality map based on a norm). Given a norm }¨} : R n Ñ R, we consider the duality map:

dualize }¨} g :" arg max

$tPR n :}t}"1 g J t, (6$$)$where, if the arg max is not unique, dualize }¨} returns any maximizer.

Given these definitions, minimizing the expression in the right-hand side of Equation ( [4](#formula_1)) can be done using the following standard proposition, for which Bernstein & Newhouse (2024) provide a proof: Proposition 1 (Steepest descent under a norm). For any g P R n thought of as "the gradient", any λ ě 0 thought of as "the sharpness", and any norm }¨} : R n Ñ R with dual norm }¨} : and duality map dualize }¨} :

$arg min ∆wPR n " g J ∆w `λ 2 }∆w} 2 ȷ " ´}g} : λ ˆdualize }¨} g. (7$$)$In words: to find the minimizer of a linear term penalized by a squared norm, we need only evaluate the dual norm and a duality map. In this paper, we focus on constructing a duality map for the modular norm, which is defined on general neural architectures. The next section reviews duality maps for more standard norms.

## Basic Norms and Duality Maps

Many basic norms and duality maps are already covered in prior work [(Carlson et al., 2016;](#b7)[2015a;](#)[b;](#)[Flynn, 2017)](#b14). For some warmup examples, the following duality maps for vector norms are standard:

Example 1 (Duality map for the Euclidean norm). For a vector g P R d , we have dualize }¨}2 g " g{}g} 2 .

Example 2 (Duality map for the infinity norm). For a vector g P R d , we have dualize }¨}8 g " signpgq, where the sign function is applied entrywise and we are free to take signp0q " 0.

In neural networks, the weight spaces of individual layers tend to have matrix structure. And layers with the same shape weight matrix may have semantically different input and output spaces-think embedding versus linear layers in a transformer. As such, we will need duality maps for different induced operator norms:

Definition 3 (Induced operator norm). Given a matrix M P R doutˆdin and two normed vector spaces pR din , }¨} α q and pR dout , }¨} β q, the "α to β" induced operator norm is given by:

$}M } αÑβ " max xPR d in }M x} β }x} α . (8$$)$For tensors, we define the duality map via dualize }¨} G :" arg max }T }"1 flattenpGq J flattenpT q. For linear layers, we will need the duality map for the RMS Ñ RMS induced operator norm. This ends up as a rescaled version of the spectral norm duality map from prior work [(Carlson et al., 2015b;](#)[Flynn, 2017)](#b14).

Example 3 (Duality map for the RMS Ñ RMS operator norm).

For a vector v P R d , we define the RMS norm to be the normalized Euclidean norm: }v} RMS " }v} 2 { ? d. Given a matrix W P R doutˆdin , the RMS Ñ RMS induced operator norm resolves to a rescaled spectral norm: }W } RMSÑRMS " a d in {d out ˆ}W } ˚, where }¨} denotes the standard spectral norm. For a matrix G P R doutˆdin with reduced singular value decomposition G " U ΣV J , the corresponding duality map is given by dualize

$}¨} RMSÑRMS G " a d out {d in ˆU V J .$And for embedding layers, we will need the duality map for the ℓ 1 Ñ RMS operator norm:

Example 4 (Duality map for the ℓ 1 Ñ RMS operator norm). Given a matrix W P R doutˆdin , the ℓ 1 Ñ RMS induced operator norm resolves to the max RMS norm of the columns: }W } ℓ1ÑRMS " max i }col i pW q} RMS .

For a matrix G P R doutˆdin , the corresponding duality map dualize }¨} ℓ 1 ÑRMS G simply normalizes each column of G to have unit RMS norm: col i pGq Þ Ñ col i pGq{}col i pGq} RMS for each i " 1, ..., d in .

## The Modular Norm

The modular norm [(Large et al., 2024)](#b26) is intended to help characterize the heterogeneous curvature of general neural architectures. The construction first defines an abstract module type along with a notion of what is a good, or well-normed, module. Then combination rules are given for constructing new well-normed modules from a library of existing well-normed modules. So modules are a special case of combinator pattern from functional programming [(Haskell Wiki Contributors, 2007)](#b19). Modules are also related to a monoidal category from category theory [(Fong & Spivak, 2019)](#b15). We begin by defining the abstract notion of a module:

Definition 4 (Module). Given input vector space X , output vector space Y and weight vector space W, a module M is an object with the following four attributes:

(a) a function, M.forward : W ˆX Ñ Y, which maps an input and a weight vector to an output;

(b) a number, M.mass ě 0, which is used to set the proportion of feature learning that this module contributes to any supermodule;

(c) a number, M.sensitivity ě 0, which estimates the module's sensitivity to input perturbations;

(d) a norm over the weight space, M.norm : W Ñ R ě0 , sometimes abbreviated to just }¨} M .

We shall care most about modules that are well-normed, which amounts to requiring that the forward function is Lipschitz-continuous in the weights with constant 1 and in the inputs with constant M.sensitivity:

Definition 5 (Well-normed module). Let M be a module on pX , Y, Wq, where the input and output spaces have respective norms }¨} X and }¨} Y . M is well-normed if for all inputs x P X and weights w P W: }∇ w M.forwardpw, xq ˛∆w} Y ď M.normp∆wq for all ∆w P W; (9) }∇ x M.forwardpw, xq ˛∆x} Y ď M.sensitivity ˚}∆x} X for all ∆x P X . (10)

The ˛operator denotes summation over any shared tensor indices. This definition of well-normed-ness can be used as a guiding principle in the design of a library of atomic (i.e. handwritten) modules. First, norms should be assigned to the input and output space of each module based on the semantics of M.forward. Then a norm M.norm should be assigned to the module's weight space and a number M.sensitivity should be chosen to make the module well-normed. Examples are given in Section 4.1.

Given such a library of well-normed atomic modules, a compound module built through any arbitrary sequence of module compositions and module concatenations is automatically well-normed [(Large et al., 2024)](#b26). And if the atomic modules in the library are not only well-normed but are also smooth in an appropriate sense, then [Large et al. (2024)](#b26) give an automatic procedure for computing sharpness coefficients for any compound module built from the library. The relevant definition of module composition is as follows: Definition 6 (Module composition). Consider module M 1 with input, output and weight space pX 1 , Y 1 , W 1 q and module M 2 with input, output and weight space pX 2 , Y 2 , W 2 q. M 1 and M 2 are composable if X 2 " Y 1 .

Their composite module M " M 2 ˝M1 has input, output and weight space pX 1 , Y 2 , W 1 ˆW2 q and attributes:

(a) M.forwardppw 1 , w 2 q, xqq " M 2 .forwardpw 2 , M 1 .forwardpw 1 , xqq;

(b) M.mass " M 1 .mass `M2 .mass;

(c) M.sensitivity " M 1 .sensitivity ˚M2 .sensitivity;

(d) M.normppw 1 , w 2 qq given by: max ˆM2 .sensitivity ˚M.mass

$M 1 .mass ˚M1 .normpw 1 q, M.mass M 2 .mass ˚M2 .normpw 2 q ˙,$where if M 1 .mass or M 2 .mass is zero, the corresponding term in the max is set to zero.

So the composite norm is taken to be a weighted max over the norms of the two sub-modules, where the weight space of the first module is coupled to the input sensitivity of the second module. The module masses provide freedom to tune the importance of each sub-module in the norm, and [Large et al. (2024)](#b26) prove that module mass provides precise control over the amount of feature learning that can happen in each sub-module.

Module concatenation is defined in a similar way to module composition: Definition 7 (Module concatenation). Consider module M 1 with input, output and weight space pX 1 , Y 1 , W 1 q and module M 2 with input, output and weight space pX 2 , Y 2 , W 2 q. We say that M 1 and M 2 are concatenatable if their input spaces match: X 1 " X 2 . The tuple module M " pM 1 , M 2 q has input, output and weight space pX 1 , Y 1 ˆY2 , W 1 ˆW2 q and the following list of attributes:

(a) M.forwardppw 1 , w 2 q, xqq " pM 1 .forwardpw 1 , xq, M 2 .forwardpw 2 , xqq;

(b) M.mass " M 1 .mass `M2 .mass;

(c) M.sensitivity " M 1 .sensitivity `M2 .sensitivity;

(d) M.normpw 1 , w 2 q given by: max

$ˆM.mass M 1 .mass ˚M1 .normpw 1 q, M.mass M 2 .mass ˚M2 .normpw 2 q ˙,$where if M 1 .mass or M 2 .mass is zero, the corresponding term in the max is set to zero.

A shortcoming of the paper by [Large et al. (2024)](#b26) is that the power of the modular norm is not fully leveraged. In particular, the authors do modular normalization of training, where weight updates to modules are sometimes just naïvely divided by their norm. In this paper we make fuller use of the geometry implied by the modular norm by constructing the corresponding duality map, which we call modular dualization.

## Modular Dualization

In this section, we construct a duality map for general neural architectures. Our strategy is to first write down duality maps for atomic modules, i.e. individual layers. We then extend to arbitrary compound modules, i.e. full neural networks, by showing how duality maps should pass through composition and concatenation.

## Duality Maps for Atomic Modules

To construct a duality map for an atomic module A, the idea is to first fix norms on the input and output spaces that respect the semantics of A.forward. We should select norms that describe both how large we would like the inputs and outputs to be, and in what geometry we would like the outputs to evolve. Then we place a norm on the weight space such that A is well-normed: this is typically the operator norm (Definition 3) induced by the input and output norms. Finally we are in position to solve for the duality map, which we shall call A.dualize. We now give some examples of this procedure for the basic layer types of Linear, Embed and Conv2D. The results are summarized in Table [1](#).

We start with the canonical example of an atomic module:

Example 5 (The Linear module). The Linear module sends inputs from X " R din to outputs in Y " R dout . The weight space is given by the matrix space W " R doutˆdin . We endow the Linear module with attributes:

1. Linear.forwardpW , xq " W x, the matrix-vector product; 2. Linear.sensitivity " 1;

3. Linear.mass " µ, where µ ě 0 is a hyperparameter; 4. Linear.normpW q " }W } RMSÑRMS , the RMS Ñ RMS induced operator norm.

Since the Linear module is intended to map to and from vectors of roughly unit RMS norm, we place the RMS norm on both the input and output space: }¨} X " }¨} RMS and }¨} Y " }¨} RMS . Then Linear is well-normed if the inputs and weights belong to the unit balls ␣ x P R din : }x} X ď 1 ( and ␣ W P R doutˆdin : Linear.normpW q ď 1 ( . Referring back to Section 3.3, the duality map corresponding to Linear.norm is then given by: 5. Linear.dualizepGq " b dout din ˆU V J , where the gradient G P R doutˆdin has reduced SVD G " U ΣV J . This single duality map recovers essential features of both maximal update parameterization [(Yang & Hu, 2021, µP)](#) and Shampoo [(Gupta et al., 2018)](#b18). In particular, the factor of a d out {d in in Linear.dualize recovers spectral update scaling [(Yang et al., 2023)](#b38) that leads to µP. (Initializing such that Linear.normpW q " 1 also recovers µP initialization scaling.) And the mapping G Þ Ñ U V J is equivalent to Shampoo without accumulation [(Bernstein & Newhouse, 2024)](#b2). As such, we believe that duality maps may help reconcile different strands of deep learning research and provide a unifying basis for fast and scalable training algorithms.

The Embed module provides a useful counterpoint to the Linear module. The difference between the two modules stems from the fact that the input spaces of Embed and Linear have different semantics. Example 6 (The Embed module). The Embed module sends inputs from X " R din to outputs in Y " R dout . The weight space is given by the matrix space W " R doutˆdin . We endow the Embed module with attributes: 1. Embed.forwardpW , xq " W x, the matrix-vector product; 2. Embed.sensitivity " 1; 3. Embed.mass " µ, where µ ě 0 is a hyperparameter; 4. Embed.normpW q " }W } ℓ1ÑRMS , the ℓ 1 Ñ RMS induced operator norm.

Embed is intended to map from one-hot vectors to vectors of roughly unit RMS norm, so we place the ℓ 1 norm on the input space and the RMS norm on the output space: }¨} X " }¨} ℓ1 and }¨} Y " }¨} RMS . Then Embed is well-normed if the inputs and weights belong to the unit balls

$␣ x P R din : }x} X ď$1 ( and ␣ W P R doutˆdin : Embed.normpW q ď 1 ( . Referring back to Section 3.3, the duality map for Embed.norm is: 5. Embed.dualizepGq performs the mapping col j pGq Þ Ñ colj pGq }colj pGq} RMS for each column index j " 1, ..., d in . Module Weight Space W Module.norm Module.dualize Linear R doutˆdin

$W Þ Ñ }W } RMSÑRMS G Þ Ñ b dout din ˆU V J Embed R doutˆdin W Þ Ñ }W } ℓ1ÑRMS col j pGq Þ Ñ colj pGq }colj pGq} RMS Conv2D R doutˆdinˆkˆk W Þ Ñ k 2 max k i,j"1 }W ¨¨ij } RMSÑRMS G ¨¨ij Þ Ñ 1 k 2 b dout din ˆUij V J ij$Table [1](#): Duality maps for three atomic modules: Linear, Embed, and Conv2D. These atomic modules are sufficient to build convolutional neural networks and transformers. In Linear.dualize, we let U ΣV J denote the reduced SVD of the gradient matrix G. In Conv2D.dualize, we let U ij Σ ij V J ij denote the reduced SVD of the slice of the gradient tensor G ¨¨ij at kernel indices i and j. Section 5 provides GPU-friendly algorithms for computing these duality maps based on a family of Newton-Schulz iterations.

Finally, we consider a Conv2D module with a k ˆk kernel. Conv2D has a more involved tensor structure than Linear and Embed. The calculations work by slicing up the weight tensor into a collection of k 2 matrices.

Example 7 (The Conv2D module). The Conv2D module sends inputs from X " R WinˆHinˆdin to outputs in Y " R WoutˆHoutˆdout . We think of this as mapping an input image of width W in , height H in and with d in color channels to an output image of width W out , height H out and with d out color channels. The weight space is given by the tensor space W " R doutˆdinˆkˆk , where k is the kernel size. We endow Conv2D with attributes:

1. Conv2D.forwardpW , xq " W f x, where f denotes 2D convolution; 2. Conv2D.sensitivity " 1; 3. Conv2D.mass " µ, where µ ě 0 is a hyperparameter; 4. Conv2D.normpW q " k 2 max k i,j"1 }W ¨¨ij } RMSÑRMS , the max RMS Ñ RMS norm over kernel indices. We would like pixel intensities in the inputs and outputs to be order one and undergo order one change. We formalize this by taking the input and output norms to be the spatial maximum of the RMS norms of all the color channel vectors: }x} X " max Win w"1 max Hin h"1 }x wh¨}RMS and }y} Y " max Wout w"1 max Hout h"1 }y wh¨}RMS . Then Conv2D is well-normed if the inputs and weights belong to the unit balls ␣ x P R WinˆHinˆdin : }x} X ď 1 ( and ␣ W P R doutˆdinˆkˆk : Conv2D.normpW q ď 1 ( . Since the duality map for a max of norms decouples into one duality map per sub-norm, the duality map corresponding to Conv2D.norm is given by:

$5. Conv2D.dualizepGq does G ¨¨ij Þ Ñ 1 k 2 b dout din ˆUij V J ij , where G ¨¨ij has reduced SVD U ij Σ ij V J ij .$
## Duality Maps for Bond Modules

Large et al. ( [2024](#)) define another class of basic modules: bond modules. Bonds are handwritten modules without weights. An example of a bond is the ReLU nonlinearity. For a bond B, the weight space is the zero vector space W " t0u and the modular norm B.norm " 0 Þ Ñ 0. As such, the corresponding duality map is also B.dualize " 0 Þ Ñ 0. In a software package, one need not write norms or duality maps for bond modules.

## Duality Maps for Compound Modules

First, given two composable modules M 1 and M 2 , the duality map for the composite M " M 2 ˝M1 is given by:

$M.dualizepg 1 , g 2 q " ˆ1 M 2 .sensitivity ˚M1 .mass M.mass ˚M1 .dualizepg 1 q, M 2 .mass M.mass ˚M2 .dualizepg 2 q ˙.(11)$And second, given two concatenatable modules M 1 and M 2 , the duality map for the tuple M " pM 1 , M 2 q is:

$M.dualizepg 1 , g 2 q " ˆM1 .mass M.mass ˚M1 .dualizepg 1 q, M 2 .mass M.mass ˚M2 .dualizepg 2 q ˙.(12)$The proofs of Equations ( [11](#formula_16)) and ( [12](#formula_17)) follow in a straightforward manner from Definitions 6 and 7.

## Fast Duality Maps

For modular dualization to be practically feasible, we need ways of computing duality maps quickly. Inspecting the duality maps listed in Table [1](#), we see that Embed.dualize is easy to implement since it just involves computing vector norms of matrix columns. But Linear.dualize and Conv2D.dualize involve the projection:

$G " U ΣV J Þ Ñ U V J , (13$$)$where U ΣV J is the reduced SVD of the matrix G. Since computing SVDs can be slow [(Carlson et al., 2015b;](#)[Flynn, 2017](#b14)), here we discuss three fast approximations to Equation ( [13](#formula_18)) via sketching, iterations for inverse matrix roots, and a family of rectangular Newton-Schulz iterations. Which method works best in practice may depend on the condition number of the matrix G or the available computational resources.

## Sketching

Sketching is a randomized method [(Martinsson & Tropp, 2020)](#b28) that can be used to build low-rank approximations to the SVD. [Carlson et al. (2015b)](#) already used sketching to provide a fast approximation to their #-operator. More recent papers have experimented with sketching in the context of Shampoo-type algorithms [(Feinberg et al., 2023)](#b13). A potential downside of approximating Equation ( [13](#formula_18)) via sketching is that randomized SVD methods usually try to accurately approximate the largest singular values of a matrix [(Martinsson & Tropp, 2020, Section 11.](#)2) while the value of Equation ( [13](#formula_18)) may lie in its action on the small singular values.

## Iterations for Inverse Matrix Roots

Given a full rank matrix G with reduced SVD U ΣV J , we have that:

$U V J " pGG J q ´1{4 G pG J Gq ´1{4 " pGG J q ´1{2 G " G pG J Gq ´1{2 . (14$$)$This provides a route to approximating Equation ( [13](#formula_18)) since one can compute inverse matrix roots such as pGG J q ´1{2 via Newton iteration [(Lakić, 1998)](#b25). This is discussed in Chapter 7 of Higham (2008)'s book and also see [Anil et al. (2020)](#b1)'s paper. Care must be taken with inverses whenever the matrix G is ill-conditioned.

## Rectangular Newton-Schulz Iteration

We developed a "rectangular Newton-Schulz iteration" for computing U V J by adapting Equation [5](#).22 in Higham (2008)'s book for computing the "matrix sign function". We later discovered that this iteration has a long history [(Kovarik, 1970;](#b24)[Björck & Bowie, 1971)](#b4). In short, the method works by first normalizing the matrix G according to X 0 " G{}G} ℓ2Ñℓ2 (or alternatively X 0 " G{}G} F ) and then iterating:

$X t`1 " 3 2 ¨Xt ´1 2 ¨Xt X J t X t , (15$$)$then as t Ñ 8, the sequence X t Ñ U V J . To see this, one can plot the univariate cubic function f pxq :" 3 2 ¨x ´1 2 ¨x3 and see that, for 0 ă x ă ? 3, iterating this cubic will push x closer and closer to `1. The final step is to realize that the effect of the iteration in Equation ( [15](#formula_22)) is to apply this cubic f pxq to each singular value of X t . This shows that the spectral normalization X 0 " G{}G} ℓ2Ñℓ2 is stronger than what is required: we need only ensure that X 0 has singular values no greater than ? 3 for the iteration to converge.

The iteration in Equation ( [15](#formula_22)) has the advantage over sketching that it always works on all singular values, and since the iteration does not compute inverse matrix roots it is well-behaved even on low-rank matrices.

Finally, there are in fact a family of degree 2n `1 polynomial iterations of the form

$X t`1 " a ¨Xt `b ¨Xt X J t X t `c ¨pX t X J t q 2 X t `... `z ¨pX t X J t q n X t (16)$for suitable a, b, c, ..., z that could be used instead of Equation ( [15](#formula_22)). One should choose coefficients a, b, c, ..., z so that the univariate polynomial gpxq " a ¨x `b ¨x3 `c ¨x5 `... `z ¨x2n`1 is a suitable approximation to signpxq. One may try to further accelerate the iteration by "tuning" the coefficients a, b, c, ..., z empirically.

## Discussion

This paper develops the theory of modular duality and the procedure of modular dualization as means to construct duality maps for general neural architectures. Here, we comment on implications and connections.

## A Type System for Deep Learning

Part of the inspiration for this work is the idea of building a fully-fledged type system for deep learning. We think that activation spaces should be typed by their intended norm and the intended size of activations in that norm. This information would help in the construction of well-normed modules (see Section 4.1).

Modules should be typed according to Definition 4. And, as suggested in the introduction, gradients should be explicitly typed as dual vectors. A duality map should flip the type of a dual vector to a primal vector. We plan to use the Modula deep learning package [(Large et al., 2024)](#b26) as a testbed for these ideas.

## Neural Network Speedrunning

We believe that the ideas in this paper can help in the design of faster training methods. In fact, a new NanoGPT training speed record was recently set (Jordan, 2024) using a Newton-Schulz-based duality map. We communicated the method to Keller Jordan through our workshop paper [(Bernstein & Newhouse, 2024)](#b2).

## Modular Duality: A Unifying Theoretical Framework for Fast and Scalable Training

An important topic in contemporary optimization research is the design of fast and scalable training methods for neural networks. In fact, the theme of the Optimization for Machine Learning workshop at this year's NeurIPS conference is "scaling up optimization" (OPT, 2024). Two popular methods in this research space are maximal update parameterization [(Yang & Hu, 2021, µP)](#), which allows for increasing network width without changing the optimal learning rate, and Shampoo [(Gupta et al., 2018)](#b18), a variant of which [(Shi et al., 2023)](#b32) won a speed challenge at the inaugural AlgoPerf optimization competition [(Dahl et al., 2023)](#b10).

We showed in Section 4.1 that essential features of both µP and Shampoo are recovered from the single duality map Linear.dualize. We think that, on a basic theoretical level, µP and Shampoo should be viewed as partial approximations to this duality map. This observation helps put µP and Shampoo on a consistent theoretical footing, orients the methods with respect to overlooked prior work on spectral descent [(Carlson et al., 2015b)](#) and duality structure gradient descent [(Flynn, 2017)](#b14), and suggests new ways to generalize these methods to arbitrary layer types and network architectures via the modular norm and modular dualization.

## On the Alignment of Activations and Updates

Recent work [(Yang et al., 2023;](#b38)[Everett et al., 2024;](#b12)[Large et al., 2024)](#b26) has singled out the following question as important to the design of scalable deep learning systems: to what extent do gradient updates to neural network layers align with incoming activation vectors? This question is important since it helps inform how large weight updates need to be to induce a certain amount of change in layer outputs. Duality maps such as Linear.dualize and Conv2D.dualize may help simplify the answer to this question, since they project gradients to scaled semi-orthogonal matrices for which all singular values have the same magnitude.

## A Numerical Paradox: The Weights Don't Change!

Past work [(Lee et al., 2019;](#b27)[Jesus et al., 2021)](#b22) has pointed out an apparent paradox in deep learning: the weights seem to move a vanishing amount from initialization in the limit of large network width. This finding has motivated a substantial amount of work on linearized training dynamics [(Jacot et al., 2018)](#b21). We attempted to resolve this paradox in prior work by showing that the weights move a roughly constant amount at any width when the change is measured in spectral norm [(Yang et al., 2023)](#b38). But duality maps change the story again: Linear.dualize ramps up the stable rank of updates, so the weights should move a non-trivial relative amount at large width even in the Frobenius norm-provided the batch size is not too small.

## Conclusion

This paper has proposed a recursive procedure called modular dualization for building duality maps for general neural architectures. The procedure unifies past strands of optimization research on Shampoo [(Gupta et al., 2018)](#b18) and µP [(Yang & Hu, 2021)](#b37). Partial implementations have already led to significant wall-clock speedups in transformer training [(Jordan, 2024)](#b23). The rectangular Newton-Schulz iteration provides a GPU-friendly and numerically stable means of dualizing under the RMS Ñ RMS operator norm, while avoiding some of the downsides of sketching-based approaches [(Carlson et al., 2015b)](#). Overall, we hope that our theory of modular duality provides a clarifying toolkit for the design and analysis of deep learning systems.

