<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Modular Duality in Deep Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-12-06">6 Dec 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jeremy</forename><surname>Bernstein</surname></persName>
							<email>jbernstein@mit.edu</email>
						</author>
						<author>
							<persName><forename type="first">Laker</forename><surname>Newhouse</surname></persName>
							<email>lakern@mit.edu</email>
						</author>
						<title level="a" type="main">Modular Duality in Deep Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-12-06">6 Dec 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">A8816667BFC0B06ADBFE16FBB036FD57</idno>
					<idno type="arXiv">arXiv:2410.21265v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>An old idea in optimization theory says that since the gradient is a dual vector it may not be subtracted from the weights without first being mapped to the primal space where the weights reside. We take this idea seriously in this paper and construct such a duality map for general neural networks. Our map, which we call modular dualization, forms a unifying theoretical basis for training algorithms that are a) fast and b) scalable. Modular dualization involves first assigning operator norms to layers based on the semantics of each layer, and then using these layerwise norms to recursively induce a duality map on the weight space of the full neural architecture. We conclude by deriving GPU-friendly algorithms for dualizing Embed, Linear and Conv2D layers-the latter two methods are based on a rectangular Newton-Schulz iteration <ref type="bibr" target="#b24">(Kovarik, 1970;</ref><ref type="bibr" target="#b4">Björck &amp; Bowie, 1971)</ref>. A variant of our methods was used to set speed records for training NanoGPT. Overall, we hope that our theory of modular duality will yield a next generation of fast and scalable optimizers for general neural architectures.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this paper, we pursue a rigorous and first-principles theoretical framework for designing neural network training algorithms. We hope that building such a framework will facilitate the design of a next generation of fast and scalable optimizers that are automatically tailored to different neural architectures.</p><p>While gradient descent is the workhorse of modern machine learning, the most vanilla form of the algorithm does not, in our view, pass a basic type check. For a gradient update to type check, we insist that the gradient must be passed through a duality map before being multiplied by a learning rate and applied to the weights: weight ´LR ˚weight.grad type check: failed! (1)</p><p>weight ´LR ˚dualizepweight.gradq type check: passed!</p><p>Why? The reason is that the loss function may not be equally smooth in all directions in weight space, and there is no reason for the sizes of different components of the raw gradient vector to respect this heterogeneity.</p><p>In other words, the geometry of the loss function may be non-isotropic. Insisting on a type check should force the user to become cognizant of this issue and to find a suitable duality map. A good duality map should adjust the size and direction of the gradient to respect the smoothness structure of the loss function.</p><p>Duality maps on vector spaces are commonplace in physics and applied math. Examples include the musical isomorphism in differential geometry <ref type="bibr" target="#b17">(Grosse, 2022)</ref>, raising and lowering indices in general relativity <ref type="bibr" target="#b9">(Carroll, 2019)</ref> and the bra-ket notation in quantum mechanics <ref type="bibr" target="#b31">(Sakurai &amp; Napolitano, 2020)</ref>. Duality maps are also central to several optimization theories including mirror descent <ref type="bibr" target="#b29">(Nemirovsky &amp; Yudin, 1983)</ref>, natural gradient descent <ref type="bibr" target="#b0">(Amari, 2016)</ref> and steepest descent on a normed space <ref type="bibr" target="#b5">(Boyd &amp; Vandenberghe, 2004)</ref>. Despite the efforts of some prescient papers <ref type="bibr">(Carlson et al., 2015b;</ref><ref type="bibr" target="#b14">Flynn, 2017)</ref>, the latter kind of duality map involving normed vector spaces is yet to puncture the deep learning mainstream.</p><p>We believe that duality is a key theoretical concept that will help in building performant large-scale machine learning systems. To support this belief, we show in this paper that two important and seemingly disparate methods in contemporary optimization research may be seen as approximations to a single duality map. These methods are maximal update parameterization <ref type="bibr" target="#b37">(Yang &amp; Hu, 2021)</ref>, which is aimed at scalable training, and Shampoo <ref type="bibr" target="#b32">(Shi et al., 2023)</ref>, which is targeted at fast training. We show in Section 4.1 that both methods emerge as partial approximations to a single duality map induced by the RMS-RMS operator norm.</p><p>The main contribution of this paper is to describe a procedure for constructing duality maps for general neural architectures. The procedure, which we call modular dualization, works in three steps:</p><p>Step 1: Operator norms are assigned to individual layers based on the input-output semantics of each layer;</p><p>Step 2: Based on these operator norms, duality maps are constructed for individual layers;</p><p>Step 3: Given the layerwise duality maps and the structure of the neural architecture, a single duality map is recursively induced on the full weight space of the architecture.</p><p>To instantiate this procedure for a rich family of neural architectures-including convolutional networks and transformers-we write down duality maps for Linear, Embed and Conv2D layers. We also provide GPU-friendly algorithms for computing these duality maps. Overall, we hope that modular dualization will help in the principled design of the machine learning systems of the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>This paper constructs a duality map for general neural architectures. Our approach is based on assigning operator norms to individual network layers and using these layerwise norms to recursively induce a duality map on the full neural architecture. The most closely related prior work is a series of papers on spectral descent <ref type="bibr">(Carlson et al., 2015a;</ref><ref type="bibr">b;</ref><ref type="bibr" target="#b0">2016)</ref> and a paper on duality structure gradient descent <ref type="bibr" target="#b14">(Flynn, 2017)</ref>.</p><p>Spectral descent has been applied to restricted Boltzmann machines <ref type="bibr">(Carlson et al., 2015a)</ref> and discrete graphical models <ref type="bibr" target="#b7">(Carlson et al., 2016)</ref>, but let us focus on the more closely related paper on spectral descent for deep learning <ref type="bibr">(Carlson et al., 2015b)</ref>. In that paper, the authors propose assigning the Schatten-8 norm (a.k.a. spectral norm) to individual linear layers. This assignment is based on the observation that neural networks admit natural majorization bounds in the Schatten-8 norm. The authors call the corresponding duality map for linear layers the "#-operator"-a name presumably inspired by the musical isomorphism <ref type="bibr" target="#b17">(Grosse, 2022)</ref>. The authors propose a cheap approximation to the #-operator based on sketching <ref type="bibr" target="#b28">(Martinsson &amp; Tropp, 2020)</ref>, and they also propose a way to mix RMSprop-style pre-conditioning information <ref type="bibr" target="#b35">(Tieleman &amp; Hinton, 2012)</ref> into the weight updates. In contrast to our work, the authors only derive duality maps for single linear layers, and these maps are then heuristically extended to all-layer updates. Nonetheless, the authors achieve substantial wall clock speedups using variants of spectral descent to train small networks. Now, let us turn our attention to duality structure gradient descent <ref type="bibr" target="#b14">(Flynn, 2017)</ref>, which constructs a duality map on the full weight space of the neural architecture based on identifying a Finsler structure <ref type="bibr" target="#b11">(Deimling, 1985)</ref> inherent to neural networks. Similar to modular dualization, Flynn (2017)'s duality map works by assigning duality maps to each layer and then inducing a duality map on the full weight space. The substantial difference to our approach is that Flynn (2017) leverages a weighted sum (L 1 combination) of layerwise norms to construct his full duality map. This leads to optimization methods that only update a single layer at each iteration, and the methods need to be heuristically extended to achieve all-layer updates. In contrast, we leverage the modular norm <ref type="bibr" target="#b26">(Large et al., 2024)</ref>, which takes a weighted max (L 8 combination) of layerwise norms. In turn, our duality map leads directly to more conventional all-layer optimizers.</p><p>Another important difference between our work on modular duality and prior work on duality structure gradient descent is that we fully "modularize" our theory-meaning that our construction is explicitly recursive-and as such it is easy to code up into a software package. In this regard, we are inspired by a line of work that attempts to build optimization algorithms that automatically adapt to the structure of general computation graphs. The earliest work we know of in this category is the PhD thesis of Grant ( <ref type="formula">2004</ref>) on disciplined convex programming, which aims to infer the convexity properties of general functions by breaking them up into subexpressions and applying composition theorems from convex analysis. More recent progress in this vein includes work on universal majorization-minimization algorithms <ref type="bibr" target="#b34">(Streeter &amp; Dillon, 2022;</ref><ref type="bibr" target="#b33">Streeter, 2023)</ref> and related papers on automatic majorization <ref type="bibr" target="#b36">(Tran et al., 2015;</ref><ref type="bibr" target="#b3">Bernstein et al., 2023)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Theoretical Preliminaries</head><p>In this section, we introduce duality maps, a means of constructing duality maps based on norms, and finally a norm called the modular norm that is well-suited to describe the geometry of general neural architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Duality Maps</head><p>Given a vector space V, we say that a function f : V Ñ R is a linear functional on V if f is linear. We define the dual space V ˚to be the set of linear functionals on the vector space V. The dual space is itself a vector space provided that addition is defined pointwise pf `gqpxq :" f pxq `gpxq and scalar multiplication is defined pointwise pαf qpxq :" αf pxq for any scalar α. By duality map we simply mean any function that sends members of the dual vector space V ˚to the primal vector space V. The function need not be an involution.</p><p>Let L : W Ñ R denote the loss of a differentiable machine learning model with weight space W " R n . The Taylor expansion of the loss at weight setting w P W is given by: Lpw `∆wq " Lpwq `∇w Lpwq J ∆w `higher-order terms.</p><p>(3)</p><p>Observe that, in the first-order term, the gradient ∇ w Lpwq is acting as a linear functional: it is pairing with the weight vector ∆w P W in a linear way to produce a real number. As such, we shall say that the gradient belongs to the dual weight space: ∇ w Lpwq P W ˚. We shall forbid ourselves from directly subtracting a member of the dual weight space W ˚from the weight space W. If we would like to conduct a gradient descent update, then we had better find a duality map to send the gradient back to the primal space W.</p><p>This restriction may seem absurd! After all, here the weight space W and its dual W ˚are both just R n . However, insisting upon this type check serves to remind us that the curvature of the loss function may be highly heterogeneous. The next section will show one way to construct duality maps to account for this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Steepest Descent on a Normed Space</head><p>Suppose that we have found a norm }¨} : W Ñ R and a sharpness parameter λ ą 0 that serve as a good model of the higher-order terms in the Taylor expansion of the loss function given in Equation (3):</p><formula xml:id="formula_1">Lpw `∆wq « Lpwq `∇w Lpwq J ∆w `λ 2 ¨}∆w} 2 . (<label>4</label></formula><formula xml:id="formula_2">)</formula><p>In other words, the norm provides a good characterization of the heterogeneity in curvature of the loss function.</p><p>Then it makes sense to solve for a weight update ∆w by minimizing the right-hand side of Equation ( <ref type="formula" target="#formula_1">4</ref>). We will show that the minimizer can be expressed in terms of a dual norm and a duality map: Definition 1 (Dual norm). Given a norm }¨} : R n Ñ R, the dual norm }¨} : of a vector g P R n is given by: }g} : :" max</p><formula xml:id="formula_3">tPR n :}t}"1 g J t.</formula><p>(5)</p><p>Definition 2 (Duality map based on a norm). Given a norm }¨} : R n Ñ R, we consider the duality map:</p><p>dualize }¨} g :" arg max</p><formula xml:id="formula_4">tPR n :}t}"1 g J t, (<label>6</label></formula><formula xml:id="formula_5">)</formula><p>where, if the arg max is not unique, dualize }¨} returns any maximizer.</p><p>Given these definitions, minimizing the expression in the right-hand side of Equation ( <ref type="formula" target="#formula_1">4</ref>) can be done using the following standard proposition, for which Bernstein &amp; Newhouse (2024) provide a proof: Proposition 1 (Steepest descent under a norm). For any g P R n thought of as "the gradient", any λ ě 0 thought of as "the sharpness", and any norm }¨} : R n Ñ R with dual norm }¨} : and duality map dualize }¨} :</p><formula xml:id="formula_6">arg min ∆wPR n " g J ∆w `λ 2 }∆w} 2 ȷ " ´}g} : λ ˆdualize }¨} g. (<label>7</label></formula><formula xml:id="formula_7">)</formula><p>In words: to find the minimizer of a linear term penalized by a squared norm, we need only evaluate the dual norm and a duality map. In this paper, we focus on constructing a duality map for the modular norm, which is defined on general neural architectures. The next section reviews duality maps for more standard norms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Basic Norms and Duality Maps</head><p>Many basic norms and duality maps are already covered in prior work <ref type="bibr" target="#b7">(Carlson et al., 2016;</ref><ref type="bibr">2015a;</ref><ref type="bibr">b;</ref><ref type="bibr" target="#b14">Flynn, 2017)</ref>. For some warmup examples, the following duality maps for vector norms are standard:</p><p>Example 1 (Duality map for the Euclidean norm). For a vector g P R d , we have dualize }¨}2 g " g{}g} 2 .</p><p>Example 2 (Duality map for the infinity norm). For a vector g P R d , we have dualize }¨}8 g " signpgq, where the sign function is applied entrywise and we are free to take signp0q " 0.</p><p>In neural networks, the weight spaces of individual layers tend to have matrix structure. And layers with the same shape weight matrix may have semantically different input and output spaces-think embedding versus linear layers in a transformer. As such, we will need duality maps for different induced operator norms:</p><p>Definition 3 (Induced operator norm). Given a matrix M P R doutˆdin and two normed vector spaces pR din , }¨} α q and pR dout , }¨} β q, the "α to β" induced operator norm is given by:</p><formula xml:id="formula_8">}M } αÑβ " max xPR d in }M x} β }x} α . (<label>8</label></formula><formula xml:id="formula_9">)</formula><p>For tensors, we define the duality map via dualize }¨} G :" arg max }T }"1 flattenpGq J flattenpT q. For linear layers, we will need the duality map for the RMS Ñ RMS induced operator norm. This ends up as a rescaled version of the spectral norm duality map from prior work <ref type="bibr">(Carlson et al., 2015b;</ref><ref type="bibr" target="#b14">Flynn, 2017)</ref>.</p><p>Example 3 (Duality map for the RMS Ñ RMS operator norm).</p><p>For a vector v P R d , we define the RMS norm to be the normalized Euclidean norm: }v} RMS " }v} 2 { ? d. Given a matrix W P R doutˆdin , the RMS Ñ RMS induced operator norm resolves to a rescaled spectral norm: }W } RMSÑRMS " a d in {d out ˆ}W } ˚, where }¨} denotes the standard spectral norm. For a matrix G P R doutˆdin with reduced singular value decomposition G " U ΣV J , the corresponding duality map is given by dualize</p><formula xml:id="formula_10">}¨} RMSÑRMS G " a d out {d in ˆU V J .</formula><p>And for embedding layers, we will need the duality map for the ℓ 1 Ñ RMS operator norm:</p><p>Example 4 (Duality map for the ℓ 1 Ñ RMS operator norm). Given a matrix W P R doutˆdin , the ℓ 1 Ñ RMS induced operator norm resolves to the max RMS norm of the columns: }W } ℓ1ÑRMS " max i }col i pW q} RMS .</p><p>For a matrix G P R doutˆdin , the corresponding duality map dualize }¨} ℓ 1 ÑRMS G simply normalizes each column of G to have unit RMS norm: col i pGq Þ Ñ col i pGq{}col i pGq} RMS for each i " 1, ..., d in .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">The Modular Norm</head><p>The modular norm <ref type="bibr" target="#b26">(Large et al., 2024)</ref> is intended to help characterize the heterogeneous curvature of general neural architectures. The construction first defines an abstract module type along with a notion of what is a good, or well-normed, module. Then combination rules are given for constructing new well-normed modules from a library of existing well-normed modules. So modules are a special case of combinator pattern from functional programming <ref type="bibr" target="#b19">(Haskell Wiki Contributors, 2007)</ref>. Modules are also related to a monoidal category from category theory <ref type="bibr" target="#b15">(Fong &amp; Spivak, 2019)</ref>. We begin by defining the abstract notion of a module:</p><p>Definition 4 (Module). Given input vector space X , output vector space Y and weight vector space W, a module M is an object with the following four attributes:</p><p>(a) a function, M.forward : W ˆX Ñ Y, which maps an input and a weight vector to an output;</p><p>(b) a number, M.mass ě 0, which is used to set the proportion of feature learning that this module contributes to any supermodule;</p><p>(c) a number, M.sensitivity ě 0, which estimates the module's sensitivity to input perturbations;</p><p>(d) a norm over the weight space, M.norm : W Ñ R ě0 , sometimes abbreviated to just }¨} M .</p><p>We shall care most about modules that are well-normed, which amounts to requiring that the forward function is Lipschitz-continuous in the weights with constant 1 and in the inputs with constant M.sensitivity:</p><p>Definition 5 (Well-normed module). Let M be a module on pX , Y, Wq, where the input and output spaces have respective norms }¨} X and }¨} Y . M is well-normed if for all inputs x P X and weights w P W: }∇ w M.forwardpw, xq ˛∆w} Y ď M.normp∆wq for all ∆w P W; (9) }∇ x M.forwardpw, xq ˛∆x} Y ď M.sensitivity ˚}∆x} X for all ∆x P X . (10)</p><p>The ˛operator denotes summation over any shared tensor indices. This definition of well-normed-ness can be used as a guiding principle in the design of a library of atomic (i.e. handwritten) modules. First, norms should be assigned to the input and output space of each module based on the semantics of M.forward. Then a norm M.norm should be assigned to the module's weight space and a number M.sensitivity should be chosen to make the module well-normed. Examples are given in Section 4.1.</p><p>Given such a library of well-normed atomic modules, a compound module built through any arbitrary sequence of module compositions and module concatenations is automatically well-normed <ref type="bibr" target="#b26">(Large et al., 2024)</ref>. And if the atomic modules in the library are not only well-normed but are also smooth in an appropriate sense, then <ref type="bibr" target="#b26">Large et al. (2024)</ref> give an automatic procedure for computing sharpness coefficients for any compound module built from the library. The relevant definition of module composition is as follows: Definition 6 (Module composition). Consider module M 1 with input, output and weight space pX 1 , Y 1 , W 1 q and module M 2 with input, output and weight space pX 2 , Y 2 , W 2 q. M 1 and M 2 are composable if X 2 " Y 1 .</p><p>Their composite module M " M 2 ˝M1 has input, output and weight space pX 1 , Y 2 , W 1 ˆW2 q and attributes:</p><p>(a) M.forwardppw 1 , w 2 q, xqq " M 2 .forwardpw 2 , M 1 .forwardpw 1 , xqq;</p><p>(b) M.mass " M 1 .mass `M2 .mass;</p><p>(c) M.sensitivity " M 1 .sensitivity ˚M2 .sensitivity;</p><p>(d) M.normppw 1 , w 2 qq given by: max ˆM2 .sensitivity ˚M.mass</p><formula xml:id="formula_11">M 1 .mass ˚M1 .normpw 1 q, M.mass M 2 .mass ˚M2 .normpw 2 q ˙,</formula><p>where if M 1 .mass or M 2 .mass is zero, the corresponding term in the max is set to zero.</p><p>So the composite norm is taken to be a weighted max over the norms of the two sub-modules, where the weight space of the first module is coupled to the input sensitivity of the second module. The module masses provide freedom to tune the importance of each sub-module in the norm, and <ref type="bibr" target="#b26">Large et al. (2024)</ref> prove that module mass provides precise control over the amount of feature learning that can happen in each sub-module.</p><p>Module concatenation is defined in a similar way to module composition: Definition 7 (Module concatenation). Consider module M 1 with input, output and weight space pX 1 , Y 1 , W 1 q and module M 2 with input, output and weight space pX 2 , Y 2 , W 2 q. We say that M 1 and M 2 are concatenatable if their input spaces match: X 1 " X 2 . The tuple module M " pM 1 , M 2 q has input, output and weight space pX 1 , Y 1 ˆY2 , W 1 ˆW2 q and the following list of attributes:</p><p>(a) M.forwardppw 1 , w 2 q, xqq " pM 1 .forwardpw 1 , xq, M 2 .forwardpw 2 , xqq;</p><p>(b) M.mass " M 1 .mass `M2 .mass;</p><p>(c) M.sensitivity " M 1 .sensitivity `M2 .sensitivity;</p><p>(d) M.normpw 1 , w 2 q given by: max</p><formula xml:id="formula_12">ˆM.mass M 1 .mass ˚M1 .normpw 1 q, M.mass M 2 .mass ˚M2 .normpw 2 q ˙,</formula><p>where if M 1 .mass or M 2 .mass is zero, the corresponding term in the max is set to zero.</p><p>A shortcoming of the paper by <ref type="bibr" target="#b26">Large et al. (2024)</ref> is that the power of the modular norm is not fully leveraged. In particular, the authors do modular normalization of training, where weight updates to modules are sometimes just naïvely divided by their norm. In this paper we make fuller use of the geometry implied by the modular norm by constructing the corresponding duality map, which we call modular dualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Modular Dualization</head><p>In this section, we construct a duality map for general neural architectures. Our strategy is to first write down duality maps for atomic modules, i.e. individual layers. We then extend to arbitrary compound modules, i.e. full neural networks, by showing how duality maps should pass through composition and concatenation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Duality Maps for Atomic Modules</head><p>To construct a duality map for an atomic module A, the idea is to first fix norms on the input and output spaces that respect the semantics of A.forward. We should select norms that describe both how large we would like the inputs and outputs to be, and in what geometry we would like the outputs to evolve. Then we place a norm on the weight space such that A is well-normed: this is typically the operator norm (Definition 3) induced by the input and output norms. Finally we are in position to solve for the duality map, which we shall call A.dualize. We now give some examples of this procedure for the basic layer types of Linear, Embed and Conv2D. The results are summarized in Table <ref type="table">1</ref>.</p><p>We start with the canonical example of an atomic module:</p><p>Example 5 (The Linear module). The Linear module sends inputs from X " R din to outputs in Y " R dout . The weight space is given by the matrix space W " R doutˆdin . We endow the Linear module with attributes:</p><p>1. Linear.forwardpW , xq " W x, the matrix-vector product; 2. Linear.sensitivity " 1;</p><p>3. Linear.mass " µ, where µ ě 0 is a hyperparameter; 4. Linear.normpW q " }W } RMSÑRMS , the RMS Ñ RMS induced operator norm.</p><p>Since the Linear module is intended to map to and from vectors of roughly unit RMS norm, we place the RMS norm on both the input and output space: }¨} X " }¨} RMS and }¨} Y " }¨} RMS . Then Linear is well-normed if the inputs and weights belong to the unit balls ␣ x P R din : }x} X ď 1 ( and ␣ W P R doutˆdin : Linear.normpW q ď 1 ( . Referring back to Section 3.3, the duality map corresponding to Linear.norm is then given by: 5. Linear.dualizepGq " b dout din ˆU V J , where the gradient G P R doutˆdin has reduced SVD G " U ΣV J . This single duality map recovers essential features of both maximal update parameterization <ref type="bibr">(Yang &amp; Hu, 2021, µP)</ref> and Shampoo <ref type="bibr" target="#b18">(Gupta et al., 2018)</ref>. In particular, the factor of a d out {d in in Linear.dualize recovers spectral update scaling <ref type="bibr" target="#b38">(Yang et al., 2023)</ref> that leads to µP. (Initializing such that Linear.normpW q " 1 also recovers µP initialization scaling.) And the mapping G Þ Ñ U V J is equivalent to Shampoo without accumulation <ref type="bibr" target="#b2">(Bernstein &amp; Newhouse, 2024)</ref>. As such, we believe that duality maps may help reconcile different strands of deep learning research and provide a unifying basis for fast and scalable training algorithms.</p><p>The Embed module provides a useful counterpoint to the Linear module. The difference between the two modules stems from the fact that the input spaces of Embed and Linear have different semantics. Example 6 (The Embed module). The Embed module sends inputs from X " R din to outputs in Y " R dout . The weight space is given by the matrix space W " R doutˆdin . We endow the Embed module with attributes: 1. Embed.forwardpW , xq " W x, the matrix-vector product; 2. Embed.sensitivity " 1; 3. Embed.mass " µ, where µ ě 0 is a hyperparameter; 4. Embed.normpW q " }W } ℓ1ÑRMS , the ℓ 1 Ñ RMS induced operator norm.</p><p>Embed is intended to map from one-hot vectors to vectors of roughly unit RMS norm, so we place the ℓ 1 norm on the input space and the RMS norm on the output space: }¨} X " }¨} ℓ1 and }¨} Y " }¨} RMS . Then Embed is well-normed if the inputs and weights belong to the unit balls</p><formula xml:id="formula_13">␣ x P R din : }x} X ď</formula><p>1 ( and ␣ W P R doutˆdin : Embed.normpW q ď 1 ( . Referring back to Section 3.3, the duality map for Embed.norm is: 5. Embed.dualizepGq performs the mapping col j pGq Þ Ñ colj pGq }colj pGq} RMS for each column index j " 1, ..., d in . Module Weight Space W Module.norm Module.dualize Linear R doutˆdin</p><formula xml:id="formula_14">W Þ Ñ }W } RMSÑRMS G Þ Ñ b dout din ˆU V J Embed R doutˆdin W Þ Ñ }W } ℓ1ÑRMS col j pGq Þ Ñ colj pGq }colj pGq} RMS Conv2D R doutˆdinˆkˆk W Þ Ñ k 2 max k i,j"1 }W ¨¨ij } RMSÑRMS G ¨¨ij Þ Ñ 1 k 2 b dout din ˆUij V J ij</formula><p>Table <ref type="table">1</ref>: Duality maps for three atomic modules: Linear, Embed, and Conv2D. These atomic modules are sufficient to build convolutional neural networks and transformers. In Linear.dualize, we let U ΣV J denote the reduced SVD of the gradient matrix G. In Conv2D.dualize, we let U ij Σ ij V J ij denote the reduced SVD of the slice of the gradient tensor G ¨¨ij at kernel indices i and j. Section 5 provides GPU-friendly algorithms for computing these duality maps based on a family of Newton-Schulz iterations.</p><p>Finally, we consider a Conv2D module with a k ˆk kernel. Conv2D has a more involved tensor structure than Linear and Embed. The calculations work by slicing up the weight tensor into a collection of k 2 matrices.</p><p>Example 7 (The Conv2D module). The Conv2D module sends inputs from X " R WinˆHinˆdin to outputs in Y " R WoutˆHoutˆdout . We think of this as mapping an input image of width W in , height H in and with d in color channels to an output image of width W out , height H out and with d out color channels. The weight space is given by the tensor space W " R doutˆdinˆkˆk , where k is the kernel size. We endow Conv2D with attributes:</p><p>1. Conv2D.forwardpW , xq " W f x, where f denotes 2D convolution; 2. Conv2D.sensitivity " 1; 3. Conv2D.mass " µ, where µ ě 0 is a hyperparameter; 4. Conv2D.normpW q " k 2 max k i,j"1 }W ¨¨ij } RMSÑRMS , the max RMS Ñ RMS norm over kernel indices. We would like pixel intensities in the inputs and outputs to be order one and undergo order one change. We formalize this by taking the input and output norms to be the spatial maximum of the RMS norms of all the color channel vectors: }x} X " max Win w"1 max Hin h"1 }x wh¨}RMS and }y} Y " max Wout w"1 max Hout h"1 }y wh¨}RMS . Then Conv2D is well-normed if the inputs and weights belong to the unit balls ␣ x P R WinˆHinˆdin : }x} X ď 1 ( and ␣ W P R doutˆdinˆkˆk : Conv2D.normpW q ď 1 ( . Since the duality map for a max of norms decouples into one duality map per sub-norm, the duality map corresponding to Conv2D.norm is given by:</p><formula xml:id="formula_15">5. Conv2D.dualizepGq does G ¨¨ij Þ Ñ 1 k 2 b dout din ˆUij V J ij , where G ¨¨ij has reduced SVD U ij Σ ij V J ij .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Duality Maps for Bond Modules</head><p>Large et al. ( <ref type="formula">2024</ref>) define another class of basic modules: bond modules. Bonds are handwritten modules without weights. An example of a bond is the ReLU nonlinearity. For a bond B, the weight space is the zero vector space W " t0u and the modular norm B.norm " 0 Þ Ñ 0. As such, the corresponding duality map is also B.dualize " 0 Þ Ñ 0. In a software package, one need not write norms or duality maps for bond modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Duality Maps for Compound Modules</head><p>First, given two composable modules M 1 and M 2 , the duality map for the composite M " M 2 ˝M1 is given by:</p><formula xml:id="formula_16">M.dualizepg 1 , g 2 q " ˆ1 M 2 .sensitivity ˚M1 .mass M.mass ˚M1 .dualizepg 1 q, M 2 .mass M.mass ˚M2 .dualizepg 2 q ˙.<label>(11)</label></formula><p>And second, given two concatenatable modules M 1 and M 2 , the duality map for the tuple M " pM 1 , M 2 q is:</p><formula xml:id="formula_17">M.dualizepg 1 , g 2 q " ˆM1 .mass M.mass ˚M1 .dualizepg 1 q, M 2 .mass M.mass ˚M2 .dualizepg 2 q ˙.<label>(12)</label></formula><p>The proofs of Equations ( <ref type="formula" target="#formula_16">11</ref>) and ( <ref type="formula" target="#formula_17">12</ref>) follow in a straightforward manner from Definitions 6 and 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Fast Duality Maps</head><p>For modular dualization to be practically feasible, we need ways of computing duality maps quickly. Inspecting the duality maps listed in Table <ref type="table">1</ref>, we see that Embed.dualize is easy to implement since it just involves computing vector norms of matrix columns. But Linear.dualize and Conv2D.dualize involve the projection:</p><formula xml:id="formula_18">G " U ΣV J Þ Ñ U V J , (<label>13</label></formula><formula xml:id="formula_19">)</formula><p>where U ΣV J is the reduced SVD of the matrix G. Since computing SVDs can be slow <ref type="bibr">(Carlson et al., 2015b;</ref><ref type="bibr" target="#b14">Flynn, 2017</ref>), here we discuss three fast approximations to Equation ( <ref type="formula" target="#formula_18">13</ref>) via sketching, iterations for inverse matrix roots, and a family of rectangular Newton-Schulz iterations. Which method works best in practice may depend on the condition number of the matrix G or the available computational resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Sketching</head><p>Sketching is a randomized method <ref type="bibr" target="#b28">(Martinsson &amp; Tropp, 2020)</ref> that can be used to build low-rank approximations to the SVD. <ref type="bibr">Carlson et al. (2015b)</ref> already used sketching to provide a fast approximation to their #-operator. More recent papers have experimented with sketching in the context of Shampoo-type algorithms <ref type="bibr" target="#b13">(Feinberg et al., 2023)</ref>. A potential downside of approximating Equation ( <ref type="formula" target="#formula_18">13</ref>) via sketching is that randomized SVD methods usually try to accurately approximate the largest singular values of a matrix <ref type="bibr">(Martinsson &amp; Tropp, 2020, Section 11.</ref>2) while the value of Equation ( <ref type="formula" target="#formula_18">13</ref>) may lie in its action on the small singular values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Iterations for Inverse Matrix Roots</head><p>Given a full rank matrix G with reduced SVD U ΣV J , we have that:</p><formula xml:id="formula_20">U V J " pGG J q ´1{4 G pG J Gq ´1{4 " pGG J q ´1{2 G " G pG J Gq ´1{2 . (<label>14</label></formula><formula xml:id="formula_21">)</formula><p>This provides a route to approximating Equation ( <ref type="formula" target="#formula_18">13</ref>) since one can compute inverse matrix roots such as pGG J q ´1{2 via Newton iteration <ref type="bibr" target="#b25">(Lakić, 1998)</ref>. This is discussed in Chapter 7 of Higham (2008)'s book and also see <ref type="bibr" target="#b1">Anil et al. (2020)</ref>'s paper. Care must be taken with inverses whenever the matrix G is ill-conditioned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Rectangular Newton-Schulz Iteration</head><p>We developed a "rectangular Newton-Schulz iteration" for computing U V J by adapting Equation <ref type="formula">5</ref>.22 in Higham (2008)'s book for computing the "matrix sign function". We later discovered that this iteration has a long history <ref type="bibr" target="#b24">(Kovarik, 1970;</ref><ref type="bibr" target="#b4">Björck &amp; Bowie, 1971)</ref>. In short, the method works by first normalizing the matrix G according to X 0 " G{}G} ℓ2Ñℓ2 (or alternatively X 0 " G{}G} F ) and then iterating:</p><formula xml:id="formula_22">X t`1 " 3 2 ¨Xt ´1 2 ¨Xt X J t X t , (<label>15</label></formula><formula xml:id="formula_23">)</formula><p>then as t Ñ 8, the sequence X t Ñ U V J . To see this, one can plot the univariate cubic function f pxq :" 3 2 ¨x ´1 2 ¨x3 and see that, for 0 ă x ă ? 3, iterating this cubic will push x closer and closer to `1. The final step is to realize that the effect of the iteration in Equation ( <ref type="formula" target="#formula_22">15</ref>) is to apply this cubic f pxq to each singular value of X t . This shows that the spectral normalization X 0 " G{}G} ℓ2Ñℓ2 is stronger than what is required: we need only ensure that X 0 has singular values no greater than ? 3 for the iteration to converge.</p><p>The iteration in Equation ( <ref type="formula" target="#formula_22">15</ref>) has the advantage over sketching that it always works on all singular values, and since the iteration does not compute inverse matrix roots it is well-behaved even on low-rank matrices.</p><p>Finally, there are in fact a family of degree 2n `1 polynomial iterations of the form</p><formula xml:id="formula_24">X t`1 " a ¨Xt `b ¨Xt X J t X t `c ¨pX t X J t q 2 X t `... `z ¨pX t X J t q n X t (16)</formula><p>for suitable a, b, c, ..., z that could be used instead of Equation ( <ref type="formula" target="#formula_22">15</ref>). One should choose coefficients a, b, c, ..., z so that the univariate polynomial gpxq " a ¨x `b ¨x3 `c ¨x5 `... `z ¨x2n`1 is a suitable approximation to signpxq. One may try to further accelerate the iteration by "tuning" the coefficients a, b, c, ..., z empirically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>This paper develops the theory of modular duality and the procedure of modular dualization as means to construct duality maps for general neural architectures. Here, we comment on implications and connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">A Type System for Deep Learning</head><p>Part of the inspiration for this work is the idea of building a fully-fledged type system for deep learning. We think that activation spaces should be typed by their intended norm and the intended size of activations in that norm. This information would help in the construction of well-normed modules (see Section 4.1).</p><p>Modules should be typed according to Definition 4. And, as suggested in the introduction, gradients should be explicitly typed as dual vectors. A duality map should flip the type of a dual vector to a primal vector. We plan to use the Modula deep learning package <ref type="bibr" target="#b26">(Large et al., 2024)</ref> as a testbed for these ideas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Neural Network Speedrunning</head><p>We believe that the ideas in this paper can help in the design of faster training methods. In fact, a new NanoGPT training speed record was recently set (Jordan, 2024) using a Newton-Schulz-based duality map. We communicated the method to Keller Jordan through our workshop paper <ref type="bibr" target="#b2">(Bernstein &amp; Newhouse, 2024)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Modular Duality: A Unifying Theoretical Framework for Fast and Scalable Training</head><p>An important topic in contemporary optimization research is the design of fast and scalable training methods for neural networks. In fact, the theme of the Optimization for Machine Learning workshop at this year's NeurIPS conference is "scaling up optimization" (OPT, 2024). Two popular methods in this research space are maximal update parameterization <ref type="bibr">(Yang &amp; Hu, 2021, µP)</ref>, which allows for increasing network width without changing the optimal learning rate, and Shampoo <ref type="bibr" target="#b18">(Gupta et al., 2018)</ref>, a variant of which <ref type="bibr" target="#b32">(Shi et al., 2023)</ref> won a speed challenge at the inaugural AlgoPerf optimization competition <ref type="bibr" target="#b10">(Dahl et al., 2023)</ref>.</p><p>We showed in Section 4.1 that essential features of both µP and Shampoo are recovered from the single duality map Linear.dualize. We think that, on a basic theoretical level, µP and Shampoo should be viewed as partial approximations to this duality map. This observation helps put µP and Shampoo on a consistent theoretical footing, orients the methods with respect to overlooked prior work on spectral descent <ref type="bibr">(Carlson et al., 2015b)</ref> and duality structure gradient descent <ref type="bibr" target="#b14">(Flynn, 2017)</ref>, and suggests new ways to generalize these methods to arbitrary layer types and network architectures via the modular norm and modular dualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">On the Alignment of Activations and Updates</head><p>Recent work <ref type="bibr" target="#b38">(Yang et al., 2023;</ref><ref type="bibr" target="#b12">Everett et al., 2024;</ref><ref type="bibr" target="#b26">Large et al., 2024)</ref> has singled out the following question as important to the design of scalable deep learning systems: to what extent do gradient updates to neural network layers align with incoming activation vectors? This question is important since it helps inform how large weight updates need to be to induce a certain amount of change in layer outputs. Duality maps such as Linear.dualize and Conv2D.dualize may help simplify the answer to this question, since they project gradients to scaled semi-orthogonal matrices for which all singular values have the same magnitude.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">A Numerical Paradox: The Weights Don't Change!</head><p>Past work <ref type="bibr" target="#b27">(Lee et al., 2019;</ref><ref type="bibr" target="#b22">Jesus et al., 2021)</ref> has pointed out an apparent paradox in deep learning: the weights seem to move a vanishing amount from initialization in the limit of large network width. This finding has motivated a substantial amount of work on linearized training dynamics <ref type="bibr" target="#b21">(Jacot et al., 2018)</ref>. We attempted to resolve this paradox in prior work by showing that the weights move a roughly constant amount at any width when the change is measured in spectral norm <ref type="bibr" target="#b38">(Yang et al., 2023)</ref>. But duality maps change the story again: Linear.dualize ramps up the stable rank of updates, so the weights should move a non-trivial relative amount at large width even in the Frobenius norm-provided the batch size is not too small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>This paper has proposed a recursive procedure called modular dualization for building duality maps for general neural architectures. The procedure unifies past strands of optimization research on Shampoo <ref type="bibr" target="#b18">(Gupta et al., 2018)</ref> and µP <ref type="bibr" target="#b37">(Yang &amp; Hu, 2021)</ref>. Partial implementations have already led to significant wall-clock speedups in transformer training <ref type="bibr" target="#b23">(Jordan, 2024)</ref>. The rectangular Newton-Schulz iteration provides a GPU-friendly and numerically stable means of dualizing under the RMS Ñ RMS operator norm, while avoiding some of the downsides of sketching-based approaches <ref type="bibr">(Carlson et al., 2015b)</ref>. Overall, we hope that our theory of modular duality provides a clarifying toolkit for the design and analysis of deep learning systems.</p></div>		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>Many ideas in this paper were developed jointly with Tim Large before he left to work at a tech company. We are grateful to <rs type="person">Phillip Isola</rs> for invaluable discussions. We also thank <rs type="person">Jack Gallagher</rs>, <rs type="person">Keller Jordan</rs>, <rs type="person">Simo Ryu</rs>, <rs type="person">Rogier Brussee</rs>, <rs type="person">Tongzhou Wang</rs>, <rs type="person">Victor Butoi</rs>, <rs type="person">Jeffrey Cider</rs> and <rs type="person">Volkan Cevher</rs> for helpful conversations.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Shun-Ichi</forename><surname>Amari</surname></persName>
		</author>
		<title level="m">Information Geometry and Its Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Scalable second order optimization for deep learning</title>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomer</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Regan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.09018</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Old optimizer, new norm: An anthology</title>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laker</forename><surname>Newhouse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Optimization for Machine Learning</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>Cited on pages 3, 6, and 9</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Automatic Gradient Descent: Deep Learning without Hyperparameters</title>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Mingard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navid</forename><surname>Azizan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yisong</forename><surname>Yue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.05187</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>Cited on page 2</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An iterative algorithm for computing the best estimate of an orthogonal matrix</title>
		<author>
			<persName><forename type="first">Åke</forename><surname>Björck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bowie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Numerical Analysis</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lieven</forename><surname>Vandenberghe</surname></persName>
		</author>
		<title level="m">Convex Optimization</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Stochastic spectral descent for restricted Boltzmann machines</title>
		<author>
			<persName><forename type="first">David</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volkan</forename><surname>Cevher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Stochastic spectral descent for discrete graphical models</title>
		<author>
			<persName><forename type="first">David</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ya-Ping</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edo</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volkan</forename><surname>Cevher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Selected Topics in Signal Processing</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Preconditioned spectral descent for deep learning</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">E</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edo</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ya-Ping</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volkan</forename><surname>Cevher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Cited on pages 1, 2, 4, 8, 9, and 10</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Spacetime and Geometry: An Introduction to General Relativity</title>
		<author>
			<persName><forename type="first">Sean</forename><forename type="middle">M</forename><surname>Carroll</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Nado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandramouli Shama</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Hennig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sourabh</forename><surname>Medapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runa</forename><surname>Eschenhagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priya</forename><surname>Kasimbeg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juhan</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abel</forename><forename type="middle">L</forename><surname>Peirson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bilal</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Rabbat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shankar</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Snider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Amid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kongtao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rakshith</forename><surname>Vasudev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Badura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankush</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Mattson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.07179</idno>
		<title level="m">Benchmarking neural network training algorithms</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>Cited on page 9</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Nonlinear Functional Analysis</title>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Deimling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985">1985</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Scaling exponents across parameterizations and optimizers</title>
		<author>
			<persName><forename type="first">Katie</forename><forename type="middle">E</forename><surname>Everett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lechao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Izzeddin</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leslie</forename><surname>Pack Kaelbling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaehoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>Cited on page 9</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sketchy: Memory-efficient adaptive regularization with frequent directions</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Jennifer</forename><surname>Xinyi Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><surname>Hazan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>Cited on page 8</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Flynn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.00523</idno>
		<title level="m">The duality structure gradient descent algorithm: Analysis and applications to neural networks</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Cited on pages 1, 2, 4, 8, and 9</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">An Invitation to Applied Category Theory: Seven Sketches in Compositionality</title>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">I</forename><surname>Spivak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Disciplined Convex Programming</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grant</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD dissertation</note>
	<note>Cited on page 2</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Lecture 3 of CSC2541: Neural Net Training Dynamics</title>
		<author>
			<persName><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><surname>Metrics</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Shampoo: Preconditioned stochastic tensor optimization</title>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomer</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Combinator pattern. Haskell Wiki</title>
		<author>
			<persName><forename type="first">Haskell</forename><surname>Wiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Contributors</forename></persName>
		</author>
		<ptr target="https://wiki.haskell.org/Combinator_pattern" />
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Functions of Matrices</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nicholas</surname></persName>
		</author>
		<author>
			<persName><surname>Higham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Society for Industrial and Applied Mathematics</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>Cited on page 8</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neural tangent kernel: Convergence and generalization in neural networks</title>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Jacot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franck</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Hongler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Cited on page 9</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Effect of initial configuration of weights on training and function of artificial neural networks</title>
		<author>
			<persName><forename type="first">Ricardo</forename><forename type="middle">J</forename><surname>Jesus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mário</surname></persName>
		</author>
		<author>
			<persName><surname>Antunes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><forename type="middle">N</forename><surname>Da Costa</surname></persName>
		</author>
		<author>
			<persName><surname>Dorogovtsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">F</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><forename type="middle">L</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName><surname>Aguiar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Cited on page 9</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">New training speed record for @karpathy&apos;s 124M-parameter NanoGPT setup: 3.28 Fineweb validation loss in 3.7B training tokens</title>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Keller</surname></persName>
		</author>
		<ptr target="https://x.com/kellerjordan0/status/1842300916864844014" />
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="9" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Some iterative methods for improving orthonormality</title>
		<author>
			<persName><forename type="first">Zdislav</forename><surname>Kovarik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Numerical Analysis</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On the computation of the matrix k-th root</title>
		<author>
			<persName><forename type="first">Slobodan</forename><surname>Lakić</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Mathematics and Mechanics</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Scalable optimization in the modular norm</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Large</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minyoung</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyojin</forename><surname>Bahng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>Cited on pages 2, 4, 5, 7, and 9</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Wide neural networks of any depth evolve as linear models under gradient descent</title>
		<author>
			<persName><forename type="first">Jaehoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lechao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasaman</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Cited on page 9</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Randomized numerical linear algebra: Foundations and algorithms</title>
		<author>
			<persName><forename type="first">Per-Gunnar</forename><surname>Martinsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">A</forename><surname>Tropp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Numerica</title>
		<imprint>
			<biblScope unit="page" from="2" to="8" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Problem complexity and method efficiency in optimization</title>
		<author>
			<persName><forename type="first">Arkady</forename><forename type="middle">S</forename><surname>Nemirovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">B</forename><surname>Yudin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Optimization for Machine Learning</title>
		<author>
			<persName><surname>Opt</surname></persName>
		</author>
		<ptr target="https://opt-ml.org/" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>Cited on page 9</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Sakurai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jim</forename><surname>Napolitano</surname></persName>
		</author>
		<title level="m">Modern Quantum Mechanics</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">A distributed data-parallel PyTorch implementation of the distributed Shampoo optimizer for training neural networks at-scale</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Hao-Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Hsien</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shintaro</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Iwasaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijing</forename><surname>Gallego-Posada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaushik</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dheevatsa</forename><surname>Rangadurai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName><surname>Rabbat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.06497</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Universal majorization-minimization algorithms</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Streeter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.00190</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">J</forename><surname>Streeter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.11429</idno>
		<title level="m">Automatically bounding the Taylor remainder series: Tighter bounds and new applications</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Cited on page 2</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<author>
			<persName><forename type="first">Tijmen</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RMSprop. Coursera: Neural Networks for Machine Learning</title>
		<title level="s">Lecture</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fast DNN training based on auxiliary function technique</title>
		<author>
			<persName><forename type="first">Dung</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nobutaka</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Cited on page 2</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Tensor programs IV: Feature learning in infinite-width neural networks</title>
		<author>
			<persName><forename type="first">Greg</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">J</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Cited on pages 1, 6, 9, and 10</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">A spectral condition for feature learning</title>
		<author>
			<persName><forename type="first">Greg</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">B</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Bernstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.17813</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="6" to="9" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
