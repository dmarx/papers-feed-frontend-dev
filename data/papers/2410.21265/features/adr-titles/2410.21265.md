- Decision to adopt a modular duality framework for neural network training.
- Choice of operator norms for individual layers based on semantics.
- Method for constructing duality maps for different layer types.
- Implementation of a recursive approach to induce a duality map on the full weight space.
- Selection of GPU-friendly algorithms for dualizing specific layers (Embed, Linear, Conv2D).
- Justification for the necessity of a duality map in gradient descent updates.
- Decision to focus on the geometry of the loss function in weight space.
- Choice to leverage existing optimization theories (e.g., mirror descent, natural gradient descent).
- Decision to modularize the theory for easier implementation in software.
- Choice of specific prior works to build upon and differentiate from.
- Decision to use a weighted max (L8 combination) for constructing the full duality map.
- Assumptions regarding the smoothness structure of the loss function.
- Decision to derive duality maps for a rich family of neural architectures.
- Choice of specific norms (e.g., RMS, Schatten-8) for layerwise duality maps.
- Decision to emphasize the importance of type checking in gradient updates.
- Choice to explore the relationship between duality maps and existing optimization methods.