{
  "arxivId": "2410.21265",
  "title": "Modular Duality in Deep Learning",
  "authors": "Jeremy Bernstein, Laker Newhouse",
  "abstract": "An old idea in optimization theory says that since the gradient is a dual\nvector it may not be subtracted from the weights without first being mapped to\nthe primal space where the weights reside. We take this idea seriously in this\npaper and construct such a duality map for general neural networks. Our map,\nwhich we call modular dualization, forms a unifying theoretical basis for\ntraining algorithms that are a) fast and b) scalable. Modular dualization\ninvolves first assigning operator norms to layers based on the semantics of\neach layer, and then using these layerwise norms to recursively induce a\nduality map on the weight space of the full neural architecture. We conclude by\nderiving GPU-friendly algorithms for dualizing Embed, Linear and Conv2D layers\n-- the latter two methods are based on a rectangular Newton-Schulz iteration\n(Kovarik, 1970; Bj\\\"orck & Bowie, 1971). A variant of our methods was used to\nset speed records for training NanoGPT. Overall, we hope that our theory of\nmodular duality will yield a next generation of fast and scalable optimizers\nfor general neural architectures.",
  "url": "https://arxiv.org/abs/2410.21265",
  "issue_number": 757,
  "issue_url": "https://github.com/dmarx/papers-feed/issues/757",
  "created_at": "2025-01-04T06:52:09.651323",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 0,
  "last_read": null,
  "last_visited": "2025-01-02T19:43:53.760Z",
  "main_tex_file": null,
  "published_date": "2024-10-28T17:57:31Z",
  "arxiv_tags": [
    "cs.LG",
    "cs.NE",
    "stat.ML"
  ]
}