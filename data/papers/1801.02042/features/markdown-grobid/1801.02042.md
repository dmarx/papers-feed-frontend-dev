# LEARNING FROM NEIGHBORS ABOUT A CHANGING STATE

## Abstract

## 

Agents learn about a changing state using private signals and their neighbors' past estimates of the state. We present a model in which Bayesian agents in equilibrium use neighbors' estimates simply by taking weighted sums with time-invariant weights. The dynamics thus parallel those of the tractable DeGroot model of learning in networks, but arise as an equilibrium outcome rather than a behavioral assumption. We examine whether information aggregation is nearly optimal as neighborhoods grow large. A key condition for this is signal diversity: each individual's neighbors have private signals that not only contain independent information, but also have sufficiently different distributions. Without signal diversity-e.g., if private signals are i.i.d.-learning is suboptimal in all networks and highly inefficient in some. Turning to social influence, we find it is much more sensitive to one's signal quality than to one's number of neighbors, in contrast to standard models with exogenous updating rules.

## Introduction

People learn from others about conditions relevant to decisions they have to make. In many cases, the conditions-e.g., the state of a labor market facing workers, or the business environment relevant to an organization-are changing. Thus, welfare depends not on learning a static "state of the world," but rather on staying up to date with a changing state. The phenomenon of adaptation and responsiveness to new information is central in many economic applications, including in economic development, the study of organizations, and financial decision-making. When is a group of agents successful, collectively, in adapting efficiently to a changing environment? The answers lie partly in the structure of the social networks that shape agents' social learning opportunities. Our model is designed to analyze how a group's adaptability is shaped by the properties of such networks, the inflows of information into society, and the interplay of the two.

We consider overlapping generations of agents who are interested in tracking an unobserved state that evolves over time. [1](#) The state is a Gaussian AR(1) process: somewhat persistent, but with constant innovations to learn about. Each agent, before making a decision, engages in social learning: she observes the actions of some members of prior generations, which reveal their estimates of the state. The social learning opportunities are embedded in a network, in that one's network position determines the neighborhood of peers whom one observes. Neighborhoods reflect geographic, cultural, organizational, or other kinds of proximity. [2](#) In addition to social information, agents also receive private signals about the current state, with Gaussian distributions that may also vary with network position; in particular, some agents may receive more precise information about the state than others.

We give some examples. When a university student begins searching for jobs, she becomes interested in the state of the relevant labor market (e.g., typical wages for someone like her), which naturally vary over time. She uses her own private research (a private signal) but also learns from the choices of others (e.g., recent graduates) who have recently faced a similar problem. Whom she can learn from will depend on her academic specialization, dormitory, extracurricular activities, and so forth: she will predominantly observe predecessors who are "nearby" in these ways. Similarly, when a new cohort of professionals enters a firm such as a management consultancy or law practice, they learn about the business environment from their seniors. Who works with whom, and therefore who learns from whom, is shaped by the structure of the organization. Beyond heterogeneity in network position, agents differ in the precision of the private signals they can access from outside the network: for example, people with quantitative training may be better placed to learn from external statistical reports.

We now detail our three main contributions. The first is to develop a tractable model of learning about a changing state in which Bayesian updating has a simple form: each agent forms an estimate by taking a weighted average of neighbors' estimates and her private signal. Because the environment is stationary, the weights are stationary as well. Equilibrium behavior thus yields a dynamic paralleling the DeGroot (1974) network learning model, which is famous for its tractability, but which has been criticized for its lack of canonical foundations (Molavi et al., 2018; Golub and Sadler, 2016). The weights in agents' learning rules are endogenously determined because, when each agent extracts information from neighbors' estimates, the information content of those estimates depends on the neighbors' learning rules. We characterize these weights and the distributions of behavior in a stationary equilibrium. [3](#) These characterizations permit the analysis of comparative statics of behavior and welfare. The model is well-behaved computationally: equilibria can be calculated quickly in networks of thousands of nodes, which makes the framework useful for structural exercises. Finally, the basic framework and stationary equilibrium definition extend readily to accommodate various kinds of behavioral updating rules, e.g., ones that neglect correlations in neighbors' actions.

Our second contribution is to analyze when equilibrium learning facilitates good information aggregation. Here we have positive and negative results. The main positive finding is that Bayesian agents in equilibrium can achieve good aggregation under a signal diversity condition. To formalize "good aggregation," we first note that in our model social information is valuable to agents insofar as it allows them to estimate the state before the current period; our measure of aggregation quality is the accuracy of these estimates. We say good aggregation occurs if each agent has an estimate nearly as precise as if she simply knew the previous state. We say signal diversity holds if each individual has at least two different private signal precisions represented in large numbers in her neighborhood. The positive result says that signal diversity is sufficient for good aggregation, robustly across a large class of random networks (ones arising from stochastic block models satisfying certain technical conditions). Signal diversity is valuable because it leads to diversity of neighbors' strategies: it makes them use recent and older information differently from one another. An agent observing such neighbors, in turn, can use the diversity for statistical identification, constructing a precise estimate of the most recent state. We illustrate this key idea in an example at the end of the introduction.

We complement the positive finding with two negative results showing that both the "signal diversity" and the "Bayesian" conditions are important. First, signal homogeneity turns out [3](#) In Bayesian models of learning about a fixed state there is a time-dependence whereby rational updating rules depend on the time elapsed since the learning process started. Studies of such models often focus on an eventual rate of learning about a fixed state. See, for instance, Molavi, Tahbaz-Salehi, and Jadbabaie (2018)  and Harel, Mossel, Strack, and Tamuz (2021). This time-dependence is absent in our stationary environment; equilibrium outcomes can be summarized by steady-state updating weights and error rates.

to be a fundamental obstruction to good aggregation. In an environment where everyone's private signals are conditionally independent and identically distributed, equilibrium aggregation is bound to be inefficient. We begin by establishing this point in highly symmetric networks, where the failure of aggregation is shown to have severe welfare consequences, making each agent worse off by an unbounded amount relative to a world with signal diversity. A more general finding is that in large networks with signal homogeneity, it is impossible in equilibrium to achieve accuracies of aggregation of the same order as in our positive result under signal diversity. One might have thought that diversity of neighbors' network positions (and thus learning opportunities) can substitute for diversity of their private signal distributions; our negative result shows that network diversity is a poor substitute for signal diversity.

We next show that the "Bayesian" condition is also important for the good aggregation result. To do this, we contrast the learning of Bayesians best-responding to others' learning rules with that of a naive population too unsophisticated to account for correlations in neighbors' learning errors, as in some canonical behavioral learning models (Eyster and Rabin,  2010). [4](#) We identify a class of such models in which information aggregation is essentially guaranteed to fall short of good aggregation benchmarks for all agents. The deficiencies of naive learning rules are different from and more severe than those in similar problems with an unchanging state, where naive heuristics can aggregate information very well. [5](#) Our third contribution is to study social influence, an outcome of central importance in network theory. We define a notion of steady-state social influence-how an idiosyncratic change in an individual's information affects others' average behavior. This is analogous to the definition of influence in the standard DeGroot model (where updating weights are exogenous). The endogenous determination of weights makes a big difference for how the environment affects social influence. Relative to the DeGroot model benchmark studied in DeMarzo, Vayanos, and Zweibel (2003), an agent's social influence is much more sensitive to the quality of her private information. On the other hand, just as in the standard benchmark, an agent's influence is approximately proportional to her degree.

A closing discussion makes two main points. First, some of our theoretical aggregation results use large random graphs. We perform a numerical exercise to show that the main message about information aggregation-diversity of signal types helps learning-remains valid when we calculate equilibria on graphs reflecting real social networks. Second, our analysis generalizes readily to richer models of multidimensional states and signals. As one application of such a generalization, we consider a manager who wishes to facilitate better learning in an organization, and ask what distributions of expertise such a designer would prefer. Our results provide a distinctive rationale for informational specialization as a design feature that facilitates good information aggregation.

An example. We now present a simple example that illustrates our dynamic model, highlights obstacles to learning that distinctively arise in a dynamic environment, and gives a sense of some of the main forces that play a role in our results on the quality of learning.

Consider a particular environment, with a single perfectly informed source S; many media outlets M 1 , . . . , M n with access to the source as well as some independent private information; and the general public. The public consists of many individuals who learn only from the media outlets. We are interested in how well each member of the public could learn by following many media outlets. More precisely, we consider the example shown in Figure [1](#fig_3).1 and think of P as a generic member of the large public. The state θ t follows a Gaussian random walk: θ t = θ t-1 + ν t , where the innovations ν t are standard normal. Each period, the source learns the state θ t and takes an action (which can be thought of simply as making an announcement) that reveals it. The media outlets observe the source's action from the previous period, which is θ t-1 . At each time period, they also receive noisy private signals, s M i ,t = θ t + η M i ,t with normally distributed, independent, mean-zero errors η M i ,t . They then announce their posterior means of θ t , which we denote by a M i ,t . The member of the public, in a given period t, makes an estimate based on the observations a M 1 ,t-1 , . . . , a Mn,t-1 of media outlets' actions in the previous period. All agents are short-lived: they see actions in their neighborhoods one period ago, and then they take an action that reveals their posterior belief of the state.

If we had an unchanging state but the same signals and observation structure, learning would trivially be perfect: media outlets would learn the state from the source and report it to the public. In the dynamic environment, given that P has no signal, she can at best hope to learn θ t-1 (and use that to estimate θ t ). Can this benchmark be achieved, and if so, when?

A typical estimate of a media outlet at time t is a linear combination of s M i ,t and θ t-1 (the latter being the information that the media outlets learned from the source S). In particular, the estimate of media outlet M i at time t can be expressed as

$a M i ,t = w i s M i ,t + (1 -w i )θ t-1 ,$where the weight w i on the media outlet's signal is increasing in the precision of that signal. We give the public no private signal, for simplicity only.

Suppose first that the media outlets have identically distributed private signals. Because the member of the public observes many symmetric media outlets, it turns out that her best estimate of the state, a P,t , is simply the average of the estimates of the media outlets. Since each of these outlets uses the same weight w i = w on its private signal, we may write

$a P,t = w n i=1 s M i ,t-1 n + (1 -w)θ t-2 ≈ wθ t-1 + (1 -w)θ t-2 .$That is, P 's estimate is an average of media private signals from last period (t -1), combined with what the media learned from the source, which tracks the state in the period before that (t -2). In the approximate equality, we have used the fact that an average of many private signals is approximately equal to the state, by our assumption of independent errors. No matter how many media outlets there are, and even though each has independent information about θ t-1 , the public's beliefs are confounded by older information. What if, instead, half of the media outlets (say M 1 , . . . , M n/2 ) have more precise private signals than the other half, perhaps because these outlets have invested more in expertise on this topic? The media outlets with more precise signals, called group A, will then place weight w A on their private signals, while the media outlets with less precise signals (group B) use a smaller weight, w B . We will now argue that a member of the public can extract more information from the media in this setting. In particular, she can first compute the averages of the two groups' actions type A average action at time t -1

$= w A n/2 i=1 s M i ,t-1 n/2 + (1 -w A )θ t-2 ≈ w A θ t-1 + (1 -w A )θ t-2 type B average action at time t -1 = w B n i=n/2+1 s M i ,t-1 n/2 + (1 -w B )θ t-2 ≈ w B θ t-1 + (1 -w B )θ t-2 .$Then, since w A > w B , the public knows two distinct linear combinations of θ t-1 and θ t-2 . The state θ t-1 is identified from these. So the member of the public can form a very precise estimate of θ t-1 -which, recall, is as well as she can hope to do. The key force is that the two groups of media outlets give different mixes of the old information and the more recent state, and by understanding this, the public can infer both. Indeed, to recover θ t-1 , the public puts a negative weight on the actions of media outlets of type B, which allows it to subtract off old information and focus on the recent state, θ t-1 . One can show that if, in contrast, agents are naive, e.g., if they think that all of the estimates of the media are uncorrelated (or only mildly correlated) conditional on the state, they will put positive weights on their observations and will again be bounded from learning the state. This illustration relied on a particular network with several special features: a very "central" source, one-directional links, and no communication among the media outlets or the public. We will show that the same considerations determine learning quality in a large class of random networks in which agents have many neighbors, with complex connections among them. Quite generally, if each neighborhood contains a diversity of signal types, agents can concentrate on new developments in the state while filtering out old, less relevant information and thus estimate the changing state as accurately as physical constraints allow.

Outline. Section 2 sets up the basic model and discusses its interpretation. Section 3 defines equilibrium, shows its existence, and characterizes it. Section 4 reports our main theoretical results on the quality of information aggregation. In Section 5, we discuss learning outcomes under a variety of non-Bayesian models. Section 6 defines and analyzes social influence. Section 7 relates our model and results to the social learning literature. In Section 8, we describe our numerical exercise with network data from Indian villages and discuss a simple extension to multi-dimensional states to interpret our results on signal diversity.

## Model

State of the world. There is a discrete set of instants, T = Z = {. . . , -2, -1, 0, 1, 2, . . .} . At each time t ∈ T , there is a state, a random variable θ t taking values in R. This state evolves as an AR(1) stochastic process. That is,

$θ t+1 = ρθ t + ν t+1 , (2.1)$where ρ is a constant with 0 < |ρ| ≤ 1 and ν t+1 ∼ N (0, σ 2 ν ) are independent innovations. When |ρ| < 1 we have the explicit formula

$θ t = ∞ =0 ρ ν t-,$and thus the state at any time t has the stationary distribution θ t ∼ N 0, σ 2 ν 1-ρ 2 . We maintain the normalization throughout that innovations have variance 1, i.e., σ ν = 1.

We will occasionally examine an alternative specification (making our departure from the main model explicit) where there is a starting time, so that T = Z ≥0 = {0, 1, 2, . . .}, and the state process is defined as in (2.1) starting at time 0 with some specified distribution for θ 0 .

Information and observations. There is a set of nodes is N = {1, 2, . . . , n}. Each node i can be thought of as a location, and is associated with a set N i ⊆ N of nodes that i can observe, called its neighborhood. 6 t = -1 . . .

## • • •

(i, -1) observes t = -1 signal and acts t = 0 (i, 0) born and observes t = -1 actions (i, 0) observes t = 0 signal and acts t = 1

• • • (i, 1) born and observes t = 0 actions (i, 1) observes t = 1 signal and acts Figure 2.1. An illustration, with memory m = 1, of the overlapping generations at a node i. At time t -1, agent (i, t) is born and observes contemporaneous actions in her neighborhood. At time t, she observes her private signal s i,t and takes her action a i,t .

Each node is populated by a sequence of agents in overlapping generations. For each time t, there is a node-i agent, labeled (i, t), who takes that node's time-t action a i,t . This agent is born at time tm and has m periods to observe the actions taken in her neighborhood before she acts. Thus, when taking her action, the agent (i, t) knows a j,t-for all nodes j ∈ N i and all lags ∈ {1, 2, . . . , m}. We call m the memory; it reflects how many periods of actions in her neighborhood an agent passively observes before acting. (See Figure [2](#).1 for an illustration.) One interpretation is that a node corresponds to a role in an organization. A worker in that role has some time to observe colleagues in related roles before choosing a once-and-for-all action herself. Much of our analysis is done for an arbitrary finite m; we view the restriction to finite memory as useful for avoiding technical complications, but because m can be arbitrarily large, this restriction has little substantive content. [7](#) In addition to social information from her neighborhood, each agent also observes a private signal,

$s i,t = θ t + η i,t ,$where the error term η i,t ∼ N (0, σ 2 i ) has a variance σ 2 i > 0 that depends on the node but not on the time period. All the errors η i,t and state innovations ν t are independent of one another. An agent's information is a vector consisting of her private signal and all of her social observations. An important special case will be m = 1, where agents observe only one period of others' actions before acting themselves, so that the agent's information is (s i,t , (a j,t-1 ) j∈N i ).

The network G = (N, E) is the set of nodes N together with the (fixed) set of links E, defined as the subset of pairs (i, j) ∈ N × N such that j ∈ N i . This network (also called a graph), which determines the observation structure, is common knowledge, as is the informational environment (i.e., the joint distribution of all exogenous random variables).

An environment is specified by (G, σ), where σ = (σ i ) i∈N is the profile of signal variances.

Preferences and best responses. When an agent (i, t) makes her once-and-for-all choice a i,t ∈ R, her utility is given by

$u i,t (a i,t ) = -E[(a i,t -θ t ) 2 ].$(2.2) By a standard fact about squared-error loss functions, given the distribution of (a N i ,t-) m =1 , the optimal choice of agent (i, t) is to set her action equal to her expectation of the state:

$a i,t = E[θ t | (a N i ,t-) m =1 , s i,t i's information$].

(2.3)

Here the notation a N i ,t refers to the vector (a j,t ) j∈N i of time-t actions in the agent's neighborhood. An action can be interpreted as an agent's estimate of the state, and we will sometimes use this terminology. The conditional expectation (2.3) depends, of course, on the prior of agent (i, t) about θ t , which, under correctly specified beliefs, has distribution θ t ∼ N 0, σ 2 ν 1-ρ 2 . We allow the prior to be any normal distribution or a diffuse improper prior. 8 It saves on notation to analyze the case where all agents have improper priors. Because actions under a normal prior are related to actions under the improper prior by a simple linear bijection-and thus have the same information content for other agents-all results immediately extend to the general case.

The doubly-infinite time axis introduces some subtleties into the definition of strategy profiles; complete details are formalized in online Appendix OA3.

## Updating and equilibrium

In this section we study agents' learning behavior and present a notion of stationary equilibrium. We begin with the canonical case of Bayesian agents with correct models of others' behavior; we study other behavioral assumptions in Section 5 below.

## 3.1.

Best-response behavior. The first step is to analyze optimal updating behavior in response to others' strategies. A strategy of an agent is linear if the action taken is a linear function of the variables she observes. We will analyze agents' best responses to linear strategies, showing that they are linear and computing them explicitly. 9

Fix an agent (i, t) and some linear strategy profile played before time t. By (2.3), this agent's best-response action a i,t is her conditional expectation of θ t given her information,

$E[θ t | (a N i ,t-) m =1 , s i,t ].$Each action before time t can be written as a (possibly infinite) sum of past signals s j,t . It follows that all random variables appearing in the conditional expectation are jointly Gaussian. That implies that a

$i,t = E[θ t | (a N i ,t-) m =1 , s i,t ] is an affine function of s i,t8$We take priors about θ t , like the information structure and network, to be common knowledge.

This analysis applies both to the main T = Z model and the alternative with T = Z ≥0 . For a discussion of why it is natural to consider linear opponent strategies, see Section 3.2 below.

and (a N i ,t-) m =1 (see Eaton 1983, Section 4.3). We now analyze this conditional expectation in detail.

## Actions as estimates of states:

A key covariance matrix. Agents learn partly from past agents' actions, so the joint distribution of actions as estimates of the state at an arbitrary time t will be important to track. An arbitrary agent's scaled past action ρ a i,t-gives an estimate of the state in the sense that E[θ t | ρ a i,t-] = ρ a i,t-. Turning to second moments, define the covariance matrix of the errors in these estimates over the most recent m periods:

$V t = Cov ρ a i,t--θ t i∈N 0≤ ≤m-1 .$In the case m = 1, we have V t = Cov (a i,tθ t ) i∈N . We will often refer to V t simply as the covariance matrix of the model, as it will play a central role in our subsequent analysis.

3.1.2. Best-response weights. The information of agent (i, t) at time t may be represented as a random vector

$z i,t = ρ a j,t-j∈N i 1≤ ≤m , s i,t .$We will calculate the conditional expectation E[θ t | z i,t ] in terms of a covariance matrix constructed from (i, t)'s observations,

$C i,t-1 = Cov(z i,t -θ t 1) = ρ 2 V N i ,t-1 + 11 0 0 σ 2 i ,$where V N i ,t-1 is the submatrix of V t-1 corresponding to indices in i's neighborhood.[foot_6](#foot_6) Now we have the following formula for best-response actions, which, for simplicity, we give in the case where the agent has an improper prior[foot_7](#foot_7) about θ t .

$a i,t = 1 C -1 i,t-1 1 C -1 i,t-1 1 agent (i, t)'s weights •       ρa N i ,t-1 . . . ρ m a N i ,t-m s i,t       agent (i, t)'s observations . (3.1)$Expression (3.1) is a linear combination of the agent's signal and the observed actions; the weights in this linear combination depend on the matrix V t-1 , but not on realizations of any random variables. Section OA4.1 of the online Appendix gives the details of the standard calculations underlying the formula.

Our analysis extends immediately to any proper normal prior for θ t : To get an agent's estimate of θ t , the formula in (3.1) would simply be averaged with a constant term accounting for the prior, and everyone could invert this deterministic operation to recover the same information from others' actions.

We denote by (W t , w s t ) a weight profile in period t, with w s t ∈ R n being the weights agents place on their private signals and W t being the weights they place on their other information.

3.1.3. The evolution of covariance matrices under best-response behavior. Assuming agents best-respond according to the optimal weights just described in (3.1), we can compute the resulting next-period covariance matrix V t from the previous covariance matrix. Letting V be the space of covariance matrices, this defines a map Φ : V → V, given by

$Φ : V t-1 → V t . (3.2)$This map gives the basic dynamics of the model: how an arbitrary variance-covariance matrix V t-1 maps to a new one when all agents best-respond to V t-1 . The variance-covariance matrix V t-1 (along with parameters of the model) determines (i) the weights agents place on their observations in (3.1), and (ii) the distributions of the random variables that are being combined in this expression. This yields the deterministic updating dynamic Φ. A consequence is that the weights agents place on observations are (commonly) known, and do not depend on any random realizations.

Example 1. We compute the map Φ explicitly in the case m = 1. We refer to the weight agent (i, t) optimally places on ρa j,t-1 as W ij and the weight on s i,t , her private signal, as w s i . Note we have, from (3.1) above, explicit expressions for these weights. Then

$[Φ(V )] ii = (w s i ) 2 σ 2 i + k,k W ik W ik (ρ 2 V kk + 1) and [Φ(V )] ij = k,k W ik W jk (ρ 2 V kk + 1).$(3.3) 3.2. Stationary equilibrium in linear strategies. We will now turn our attention to stationary equilibria in linear strategies-ones in which all agents' strategies are linear with time-invariant coefficients-though, of course, we will allow all agents to consider deviating to arbitrary strategies, including non-linear ones. Once we establish the existence of such equilibria, we will use the word equilibrium to refer to one of these unless otherwise noted.

A reason for focusing on equilibria in linear strategies comes from noting that, in the variant of the model with a starting time (i.e., the case T = Z ≥0 ) agents begin by using only private signals, and they do this linearly. After that, inductively applying the reasoning of Section 3.1, best-responses are linear at all future times. Taking time to extend infinitely backward is an idealization that allows us to focus on exactly stationary behavior.

We now show the existence of stationary equilibria in linear strategies.

Proposition 1. A stationary equilibrium in linear strategies exists, and is associated with a covariance matrix V such that Φ( V ) = V .

The proof appears in Appendix A, and we sketch the key ideas below.

At such an equilibrium, the covariance matrix V t and all agent strategies are time-invariant. Actions are linear combinations of observations with stationary weights, which we denote by W ij and w s i . The form of these rules has some resemblance to static equilibrium notions studied in the rational expectations literature (e.g., Vives, 1993; Babus and Kondor, 2018;  Lambert, Ostrovsky, and Panov, 2018; Mossel, Mueller-Frank, Sly, and Tamuz, 2020). It also has a similar form to the DeGroot (1974) and Friedkin and Johnsen (1997) updating rules, typically imposed as behavioral heuristics. In our dynamic environment, such a solution emerges as a stationary equilibrium.

## 3.2.1.

Proof sketch for the existence result. The goal is to apply the Brouwer fixed-point theorem to show there is a covariance matrix V that remains unchanged under updating. To find a convex, compact set to which we can apply the fixed-point theorem, we use the fact that when all agents best-respond to any beliefs about prior actions, all action variances lie in a compact set of positive numbers. This is because all agents' actions must be at least as precise in estimating θ t as their private signals, and cannot be more precise than estimates given perfect knowledge of θ t-1 combined with the private signal. This establishes bounds on action variances. The Cauchy-Schwartz inequality then bounds covariances in terms of corresponding variances. All matrices respecting these bounds constitute a compact, convex set containing the image of Φ. This and the continuity of Φ allow us to apply the Brouwer fixed-point theorem.

3.2.2. Other remarks. In the case of m = 1, we can use the formula of Example 1, equation (3.3), to write the fixed-point condition Φ( V ) = V explicitly. More generally, for any m, equation (3.1) gives a formula in terms of V for the weights W ij and w s i in the best response to V , and this can be used to describe the equilibrium V ij as solving a system of polynomial equations. These equations typically have large degree and cannot be solved analytically except in very simple cases, but they can readily be used to solve for equilibria numerically. A related feature of the model is that standard methods can easily be applied to estimate it and test hypotheses within it (see Appendix OA2 for details).

The main insight is that we can analyze equilibria through action covariances. This idea applies equally to many extensions and variations of our basic model, as illustrated by two examples: (1) We assume that agents observe neighbors perfectly, but one could define other observation structures. For instance, agents could observe actions with noise, or they could observe some set of linear combinations of neighbors' actions with noise. Similarly, agents could be observing predecessors' actions for heterogeneous durations before acting (i.e., nodespecific m). (2) We assume agents are Bayesian and best-respond to the true distribution of actions, but the same proof would also show that equilibria exist under other behavioral rules (see Section 5.1). [12](#) Proposition 1 shows that there exists a stationary linear equilibrium. We show later, as part of Proposition 2, that there is a unique stationary linear equilibrium in networks having a particular structure. In general, uniqueness of the equilibrium is an open question that we leave for future work. [13](#) In Section 4.2.3 and Appendix OA5, we discuss the T = Z ≥0 variant of the model, which has a unique equilibrium, and relate it to our main model.

How much information does each agent need to play her equilibrium strategy? In a stationary equilibrium, she only needs to know the steady-state variance-covariance matrix V N i in her neighborhood. Then her problem of inferring θ t-1 becomes essentially a linear regression problem. If historical empirical data on neighbors' error variances and covariances are available, then V N i can be estimated from such data.

## How good is information aggregation in equilibrium?

In this section we analyze the quality of information aggregation in stationary equilibrium. First, recall that in any agent's time-t decision problem, θ t-1 is a sufficient statistic for social information, because the difference θ tρθ t-1 is independent of all actions taken at or before time t -1. Let the social signal of agent (i, t) be defined as her estimate of θ t-1 based on social information:

$r i,t = E[θ t-1 | (a N i ,t-) m =1$]. We will be interested in the error in this estimate: Definition 1. For a given strategy profile, define the aggregation error κ 2 i,t = Var(r i,tθ t-1 ) to be the expected squared error in the social signal as a prediction of θ t-1 .

The aggregation error measures how well an agent can extract information from social observations. Note that agent i's aggregation error is a monotone transformation of her expected utility. [14](#) How efficient is aggregation? The environment features informational externalities: players do not internalize the impact of their learning rules on others' learning. Consequently, there is no reason to expect outcomes to be efficient in any exact sense. And we have seen that the details of equilibrium in a particular network can be complicated. However, it turns out [12](#) What is important in the proof is that actions depend continuously on the covariance structure of an agent's observations; the action variances are uniformly bounded under the rule agents play; and there is a vanishing dependence of behavior on the very distant past. [13](#) We have checked numerically that Φ is not, in general, a contraction in various norms (entrywise sup, Euclidean operator norm, etc.). In computing equilibria numerically for many examples, we have not been able to find a case of equilibrium multiplicity. Indeed, in all of our numerical examples, repeatedly applying Φ to an initial covariance matrix converges to the same fixed point for any starting conditions. 14 In fact, for any decision dependent on θ t , an agent is better off with a lower value of κ 2 i,t . This is a consequence of the fact that unidimensional Gaussian signals can be Blackwell ordered by their precision.

that much more can be said about the behavior of aggregation errors as neighborhood sizes grow. In this section, we study the asymptotic efficiency of information aggregation. We give conditions under which aggregation error decays as quickly as physically possible, and different conditions under which it remains far from efficient levels even when agents have arbitrarily many observations. We discuss the case m = 1 for simplicity but the reasoning extends easily to other values of m.

A benchmark lower bound on aggregation error. A first observation is a lower bound on the aggregation error (in terms of an asymptotic rate as a function of a node's degree) under any behavior of agents. This establishes a benchmark relative to which we can assess equilibrium outcomes.

Let d i denote the out-degree of a node i.

Fact 1. Fix ρ ∈ (-1, 1) as well as upper and lower bounds for private signal variances, so that σ 2 i ∈ [σ 2 , σ 2 ] for all i. On any network and for all strategy profiles, we have κ 2 i,t ≥ c/d i for all i and t, where c is a constant that depends only on ρ, σ 2 , and σ 2 .

The lower bound is reminiscent of the central limit theorem: if an agent had d i conditionally independent noisy signals about θ t-1 (e.g., by observing neighbors' private signals directly), then the variance of her estimate would be of order 1/d i . Fact 1 notes that it is not possible for aggregation errors to decay (as a function of degree) any faster than that.

For an intuition, imagine that an agent sees neighbors' private signals (not just actions) one period after they are received, and all other private signals two periods after they are received; this clearly gives an upper bound on the quality of the agent's possible aggregation given physical communication constraints. The information that is two periods old cannot be very informative about θ t-1 because of the movement in the state from period t -2 to t -1; a large constant number z of signals about θ t-1 would be better. Thus, a lower bound on aggregation error is given by the error that could be achieved with d i + z independent signals about θ t-1 of the best possible precision (σ -2 ). The bound follows from these observations. Outline of results: When is aggregation comparable to the benchmark? Fact 1 places a lower bound on aggregation error given the physical constraints. Even efficient learning could not do better than this bound. We examine when equilibrium learning can achieve aggregation of similar quality. More precisely, we ask when there is a stationary equilibrium where the aggregation error at node i satisfies κ 2 i ≤ C/d i for all i, for some constant C.

In Section 4.2 we establish a good-aggregation result: outcomes comparable to the benchmark are achieved in equilibrium in a class of networks. The key condition enabling the asymptotically efficient equilibrium outcome is called signal diversity: each individual has access to enough neighbors with multiple different kinds of private signals. The fact that neighbors use private information differently turns out to give the agents enough power to identify θ t-1 with equilibrium aggregation error that decays at a rate matching the lower bound of Fact 1 up to a multiplicative constant.

In Section 4.3, we turn to negative results. Without signal diversity, equilibrium aggregation can be extremely bad. Our first negative result shows that when signals are exchangeable, it may be that the aggregation error κ 2 i does not approach zero in any equilibrium, no matter how large neighborhoods are, though a social planner could achieve good aggregation by prescribing different updating weights. We prove this in highly symmetric networks. Once we move away from such networks, one might ask whether diversity in individuals' network positions could play a role analogous to signal diversity and enable approximately efficient learning. Our next negative result shows that this is impossible. When signals are homogeneous and all agents' degrees in network G n are bounded by d(n) (where d(n) is any unbounded sequence) then in any equilibrium, it cannot be that almost all aggregation errors are less than C/d(n) as the network grows, for any number C > 0 not depending on n.

4.1. Distributions of networks and signals. For our good-aggregation result, we study large populations and specify two aspects of the environment: network distributions and signal distributions. In terms of network distributions, we work with a standard type of random network model-a stochastic block model (see, e.g., Holland, Laskey, and Leinhardt,  1983 and Abbe, 2017). It makes the structure of equilibrium tractable while also allowing us to capture rich heterogeneity in network positions. We also specify signal distributions: how signal precisions are allocated to agents, in a way that may depend on network position. We now formalize these two primitives of the model and state the assumptions we work with.

Fix a set of network types k ∈ K = {1, 2, . . . , K}. For each pair of network types, there is a given probability p kk that each agent of network type k has a link to each agent of network type k . An assumption we maintain on these probabilities is that each network type k observes at least one network type (possibly k itself) with positive probability. There is also a vector (α 1 , . . . , α K ) of population shares of each network type, which we assume are all positive. Jointly, (p kk ) k,k ∈K and α specify the network distribution. These parameters can encode differences in expected degree and also features such as homophily (where some groups of types are linked to each other more densely than to others).

We next define signal distributions, which describe the allocation of signal variances to network types. Fix a finite set S of private signal variances, which we call signal types. [15](#) We let q kτ be the share of agents of network type k with signal type τ ; then (q kτ ) k∈K,τ ∈S defines the signal distribution.

Let the nodes in network n be partitioned into the network types N 1 n , N 2 n , . . . , N K n , with the cardinality |N k n | equal to α k n or α k n (rounding so that there are n agents in the network). We (deterministically) set the signal variances σ 2 i equal to elements of S in accordance with the signal shares (again rounding as needed). Let (G n ) ∞ n=1 be a sequence of directed or undirected random networks with these nodes, so that i ∈ N k n and j ∈ N k n are linked with probability p kk ; these link realizations are all independent.

In our setting, a stochastic block model D is specified by the linking probabilities (p kk ) k,k ∈K , the type shares α, and the signal distribution (q kτ ) k∈K,τ ∈S . We let (G n (D), σ n (D)) denote the environment (i.e., the network and the signal variances) in a random realization. We say that a network type k contains a signal type τ if q kτ > 0.

Definition 2. A stochastic block model satisfies signal diversity if each network type has a positive probability of linking with at least one network type containing two distinct signal types.

4.2. Good aggregation under diverse signals. Our first main result is that signal diversity is sufficient for good aggregation in the networks described in the previous section.

Aggregation error decays at a rate C/d i for each node i independently of the structural properties of the network.

We first define a notion of good aggregation for an agent in terms of a bound on that agent's aggregation error. Definition 3. Given ε > 0, we say that agent i achieves the ε-aggregation benchmark in a given equilibrium if the aggregation error satisfies κ 2 i ≤ ε.

We say an event (indexed by n) occurs asymptotically almost surely if the probability of the event converges to 1 as n → ∞.

Theorem 1. Fix any stochastic block model D satisfying signal diversity. There exists C > 0 such that asymptotically almost surely the environment (G n (D), σ n (D)) has an equilibrium where all agents achieve the C/n-aggregation benchmark.

So for large enough n, societies with signal diversity are very likely to aggregate information very well. The uncertainty in this statement is over the network, as there is always a small probability of a realized network which prevents learning (e.g., an agent has no neighbors). We give an outline of the argument next, and the proof appears in Appendix B.

The constant C in the theorem statement can depend on the stochastic block model D. However, given any compact set of stochastic block models D, we can choose a single C > 0 for which the result holds uniformly across D. [16](#) Thus, the theorem can be applied without detailed information on how the random graphs are generated, as long as some bounds are known about which models are possible. 4.2.1. Discussion of the proof. To give intuition for Theorem 1, we first describe why the theorem holds on the complete network[foot_10](#foot_10) with two signal types A and B in the m = 1 case. This echoes the intuition of the example in the introduction. We then discuss the challenges involved in generalizing the result to our general stochastic block model networks, and the techniques we use to overcome those challenges.

Consider a time-t agent, (i, t). Recall that the social signal r i,t is the optimal estimate of θ t-1 based on the actions (i, t) has observed in her neighborhood. In the complete network, all agents have the same social signal, which we call r t . [18](#) At any equilibrium, each agent's action is a weighted average of her private signal and this social signal.

$a i,t = w s i s i,t + (1 -w s i )r t . (4.1)$The weights on the two random variables on the right-hand side sum to 1 because both s i,t and r t are unbiased estimates of θ t , and so is the left-hand side a i,t . The weight w s i on the private signal depends on the precision of this signal relative to the social signal. We call the weights used by agents of the two distinct signal types w s A and w s B . Suppose signal type A is more precise than signal type B, so that w s A > w s B . Now, turning our attention to the next period of updating, observe that each time-(t + 1) agent can compute two averages of the time-t actions-one for each signal type. Using (4.1) to rewrite a i,t and then plugging in s i,t = θ t + η i,t :

$type A average action at time t = 1 n A i:σ 2 i =σ 2 A a i,t = w s A θ t + (1 -w s A )r t + O p (n -1/2 ), type B average action at time t = 1 n B i:σ 2 i =σ 2 B a i,t = w s B θ t + (1 -w s B )r t + O p (n -1/2 ).$Here n A and n B denote the numbers of agents of each type (recalling we assumed each type is a positive share of the population size, n). The O p (n -1/2 ) error terms 19 come from the average signal noises η i,t of agents in each group; the bound holds with high probability by the central limit theorem. In other words, each time-(t+1) agent can obtain precise estimates of two different convex combinations of θ t and r t . Because the two weights, w s A and w s B , are distinct, she can approximately (up to signal error) solve for θ t as a linear combination of the average actions taken by each type she observes. It follows the agent must have an estimate at least as precise as what she can obtain by the strategy we have described, and will thus be very close the benchmark. Since the equilibrium in question was arbitrary, this shows that aggregation approaches the benchmark in any equilibrium. The estimator of θ t in this strategy places negative weight on 1

$n B i:σ 2 i =σ 2 B a i,t$, thus anti-imitating the agents of signal type B-those with the less precise private signal. The logic of Proposition 3 in Section 5.2 implies that anti-imitation necessarily occurs in any equilibrium in which agents aggregate information precisely.

To extend the ideas just presented to the more general setting of Theorem 1, we need to show that each individual observes a large number of neighbors of at least two signal types who also have similar social signals. More precisely, the proof shows that agents with the same network type have highly correlated social signals. Showing this is much more subtle than it was in the above illustration. In general, the social signals in an arbitrary network realization are endogenous objects that depend to some extent on all the links.

A key insight allowing us to overcome this difficulty is a useful general fact about sufficiently dense stochastic block models: despite a lot of idiosyncratic randomness in direct connections, the law of large numbers implies the number of paths of length two between any agent i of type k and any agent j of type k going through an agent of type k is nearly determined by the types k, k , and k , with a small relative error. [20](#) We can leverage this to deduce some important facts about the updating map Φ (recall Section 3.1.3) in the realized random network, and specifically about the evolution of social signals.

In particular, if we look at the set of covariance matrices where all social signals are close to perfect, we can show that the composition Φ 2 := Φ • Φ maps this set to itself. In other words, if social signals are very precise, then they will remain very precise two periods later. If the two-step path counts were determined by types exactly, it would not be too difficult to show this by elaborating the reasoning in the complete graph example, because neighbors of the same type would be effectively exchangeable. We show that despite the path counts being known only approximately, the desired conclusion holds. This is nontrivial because the weights agents use in their updating-and thus the evolution of social signals-could depend sensitively on realized network structure; small relative errors could matter. A key step is to develop results on matrix perturbations to show that small relative changes in the network actually do not affect Φ 2 too much. A fixed-point theorem then implies there is a fixed point of Φ 2 in the set of outcomes with very precise social signals. With some further analysis we can deduce that this implies the existence of an equilibrium (corresponding to a fixed point of Φ) with nearly perfect aggregation. 4.2.2. Sparser random graphs. In the random graphs we have defined in Section 4.1, the group-level linking probabilities (p kk ) are, for simplicity, held fixed as n grows. This yields expected degrees that grow linearly in the population size, which may not be the desired asymptotic model. We can, however, establish versions of our results in a class of models much more flexible with respect to degrees. While it is important to have neighborhoods "large enough" (i.e., growing in n) to permit the application of laws of large numbers, their rate of growth can be considerably slower than linear. For example, our proof can be extended directly to degrees that scale as n α for any α > 0 to show that asymptotically almost surely, there exists an equilibrium where the C/n α -aggregation benchmark is achieved for all agents. Instead of studying Φ 2 and two-step paths, one can extend the same sort of analysis to the L-fold composition Φ L , which reflects L-step paths. In order to do this, one uses the fact that for L larger than 1/α, the number of paths of length L between any two nodes is determined by the types involved in the path with a small relative error. Elaborating the proof of the theorem above, we can then characterize the behavior of Φ L and finally deduce the claimed aggregation property for Φ. 4.2.3. The good-aggregation outcome as a unique prediction. The theorem above says good aggregation is supported in an equilibrium but does not state that this is the unique equilibrium outcome. To deal with this issue, we study the alternative model with T = Z ≥0 (where agents begin with only their own signals and then best-respond to the previous distribution of behavior at each time). We show that, as n → ∞, its long-run outcomes get arbitrarily close to the good-aggregation equilibrium of Theorem 1 under the same conditions. Thus, even if there were other equilibria of the stationary model, they could not be approached via the natural iterative procedure coming from the T = Z ≥0 model. Formal statements and details are in Appendix OA5.

## Aggregation under homogeneous signals.

Having established conditions for good aggregation under signal diversity, we now explore what happens without signal diversity. Our general message is that aggregation is worse.

To gain an intuition for this, note that it is essential to the argument described in the previous subsection that different agents have different signal precisions. Recall the complete network case. From the perspective of an agent (i, t + 1), the fact that type A and type B neighbors place different weights on the social signal r t keeps their behavior from being collinear, and allows (i, t) to separate θ t from a confound. In that example, if type A and B agents had the same signal types, they would use the same weights, and our agent trying to learn from them would face a collinearity problem.

We begin by studying graphs having a symmetric structure and show that learning outcomes are necessarily bounded very far from good aggregation. We then turn to arbitrary large graphs and prove a lower bound on aggregation error that implies the homogeneoussignals regime has, quite generally, worse outcomes for some agents than those achieved by everyone in our good-aggregation result.

## Aggregation in networks with symmetric neighbors.

Definition 4. A network G has symmetric neighbors if, whenever j, j ∈ N i for some i, then N j = N j .

In the undirected case, the graphs with symmetric neighbors are the complete network and complete bipartite networks. [21](#) For directed graphs, the condition allows a larger variety of networks.

Proposition 2. Consider a sequence (G n ) ∞ n=1 of strongly connected graphs with symmetric neighbors. Assume that all signal variances are equal, and that m = 1. Then there is a unique equilibrium on each G n , and there exists an ε > 0 such that the ε-aggregation benchmark is not achieved by any agent i at this equilibrium for any n.

All agents have non-vanishing aggregation errors at the unique equilibrium. So all agents learn poorly compared to the diverse signals case. The proof of this proposition, and the proofs of all subsequent results, appear in Appendix OA4.

This failure of good aggregation is not due simply to a lack of sufficient information in the environment: On the complete graph with exchangeable (i.e., non-diverse) signals, a social planner who set weights for all agents could achieve ε-aggregation for any ε > 0 when n is large. See Appendix OA7 for a formal statement, proof and numerical results. [22](#) In this sense, the social learning externalities are quite severe: a small change in weights for each individual could yield a very large benefit in a world of homogeneous signal types.

We now give intuition for Proposition 2. In a graph with symmetric neighbors and homogeneous signals, in the unique equilibrium,[foot_17](#foot_17) actions of any agent's neighbors are exchangeable. So Bayesian estimates (and thus actions) must weight all neighbors equally, which prevents the sort of inference of the most recent state illustrated in Section 4.2.1. This is easiest to see on the complete graph, where all observations are exchangeable. So, in any equilibrium, each agent's action at time t is equal to a weighted average of her own signal and the average action

$1 |N i | j∈N i a j,t-1 : a i,t = w s i s i,t + (1 -w s i ) 1 |N i | j∈N i a j,t-1 . (4.2)$By iteratively using this equation, we can see that actions must place substantial weight on the average of signals from, e.g., two periods ago, and indeed further back. Note that all signals s j,t at past times t take the form θ t + η i,t . Thus, although the effect of signal errors η i,t vanishes (by averaging) as n grows large, the correlated error from past changes in the state ν t never "washes out" of estimates, and this is what prevents vanishing aggregation errors.

The bad-aggregation result as stated applies to exactly homogeneous signal types only. In fact, in finite networks we need sufficiently heterogeneous signals to avoid bad learning outcomes; this is illustrated in Appendix OA1. In Section 4.4 we discuss the welfare implications of this failure of aggregation.

As a consequence of Theorem 1 and Proposition 2, we can give an example where making one node's private information less precise helps all agents.

Corollary 1. There exists a network G, a vector of signal precisions σ, and an agent i ∈ G such that increasing σ 2 i yields a Pareto improvement at the unique equilibrium.

To prove the corollary, we consider the complete graph with homogeneous signals and large n. By Proposition 2, all agents have non-vanishing aggregation errors. If we instead give agent 1 a very uninformative signal, all players can anti-imitate agent 1 and achieve vanishing aggregation errors. When the signals at the initial configuration are sufficiently imprecise, this gives a Pareto improvement. There are also examples where severing links in the observational network can yield a Pareto improvement, as reported in an earlier version of the present paper (Dasaratha, Golub, and Hak, 2018). 4.3.2. Aggregation in arbitrary networks. Section 4.3.1 showed aggregation errors are nonvanishing when signal endowments and neighborhoods are symmetric. A natural question is whether asymmetry in network positions can substitute for asymmetry in signal endowments. In Section 4.2 the key point was that different neighbors' actions were informative about different linear combinations of θ t and older information, and this permitted filtering. Perhaps different network positions can achieve the same effect?

We thus move to arbitrary networks and show a weaker but much more general result. Consider any sequence of equilibria on any networks with symmetric signal endowments. Our result here is that no equilibrium achieves C/n-aggregation for almost all agents, no matter what C is. In particular, this implies that the rate of learning (as n grows) is slower than at the good-learning equilibrium with diversity of signal endowments from Theorem 1. Moreover, if degrees are bounded above by some d(n) growing at rate slower than n, we prove the stronger statement that no equilibrium achieves C/d(n)-aggregation for almost all agents.

$Theorem 2. Let C > 0. Let (G n ) ∞$n=1 be an arbitrary sequence of networks and suppose all private signals have variance σ 2 . If all agents' in-degrees and out-degrees are bounded above by some d(n) → ∞, then in any sequence of equilibria, the aggregation error κ 2 i is greater than C/d(n) for a non-vanishing fraction of agents i.

In addition to considering arbitrary networks, we allow the memory m to be an arbitrary positive integer. Because the assumptions are much weaker, we obtain a weaker conclusion than in Proposition 2. While Proposition 2 shows that aggregation errors are non-vanishing, this theorem shows that aggregation errors cannot vanish quickly, but does not rule out aggregation errors vanishing more slowly.

The basic intuition is that to avoid putting substantial weight on θ t-2 , an agent at time t must anti-imitate some neighbors. If all or almost all neighbors achieve C/n-aggregation for some C and have identical types of private signals, there is not much diversity among neighbors. So more and more anti-imitation is needed as n grows large in the sense that the total positive weight and total negative weight on neighbors both grow large. But then the contribution to the agent's variance from neighbors' private signal errors cannot vanish quickly.

We can combine Theorems 1 and 2 to compare the value of signal diversity and network diversity. With diversity of signal endowments, there exists a C > 0 such that asymptotically almost surely there is a good-learning equilibrium achieving the C/n-aggregation benchmark for all agents under the stochastic block model. With exchangeable signals, it is not possible to find equilibria achieving the same aggregation rate in n under any sequence of networks. Thus, Theorem 2 shows that network heterogeneity cannot improve learning outcomes as much as signal heterogeneity. Section 8.1 complements the asymptotic results with numerical results in finite networks. It shows that in our model on real-world (highly asymmetric) social networks, signal heterogeneity improves learning outcomes much more than choosing a very favorable network structure but homogeneous signals.

4.4. The welfare loss associated with homogeneity. The results derived so far in this section show that there is a qualitative difference in how well agents are able to infer recent states across the homogeneous and heterogeneous signal settings. How important is this difference for welfare? We illustrate next that the welfare loss associated with signal homogeneity can be arbitrarily severe.

To gain an intuition for this, note that with homogeneous signals, period-t actions are confounded by previous states. These confounds include θ t-2 , which all t -1 agents use in the same way (as illustrated in the example of the introduction). But the confounds also include θ t-3 , which could not be filtered out by t -1 agents, and so forth. The more weight agents place on social information (i.e., the more informative the past is), the more severe this confounding is. If the state is highly persistent and private signals are not very precise, then the confounds from periods even very long ago are substantial. The following corollary quantifies this effect.

Corollary 2. Consider a complete graph with all signal variances equal to σ 2 , and let m = 1. Then, in any symmetric strategy profile,

$Var(a i,t -θ t ) ≥ (1 -w s ) 2 1 -(1 -w s ) 2 ρ 2 ,$where w s is the weight agents place on their own signals. As ρ → 1 from below and σ -2 → 0, agent i's action error in the unique equilibrium tends to infinity. Moreover, this convergence is uniform in n. [24](#) The corollary guarantees that we can choose (σ -2 , ρ) so that the error is arbitrarily large, uniformly in n. In contrast, recall that our main positive result shows that the C/naggregation benchmark would be achieved with signal heterogeneity. [25](#) When this benchmark is achieved, each individual obtains a variance Var(a i,tθ t ) that is at worst 1 if n is large enough. [26](#) This bound on variance does not depend on σ 2 or ρ. Thus welfare can be arbitrarily worse in environments with signal homogeneity compared to ones with heterogeneity.

In large complete graphs with homogeneous signals, we can explicitly characterize the limit action variance (and therefore welfare). Let V ∞ denote the limit, as n grows large, of Var(a i,tθ t ). Let Cov ∞ denote the limit covariance of any two agents' errors. By direct computation using equation (3.3), these can be seen to be related by the following equations, which have a unique solution:

$V ∞ = 1 σ -2 + (ρ 2 Cov ∞ +1) -1 , Cov ∞ = (ρ 2 Cov ∞ +1) -1 [σ -2 + (ρ 2 Cov ∞ +1) -1 ] 2 . (4.3)$These equations also let us extend Corollary 2 beyond the complete graph. The V ∞ and Cov ∞ solving (4.3) describe the limits of all variances and covariances in any graph with symmetric neighbors where degrees tend uniformly to infinity.[foot_21](#foot_21) As σ -2 → 0 and ρ → 1 from below, equations (4.3) show that V ∞ and therefore Cov ∞ diverge to infinity, just as in the complete-network case. This shows the welfare loss from homogeneity can also be arbitrarily severe in graphs with symmetric neighbors and large degrees.

## The importance of understanding correlations

In the positive result on achieving the C/n-aggregation benchmark (Theorem 1), a key aspect of the argument involved agents filtering out confounding information from their neighbors' estimates-i.e., responding in a sophisticated way to the correlation structure of those estimates. In this section, we demonstrate that this sort of behavior is essential for nearly perfect aggregation, and that more naively imitative heuristics yield outcomes far from the benchmark. Empirical studies have found evidence (depending on the setting and the subjects) consistent with both equilibrium behavior and naive inference in the presence of correlated observations (e.g., Eyster, Rabin, and Weizsacker, 2015; Dasaratha and He, 2021;  Enke and Zimmermann, 2019).

We begin with a canonical model of agents who do not account for correlations among their neighbors' estimates conditional on the state, and show by example that naive agents achieve much worse learning than Bayesian agents, and thus have non-vanishing aggregation errors. We then formalize the idea that accounting for correlations in neighbors' actions is crucial to reaching the benchmark. This is done by demonstrating a general lack of good aggregation by agents who use imitative strategies, rather than filtering in a sophisticated way. Finally, we show that even in fixed, finite networks, any positive weights chosen by optimizing agents will be Pareto-dominated.

5.1. Naive agents. In this part we introduce agents who misunderstand the distribution of the signals they are facing and who therefore do not update as Bayesians with a correct understanding of their environment. We consider a particular form of misspecification that simplifies solving for equilibria analytically:[foot_22](#foot_22) Definition 5. We call an agent naive if she believes that all neighbors choose actions equal to their private signals and maximizes her expected utility given these incorrect beliefs.

Equivalently, a naive agent believes her neighbors all have empty neighborhoods. This is the analogue, in our model, of "best-response trailing naive inference" (Eyster and Rabin,  2010). So naive agents understand that their neighbors' actions from the previous period are estimates of θ t-1 , but they think these are conditionally independent given the state, and that the precision of each estimate is equal to the signal precision of the corresponding agent. They then play their expectation of the state given this misspecified theory of others' play.

In Figure [5](#).1, we compare Bayesian and naive learning outcomes. We consider a complete network with 600 agents and ρ = 0.9. Half of agents have signal variance σ 2 A = 2, while we vary the signal variance σ 2 B of the remaining agents. The figure shows the average social signal variance for the group of agents with private signal variance σ 2 A = 2. It suggests that naive agents learn substantially worse than rational agents, whether signals are diverse or not. We prove this holds for general stochastic block models and provide formulas for variances under naive learning in Appendix OA6. 5.2. More general learning rules: Understanding correlation is essential for good aggregation. We now show more generally that a sophisticated response to correlation is needed to achieve vanishing aggregation errors on any sequence of growing networks. To this end, we make the following definition: Bayesian and naive learning on a complete graph and n = 600 agents divided into two groups of equal size. The plot shows the aggregation error in group A as group B's private signal variance varies, fixing group A's private signal variance at σ 2 A = 2.

Definition 6. The steady state associated with weights W and w s is the (unique) covariance matrix V * such that if actions have a variance-covariance matrix given by V t = V * and nextperiod actions are set using weights (W , w s ), then V t+1 = V * as well.

In this definition of steady state, instead of best-responding to others' actual distributions of play, agents use exogenous weights W in all periods.

By a straightforward application of the contraction mapping theorem, if agents use any non-negative weights under which covariances remain bounded at all times, there is a unique steady state.

Consider a sequence of networks (G n ) ∞ n=1 with n agents in G n .

Proposition 3. Fix any sequence (V * (n)) ∞ n=1 , with each V * (n) being a steady state under non-negative weights in the network G n . Suppose that all private signal variances are bounded below by σ 2 > 0 and that all agents place weight at most w < 1 on their private signals.

Then there is an ε > 0 such that, for all n, the ε-aggregation benchmark is not achieved by any agent i at the steady state V * (n).

The essential idea is that at time t+1, observed time-t actions all put weight on actions from period t-1, which causes θ t-1 to have a (positive weight) contribution to all observed actions. Agents do not know θ t-1 and, with positive weights, cannot take any linear combination that would recover it. Even with a very large number of observations, this confound prevents agents from learning the time-t state precisely.

We now explain why we impose an assumption of all weights on private signals being bounded away from 1. If there were many autarkic agents who simply reported their private signals (i.e., placed weight 1 on these signals), some other agent could learn well without adjusting for correlations by observing the autarkic agents. Note that in this case, all of the autarkic agents would have non-vanishing aggregation errors. This illustrates that a weaker conclusion than that of the proposition can be established more generally. If we did not impose a bound on private signal weights, learning would fail in the weaker sense that some agent must fail to achieve the ε-aggregation benchmark for small enough ε.

On undirected networks, the proposition implies that aggregation errors do not vanish under naive inference or under various other specifications of non-Bayesian inference implying nonnegative weights. Moreover, the same argument shows that in any sequence of Bayesian equilibria on undirected networks where all agents use positive weights, no agent can learn well.

5.3. Without anti-imitation, outcomes are Pareto-inefficient. The previous section argued that anti-imitation is critical to achieving vanishing aggregation errors. We now show that even in small networks, where that benchmark is not relevant, any equilibrium without anti-imitation is Pareto-inefficient relative to another steady state. This result complements our asymptotic analysis by showing a different sense (relevant for small networks) in which anti-imitation is necessary to make the best use of information.

Proposition 4. Suppose the network G is strongly connected and some agent has more than one neighbor. Given any naive equilibrium or any Bayesian equilibrium where all weights are positive, the action variances at that equilibrium are Pareto-dominated by action variances at another steady state.

The basic argument behind Proposition 4 is that if agents place marginally more weight on their private signals, this introduces more independent information that eventually benefits everyone. In a review of sequential learning experiments, Weizsäcker (2010) finds that subjects weight their private signals more heavily than is optimal (given the empirical behavior of others they observe). Proposition 4 implies that in our environment with optimizing agents, it is actually welfare-improving for individuals to "overweight" their own information relative to best-response behavior.

The condition on equilibrium weights says that no agent anti-imitates any of her neighbors. This assumption makes the analysis tractable, but we believe the basic force also works in finite networks with some anti-imitation. In the proof in Appendix OA4, we state and prove a more general result where weights are non-negative but need not all arise from Bayesian or naive updating.

Proof sketch. The idea of the proof for the Bayesian equilibrium case is to begin at the steady state and then marginally shift each agent's weights toward her private signal. This means agents' actions are less correlated but, by the envelope theorem, not significantly worse in the next period. We show that if all agents continue using these new weights, the decreased correlation eventually benefits everyone. To do this, we use the absence of antiimitation, which implies a certain monotonicity in the updating function whereby the initial decrease in correlation results in all agents' variances decreasing.

The proof in the naive case is simpler. Here a naive agent is overconfident about the quality of her social information, so she would benefit from shifting some weight from her social information to her signal. This deviation also reduces her correlation with other agents, so it is Pareto-improving.

## Social influence

A canonical question about learning in networks is how much influence various agents have in affecting aggregate behavior. This is a focus of studies including DeMarzo, Vayanos, and  Zweibel (2003) and Golub and Jackson (2010) in the DeGroot model with an unchanging state. In this section, we define a suitable analogue of social influence for our dynamic environment. We then study how an agent's influence depends on her signal precision and degree. We find that, relative to benchmark results from the DeGroot model, influence is more sensitive to signal precisions, while social connectedness plays a similar role in both models.

6.1. Defining social influence. We define the total influence of node i in a stationary equilibrium with weights ( W , w s ) to be the total weight that all actions place on the private signal of agent (i, t). The total influence measures the total increase in actions if s i,t increases by 1 (due to an idiosyncratic shock).[foot_23](#foot_23) At equilibrium, the total influence of i is:

$TI(i) = j∈N ∞ k=0 ρ k W k ji w s i .$This expression for total influence is a version of Katz-Bonacich centrality with respect to the matrix W of weights. The decay parameter is the persistence ρ of the of the AR(1) state process.

We define the social influence of i to be the total weight that all actions in future periods place on the private signal of agent (i, t). At equilibrium, the social influence of i is:

$SI(i) = j∈N ∞ k=1 ρ k W k ji w s i = TI(i) -w s i .$The social influence measures the influence of an agent at node i on other agents. Social influence and total influence differ only by the weight (i, t) places on her own current signal, because an agent's signal realization does not affect others' actions in the same period. Note that agent i's social influence depends on the weight w s i she places on her own signal as well as the weights agents place on each others' actions.

The next result, which follows from Proposition 1 on equilibrium existence, shows that the summation that defines social influence is guaranteed to converge at equilibrium, which makes social influence (and similarly total influence) well-defined. [30](#) Proposition 5. The social influence SI(i) is well-defined at any equilibrium and is equal to

$1 (I -ρ W ) -1 -1 i w s i .$We show this as follows: if social influence did not converge, some agents would have actions with very large variances (because their actions would depend sensitively on small idiosyncratic shocks). But then these agents would have simple deviations that would improve their accuracy, such as following their private signals. So this could not happen in equilibrium. Once the infinite series defining social influence is shown to be convergent, the proposition follows by a standard Neumann series identity.

In general, social influence can be negative: an agent's net effect on others can be in the opposite direction of her signal. 6.2. Which agents are influential? We now ask how the social influence SI(i) of an agent depends on her signal precision and degree. To facilitate the most direct comparison with standard results in models with a fixed state, such as DeMarzo, Vayanos, and Zweibel (2003), we focus on cases where social influences are positive.

To examine the effect of signal precision on social influence, we first study complete networks with n ≥ 2 agents and two private signal variances: half the agents have more precise signals, and the other half have less precise signals. We call the two groups' signal variances σ 2

A and σ 2 B and the corresponding agents' social influences SI(A) and SI(B). We show that the ratio between the two groups' social influences in equilibrium is larger than the ratio between their signal precisions (whenever the imprecise group has positive social influence). Proposition 6. On a complete network with m = 1 and signal variances

$σ 2 A < σ 2 B , in the unique equilibrium it holds that SI(A) SI(B) > σ -2 A σ -2 B whenever SI(B) > 0.$The proposition says that increasing a group's precision increases their influence more than proportionately. As we have seen in our main results, if the precision difference is large enough, then it is optimal to place zero or negative weight on the less precise group. The result says 

$2 A or σ 2 B .$Each entry is computed from 100 runs with persistence ρ = 0.9. Each table entry reports the ratio R σ = SI(A)/SI(B)

$σ -2 A /σ -2 B$for the precision parameters corresponding to that entry.

that even before this happens, imprecision reduces a group's influence considerably-and, as we will discuss below, more than in benchmark models of social influence.

The proposition assumes the network is complete, but numerical evidence suggests that on other networks, too, agents with more precise signals tend to be much more influential. We simulate a configuration model with n = 40 nodes, each with degree d = 5.[foot_25](#foot_25) Nodes are randomly assigned to have a precise signal with variance σ 2

A or an imprecise signal with variance σ 2 B (with equal probability). We are interested in the ratio SI(A)/SI(B) in this more complicated environment. If social influence were approximately proportional to precision, then SI(A)/SI(B) would be approximately σ -2

A /σ -2 B . To assess by how much the influence of the precise group exceeds the level suggested by this benchmark, we will look at the ratio

$R σ = SI(A)/SI(B) σ -2 A /σ -2 B .$Table [1](#tab_2) reports this ratio over 100 runs of the simulation model for various pairs of σ 2 A and σ 2 B , each in the interval [0.5, 5]. The entries of the table would be equal to 1 if influence is proportional to precision. Instead, all off-diagonal entries are greater than one (or negative), meaning social influence depends more (and often much more) on signal precision than in the proportional benchmark.

Having examined how influence depends on precisions, we turn to how it depends on degrees. We again use a configuration model, which allows us to fix any desired empirical degree distribution and generate the graphs uniformly conditional on the degrees. We will find that social influence depends less on degree than on precision. We simulate a configuration 

$R d = SI(A)/SI(B) d A /d B .$over 100 runs of the simulation model for degrees between 1 and 10. Again, the entries would be equal to 1 if social influence were proportional to degree. Social influence is indeed approximately proportional to degree: the entries in the table range between 0.91 and 1.11.

Remark 1. A simple intuition explains why social influence depends more on private information than on network position. Increasing an agent's private signal precision and her degree both tend to make her action more accurate. Increasing private signal precision has the additional effect of increasing an agent's weight on her private information, which is recent and independent of other agents' actions. This provides more reason for others to place weight on her actions, amplifying the effect of the increased accuracy. In contrast, increasing degree tends to make an agent place more weight on her social information, which is older and more correlated with others. This countervails the effect of increased accuracy, making the agent a less appealing source for others.

The exercises so far varied only one of signal precision or degree, and we now explore how social influence depends on precision and degree jointly. To do so, we compute equilibrium social influences on 5,000 networks with n = 40 agents in each. Each agent is randomly assigned a degree chosen uniformly from {1, 2, . . . , 7} and a private signal variance chosen uniformly and independently from {0.5, 1, . . . , 3.5}. Networks are then drawn via the configuration model. Figure [6](#).1 plots the level curves for average social influence conditional on node attributes. The steepness of the level curves shows that social influence again depends more on signal variance than degree, especially when signals are less precise. 6.3. Comparison with a DeGroot benchmark. The results above are interesting to compare with those of canonical network models with a fixed state. A relevant benchmark is a version of the DeGroot model studied by DeMarzo, Vayanos, and Zweibel (2003). Agents start with an improper prior, receive independent normal private signals s i (with different precisions) about the state once, and then each takes an action a i,0 equal to her expectation of the state θ. After this, agents observe their neighbors' actions and take actions a i,1 , which are Bayesian expectations of the state θ given their observations. In all subsequent periods t > 1, agents observe their neighbors' actions a j,t-1 and take actions a i,t as if a j,t-1 had the same distribution as j's private signal. That is, they naively repeat their optimal strategy from the first period, which DeMarzo et al. (2003) interpret as a quasi-Bayesian, boundedly rational procedure.

One natural measure of social influence is the influence of s i on the long-run consensus estimate lim t→∞ a j,t held by any agent. In an undirected, connected, and aperiodic network, this limit exists and the influence of agent i is proportional to her private signal precision σ -2 i and to her degree d i . Compared to this benchmark, social influence in our changing-state model is more sensitive to signal precision (in the complete graph and in our simulations for configuration models). On the other hand, the dependence of social influence on degree is very similar to the DeGroot benchmark-approximately proportional. To summarize, influence depends more on an agent's private information, while the dependence on network position is remarkably similar. The difference between the benchmark and our model is explained in Remark 1.

## Related literature

Whether decentralized communication can facilitate efficient adaptation to a changing world is a fundamental question in economic theory, related to questions raised by Hayek  (1945) [foot_26](#foot_26) and central to certain applied problems, e.g., in real business cycle models with consumers and firms learning about evolving states. [33](#) Nevertheless, there is relatively little modeling of Bayesian learning of dynamic states in the large literature on social learning and information aggregation in networks, whose most relevant papers we now review. [34](#) Play in the stationary linear equilibria of our model closely resembles behavior in the DeGroot (1974) model, where agents update by linearly aggregating network neighbors' past estimates, with constant weights on neighbors over time. DeMarzo, Vayanos, and Zweibel  (2003), in a Gaussian environment with an unchanging state, derive DeGroot learning as the Bayesian behavior in the first round of communication, and use that as a foundation for a DeGroot rule as a boundedly-rational heuristic. Molavi, Tahbaz-Salehi, and Jadbabaie (2018)  present new bounded-rationality foundations for the DeGroot rule. Our different environment offers a different foundation for averaging rules with time-invariant weights: as a stationary equilibrium of a stationary environment. [35](#) Though the updating rule resembles those studied in fixed-state environments, we have stressed that the learning implications are quite different.

Several recent papers in engineering and computer science study dynamic environments similar to ours. Shahrampour, Rakhlin, and Jadbabaie ( [2013](#)) study an exogenous-weights version, interpreted as a set of Kalman filters under the control of a planner. They bound measures of welfare in terms of the persistence of the state process (ρ) and network properties, such as the spectral gap. Frongillo, Schoenebeck, and Tamuz (2011) study a ρ = 1 model of the state. They characterize the steady-state distribution of behavior for any weights, and calculate equilibrium weights on a complete network, which they show are inefficient. Our Proposition 4 documents a related inefficiency; the quality of equilibrium learning in large, incomplete networks and social influence in equilibrium are topics not considered in these papers. In economics, Alatas, Banerjee, Chandrasekhar, Hanna, and Olken (2016)  perform an empirical exercise in a similar model with a quasi-Bayesian learning rule. Their estimation assumes agents ignore the correlations between social observations, similarly to our naive models. [36](#) Our results show that the degree of rationality can be pivotal for the outcomes of such processes, and provide foundations for structural inference to test various behavioral assumptions.

Our results about when agents learn well are related to two phenomena that have played an important role in the social learning literature. One theme in this literature is that heterogeneity-in agents' neighborhoods or preferences-can be helpful for learning. A manifestation of this is the usefulness of sacrificial lambs (typically studied in sequential social learning models with a fixed state): a small set of agents who observe nobody can help everyone else learn well, because their actions are then informative only about their private signals, and unconfounded by an information cascade (Sgroi, 2002, Arieli and Mueller-Frank, 2019). Heterogeneity in preferences can serve a similar purpose: if preferences have full support, there is a positive probability that preference bias counteracts available social information, causing an agent to follow her private signal (Goeree, Palfrey, and Rogers, 2006, Lobel and  Sadler, 2016). A crucial difference is that our mechanism does not rely on any agents simply revealing their private signals: heterogeneity helps by changing how neighbors use their social information, which in turn aids an agent in inferring a common confound. [37](#) Second, a robust aspect of rational learning in sequential models is the phenomenon of anti-imitation, as discussed, e.g., by Eyster and Rabin (2014). They give general conditions for fully Bayesian agents to anti-imitate in a standard sequential model. We find that antiimitation is also an important feature in our dynamic model, and in our context is crucial for good learning. Despite this similarity, there is an important contrast between our environment and standard sequential models. In those models, while rational agents do prefer to antiimitate, individuals and society as a whole can often obtain good outcomes using heuristics without any anti-imitation: for instance, by combining one's own private signal with the information that can be inferred from a single neighbor. Acemoglu, Dahleh, Lobel, and  Ozdaglar (2011) and Lobel and Sadler (2015) show that such a heuristic leads to asymptotic learning in a sequential model. Our dynamic learning environment is different, as shown in Proposition 3: to have any hope of approaching good aggregation benchmarks, agents must respond in a sophisticated way, with anti-imitation, to their neighbors' (correlated) estimates.

36 The paper's focus is estimating parameters of social learning rules using data from Indonesian villages, where agents are trying to estimate each other's wealth. [37](#) A bit farther afield, in Sethi and Yildiz (2012), learning outcomes when two individuals repeatedly learn from each other depend on whether their (heterogeneous) priors are independent or correlated; the common thread is that a natural assumption about agents' attributes (independent priors in their case) leads to an identification problem. The mechanics are otherwise quite different.

## Discussion and extensions

8.1. Aggregation and its absence without asymptotics: Numerical results. The message of Section 4 is that signal diversity enables good aggregation, and signal homogeneity obstructs it. The theoretical results were asymptotic, and relied on various assumptions about network structure. It is natural to ask whether our main conclusions hold up in realistic finite networks. To analyze this, we numerically study equilibria of our model on graphs reflecting social relationships measured in Indian villages (Banerjee, Chandrasekhar, Duflo,  and Jackson, 2013). This subsection briefly summarizes our findings; we describe the exercise fully in Appendix OA1.

We examine the benefits of signal heterogeneity for equilibrium aggregation. The network data are essentially the only empirical input to our exercise. [38](#) Given a network, we compute equilibria using our model and parameters chosen for illustration. We compare two environments that differ in signal allocations: (i) a homogeneous case, with all signal variances set to 2, and (ii) a heterogeneous case, where half of the nodes have a signal variance greater than 2 (which we vary) and half of the nodes have a signal variance less than 2. [39](#) We first compare the value of a good network with the value of heterogeneous signals. Some networks have better learning than others even with homogeneous signals. We define the network-driven variation in learning to be the standard deviation of learning quality (aggregation error) across villages in the homogeneous case. Our main finding is that increasing the private signal variance for half of the agents by 50%, and reducing the signal variance of the others to keep total information constant, changes social signal error variance by 6.5 times the network-driven variation. In fact, introducing this amount of private signal heterogeneity improves learning much more than the most favorable network among the villages.

Though the asymptotic prediction changes starkly depending on whether signal precisions are identical or not, considerable heterogeneity is actually required to achieve the benefits of signal diversity in a finite network. Starting from homogeneous signals and increasing signal diversity, aggregation error changes very slightly at first. Once the variance of the less precise signal has increased by 50% relative to the starting point, learning quality has moved about halfway to what is achievable with the most extreme signal heterogeneity. 8.2. Multidimensional states and informational specialization. Our formal analysis assumed a one-dimensional state and one-dimensional signals, which varied only in their precisions. Our message about the value of diversity is, however, better interpreted in a mathematically equivalent multidimensional model.

Consider Bayesian agents who learn and communicate about two independent dimensions simultaneously, each one working as in our model. If all agents have equally precise signals about both dimensions, then society may not learn well about either of them. In contrast, if half the agents have superior signals about one dimension and inferior signals about the other (and the other half has the reverse), then society can learn well about both dimensions. Thus, the designer has a strong preference for an organization with informational specialization where some, but not all, agents are expert in a particular dimension. [40](#) Of course, there are many familiar reasons for specialization in having precise information about an issue. For instance, it may be that specialization is technologically efficient, or makes it easier to provide incentives. Crucially, specialization is valuable in our setting for a distinct reason: it helps agents with their inference problems.

More generally, one could readily extend our model and equilibrium concept to a multidimensional state θ t ∈ R d and arbitrary Gaussian signals about it, with flexible correlations. We would expect to find suitable generalizations of the basic message that sufficient diversity within neighborhoods (in terms of signal types) facilitates learning. The assumption that agents know neighbors' signal distributions is clearly very helpful for tractability; it would be interesting to consider models in which agents are also uncertain about these distributions.

We begin by defining the compact set K. Recalling Section 3.1.2, entries of V t are covariances between pairs of neighbor errors from any periods twhere 1 ≤ ≤ m. Let k, l be two indices of such actions, corresponding to actions taken at nodes i and j respectively (at potentially different times), and let

$σ 2 i = max σ 2 i , ρ m-1 σ 2 i + 1-ρ m-1 1-ρ$. Now let K ⊂ V be the subset of symmetric positive semi-definite matrices V t such that, for any such k, l,

$V kk,t ∈ min 1 1 + σ -2 i , ρ m-1 1 + σ -2 i + 1 -ρ m-1 1 -ρ , max σ 2 i , ρ m-1 σ 2 i + 1 -ρ m-1 1 -ρ V kl,t ∈ [-σ i σ j , σ i σ j ] .$This set is closed and convex, and we claim that Φ(K) ⊂ K.

To show this claim, we will first find upper and lower bounds on the variance of any neighbor's action (at any period in memory). For the upper bound, note that a Bayesian agent will not choose an action with a larger variance than her signal, which has variance σ 2 i . For a lower bound, note that if she knew the previous period's state and her own signal, then the variance of her action would be 1 1+σ -2 i . Thus an agent observing only noisy estimates of θ t and her own signal can do no better.

By the same reasoning applied to the node-i agent from m periods ago, the error variance of ρ m a i,t-mθ t is at most ρ m σ 2 i + 1-ρ m 1-ρ and at least ρ m 1+σ -2 i + 1-ρ m 1-ρ . This establishes bounds on V kk,t for observations k from either the most recent or the oldest available period. The corresponding bounds from the periods between tm + 1 and t are always weaker than at least one of the two bounds we have described, so we need only take minima and maxima over two terms.

This established the claimed bound on the variances. The bounds on covariances follow from Cauchy-Schwartz.

We have now established that there is a variance-covariance matrix V such that Φ( V ) = V . By definition of Φ, this means there exists some weight profile ( W , w s ) such that, when applied to prior actions that have variance-covariance matrix V , produce variance-covariance matrix V . However, it still remains to show that this is the variance-covariance matrix reached when agents have been using the weights ( W , w s ) forever.

To show this, first observe that if agents have been using the weights ( W , w s ) forever, the variance-covariance matrix V t in any period is uniquely determined and does not depend on t; call this V . 41 This is because actions can be expressed as linear combinations of private signals with coefficients depending only on the weights. Second, it follows from our construction above of the matrix V and the weights ( W , w s ) that there is a distribution of actions where the variance-covariance matrix is V in every period and agents are using weights ( W , w s ) in every period. Combining the two statements shows that in fact V = V , and this completes the proof. Note that this argument also establishes that the response profile we have constructed is a strategy profile: under the responses used, we can write formally the dependence of actions on all prior signals, and verify using the observations on decay of dependence across time that the formula is summable and hence defines unique actions.

Appendix B. Proof of Theorem 1 B.1. Notation and key notions. Let S be the (by assumption finite) set of all possible signal variances, and let σ 2 be the largest of them. The proof will focus on the covariances of errors in social signals. Suppose that all agents have at least one neighbor. Take two arbitrary agents i and j. Recall that both r i,t and r j,t have mean θ t-1 , because each is an unbiased estimate[foot_33](#foot_33) of θ t-1 ; we will thus focus on the errors r i,tθ t-1 . Let A t denote the variance-covariance matrix (Cov(r i,tθ t-1 , r j,tθ t-1 )) i,j and let W be the set of such covariance matrices. For all i, j note that Cov(r i,tθ t-1 , r j,tθ t-1 ) ∈ [-σ 2 , σ 2 ] using the Cauchy-Schwarz inequality and the fact that Var(r i,tθ t-1 ) ∈ [0, σ 2 ] for all i. This fact about variances says that no social signal is worse than putting all weight on an agent who follows only her private signal. Thus the best-response map Φ is well-defined and induces a map Φ on W.

Next, for any ψ, ζ > 0 we will define the subset W ψ,ζ ⊂ W to be the set of covariance matrices in W such that both of the following hold:

1. for any pair of distinct agents[foot_34](#foot_34) i ∈ G k n and j ∈ G k n , Cov(r i,tθ t-1 , r

j,tθ t-1 ) = ψ kk + ζ ij where (i) ψ kk depends only on the network types of the two agents (k and k , which may be the same); (ii) |ψ kk | < ψ; and (iii) |ζ ij | < ζ; 2. for any single agent i ∈ G k n , Var(r i,tθ t-1 ) = ψ k + ζ ii where (i) ψ k only depends on the network type of the agent; (ii) |ψ k | < ψ, and (iii)

$|ζ ii | < ζ.$This is the space of covariance matrices such that each covariance is split into two parts.

Considering (1) first, ψ kk is an effect that depends only on i's and j's network types, while ζ ij adjusts for the individual-level heterogeneity arising from different link realizations. The description of the decomposition in (2) is analogous.

B.2. Proof strategy.

## B.2.1.

A set W ψ,ζ of outcomes with good learning. Our goal is to show that as n grows large, there is an equilibrium in which Var(r i,tθ t-1 ) becomes very small, which then implies that the agents asymptotically learn. To this end we define a set of covariances with this property as well as some other useful properties. We will take ψ and ζ to be arbitrarily small numbers and show that for large enough n, with high probability (which we abbreviate "asymptotically almost surely" or "a.a.s.") there is an equilibrium with a social error covariance matrix A t in the set W ψ,ζ . That will imply that, in this equilibrium, Var(r i,tθ t-1 ) becomes arbitrarily small as we take the constants ψ and ζ to be small. In our constructions, the ζ ij (resp., ζ i ) terms will be set to much smaller values than the ψ kk (resp., ψ k ) terms, because grouplevel covariances are more predictable and less sensitive to idiosyncratic realizations than individual-level covariances.

B.2.2. Approach to showing that W ψ,ζ contains an equilibrium. To show that there is (a.a.s.) an equilibrium outcome with a social error covariance matrix A t in the set W ψ,ζ , the plan is to construct a set so that (a.a.s.) W ⊂ W ψ,ζ and Φ(W) ⊂ W. This set will contain an equilibrium by the Brouwer fixed point theorem, and therefore so will W ψ,ζ .

To construct the set W, we will fix a positive constant β (to be determined later), and define

$W = W β n , 1 n ∪ Φ W β n , 1 n .$We will then prove that, for large enough n, (i) Φ(W) ⊆ W and (ii) for another suitable positive constant λ,

$W ⊂ W β n , λ n .$This will allow us to establish that (a.a.s.) W ⊂ W ψ,ζ and Φ(W) ⊂ W, with ψ and ζ being arbitrarily small numbers.

The following two lemmas will allow us to deduce (immediately after stating them) properties (i) and (ii) of W.

Lemma 1. There is a function λ(β) ≥ 1 such that the following holds. For all large enough β and all λ ≥ λ(β), for n sufficiently large we have Φ

$W β n , 1 n ⊂ W β n , λ n with probability at least 1 -1 n .$Lemma 2. For all large enough β, for n sufficiently large, Φ

$2 W β n , 1 n ⊂ W β n , 1 n , with proba- bility at least 1 -1 n . 44$Putting these lemmas together, a.a.s. we have,

$Φ 2 W β n , 1 n ⊂ W β n , 1 n and Φ W β n , 1 n ⊂ W β n , λ n . From this it follows that W = W β n , 1 n ∪ Φ W β n , 1$n is mapped to a subset of itself by Φ, and contained in W β n , λ n , as claimed. B.2.3. Proving the lemmas by analyzing how Φ and Φ 2 act on sets W ψ,ζ . The lemmas are about how Φ and Φ 2 act on the covariance matrix A t , assuming it is in a certain set W ψ,ζ , to yield new covariance matrices. Thus, we will prove these lemmas by studying two periods of updating. The analysis will come in five steps.

Step 1: No-large-deviations (NLD) networks and the high-probability event.

Step 1 concerns the "with high probability" part of the lemmas. In the entire argument, we condition on the event of a no-large-deviations (NLD) network realization, which says that certain realized statistics in the network (e.g., number of paths between two nodes) are close to their expectations. The expectations in question depend only on agents' types. Therefore, on the NLD realization, the realized statistics do not vary much based on which exact agents we focus on, but rather depend only on their types. Step 1 defines the NLD event E formally and shows that it has high probability. We use the structure of the NLD event throughout our subsequent steps, as we mention below.

Step 2: Weights in one step of updating are well-behaved. We are interested in Φ and Φ 2 , which describe how the covariance matrix A t of social signal errors changes under updating. How this works is determined by the "basic" updating map Φ, and so we begin by studying the weights involved in it and then make deductions about the implications for the evolution of the variance-covariance matrix A t .

The present step establishes that in one step of updating, the weight W ij,t+1 that agent (i, t + 1) places on the action of another agent j in period t, does not depend too much on the identities of i and j. It only depends on their (network and signal) types. This is established by using our explicit formula for weights in terms of covariances. We rely on (i) the fact that covariances are assumed to start out in a suitable W ψ,ζ , and (ii) our conditioning on the NLD event E. The NLD event is designed so that the network quantities that go into determining the weights depend only on the types of i and j (because the NLD event forbids too much variation within type). The restriction to A t ∈ W ψ,ζ ensures that covariances in the initial period t do not vary too much with type, either.

Step 3: Lemma 1:

$Φ W β n , 1 n ⊂ W β n , λ n .$Once we have analyzed one step of updating, it is natural to consider the implications for the covariance matrix. Because we now have a bound on how much weights can vary after one step of updating, we can compute bounds on covariances. We show that if covariances A t are in W β n , 1 n , then after one step, covariances are in W β n , λ n . Note that the introduction of another parameter λ on the right-hand side implies that this step might worsen our control on covariances somewhat, but in a bounded way.

Step 4: Weights in two steps of updating are well-behaved. Here we establish that the statement made in Step 2 remains true when we replace t + 1 by t + 2. By the same sort of reasoning as in Step 2, an additional period of updating cannot create too much further idiosyncratic variation in weights. Proving this requires analyzing the covariance matrices of various social signals (i.e., the A t+1 that the updating induces), which is why we needed to do Step 3 first.

Step 5: Lemma 2:

$Φ 2 W β n , 1 n ⊂ W β n , 1 n$. Now we use our understanding of weights from the previous steps, along with additional structure, to show the key remaining fact. What we have established so far about weights allows us to control the weight that a given agent's estimate at time t + 2 places on the social signal of another agent at time t. This is Step 5(a). In the second part, Step 5(b), we use that to control the covariances in A t+2 . It is important in this part of the proof that different agents have very similar "second-order neighborhoods": the paths of length 2 beginning from an agent are very similar, in terms of their counts and what types of agents they go through. We use our control of second-order neighborhoods, as well as the assumptions on variation across entries of A t to bound this variation well enough to conclude that A t+2 ∈ W β n , 1 n . B.3. Carrying out the steps.

## B.3.1.

Step 1. Here we formally define the NLD event, which we call E. It is given by E = ∩ 5 i=1 E i , where the events E i will be defined next. (E 1 ) Let X

(1) i,τ k be the number of agents having signal type τ and network type k who are observed by i. The event E 1 is that this quantity is close to its expected value in the following sense, simultaneously for all possible values of the subscript:

$(1 -ζ 2 )E[X (1) i,τ k ] ≤ X (1) i,τ k ≤ (1 + ζ 2 )E[X (1) i,τ k ]. (E 2 ) Let X (2)$ii ,τ k be the number of agents having signal type τ and network type k who are observed by both i and i . The event E 2 is that this quantity is close to its expected value in the following sense, simultaneously for all possible values of the subscript:

$(1 -ζ 2 )E[X (2) ii ,τ k ] ≤ X (2) ii ,τ k ≤ (1 + ζ 2 )E[X (2) ii ,τ k ]. (E 3 ) Let X (3)$i,τ k,j be the number of agents having signal type τ and network type k who are observed by agent i and who observe agent j. The event E 3 is that this quantity is close to its expected value in the following sense, simultaneously for all possible values of the subscript:

$(1 -ζ 2 )E[X (3) i,τ k,j ] ≤ X (3) i,τ k,j ≤ (1 + ζ 2 )E[X (3) i,τ k,j ]. (E 4 ) Let X (4)$ii ,τ k,j be the number of agents having signal type τ and network type k who are observed by both agent i and i and who observe j. The event E 4 is that this quantity is close to its expected value in the following sense, simultaneously for all possible values of the subscript:

(1

$-ζ 2 )E[X (4) ii ,τ k ,j ] ≤ X (4) ii ,τ k ,j ≤ (1 + ζ 2 )E[X (4)$ii ,τ k ,j ].

(E 5 ) Let X

(5) i,τ k,jj be the number of agents of signal type τ and network type k who are observed by agent i and who observe both j and j . The event E 5 is that this quantity is close to its expected value in the following sense, simultaneously for all possible values of the subscript:

(1

$-ζ 2 )E[X (5) i,τ k,jj ] ≤ X (5) i,τ k,jj ≤ (1 + ζ 2 )E[X(5)$i,τ k,jj ]. We claim that the probability of the complement of the event E vanishes exponentially. We can check this by showing that the probability of each of the E i vanishes exponentially. For E 1 , for example, the bounds will hold unless at least one agent has degree outside the specified range. The probability of this is bounded above by the sum of the probabilities of each individual agent having degree outside the specified range. By Chebyshev's inequality, the probability a given agent has degree outside this range vanishes exponentially. Because there are n agents in G n , this sum vanishes exponentially as well. The other cases are similar.

For the rest of the proof, we condition on the event E.

## B.3.2.

Step 2. As a shorthand, let ψ = β/n for a sufficiently large constant β, and let ζ = 1/n.

Lemma 3. Suppose that in period t the matrix A = A t of covariances of social signals satisfies A ∈ W ψ,ζ and all agents are optimizing in period t + 1. Then there is a γ so that for all n sufficiently large,

$W ij,t+1 W i j ,t+1 ∈ 1 - γ n , 1 + γ n .$whenever i and i have the same network and signal types and j and j have the same network and signal types.

To prove this lemma, we will use the formula given by (3.1) for row i of the matrix W t+1 :

$W i•,t+1 = 1 C -1 i,t 1 C -1 i,t 1$. This says that in period t + 1, agent i's weight on agent j is proportional to the sum of the entries of column j of C -1 i,t . We want to show that the change in weights is small as the covariances of observed social signals vary slightly. To do so we will use the Taylor expansion of f (A) = C -1 i,t around the covariance matrix A(0) at which all ψ kk = 0, ψ k = 0 and ζ ij = 0. We begin with the first partial derivative of f at A(0) in an arbitrary direction. Let A(x) be any perturbation of A(0) in one parameter, i.e., A(x) = A(0) + xM for some constant matrix M with entries in [-1, 1]. Let C i (x) be the matrix of covariances of the actions observed by i given that the covariances of agents' social signals were A(x). There exists a constant γ 1 depending only on the possible signal types such that each entry of C i (x)-C i (x ) has absolute value at most γ 1 (xx ) whenever both x and x are small.

We will now show that the column sums of C i (x) -1 are close to the column sums of C(0) -1 i . To do so, we will evaluate the formula

$∂f (A(x)) ∂x = ∂C i (x) -1 ∂x = C i (x) -1 ∂C i (x) ∂x C i (x) -1 (B.1)$at zero. If we can bound each column sum of this expression (evaluated at zero) by a constant (depending only on the signal types and the number of network types K), then the first derivative of f will also be bounded by a constant. Recall that S is the set of signal types and let S = |S|; index the signal types by numbers ranging from 1 to S. To bound the column sums of C i (0) -1 , suppose that the agent observes r i agents from each signal type 1 ≤ i ≤ S. Reordering so that all agents of each signal type are grouped together, we can write, for a suitable matrix Y and vector z:

$C i (0) =       Y 11 1 1 ×r 1 + z 1 I r 1 Y 12 1 r 1 ×r 2 Y S1 1 r 1 ×r S Y 12 1 r 2 ×r 1 Y 22 1 r 2 ×r 2 + z 2 I r 2 . . . . . . Y 1S 1 r S ×r 1 • • • Y SS 1 r S ×r S + z S I r S      $Therefore, the covariance matrix C i (0) can be written as a block matrix with blocks Y τ τ 1 rτ ×r τ + z τ δ τ τ I rτ where 1 ≤ τ, τ ≤ S and δ τ τ = 1 for τ = τ and 0 otherwise. We now have the following important approximation of the inverse of this matrix. [45](#) Lemma 4 (Pinelis ( [2018](#))). Let C be a block matrix with blocks given by

$Y τ τ 1 rτ ×r τ + z τ δ τ τ I rτ for all τ, τ ∈ S. As n → ∞, the (τ, τ ) block of C -1 satisfies 1 z τ I rτ - 1 z τ r τ 1 rτ ×rτ + O(1/n 2 )$while the off-diagonal blocks are O(1/n 2 ).

Proof. Note that the block (τ, τ ) of C -1 has the form

$E τ τ 1 rτ ×r τ + d τ δ τ τ I rτ$for some matrix E and vector d. Here δ denotes the Kronecker delta. Therefore, the (τ, τ ) block of CC -1 can be written (using that 1 r×r 1 r ×r = r 1 r×r ) as

$τ (Y τ τ 1 rτ ×r τ + z τ δ τ τ I rτ )(E τ τ 1 r τ ×r τ + d τ δ τ τ I r τ ) = (Y τ τ d τ + τ (Y τ τ r τ + δ τ τ z τ )E τ τ ) 1 rτ ×r τ + z τ d τ δ τ τ I rτ . (B.2)$For any vector v ∈ R S , let D v denote the diagonal matrix with v τ in the (τ, τ ) diagonal entry and v • v denote the pointwise product of two vectors. Breaking up the fact that (B.2) [45](#) We are very grateful to Iosif Pinelis for this argument.

equals I into its off-diagonal and diagonal parts, we have

$Y D d + (Y D r + D z )E = 0 and D d = D -1 z .$Hence,

$E = -(Y D r + D z ) -1 Y D d = -(I S + D -1 r Y -1 D z ) -1 (Y D r ) -1 Y D -1 z = -(I S + D -1 r Y -1 D z ) -1 D -1 z•r = -D -1 z•r + O(1/n 2 ).$Therefore as n → ∞ the off-diagonal blocks of C -1 will be O(1/n 2 ) while the (τ, τ ) diagonal block is 1

$z τ I rτ - 1 z τ r τ 1 rτ ×rτ + O(1/n 2 )$as desired.

Using Lemma 4 we can analyze the column sums of[foot_35](#foot_35) C i (0) -1 M C i (0) -1 . In more detail, we use the formula of the lemma to estimate both copies of C i (0) -1 , and then expand this to write an expression for any column sum of C i (0) -1 M C i (0) -1 . It follows straightforwardly from this calculation that all these column sums are O(1/n) whenever all entries of M are in [-1, 1].

We can bound the higher-order terms in the Taylor expansion by the same technique: by differentiating equation (B.1) repeatedly in x, we obtain an expression for the k th derivative in terms of C i (0) -1 and M :

$f (k) (0) = k!C i (0) -1 M C i (0) -1 M C i (0) -1 • . . . • M C i (0) -1 ,$where M appears k times in the product. By the same argument as above, we can show that the column sums of f (k) (0) k! are bounded by a constant independent of n. The Taylor expansion is

$f (A) = k f (k) (0) k! x k .$Since we take A ∈ W ψ,ζ , we can assume that x is O(1/n). Because the column sums of each summand are bounded by a constant times x k , the column sums of f (A) are bounded by a constant.

Finally, because the variation in the column sums is O(1/n) and the weights are proportional to the column sums, each weight varies by at most a multiplicative factor of γ 1 /n for some γ 1 . We find that the first part of the lemma, which bounded the ratios between weights W ij,t+1 /W i j ,t+1 , holds.

## B.3.3.

Step 3. We complete the proof of Lemma 1, which states that the covariance matrix of r i,t+1 is in W ψ,ζ . Recall that ζ = λ/n for some constant n, so we are showing that if the covariance matrix of the r i,t is in a neighborhood W ψ,ζ , then the covariance matrix in the next period is in a somewhat larger neighborhood W ψ,ζ . The remainder of the argument then follows by the same arguments as in the proof of the first part of the lemma: we now bound the change in time-(t + 2) weights as we vary the covariances of time-(t + 1) social signals within this neighborhood.

Recall that we decomposed each covariance Cov(r i,tθ t-1 , r j,tθ t-1 ) = ψ kk + ζ ij into a term ψ kk depending only on the types of the two agents and a term ζ ij , and similarly for variances. To show the covariance matrix is contained in W ψ,ζ , we bound each of these terms suitably.

We begin with ζ ij (and ζ i ). We can write

$r i,t+1 = j W ij,t+1 1 -w s i,t+1 ρa i,t = j W ij,t+1 1 -w s i,t+1 ρ w s j,t s j,t + (1 -w s j,t )r j,t .$By the first part of the lemma, the ratio between any two weights (both of the form W ij,t+1 , w s i,t+1 , or w s j,t ) corresponding to pairs of agents of the same types is in [1γ 1 /n, 1 + γ 1 /n] for a constant γ 1 . We can use this to bound the variation in covariances of r i,t+1 within types by ζ : we take the covariance of r i,t+1 and r j,t+1 using the expansion above and then bound the resulting summation by bounding all coefficients.

Next we bound ψ kk (and ψ k ). It is sufficient to show that Var(r i,t+1θ t ) is at most ψ. To do so, we will give an estimator of θ t with variance less than β/n, and this will imply Var(r i,t+1θ t ) < β/n = ψ (recall r i,t+1 is the estimate of θ t given agent i's social observations in period t + 1). Since this bounds all the variance terms by ψ, the covariance terms will also be bounded by ψ in absolute value.

Fix an agent i of network type k and consider some network type k such that p kk > 0. Then there exists two signal types, which we call A and B, such that i observes Ω(n) agents of each of these signal types in G k n . [47](#) The basic idea will be that we can approximate θ t well by taking a linear combination of the average of observed agents of network type k and signal type A and the average of observed agents of network type k and signal type B.

In more detail: Let N i,A be the set of agents of type A in network type k observed by i and N i,B be the set of agents of type B in network type k observed by i. Then fixing some agent j 0 of network type k,

$1 |N i,A | j∈N i,A a j,t-1 = σ -2 A 1 + σ -2 A θ t + 1 1 + σ -2 A r j 0 ,t-1 + noise$where the noise term has variance of order 1/n and depends on signal noise, variation in r j,t , and variation in weights. These bounds on the noise term follow from the assumption that the covariance matrix of the r i,t is in a neighborhood W ψ,ζ and our analysis of variation in weights. Similarly

$1 |N i,B | j∈N i,B a j,t-1 = σ -2 B 1 + σ -2 B θ t + 1 1 + σ -2 B r j 0 ,t-1 + noise$where the noise term has the same properties. Because σ 2 A = σ 2 B , we can write θ t as a linear combination of these two averages with coefficients independent of n up to a noise term of order 1/n. We can choose β large enough such that this noise term has variance most β/n for all n sufficiently large. This completes the proof of Lemma 1.

## B.3.4.

Step 4: We now give the two-step version of Lemma 3.

Lemma 5. Suppose that in period t the matrix A = A t of covariances of social signals satisfies A ∈ W ψ,ζ and all agents are optimizing in periods t + 1 and t + 2. Then there is a γ so that for all n sufficiently large,

$W ij,t+2 W i j ,t+2 ∈ 1 - γ n , 1 + γ n .$whenever i and i have the same network and signal types and j and j have the same network and signal types.

Given what we established about covariances in Step 3, the lemma follows by the same argument as the proof of Lemma 3.

Step 5: Now that Lemma 5 is proved, we can apply it to show that Φ 2 (W ψ,ζ ) ⊂ W ψ,ζ . We will do this by first writing the time-(t + 2) behavior in terms of agents' time-t observations (Step 5(a)), which comes from applying Φ twice. This gives a formula that can be used for bounding the covariances[foot_36](#foot_36) of time-(t+2) actions in terms of covariances of time-t actions.

Step 5(b) then applies this formula to show we can take ζ ij and ζ i to be sufficiently small. (Recall the notation introduced in Section B.1 above.) We split our expression for r i,t+2 into several groups of terms and show that the contribution of each group of terms depends only on agents' types up to a small noise term. Step 5(c) notes that we can also take ψ kk and ψ k to be sufficiently small.

Step 5(a): We calculate:

$r i,t+2 = j W ij,t+2 1 -w s i,t+2 ρa j,t+1 = ρ j W ij,t+2 1 -w s i,t+2 w s j,t+1 s j,t+1 + j,j W ij,t+2 1 -w s i,t+2 W jj ,t+1 ρa j ,t = ρ j W ij,t+2 1 -w s i,t+2 w s j,t+1 s j,t+1 + ρ j,j W ij,t+2 1 -w s i,t+2$W jj ,t+1 w s j ,t s j ,t

$+ j,j W ij,t+2 1 -w s i,t+2 W jj ,t+1 (1 -w s j ,t )r j ,t .$Let h ij ,t be the coefficient on r j ,t in this expansion of r i,t+2 . Explicitly,

$h ij ,t = j W ij,t+2 1 -w s i,t+2 W jj ,t+1 (1 -w s j ,t ). (B.$3)

The coefficient h ij ,t adds up the influence of r j ,t on r i,t+2 over all paths of length two. First, we establish a lemma about how much these weights vary.

Lemma 6. There exists γ such that for n sufficiently large, when i and i have the same network types and j and j have the same network and signal types, the ratio

$h ij ,t /h i j ,t is in [1 -γ/n, 1 + γ/n].$Proof. Fix i and j . For each network type k and signal type τ , consider the number of agents j of network type k and signal type τ who are observed by i and who observe j . This number varies by at most a factor ζ 2 as we vary i and j , preserving signal and network types.

For each such j, the contribution to h ij ,t due to weight on that agent's action is (recalling

$(B.3)) W ij,t+2 1 -w s i,t+2 W jj ,t+1 (1 -w s j ,t ).$By applying Lemma 3 repeatedly, we can choose γ 1 such that each of these contributions varies by at most a factor of γ 1 /n as we change i in G k and j in G k . Thus, h ij ,t is a sum of terms which vary by at most a multiplicative factor of γ 1 /n as we vary i and j preserving signal and network types. If we can show that the sum of the absolute values of these terms is bounded, then it will follow that h ij ,t varies by at most a multiplicative factor of γ/n for some n. This bound on the sum of absolute values follows from the calculation of weights in the proof of Lemma 3.

Step 5(b): We first show that fixing the values of ψ kk and ψ k in period t, the variation in the covariances Cov(r i,t+2θ t+1, r i ,t+2θ t+1 ) of these terms as we vary i and i over network types is not larger than ζ. From the formula above, we observe that we can decompose r i,t+2θ t+1 as a linear combination of three mutually independent groups of terms:

(i) signal error terms η j,t+1 and η j ,t ;

(ii) the errors r j ,tθ t in the social signals from period t; and (iii) changes in state ν t and ν t+1 between periods t and t + 2. Note that the terms r j ,tθ t are linear combinations of older signal errors and changes in the state. We bound each of the three groups in turn:

(i) Signal errors: We first consider the contribution of signal errors. When i and i are distinct, the number of such terms is close to its expected value because we are conditioning on the events E 2 and E 4 defined in Section B.1. Moreover the weights are close to their expected values by Step 2, so the variation is bounded suitably. When i and i are equal, we use the facts that the weights are close to their expected values and the variance of an average of Ω(n) signals is small.

(ii) Social signals: We now consider terms r j ,tθ t , which correspond to the third summand in our expression for r i,t+2 . Since we will analyze the weight on ν t below, it is sufficient to study the terms r j ,tθ t-1 .

By Lemma 6, the coefficients placed on r j ,t by i and on r j ,t by i vary by a factor of at most 2γ/n. Moreover, the absolute value of each of these covariances is bounded above by ψ and the variation in these terms is bounded above by ζ. We conclude that the variation from these terms has order 1/n 2 .

(iii) Innovations: Finally, we consider the contribution of the innovations ν t and ν t+1 . We treat ν t+1 first. We must show that any two agents of the same types place the same weight on the innovation ν t+1 (up to an error of order 1 n 2 ). This will imply that the contributions of timing to the covariances Cov(r i,t+2θ t+1, r i ,t+2θ t+1 ) can be expressed as a term that can be included in the relevant ψ kk and a lower-order term which can be included in ζ ii .

The weight an agent places on ν t+1 is equal to the weight she places on signals from period t + 1. So this is equivalent to showing that the total weight

$ρ j W ij,t+2 1 -w s i,t+2 w s j,t+1$agent i places on period t + 1 depends only on the network type k of agent i and O p (1/n 2 ) terms. We will first show the average weight placed on time-(t + 1) signals by agents of each signal type depends only on k. We will then show that the total weights on agents of each signal type do not depend on n.

Suppose for simplicity here that there are two signal types A and B; the general case is the same. We can split the sum from the previous paragraph into the subgroups of agents with signal types A and B:

$ρ j:σ 2 j =σ 2 A W ij,t+2 1 -w s i,t+2 w s j,t+1 + ρ j:σ 2 j =σ 2 B W ij,t+2 1 -w s i,t+2$w s j,t+1 .

$Letting W A i = σ 2 j =σ 2 A W ij,t+2 1-w s i,t+2$be the total weight placed on agents with signal type A and similarly for signal type B, we can rewrite this as:

$W A i ρ j:σ 2 j =σ 2 A W ij,t+2 W A i (1 -w s i,t+2 ) w s j,t+1 + W B i ρ j:σ 2 j =σ 2 B W ij,t+2 W B i (1 -w s i,t+2 ) w s j,t+1 .$The coefficients

$W ij,t+2 W A i (1-w s i,t+2$) in the first sum now sum to one, and similarly for the second. We want to check that the first sum j:σ 2

$j =σ 2 A W ij,t+2 W A i (1-w s i,t+2$) w s j,t+1 does not depend on k, and the second sum is similar.

For each j in group A,

$w s j,t+1 = σ -2 A σ -2 A + (ρ 2 κ j,t+1 + 1) -1$, where we define κ 2 j,t+1 = Var(r j,t+1θ t ) to be the error variance of the social signal. Because κ j,t+1 is close to zero, we can approximate w s j,t+1 locally as a linear function µ 1 κ j,t+1 + µ 2 where µ 1 < 1 (up to order 1 n 2 terms). So we can write the sum of interest as

$j:σ 2 j =σ 2 A W ij,t+2 W A i (1 -w s i,t+2 ) µ 1 j ,j$W jj ,t+1 W jj ,t+1 (ρ 2 V j j ,t + 1) + µ 2 .

By Lemma 3, the weights vary by at most a multiplicative factor contained in [1-γ/n, 1+γ/n]. The number of paths from i to j passing through agents of any network type k and any signal type is close to its expected value (which depends only on i's network type), and the weight on each path depends only on the types involved up to a factor in [1γ/n, 1 + γ/n].

The variation in V j j ,t consists of terms of the form ψ k k , ψ k , and ζ j j , all of which are O p (1/n), and terms from signal errors η j ,t . The signal errors only contribute when j = j , and so only contribute to a fraction of the summands of order 1/n. So we can conclude the total variation in this sum as we change i within the network type k has order 1/n 2 . Now that we know each the average weight on private signals of the observed agents of each signal type depends only on k, it remains to check that W A i and W B i only depend on k. The coefficients W A i and W B i are the optimal weights on the group averages

$j:σ 2 j =σ 2 A W ij,t+2 W A i (1 -w s i,t+2 )$ρa j,t+1 and

$j:σ 2 j =σ 2 B W ij,t+2 W B i (1 -w s i,t+2 )$ρa j,t+1 , so we need to show that the variances and covariance of these two terms depend only on k.

We check the variance of the first sum: we can expand

$σ 2 j =σ 2 A W ij,t+2 W A i (1 -w s i,t+2 ) ρa j,t+1 = σ 2 j =σ 2 A W ij,t+2 W A i (1 -w s i,t+2 )$ρ w s j,t+1 s j,t+1 + (1w s j,t+1 )r j,t+1 .

We can again bound the signal errors and social signals as in the previous parts of this proof, and show that the variance of this term depends only on k up to error terms that are O p (1/n 2 ).

The second variance and covariance are similar, so W A i and W B i depend only on k up to error terms that are O p (1/n 2 ).

This takes care of the innovation ν t+1 . Because we have included any innovations prior to ν t in the social signals r j ,t , to complete Step 5(b) we need only show the weight on ν t depends only on the network type k of an agent.

The analysis is a simpler version of the analysis of the weight on ν t+1 . It is sufficient to show the total weight placed on period t social signals depends only on the network type of k of an agent i. This weight is equal to

$ρ 2 j,j W ij,t+2 1 -w s i,t+2 • W jj ,t+1 • (1 -w s j ,t ).$As in the ν t+1 case, we can approximate (1w s j ,t ) as a linear function of κ j ,t up to O p (1/n 2 ) terms. Because the number of paths to each agent j though a given type and the weights on each such path cannot vary too much within types, the same argument shows that this sum depends only on k, up to error terms that are O p (1/n 2 ). Thus Step 5(b) is complete.

Step 5(c): The final step is to verify that we can take ψ kk and ψ k to be smaller than ψ. It is sufficient to show that the variance Var(r i,t+2θ t+1 ) of each social signal about θ t+1 is at most ψ. The proof is the same as in Step 2(b).

## LEARNING FROM NEIGHBORS ABOUT A CHANGING STATE ONLINE APPENDIX

KRISHNA DASARATHA, BENJAMIN GOLUB, AND NIR HAK This document contains supporting material for the paper "Learning from Neighbors about a Changing State," which herein we refer to as the "main paper" or simply "paper."

OA1. Numerical results in real networks (online appendix)

The message of Section 4 is that signal diversity enables good aggregation, and signal homogeneity obstructs it. The theoretical results in that section, however, were asymptotic, and the good-aggregation result used some assumptions on the distribution of graphs. In this section we show that the substantive message applies to realistic networks with moderate degrees. We do this by computing equilibria for actual social networks from the data in Banerjee, Chandrasekhar, Duflo, and Jackson (2013b). This data set contains the social networks of villages in rural India. 1 There are 43 networks in the data, with an average network size of 212 nodes (standard deviation = 53.5), and an average degree of 19 (standard deviation = 7.5).

Our simulation exercises measure the benefits of heterogeneity for equilibrium aggregation. For each network, we calculate the equilibrium with ρ = 0.9 for two types of environments. The first is the homogeneous case, with all signal variances set to 2. The second is a heterogeneous case, where half of the agents have a signal variance greater than 2 and half of villagers have a signal variance less than 2, chosen to hold constant the total amount of information that reaches the community via private signals. That is, we set the signal variances so that the average precision in each village is 1 2 , as in the homogeneous case. This signal assignment holds fixed the average utility when all villagers are autarkic, or equivalently holds fixed the average utility when all villagers know the state θ t-1 in the previous period exactly. At the same time, it varies the level of heterogeneity in signal endowments. Villagers are randomly assigned to better or worse private signals, and the simulation results do not depend substantially on the realized random assignment. Our outcomes will be the average social signal error variance in each village and the average social signal error variance across all villages.

Date: November 1, 2022.

## 1

We take the networks that were used in the estimation in Banerjee, Chandrasekhar, Duflo, and Jackson  (2013a). As in their work, we take every reported relationship to be reciprocal for the purposes of sharing information. This makes the graphs undirected.

0.35 0.4 0.45 0.5 0.55 0.6 0.65 Average Aggregation Error (Heterogeneous Signals) 0.35 0.4 0.45 0.5 0.55 0.6 0.65 Average Aggregation Error (Homogeneous Signals) (a) 45 Degree Line Average 2 2.2 2.4 2.6 2.8 3 3.2 3.4 3.6 3.8 4 Private Signal Variance variance of agents in each village, in the homogeneous and heterogeneous cases. In the homogeneous case all agents have private signal variance 2. In the heterogeneous case, half of agents have private signal variance 3 2 and half of agents have private signal variance 3. (b) The average social signal variance for all agents as we vary the worse private signal variance from 2 to 4 and hold fixed the average precision of private signals.

It is useful to begin by looking at the equilibrium average aggregation errors, i.e., social signal variances, in the case of homogeneous signals. This is the horizontal coordinate in Figure [OA1](#fig_3).1(a); each village is a data point, and the points have a standard deviation of 0.013. In this case, differences in learning outcomes are due only to differences in the network structure, and we will call this number the network-driven variation. Now we introduce some private signal diversity. In our first exercise, we change the variance of the worse private signal from 2 (homogeneous signals) to 3 (heterogeneous signals), and adjust the other variance as discussed above to hold fixed the total amount of information coming into the network. The vertical coordinate in Figure [OA1](#fig_3).1(b) depicts the equilibrium aggregation error in each village. The average of this number across all villages falls to 0.470, compared to 0.555 (in the homogeneous case). Therefore, adding heterogeneity by increasing the private signal variance for half of the agents by 50% changes social signal error variance by 6.5 times the network-driven variation. Learning is much better with some private signal heterogeneity than in villages with very favorable networks (i.e., those that achieve the best aggregation under homogeneous signals).

In Figure [OA1](#fig_3).1(b), rather than working with the particular choice of 3 for the variance of the private signal, we look across all choices of this variance between 2 and 4 and plot the average equilibrium social signal variance across all villages.

Figure [OA1](#fig_3).1(b) also sheds light on the value of a small amount of heterogeneity. The results in Section 4 can be summarized as saying that, to achieve the aggregation benchmark of essentially knowing the previous period's state, there need to be at least two different private signal variances in the network. Formally, this is a knife-edge result: As long as private signal variances differ at all, then as n → ∞, aggregation errors vanish; with exactly homogeneous signal endowments, aggregation errors are much higher. The figure shows that the transition from the first regime to the second is actually gradual. In particular, a very small amount of heterogeneity provides little benefit in finite networks, as there is not enough diversity of signal endowments for villagers to anti-imitate. However, a 50% change in the variance of one of the signals (equivalently, a 22% change in its standard deviation) makes the community much better able to use the same total amount of information.

## OA2. Identification and testable implications (online appendix)

One of the main advantages of the parametrization we have studied is that standard methods can easily be applied to estimate the model and test hypotheses within it. The key feature making the model econometrically well-behaved is that, in the solutions we focus on, agents' actions are linear functions of the random variables they observe. Moreover, the evolution of the state and arrival of information creates exogenous variation. We briefly sketch how these features can be used for estimation and testing.

Assume the following. The analyst obtains noisy measurements a i,t = a i,t + ξ i,t of agent's actions (where ξ i,t are i.i.d., mean-zero error terms). He knows the parameter ρ governing the stochastic process, but may not know the network structure or the qualities of private signals (σ i ) n i=1 . Suppose also that the analyst observes the state θ t ex post (perhaps with a long delay). [2](#) Now, consider any steady state in which agents put constant weights W ij on their neighbors and w s i on their private signals over time. We will discuss the case of m = 1 to save on notation, though all the statements here generalize readily to arbitrary m.

We first consider how to estimate the weights agents are using, and to back out the structural parameters of our model when it applies. The strategy does not rely on uniqueness of equilibrium. We can identify the weights agents are using through standard vector autoregression methods. In steady state,

$a i,t = j W ij ρa j,t-1 + w s i θ t + ζ i,t , (OA-1)$where ζ i,t = w s i η i,tj W ij ρξ j,t-1 +ξ i,t are error terms i.i.d. across time. The first term of this expression for ζ i,t is the error of the signal that agent i receives at time t. The summation combines the measurement errors from the observations a j,t-1 from the previous period.[foot_38](#foot_38) Thus, we can obtain consistent estimators W ij and w s i for W ij and w s i , respectively.

We now turn to the case in which agents are using equilibrium weights. First, and most simply, our estimates of agents' equilibrium weights allow us to recover the network structure. If the weight W ij is non-zero for any i and j, then agent i observes agent j. Generically the converse is true: if i observes j then the weight W ij is non-zero. Thus, network links can generically be identified by testing whether the recovered social weights are nonzero. For such tests (and more generally) the standard errors in the estimators can be obtained by standard techniques. [4](#) Now we examine the more interesting question of how structural parameters can be identified assuming an equilibrium is played, and also how to test the assumption of equilibrium.

The first step is to compute the empirical covariances of action errors from observed data; we call these V ij . Under the assumption of equilibrium, we now show how to determine the signal variances using the fact that equilibrium is characterized by Φ( V ) = V and recalling the explicit formula (3.3) for Φ. In view of this formula, the signal variances σ 2 i are uniquely determined by the other variables:

$V ii = j k W ij W ik ρ 2 V jk + 1 + ( w s i ) 2 σ 2 i . (OA-2)$Replacing the model parameters other than σ 2 i by their empirical analogues, we obtain a consistent estimate σ 2 i of σ i . This estimate could be directly useful-for example, to an analyst who wants to choose an "expert" from the network and ask about her private signals.

Note that our basic VAR for recovering the weights relies only on constant linear strategies and does not assume that agents are playing any particular strategy within this class. Thus, if agents are using some other behavioral rule (e.g., optimizing in a misspecified model) we can replace (OA-2) by a suitable analogue that reflects the bounded rationality in agents' inference. If such a steady state exists, and using the results in this section, one can create an econometric test that is suitable for testing how agents are behaving. For instance, we can test the hypothesis that they are Bayesian against the naive alternative of our Section 5.1.

## OA3. Details of definitions (online appendix)

OA3.1. Exogenous random variables. Fix a probability space (Ω, F, P). Let (ν t , η i,t ) t∈Z,i∈N be normal, mutually independent random variables, with ν t having variance 1 and η i,t having variance σ 2 i . Also take a stochastic process (θ t ) t∈Z , such that for each t ∈ Z, we have (for 0 < |ρ| ≤ 1)

$θ t = ρθ t-1 + ν t .$Such a stochastic process exists by standard constructions of the AR(1) process or, in the case of ρ = 1, of the Gaussian random walk on a doubly infinite time domain. Define s i,t = θ t +η i,t .

OA3.2. Formal definition of game and stationary linear equilibria. Players and strategies. The set of players (or agents) is A = {(i, t) : i ∈ N, t ∈ Z}. The set of (pure) responses of an agent (i, t) is defined to be the set of all Borel-measurable functions ξ (i,t) : R × (R |N (i)| ) m → R, mapping her own signal and her neighborhood's actions, (s i,t , (a N i ,t-ℓ ) m ℓ=1 ), to a real-valued action a i,t . We call the set of these functions Ξ (i,t) . Let Ξ = (i,t)∈A Ξ (i,t) be the set of response profiles. We now define the set of (unambiguous) strategy profiles, Ξ ⊂ Ξ. We say that a response profile ξ ∈ Ξ is a strategy profile if the following two conditions hold 1. There is a tuple of real-valued random variables (a i,t ) i∈N,t∈Z on (Ω, F, P) such that for each (i, t) ∈ A, we have

$a i,t = ξ (i,t) (s i,t , (a N i ,t-ℓ ) m ℓ=1 ) .$2. Any two tuples of real-valued random variables (a i,t ) i∈N,t∈Z satisfying Condition 1 are equal almost surely.

That is, a response profile is a strategy profile if there is an essentially unique specification of behavior that is consistent with the responses: i.e., if the responses uniquely determine the behavior of the population, and hence payoffs. 5 Note that if ξ ∈ Ξ, then it can be checked that ξ = (ξ ′ (i,t) , ξ -(i,t) ) ∈ Ξ whenever ξ ′ (i,t) ∈ Ξ (i,t) . Thus, if we start with a strategy profile and consider agent (i, t)'s deviations, they are unrestricted: she may consider any response. Payoffs. The payoff of an agent (i, t) under any strategy profile ξ ∈ Ξ is

$u i,t (ξ) = -E (a i,t -θ t ) 2 ∈ [-∞, 0],$where the actions a i,t are taken according to ξ (i,t) and the expectation is taken in the probability space we have described. This expectation is well-defined because inside the expectation there is a non-negative, measurable random variable, for which an expectation is always defined, though it may be infinite. Equilibria. A (Nash) equilibrium is defined to be a strategy profile ξ ∈ Ξ such that, for each (i, t) ∈ A and each ξ ∈ Ξ such that ξ = (ξ ′ (i,t) , ξ -(i,t) ) for some ξ ′ (i,t) ∈ Ξ (i,t) , we have

$u i,t ( ξ) ≤ u i,t (ξ).$5 Condition 1 is necessary to rule out response profiles such as the one given by ξ i,t (s i,t , a i,t-1 ) = |a i,t-1 | + 1. This profile, despite consisting of well-behaved functions, does not correspond to any specification of behavior for the whole population (because time extends infinitely backward). Condition 2 is necessary to rule out response profiles such has the one given by ξ i,t (s i,t , a i,t-1 ) = a i,t-1 , which have many satisfying action paths, leaving payoffs undetermined.

For p ∈ Z, we define the shift operator T p to translate variables to time indices shifted p steps forward. This definition may be applied, for example, to Ξ.[foot_40](#foot_40) A strategy profile ξ ∈ Ξ is stationary if, for all p ∈ Z, we have T p ξ = ξ.

We say ξ ∈ Ξ is a linear strategy profile if each ξ i is a linear function. Our analysis focuses on stationary, linear equilibria.

## OA4. Remaining proofs (online appendix)

OA4.1. Details of calculations for equation (3.1) for best-response actions. Let P = N (µ prior , σ 2 prior ) be a normal prior over θ t . First, we will establish that for any ℓ ≥ 0 we have

$E P [θ t | ρ ℓ a i,t-ℓ ] = ρ ℓ a i,t-ℓ .$(OA-1)

Note that

$θ t = ρ ℓ θ t-ℓ + ℓ k=1 ρ ℓ-k ν t-ℓ+k . (OA-2)$Take expectations E P [• | ρ ℓ a i,t-ℓ ] conditional on a i,t-ℓ on both sides and note that the summation makes no contribution, since the innovations are independent of a i,t-ℓ and have mean zero. Now we will show that E P [θ t-ℓ | a i,t-ℓ ] = a i,t-ℓ , which will establish (OA-1). To this end, note

$E P [θ t-ℓ -a i,t-ℓ | a i,t-ℓ ] = E P [E P [θ t-ℓ -a i,t-ℓ | z i,t-ℓ ] | a i,t-ℓ ] tower property = E P [a i,t-ℓ -a i,t-ℓ | a i,t-ℓ ] see below = 0.$For the penultimate step, we first observe that by definition a i,t-ℓ minimizes E P [(θ t-ℓ -a i,t-ℓ ) 2 ] given the information z i,t-ℓ , and so (by the characterization of the conditional expectation as minimizing expected squared-error loss) we must have a i,t-ℓ = E P [θ t-ℓ | z i,t-ℓ ]. We also use that a i,t-ℓ is measurable with respect to z i,t-ℓ , so that E P [a i,t-ℓ | z i,t-ℓ ] = a i,t-ℓ . This completes the proof of (OA-1).

Now to establish the updating formula (3.1), fix an agent (i, t) and write z := z i,t . We will transform updating problem into a canonical form to apply standard results on Bayesian conditioning with normal distributions. Let the unknown parameters θ t and ν t ′ for t ′ ∈ {tm + 1, tm + 2, . . . , t}; call the vector of all these parameters v. The agent's information z can be written as z = Zv + y where Z is a matrix and y is a vector which has mean zero and is independent of v. Then standard results on Gaussian updating imply the posterior mean of θ t can be expressed as

$E P [θ t | z] = Y P := ωµ prior + (1 -ω)λ * z (OA-3)$where µ prior is the prior mean of θ t and λ * is a row vector that does not depend on the prior. Also, as the prior on θ t becomes diffuse, so that σ -2 prior → 0, we have ω → 0. We claim λ * 1 = 1. To show this, take an unconditional expectation in (OA-3). The lefthand side equals E P [Y P ] = µ prior irrespective of the prior P . On the right hand-side, for every k, we have that E[z k ] = µ prior for every k as well.[foot_41](#foot_41) These facts imply the claim. Now consider the problem of finding a row vector λ satisfying λ1 = 1 to minimize E(λ) = E[(λzθ t ) 2 ] . We claim that λ * solves this problem. If not, then there is a λ * * such that E(λ * * ) < E(λ * ) and then for a sufficiently diffuse prior, the z-measurable random variable

$Y = λ * * z satisfies E P [(Y -θ t ) 2 ] < E P [(Y P -θ t ) 2 ]$, in contradiction to a property of conditional expectation of being the best L 2 approximation to θ t . Now we characterize λ * . Note that the following equations hold whenever λ1 = 1.

$E(λ) = E[(λz -θ t ) 2 ] = Var [λz -θ t ] = Var k λ k (z k -θ t ) = k,k ′ Cov [z k -θ t , z k ′ -θ t ] = λC i,t λ ⊤ .$Thus λ * may be characterized as minimizing λC i,t λ ⊤ subject to λ1 = 1. The first-order conditions give that λ * C = γ1 for some constant γ and then using the constraint gives the solution

$λ * = 1 ⊤ C -1 1 ⊤ C -1 1 .$OA4.2. Proof of Proposition 2. We first check there is a unique equilibrium and then prove the remainder of Proposition 2.

Lemma 1. Suppose G has symmetric neighbors. Then there is a unique equilibrium.

Proof of Lemma 1. We will show that when the network satisfies the condition in the proposition statement, Φ induces a contraction on a suitable space. For each agent, we can consider the variance of the best estimator for yesterday's state based on observed actions. We can analyze these variances using the envelope theorem. Moreover, the space of these variances is a sufficient statistic for determining all agent strategies and action variances. Let r i,t be i's social signal-the best estimator of θ t-1 based on the period t -1 actions of agents in N i -and let κ 2 i,t be the variance of r i,tθ t-1 .

We claim that Φ induces a map Φ on the space of variances κ 2 i,t , which we denote V. We must check the period t variances (κ 2 i,t ) i uniquely determine all period t + 1 variances (κ 2 i,t+1 ) i : The variance V ii,t of agent i's action, as well as the covariances V ii ′ ,t of all pairs of agents i, i ′ with N i = N i ′ , are determined by κ 2 i,t . Moreover, by the condition on our network, these variances and covariances determine all agents' strategies in period t + 1, and this is enough to pin down all period t + 1 variances κ 2 i,t+1 . The proof proceeds by showing Φ is a contraction on V in the sup norm. For each agent j, we have N i = N i ′ for all i, i ′ ∈ N j . So the period t actions of an agent i ′ in N j are

$a i ′ ,t = (ρ 2 κ 2 i,t + 1) -1 σ -2 i ′ + (ρ 2 κ 2 i,t + 1) -1 • r i,t + σ -2 i ′ σ -2 i ′ + (ρ 2 κ 2 i,t + 1) -1 • s i ′ ,t (OA-4)$where s i ′ ,t is agent (i ′ )'s signal in period t and r i,t the social signal of i (the same one that i ′ has). It follows from this formula that each action observed by j is a linear combination of a private signal and a common estimator r i,t , with positive coefficients which sum to one. For simplicity we write

$a i ′ ,t = b 0 • r i,t + b i ′ • s i ′ ,t(OA-5)$(where b 0 and b i ′ depend on i ′ and t, but we omit these subscripts). We will use the facts 0 < b 0 < 1 and 0 < b i ′ < 1. We are interested in how κ 2 j,t+1 = Var(r j,t+1θ t ) depends on κ 2 i,t = Var(r i,tθ t-1 ). The estimator r j,t+1 is a linear combination of observed actions a i ′ ,t , and therefore can be expanded as a linear combination of signals s i ′ ,t and the estimator r i,t . We can write

$r j,t+1 = c 0 • (ρr i,t ) + i ′ c i ′ s i ′ ,t (OA-6)$and therefore (taking variances of both sides)

$κ 2 j,t+1 = Var(r j,t+1 -θ t ) = c 2 0 Var(ρr i,t -θ t ) + i ′ c 2 i ′ σ 2 i ′ = c 2 0 (ρ 2 κ 2 i,t + 1) + i ′ c 2 i ′ σ 2 i ′$The desired result, that Φ is a contraction, will follow if we can show that the derivative

$dκ 2 j,t+1 dκ 2 i,t = c 2 0 ρ 2 ∈ [0, δ]$for some δ < 1. By the envelope theorem, when calculating this derivative, we can assume that the weights placed on actions a i ′ ,t by the estimator r j,t+1 do not change as we vary κ 2 i,t , and therefore c 0 and the c i ′ above do not change. So it is enough to show the coefficient c 0 is in [0, 1].

The intuition for the lower bound is that anti-imitation (agents placing negative weights on observed actions) only occurs if observed actions put too much weight on public information. But if c 0 < 0, then the weight on public information is actually negative so there is no reason to anti-imitate. This is formalized in the following lemma.

Lemma 2. Suppose j has symmetric neighbors. Then the social signal of an agent at node j places non-negative weight on a neighbor i's social signal from the previous period, i.e., c 0 ≥ 0.

Proof. To check this formally, suppose that c 0 is negative. Then the social signal r j,t+1 puts negative weight on some observed action-say the action a k,t of agent k. We want to check that the covariance of r j,t+1θ t and a k,tθ t is negative. Using (OA-5) and (OA-6), we compute that

$Cov(r j,t+1 -θ t , a k,t -θ t ) = Cov   c 0 (ρr i,t -θ t ) + i ′ ∈Nj c i ′ (s i ′ ,t -θ t )), b 0 (ρr i,t -θ t ) + b k (s k,t -θ t )   = c 0 b 0 Var(ρr i,t -θ t ) + c k b k Var(s k,t -θ t )$because all distinct summands above are mutually independent. We have b 0 , b k > 0, while c 0 < 0 by assumption and c k < 0 because the estimator r j,t+1 puts negative weight on a k,t . So the expression above is negative. Therefore, it follows from the usual Gaussian Bayesian updating formula that the best estimator of θ t given r j,t+1 and a k,t puts positive weight on a k,t . However, this is a contradiction: the best estimator of θ t given r j,t+1 and a k,t is simply r j,t+1 , because r j,t+1 was defined as the best estimator of θ t given observations that included a k,t . This completes the proof of Lemma 2. □

We now complete the proof of Lemma 1. For the upper bound c 0 ≤ 1, the idea is that r j,t+1 puts more weight on agents with better signals while these agents put little weight on public information, which keeps the overall weight on public information from growing too large.

Note that r j,t+1 is a linear combination of actions ρa i ′ ,t for i ′ ∈ N j , with coefficients summing to 1. The only way the coefficient on ρr i,t in r j,t+1 could be at least 1 would be if some of these coefficients on ρa i ′ ,t were negative and the estimator r j,t+1 placed greater weight on actions a i ′ ,t which placed more weight on r i,t .

Applying the formula (OA-4) for a i ′ ,t , we see that the coefficient b 0 on ρr i,t is less than 1 and increasing in σ i ′ . On the other hand, it is clear that the weight on a i ′ ,t in the social signal r j,t+1 is decreasing in σ i ′ : more weight should be put on more precise individuals. So in fact the estimator r j,t+1 places less weight on actions a i ′ ,t which placed more weight on r i,t .

Moreover, the coefficients placed on private signals are bounded below by a positive constant when we restrict to covariances in the image of Φ (because all covariances are bounded as in the proof of Proposition 1). Therefore, each agent (i ′ , t) with i ′ ∈ N j places weight at most one on the estimator ρr i,t-1 . Agent j's social signal r j,t+1 is a sum of these agents' actions with coefficients summing to 1 and satisfying the monotonicity property above. We conclude that the coefficient on ρr i,t in the expression for r j,t+1 is at most one. This completes the proof of Lemma 1. □

We now prove Proposition 2.

Proof of Proposition 2. By Lemma 1 there is a unique equilibrium on any network G with symmetric neighbors. Let ε > 0.

Consider any agent (i, t). Her neighbors have the same private signal qualities and the same neighborhoods (by the symmetric neighbors assumption). So there exists an equilibrium where for all i, the actions of agent (i, t)'s neighbors are exchangeable. By uniqueness, this in fact holds at the sole equilibrium.

So agent (i, t)'s social signal is an average of her neighbors' actions:

$r i,t = 1 |N i | j∈N i a j,t-1 .$Suppose the ε-aggregation benchmark is achieved. Then all agents must place weight at least

$(1+ε) -1 (1+ε) -1 +σ -$2 on their social signals. So at time t, the social signal r i,t places weight at least (1+ε) -1 (1+ε) -1 +σ -2 on signals from t -2 or earlier. Let Y be any linear combination of signals from t -2 or earlier with weights summing to 1. Then E[(Yθ t-1 ) 2 ] ≥ 1.[foot_42](#foot_42) It follows that for ε sufficiently small the social signal r i,t is bounded away from a perfect estimate of θ t-1 . This gives a contradiction. □ OA4.3. Proof of Corollary 1. Consider a complete graph in which all agents have signal variance σ 2 and memory m = 1. By Proposition 2, as n grows large the variances of all agents converge to

$A > (1 + σ -2 ) -1 .$Choose σ 2 large enough such that A > 1. To see that we can do this, note that as σ 2 grows large, the weight each agent places on their private signal vanishes. So the weight on signals from at least k periods ago approaches one for any k. Taking σ 2 such that this holds for k sufficiently large, we have A > 1.

Now suppose that we increase σ 2 1 to ∞. Then a 1,t = r 1,t in each period, so all agents can infer all private signals from the previous period. As n grows large, the variance of agent 1 converges to 1 and the variances of all other agents converge to (1 + σ -2 ) -1 . By our choice of σ 2 , this gives a Pareto improvement. We can see by continuity that the same argument holds for σ 2 1 finite but sufficiently large.

OA4.4. Proof of Corollary 2. Our goal is to estimate Var(a i,tθ t ). First, observe that

$a i,t = w s s i,t + (1 -w s ) 1 n j a j,t-1 = w s (θ t + ε i,t ) + (1 -w s ) 1 n j a j,t-1 .$This implies, inductively, that

$a i,t -θ t = w s ε i,t + ∞ ℓ=1 w s (1 -w s ) ℓ (θ t -θ t-ℓ + ζ t ) ,$where the ζ t are mean-zero random variables independent of all other random variables in the expression. (They are linear combinations of agents' signal noise realizations.) Thus,

$Var(a i,t -θ t ) ≥ Var w s ∞ ℓ=1 (1 -w s ) ℓ (θ t -θ t-ℓ ) Noting that θ t = ∞ k=0 ρ k ν t-k , we may write θ t -θ t-ℓ = ℓ-1 k=0 ρ k ν t-k and therefore Var(a i,t -θ t ) ≥ Var w s ∞ ℓ=1 (1 -w s ) ℓ ℓ-1 k=0 ρ k ν t-k = w 2 s Var ∞ k=0 ν t-k ρ k ∞ ℓ=k+1 (1 -w s ) ℓ = w 2 s Var ∞ k=0 ν t-k ρ k ∞ ℓ=k+1 (1 -w s ) ℓ = w 2 s ∞ k=0 ρ k ∞ ℓ=k+1 (1 -w s ) ℓ 2 = (1 -w s ) 2 1 -(1 -w s ) 2 ρ 2 . This proves the bound Var(a i,t -θ t ) ≥ (1 -w s ) 2 1 -(1 -w s ) 2 ρ 2 .$It remains to show the variances diverge to infinity as σ 2 → ∞ and ρ → 1 from below. Choose a sequence of pairs (σ 2 , ρ) → (∞, 1). If w s → 0 along any subsequence of this sequence, then along the subsequence we have (1-ws) [2](#)1-(1-ws) 2 ρ 2 → ∞ and so Var(a i,tθ t ) → ∞ as well. If w s is non-vanishing, then Var(a i,tθ t ) → ∞ since the action variance is at least w 2 s σ 2 and σ 2 → ∞. Finally, note that these bounds are both independent of n, so Var(a i,t -θ t ) → ∞ uniformly in n.

OA4.5. Proof of Theorem 2. Suppose that all private signals have variance σ 2 > 0. Fix a sequence of networks G n and an equilibrium on each G n . We will show that given any constant C > 0 and any sequence of equilibria, the fraction of agents i such that

$κ 2 i ≤ C d is bounded away from one.$We first prove the result in the case m = 1. For each n, let G n be the set of agents i satisfying

$κ 2 i ≤ C d ,$i.e., the set of agents who do learn well. Assume for the sake of contradiction that |Gn| n → 1 as n → ∞ along some subsequence and pass to that subsequence.

For each j, we can express the action a j,t as a weighted sum of innovations and signal errors,[foot_43](#foot_43) with all terms on the right-hand side conditionally independent:

$a j,t = θ t - ∞ l=0 w j,t (ν t-l )(ρ l ν t-l ) + l,j ′ w j,t (η j ′ ,t-l )(ρ l η j ′ ,t-l ).$This expression is unique.

Lemma 3. For all j ∈ G n we must have

$w j,t (ν t ) ∈ 1 σ -2 + 1 - C ′ d , 1 σ -2 + 1$for some C ′ > 0 (independent of j and n).

Proof. By the standard updating formula, the optimal weight w j,t (ν t ) is

$(ρ 2 κ 2 j,t +1) -1 (ρ 2 κ 2 j,t +1) -1 +σ -2$, where κ 2 j,t is the variance of the best estimator of θ t-1 based on (j, t)'s social observations. The upper bound follows because this is minimized when κ 2 j,t = 0. For the lower bound,

$w j,t (ν t ) = (ρ 2 κ 2 j,t + 1) -1 (ρ 2 κ 2 j,t + 1) -1 + σ -2 = 1 (1 + σ -2 ) + σ -2 ρ 2 κ 2 j,t = 1 1 + σ -2 - σ -2 ρ 2 (1 + σ -2 ) 2 κ 2 j,t + O(κ 4 j,t$). For κ 2 j,t in any neighborhood of zero, we can choose C ′′ such that the non-constant terms in the final expression are bounded below by -C ′′ κ 2 j,t . Since by assumption we have κ 2 j,t ≤ C d , the lemma follows with C ′ = C • C ′′ . □ There are at most (n-|G n |)d in-coming links to agents outside G n . Each agent who observes at least 2(n-|Gn|) n • d agents outside G n accounts for at least 2(n-|Gn|) n • d of those links, so there can be at most n 2 such agents. Since |Gn| n → 1, there is an agent i ∈ G n who observes fewer than 2(n-|Gn|) n • d agents outside G n . Consider the action of such an agent

$i ∈ G n in period t + 1. Since κ 2 i,t+1 ≤ C d ,$the weight on the innovation from the previous period satisfies

$(w i,t+1 (ν t )ρ) 2 ≤ C d . (OA-7)$On the other hand, we can express this weight in terms of neighbors' weights as

$w i,t+1 (ν t ) = j ρw ij,t+1 w j,t (ν t ).$We will show that if this weight w i,t+1 (ν t ) vanishes, then the contribution of private signal errors to κ i,t+1 must grow asymptotically faster than 1/d. We can split this summation as

$w i,t+1 (ν t ) = ρ j∈Gn w ij,t+1 w j,t (ν t ) + ρ j / ∈Gn w ij,t+1 w j,t (ν t ).$We now consider two cases, depending on whether j / ∈Gn |w ij,t+1 | → 0, i.e., whether the sum of the absoulte values of the weights on agents outside G n is vanishing.

Case 1: lim inf n j / ∈Gn |w ij,t+1 | = 0. We can pass to a subsequence along which j / ∈Gn |w ij,t+1 | → 0.

We claim that it follows from the bounds on w j,t (ν t ) in Lemma 3 that this can only occur

$if j |w ij,t+1 | → ∞. If j |w ij,t+1 | is bounded, w i,t+1 (ν t ) = ρ j∈Gn w ij,t+1 w j,t (ν t ) + ρ j / ∈Gn w ij,t+1 w j,t (ν t ) = ρ j∈Gn w ij,t+1 w j,t (ν t ) + o(1).$The second equality holds because j / ∈Gn |w ij,t+1 | → 0 and w j,t (ν t ) ∈ [0, 1] for all j. Therefore,

$w i,t+1 (ν t ) = ρ j∈Gn w ij,t+1 w j,t (ν t ) + o(1) ≥ ρ 1 1 + σ -2 σ -2 σ -2 + 1 -o (1) + o(1),$and the right-hand side is non-vanishing. Here the first term on the right-hand side is the limit of the sum if all of the terms w j,t (ν t ) were equal to the upper bound σ -2 σ -2 +1 . The first o(1) error term corresponds to the variation in w j,t (ν t ) across j, which is O( 1 d ) by Lemma 3 and has bounded coefficients. Thus w i,t+1 (ν t ) is non-vanishing, but this contradicts the inequality (OA-7). We have proven the claim. The contribution to κ 2 i,t+1 from signal errors η j,t is j |w ij,t+1 | 2 (w s j,t ) 2 σ 2 . Since w s j,t = 1w j,t (ν t ) converge uniformly to a constant σ -2 σ -2 +1 , we can bound this contribution below by an expression that is proportional to

$j |w ij,t+1 | 2 .$The summation has at most d non-zero terms. Applying the standard bound ∥v∥

$1 ≤ √ n∥v∥ 2 on L p norms on R n , j |w ij,t+1 | 2 ≥ 1 d j |w ij,t+1 | 2 .$The right-hand side of this inequality grows at a rate faster than

1 d by the claim j |w ij,t+1 | → ∞, and so the social signal error grows at a rate faster than 1 d . This gives a contradiction. Case 2: lim inf n j / ∈Gn |w ij,t+1 | > 0. As in Case 1, the contribution to signal errors from neighbors j / ∈ G n is proportional to j / ∈Gn |w ij,t+1 | 2 . By our choice of the agent i, she observes at most 2(n-|Gn|) n • d agents outside G n . The same standard bound on L p norms gives j / ∈Gn

$|w ij,t+1 | 2 ≥ 1 d • n 2(n -|G | n )   j / ∈Gn |w ij,t+1 |   2 .$By assumption, the cardinality n -|G n | of the complement of G n is o(n) and ( j / ∈Gn |w ij,t+1 |) 2 is non-vanishing. So the right-hand side grows at a rate faster than 1 d . Thus the social signal error grows at a rate faster than 1 d , which again gives a contradiction. This completes the proof in the case m = 1, and we next turn to the general argument. Now, suppose m ≥ 1 is arbitrary. As before, for each agent (j, t),we can write:

$a j,t = θ t - ∞ l=0 w j,t (ν t-l )(ρ l ν t-l ) + l,j ′ w j,t (η j ′ ,t-l )(ρ l η j ′ ,t-l ).$For each n, let G n be the set of i satisfying

$κ 2 i ≤ C d .$Suppose lim sup n |G n |/n = 1. Passing to a subsequence, we can assume that lim n |G n |/n = 1, i.e., the fraction of agents in G n converges to one. As in the m = 1 proof above, we can choose i ∈ G n who observes fewer than 2(n-|Gn|) n

• d agents outside G n . Choose any such i and consider the agent (i, t) with i ∈ G n , who observes neighbors' actions in periods t -1, . . . , tm. For each 1 ≤ l ≤ m, we will write w (i,t), (j,t-l)  for the weight that agent (i, t) places on the action of agent (j, tl). By the same argument as in Case 2 of the m = 1 proof above, lim inf n j / ∈Gn |w (i,t),(j,t-l) | = 0 for each l (since the fraction of agents outside G n is vanishing). Passing to a subsequence, we can assume that lim n j / ∈Gn |w (i,t),(j,t-l) | = 0. We can express agent (i, t)'s action: a i,t = 1≤l≤m   j∈Gn w (i,t),(j,t-l) ρ l a j,t-l + j / ∈Gn w (i,t),(j,t-l) ρ l a j,t-l   .

We will show that this expression places non-vanishing weight on the innovation ν t-l for some l ≥ 1. This will contradict our assumption that i ∈ G n .

Since lim n j / ∈Gn |w (i,t),(j,t-l) | = 0 and the weight each agent places on ν t-l is bounded, it is sufficient to show that 1≤l≤m j∈Gn w (i,t),(j,t-l) ρ l a j,t-l places non-vanishing weight on the innovation ν t-l for some l ≥ 1.

For each (j, t ′ ) such that j ∈ G n , we have

$a j,t ′ = θ t ′ -1 + σ -2 s j,t ′ 1 + σ -2 + ϵ j,t ′ ,$where Var(ϵ j,t ′ ) → 0. This is because a j,t ′ = (ρ 2 κ 2 i,t + 1) -1 r i,t + σ -2 s j,t ′ (ρ 2 κ 2 i,t + 1) -1 + σ -2 , and we have κ 2 i,t = Var(r i,tθ t-1 ) → 0. Using this expression for a j,t ′ , we obtain 1≤l≤m j∈Gn w (i,t),(j,t-l) ρ l a j,t-l = 1≤l≤m j∈Gn w (i,t),(j,t-l) ρ l θ t-l-1 + σ -2 s j,t-l 1 + σ -2 + ϵ j,t-l

By the same argument as in Case 1 of the m = 1 proof above, 1≤l≤m j∈Gn |w (i,t),(j,t-l) | must be bounded (or else the contributions of signal errors to κ 2 i would be too large to have i ∈ G n ). Therefore, it is sufficient to show that 1≤l≤m j∈Gn w (i,t),(j,t-l) ρ l • θ t-l-1 + σ -2 s j,t-l 1 + σ -2 places non-vanishing weight on the innovation ν t-l for some l ≥ 1.

This holds for the largest l such that j∈Gn w (i,t),(j,t-l) is non-vanishing. Such an l must exist, because 1≤l≤m j∈Gn w (i,t),(j,t-l) → 1 1 + σ -2 since i ∈ G n . OA4.6. Proof of Proposition 3. For each agent i, we can write a i,t = w s i s i,t + j W ij ρa j,t-1 = w s i s i,t + j W ij ρw s j s j,t + j ′ W jj ′ ρa j ′ ,t-2 .

Because we assume w s i < w < 1 and w s j < w < 1 for all j, the total weight j,j ′ W ij W jj ′ ρ on terms a j ′ ,t-2 is bounded away from zero. Because the error variance of each of these terms is greater than 1, this implies agent i fails to achieve the ε-aggregation benchmark for ε > 0 sufficiently small. OA4.7. Proof of Proposition 4. We prove the following statement, which includes the proposition as special cases.

Proposition 1. Suppose the network G is strongly connected. 10 Consider weights W and w s and suppose they are all positive, with an associated steady state V t . Suppose either (1) there is an agent i whose weights are a Bayesian best response to V t , and some agent observes that agent and at least one other neighbor; or

(2) there is an agent whose weights are a naive best response to V t , and who observes multiple neighbors. Then the steady state V t is Pareto-dominated by another steady state.

We provide the proof in the case m = 1 to simplify notation. The argument carries through with arbitrary finite memory.

Case (1): Consider an agent l who places positive weight on a rational agent k and positive weight on at least one other agent. Define weights W by W ij = W ij and w s i = w s i for all i ̸ = k, W kj = (1ϵ)W kj for all j ≤ n, and w s k = (1ϵ)w s k + ϵ, where W ij and w s i are the weights at the initial steady state. In words, agent k places weight (1ϵ) on her equilibrium strategy and extra weight ϵ on her private signal. All other players use the same weights as at the steady state.

Suppose we are at the initial steady state until time t, but in period t and all subsequent periods agents instead use weights W . These weights give an alternate updating function Φ on the space of covariance matrices. Because the weights W are positive and fixed, all coordinates of Φ are increasing, linear functions of all previous period variances and covariances.

Explicitly, the diagonal terms are [Φ(V t )] ii = (w s i ) 2 σ 2 i + j,j ′ ≤n

$W ij W ij ′ (ρ 2 V jj ′ ,t + 1)$and the off-diagonal terms are [Φ(V t )] ii ′ = j,j ′ ≤n W ij W i ′ j ′ (ρ 2 V jj,t ′ + 1).

So it is sufficient to show the variances Φ h (V t ) after applying Φ for h periods Pareto dominate the variances in V t for some h. In period t, the change in weights decreases the covariance V jk,t of k and some other agent j, who l also observes, by f (ϵ) of order Θ(ϵ). By the envelope the change in weights only increases the variance V kk by O(ϵ 2 ). Taking ϵ sufficiently small, we can ignore O(ϵ 2 ) terms.

There exists a constant δ > 0 such that all initial weights on observed neighbors are at least δ. Then each coordinate [Φ(V )] ii is linear with coefficient at least δ 2 on each variance or covariance of agents observed by i.

Because agent l observes k and another agent, agent l's variance will decrease below its equilibrium level by at least δ 2 f (ϵ) in period t + 1. Because Φ is increasing in all entries and we are only decreasing covariances, agent l's variance will also decrease below its initial level by at least δ 2 f (ϵ) in all periods t ′ > t + 1.

Because the network is strongly connected and finite, the network has a diameter. After d + 1 periods, the variances of all agents have decreased by at least δ 2d+2 f (ϵ) from their initial levels. This gives a Pareto improvement.

Case (2): Consider a naive agent k who observes at least two neighbors. We can write agent k's period t action as a k,t = w s k s i,t + j∈N i W kj ρa j,t-1 .

Define new weights W as in the proof of case (1). Because agent k is naive and the summation j∈N i W kj ρa j,t-1 has at least two terms, she believes the variance of this summation is smaller than its true value. So marginally increasing the weight on s k,t and decreasing the weight on this summation decreases her action variance. This deviation also decreases her covariance with any other agent. The remainder of the proof proceeds as in case (1). OA4.8. Proof of Proposition 5. Suppose the social influence

$SI(i) = j∈N ∞ k=1 ρ k W k ji w s i = 1 ′ I -ρ W -1 -1 ′ i w s i$does not converge for some i. Then in particular, there exists j such that ∞ k=0 ρ ℓ W k ji w s i does not converge. We can write

$a j,t = ∞ ℓ=0 j ′ ∈N ρ ℓ W ℓ jj ′ w s j ′ s j ′ ,t-ℓ .$This expression is the sum of ∞ ℓ=0 ρ W ℓ ji w s i η i,t-ℓ and independent terms corresponding to signal errors of agents other than i and changes in the state. Because ∞ ℓ=0 ρ ℓ W ℓ ji w s i does not converge, the payoff to action a j,t must therefore be -∞. But we showed in the proof of Proposition 1 that agent j's equilibrium payoff is at least -σ 2 j , which gives a contradiction. Given convergence, the expression for SI(i) follows from the Neumann series identity

$∞ k=0 M k = (I -M ) -1 .$OA4.9. Proof of Proposition 6. The social signal r i,t is the same for all agents, and we will refer to it as r t . We can express the social signal as

$r t = w A i:σ i =σ A a i,t-1 + w B i:σ i =σ B a i,t-1 (OA-8)$for some weights w A and w B . We can rewrite the actions a i,t-1 for i with signal variance σ 2 A as

$a i,t-1 = K K + σ -2 A ρr i,t-1 + σ A -2 K + σ -2 A s i,t-1 ,$where K = ρ 2 κ 2 t-1 + 1 is the equilibrium variance of ρr t-1 about the state θ t-1 . The analogous formula holds for agents i with signal variance σ 2 B . Substituting the formulas for a i,t-1 into equation (OA-8) and taking variances, whenever σ -2 A > σ -2 B and this fraction is positive. To see this, note that the difference between the numerator and denominator of the fraction is

$Knσ 2 B -K(n -2)σ 2 A -Knσ 2 A -K(n -2)σ 2 B = 2K(n -1)(σ 2 B -σ 2 A ) > 0 as desired.$OA5. Model with a starting time (online appendix)

In introducing the model (Section 2), we made the set of time indices T equal to Z, the set of all integers. Here we study the variant with an initial time period, t = 0: thus, we take T to be Z ≥0 , the non-negative integers. This section shows that there is a unique equilibrium outcome. In large networks, a suitable analogue of Theorem 1 holds, with both aggregation quality and outcomes similar to those obtained there. Similarly, the negative result of Proposition 2 also has a counterpart in this model. Let θ 0 be drawn according to the stationary distribution of the state process: θ 0 ∼ N 0, 1 1-ρ . After this, the state random variables θ t satisfy the AR(1) evolution

$θ t+1 = ρθ t + ν t+1 ,$where ρ is a constant with 0 < |ρ| < 1 and ν t+1 ∼ N (0, σ 2 ν ) are independent innovations. Actions, payoffs, signals, and observations are the same as in the main model, with the obvious modification that in the initial periods, t < m, information sets are smaller as there are not yet prior actions to observe. 11 To save on notation, we write actions as if agents had an improper prior, understanding that the adjustment for actions taken under the natural prior θ t ∼ N 0, 1 1-ρ is immediate. In this model, there is a straightforward prediction of behavior. A Nash equilibrium here refers to an equilibrium of the game involving all agents (i, t) for all time indices in T .

Fact 1. In the model with T = Z ≥0 , there is a unique Nash equilibrium, and it is in linear strategies. The initial generation (t = 0) plays a linear strategy based on private signals only. In any period t > 0, given linear strategies from prior periods, players' best responses are linear. For time periods t > m, we have

$V t = Φ(V t-1 ).$This fact follows from the observation that the initial (t = 0) generation faces a problem of forming a conditional expectation of a Gaussian state based on Gaussian signals, so their optimal strategies are linear. From then on, the analysis of Section 3.1 characterizes bestresponse behavior inductively. Note that for arbitrary environments, the fact does not imply that V t must converge.

Our main purpose in this section is to give analogues of the main results on learning in large networks. We use the same definition of an environment-in terms of the distribution of networks and signals-as in Section 4.1. For simplicity, we work with m = 1, though the arguments for our positive result extend straightforwardly.

The analogue of Theorem 1 is:

Theorem 1. Consider the T = Z ≥0 model. If an environment satisfies signal diversity, there is C > 0 such that asymptotically almost surely κ 2 i,t < C/n for all i at all times t ≥ 1 in the unique Nash equilibrium.

In particular, this implies that the covariance matrix in each period t ≥ 1 is very close (in the Euclidean norm) to the good-learning equilibrium from Theorem 1. We sketch the proof, which uses the material we developed in Appendix B. We define A t as in that proof (Section B.1). Take a β > 0, to be specified later, and consider

$W = W β n , 1 n ∪ Φ W β n , 1 n .$First, for large enough β, we have that A 1 ∈ W: In the unique Nash equilibrium, at t = 1, agents simply take weighted averages of their neighbors' signals, weighted by their precisions. So A 1 ∈ W by the central limit theorem for β sufficiently large. Second, we use the previously established fact (recall Section B.2.2) that Φ(W) ⊂ W to deduce that A t ∈ W at all future times. Finally, we observe that W ⊆ W β n , 1 n by construction. Without signal diversity, bad learning can occur forever, in the unique equilibrium. The analogue of Proposition 2 is immediate. In graphs with symmetric neighbors, Φ is a contraction when m = 1. So iteration of it arrives at the unique fixed point, and thus a learning outcome far from the benchmark.

## OA6. Naive Agents (online appendix)

In this section we provide rigorous detail for the analysis given in 5.1. We will describe outcomes with two signal types, σ 2 A and σ 2 B . 12 We use the same random network model as in Section 4.2 and assume each network type contains equal shares of agents with each signal type.

We can define variances

$V ∞ A = ρ 2 κ 2 t + 1 + σ -2 A 1 + σ -2 A 2 , V ∞ B = ρ 2 κ 2 t + 1 + σ -2 B 1 + σ -2 B 2 (OA-1)$as in the proof of Theorem 1 that for n sufficiently large, the event E occurs with probability at least 1ζ. We condition on E for the remainder of the proof. Let V ε be the ε-ball around in V sym the sup norm. We claim that for n sufficiently large, the updating map preserves this ball: Φ naive (V ε ) ⊂ V ε . We have Φ naive ( V sym ) = V sym up to terms of O(1/n). As we showed in the first paragraph of this proof, the partial derivatives of Φ naive are bounded above by a constant less than one. For n large enough, these facts imply Φ naive (V ε ) ⊂ V ε . We conclude there is an equilibrium in V ε by the Brouwer fixed point theorem.

Finally, we compare the equilibrium variances to the ε-aggregation benchmark and to V ∞ . It is easy to see these variances are worse than the ε-aggregation benchmark for n large for some ε > 0, and therefore by Theorem 1 also asymptotically worse than the Bayesian case when σ 2 A ̸ = σ 2 B . In the case σ 2 A = σ 2 B , it is sufficient to show that Bayesian agents place more weight on their private signals (since asymptotically action error comes from past changes in the state and not signal errors). Call the private signal variance σ 2 . For Bayesian agents, we showed in Theorem 1 that the weight on the private signal is equal to

$σ -2 σ -2 +(ρ 2 Cov ∞ +1) -1 where Cov ∞ solves Cov ∞ = (ρ 2 Cov ∞ +1) -1 [σ -2 + (ρ 2 Cov ∞ +1) -1 ] 2 .$For naive agents, the weight on the private signal is equal to σ -2 σ -2 +1 , which is smaller since Cov ∞ > 0.

OA7. Socially optimal learning outcomes with non-diverse signals (online appendix)

In this section, we show that a social planner can achieve vanishing aggregation errors even when signals are non-diverse. Thus, slower rate of learning at equilibrium with nondiverse signals is a consequence of individual incentives rather than a necessary feature of the environment.

Let G n be the complete network with n agents. Suppose that σ 2 i = σ 2 for all i and m = 1.

Proposition 3. Let ε > 0. Under the assumptions in this section, for n sufficiently large there exist weights weights W and w s such that at the corresponding steady state on G n , the ε-aggregation benchmark is achieved.

Proof. An agent with a social signal equal to θ t-1 would place weight σ -2 σ -2 +1 on her private signal and weight 1 σ -2 +1 on her social signal. Let w s A = σ -2 σ -2 +1 + δ and w s B = σ -2 σ -2 +1δ, where we will take δ > 0 to be small.

Assume that the first ⌊n/2⌋ agents place weight w s A on their private signals and weight 1w s A on a common social signal r t we will define, while the remaining agents place weight

![Figure 1.1. The network used in the "value of diversity" example]()

![Figure 5.1. Bayesian and naive learning on a complete graph and n = 600 agents divided]()

![Figure 6.1. Level curves for average social influence of agents in a configuration model with 5,000 networks with n = 40 agents in each and persistence ρ = 0.9. Degrees are chosen uniformly from {1, 2, . . . , 7} and private signal variances are chosen uniformly from {0.5, 1, . . . , 3.5}. The figure shows level curves for average social influence (drawn via cubic interpolation) on a log-log plot. If proportional changes in degree and signal variances mattered equally, these level curves would have slope 1.]()

![Figure OA1.1. Social signal variance in Indian villages. (a) The average social signal]()

![The table shows how far influence ratios are from a benchmark of being proportional to precision. We use a configuration model with a regular network and heterogeneous signal variances; there are n = 40 agents and the degree is d = 5. Agents are randomly assigned to signal variances σ]()

![The table shows how far influence ratios are from a benchmark of being proportional to degree. We use a configuration model with two possible degrees on n = 40 agents with homogeneous signal variance σ 2 = 2. Agents are randomly assigned to degrees d A or d B . Each entry is computed from 100 runs with persistence ρ = 0.9. Each table entry reports the ratio R d = SI(A)/SI(B)d A /d Bfor the degree parameters corresponding to that entry.model with n = 40 nodes, each randomly assigned degrees d A or d B (with equal probability of each) and with σ 2 = 2 for all agents. Table 2 reports the ratio]()

Cf.[Banerjee and Fudenberg (2004)](#b10) and[Wolitzky (2018)](#b48), with overlapping generations and a fixed state.

Sethi and Yildiz (2019)  argue that, even without explicit communication costs or constraints, familiarity and shared context determine the network in which people can effectively communicate.

See also[Bala and Goyal (1998)](#b8), a seminal model of boundedly rational learning rules in networks.

In analogous fixed-state environments where individuals have sufficiently many observations, if everyone uses certain simple and stationary DeGroot-style heuristics (requiring no sophistication about correlations between neighbors' behavior), they can learn the state quite precisely[(Golub and Jackson, 2010;](#b23)[Jadbabaie, Molavi, Sandroni, and Tahbaz-Salehi, 2012)](#b29). A changing state makes such imitative heuristics quite inefficient.

For all results, a node i's neighborhood can, but need not, include i itself.

It is worth noting that even when the memory m is small, observed actions can indirectly incorporate signals from much further in the past.

We rewrite ρ a j,t-θ t = ρ(ρ -1 a j,t-θ t-1 )ν t , where Var[ν t ] = 1. The covariances of the term in parentheses are entries of V t-1 . For the block structure, note the private signal errors η i,t are independent of events before t.

The assumptions of finitely many signal types and network types are for technical convenience only, and could be relaxed.

The reason is that the distribution of aggregation errors is upper hemicontinuous in model parameters, so if the desired bounds hold for each point in a compact set, they can be made uniform.

Note this is a special case of the stochastic block model.

In particular, agent (i, t) sees everyone's past action, including the one taken last period at the same node.

The notation means the errors are bounded by Cn -1/2 for a C > 0 with high probability(Janson,  

2011).

For simplicity we first present the argument in a random graph family where the number of two-step paths is nearly deterministic. The argument extends to a larger class of models where the same property applies to longer paths, as we discuss in the next subsection.

These are both special cases of our stochastic block model from Section 4.2, so Theorem 1 applies to these network structures when signal diversity is satisfied.

We thank Alireza Tahbaz-Salehi for suggesting this analysis.

The proof of the proposition establishes uniqueness by showing that Φ is a contraction in a suitable sense.

For any v, there are ρ < 1 and σ -2 > 0 such that if ρ > ρ and σ -2 < σ -2 , then Var(a i,tθ t ) ≥ v for all n.

For example, by making half the agents' signals strictly worse.

Note the agent can use the estimate of last period's state, which has an error of order C/n. If the agent simply set her action equal to this estimate, then she would achieve Var(θ tθ t-1 ) = 1, since the state innovation has variance 1. Additionally using her private signal does strictly better than this.

Indeed, it can be deduced (as in the proof of Corollary 2) that agents' actions are equal to an appropriately discounted sum of past θ t-, up to error terms (arising from η i,t-) that vanish asymptotically. The weights on past states are the same as in the complete-network case, which is why the characterization of (4.3) applies.

There are a number of possible variants of our behavioral assumption, and it is straightforward to numerically study alternative specifications of behavior in our model(Alatas et al., 2016 consider one such  variant).

Note that in a stationary equilibrium, this depends on the node and not the time, so we speak interchangeably of the influence of a node and that of an agent at this node.

Since W can contain both positive and negative numbers, some of them potentially large, it is not immediately obvious that the summation converges.

This model works by creating n nodes, each with d "stubs" sticking out of it, and then performing a random matching of the stubs to create a graph. See[Jackson (2010)](#b28), Section 4.5.10, for details.

"If . . . the economic problem of society is mainly one of rapid adaptation to changes in the particular circumstances of time and place . . . there still remains the problem of communicating to [each individual] such further information as he needs." Hayek's main concern was aggregation of information through markets, but the same questions apply more generally.

See[Angeletos and La'O (2010)](#b4) for a survey of related models that are used to study real business cycles. More recent developments include[Angeletos and Lian (2018)](#b5) and[Molavi (2019)](#b34), with the latter allowing a form of misspecification.

For more complete surveys of different parts of this literature, see, among others,Acemoglu and Ozdaglar  (2011),[Golub and Sadler (2016)](#b24), and[Mossel and Tamuz (2017)](#b38). See[Moscarini, Ottaviani, and Smith (1998)](#b36) for an early model in a binary-action environment, where it is shown that a changing state can break information cascades.

Indeed, agents behaving according to the DeGroot heuristic in other environments might have to do with their experiences in stationary environments where it is closer to optimal.

In particular, we have no data on signal qualities; when we introduce signal heterogeneity, we simply posit that households without electricity have worse access to external information.

We choose the larger signal variance so that the average precision in each village is 1 2 , which holds the total inflow of information constant in a sense made precise in the appendix.

The variance-covariance matrices are well-defined because the (W, w s ) weights yield unambiguous strategy profiles in the sense of Appendix OA3.

This is because it is a linear combination, with coefficients summing to 1, of unbiased estimates of θ t-1 .

Throughout this proof, we abuse terminology by referring to agents and nodes interchangeably when the relevant t is clear or specified nearby.

Recall we wrote A(x) = A(0) + xM , and in (B.1) we expressed the derivative of f in x in terms of the matrix we exhibit here.

We take this term to refer to variances, as well.

We can instead assume that the analyst observes (a proxy for) the private signal s i,t of agent i; we mention how below.

This system defines a VAR(1) process (or generally VAR(m) for memory length m).

Methods involving regularization may be practically useful in identifying links in the network.[Manresa (2013)](#b51) proposes a regularization (LASSO) technique for identifying such links (peer effects). In a dynamic setting such as ours, with serial correlation, the techniques required will generally be more complicated.

I.e., σ ′ = T p σ is defined by σ (i,t) = σ (i,t-p) .

Note E P [θ tz k ] = E[E[θ tz k | z k ]] = E[E[z kz k | z k ]] = 0,where we have used (OA-1.

This is because ρθ t-2 is a sufficient statistic and an unbiased estimator for θ t-1 given signals up to t -2, and E[(ρθ t-2θ t-1 ) 2 ] = 1.

To simplify calculations, we write this expression with a negative coefficient on the first sum so that the terms w j,t (ν t-l ) are positive. The weight that j places on ν t-l is in fact -w j,t (ν t-l ).

