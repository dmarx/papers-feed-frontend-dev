<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Introduction to Johnson-Lindenstrauss Transforms</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2021-03-02">2nd March 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Casper</forename><forename type="middle">Benjamin</forename><surname>Freksen</surname></persName>
							<email>casper@freksen.dk</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Aarhus University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">An Introduction to Johnson-Lindenstrauss Transforms</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-03-02">2nd March 2021</date>
						</imprint>
					</monogr>
					<idno type="MD5">1769C9D187223BD99F77EADFCF38CEF4</idno>
					<idno type="arXiv">arXiv:2103.00564v1[cs.DS]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-24T06:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Johnson-Lindenstrauss Transforms are powerful tools for reducing the dimensionality of data while preserving key characteristics of that data, and they have found use in many fields from machine learning to differential privacy and more. This note explains what they are; it gives an overview of their use and their development since they were introduced in the 1980s; and it provides many references should the reader wish to explore these topics more deeply.</p><p>The text was previously a main part of the introduction of my PhD thesis [Fre20], but it has been adapted to be self contained and serve as a (hopefully good) starting point for readers interested in the topic.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1 The Why, What, and How</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">The Problem</head><p>Consider the following scenario: We have some data that we wish to process but the data is too large, e.g. processing the data takes too much time, or storing the data takes too much space. A solution would be to compress the data such that the valuable parts of the data are kept and the other parts discarded. Of course, what is considered valuable is defined by the data processing we wish to apply to our data. To make our scenario more concrete let us say that our data consists of vectors in a high dimensional Euclidean space, R 𝑑 , and we wish to find a transform to embed these vectors into a lower dimensional space, R 𝑚 , where 𝑚 ≪ 𝑑, so that we can apply our data processing in this lower dimensional space and still get meaningful results. The problem in this more concrete scenario is known as dimensionality reduction.</p><p>As an example, let us pretend to be a library with a large corpus of texts and whenever a person returns a novel that they liked, we would like to recommend some similar novels for them to read next. Or perhaps we wish to be able to automatically categorise the texts into groups such as fiction/non-fiction or child/young-adult/adult literature. To be able to use the wealth of research on similarity search and classification we need a suitable representation of our texts, and here a common choice is called bag-of-words. For a language or vocabulary with 𝑑 different words, the bag-of-words representation of a text 𝑡 is a vector 𝑥 ∈ R 𝑑 whose 𝑖th entry is the number of times the 𝑖th word occurs in 𝑡. For example, if the language is just ["be", "is", "not", "or", "question", "that", "the", "to"] then the text "to be or not to be" is represented as (2, 0, 1, 1, 0, 0, 0, 2) ᵀ . To capture some of the context of the words, we can instead represent a text as the count of so-called 𝑛-grams1, which are sequences of 𝑛 consecutive words, e.g. the 2-grams of "to be or not to be" are ["to be", "be or", "or not", "not to", "to be"], and we represent such a bag-of-𝑛-grams as a vector in R (𝑑 𝑛 ) . To compare two texts we compute the distance between the vectors of those texts, because the distance between vectors of texts with mostly the same words (or 𝑛-grams) is small2. For a more realistic language such as English with 𝑑 ≈ 171000 words <ref type="bibr">[SW89]</ref> or that of the "English" speaking internet at 𝑑 ≳ 4790000 words <ref type="bibr" target="#b211">[WZ05]</ref>, the dimension quickly becomes infeasable. While we only need to store the nonzero counts of words (or 𝑛-grams) to represent a vector, many data processing algorithms have a dependency on the vector dimension 𝑑 (or 𝑑 𝑛 ), e.g. using nearest-neighbour search to find similar novels to recommend <ref type="bibr" target="#b5">[AI17]</ref> or using neural networks to classify our texts <ref type="bibr" target="#b169">[Sch18]</ref>. These algorithms would be infeasible for our library use case if we do not first reduce the dimension of our data.</p><p>A seemingly simple approach would be to select a subset of the coordinates, say if the data contained redundant or irrelevant coordinates. This is known as feature selection [JS08; HTF17; Jam+13a], and can be seen as projecting3 onto an axis aligned subspace, i.e. a subspace whose basis is a subset of {𝑒 1 , . . . , 𝑒 𝑑 }.</p><p>We can build upon feature selection by choosing the basis from a richer set of vectors. For instance, in principal component analysis as dimensionality reduction (PCA) <ref type="bibr" target="#b156">[Pea01;</ref><ref type="bibr" target="#b90">Hot33]</ref> we let the basis of the subspace be the 𝑚 first eigenvectors (ordered decreasingly by eigenvalue) of 𝑋 ᵀ 𝑋, where the rows of 𝑋 ∈ R 𝑛×𝑑 are our 𝑛 high dimensional vectors4. This subspace maximises the variance of the data in the sense that the first eigenvector is the axis that maximises variance and subsequent eigenvectors are the axes that maximise variance subject to being orthogonal to all previous eigenvectors <ref type="bibr" target="#b137">[LRU20b;</ref><ref type="bibr" target="#b93">HTF17;</ref><ref type="bibr" target="#b101">Jam+13b]</ref>.</p><p>But what happens if we choose a basis randomly?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">The Johnson-Lindenstrauss Lemma(s)</head><p>In 19845 it was discovered that projecting onto a random basis approximately preserves pairwise distances with high probability. In order to prove a theorem regarding Lipschitz extensions of 1 These are sometimes referred to as shingles. 2 We might wish to apply some normalisation to the vectors, e.g. tf-idf <ref type="bibr" target="#b136">[LRU20a]</ref>, so that rare words are weighted more and text length is less significant. 3 Here we slightly bend the definition of projection in the sense that we represent a projected vector as the coefficients of the of the linear combination of the (chosen) basis of the subspace we project onto, rather than as the result of that linear combination. If 𝐴 ∈ R 𝑚×𝑑 is a matrix with the subspace basis vectors as rows, we represent the projection of a vector 𝑥 ∈ R 𝑑 as the result of 𝐴𝑥 ∈ R 𝑚 rather than 𝐴 ᵀ 𝐴𝑥 ∈ R 𝑑 .</p><p>time8 of 𝒪(|𝑋 |𝑑 2 + 𝑑 𝜔 ) [DDH07], compared to Θ(|𝑋 |𝑑 log 𝑑) and9 Θ(∥𝑋 ∥ 0 𝜀 -1 log|𝑋 |) that can be achieved by the JLDs "FJLT" and "Block SparseJL", respectively, which will be introduced in section 1.4. As such, PCA and JLDs are different tools appropriate for different scenarios (see e.g. <ref type="bibr" target="#b37">[Bre+19]</ref> where the two techniques are compared empirically in the domain of medicinal imaging; see also [Das00; BM01; FB03; FM03; Tan+05; DB06; Arp+14; Woj+16; Bre+20]). That is not to say the two are mutually exclusive, as one could apply JL to quickly shave off some dimensions followed by PCA to more carefully reduce the remaining dimensions [e.g. RST09; HMT11; Xie+16; Yan+20]. For more on PCA, we refer the interested reader to <ref type="bibr" target="#b104">[Jol02]</ref>, which provides an excellent in depth treatment of the topic.</p><p>One natural question to ask with respect to JLDs and JLTs is if the target dimension is optimal. This is indeed the case as Kane, Meka, and Nelson <ref type="bibr" target="#b115">[KMN11]</ref> and Jayram and Woodruff <ref type="bibr" target="#b107">[JW13]</ref> independently give a matching lower bound of 𝑚 = Ω(𝜀 -2 log 1 𝛿 ) for any JLD that satisfies lemma 1.2, and Larsen and Nelson <ref type="bibr" target="#b134">[LN17]</ref> showed that the bound in lemma 1.1 is optimal up constant factors for almost the entire range of 𝜀 with the following theorem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 1.3 ([LN17]</head><p>). For any integers 𝑛, 𝑑 ≥ 2 and lg 0.5001 𝑛/ √︁ min{𝑑, 𝑛} &lt; 𝜀 &lt; 1 there exists a set of points 𝑋 ⊂ R 𝑑 of size 𝑛 such that any function 𝑓 : 𝑋 → R 𝑚 satisfying eq. (1) must have</p><formula xml:id="formula_0">𝑚 = Ω (︁ 𝜀 -2 log(𝜀 2 𝑛) )︁ .<label>(3)</label></formula><p>Note that if 𝜀 ≤ √︁ lg 𝑛/min{𝑑, 𝑛} then 𝜀 -2 lg 𝑛 ≥ min{𝑑, 𝑛}, and embedding 𝑋 into dimension min{𝑑, |𝑋 |} can be done isometrically by the identity function or by projecting onto span(𝑋), respectively.</p><p>Alon and Klartag <ref type="bibr" target="#b6">[AK17]</ref> extended the result in <ref type="bibr" target="#b134">[LN17]</ref> by providing a lower bound for the gap in the range of 𝜀.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 1.4 ([AK17]</head><p>). There exists an absolute positive constant 0 &lt; 𝑐 &lt; 1 so that for any 𝑛 ≥ 𝑑 &gt; 𝑐𝑑 ≥ 𝑚 and for all 𝜀 ≥ 2/ √ 𝑛, there exists a set of points 𝑋 ⊂ R 𝑑 of size 𝑛 such that any function 𝑓 : 𝑋 → R 𝑚 satisfying eq. (1) must have</p><formula xml:id="formula_1">𝑚 = Ω (︁ 𝜀 -2 log(2 + 𝜀 2 𝑛) )︁ .<label>(4)</label></formula><p>It is, however, possible to circumvent these lower bounds by restricting the set of input vectors we apply the JLTs to. For instance, Klartag and Mendelson <ref type="bibr" target="#b114">[KM05]</ref>, Dirksen <ref type="bibr" target="#b62">[Dir16]</ref>, and Bourgain, Dirksen, and Nelson <ref type="bibr" target="#b28">[BDN15b]</ref> provide target dimension upper bounds for JLTs that are dependent on statistical properties of the input set 𝑋. Similarly, JLTs can be used to approximately preserve pairwise distances simultaneously for an entire subspace using 𝑚 = Θ(𝜀 -2 𝑡 log(𝑡/𝜀)), where 𝑡 denotes the dimension of the subspace <ref type="bibr" target="#b168">[Sar06]</ref>, which is a great improvement when 𝑡 ≪ |𝑋 |, 𝑑.</p><p>Another useful property of JLTs is that they approximately preserve dot products. Corollary 1.5 formalises this property in terms of lemma 1.1, though it is sometimes [Sar06; AV06] stated in terms of lemma 1.2. Corollary 1.5 has a few extra requirements on 𝑓 and 𝑋 compared 8 Here 𝜔 ≲ 2.373 is the exponent from the running time of squared matrix multiplication [Wil12; Le 14]. 9 Here ∥𝑋 ∥ 0 is the total number of nonzero entries in the set of vectors 𝑋, i.e. ∥𝑋 ∥ 0 ≔ ∑︁ 𝑥∈𝑋 ∥𝑥 ∥ 0 where ∥𝑥 ∥ 0 ≔ |{𝑖 | 𝑥 𝑖 ≠ 0}| for a vector 𝑥.</p><p>to lemma 1.1, but these are not an issue if the JLT is sampled from a JLD, or if we add the negations of all our vectors to 𝑋, which only slightly increases the target dimension.</p><p>Corollary 1.5. Let 𝑑, 𝜀, 𝑋 and 𝑓 be as defined in lemma 1.1, and furthermore let 𝑓 be linear. Then for every 𝑥, 𝑦 ∈ 𝑋, if -𝑦 ∈ 𝑋 then</p><formula xml:id="formula_2">|⟨ 𝑓 (𝑥), 𝑓 (𝑦)⟩ -⟨𝑥, 𝑦⟩| ≤ 𝜀∥𝑥 ∥ 2 ∥𝑦 ∥ 2 .</formula><p>(5)</p><p>Proof. If at least one of 𝑥 and 𝑦 is the 0-vector, then eq. ( <ref type="formula">5</ref>) is trivially satisfied as 𝑓 is linear. If 𝑥 and 𝑦 are both unit vectors then we assume w.l.o.g. that ∥𝑥 + 𝑦∥ 2 ≥ ∥𝑥 -𝑦 ∥ 2 and we proceed as follows, utilising the polarisation identity: 4⟨𝑢,</p><formula xml:id="formula_3">𝑣⟩ = ∥𝑢 + 𝑣∥ 2 2 -∥𝑢 -𝑣 ∥ 2 2 . 4 |︁ |︁ ⟨ 𝑓 (𝑥), 𝑓 (𝑦)⟩ -⟨𝑥, 𝑦⟩ |︁ |︁ = |︁ |︁ ∥ 𝑓 (𝑥) + 𝑓 (𝑦)∥ 2 2 -∥ 𝑓 (𝑥) -𝑓 (𝑦)∥ 2 2 -4⟨𝑥, 𝑦⟩ |︁ |︁ ≤ |︁ |︁ (1 + 𝜀)∥𝑥 + 𝑦∥ 2 2 -(1 -𝜀)∥𝑥 -𝑦∥ 2 2 -4⟨𝑥, 𝑦⟩ |︁ |︁ = |︁ |︁ 4⟨𝑥, 𝑦⟩ + 𝜀(∥𝑥 + 𝑦 ∥ 2 2 + ∥𝑥 -𝑦∥ 2 2 ) -4⟨𝑥, 𝑦⟩ |︁ |︁ = 𝜀(2∥𝑥 ∥ 2 2 + 2∥𝑦 ∥ 2 2 ) = 4𝜀.</formula><p>Otherwise we can reduce to the unit vector case.</p><formula xml:id="formula_4">|⟨ 𝑓 (𝑥), 𝑓 (𝑦)⟩ -⟨𝑥, 𝑦⟩| = |︁ |︁ |︁ |︁ ⟨︂ 𝑓 (︁ 𝑥 ∥𝑥 ∥ 2 )︁ , 𝑓 (︁ 𝑦 ∥ 𝑦∥ 2 )︁ ⟩︂ - ⟨︂ 𝑥 ∥𝑥 ∥ 2 , 𝑦 ∥ 𝑦∥ 2 ⟩︂ |︁ |︁ |︁ |︁ ∥𝑥 ∥ 2 ∥ 𝑦∥ 2 ≤ 𝜀∥𝑥 ∥ 2 ∥ 𝑦∥ 2 .</formula><p>□ F 8 f Before giving an overview of the development of JLDs in section 1.4, let us return to our scenario and example in section 1.1 and show the wide variety of fields where dimensionality reduction via JLTs have found use. Furthermore, to make us more familiar with lemma 1.1 and its related concepts, we will pick a few examples of how the lemma is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">The Use(fulness) of Johnson-Lindenstrauss</head><p>JLDs and JLTs have found uses and parallels in many fields and tasks, some of which we will list below. Note that there are some overlap between the following categories, as e.g. <ref type="bibr" target="#b71">[FB03]</ref> uses a JLD for an ensemble of weak learners to learn a mixture of Gaussians clustering, and <ref type="bibr" target="#b159">[PW15]</ref> solves convex optimisation problems in a way that gives differential privacy guarantees.</p><p>Nearest-neighbour search have benefited from the Johnson-Lindenstrauss lemmas on multiple occasions, including [Kle97; KOR00], which used JL to randomly partition space rather than reduce the dimension, while others [AC09; HIM12] used the dimensionality reduction properties of JL more directly. Variations on these results include consructing locality sensitive hashing schemes <ref type="bibr" target="#b57">[Dat+04]</ref> and finding nearest neighbours without false negatives <ref type="bibr" target="#b182">[SW17]</ref>.</p><p>Clustering with results in various sub-areas such as mixture of Gaussians [Das99; FB03; UDS07], subspace clustering [HTB17], graph clustering [SI09; Guo+20], self-organising maps [RK89; Kas98], and 𝑘-means [Bec+19; Coh+15; Bou+14; Liu+17; SF18], which will be explained in more detail in section 1.3.1. Outlier detection where there have been works for various settings of outliers, including approximate nearest-neighbours [dVCH10; SZK15] and Gaussian vectors [NC20], while [Zha+20] uses JL as a preprocessor for a range of outlier detection algorithms in a distributed computational model, and [AP12] evaluates the use of JLTs for outlier detection of text documents. Ensemble learning where independent JLTs can be used to generate training sets for weak learners for bagging [SR09] and with the voting among the learners weighted by how well a given JLT projects the data [CS17; Can20]. The combination of JLTs with multiple learners have also found use in the regime of learning high-dimensional distributions from few datapoints (i.e. |𝑋 | ≪ 𝑑) [DK13; ZK19; Niy+20].</p><p>Adversarial machine learning where Johnson-Lindenstrauss can both be used to defend against adversarial input [Ngu+16; Wee+19; Tar+19] as well as help craft such attacks <ref type="bibr" target="#b128">[Li+20]</ref>.</p><p>Miscellaneous machine learning where, in addition to the more specific machine learning topics mentioned above, Johnson-Lindenstrauss has been used together with support vector machines [CJS09; Pau+14; LL20], Fisher's linear discriminant <ref type="bibr" target="#b64">[DK10]</ref>, and neural networks <ref type="bibr" target="#b169">[Sch18]</ref>, while <ref type="bibr" target="#b123">[KY20]</ref> uses JL to facilitate stochastic gradient descent in a distributed setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Numerical linear algebra</head><p>with work focusing on low rank approximation [Coh+15; MM20], canonical correlation analysis [Avr+14], and regression in a local [THM17; MM09; Kab14; Sla17] and a distributed [HMM16] computational model. Futhermore, as many of these subfields are related some papers tackle multiple numerical linear algebra problems at once, e.g. low rank approximation, regression, and approximate matrix multiplication [Sar06], and a line of work [MM13; CW17; NN13a] have used JLDs to perform subspace embeddings which in turn gives algorithms for ℓ 𝑝 regression, low rank approximation, and leverage scores. For further reading, there are surveys [Mah11; HMT11; Woo14] covering much of JLDs' use in numerical linear algebra. Convex optimisation in which Johnson-Lindenstrauss has been used for (integer) linear programming [VPL15] and to improve a cutting plane method for finding a point in a convex set using a separation oracle [TSC15; Jia+20]. Additionally, [Zha+13] studies how to recover a high-dimensional optimisation solution from a JL dimensionality reduced one. Differential privacy have utilised Johnson-Lindenstrauss to provide sanitised solutions to the linear algebra problems of variance estimation [Blo+12], regression [She19; SKD19; ZLW09], Euclidean distance estimation [Ken+13; LKR06; GLK13; Tur+08; Xu+17], and low-rank factorisation [Upa18], as well as convex optimisation [PW15; KJ16], collaborative filtering [Yan+17] and solutions to graph-cut queries [Blo+12; Upa13]. Furthermore, [Upa15] analysis various JLDs with respect to differential privacy and introduces a novel one designed for this purpose.</p><p>Neuroscience where it is used as a tool to process data in computational neuroscience [GS12; ALG13], but also as a way of modelling neurological processes [GS12; ALG13; All+14; PP14]. Interestingly, there is some evidence [MFL08; SA09; Car+13] to suggest that JL-like operations occur in nature, as a large set of olifactory sensory inputs (projection neurons) map onto a smaller set of neurons (Kenyon cells) in the brains of fruit flies, where each Kenyon cell is connected to a small and seemingly random subset of the projection neurons. This is reminiscent of sparse JL constructions, which will be introduced in section 1.4.1, though I am not neuroscientifically adept enough to judge how far these similarities between biological constructs and randomised linear algebra extend.</p><p>Other topics where Johnson-Lindenstrauss have found use include graph sparsification <ref type="bibr" target="#b181">[SS11]</ref>, graph embeddings in Euclidean spaces <ref type="bibr" target="#b76">[FM88]</ref>, integrated circuit design <ref type="bibr" target="#b200">[Vem98]</ref>, biometric authentication <ref type="bibr" target="#b19">[Arp+14]</ref>, and approximating minimum spanning trees <ref type="bibr" target="#b86">[HI00]</ref>.</p><p>For further examples of Johnson-Lindenstrauss use cases, please see <ref type="bibr" target="#b97">[Ind01;</ref><ref type="bibr" target="#b199">Vem04]</ref>. Now, let us dive deeper into the areas of clustering and streaming algorithms to see how Johnson-Lindenstrauss can be used there.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.1">Clustering</head><p>Clustering can be defined as partitioning a dataset such that elements are similar to elements in the same partition while being dissimilar to elements in other partitions. A classic clustering problem is the so-called 𝑘-means clustering where the dataset 𝑋 ⊂ R 𝑑 consists of points in Euclidean space. The task is to choose 𝑘 cluster centers 𝑐 1 , . . . , 𝑐 𝑘 such that they minimise the sum of squared distances from datapoints to their nearest cluster center, i.e. arg min</p><formula xml:id="formula_5">𝑐 1 ,...,𝑐 𝑘 ∑︂ 𝑥∈𝑋 min 𝑖 ∥𝑥 -𝑐 𝑖 ∥ 2 2 .<label>(6)</label></formula><p>This creates a Voronoi partition, as each datapoint is assigned to the partition corresponding to its nearest cluster center. We let 𝑋 𝑖 ⊆ 𝑋 denote the set of points that have 𝑐 𝑖 as their closest center. It is well known that for an optimal choice of centers, the centers are the means of their corresponding partitions, and furthermore, the cost of any choice of centers is never lower than the sum of squared distances from datapoints to the mean of their assigned partition, i.e.</p><formula xml:id="formula_6">∑︂ 𝑥∈𝑋 min 𝑖 ∥𝑥 -𝑐 𝑖 ∥ 2 2 ≥ 𝑘 ∑︂ 𝑖=1 ∑︂ 𝑥∈𝑋 𝑖 ∥︁ ∥︁ ∥︁𝑥 - 1 |𝑋 𝑖 | ∑︂ 𝑦∈𝑋 𝑖 𝑦 ∥︁ ∥︁ ∥︁ 2 2 . (<label>7</label></formula><formula xml:id="formula_7">)</formula><p>It has been shown that finding the optimal centers, even for 𝑘 = 2, is NP-hard [Alo+09; Das08]; however, various heuristic approaches have found success such as the commonly used Lloyd's algorithm <ref type="bibr" target="#b132">[Llo82]</ref>. In Lloyd's algorithm, after initialising the centers in some way we iteratively improve the choice of centers by assigning each datapoint to its nearest center and then updating the center to be the mean of the datapoints assigned to it. These two steps can then be repeated until some termination criterion is met, e.g. when the centers have converged. If we let 𝑡 denote the number of iterations, then the running time becomes 𝒪(𝑡 |𝑋 |𝑘𝑑), as we use 𝒪(|𝑋 |𝑘𝑑) time per iteration to assign each data point to its nearest center. We can improve this running time by quickly embedding the datapoints into a lower dimensional space using a JLT and then running Lloyd's algorithm in this smaller space. The Fast Johnson-Lindenstrauss Transform, which we will introduce later, can for many sets of parameters embed a vector in 𝒪(𝑑 log 𝑑) time reducing the total running time to 𝒪(|𝑋 |𝑑 log 𝑑 + 𝑡|𝑋 |𝑘𝜀 -2 log |𝑋 |). However, for this to be useful we need the partitioning of points in the lower dimensional space to correspond to an (almost) equally good partition in the original higher dimensional space.</p><p>In order to prove such a result we will use the following lemma, which shows that the cost of a partitioning, with its centers chosen as the means of the partitions, can be written in terms of pairwise distances between datapoints in the partitions.</p><formula xml:id="formula_8">Lemma 1.6. Let 𝑘, 𝑑 ∈ N 1 and 𝑋 𝑖 ⊂ R 𝑑 for10 𝑖 ∈ [𝑘]. 𝑘 ∑︂ 𝑖=1 ∑︂ 𝑥∈𝑋 𝑖 ∥︁ ∥︁ ∥︁𝑥 - 1 |𝑋 𝑖 | ∑︂ 𝑦∈𝑋 𝑖 𝑦 ∥︁ ∥︁ ∥︁ 2 2 = 1 2 𝑘 ∑︂ 𝑖=1 1 |𝑋 𝑖 | ∑︂ 𝑥,𝑦∈𝑋 𝑖 ∥𝑥 -𝑦 ∥ 2 2 .<label>(8)</label></formula><p>The proof of lemma 1.6 consists of various linear algebra manipulations and can be found in section 2.1. Now we are ready to prove the following proposition, which states that if we find a partitioning whose cost is within (1 + 𝛾) of the optimal cost in low dimensional space, that partitioning when moving back to the high dimensional space is within (1 + 4𝜀)(1 + 𝛾) of the optimal cost there.</p><formula xml:id="formula_9">Proposition 1.7. Let 𝑘, 𝑑 ∈ N 1 , 𝑋 ⊂ R 𝑑 , 𝜀 ≤ 1/2, 𝑚 = Θ(𝜀 -2 log |𝑋 |)</formula><p>, and 𝑓 : 𝑋 → R 𝑚 be a JLT. Let 𝑌 ⊂ R 𝑚 be the embedding of 𝑋. Let 𝜅 * 𝑚 denote the optimal cost of a partitioning of 𝑌, with respect to eq. (6). Let 𝑌 1 , . . . , 𝑌 𝑘 ⊆ 𝑌 be a partitioning of 𝑌 with cost 𝜅 𝑚 such that 𝜅 𝑚 ≤ (1 + 𝛾)𝜅 * 𝑚 for some 𝛾 ∈ R.</p><p>Let 𝜅 * 𝑑 be the cost of an optimal partitioning of 𝑋 and 𝜅 𝑑 be the cost of the partitioning 𝑋 1 , . . . , 𝑋 𝑘 ⊆ 𝑋,</p><formula xml:id="formula_10">satisfying 𝑌 𝑖 = { 𝑓 (𝑥) | 𝑥 ∈ 𝑋 𝑖 }. Then 𝜅 𝑑 ≤ (1 + 4𝜀)(1 + 𝛾)𝜅 * 𝑑 .<label>(9)</label></formula><p>Proof. Due to lemma 1.6 and the fact that 𝑓 is a JLT we know that the cost of our partitioning is approximately preserved when going back to the high dimensional space, i.e. 𝜅 𝑑 ≤ 𝜅 𝑚 /(1 -𝜀). Furthermore, since the cost of 𝑋's optimal partitioning when embedded down to 𝑌 cannot be lower than the optimal cost of partitioning 𝑌, we can conclude</p><formula xml:id="formula_11">𝜅 * 𝑚 ≤ (1 + 𝜀)𝜅 * 𝑑 . Since 𝜀 ≤ 1/2, we have 1/(1 -𝜀) = 1 + 𝜀/(1 -𝜀) ≤ 1 + 2𝜀 and also (1 + 𝜀)(1 + 2𝜀) = (1 + 3𝜀 + 2𝜀 2 ) ≤ 1 + 4𝜀.</formula><p>Combining these inequalities we get</p><formula xml:id="formula_12">𝜅 𝑑 ≤ 1 1 -𝜀 𝜅 𝑚 ≤ (1 + 2𝜀)(1 + 𝛾)𝜅 * 𝑚 ≤ (1 + 2𝜀)(1 + 𝛾)(1 + 𝜀)𝜅 * 𝑑 ≤ (1 + 4𝜀)(1 + 𝛾)𝜅 * 𝑑 .</formula><p>□ By pushing the constant inside the Θ-notation, proposition 1.7 shows that we can achieve a (1 + 𝜀) approximation11 of 𝑘-means with 𝑚 = Θ(𝜀 -2 log|𝑋 |). However, by more carefully analysing which properties are needed, we can improve upon this for the case where 𝑘 ≪ |𝑋 |. Boutsidis et al. <ref type="bibr" target="#b36">[Bou+14]</ref> showed that projecting down to a target dimension of 𝑚 = Θ(𝜀 -2 𝑘) suffices for a slightly worse 𝑘-means approximation factor of (2 + 𝜀). This result was expanded upon in two ways by Cohen et al. <ref type="bibr" target="#b47">[Coh+15]</ref>, who showed that projecting down to 𝑚 = Θ(𝜀 -2 𝑘) achieves a (1 + 𝜀) approximation ratio, while projecting all the way down to 𝑚 = Θ(𝜀 -2 log 𝑘) still suffices for a (9+𝜀) approximation ratio. The (1+𝜀) case has recently been further improved upon by both Becchetti et al. <ref type="bibr" target="#b29">[Bec+19]</ref>, who have shown that one can achieve the (1 + 𝜀) approximation ratio for 𝑘-means when projecting down to 𝑚 = Θ</p><formula xml:id="formula_13">(︁ 𝜀 -6 (log 𝑘 + log log |𝑋 |) log 𝜀 -1 )︁</formula><p>, and by Makarychev, Makarychev, and Razenshteyn <ref type="bibr" target="#b144">[MMR19]</ref>, who independently have proven an even better bound of 𝑚 = Θ (︁</p><formula xml:id="formula_14">𝜀 -2 log 𝑘/𝜀 )︁</formula><p>, essentially giving a "best of worlds" result with respect to <ref type="bibr" target="#b47">[Coh+15]</ref>.</p><p>For an overview of the history of 𝑘-means clustering, we refer the interested reader to <ref type="bibr" target="#b34">[Boc08]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.2">Streaming</head><p>The field of streaming algorithms is characterised by problems where we receive a sequence (or stream) of items and are queried on the items received so far. The main constraint is usually that we only have limited access to the sequence, e.g. that we are only allowed one pass over it, and that we have very limited space, e.g. polylogarithmic in the length of the stream. To make up for these constraints we are allowed to give approximate answers to the queries. The subclass of streaming problems we will look at here are those where we are only allowed a single pass over the sequence and the items are updates to a vector and a query is some statistic on that vector, e.g. the ℓ 2 norm of the vector. More formally, and to introduce the notation, let 𝑑 ∈ N 1 be the number of different items and let 𝑇 ∈ N 1 be the length of the stream of updates (𝑖 𝑗 , 𝑣 𝑗 ) ∈ [𝑑] × R for 𝑗 ∈ [𝑇], and define the vector 𝑥 at time 𝑡 as 𝑥 (𝑡) ≔ ∑︁ 𝑡 𝑗=1 𝑣 𝑗 𝑒 𝑖 𝑗 . A query 𝑞 at time 𝑡 is then a function of 𝑥 (𝑡) , and we will omit the (𝑡) superscript when referring to the current time.</p><p>There are a few common variations on this model with respect to the updates. In the cash register model or insertion only model 𝑥 is only incremented by bounded integers, i.e. 𝑣 𝑗 ∈ [𝑀],</p><p>for some 𝑀 ∈ N 1 . In the turnstile model, 𝑥 can only be incremented or decremented by bounded integers, i.e. 𝑣 𝑗 ∈ {-𝑀, . . . , 𝑀} for some 𝑀 ∈ N 1 , and the strict turnstile model is as the turnstile model with the additional constraint that the entries of 𝑥 are always non-negative, i.e. 𝑥 (𝑡)</p><formula xml:id="formula_15">𝑖 ≥ 0, for all 𝑡 ∈ [𝑇] and 𝑖 ∈ [𝑑].</formula><p>As mentioned above, we are usually space constrained so that we cannot explicitely store 𝑥 and the key idea to overcome this limitation is to store a linear sketch of 𝑥, that is storing 𝑦 ≔ 𝑓 (𝑥), where 𝑓 : R 𝑑 → R 𝑚 is a linear function and 𝑚 ≪ 𝑑, and then answering queries by applying some function on 𝑦 rather than 𝑥. Note that since 𝑓 is linear, we can apply it to each update individually and compute 𝑦 as the sum of the sketched updates. Furthermore, we can aggregate results from different streams by adding the different sketches, allowing us to distribute the computation of the streaming algorithm.</p><p>The relevant Johnson-Lindenstrauss lemma in this setting is lemma 1.2 as with a JLD we get linearity and are able to sample a JLT before seeing any data at the cost of introducing some failure probability.</p><p>Based on JLDs, the most natural streaming problem to tackle is second frequency moment estimation in the turnstile model, i.e. approximating ∥𝑥 ∥ 2 2 , which has found use in database query optimisation [Alo+02; WDJ91; DeW+92] and network data analysis [Gil+01; CG05] among other areas. Simply letting 𝑓 be a sample from a JLD and returning ∥ 𝑓 (𝑥)∥ 2 2 on queries, gives a factor (1 ± 𝜀) approximation with failure probability 𝛿 using 𝒪(𝜀 -2 log 1 𝛿 + | 𝑓 |) words12 of space, where | 𝑓 | denotes the words of space needed to store and apply 𝑓 . However, the approach taken by the streaming literature is to estimate ∥𝑥 ∥ 2 2 with constant error probability using 𝒪(𝜀 -2 + | 𝑓 |) words of space, and then sampling 𝒪(log 1 𝛿 ) JLTs 𝑓 1 , . . . , 𝑓 𝒪(log 1/𝛿) : R 𝑑 → R 𝒪(𝜀 -2 ) and responding to a query with median 𝑘 ∥ 𝑓 𝑘 (𝑥)∥ 2 2 , which reduces the error probability to 𝛿. This allows for simpler analyses as well more efficient embeddings (in the case of Count Sketch) compared to using a single bigger JLT, but it comes at the cost of not embedding into ℓ 2 , which is needed for some applications outside of streaming. With this setup the task lies in constructing space efficient JLTs and a seminal work here is the AMS Sketch a.k.a. AGMS Sketch a.k.a. Tug-of-War Sketch [AMS99; Alo+02], whose JLTs can be defined as 𝑓 𝑖 ≔ 𝑚 -1/2 𝐴𝑥, where 𝐴 ∈ {-1, 1} 𝑚×𝑑 is a random matrix. The key idea is that each row 𝑟 of 𝐴 can be backed by a hash function 𝜎 𝑟 : [𝑑] → {-1, 1} that need only be 4-wise independent, meaning that for any set of 4 distinct keys {𝑘 1 , . . . 𝑘 4 } ⊂ [𝑑] and 4 (not necessarily distinct) values 𝑣 1 , . . . 𝑣 4 ∈ {-1, 1}, the probability that the keys hash to those values is</p><formula xml:id="formula_16">Pr 𝜎 𝑟 [ ⋀︁ 𝑖 𝜎 𝑟 (𝑘 𝑖 ) = 𝑣 𝑖 ] = |︁ |︁ {-1, 1} |︁ |︁ -4</formula><p>. This can for instance13 be attained by implementing 𝜎 𝑟 as 3rd degree polynomial modulus a sufficiently large prime with random coefficients <ref type="bibr" target="#b203">[WC81]</ref>, and so such a JLT need only use 𝒪(𝜀 -2 ) words of space. Embedding a scaled standard unit vector with such a JLT takes 𝒪(𝜀 -2 ) time leading to an overall update time of the AMS Sketch of 𝒪(𝜀 -2 log 1 𝛿 ). A later improvement of the AMS Sketch is the so-called Fast-AGMS Sketch [CG05] a.k.a. Count Sketch [CCF04; TZ12], which sparsifies the JLTs such that each column in their matrix representations only has one non-zero entry. Each JLT can be represented by a pairwise independent hash function ℎ : [𝑑] → [𝒪(𝜀 -2 )] to choose the position of each nonzero entry and a 4-wise independent hash function 𝜎 : [𝑑] → {-1, 1} to choose random signs as before.</p><p>This reduces the standard unit vector embedding time to 𝒪(1) and so the overall update time becomes 𝒪(log 1 𝛿 ) for Count Sketch. It should be noted that the JLD inside Count Sketch is also known as Feature Hashing, which we will return to in section 1.4.1.</p><p>Despite not embedding into ℓ 2 , due to the use of the non-linear median, AMS Sketch and Count Sketch approximately preserve dot products similarly to corollary 1.5 [CG05, Theorem 2.1 and Theorem 3.5]. This allows us to query for the (approximate) frequency of any particular item as median</p><formula xml:id="formula_17">𝑘 ⟨ 𝑓 𝑘 (𝑥), 𝑓 𝑘 (𝑒 𝑖 )⟩ = ⟨𝑥, 𝑒 𝑖 ⟩ ± 𝜀∥𝑥∥ 2 ∥𝑒 𝑖 ∥ 2 = 𝑥 𝑖 ± 𝜀∥𝑥∥ 2</formula><p>with probability at least 1 -𝛿. This can be extended to finding frequent items in an insertion only stream <ref type="bibr" target="#b43">[CCF04]</ref>. The idea is to use a slightly larger14 Count Sketch instance to maintain a heap of the 𝑘 approximately most frequent items of the stream so far. That is, if we let 𝑖 𝑘 denote the 𝑘th most frequent item (i.e.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>|︁ |︁ {𝑗 | 𝑥 𝑗 ≥ 𝑥 𝑖 𝑘 }</head><p>|︁ |︁ = 𝑘), then with probability 1 -𝛿 we have 𝑥 𝑗 &gt; (1 -𝜀)𝑥 𝑖 𝑘 for every item 𝑗 in our heap.</p><p>For more on streaming algorithms, we refer the reader to <ref type="bibr" target="#b146">[Mut05]</ref> and <ref type="bibr" target="#b148">[Nel11]</ref>, which also relates streaming to Johnson-Lindenstrauss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4">The Tapestry of Johnson-Lindenstrauss Transforms</head><p>Isti mirant stella -Scene 32, The Bayeux Tapestry <ref type="bibr" target="#b195">[Unk70]</ref> As mentioned in section 1.2, the original JLD from <ref type="bibr" target="#b103">[JL84]</ref> is a distribution over functions 𝑓 : R 𝑑 → R 𝑚 , where15 𝑓 (𝑥) = (𝑑/𝑚) 1/2 𝐴𝑥 and 𝐴 is a random 𝑚 × 𝑑 matrix whose rows form an orthonormal basis of some 𝑚-dimensional subspace of R 𝑑 , i.e. the rows are unit vectors and pairwise orthogonal. While Johnson and Lindenstrauss <ref type="bibr" target="#b103">[JL84]</ref> showed that 𝑚 = Θ(𝜀 -2 log |𝑋 |) suffices to prove lemma 1.1, they did not give any bounds on the constant in the big-𝒪 expression. This was remedied in <ref type="bibr" target="#b76">[FM88]</ref>, which proved that 𝑚 = </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>⌉︁</head><p>. The next thread of JL research worked on simplifying the JLD constructions as Indyk and Motwani <ref type="bibr" target="#b87">[HIM12]</ref> showed that sampling each entry in the matrix i.i.d. from a properly scaled Gaussian distribution is a JLD. The rows of such a matrix do not form a basis as they are with high probability not orthogonal; however, the literature still refer to this and most other JLDs as random projections. Shortly thereafter Arriaga and Vempala [AV06] constructed a JLD by sampling i.i.d. from a Rademacher16 distribution, and Achlioptas <ref type="bibr" target="#b3">[Ach03]</ref> sparsified 14 Rather than each JLT having a target dimension of 𝒪(𝜀 -2 ), the analysis needs the target dimension to be</p><formula xml:id="formula_18">𝒪 (︂ ∥ tail 𝑘 (𝑥)∥ 2 2 (𝜀𝑥 𝑖 𝑘 ) 2 )︂</formula><p>, where tail 𝑘 (𝑥) denotes 𝑥 with its 𝑘 largest entries zeroed out.</p><p>the Rademacher construction such that the entries 𝑎 𝑖𝑗 are sampled i.i.d. with Pr[𝑎 𝑖𝑗 = 0] = 2/3 and Pr[𝑎 𝑖𝑗 = -1] = Pr[𝑎 𝑖𝑗 = 1] = 1/6. We will refer to such sparse i.i.d. Rademacher constructions as Achlioptas constructions. The Gaussian and Rademacher results have later been generalised [Mat08; IN07; KM05] to show that a JLD can be constructed by sampling each entry in a 𝑚 × 𝑑 matrix i.i.d. from any distribution with mean 0, variance 1, and a subgaussian tail17. It should be noted that these developments have a parallel in the streaming literature as the previously mentioned AMS Sketch [AMS99; Alo+02] is identical to the Rademacher construction <ref type="bibr" target="#b20">[AV06]</ref>, albeit with constant error probability.</p><p>As for the target dimension for these constructions, <ref type="bibr" target="#b87">[HIM12]</ref> proved that the Gaussian</p><formula xml:id="formula_19">construction is a JLD if 𝑚 ≥ 8(𝜀 2 -2𝜀 3 /3) -1 (︁ ln |𝑋 | + 𝒪(log 𝑚)</formula><p>)︁</p><p>, which roughly corresponds to an additional additive 𝒪(𝜀 -2 log log |𝑋 |) term over the original construction. This additive log log term was shaved off by the proof in <ref type="bibr" target="#b61">[DG02]</ref>, which concerns itself with the original JLD construction but can easily18 be adapted to the Gaussian construction, and the proof in <ref type="bibr" target="#b20">[AV06]</ref>, which also give the same log log free bound for the dense Rademacher construction. Achlioptas <ref type="bibr" target="#b3">[Ach03]</ref> showed that his construction also achieves</p><formula xml:id="formula_20">𝑚 = ⌈︁ 8(𝜀 2 -2𝜀 3 /3) -1 ln |𝑋 |</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>⌉︁</head><p>. The constant of 8 has been improved for the Gaussian and dense Rademacher constructions in the sense that Rojo and Nguyen [RN10; Ngu09] have been able to replace the bound with more intricate19 expressions, which yield a 10 to 40 % improvement for many sets of parameters. However, in the distributional setting it has been shown in <ref type="bibr" target="#b30">[BGK18]</ref> that 𝑚 ≥ 4𝜀 -2 ln 1 𝛿 (1 -𝑜(1)) is necessary for any JLD to satisfy lemma 1.2, which corresponds to a constant of 8 if we prove lemma 1.1 the usual way by setting 𝛿 = 𝑛 -2 and union bounding over all pairs of vectors.</p><p>There seems to have been some confusion in the literature regarding the improvements in target dimension. The main pitfall was that some papers [e.g. Ach03; HIM12; DG02; RN10; BGK18] were only referring to <ref type="bibr" target="#b76">[FM88]</ref> when referring to the target dimension bound of the original construction. As such, [Ach03; HIM12] mistakenly claim to improve the constant for the target dimension with their constructions. Furthermore, <ref type="bibr" target="#b2">[Ach01]</ref> is sometimes [e.g. in AC09; Mat08; Sch18] the only work credited for the Rademacher construction, despite it being developed independently and published 2 years prior in <ref type="bibr" target="#b21">[AV99]</ref>.</p><p>All the constructions that have been mentioned so far in this section, embed a vector by performing a relatively dense and unstructured matrix-vector multiplication, which takes Θ(𝑚∥𝑥 ∥ 0 ) = 𝒪(𝑚𝑑) time20 to compute. This sparked two distinct but intertwined strands of research seeking to reduce the embedding time, namely the sparsity-based JLDs which dealt with the density of the embedding matrix and the fast Fourier transform-based which introduced more structure to the matrix. 17 A real random variable 𝑋 with mean 0 has a subgaussian tail if there exists constants 𝛼, 𝛽 &gt; 0 such that for all 𝜆 &gt; 0, Pr</p><formula xml:id="formula_21">[︁ |𝑋 | &gt; 𝜆 ]︁ ≤ 𝛽𝑒 -𝛼𝜆 2</formula><p>. 18 The main part of the proof in <ref type="bibr" target="#b61">[DG02]</ref> is showing that the ℓ 2 norm of a vector of i.i.d. Gaussians is concentrated around the expected value. A vector projected with the Gaussian construction is distributed as a vector of i.i.d. Gaussians. 19 For example, one of the bounds for the Rademacher construction is 𝑚 ≥ and <ref type="figure">Φ</ref> -1 is the quantile function of the standard Gaussian random variable.</p><formula xml:id="formula_22">2(𝑑-1)𝛼 2 𝑑𝜀 2 , where 𝛼 ≔ 𝑄+ √ 𝑄 2 +5.98 2 , 𝑄 ≔ Φ -1 (1 -1/|𝑋 | 2 ),</formula><p>20 ∥𝑥 ∥ 0 is the number of nonzero entries in the vector 𝑥.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4.1">Sparse Johnson-Lindenstrauss Transforms</head><p>The simple fact underlying the following string of results is that if 𝐴 has 𝑠 nonzero entries per column, then 𝑓 (𝑥) = 𝐴𝑥 can be computed in Θ(𝑠 ∥𝑥∥ 0 ) time. The first result here is the Achlioptas construction [Ach03] mentioned above, whose column sparsity 𝑠 is 𝑚/3 in expectancy, which leads to an embedding time that is a third of the full Rademacher construction21. However, the first superconstant improvement is due to Dasgupta, Kumar, and Sarlós <ref type="bibr" target="#b66">[DKS10]</ref>, who based on heuristic approaches [Wei+09; Shi+09b; LLS07; GD08] constructed a JLD with 𝑠 = 𝒪(𝜀 -1 log 1 𝛿 log 2 𝑚 𝛿 ). Their construction, which we will refer to as the DKS construction, works by sampling 𝑠 hash functions ℎ 1 , . . . , ℎ 𝑠 : [𝑑] → {-1, 1} × [𝑚] independently, such that each source entry 𝑥 𝑖 will be hashed to 𝑠 random signs 𝜎 𝑖,1 , . . . , 𝜎 𝑖,𝑠 and 𝑠 target coordinates 𝑗 𝑖,1 , . . . , 𝑗 𝑖,𝑠 (with replacement). The embedding can then be defined as 𝑓 (𝑥) ≔ ∑︁ 𝑖 ∑︁ 𝑘 𝑒 𝑗 𝑖,𝑘 𝜎 𝑖,𝑘 𝑥 𝑖 , which is to say that every source coordinate is hashed to 𝑠 output coordinates and randomly added to or subtracted from those output coordinates. The sparsity analysis was later tightened to show that 𝑠 = 𝒪(𝜀 -1 log 1 𝛿 log 𝑚 𝛿 ) suffices [KN10; KN14] and even 𝑠 = 𝒪</p><formula xml:id="formula_23">(︂ 𝜀 -1 (︁ log 1 𝛿 log log log 1 𝛿 log log 1 𝛿 )︁ 2 )︂</formula><p>suffices for the DKS construction assuming 𝜀 &lt; log -2 1 𝛿 <ref type="bibr" target="#b35">[BOR10]</ref>, while <ref type="bibr" target="#b119">[KN14]</ref> showed that 𝑠 = Ω(𝜀 -1 log 2 1 𝛿 /log 2 1 𝜀 ) is neccessary for the DKS construction. Kane and Nelson <ref type="bibr" target="#b119">[KN14]</ref> present two constructions that circumvent the DKS lower bound by ensuring that the hash functions do not collide within a column, i.e. 𝑗 𝑖,𝑎 ≠ 𝑗 𝑖,𝑏 for all 𝑖, 𝑎, and 𝑏. The first construction, which we will refer to as the graph construction, simply samples the 𝑠 coordinates without replacement. The second construction, which we will refer to as the block construction, partitions the output vector into 𝑠 consecutive blocks of length 𝑚/𝑠 and samples one output coordinate per block. Note that the block construction is the same as Count Sketch from the streaming literature [CG05; CCF04], though the hash functions differ and the output is interpreted differently. Kane and Nelson <ref type="bibr" target="#b119">[KN14]</ref> prove that 𝑠 = Θ(𝜀 -1 log 1 𝛿 ) is both neccessary and sufficient in order for their two constructions to satisfy lemma 1.2. Note that while Count Sketch is even sparser than the lower bound for the block construction, it does not contradict it as Count sketch does not embed into ℓ 𝑚 2 as it computes the median, which is nonlinear. As far as general sparsity lower bounds go, <ref type="bibr" target="#b66">[DKS10]</ref> shows that an average column sparsity of</p><formula xml:id="formula_24">𝑠 avg = Ω (︁ min{𝜀 -2 , 𝜀 -1 √︁ log 𝑚 𝑑} )︁</formula><p>is neccessary for a sparse JLD, while Nelson and Nguyê ˜n <ref type="bibr" target="#b153">[NN13b]</ref> improves upon this by showing that there exists a set of points 𝑋 ∈ R 𝑑 such that any JLT for that set must have column sparsity 𝑠 = Ω(𝜀 -1 log |𝑋 |/log 1 𝜀 ) in order to satisfy lemma 1.1. And so it seems that we have almost reached the limit of the sparse JL approach, but why should theory be in the way of a good result? Let us massage the definitions so as to get around these lower bounds.</p><p>The hard instances used to prove the lower bounds [NN13b; KN14] consist of very sparse vectors, e.g. 𝑥 = (1/ √ 2, 1/ √ 2, 0, . . . , 0) ᵀ , but the vectors we are interested in applying a JLT to might not be so unpleasant, and so by restricting the input vectors to be sufficiently "nice", we can get meaningful result that perform better than what the pessimistic lower bound would indicate. The formal formulation of this niceness is bounding the ℓ ∞ /ℓ 2 ratio of the vectors lemmas 1.1 and 1.2 need apply to. Let us denote this norm ratio as 𝜈 ∈ [1/ √ 𝑑, 1], and revisit some of the sparse JLDs. The Achlioptas construction <ref type="bibr" target="#b3">[Ach03]</ref> can be generalised so that the expected number of nonzero entries per column is 𝑞𝑚 rather than 1 3 𝑚 for a parameter 𝑞 ∈ [0, 1]. Ailon and Chazelle <ref type="bibr" target="#b1">[AC09]</ref> show that if 𝜈 = 𝒪 (︁ √︂</p><formula xml:id="formula_25">log 1 𝛿 / √ 𝑑 )︁ then choosing 𝑞 = Θ (︁ log 2 1/𝛿 𝑑 )︁</formula><p>and sampling the nonzero entries from a Gaussian distribution suffices. This result is generalised in <ref type="bibr" target="#b139">[Mat08]</ref> by proving that for all 𝜈 ∈ [1/ √ 𝑑, 1] choosing 𝑞 = Θ(𝜈 2 log 𝑑 𝜀𝛿 ) and sampling the nonzero entries from a Rademacher distribution is a JLD for the vectors constrained by that 𝜈.</p><p>Be aware that sometimes [e.g. in DKS10; BOR10] this bound22 on 𝑞 is misinterpreted as a lower bound stating that 𝑞𝑚 = Ω ˜(𝜀 -2 ) is neccessary for the Achlioptas construction when 𝜈 = 1. However, Matoušek <ref type="bibr" target="#b139">[Mat08]</ref> only loosely argues that his bound is tight for 𝜈 ≤ 𝑑 -0.1 , and if it indeed was tight at 𝜈 = 1, the factors hidden by the Ω ˜would lead to the contradiction that 𝑚 ≥ 𝑞𝑚 = Ω(𝜀 -2 log 1 𝛿 log 𝑑 𝜀𝛿 ) = 𝜔(𝑚). The heuristic [Wei+09; Shi+09b; LLS07; GD08] that [DKS10] is based on is called Feature Hashing a.k.a. the hashing trick a.k.a. the hashing kernel and is a sparse JL construction with exactly 1 nonzero entry per column23. The block construction can then be viewed as the concatenation of 𝑠 = Θ(𝜀 -1 log 1 𝛿 ) feature hashing instances, and the DKS construction can be viewed as the sum of 𝑠 = 𝒪(𝜀 -1 log 1 𝛿 log 𝑚 𝛿 ) Feature Hashing instances or alternatively as first duplicating each entry of 𝑥 ∈ R 𝑑 𝑠 times before applying Feature Hashing to the enlarged vector 𝑥 ′ ∈ R 𝑠𝑑 : Let 𝑓 dup : R 𝑑 → R 𝑠𝑑 be a function that duplicates each entry in its input 𝑠 times, i.e.</p><formula xml:id="formula_26">𝑓 dup (𝑥) (𝑖-1)𝑠+𝑗 = 𝑥 ′ (𝑖-1)𝑠+𝑗 ≔ 𝑥 𝑖 for 𝑖 ∈ [𝑑], 𝑗 ∈ [𝑠], then 𝑓 DKS = 𝑓 FH • 𝑓 dup .</formula><p>This duplication is the key to the analysis in <ref type="bibr" target="#b66">[DKS10]</ref> as 𝑓 dup is isometric (up to normalisation) and it ensures that the ℓ ∞ /ℓ 2 ratio of 𝑥 ′ is small, i.e. 𝜈 ≤ 1/</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>√</head><p>𝑠 from the point of view of the Feature Hashing data structure ( 𝑓 FH ). And so, any lower bound on the sparsity of the DKS construction (e.g. the one given in <ref type="bibr" target="#b119">[KN14]</ref>) gives an upper bound on the values of 𝜈 for which Feature Hashing is a JLD: If 𝑢 is a unit vector such that a DKS instance with sparsity 𝑠 ˆfails to preserve 𝑢s norm within 1 ± 𝜀 with probability 𝛿, then it must be the case that Feature Hashing fails to preserve the norm of 𝑓 dup (𝑢) within 1 ± 𝜀 with probability 𝛿, and therefore the ℓ ∞ /ℓ 2 ratio for which Feature Hashing can handle all vectors is strictly less than 1/ √ 𝑠 ˆ.</p><p>Written more concisely the statement is</p><formula xml:id="formula_27">𝑠 DKS = Ω(𝑎) =⇒ 𝜈 FH = 𝒪(1/ √ 𝑎) and by contraposi- tion24 𝜈 FH = Ω(1/ √ 𝑎) =⇒ 𝑠 DKS = 𝒪(𝑎)</formula><p>, where 𝑠 DKS is the minimum column sparsity of a DKS construction that is a JLD, 𝜈 FH is the maximum ℓ ∞ /ℓ 2 constraint for which Feature Hashing is a JLD, and 𝑎 is any positive expression. Furthermore, if we prove an upper bound on 𝜈 FH using a hard instance that is identical to an 𝑥 ′ that the DKS construction can generate after duplication, we can replace the previous two implications with bi-implications.</p><p>[Wei+09] claims to give a bound on 𝜈 FH , but it sadly contains an error in its proof of this bound [DKS10; Wei+10]. Dahlgaard, Knudsen, and Thorup <ref type="bibr" target="#b67">[DKT17]</ref> improve the 𝜈 FH lower bound to</p><formula xml:id="formula_28">𝜈 FH = Ω (︂ √︃ 𝜀 log(1+ 4 𝜀 ) log 1 𝛿 log 𝑚 𝛿 )︂</formula><p>, and Freksen, Kamma, and Larsen <ref type="bibr" target="#b72">[FKL18]</ref> give an intricate but tight bound for 𝜈 FH shown in theorem 1.8, where the hard instance used to prove the upper 22 Which seems to be the only thing in <ref type="bibr" target="#b139">[Mat08]</ref> related to a bound on 𝑞. 23 i.e. the DKS, graph, or block construction with 𝑠 = 1. 24 Contraposition is (𝑃 =⇒ 𝑄) =⇒ (¬𝑄 =⇒ ¬𝑃) and it does not quite prove what was just claimed without some assumptions that 𝑠 DKS , 𝜈 FH , and 𝑎 do not behave too erratically.</p><p>bound is identical to an 𝑥 ′ from the DKS construction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 1.8 ([FKL18]</head><p>). There exist constants 𝐶 ≥ 𝐷 &gt; 0 such that for every 𝜀, 𝛿 ∈ (0, 1) and 𝑚 ∈ N 1 the following holds. If</p><formula xml:id="formula_29">𝐶 lg 1 𝛿 𝜀 2 ≤ 𝑚 &lt; 2 𝜀 2 𝛿 then 𝜈 FH (𝑚, 𝜀, 𝛿) = Θ (︃ √ 𝜀 min {︂ log 𝜀𝑚 log 1 𝛿 log 1 𝛿 , ⌜ ⃓ ⎷ log 𝜀 2 𝑚 log 1 𝛿 log 1 𝛿 }︂ )︃ . Otherwise, if 𝑚 ≥ 2 𝜀 2 𝛿 then 𝜈 FH (𝑚, 𝜀, 𝛿) = 1. Moreover if 𝑚 &lt; 𝐷 lg 1 𝛿 𝜀 2 then 𝜈 FH (𝑚, 𝜀, 𝛿) = 0. Furthermore, if an 𝑥 ∈ {0, 1} 𝑑 satisfies 𝜈 FH &lt; ∥𝑥∥ -1 2 &lt; 1 then Pr 𝑓 ∼FH [︂ |︁ |︁ ∥ 𝑓 (𝑥)∥ 2 2 -∥𝑥∥ 2 2 |︁ |︁ &gt; 𝜀∥𝑥 ∥ 2 2 ]︂ &gt; 𝛿.</formula><p>This bound gives a tight tradeoff between target dimension 𝑚, distortion 𝜀, error probability 𝛿, and ℓ ∞ /ℓ 2 constraint 𝜈 for Feature Hashing, while showing how to construct hard instances for Feature Hashing: Vectors with the shape 𝑥 = (1, . . . , 1, 0, . . . , 0) ᵀ are hard instances if they contain few 1s, meaning that Feature Hashing cannot preserve their norms within 1 ± 𝜀 with probability 𝛿. Theorem 1.8 is used in corollary 1.9 to provide a tight tradeoff between 𝑚, 𝜀, 𝛿, 𝜈, and column sparsity 𝑠 for the DKS construction.</p><p>Corollary 1.9. Let 𝜈 DKS ∈ [1/ √ 𝑑, 1] denote the largest ℓ ∞ /ℓ 2 ratio required, 𝜈 FH denote the ℓ ∞ /ℓ 2 constraint for Feature Hashing as defined in theorem 1.8, and 𝑠 DKS ∈ [𝑚] as the minimum column sparsity such that the DKS construction with that sparsity is a JLD for the subset of vectors 𝑥 ∈ R 𝑑 that satisfy ∥𝑥∥ ∞ /∥𝑥 ∥ 2 ≤ 𝜈 DKS . Then</p><formula xml:id="formula_30">𝑠 DKS = Θ (︂ 𝜈 2 DKS 𝜈 2 FH )︂ .</formula><p>The proof of this corollary is deferred to section 2.2. Jagadeesan <ref type="bibr" target="#b98">[Jag19]</ref> generalised the result from <ref type="bibr" target="#b72">[FKL18]</ref> to give a lower bound25 on the 𝑚, 𝜀, 𝛿, 𝜈, and 𝑠 tradeoff for any sparse Rademacher construction with a chosen column sparsity, e.g. the block and graph constructions, and gives a matching upper bound for the graph construction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4.2">Structured Johnson-Lindenstrauss Transforms</head><p>As we move away from the sparse JLDs we will slightly change our idea of what an efficient JLD is. In the previous section the JLDs were especially fast when the vectors were sparse, as the running time scaled with ∥𝑥∥ 0 , whereas we in this section will optimise for dense input vectors such that an embedding time of 𝒪(𝑑 log 𝑑) is a satisfying result.</p><p>The chronologically first asymptotic improvement over the original JLD construction is due to Ailon and Chazelle <ref type="bibr" target="#b1">[AC09]</ref> who introduced the so-called Fast Johnson-Lindenstrauss Transform (FJLT). As mentioned in the previous section, <ref type="bibr" target="#b1">[AC09]</ref> showed that we can use a very sparse (and therefore very fast) embedding matrix as long as the vectors have a low ℓ ∞ /ℓ 2 ratio, and furthermore that applying a randomised Walsh-Hadamard transform to a vector results in a low ℓ ∞ /ℓ 2 ratio with high probability. And so, the FJLT is defined as 𝑓 (𝑥) ≔ 𝑃𝐻𝐷𝑥, where 𝑃 ∈ R 𝑚×𝑑 is a sparse Achlioptas matrix with Gaussian entries and 𝑞 = Θ (︁ log 2 1/𝛿 𝑑 )︁</p><p>, 𝐻 ∈ {-1, 1} 𝑑×𝑑 is a Walsh-Hadamard matrix26, and 𝐷 ∈ {-1, 0, 1} 𝑑×𝑑 is a random diagonal matrix with i.i.d. Rademachers on the diagonal. As the Walsh-Hadamard transform can be computed using a simple recursive formula, the expected embedding time becomes 𝒪(𝑑 log 𝑑 + 𝑚 log 2 1 𝛿 ). And as mentioned, <ref type="bibr" target="#b139">[Mat08]</ref> showed that we can sample from a Rademacher rather than a Gaussian distribution when constructing the matrix 𝑃. The embedding time improvement of FJLT over previous constructions depends on the relationship between 𝑚 and 𝑑. If 𝑚 = Θ(𝜀 -2 log 1 𝛿 ) and 𝑚 = 𝒪(𝜀 -4/3 𝑑 1/3 ), FJLT's embedding time becomes bounded by the Walsh-Hadamard transform at 𝒪(𝑑 log 𝑑), but at 𝑚 = Θ(𝑑 1/2 ) FJLT is only barely faster than the original construction.</p><p>Ailon and Liberty <ref type="bibr" target="#b8">[AL09]</ref> improved the running time of the FJLT construction to 𝒪(𝑑 log 𝑚) for 𝑚 = 𝒪(𝑑 1/2-𝛾 ) for any fixed 𝛾 &gt; 0. The increased applicable range of 𝑚 was achieved by applying multiple randomised Walsh-Hadamard transformations, i.e. replacing 𝐻𝐷 with ∏︁ 𝑖 𝐻𝐷 (𝑖) , where the 𝐷 (𝑖) s are a constant number of independent diagonal Rademacher matrices, as well as by replacing 𝑃 with 𝐵𝐷 where 𝐷 is yet another diagonal matrix with Rademacher entries and 𝐵 is consecutive blocks of specific partial Walsh-Hadamard matrices (based on so-called binary dual BCH codes [see e.g. MS77]). The reduction in running time comes from altering the transform slightly by partitioning the input into consecutive blocks of length poly(𝑚) and applying the randomised Walsh-Hadamard transforms to each of them independently. We will refer to this variant of FJLT as the BCHJL construction.</p><p>The next pattern of results has roots in compressed sensing and approaches the problem from another angle: Rather than being fast only when 𝑚 ≪ 𝑑, they achieve 𝒪(𝑑 log 𝑑) embedding time even when 𝑚 is close to 𝑑, at the cost of 𝑚 being suboptimal. Before describing these constructions, let us set the scene by briefly introducing some concepts from compressed sensing.</p><p>Roughly speaking, compressed sensing concerns itself with recovering a sparse signal via a small number of linear measurements and a key concept here is the Restricted Isometry Property [CT05; CT06; CRT06; Don06]. Definition 1.10 (Restricted Isometry Property). Let 𝑑, 𝑚, 𝑘 ∈ N 1 with 𝑚, 𝑘 &lt; 𝑑 and 𝜀 ∈ (0, 1). A linear function 𝑓 : R 𝑑 → R 𝑚 is said to have the Restricted Isometry Property of order 𝑘 and level 𝜀 (which we will denote as</p><formula xml:id="formula_31">(𝑘, 𝜀)-RIP) if for all 𝑥 ∈ R 𝑑 with ∥𝑥∥ 0 ≤ 𝑘, |︁ |︁ ∥ 𝑓 (𝑥)∥ 2 2 -∥𝑥 ∥ 2 2 |︁ |︁ ≤ 𝜀∥𝑥 ∥ 2 2 .<label>(10)</label></formula><p>In the compressed sensing literature it has been shown [CT06; RV08] that the subsampled Hadamard transform (SHT) defined as 𝑓 (𝑥) ≔ 𝑆𝐻𝑥, has the (𝑘, 𝜀)-RIP with high probability 26 One definition of the Walsh-Hadamard matrices is that the entries are 𝐻 𝑖𝑗 = (-1) ⟨𝑖-1,𝑗-1⟩ for all 𝑖, 𝑗 ∈ [𝑑], where ⟨𝑎, 𝑏⟩ denote the dot product of the (lg 𝑑)-bit vectors corresponding to the binary representation of the numbers 𝑎, 𝑏 ∈ {0, . . . , 𝑑 -1}, and 𝑑 is a power of two. To illustrate its recursive nature, a large Walsh-Hadamard matrix can be described as a Kronecker product of smaller Walsh-Hadamard matrices, i.e. if 𝑑 &gt; 2 and 𝐻 (𝑛) refers to a 𝑛 × 𝑛 Walsh-Hadamard matrix, then</p><formula xml:id="formula_32">𝐻 (𝑑) = 𝐻 (2) ⊗ 𝐻 (𝑑/2) = (︃ 𝐻 (𝑑/2) 𝐻 (𝑑/2) 𝐻 (𝑑/2) -𝐻 (𝑑/2) )︃ . ⎛ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎝ 𝑡 0 𝑡 1 𝑡 2 • • • 𝑡 𝑑-1 𝑡 -1 𝑡 0 𝑡 1 • • • 𝑡 𝑑-2 𝑡 -2 𝑡 -1 𝑡 0 • • • 𝑡 𝑑-3 . . . . . . . . . . . . . . . 𝑡 -(𝑚-1) 𝑡 -(𝑚-2) 𝑡 -(𝑚-3) • • • 𝑡 𝑑-𝑚 ⎞ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎠ Figure 1:</formula><p>The structure of a Toeplitz matrix.</p><p>for</p><formula xml:id="formula_33">𝑚 = Ω (︁ 𝜀 -2 𝑘 log 4 𝑑</formula><p>)︁</p><p>while allowing a vector to be embedded in 𝒪(𝑑 log 𝑑) time. Here 𝐻 ∈ {-1, 1} 𝑑×𝑑 is the Walsh-Hadamard matrix and 𝑆 ∈ {0, 1} 𝑚×𝑑 samples 𝑚 entries of 𝐻𝑥 with replacement, i.e. each row in 𝑆 has one non-zero entry per row, which is chosen uniformly and independently, i.e. 𝑆 is a uniformly random feature selection matrix. Inspired by this transform and the FJLT mentioned previously, Ailon and Liberty <ref type="bibr" target="#b10">[AL13]</ref> were able to show that the subsampled randomised Hadamard transform (SRHT) defined as 𝑓 (𝑥) ≔ 𝑆𝐻𝐷𝑥, is a JLT if 𝑚 = Θ(𝜀 -4 log |𝑋 | log 4 𝑑). Once again 𝐷 denotes a random diagonal matrix with Rademacher entries, and 𝑆 and 𝐻 is as in the SHT. Some related results include Do et al. <ref type="bibr" target="#b68">[Do+09]</ref> who before <ref type="bibr" target="#b10">[AL13]</ref> were able to get a bound of 𝑚 = Θ(𝜀 -2 log 3 |𝑋 |) in the large set case where |𝑋 | ≥ 𝑑, <ref type="bibr" target="#b187">[Tro11]</ref> which showed how the SRHT construction approximately preserves the norms of a subspace of vectors, and <ref type="bibr" target="#b131">[LL20]</ref> which modified the sampling matrix 𝑆 to improve precision when used as a preprocessor for support vector machines (SVMs) by sacrificing input data independence.</p><p>This target dimension bound of <ref type="bibr" target="#b10">[AL13]</ref> was later tightened by Krahmer and Ward <ref type="bibr" target="#b122">[KW11]</ref>, who showed that 𝑚 = Θ(𝜀 -2 log |𝑋 | log 4 𝑑) suffices for the SRHT. This was a corollary of a more general result, namely that if 𝜎 : R 𝑑 → R 𝑑 applies random signs equivalently to the 𝐷 matrices mentioned previously and 𝑓 : R 𝑑 → R 𝑚 has the (︁</p><formula xml:id="formula_34">Ω(log |𝑋 |), 𝜀/4</formula><p>)︁</p><p>-RIP then 𝑓 • 𝜎 is a JLT with high probability. An earlier result by Baraniuk et al. <ref type="bibr" target="#b26">[Bar+08]</ref> showed that a transform sampled from a JLD has the (︁ 𝒪(𝜀 2 𝑚/log 𝑑), 𝜀 )︁ -RIP with high probability. And so, as one might have expected from their appearance, the Johnson-Lindenstrauss Lemma and the Restricted Isometry Property are indeed cut from the same cloth.</p><p>Another transform from the compressed sensing literature uses so-called Toeplitz or partial circulant matrices [Baj+07; Rau09; Rom09; Hau+10; RRT12; Baj12; DJR19], which can be defined in the following way. For 𝑚, 𝑑 ∈ N 1 we say that 𝑇 ∈ R 𝑚×𝑑 is a real Toeplitz matrix if there exists 𝑡 -(𝑚-1) , 𝑡 -(𝑚-2) . . . , 𝑡 𝑑-1 ∈ R such that 𝑇 𝑖𝑗 = 𝑡 𝑗-𝑖 . This has the effect that the entries on any one diagonal are the same (see fig. <ref type="figure" target="#fig_1">1</ref>) and computing the matrix-vector product corresponds to computing the convolution with a vector of the 𝑡s. Partial circulant matrices are special cases of Toeplitz matrices where the diagonals "wrap around" the ends of the matrix, i.e. 𝑡 -𝑖 = 𝑡 𝑑-𝑖 for all 𝑖 ∈ [𝑚 -1].</p><p>As a JLT, the Toeplitz construction is 𝑓 (𝑥) ≔ 𝑇𝐷𝑥, where 𝑇 ∈ {-1, 1} 𝑚×𝑑 is a Toeplitz matrix with i.i.d. Rademacher entries and 𝐷 ∈ {-1, 0, 1} 𝑑×𝑑 is a diagonal matrix with Rademacher entries as usual. Note that the convolution of two vectors corresponds to the entrywise product in Fourier space, and we can therefore employ fast Fourier transform (FFT) to embed a vector with the Toeplitz construction in time 𝒪(𝑑 log 𝑑). This time can even be reduced to 𝒪(𝑑 log 𝑚) as we realise that by partitioning 𝑇 into 𝑑 𝑚 consecutive blocks of size 𝑚 × 𝑚, each block is also a Toeplitz matrix, and by applying each individually the embedding time becomes 𝒪( 𝑑 𝑚 𝑚 log 𝑚). Combining the result from <ref type="bibr" target="#b122">[KW11]</ref> with RIP bounds for Toeplitz matrices <ref type="bibr" target="#b164">[RRT12]</ref> gives that 𝑚 = Θ (︁</p><formula xml:id="formula_35">𝜀 -1 log 3/2 |𝑋 | log 3/2 𝑑 + 𝜀 -2 log |𝑋 | log 4 𝑑 )︁</formula><p>is sufficient for the Toeplitz construction to be a JLT with high probability. However, the Toeplitz construction has also been studied directly as a JLD without going via its RIP bounds. Hinrichs and Vybíral <ref type="bibr" target="#b94">[HV11]</ref> showed that 𝑚 = Θ(𝜀 -2 log 3 1 𝛿 ) is sufficient for the Toeplitz construction, and this bound was improved shortly thereafter in <ref type="bibr" target="#b202">[Vyb11]</ref> to 𝑚 = Θ(𝜀 -2 log 2 1 𝛿 ). The question then is if we can tighten the analysis to shave off the last log factor and get the elusive result of a JLD with optimal target dimension and 𝒪(𝑑 log 𝑑) embedding time even when 𝑚 is close to 𝑑. Sadly, this is not the case as Freksen and Larsen <ref type="bibr" target="#b74">[FL20]</ref> showed that there exists vectors27 that necessitates 𝑚 = Ω(𝜀 -2 log 2 1 𝛿 ) for the Toeplitz construction.</p><p>Just as JLTs are used as preprocessors to speed up algorithms that solve the problems we actually care about, we can also use a JLT to speed up other JLTs in what one could refer to as compound JLTs. More explicitely if 𝑓 1 : R 𝑑 → R 𝑑 ′ and 𝑓 2 : R 𝑑 ′ → R 𝑚 with 𝑚 ≪ 𝑑 ′ ≪ 𝑑 are two JLTs and computing 𝑓 1 (𝑥) is fast, we could hope that computing ( 𝑓 2 • 𝑓 1 )(𝑥) is fast as well as 𝑓 2 only need to handle 𝑑 ′ dimensional vectors and hope that ( 𝑓 2 • 𝑓 1 ) preserves the norm sufficiently well since both 𝑓 1 and 𝑓 2 approximately preserve norms individually. As presented here, the obvious candidate for 𝑓 1 is one of the RIP-based JLDs, which was succesfully applied in <ref type="bibr" target="#b31">[BK17]</ref>. In their construction, which we will refer to as GRHD28, 𝑓 1 is the SRHT and 𝑓 2 is the dense Rademacher construction (i.e. 𝑓 (𝑥) ≔ 𝐴 Rad 𝑆𝐻𝐷𝑥), and it can embed a vector in time 𝒪(𝑑 log 𝑚) for 𝑚 = 𝒪(𝑑 1/2-𝛾 ) for any fixed 𝛾 &gt; 0. This is a similar result to the construction of Ailon and Liberty <ref type="bibr" target="#b8">[AL09]</ref>, but unlike that construction, GRHD handles the remaining range of 𝑚 more gracefully as for any 𝑟 ∈ [1/2, 1] and 𝑚 = 𝒪(𝑑 𝑟 ), the embedding time for GRHD becomes 𝒪(𝑑 2𝑟 log 4 𝑑). However the main selling point of the GRHD construction is that it allows the simultaneous embedding of sufficiently large sets of points 𝑋 to be computed in total time 𝒪(|𝑋 |𝑑 log 𝑚), even when 𝑚 = Θ(𝑑 1-𝛾 ) for any fixed 𝛾 &gt; 0, by utilising fast matrix-matrix multiplication techniques <ref type="bibr" target="#b135">[LR83]</ref>. Another compound JLD is based on the so-called lean Walsh transforms (LWT) <ref type="bibr" target="#b126">[LAS11]</ref>,</p><p>which are defined based on so-called seed matrices. For 𝑟, 𝑐 ∈ N 1 we say that 𝐴 1 ∈ C 𝑟×𝑐 is a seed matrix if 𝑟 &lt; 𝑐, its columns are of unit length, and its rows are pairwise orthogonal and have the same ℓ 2 norm. As such, partial Walsh-Hadamard matrices and partial Fourier matrices are seed matrices (up to normalisation); however, for simplicity's sake we will keep it real by focusing on partial Walsh-Hadamard matrices. We can then define a LWT of order 𝑙 ∈ N 1 based on this seed as</p><formula xml:id="formula_36">𝐴 𝑙 ≔ 𝐴 ⊗𝑙 1 = 𝐴 1 ⊗ • • • ⊗ 𝐴 1 ,</formula><p>where ⊗ denotes the Kronecker product, which we will quickly define. Let 𝐴 be a 𝑚 × 𝑛 matrix and 𝐵 be a 𝑝 × 𝑞 matrix, then the Kronecker product 27 Curiously, the hard instances for the Toeplitz construction are very similar to the hard instances for Feature Hashing used in <ref type="bibr" target="#b72">[FKL18]</ref>. 28 Due to the choice of matrix names in <ref type="bibr" target="#b31">[BK17]</ref>.</p><p>𝐴 ⊗ 𝐵 is the 𝑚𝑝 × 𝑛𝑞 block matrix defined as</p><formula xml:id="formula_37">𝐴 ⊗ 𝐵 ≔ ⎛ ⎜ ⎜ ⎝ 𝐴 11 𝐵 • • • 𝐴 1𝑛 𝐵 . . . . . . . . . 𝐴 𝑚1 𝐵 • • • 𝐴 𝑚𝑛 𝐵 ⎞ ⎟ ⎟ ⎠ .</formula><p>Note that 𝐴 𝑙 is a 𝑟 𝑙 × 𝑐 𝑙 matrix and that any Walsh-Hadamard matrix can be written as 𝐴 𝑙 for some 𝑙 and the 2 × 2 Walsh-Hadamard matrix29 as 𝐴 1 . Furthermore, for a constant sized seed the time complexity of applying 𝐴 𝑙 to a vector is 𝑂(𝑐 𝑙 ) by using an algorithm similar to FFT. We can then define the compound transform which we will refer to as LWTJL, as 𝑓 (𝑥) ≔ 𝐺𝐴 𝑙 𝐷𝑥, where 𝐷 ∈ {-1, 1} 𝑑×𝑑 is a diagonal matrix with Rademacher entries, 𝐴 𝑙 ∈ R 𝑟 𝑙 ×𝑑 is a LWT, and 𝐺 ∈ R 𝑚×𝑟 𝑙 is a JLT, and 𝑟 and 𝑐 are constants. One way to view LWTJL is as a variant of GRHD where the subsampling occurs on the seed matrix rather than the final Walsh-Hadamard matrix. If 𝐺 can be applied in 𝒪(𝑟 𝑙 log 𝑟 𝑙 ) time, e.g. if 𝐺 is the BCHJL construction <ref type="bibr" target="#b8">[AL09]</ref> and 𝑚 = 𝒪(𝑟 𝑙(1/2-𝛾) ), the total embedding time becomes 𝒪(𝑑), as 𝑟 𝑙 = 𝑑 𝛼 for some 𝛼 &lt; 1. However, in order to prove that LWTJL satisfies lemma 1.2 the analysis of <ref type="bibr" target="#b126">[LAS11]</ref> imposes a few requirements on 𝑟, 𝑐, and the vectors we wish to embed, namely that log 𝑟/log 𝑐 ≥ 1 -2𝛿 and 𝜈 = 𝒪(𝑚 -1/2 𝑑 -𝛿 ), where 𝜈 is an upper bound on the ℓ ∞ /ℓ 2 ratio as introduced at the end of section 1.4.1. The bound on 𝜈 is somewhat tight as shown in proposition 1.11.</p><p>Proposition 1.11. For any seed matrix define LWT as the LWTJL distribution seeded with that matrix.</p><p>Then for all 𝛿 ∈ (0, 1), there exists a vector 𝑥 ∈ C 𝑑 (or 𝑥 ∈ R 𝑑 , if the seed matrix is a real matrix) satisfying ∥𝑥 ∥ ∞ /∥𝑥 ∥ 2 = Θ(log -1/2 1 𝛿 ) such that</p><formula xml:id="formula_38">Pr 𝑓 ∼LWT [ 𝑓 (𝑥) = 0] &gt; 𝛿.<label>(11)</label></formula><p>The proof of proposition 1.11 can be found in section 2.3, and it is based on constructing 𝑥 as a few copies of a vector that is orthogonal to the rows of the seed matrix.</p><p>The last JLD we will cover is based on so-called Kac random walks, and despite Ailon and Chazelle <ref type="bibr" target="#b1">[AC09]</ref> conjecturing that such a construction could satisfy lemma 1.1, it was not until Jain et al. <ref type="bibr" target="#b99">[Jai+20]</ref> that a proof was finally at hand. As with the lean Walsh transforms above, let us first define Kac random walks before describing how they can be used to construct JLDs. A Kac random walk is a Markov chain of linear transformations, where for each step we choose two coordinates at random and perform a random rotation on the plane spanned by these two coordinates, or more formally: Definition 1.12 (Kac random walk <ref type="bibr" target="#b109">[Kac56]</ref>). For a given dimention 𝑑 ∈ N 1 , let 𝐾 (0) ≔ 𝐼 ∈ {0, 1} 𝑑×𝑑 be the identity matrix, and for each 𝑡 &gt; 0 sample (𝑖 𝑡 , 𝑗 𝑡 ) ∈</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(︁ [𝑑]</head><p>2 )︁ and 𝜃 𝑡 ∈ [0, 2𝜋) independently and uniformly at random. Then define the Kac random walk of length 𝑡 as 𝐾 (𝑡) ≔ 𝑅 (𝑖 𝑡 ,𝑗 𝑡 ,𝜃 𝑡 ) 𝐾 (𝑡-1) , where 𝑅 (𝑖,𝑗,𝜃) ∈ R 𝑑×𝑑 is the rotation in the (𝑖, 𝑗) plane by 𝜃 and is given by 𝑅 (𝑖,𝑗,𝜃) 𝑒 𝑘 ≔ 𝑒 𝑘 ∀𝑘 ∉ {𝑖, 𝑗}, 𝑅 (𝑖,𝑗,𝜃) (𝑎𝑒 𝑖 + 𝑏𝑒 𝑗 ) ≔ (𝑎 cos 𝜃 -𝑏 sin 𝜃)𝑒 𝑖 + (𝑎 sin 𝜃 + 𝑏 cos 𝜃)𝑒 𝑗 .</p><p>The main JLD introduced in [Jai+20], which we will refer to as KacJL, is a compound JLD where both 𝑓 1 and 𝑓 2 consists of a Kac random walk followed by subsampling, which can be defined more formally in the following way. Let 𝑇 1 ≔ Θ(𝑑 log 𝑑) be the length of the first Kac random walk, 𝑑 ′ ≔ min{𝑑, Θ(𝜀 -2 log |𝑋 | log 2 log |𝑋 | log 3 𝑑)} be the intermediate dimension, 𝑇 2 ≔ Θ(𝑑 ′ log |𝑋 |) be the length of the second Kac random walk, and 𝑚 ≔ Θ(𝜀 -2 log |𝑋 |) be the target dimension, and then define the JLT as 𝑓 (𝑥) = ( 𝑓 2 • 𝑓 1 )(𝑥) ≔ 𝑆 (𝑚,𝑑 ′ ) 𝐾 (𝑇 2 ) 𝑆 (𝑑 ′ ,𝑑) 𝐾 (𝑇 1 ) 𝑥, where 𝐾 (𝑇 1 ) ∈ R 𝑑×𝑑 and 𝐾 (𝑇 2 ) ∈ R 𝑑 ′ ×𝑑 ′ are independent Kac random walks of length 𝑇 1 and 𝑇 2 , respectively, and 𝑆 (𝑑 ′ ,𝑑) ∈ {0, 1} 𝑑 ′ ×𝑑 and 𝑆 (𝑚,𝑑 ′ ) ∈ {0, 1} 𝑚×𝑑 ′ projects onto the first 𝑑 ′ and 𝑚 coordinates30, respectively. Since 𝐾 (𝑇) can be applied in time 𝒪(𝑇), the KacJL construction is JLD with embedding time 𝒪(𝑑 log 𝑑 + min{𝑑 log |𝑋 |, 𝜀 -2 log 2 |𝑋 | log 2 log |𝑋 | log 3 𝑑}) with asymptotically optimal target dimension, and by only applying the first part ( 𝑓 1 ), KacJL achieves an embedding time of 𝒪(𝑑 log 𝑑) but with a suboptimal target dimension of 𝒪(𝜀 -2 log |𝑋 | log 2 log |𝑋 | log 3 𝑑). Jain et al. <ref type="bibr" target="#b99">[Jai+20]</ref> also proposes a version of their JLD construction that avoids computing trigonometric functions31 by choosing the angles 𝜃 𝑡 uniformly at random from the set {𝜋/4, 3𝜋/4, 5𝜋/4, 7𝜋/4} or even the singleton set {𝜋/4}. This comes at the cost32 of increasing 𝑇 1 by a factor of log log 𝑑 and 𝑇 2 by a factor of log 𝑑, and for the singleton case multiplying with random signs (as we have done with the 𝐷 matrices in many of the previous constructions) and projecting down onto a random subset of coordinates rather than the 𝑑 ′ or 𝑚 first.</p><p>F 8 f This concludes the overview of Johnson-Lindenstrauss distributions and transforms, though there are many aspects we did not cover such as space usage, preprocessing time, randomness usage, and norms other than ℓ 2 . However, a summary of the main aspects we did cover (embedding times and target dimensions of the JLDs) can be found in table 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Name</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Embedding time</head><p>Target </p><formula xml:id="formula_39">dimension Ref. Constraints Original 𝒪(∥𝑥∥ 0 𝑚) 𝒪(𝜀 -2 log 1 𝛿 ) [JL84] Gaussian 𝒪(∥𝑥∥ 0 𝑚) 𝒪(𝜀 -2 log 1 𝛿 ) [HIM12] Rademacher 𝒪(∥𝑥∥ 0 𝑚) 𝒪(𝜀 -2 log 1 𝛿 ) [AV06] Achlioptas 𝒪( 1 3 ∥𝑥∥ 0 𝑚) 𝒪(𝜀 -2 log 1 𝛿 ) [Ach03] DKS 𝒪(∥𝑥∥ 0 𝜀 -1 log 𝑚 𝛿 log 1 𝛿 ) 𝒪(𝜀 -2 log 1 𝛿 ) [DKS10; KN14] Block JL 𝒪(∥𝑥∥ 0 𝜀 -1 log 1 𝛿 ) 𝒪(𝜀 -2 log 1 𝛿 ) [KN14] Feature Hashing 𝒪(∥𝑥∥ 0 ) 𝒪(𝜀 -2 𝛿 -1 ) [Wei+09; FKL18] Feature Hashing 𝒪(∥𝑥∥ 0 ) 𝒪(𝜀 -2 log 1 𝛿 ) [FKL18] 𝜈 ≪ 1 FJLT 𝒪(𝑑 log 𝑑 + 𝑚 log 2 1 𝛿 ) 𝒪(𝜀 -2 log 1 𝛿 ) [AC09] BCHJL 𝒪(𝑑 log 𝑚) 𝒪(𝜀 -2 log 1 𝛿 ) [AL09] 𝑚 = 𝑜(𝑑 1/2 ) SRHT 𝒪(𝑑 log 𝑑) 𝒪(𝜀 -2 log |𝑋 | log 4 𝑑) [AL13; KW11] SRHT 𝒪(𝑑 log 𝑑) 𝒪(𝜀 -2 log 3 |𝑋 |) [Do+09] |𝑋 | ≥ 𝑑 Toeplitz 𝒪(𝑑 log 𝑚) 𝒪 (︂ 𝜀 -1 log 3/2 |𝑋 | log 3/2 𝑑 + 𝜀 -2 log |𝑋 | log 4 𝑑 )︂ [KW11] Toeplitz 𝒪(𝑑 log 𝑚) 𝒪(𝜀 -2 log 2 1 𝛿 ) [HV11; Vyb11] Toeplitz 𝒪(𝑑 log 𝑚) Ω(𝜀 -2 log 2 1 𝛿 ) [FL20] GRHD 𝒪(𝑑 log 𝑚) 𝒪(𝜀 -2 log 1 𝛿 ) [BK17] 𝑚 = 𝑜(𝑑 1/2 ) GRHD 𝒪(𝑑 2𝑟 log 4 𝑑) 𝒪(𝜀 -2 log 1 𝛿 ) [BK17] 𝑚 = 𝑂(𝑑 𝑟 ) LWTJL 𝒪(𝑑) 𝒪(𝜀 -2 log 1 𝛿 ) [LAS11] 𝜈 = 𝒪(𝑚 -1/2 𝑑 -𝛿 ) KacJL 𝒪 (︃ 𝑑 log 𝑑 +min {︂ 𝑑 log |𝑋 |, 𝜀 -2 log 2 |𝑋 | log 2 log |𝑋 | log 3 𝑑 }︂ )︃ 𝒪(𝜀 -2 log |𝑋 |) [Jai+20] KacJL 𝒪(𝑑 log 𝑑) 𝒪(𝜀 -2 log |𝑋 | log 2 log |𝑋 | log 3 𝑑) [Jai+20]</formula><formula xml:id="formula_40">∑︂ 𝑖=1 ∑︂ 𝑥∈𝑋 𝑖 ∥︁ ∥︁ ∥︁𝑥 - 1 |𝑋 𝑖 | ∑︂ 𝑦∈𝑋 𝑖 𝑦 ∥︁ ∥︁ ∥︁ 2 2 = 1 2 𝑘 ∑︂ 𝑖=1 1 |𝑋 𝑖 | ∑︂ 𝑥,𝑦∈𝑋 𝑖 ∥𝑥 -𝑦 ∥ 2 2 .</formula><p>In order to prove lemma 1.6 we will need the following lemma. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>□</head><p>Proof of lemma 1.6. We will first prove an identity for each partition, so let 𝑋 𝑖 ⊆ 𝑋 ⊂ R 𝑑 be any partition of the dataset 𝑋 and define 𝜇 𝑖 ≔ 1</p><formula xml:id="formula_41">|𝑋 𝑖 | ∑︁ 𝑥∈𝑋 𝑖 𝑥 as the mean of 𝑋 𝑖 . 1 2|𝑋 𝑖 | ∑︂ 𝑥,𝑦∈𝑋 𝑖 ∥𝑥 -𝑦 ∥ 2 2 = 1 2|𝑋 𝑖 | ∑︂ 𝑥,𝑦∈𝑋 𝑖 ∥(𝑥 -𝜇 𝑖 ) -(𝑦 -𝜇 𝑖 )∥ 2 2 = 1 2|𝑋 𝑖 | ∑︂ 𝑥,𝑦∈𝑋 𝑖 (︁ ∥𝑥 -𝜇 𝑖 ∥ 2 2 + ∥ 𝑦 -𝜇 𝑖 ∥ 2 2 -2⟨𝑥 -𝜇 𝑖 , 𝑦 -𝜇 𝑖 ⟩ )︁ = ∑︂ 𝑥∈𝑋 𝑖 ∥𝑥 -𝜇 𝑖 ∥ 2 2 - 1 2|𝑋 𝑖 | ∑︂ 𝑥,𝑦∈𝑋 𝑖 2⟨𝑥 -𝜇 𝑖 , 𝑦 -𝜇 𝑖 ⟩ = ∑︂ 𝑥∈𝑋 𝑖 ∥𝑥 -𝜇 𝑖 ∥ 2 2 ,</formula><p>where the last equality holds by lemma 2.1. We now substitute each term in the sum in lemma 1.6 using the just derived identity:</p><formula xml:id="formula_42">𝑘 ∑︂ 𝑖=1 ∑︂ 𝑥∈𝑋 𝑖 ∥︁ ∥︁ ∥︁𝑥 - 1 |𝑋 𝑖 | ∑︂ 𝑦∈𝑋 𝑖 𝑦 ∥︁ ∥︁ ∥︁ 2 2 = 1 2 𝑘 ∑︂ 𝑖=1 1 |𝑋 𝑖 | ∑︂ 𝑥,𝑦∈𝑋 𝑖 ∥𝑥 -𝑦 ∥ 2 2 .</formula><p>□</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Super Sparse DKS</head><p>The tight bounds on the performance of feature hashing presented in theorem 1.8 can be extended to tight performance bounds for the DKS construction. Recall that the DKS construction, parameterised by a so-called column sparsity 𝑠 ∈ N 1 , works by first mapping a vector 𝑥 ∈ R 𝑑 to an 𝑥 ′ ∈ R 𝑠𝑑 by duplicating each entry in 𝑥 𝑠 times and then scaling with 1/ √ 𝑠, before applying feature hashing to 𝑥 ′ , as 𝑥 ′ has a more palatable ℓ ∞ /ℓ 2 ratio compared to 𝑥. The setting for the extended result is that if we wish to use the DKS construction but we only need to handle vectors with a small ∥𝑥 ∥ ∞ /∥𝑥 ∥ 2 ratio, we can choose a column sparsity smaller than the usual Θ(𝜀 -1 log 1 𝛿 log 𝑚 𝛿 ) and still get the Johnson-Lindenstrauss guarantees. This is formalised in corollary 1.9. The two pillars of theorem 1.8 we use in the proof of corollary 1.9 is that the feature hashing tradeoff is tight and that we can force the DKS construction to create hard instances for feature hashing. </p><p>The upper bound part of the Θ in corollary 1.9 shows how sparse we can choose the DKS construction to be and still get Johnson-Lindenstrauss guarantees for the data we care about, while the lower bound shows that if we choose a sparsity below this bound, there exists vectors who get distorted too much too often despite having an ℓ ∞ /ℓ 2 ratio of at most 𝜈 DKS .</p><p>Proof of corollary 1.9. Let us first prove the upper bound:</p><formula xml:id="formula_44">𝑠 DKS = 𝒪 (︁ 𝜈 2 DKS 𝜈 2 FH )︁ . Let 𝑠 ≔ Θ (︁ 𝜈 2 DKS 𝜈 2 FH )︁</formula><p>∈ [𝑚] be the column sparsity, and let 𝑥 ∈ R 𝑑 be a unit vector with ∥𝑥∥ ∞ ≤ 𝜈 DKS . The goal is now to show that a DKS construction with sparsity 𝑠 can embed 𝑥 while preserving its norm within 1 ± 𝜀 with probability at least 1 -𝛿 (as defined in lemma 1.2).</p><p>Let 𝑥 ′ ∈ R 𝑠𝑑 be the unit vector constructed by duplicating each entry in 𝑥 𝑠 times and scaling </p><formula xml:id="formula_45">|︁ |︁ ≤ 𝜀 ]︂ = Pr 𝑔∼FH [︂ |︁ |︁ ∥ 𝑔(𝑥 ′ )∥ 2 2 -1 |︁ |︁ ≤ 𝜀 ]︂ &lt; 1 -𝛿,</formula><p>where the inequality is implied by eq. ( <ref type="formula">14</ref>) and theorem 1.8, and the fact that 𝑥 ′ has the shape of an asymptotically worst case instance for feature hashing.</p><p>□</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">LWTJL Fails for Too Sparse Vectors</head><p>Proposition 1.11. For any seed matrix define LWT as the LWTJL distribution seeded with that matrix.</p><p>Then for all 𝛿 ∈ (0, 1), there exists a vector 𝑥 ∈ C 𝑑 (or 𝑥 ∈ R 𝑑 , if the seed matrix is a real matrix) satisfying ∥𝑥 ∥ ∞ /∥𝑥 ∥ 2 = Θ(log -1/2 1 𝛿 ) such that</p><formula xml:id="formula_46">Pr 𝑓 ∼LWT [ 𝑓 (𝑥) = 0] &gt; 𝛿.</formula><p>Proof. The main idea is to construct the vector 𝑥 out of segments that are orthogonal to the seed matrix with some probability, and then show that 𝑥 is orthogonal to all copies of the seed matrix simultaneously with probability larger than 𝛿.</p><p>Let 𝑟, 𝑐 ∈ N 1 be constants and 𝐴 1 ∈ C 𝑟×𝑐 be a seed matrix. Let 𝑑 be the source dimension of the LWTJL construction, 𝐷 ∈ {-1, 0, 1} 𝑑×𝑑 be the random diagonal matrix with i.i.d. Rademachers, 𝑙 ∈ N 1 such that 𝑐 𝑙 = 𝑑, and 𝐴 𝑙 ∈ C 𝑟 𝑙 ×𝑐 𝑙 be the the LWT, i.e. 𝐴 𝑙 ≔ 𝐴 ⊗𝑙 1 . Since 𝑟 &lt; 𝑐 there exists a nontrivial vector 𝑧 ∈ C 𝑐 \ {0} that is orthogonal to all 𝑟 rows of 𝐴 1 and ∥𝑧 ∥ ∞ = Θ(1). Now define 𝑥 ∈ C 𝑑 as 𝑘 ∈ N 1 copies of 𝑧 followed by a padding of 0s, where 𝑘 = ⌊ 1 𝑐 lg 1 𝛿 -1⌋. Note that if the seed matrix is real, we can choose 𝑧 and therefore 𝑥 to be real as well.</p><p>The first thing to note is that ∥𝑥∥ 0 ≤ 𝑐 𝑘 &lt; lg 1 𝛿 , which implies that Pr 𝐷 [𝐷𝑥 = 𝑥] = 2 -∥𝑥∥ 0 &gt; 𝛿.</p><p>Secondly, due to the Kronecker structure of 𝐴 𝑙 and the fact that 𝑧 is orthogonal to the rows of 𝐴 1 , we have 𝐴𝑥 = 0.</p><p>Taken together, we can conclude </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>□</head><p>The following corollary is just a restatement of proposition 1.11 in terms of lemma 1.2, and the proof therefore follows immediately from proposition 1.11.</p><p>Corollary 2.2. For every 𝑚, 𝑑, ∈ N 1 , and 𝛿, 𝜀 ∈ (0, 1), and LWTJL distribution LWT over 𝑓 : K 𝑑 → K 𝑚 , where K ∈ {R, C} and 𝑚 &lt; 𝑑 there exists a vector 𝑥 ∈ K 𝑑 with ∥𝑥 ∥ ∞ /∥𝑥∥ 2 = Θ(log -1/2 1 𝛿 ) such that</p><formula xml:id="formula_47">Pr 𝑓 ∼LWT [︂ |︁ |︁ ∥ 𝑓 (𝑥)∥ 2 2 -∥𝑥∥ 2 2 |︁ |︁ ≤ 𝜀∥𝑥 ∥ 2 2 ]︂ &lt; 1 -𝛿.</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>⌈︁ 9 (</head><label>9</label><figDesc>𝜀 2 -2𝜀 3 /3) -1 ln |𝑋 | ⌉︁ + 1 suffices for the same JLD if 𝑚 &lt; √︁ |𝑋 |. This bound was further improved in [FM90] by removing the 𝑚 &lt; √︁ |𝑋 | restriction and lowering the bound to 𝑚 = ⌈︁ 8(𝜀 2 -2𝜀 3 /3) -1 ln |𝑋 |</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Lemma 2. 1 .</head><label>1</label><figDesc>Let 𝑑 ∈ N 1 and 𝑋 ⊂ R 𝑑 and define 𝜇 ≔ 1 |𝑋 | ∑︁ 𝑥∈𝑋 𝑥 as the mean of 𝑋, then it holds that ∑︂ 𝑥,𝑦∈𝑋 ⟨𝑥 -𝜇, 𝑦 -𝜇⟩ = 0. Proof of lemma 2.1. The lemma follows from the definition of 𝜇 and the linearity of the real inner product. ∑︂ 𝑥,𝑦∈𝑋 ⟨𝑥 -𝜇, 𝑦 -𝜇⟩ = ∑︂ 𝑥,𝑦∈𝑋 (︁ ⟨𝑥, 𝑦⟩ -⟨𝑥, 𝜇⟩ -⟨𝑦, 𝜇⟩ + ⟨𝜇, 𝜇⟩</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Corollary 1. 9 .(</head><label>9</label><figDesc>Let 𝜈 DKS ∈ [1/ √ 𝑑, 1] denote the largest ℓ ∞ /ℓ 2 ratio required, 𝜈 FH denote the ℓ ∞ /ℓ 2 constraint for feature hashing as defined in theorem 1.8, and 𝑠 DKS ∈ [𝑚] as the minimum column sparsity such that the DKS construction with that sparsity is a JLD for the subset of vectors 𝑥 ∈ R 𝑑 that satisfy ∥𝑥∥ ∞ /∥𝑥∥ 2 ≤ 𝜈 DKS . Then𝑠 DKS = Θ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>√</head><figDesc>𝑠 as in the DKS construction. We now have∥𝑥 ′ ∥ ∞ ≤ 𝜈 DKS √ 𝑠 = Θ(𝜈 FH ). (13)Let DKS denote the JLD from the DKS construction with column sparsity 𝑠, and let FH denote the feature hashing JLD. Then we can conclude Pr 𝛿, where the inequality is implied by eq. (13) and theorem 1.8. Now let us prove the lower bound: 𝑠 DKS = Ω 𝑥 = (𝜈 DKS , . . . , 𝜈 DKS , 0, . . . , 0) ᵀ ∈ R 𝑑 be a unit vector. We now wish to show that a DKS construction with sparsity 𝑠 will preserve the norm of 𝑥 to within 1 ± 𝜀 with probability strictly less than 1 -𝛿. As before, define 𝑥 ′ ∈ R 𝑠𝑑 as the unit vector the DKS construction computes when duplicating every entry in 𝑥 𝑠 times and scaling with 1/ √ 𝑠. This gives∥𝑥 ′ ∥ ∞ = 𝜈 DKS √ 𝑠 = 𝜔(𝜈 FH ).(14)Finally, let DKS denote the JLD from the DKS construction with column sparsity 𝑠, and let FH denote the feature hashing JLD. Then we can conclude Pr</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>[</head><figDesc>𝑓 (𝑥) = 0] ≥ Pr 𝐷 [𝐴 𝑙 𝐷𝑥 = 0] ≥ Pr 𝐷 [𝐷𝑥 = 𝑥] &gt; 𝛿. Now we just need to show that ∥𝑥∥ ∞ /∥𝑥 ∥ 2 = Θ(log -1/2 1 𝛿 ).Since 𝑐 is a constant and 𝑥 is consists of 𝑘 = Θ(log 1 𝛿 ) copies of 𝑧 followed by zeroes,∥𝑥∥ ∞ = ∥𝑧 ∥ ∞ = Θ(1),∥𝑧∥ 2 = Θ(1),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>The tapestry tabularised.Let 𝑘, 𝑑 ∈ N 1 and 𝑋 𝑖 ⊂ R 𝑑 for 𝑖 ∈ {1, . . . , 𝑘}, then</figDesc><table><row><cell>2 Deferred Proofs</cell></row><row><cell>2.1 𝑘-Means Cost is Pairwise Distances</cell></row><row><cell>Let us first repeat the lemma to remind ourselves what we need to show.</cell></row><row><cell>Lemma 1.6. 𝑘</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0"><p>Here it is assumed that the mean of our vectors is 0, otherwise the mean vector of our vectors should be subtracted from each of the rows of 𝑋.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1"><p>Or rather in 1982 as that was when a particular "Conference in Modern Analysis and Probability" was held at Yale University, but the proceedings were published in 1984.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_2"><p>We use[𝑘]  to denote the set {1, . . . , 𝑘}.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_3"><p>Here the approximation ratio is between any 𝑘-means algorithm running on the high dimensional original data and on the low dimensional projected data.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_4"><p>Here we assume that a word is large enough to hold a sufficient approximation of any real number we use and to hold a number from the stream, i.e. if 𝑤 denotes the number of bits in a word then 𝑤 = Ω(log 𝑑 + log 𝑀).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_5"><p>See e.g.<ref type="bibr" target="#b193">[TZ12]</ref> for other families of 𝑘-wise independent hash functions.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_6"><p>We will usually omit the normalisation or scaling factor (the (𝑑/𝑚) 1/2 for this JLD) when discussing JLDs as they are textually noisy, not that interesting, and independent of randomness and input data.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16" xml:id="foot_7"><p>The Rademacher distribution is the uniform distribution on {-1, 1}.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="21" xml:id="foot_8"><p>Here we ignore any overhead that switching to a sparse matrix representation would introduce.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="25" xml:id="foot_9"><p>Here a lower bound refers to a lower bound on 𝜈 as a function of 𝑚, 𝜀, 𝛿, and 𝑠.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="29" xml:id="foot_10"><p>Here we ignore the 𝑟 &lt; 𝑐 requirement of seed matrices.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="30" xml:id="foot_11"><p>The paper lets 𝑑 ′ and 𝑚 be random variables, but with the way the JLD is presented here a deterministic projection suffices, though it may affect constants hiding in the big-𝒪 expressions.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="31" xml:id="foot_12"><p>Recall that sin(𝜋/4) = 2 -1/2 and that similar results holds for cosine and for the other angles.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="32" xml:id="foot_13"><p>Note that the various Kac walk lengths are only shown to be sufficient, and so tighter analysis might shorten them and perhaps remove the cost of using simpler angles.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Approximate nearest neighbors and the fast Johnson-Lindenstrauss transform</title>
		<author>
			<persName><forename type="first">Nir</forename><surname>Ailon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Chazelle</surname></persName>
		</author>
		<idno type="DOI">10.1145/1132516.1132597</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th Symposium on Theory of Computing. STOC &apos;06</title>
		<meeting>the 38th Symposium on Theory of Computing. STOC &apos;06</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The fast Johnson-Lindenstrauss transform and approximate nearest neighbors</title>
		<author>
			<persName><forename type="first">Nir</forename><surname>Ailon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Chazelle</surname></persName>
		</author>
		<idno type="DOI">10.1137/060673096</idno>
	</analytic>
	<monogr>
		<title level="j">Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="302" to="322" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>Previously published as [AC06] (cit. on pp. 5, 12, 14, 15, 19, 21</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Database-friendly Random Projections</title>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Achlioptas</surname></persName>
		</author>
		<idno type="DOI">10.1145/375551.375608</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Symposium on Principles of Database Systems. PODS &apos;01</title>
		<meeting>the 20th Symposium on Principles of Database Systems. PODS &apos;01</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="12" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Achlioptas</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0022-0000(03)00025-4</idno>
	</analytic>
	<monogr>
		<title level="m">Database-friendly random projections: Johnson-Lindenstrauss with binary coins</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="11" to="14" />
		</imprint>
	</monogr>
	<note>Previously published as Ach01] (cit. on</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Memory and Computation Efficient PCA via Very Sparse Random Projections</title>
		<author>
			<persName><forename type="first">Farhad</forename><surname>Pourkamali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anaraki</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Shannon</forename><surname>Hughes</surname></persName>
		</author>
		<idno>PMLR) 2. PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning (ICML &apos;14)</title>
		<meeting>the 31st International Conference on Machine Learning (ICML &apos;14)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Nearest Neighbors in High-Dimensional Spaces</title>
		<author>
			<persName><forename type="first">Alexandr</forename><surname>Andoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Indyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Discrete and Computational Geometry. 3rd ed</title>
		<imprint>
			<publisher>CRC Press</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Optimal Compression of Approximate Inner Products and Dimension Reduction</title>
		<author>
			<persName><forename type="first">Noga</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bo'az</surname></persName>
		</author>
		<author>
			<persName><surname>Klartag</surname></persName>
		</author>
		<idno type="DOI">10.1109/FOCS.2017.65</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Symposium on Foundations of Computer Science. FOCS &apos;17</title>
		<meeting>the 58th Symposium on Foundations of Computer Science. FOCS &apos;17</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast dimension reduction using Rademacher series on dual BCH codes</title>
		<author>
			<persName><forename type="first">Nir</forename><surname>Ailon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edo</forename><surname>Liberty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th Symposium on Discrete Algorithms. SODA &apos;08</title>
		<meeting>the 19th Symposium on Discrete Algorithms. SODA &apos;08</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast Dimension Reduction Using Rademacher Series on Dual BCH Codes</title>
		<author>
			<persName><forename type="first">Nir</forename><surname>Ailon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edo</forename><surname>Liberty</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00454-008-9110-x</idno>
	</analytic>
	<monogr>
		<title level="j">Discrete &amp; Computational Geometry</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2009">2009. 18, 19, 21</date>
		</imprint>
	</monogr>
	<note>Previously published as [AL08</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An Almost Optimal Unrestricted Fast Johnson-Lindenstrauss Transform</title>
		<author>
			<persName><forename type="first">Nir</forename><surname>Ailon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edo</forename><surname>Liberty</surname></persName>
		</author>
		<idno type="DOI">10.1137/1.9781611973082.17</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd Symposium on Discrete Algorithms. SODA &apos;11. SIAM</title>
		<meeting>the 22nd Symposium on Discrete Algorithms. SODA &apos;11. SIAM</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An Almost Optimal Unrestricted Fast Johnson-Lindenstrauss Transform</title>
		<author>
			<persName><forename type="first">Nir</forename><surname>Ailon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edo</forename><surname>Liberty</surname></persName>
		</author>
		<idno type="DOI">10.1145/2483699.2483701</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions on Algorithms</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>Previously published as [AL11] (cit. on</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Statistical mechanics of complex neural systems and high dimensional data</title>
		<author>
			<persName><forename type="first">Madhu</forename><surname>Advani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhaneil</forename><surname>Lahiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<idno type="DOI">10.1088/1742-5468/2013/03/p03014</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Mechanics: Theory and Experiment</title>
		<imprint>
			<biblScope unit="issue">03</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sparse signconsistent Johnson-Lindenstrauss matrices: Compression with neuroscience-based constraints</title>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rati</forename><surname>Gelashvili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Micali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nir</forename><surname>Shavit</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1419100111</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2014">2014</date>
			<publisher>PNAS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Tracking Join and Self-Join Sizes in Limited Storage</title>
		<author>
			<persName><forename type="first">Noga</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><forename type="middle">B</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="DOI">10.1006/jcss.2001.1813</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>Previously published as [Alo+99] (cit</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">NP-hardness of Euclidean sum-of-squares clustering</title>
		<author>
			<persName><forename type="first">Amit</forename><surname>Daniel Aloise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preyas</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><surname>Popat</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10994-009-5103-0</idno>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Tracking Join and Self-Join Sizes in Limited Storage</title>
		<author>
			<persName><forename type="first">Noga</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><forename type="middle">B</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="DOI">10.1145/303976.303978</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Symposium on Principles of Database Systems. PODS &apos;99</title>
		<meeting>the 18th Symposium on Principles of Database Systems. PODS &apos;99</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The Space Complexity of Approximating the Frequency Moments</title>
		<author>
			<persName><forename type="first">Yossi</forename><surname>Noga Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName><surname>Szegedy</surname></persName>
		</author>
		<idno type="DOI">10.1145/237814.237823</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Symposium on Theory of Computing. STOC &apos;96</title>
		<meeting>the 28th Symposium on Theory of Computing. STOC &apos;96</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The Space Complexity of Approximating the Frequency Moments</title>
		<author>
			<persName><forename type="first">Yossi</forename><surname>Noga Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName><surname>Szegedy</surname></persName>
		</author>
		<idno type="DOI">10.1006/jcss.1997.1545</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Approximate Document Outlier Detection Using Random Spectral Projection</title>
		<author>
			<persName><forename type="first">Mazin</forename><surname>Aouf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Park</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-35101-3_49</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th Australasian Joint Conference on Artificial Intelligence (AI &apos;12)</title>
		<title level="s">Lecture Notes in Computer Science (LNCS</title>
		<meeting>the 25th Australasian Joint Conference on Artificial Intelligence (AI &apos;12)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">7691</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">An Analysis of Random Projections in Cancelable Biometrics</title>
		<author>
			<persName><forename type="first">Devansh</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ifeoma</forename><surname>Nwogu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Venu</forename><surname>Govindaraju</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1401.4489[cs.CV</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="4" to="7" />
		</imprint>
	</monogr>
	<note>In: arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An algorithmic theory of learning: Robust concepts and random projection</title>
		<author>
			<persName><forename type="first">Rosa</forename><forename type="middle">I</forename><surname>Arriaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santosh</forename><forename type="middle">S</forename><surname>Vempala</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10994-006-6265-7</idno>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="161" to="182" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>Previously published as [AV99] (cit. on pp. 4, 11, 12, 21</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An Algorithmic Theory of Learning: Robust Concepts and Random Projection</title>
		<author>
			<persName><forename type="first">Rosa</forename><forename type="middle">I</forename><surname>Arriaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santosh</forename><forename type="middle">S</forename><surname>Vempala</surname></persName>
		</author>
		<idno type="DOI">10.1109/SFFCS.1999.814637</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Symposium on Foundations of Computer Science. FOCS &apos;99. IEEE</title>
		<meeting>the 40th Symposium on Foundations of Computer Science. FOCS &apos;99. IEEE</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Efficient Dimensionality Reduction for Canonical Correlation Analysis</title>
		<author>
			<persName><forename type="first">Christos</forename><surname>Haim Avron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sivan</forename><surname>Boutsidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasios</forename><surname>Toledo</surname></persName>
		</author>
		<author>
			<persName><surname>Zouzias</surname></persName>
		</author>
		<idno type="DOI">10.1137/130919222</idno>
	</analytic>
	<monogr>
		<title level="m">S111-S131</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Toeplitz-Structured Compressed Sensing Matrices</title>
		<author>
			<persName><forename type="first">Waheed</forename><surname>Uz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaman</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jarvis</forename><forename type="middle">D</forename><surname>Haupt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gil</forename><forename type="middle">M</forename><surname>Raz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">D</forename><surname>Nowak</surname></persName>
		</author>
		<idno type="DOI">10.1109/SSP.2007.4301266</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Workshop on Statistical Signal Processing. SSP &apos;07</title>
		<meeting>the 14th Workshop on Statistical Signal Processing. SSP &apos;07</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Geometry of random Toeplitz-block sensing matrices: bounds and implications for sparse signal processing</title>
		<author>
			<persName><forename type="first">Waheed</forename><surname>Uz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaman</forename><surname>Bajwa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Com</title>
		<meeting>the Com</meeting>
		<imprint/>
	</monogr>
	<note>pressed Sensing track of the 7th Conference on Defense, Security, and Sensing (DSS &apos;12</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<idno type="DOI">10.1117/12.919475</idno>
		<title level="m">Proceedings of SPIE. SPIE</title>
		<meeting>SPIE. SPIE</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">8365</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A Simple Proof of the Restricted Isometry Property for Random Matrices</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Baraniuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Davenport</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><surname>Devore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Wakin</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00365-007-9003-x</idno>
	</analytic>
	<monogr>
		<title level="j">Constructive Approximation</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Toward a Unified Theory of Sparse Dimensionality Reduction in Euclidean Space</title>
		<author>
			<persName><forename type="first">Jean</forename><surname>Bourgain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sjoerd</forename><surname>Dirksen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jelani</forename><surname>Nelson</surname></persName>
		</author>
		<idno type="DOI">10.1145/2746539.2746541</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 47th Symposium on Theory of Computing. STOC &apos;15</title>
		<meeting>the 47th Symposium on Theory of Computing. STOC &apos;15</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Toward a unified theory of sparse dimensionality reduction in Euclidean space</title>
		<author>
			<persName><forename type="first">Jean</forename><surname>Bourgain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sjoerd</forename><surname>Dirksen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jelani</forename><surname>Nelson</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00039-015-0332-9</idno>
	</analytic>
	<monogr>
		<title level="j">Geometric and Functional Analysis (GAFA)</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Previously published as [BDN15a</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Oblivious dimension reduction for 𝑘-means: beyond subspaces and the Johnson-Lindenstrauss lemma</title>
		<author>
			<persName><forename type="first">Luca</forename><surname>Becchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Bury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Cohen-Addad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrizio</forename><surname>Grandoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Schwiegelshohn</surname></persName>
		</author>
		<idno type="DOI">10.1145/3313276.3316318</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Symposium on Theory of Computing. STOC &apos;19</title>
		<meeting>the 51st Symposium on Theory of Computing. STOC &apos;19</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Optimal Bounds for Johnson-Lindenstrauss Transformations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuhong</forename><surname>Burr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fiona</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><surname>Knoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Optimal Fast Johnson-Lindenstrauss Embeddings for Large Data Sets</title>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Bamberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Krahmer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.01774[cs.DS</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note>In: arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The Johnson-Lindenstrauss Transform Itself Preserves Differential Privacy</title>
		<author>
			<persName><forename type="first">Jeremiah</forename><surname>Blocki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avrim</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anupam</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Or</forename><surname>Sheffet</surname></persName>
		</author>
		<idno type="DOI">10.1109/FOCS.2012.67</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Symposium on Foundations of Computer Science. FOCS &apos;12</title>
		<meeting>the 53rd Symposium on Foundations of Computer Science. FOCS &apos;12</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Random Projection in Dimensionality Reduction: Applications to Image and Text Data</title>
		<author>
			<persName><forename type="first">Ella</forename><surname>Bingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heikki</forename><surname>Mannila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Knowledge Discovery and Data Mining. KDD &apos;01</title>
		<meeting>the 7th International Conference on Knowledge Discovery and Data Mining. KDD &apos;01</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Origins and Extensions of the 𝑘-Means Algorithm in Cluster Analysis</title>
		<author>
			<persName><forename type="first">Hans-Hermann</forename><surname>Bock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Journal for History of Probability and Statistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Rademacher Chaos, Random Eulerian Graphs and The Sparse Johnson-Lindenstrauss Transform</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Braverman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafail</forename><surname>Ostrovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Rabani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1011.2590[cs.DS</idno>
	</analytic>
	<monogr>
		<title level="m">Isaac Newton Institute for Mathematical Sciences</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note>In: arXiv e-prints Presented at the Embedding worshop DANW01</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Randomized Dimensionality Reduction for 𝑘-Means Clustering</title>
		<author>
			<persName><forename type="first">Christos</forename><surname>Boutsidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasios</forename><surname>Zouzias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petros</forename><surname>Drineas</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIT.2014.2375327</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="1045" to="1062" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>Previously published as [BZD10] (cit. on pp. 6, 9</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">On the Reconstruction Accuracy of Multi-Coil MRI with Orthogonal Projections</title>
		<author>
			<persName><forename type="first">Anna</forename><surname>Breger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Ramos Llorden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gonzalo</forename><surname>Vegas Sanchez-Ferrero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Scott Hoge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Ehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl-Fredrik</forename><surname>Westin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13422</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note>In: arXiv e-prints physics.med-ph</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">On Orthogonal Projections for Dimension Reduction and Applications in Augmented Target Loss Functions for Learning Problems</title>
		<author>
			<persName><forename type="first">Anna</forename><surname>Breger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ignacio</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavol</forename><surname>Orlando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monika</forename><surname>Harár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sophie</forename><surname>Dörfler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Klimscha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bianca</forename><forename type="middle">S</forename><surname>Grechenig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ursula</forename><surname>Gerendas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
		<author>
			<persName><surname>Ehler</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10851-019-00902-2</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Imaging and Vision</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Random Projections for 𝑘-means Clustering</title>
		<author>
			<persName><forename type="first">Christos</forename><surname>Boutsidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasios</forename><surname>Zouzias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petros</forename><surname>Drineas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 23. NIPS &apos;10</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Random projections: Data perturbation for classification problems</title>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">I</forename><surname>Cannings</surname></persName>
		</author>
		<idno type="DOI">10.1002/wics.1499</idno>
	</analytic>
	<monogr>
		<title level="j">WIREs Computational Statistics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Random convergence of olfactory inputs in the Drosophila mushroom body</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Sophie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vanessa</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Ruta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Abbott</surname></persName>
		</author>
		<author>
			<persName><surname>Axel</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature12063</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">497</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Finding Frequent Items in Data Streams</title>
		<author>
			<persName><forename type="first">Moses</forename><surname>Charikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Farach-Colton</surname></persName>
		</author>
		<idno type="DOI">10.1007/3-540-45465-9_59</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Colloquium on Automata, Languages and Programming</title>
		<title level="s">Lecture Notes in Computer Science (LNCS</title>
		<meeting>the 29th International Colloquium on Automata, Languages and Programming</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">2380</biblScope>
			<biblScope unit="page">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Finding Frequent Items in Data Streams</title>
		<author>
			<persName><forename type="first">Moses</forename><surname>Charikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Farach-Colton</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0304-3975(03)00400-6</idno>
	</analytic>
	<monogr>
		<title level="j">Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">312</biblScope>
			<biblScope unit="page" from="3" to="15" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>Previously published as [CCF02] (cit. on pp. 10, 11, 13</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Sketching Streams Through the Net: Distributed Approximate Query Tracking</title>
		<author>
			<persName><forename type="first">Graham</forename><surname>Cormode</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Minos</surname></persName>
		</author>
		<author>
			<persName><surname>Garofalakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Very Large Data Bases. VLDB &apos;05</title>
		<meeting>the 31st International Conference on Very Large Data Bases. VLDB &apos;05</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="13" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Compressed Learning: Universal Sparse Dimensionality Reduction and Learning in the Measurement Domain</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Calderbank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sina</forename><surname>Jafarpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Schapire</surname></persName>
		</author>
		<ptr target="https://core.ac.uk/display/21147568" />
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>Manuscript</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Input Sparsity Time Low-rank Approximation via Ridge Leverage Score Sampling</title>
		<author>
			<persName><forename type="first">B</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cameron</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Musco</surname></persName>
		</author>
		<author>
			<persName><surname>Musco</surname></persName>
		</author>
		<idno type="DOI">10.1137/1.9781611974782.115</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Symposium on Discrete Algorithms. SODA &apos;17</title>
		<meeting>the 28th Symposium on Discrete Algorithms. SODA &apos;17</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">38</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Dimensionality Reduction for 𝑘-Means Clustering and Low Rank Approximation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cameron</forename><surname>Elder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Musco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madalina</forename><surname>Musco</surname></persName>
		</author>
		<author>
			<persName><surname>Persu</surname></persName>
		</author>
		<idno type="DOI">10.1145/2746539.2746569</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 47th Symposium on Theory of Computing. STOC &apos;15</title>
		<meeting>the 47th Symposium on Theory of Computing. STOC &apos;15</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="163" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Robust uncertainty principles: exact signal reconstruction from highly incomplete frequency information</title>
		<author>
			<persName><forename type="first">Emmanuel</forename><forename type="middle">J</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><forename type="middle">K</forename><surname>Romberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terence</forename><surname>Tao</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIT.2005.862083</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Random Projection Ensemble Classification</title>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">I</forename><surname>Cannings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">J</forename><surname>Samworth</surname></persName>
		</author>
		<idno type="DOI">10.1111/rssb.12228</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Decoding by linear programming</title>
		<author>
			<persName><forename type="first">Emmanuel</forename><forename type="middle">J</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terence</forename><surname>Tao</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIT.2005.858979</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Near-Optimal Signal Recovery From Random Projections: Universal Encoding Strategies?</title>
		<author>
			<persName><forename type="first">Emmanuel</forename><forename type="middle">J</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terence</forename><surname>Tao</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIT.2006.885507</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Low rank approximation and regression in input sparsity time</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kenneth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">P</forename><surname>Clarkson</surname></persName>
		</author>
		<author>
			<persName><surname>Woodruff</surname></persName>
		</author>
		<idno type="DOI">10.1145/2488608.2488620</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Symposium on Theory of Computing. STOC &apos;13</title>
		<meeting>the 45th Symposium on Theory of Computing. STOC &apos;13</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Low-Rank Approximation and Regression in Input Sparsity Time</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kenneth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">P</forename><surname>Clarkson</surname></persName>
		</author>
		<author>
			<persName><surname>Woodruff</surname></persName>
		</author>
		<idno type="DOI">10.1145/3019134</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page">45</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Previously published as [CW13] (cit. on p. 6</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Experiments with Random Projection</title>
		<author>
			<persName><forename type="first">Sanjoy</forename><surname>Dasgupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference on Uncertainty in Artificial Intelligence. UAI &apos;00</title>
		<meeting>the 16th Conference on Uncertainty in Artificial Intelligence. UAI &apos;00</meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">The hardness of 𝑘-means clustering</title>
		<author>
			<persName><forename type="first">Sanjoy</forename><surname>Dasgupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CS2008-0916. University of California San Diego</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning Mixtures of Gaussians</title>
		<author>
			<persName><forename type="first">Sanjoy</forename><surname>Dasgupta</surname></persName>
		</author>
		<idno type="DOI">10.1109/SFFCS.1999.814639</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Symposium on Foundations of Computer Science. FOCS &apos;99. IEEE</title>
		<meeting>the 40th Symposium on Foundations of Computer Science. FOCS &apos;99. IEEE</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Locality-Sensitive Hashing Scheme Based on p-Stable Distributions</title>
		<author>
			<persName><forename type="first">Mayur</forename><surname>Datar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicole</forename><surname>Immorlica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vahab</forename><surname>Seyed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirrokni</forename></persName>
		</author>
		<idno type="DOI">10.1145/997817.997857</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Symposium on Computational Geometry. SoCG &apos;04</title>
		<meeting>the 20th Symposium on Computational Geometry. SoCG &apos;04</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Reducing High-Dimensional Data by Principal Component Analysis vs. Random Projection for Nearest Neighbor Classification</title>
		<author>
			<persName><forename type="first">Sampath</forename><surname>Deegalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrik</forename><surname>Boström</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICMLA.2006.43</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Machine Learning and Applications. ICMLA &apos;06</title>
		<meeting>the 5th International Conference on Machine Learning and Applications. ICMLA &apos;06</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Fast linear algebra is stable</title>
		<author>
			<persName><forename type="first">James</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioana</forename><surname>Dumitriu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Holtz</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00211-007-0114-x</idno>
	</analytic>
	<monogr>
		<title level="j">Numerische Mathematik</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Practical Skew Handling in Parallel Joins</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Dewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">F</forename><surname>Naughton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donovan</forename><forename type="middle">A</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seshadri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Very Large Data Bases. VLDB &apos;92</title>
		<meeting>the 18th International Conference on Very Large Data Bases. VLDB &apos;92</meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">An Elementary Proof of a Theorem of Johnson and Lindenstrauss</title>
		<author>
			<persName><forename type="first">Sanjoy</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anupam</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="DOI">10.1002/rsa.10073</idno>
	</analytic>
	<monogr>
		<title level="j">Random Structures &amp; Algorithms</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<author>
			<persName><forename type="first">Sjoerd</forename><surname>Dirksen</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10208-015-9280-x</idno>
	</analytic>
	<monogr>
		<title level="m">Dimensionality Reduction with Subgaussian Matrices: A Unified Theory</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">One-bit compressed sensing with partial Gaussian circulant matrices</title>
		<author>
			<persName><forename type="first">Sjoerd</forename><surname>Dirksen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans</forename><forename type="middle">Christian</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Rauhut</surname></persName>
		</author>
		<idno type="DOI">10.1093/imaiai/iaz017</idno>
	</analytic>
	<monogr>
		<title level="j">Information and Inference: A Journal of the IMA</title>
		<imprint>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Compressed Fisher Linear Discriminant Analysis: Classification of Randomly Projected Data</title>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">J</forename><surname>Durrant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ata</forename><surname>Kabán</surname></persName>
		</author>
		<idno type="DOI">10.1145/1835804.1835945</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on Knowledge Discovery and Data Mining. KDD &apos;10</title>
		<meeting>the 16th International Conference on Knowledge Discovery and Data Mining. KDD &apos;10</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Random Projections as Regularizers: Learning a Linear Discriminant Ensemble from Fewer Observations than Dimensions</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Durrant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ata</forename><surname>Kabán</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Asian Conference on Machine Learning (ACML &apos;13)</title>
		<meeting>the 5th Asian Conference on Machine Learning (ACML &apos;13)</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research (PMLR)</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">A Sparse Johnson-Lindenstrauss Transform</title>
		<author>
			<persName><forename type="first">Anirban</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamás</forename><surname>Sarlós</surname></persName>
		</author>
		<idno type="DOI">10.1145/1806689.1806737</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Symposium on Theory of Computing. STOC &apos;10</title>
		<meeting>the 42nd Symposium on Theory of Computing. STOC &apos;10</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="341" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Practical Hash Functions for Similarity Estimation and Dimensionality Reduction</title>
		<author>
			<persName><forename type="first">Søren</forename><surname>Dahlgaard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tejs</forename><surname>Knudsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikkel</forename><surname>Thorup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30. NIPS &apos;17</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Fast and efficient dimensionality reduction using Structurally Random Matrices</title>
		<author>
			<persName><forename type="first">T</forename><surname>Thong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nam</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trac</forename><forename type="middle">D</forename><surname>Hoai Nguyen</surname></persName>
		</author>
		<author>
			<persName><surname>Tran</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.2009.4959960</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Acoustics, Speech, and Signal Processing. ICASSP &apos;09</title>
		<meeting>the 34th International Conference on Acoustics, Speech, and Signal Processing. ICASSP &apos;09</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">For Most Large Underdetermined Systems of Equations, the Minimal ℓ 1 -norm Near-Solution Approximates the Sparsest Near-Solution</title>
		<author>
			<persName><forename type="first">L</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><surname>Donoho</surname></persName>
		</author>
		<idno type="DOI">10.1002/cpa.20131</idno>
	</analytic>
	<monogr>
		<title level="j">Communications on Pure and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Finding Local Anomalies in Very High Dimensional Space</title>
		<author>
			<persName><forename type="first">Timothy</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">E</forename><surname>Houle</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICDM.2010.151</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Data Mining. ICDM &apos;03</title>
		<meeting>the 10th International Conference on Data Mining. ICDM &apos;03</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Random Projection for High Dimensional Data Clustering: A Cluster Ensemble Approach</title>
		<author>
			<persName><forename type="first">Xiaoli</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fern</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Carla</forename><forename type="middle">E</forename><surname>Brodley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Machine Learning. ICML &apos;03</title>
		<meeting>the 20th International Conference on Machine Learning. ICML &apos;03</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="4" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Fully Understanding the Hashing Trick</title>
		<author>
			<persName><forename type="first">Casper</forename><surname>Benjamin Freksen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lior</forename><surname>Kamma</surname></persName>
		</author>
		<author>
			<persName><surname>Kasper Green Larsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31. NeurIPS &apos;18</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">On Using Toeplitz and Circulant Matrices for Johnson-Lindenstrauss Transforms</title>
		<author>
			<persName><forename type="first">Casper</forename><surname>Benjamin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Freksen</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Kasper</forename><surname>Green Larsen</surname></persName>
		</author>
		<idno type="DOI">10.4230/LIPIcs.ISAAC.2017.32</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Symposium on Algorithms and Computation (ISAAC &apos;17)</title>
		<meeting>the 28th International Symposium on Algorithms and Computation (ISAAC &apos;17)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
	<note>Leibniz International Proceedings in Informatics (LIPIcs). Schloss Dagstuhl</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">On Using Toeplitz and Circulant Matrices for Johnson-Lindenstrauss Transforms</title>
		<author>
			<persName><forename type="first">Casper</forename><surname>Benjamin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Freksen</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Kasper</forename><surname>Green Larsen</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00453-019-00644-y</idno>
	</analytic>
	<monogr>
		<title level="j">Algorithmica</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Experiments with Random Projections for Machine Learning</title>
		<author>
			<persName><forename type="first">Dmitriy</forename><surname>Fradkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Madigan</surname></persName>
		</author>
		<idno type="DOI">10.1145/956750.956812</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Conference on Knowledge Discovery and Data Mining. KDD &apos;03</title>
		<meeting>the 9th International Conference on Knowledge Discovery and Data Mining. KDD &apos;03</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">The Johnson-Lindenstrauss lemma and the sphericity of some graphs</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Frankl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Maehara</surname></persName>
		</author>
		<idno type="DOI">10.1016/0095-8956(88)90043-3</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Combinatorial Theory, Series B</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="355" to="362" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
	<note>cit. on pp. 7, 11, 12</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Some geometric applications of the beta distribution</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Frankl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Maehara</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF00049302</idno>
	</analytic>
	<monogr>
		<title level="j">Annals of the Institute of Statistical Mathematics (AISM)</title>
		<imprint>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">A Song of Johnson and Lindenstrauss</title>
		<author>
			<persName><forename type="first">Casper</forename><surname>Benjamin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Freksen</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Aarhus University</publisher>
			<biblScope unit="page">1</biblScope>
			<pubPlace>Aarhus, Denmark</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<author>
			<persName><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<title level="m">Mobile NLP workshop at the 46th Annual Meeting of the Association for Computational Linguistics (ACL &apos;08)</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note>Small Statistical Models by Random Feature Mixing ACL08-Mobile-NLP</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">QuickSAND: Quick Summary and Analysis of Network Data</title>
		<author>
			<persName><forename type="first">Anna</forename><forename type="middle">C</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannis</forename><surname>Kotidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanmugavelayutham</forename><surname>Muthukrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Strauss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DIMACS</title>
		<imprint>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. rep. 2001-43</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Breaching Euclidean Distance-Preserving Data Perturbation using few Known Inputs</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Giannella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hillol</forename><surname>Kargupta</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.datak.2012.10.004</idno>
	</analytic>
	<monogr>
		<title level="j">Data &amp; Knowledge Engineering</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Compressed Sensing, Sparsity, and Dimensionality in Neuronal Information Processing and Data Analysis</title>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haim</forename><surname>Sompolinsky</surname></persName>
		</author>
		<idno type="DOI">10.1146/annurev-neuro-062111-150410</idno>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Randomized spectral co-clustering for large-scale directed networks</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.12164</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>In: arXiv e-prints stat.ML</note>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">A Replacement for Voronoi Diagrams of Near Linear Size</title>
		<author>
			<persName><forename type="first">Sariel</forename><surname>Har-Peled</surname></persName>
		</author>
		<idno type="DOI">10.1109/SFCS.2001.959884</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Symposium on Foundations of Computer Science. FOCS &apos;01</title>
		<meeting>the 42nd Symposium on Foundations of Computer Science. FOCS &apos;01</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Toeplitz Compressed Sensing Matrices With Applications to Sparse Channel Estimation</title>
		<author>
			<persName><forename type="first">Jarvis</forename><forename type="middle">D</forename><surname>Haupt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Waheed</forename><surname>Uz Zaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gil</forename><forename type="middle">M</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">D</forename><surname>Raz</surname></persName>
		</author>
		<author>
			<persName><surname>Nowak</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIT.2010.2070191</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">When Crossings Count -Approximating the Minimum Spanning Tree</title>
		<author>
			<persName><forename type="first">Sariel</forename><surname>Har</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Peled</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Indyk</surname></persName>
		</author>
		<idno type="DOI">10.1145/336154.336197</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Symposium on Computational Geometry. SoCG &apos;00</title>
		<meeting>the 16th Symposium on Computational Geometry. SoCG &apos;00</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Approximate Nearest Neighbor: Towards Removing the Curse of Dimensionality</title>
		<author>
			<persName><forename type="first">Sariel</forename><surname>Har-Peled</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajeev</forename><surname>Motwani</surname></persName>
		</author>
		<idno type="DOI">10.4086/toc.2012.v008a014</idno>
	</analytic>
	<monogr>
		<title level="j">Theory of Computing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="321" to="350" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>Previously published as IM98; Har01 cit. on pp. 5, 11, 12, 21</note>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Dual-Loco: Distributing Statistical Estimation Using Random Projections</title>
		<author>
			<persName><forename type="first">Christina</forename><surname>Heinze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolai</forename><surname>Meinshausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Artificial Intelligence and Statistics (AISTATS &apos;16)</title>
		<meeting>the 19th International Conference on Artificial Intelligence and Statistics (AISTATS &apos;16)</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research (PMLR)</note>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Finding Structure with Randomness: Probabilistic Algorithms for Constructing Approximate Matrix Decompositions</title>
		<author>
			<persName><forename type="first">N</forename><surname>Halko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Martinsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">Aaron</forename><surname>Tropp</surname></persName>
		</author>
		<idno type="DOI">10.1137/090771806</idno>
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="4" to="6" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Analysis of a Complex of Statistical Variables into Principal Components</title>
		<author>
			<persName><forename type="first">Harold</forename><surname>Hotelling</surname></persName>
		</author>
		<idno type="DOI">10.1037/h0071325</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Educational Psychology</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="1933">1933</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Subspace clustering of dimensionality-reduced data</title>
		<author>
			<persName><forename type="first">Reinhard</forename><surname>Heckel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helmut</forename><surname>Bölcskei</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISIT.2014.6875384</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 47th International Symposium on Information Theory. ISIT &apos;14</title>
		<meeting>the 47th International Symposium on Information Theory. ISIT &apos;14</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Dimensionalityreduced subspace clustering</title>
		<author>
			<persName><forename type="first">Reinhard</forename><surname>Heckel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helmut</forename><surname>Bölcskei</surname></persName>
		</author>
		<idno type="DOI">10.1093/imaiai/iaw021</idno>
	</analytic>
	<monogr>
		<title level="j">Information and Inference: A Journal of the IMA</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="246" to="283" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Previously published as [HTB14] (cit. on p. 6</note>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">The Elements of Statistical Learning: Data Mining, Inference, and Prediction</title>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerome</forename><surname>Friedman</surname></persName>
		</author>
		<idno type="DOI">10.1007/b94608</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Springer</publisher>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note>nd ed. Springer Series in Statistics (SSS). 12th printing</note>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Johnson-Lindenstrauss lemma for circulant matrices</title>
		<author>
			<persName><forename type="first">Aicke</forename><surname>Hinrichs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Vybíral</surname></persName>
		</author>
		<idno type="DOI">10.1002/rsa.20360</idno>
	</analytic>
	<monogr>
		<title level="j">Random Structures &amp; Algorithms</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Approximate Nearest Neighbors: Towards Removing the Curse of Dimensionality</title>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajeev</forename><surname>Motwani</surname></persName>
		</author>
		<idno type="DOI">10.1145/276698.276876</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th Symposium on Theory of Computing. STOC &apos;98</title>
		<meeting>the 30th Symposium on Theory of Computing. STOC &apos;98</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Nearest-Neighbor-Preserving embeddings</title>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Assaf</forename><surname>Naor</surname></persName>
		</author>
		<idno type="DOI">10.1145/1273340.1273347</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions on Algorithms</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Algorithmic Applications of Low-Distortion Geometric Embeddings</title>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Indyk</surname></persName>
		</author>
		<idno type="DOI">10.1109/SFCS.2001.959878</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Symposium on Foundations of Computer Science. FOCS &apos;01</title>
		<meeting>the 42nd Symposium on Foundations of Computer Science. FOCS &apos;01</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Understanding Sparse JL for Feature Hashing</title>
		<author>
			<persName><forename type="first">Meena</forename><surname>Jagadeesan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32. NeurIPS &apos;19</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">Fast and memory-optimal dimension reduction using Kac&apos;s walk</title>
		<author>
			<persName><forename type="first">Vishesh</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natesh</forename><forename type="middle">S</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashwin</forename><surname>Sah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehtaab</forename><surname>Sawhney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10069[cs.DS</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="19" to="21" />
		</imprint>
	</monogr>
	<note>In: arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">An Introduction to Statistical Learning</title>
		<author>
			<persName><forename type="first">Gareth</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniela</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-1-4614-7138-7</idno>
	</analytic>
	<monogr>
		<title level="m">Springer Texts in Statistics (STS). 7th printing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>st cit. on p. 2</note>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">An Introduction to Statistical Learning</title>
		<author>
			<persName><forename type="first">Gareth</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniela</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-1-4614-7138-7</idno>
	</analytic>
	<monogr>
		<title level="m">Springer Texts in Statistics (STS). 7th printing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note>st cit. on p. 2</note>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">An Improved Cutting Plane Method for Convex Optimization, Convex-Concave Games and its Applications</title>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Tat Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Wai</forename><surname>Wong</surname></persName>
		</author>
		<idno type="DOI">10.1145/3357713.3384284</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Symposium on Theory of Computing. STOC &apos;20</title>
		<meeting>the 52nd Symposium on Theory of Computing. STOC &apos;20</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Extensions of Lipschitz mappings into a Hilbert space</title>
		<author>
			<persName><forename type="first">B</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joram</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><surname>Lindenstrauss</surname></persName>
		</author>
		<idno type="DOI">10.1090/conm/026/737400</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1982 Conference in Modern Analysis and Probability</title>
		<title level="s">Contemporary Mathematics</title>
		<meeting>the 1982 Conference in Modern Analysis and Probability</meeting>
		<imprint>
			<publisher>AMS</publisher>
			<date type="published" when="1984">1984</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="189" to="206" />
		</imprint>
	</monogr>
	<note>cit. on pp. 2, 3, 11, 21</note>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Principal Component Analysis. 2nd ed</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">T</forename><surname>Jolliffe</surname></persName>
		</author>
		<idno type="DOI">10.1007/b98835</idno>
	</analytic>
	<monogr>
		<title level="m">Springer Series in Statistics (SSS)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>cit. on pp. 3, 4</note>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Computational Intelligence and Feature Selection -Rough and Fuzzy Approaches. IEEE Press series on computational intelligence</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Shen</surname></persName>
		</author>
		<idno type="DOI">10.1002/9780470377888</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Optimal Bounds for Johnson-Lindenstrauss Transforms and Streaming Problems with Sub-Constant Error</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Jayram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">P</forename><surname>Woodruff</surname></persName>
		</author>
		<idno type="DOI">10.1137/1.9781611973082.1</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd Symposium on Discrete Algorithms. SODA &apos;11. SIAM</title>
		<meeting>the 22nd Symposium on Discrete Algorithms. SODA &apos;11. SIAM</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page">35</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Optimal Bounds for Johnson-Lindenstrauss Transforms and Streaming Problems with Subconstant Error</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Jayram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">P</forename><surname>Woodruff</surname></persName>
		</author>
		<idno type="DOI">10.1145/2483699.2483706</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions on Algorithms</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>Previously published as [JW11] (cit</note>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<author>
			<persName><forename type="first">Ata</forename><surname>Kabán</surname></persName>
		</author>
		<title level="m">Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics, (AISTATS &apos;14)</title>
		<meeting>the Seventeenth International Conference on Artificial Intelligence and Statistics, (AISTATS &apos;14)</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>New Bounds on Compressive Linear Least Squares Regression Proceedings of Machine Learning Research (PMLR)</note>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Foundations of Kinetic Theory</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Kac</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Berkeley Symposium on Mathematical Statistics and Probability</title>
		<meeting>the 3rd Berkeley Symposium on Mathematical Statistics and Probability</meeting>
		<imprint>
			<publisher>University of California Press</publisher>
			<date type="published" when="1956">1956</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Dimensionality Reduction by Random Mapping: Fast Similarity Computation for Clustering</title>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Kaski</surname></persName>
		</author>
		<idno type="DOI">10.1109/IJCNN.1998.682302</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Joint Conference on Neural Networks</title>
		<meeting>the 8th International Joint Conference on Neural Networks</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>ĲCNN &apos;98</note>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Privacy via the Johnson-Lindenstrauss Transform</title>
		<author>
			<persName><forename type="first">Krishnaram</forename><surname>Kenthapadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Korolova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Mironov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nina</forename><surname>Mishra</surname></persName>
		</author>
		<idno type="DOI">10.29012/jpc.v5i1.625</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Privacy and Confidentiality</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Efficient Private Empirical Risk Minimization for High-dimensional Learning</title>
		<author>
			<persName><forename type="first">Shiva</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kasiviswanathan</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33st International Conference on Machine Learning (ICML &apos;16)</title>
		<meeting>the 33st International Conference on Machine Learning (ICML &apos;16)</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research (PMLR)</note>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Two Algorithms for Nearest-Neighbor Search in High Dimensions</title>
		<author>
			<persName><forename type="first">Jon</forename><forename type="middle">M</forename><surname>Kleinberg</surname></persName>
		</author>
		<idno type="DOI">10.1145/258533.258653</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Symposium on the Theory of Computing. STOC &apos;97</title>
		<meeting>the 29th Symposium on the Theory of Computing. STOC &apos;97</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Empirical processes and random projections</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bo'az</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shahar</forename><surname>Klartag</surname></persName>
		</author>
		<author>
			<persName><surname>Mendelson</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jfa.2004.10.009</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Functional Analysis</title>
		<imprint>
			<biblScope unit="volume">225</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Almost Optimal Explicit Johnson-Lindenstrauss Families</title>
		<author>
			<persName><forename type="first">M</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghu</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jelani</forename><surname>Meka</surname></persName>
		</author>
		<author>
			<persName><surname>Nelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Workshop on Approximation Algorithms for Combinatorial Optimization Problems (APPROX &apos;11) and the 15th International Workshop on Randomization and Computation (RANDOM &apos;11)</title>
		<meeting>the 14th International Workshop on Approximation Algorithms for Combinatorial Optimization Problems (APPROX &apos;11) and the 15th International Workshop on Randomization and Computation (RANDOM &apos;11)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title/>
		<idno type="DOI">10.1007/978-3-642-22935-0_53</idno>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science (LNCS</title>
		<imprint>
			<biblScope unit="volume">6845</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2011">2011</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<monogr>
		<title level="m" type="main">A Derandomized Sparse Johnson-Lindenstrauss Transform</title>
		<author>
			<persName><forename type="first">M</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jelani</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName><surname>Nelson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1006.3585[cs.DS</idno>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note>In: arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b118">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jelani</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName><surname>Nelson</surname></persName>
		</author>
		<idno type="DOI">10.1137/1.9781611973099.94</idno>
		<title level="m">Proceedings of the 23rd Symposium on Discrete Algorithms. SODA &apos;12. SIAM</title>
		<meeting>the 23rd Symposium on Discrete Algorithms. SODA &apos;12. SIAM</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page">36</biblScope>
		</imprint>
	</monogr>
	<note>Sparser Johnson-Lindenstrauss Transforms</note>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<author>
			<persName><forename type="first">M</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jelani</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName><surname>Nelson</surname></persName>
		</author>
		<idno type="DOI">10.1145/2559902</idno>
	</analytic>
	<monogr>
		<title level="m">Sparser Johnson-Lindenstrauss Transforms</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
	<note>Previously published as [KN12] (cit. on pp. 13, 14, 21</note>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Efficient Search for Approximate Nearest Neighbor in High Dimensional Spaces</title>
		<author>
			<persName><forename type="first">Eyal</forename><surname>Kushilevitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafail</forename><surname>Ostrovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Rabani</surname></persName>
		</author>
		<idno type="DOI">10.1137/S0097539798347177</idno>
	</analytic>
	<monogr>
		<title level="j">Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="457" to="474" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note>Previously published as [KOR98] (cit. on p. 5</note>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Efficient Search for Approximate Nearest Neighbor in High Dimensional Spaces</title>
		<author>
			<persName><forename type="first">Eyal</forename><surname>Kushilevitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafail</forename><surname>Ostrovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Rabani</surname></persName>
		</author>
		<idno type="DOI">10.1145/276698.276877</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th Symposium on the Theory of Computing. STOC &apos;98</title>
		<meeting>the 30th Symposium on the Theory of Computing. STOC &apos;98</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">New and improved Johnson-Lindenstrauss embeddings via the Restricted Isometry Property</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Krahmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Ward</surname></persName>
		</author>
		<idno type="DOI">10.1137/100810447</idno>
	</analytic>
	<monogr>
		<title level="j">Journal on Mathematical Analysis</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Accelerating Randomly Projected Gradient with Variance Reduction</title>
		<author>
			<persName><forename type="first">Seongyoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seyoung</forename><surname>Yun</surname></persName>
		</author>
		<idno type="DOI">10.1109/BigComp48618.2020.00-11</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Big Data and Smart Computing. BigComp &apos;20</title>
		<meeting>the 7th International Conference on Big Data and Smart Computing. BigComp &apos;20</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Dense Fast Random Projections and Lean Walsh Transforms</title>
		<author>
			<persName><forename type="first">Edo</forename><surname>Liberty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nir</forename><surname>Ailon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Approximation Algorithms for Combinatorial Optimization Problems (APPROX &apos;08) and the 12th International Workshop on Randomization and Computation (RANDOM &apos;08)</title>
		<meeting>the 11th International Workshop on Approximation Algorithms for Combinatorial Optimization Problems (APPROX &apos;08) and the 12th International Workshop on Randomization and Computation (RANDOM &apos;08)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title/>
		<idno type="DOI">10.1007/978-3-540-85363-3_40</idno>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science (LNCS</title>
		<imprint>
			<biblScope unit="volume">5171</biblScope>
			<biblScope unit="page">37</biblScope>
			<date type="published" when="2008">2008</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Dense Fast Random Projections and Lean Walsh Transforms</title>
		<author>
			<persName><forename type="first">Edo</forename><surname>Liberty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nir</forename><surname>Ailon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Singer</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00454-010-9309-5</idno>
	</analytic>
	<monogr>
		<title level="j">Discrete &amp; Computational Geometry</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="34" to="44" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>Previously published as [LAS08] (cit. on pp. 18, 19, 21</note>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Powers of Tensors and Fast Matrix Multiplication</title>
		<author>
			<persName><forename type="first">Le</forename><surname>François</surname></persName>
		</author>
		<author>
			<persName><surname>Gall</surname></persName>
		</author>
		<idno type="DOI">10.1145/2608628.2608664</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International Symposium on Symbolic and Algebraic Computation. ISSAC &apos;14</title>
		<meeting>the 39th International Symposium on Symbolic and Algebraic Computation. ISSAC &apos;14</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<monogr>
		<title level="m" type="main">Projection &amp; Probability-Driven Black-Box Attack</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianzhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.03837[cs.CV</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>In: arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Fast Constrained Spectral Clustering and Cluster Ensemble with Random Projection</title>
		<author>
			<persName><forename type="first">Wenfen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianghong</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuexian</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.1155/2017/2658707</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Intelligence and Neuroscience</title>
		<imprint>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Random Projection-Based Multiplicative Data Perturbation for Privacy Preserving Distributed Data Mining</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hillol</forename><surname>Kargupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Ryan</surname></persName>
		</author>
		<idno type="DOI">10.1109/TKDE.2006.14</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Improved Subsampled Randomized Hadamard Transform for Linear SVM</title>
		<author>
			<persName><forename type="first">Zĳian</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Lan</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v34i04.5880</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th AAAI Conference on Artificial Intelligence (AAAI &apos;20) and the 32nd Conference on Innovative Applications of Artificial Intelligence (IAAI &apos;20) and the 10th Symposium on Educational Advances in Artificial Intelligence (EAAI &apos;20)</title>
		<meeting>the 34th AAAI Conference on Artificial Intelligence (AAAI &apos;20) and the 32nd Conference on Innovative Applications of Artificial Intelligence (IAAI &apos;20) and the 10th Symposium on Educational Advances in Artificial Intelligence (EAAI &apos;20)</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Least squares quantization in PCM</title>
		<author>
			<persName><forename type="first">P</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName><surname>Lloyd</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIT.1982.1056489</idno>
	</analytic>
	<monogr>
		<title level="m">Transactions on Information Theory</title>
		<imprint>
			<date type="published" when="1982">1982</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<monogr>
		<title level="m" type="main">Vowpal Wabbit Code Release</title>
		<author>
			<persName><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">L</forename><surname>Strehl</surname></persName>
		</author>
		<ptr target="https://hunch.net/?p=309" />
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note>visited on 16/06/2020</note>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Optimality of the Johnson-Lindenstrauss Lemma</title>
		<author>
			<persName><forename type="first">Green</forename><surname>Kasper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jelani</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName><surname>Nelson</surname></persName>
		</author>
		<idno type="DOI">10.1109/FOCS.2017.64</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Symposium on Foundations of Computer Science. FOCS &apos;17</title>
		<meeting>the 58th Symposium on Foundations of Computer Science. FOCS &apos;17</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">On the asymptotic complexity of rectangular matrix multiplication</title>
		<author>
			<persName><forename type="first">Grazia</forename><surname>Lotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Romani</surname></persName>
		</author>
		<idno type="DOI">10.1016/0304-3975(83)90054-3</idno>
	</analytic>
	<monogr>
		<title level="j">Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<monogr>
		<title level="m" type="main">Mining of Massive Datasets</title>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anand</forename><surname>Rajaraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>David Ullman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note>rd ed.</note>
</biblStruct>

<biblStruct xml:id="b137">
	<monogr>
		<title level="m" type="main">Mining of Massive Datasets</title>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anand</forename><surname>Rajaraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>David Ullman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note>rd ed.</note>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Randomized Algorithms for Matrices and Data</title>
		<author>
			<persName><forename type="first">W</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><surname>Mahoney</surname></persName>
		</author>
		<idno type="DOI">10.1561/2200000035</idno>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends® in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">On variants of the Johnson-Lindenstrauss lemma</title>
		<author>
			<persName><forename type="first">Jiří</forename><surname>Matoušek</surname></persName>
		</author>
		<idno type="DOI">10.1002/rsa.20218</idno>
	</analytic>
	<monogr>
		<title level="j">Random Structures &amp; Algorithms</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Testing Odor Response Stereotypy in the Drosophila Mushroom Body</title>
		<author>
			<persName><forename type="first">Mala</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ila</forename><surname>Fiete</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gilles</forename><surname>Laurent</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuron.2008.07.040</idno>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Compressed Least-Squares Regression</title>
		<author>
			<persName><forename type="first">Odalric-Ambrym</forename><surname>Maillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 22. NIPS &apos;09</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Low-Distortion Subspace Embeddings in Input-Sparsity Time and Applications to Robust Linear Regression</title>
		<author>
			<persName><forename type="first">Xiangrui</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<idno type="DOI">10.1145/2488608.2488621</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Symposium on Theory of Computing. STOC &apos;13</title>
		<meeting>the 45th Symposium on Theory of Computing. STOC &apos;13</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<monogr>
		<title level="m" type="main">Projection-Cost-Preserving Sketches: Proof Strategies and Constructions</title>
		<author>
			<persName><forename type="first">Cameron</forename><surname>Musco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Musco</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08434[cs.DS</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>In: arXiv e-prints Previously published as Coh+15; CMM17</note>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Performance of Johnson-Lindenstrauss Transform for 𝑘-Means and 𝑘-Medians Clustering</title>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Makarychev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yury</forename><surname>Makarychev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Razenshteyn</surname></persName>
		</author>
		<idno type="DOI">10.1145/3313276.3316350</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Symposium on Theory of Computing. STOC &apos;19</title>
		<meeting>the 51st Symposium on Theory of Computing. STOC &apos;19</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<monogr>
		<author>
			<persName><forename type="first">Jessie</forename><surname>Florence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Macwilliams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><surname>Sloane</surname></persName>
		</author>
		<title level="m">The Theory of Error-Correcting Codes</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1977">1977</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note>North-Holland Mathematical Library</note>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Data Streams: Algorithms and Applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
		<idno type="DOI">10.1561/0400000002</idno>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends® in Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<monogr>
		<title level="m" type="main">High-dimensional outlier detection using random projections</title>
		<author>
			<persName><forename type="first">Paula</forename><surname>Navarro-Esteban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>Antonio Cuesta-Albertos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.08923</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>In: arXiv e-prints stat.ME</note>
</biblStruct>

<biblStruct xml:id="b148">
	<monogr>
		<title level="m" type="main">Sketching and Streaming High-Dimensional Vectors</title>
		<author>
			<persName><forename type="first">Jelani</forename><surname>Nelson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Massachusetts Institute of Technology</publisher>
			<biblScope unit="page">11</biblScope>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Training robust models using Random Projection</title>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Vinh Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><forename type="middle">M</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sakrapee</forename><surname>Paisitkriangkrai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Leckie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kotagiri</forename><surname>Ramamohanarao</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICPR.2016.7899688</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Pattern Recognition. ICPR &apos;16</title>
		<meeting>the 23rd International Conference on Pattern Recognition. ICPR &apos;16</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<monogr>
		<title level="m" type="main">Dimension Reduction Methods with Applications to High Dimensional Data with a Censored Response</title>
		<author>
			<persName><forename type="first">Tuan</forename><forename type="middle">S</forename><surname>Nguyen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page">12</biblScope>
			<pubPlace>Houston, TX</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Rice University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b151">
	<monogr>
		<title level="m" type="main">Asymptotic Analysis of an Ensemble of Randomly Projected Linear Discriminants</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abla</forename><surname>Niyazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hayssam</forename><surname>Kammoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed-Slim</forename><surname>Dahrouj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tareq</forename><forename type="middle">Y</forename><surname>Alouini</surname></persName>
		</author>
		<author>
			<persName><surname>Al-Naffouri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08217</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>In: arXiv e-prints stat.ML</note>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">OSNAP: Faster Numerical Linear Algebra Algorithms via Sparser Subspace Embeddings</title>
		<author>
			<persName><forename type="first">Jelani</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huy</forename><surname>Lê Nguyê ˜n</surname></persName>
		</author>
		<idno type="DOI">10.1109/FOCS.2013.21</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Symposium on Foundations of Computer Science. FOCS &apos;13</title>
		<meeting>the 54th Symposium on Foundations of Computer Science. FOCS &apos;13</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Sparsity Lower Bounds for Dimensionality Reducing Maps</title>
		<author>
			<persName><forename type="first">Jelani</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huy</forename><surname>Lê Nguyê ˜n</surname></persName>
		</author>
		<idno type="DOI">10.1145/2488608.2488622</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Symposium on Theory of Computing. STOC &apos;13</title>
		<meeting>the 45th Symposium on Theory of Computing. STOC &apos;13</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">Random Projections for Support Vector Machines</title>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Boutsidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malik</forename><surname>Magdon-Ismail</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petros</forename><surname>Drineas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on Artificial Intelligence and Statistics (AISTATS &apos;13)</title>
		<meeting>the 16th International Conference on Artificial Intelligence and Statistics (AISTATS &apos;13)</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">39</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research (PMLR)</note>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">Random Projections for Linear Support Vector Machines</title>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Boutsidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malik</forename><surname>Magdon-Ismail</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petros</forename><surname>Drineas</surname></persName>
		</author>
		<idno type="DOI">10.1145/2641760</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions on Knowledge Discovery from Data</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>Previously published as [Pau+13] (cit. on p. 6</note>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">On lines and planes of closest fit to systems of points in space</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Pearson</surname></persName>
		</author>
		<idno type="DOI">10.1080/14786440109462720</idno>
	</analytic>
	<monogr>
		<title level="j">The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="1901">1901</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">A compressed sensing perspective of hippocampal function</title>
		<author>
			<persName><forename type="first">C</forename><surname>Panagiotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panayiota</forename><surname>Petrantonakis</surname></persName>
		</author>
		<author>
			<persName><surname>Poirazi</surname></persName>
		</author>
		<idno type="DOI">10.3389/fnsys.2014.00141</idno>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Systems Neuroscience</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">Randomized Sketches of Convex Programs with Sharp Guarantees</title>
		<author>
			<persName><forename type="first">Mert</forename><surname>Pilanci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISIT.2014.6874967</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 47th International Symposium on Information Theory. ISIT &apos;14</title>
		<meeting>the 47th International Symposium on Information Theory. ISIT &apos;14</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page">40</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">Randomized Sketches of Convex Programs with Sharp Guarantees</title>
		<author>
			<persName><forename type="first">Mert</forename><surname>Pilanci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIT.2015.2450722</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="5096" to="5115" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Previously published as PW14] (cit. on pp. 5, 7</note>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Circulant and Toeplitz matrices in compressed sensing</title>
		<author>
			<persName><forename type="first">Holger</forename><surname>Rauhut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Signal Processing with Adaptive Sparse Structured Representations. SPARS &apos;09</title>
		<meeting>the 2nd Workshop on Signal Processing with Adaptive Sparse Structured Representations. SPARS &apos;09</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">Self-Organizing Semantic Maps</title>
		<author>
			<persName><forename type="first">Helge</forename><forename type="middle">J</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teuvo</forename><surname>Kohonen</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF00203171</idno>
	</analytic>
	<monogr>
		<title level="j">Biological Cybernetics</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<monogr>
		<title level="m" type="main">Improving the Johnson-Lindenstrauss Lemma</title>
		<author>
			<persName><forename type="first">Javier</forename><surname>Rojo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuan</forename><forename type="middle">S</forename><surname>Nguyen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1005.1440</idno>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note>In: arXiv e-prints stat.ML</note>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title level="a" type="main">Compressive Sensing by Random Convolution</title>
		<author>
			<persName><forename type="first">Justin</forename><forename type="middle">K</forename><surname>Romberg</surname></persName>
		</author>
		<idno type="DOI">10.1137/08072975X</idno>
	</analytic>
	<monogr>
		<title level="j">Journal on Imaging Sciences</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">Restricted isometries for partial random circulant matrices</title>
		<author>
			<persName><forename type="first">Holger</forename><surname>Rauhut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><forename type="middle">K</forename><surname>Romberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">Aaron</forename><surname>Tropp</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.acha.2011.05.001</idno>
	</analytic>
	<monogr>
		<title level="j">Applied and Computational Harmonic Analysis</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<analytic>
		<title level="a" type="main">A Randomized Algorithm for Principal Component Analysis</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Rokhlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Tygert</surname></persName>
		</author>
		<idno type="DOI">10.1137/080736417</idno>
	</analytic>
	<monogr>
		<title level="j">Journal on Matrix Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1100" to="1124" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>cit. on pp. 3, 4</note>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main">On Sparse Reconstruction from Fourier and Gaussian Measurements</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Rudelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Vershynin</surname></persName>
		</author>
		<idno type="DOI">10.1002/cpa.20227</idno>
	</analytic>
	<monogr>
		<title level="j">Communications on Pure and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">Representations of Odor in the Piriform Cortex</title>
		<author>
			<persName><forename type="first">Dan</forename><forename type="middle">D</forename><surname>Stettler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Axel</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuron.2009.09.005</idno>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main">Improved Approximation Algorithms for Large Matrices via Random Projections</title>
		<author>
			<persName><forename type="first">Tamás</forename><surname>Sarlós</surname></persName>
		</author>
		<idno type="DOI">10.1109/FOCS.2006.37</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 47th Symposium on Foundations of Computer Science. FOCS &apos;06</title>
		<meeting>the 47th Symposium on Foundations of Computer Science. FOCS &apos;06</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="4" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">Stable Random Projection: Lightweight, General-Purpose Dimensionality Reduction for Digitized Libraries</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Schmidt</surname></persName>
		</author>
		<idno type="DOI">10.22148/16.025</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Cultural Analytics</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>cit. on pp. 2, 6, 12</note>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">Random Projection for k-means Clustering</title>
		<author>
			<persName><forename type="first">Sami</forename><surname>Sieranoja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pasi</forename><surname>Fränti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Artificial Intelligence and Soft Computing (ICAISC &apos;18)</title>
		<title level="s">Lecture Notes in Computer Science (LNCS</title>
		<meeting>the 17th International Conference on Artificial Intelligence and Soft Computing (ICAISC &apos;18)</meeting>
		<imprint>
			<biblScope unit="volume">10841</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-91253-0_63</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<analytic>
		<title level="a" type="main">Differentially Private Ordinary Least Squares</title>
		<author>
			<persName><forename type="first">Or</forename><surname>Sheffet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34st International Conference on Machine Learning (ICML &apos;17)</title>
		<meeting>the 34st International Conference on Machine Learning (ICML &apos;17)</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page">41</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research (PMLR)</note>
</biblStruct>

<biblStruct xml:id="b173">
	<analytic>
		<title level="a" type="main">Differentially Private Ordinary Least Squares</title>
		<author>
			<persName><forename type="first">Or</forename><surname>Sheffet</surname></persName>
		</author>
		<idno type="DOI">10.29012/jpc.654</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Privacy and Confidentiality</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Previously published as She17] (cit. on p. 6</note>
</biblStruct>

<biblStruct xml:id="b174">
	<analytic>
		<title level="a" type="main">Hash Kernels</title>
		<author>
			<persName><forename type="first">Qinfeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Petterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gideon</forename><surname>Dror</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">L</forename><surname>Strehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Artificial Intelligence and Statistics (AISTATS &apos;09)</title>
		<meeting>the 12th International Conference on Artificial Intelligence and Statistics (AISTATS &apos;09)</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">41</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main">Hash Kernels for Structured Data</title>
		<author>
			<persName><forename type="first">Qinfeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Petterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gideon</forename><surname>Dror</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>Previously published as Shi+09a] (cit</note>
</biblStruct>

<biblStruct xml:id="b176">
	<analytic>
		<title level="a" type="main">Fast Spectral Clustering with Random Projection and Sampling</title>
		<author>
			<persName><forename type="first">Tomoya</forename><surname>Sakai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atsushi</forename><surname>Imiya</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-03070-3_28</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Machine Learning and Data Mining in Pattern Recognition (MLDM &apos;09)</title>
		<title level="s">Lecture Notes in Computer Science (LNCS</title>
		<meeting>the 6th International Conference on Machine Learning and Data Mining in Pattern Recognition (MLDM &apos;09)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">5632</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<monogr>
		<title level="m" type="main">Privacy-Utility Trade-off of Linear Regression under Random Projections and Additive Noise</title>
		<author>
			<persName><forename type="first">Mehrdad</forename><surname>Showkatbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Karakus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhas</forename><forename type="middle">N</forename><surname>Diggavi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.04688[cs.LG</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>In: arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main">Compressed Least Squares Regression revisited</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Slawski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (AISTATS &apos;17)</title>
		<meeting>the 20th International Conference on Artificial Intelligence and Statistics (AISTATS &apos;17)</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research (PMLR)</note>
</biblStruct>

<biblStruct xml:id="b179">
	<analytic>
		<title level="a" type="main">Random Projection Ensemble Classifiers</title>
		<author>
			<persName><forename type="first">Alon</forename><surname>Schclar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lior</forename><surname>Rokach</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-01347-8_26</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Enterprise Information Systems (ICEIS &apos;09)</title>
		<title level="s">Lecture Notes in Business Information Processing</title>
		<meeting>the 11th International Conference on Enterprise Information Systems (ICEIS &apos;09)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b180">
	<analytic>
		<title level="a" type="main">Graph Sparsification by Effective Resistances</title>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">A</forename><surname>Spielman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Srivastava</surname></persName>
		</author>
		<idno type="DOI">10.1145/1374376.1374456</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Symposium on Theory of Computing. STOC &apos;08</title>
		<meeting>the 40th Symposium on Theory of Computing. STOC &apos;08</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page">41</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main">Graph Sparsification by Effective Resistances</title>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">A</forename><surname>Spielman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Srivastava</surname></persName>
		</author>
		<idno type="DOI">10.1137/080734029</idno>
	</analytic>
	<monogr>
		<title level="j">Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<analytic>
		<title level="a" type="main">Approximate Nearest Neighbors Search Without False Negatives For ℓ 2 For 𝑐 &gt; √︁ log log 𝑛</title>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Sankowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Wygocki</surname></persName>
		</author>
		<idno type="DOI">10.4230/LIPIcs.ISAAC.2017.63</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Symposium on Algorithms and Computation (ISAAC &apos;17)</title>
		<meeting>the 28th International Symposium on Algorithms and Computation (ISAAC &apos;17)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note>Leibniz International Proceedings in Informatics (LIPIcs). Schloss Dagstuhl</note>
</biblStruct>

<biblStruct xml:id="b183">
	<analytic>
		<title level="a" type="main">Fast and Scalable Outlier Detection with Approximate Nearest Neighbor Ensembles</title>
		<author>
			<persName><forename type="first">Erich</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Zimek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans-Peter</forename><surname>Kriegel</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-18123-3_2</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Database Systems for Advanced Applications (DASFAA &apos;15)</title>
		<title level="s">Lecture Notes in Computer Science (LNCS</title>
		<meeting>the 20th International Conference on Database Systems for Advanced Applications (DASFAA &apos;15)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9050</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b184">
	<analytic>
		<title level="a" type="main">Comparing Dimension Reduction Techniques for Document Clustering</title>
		<author>
			<persName><forename type="first">Bin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Shepherd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malcolm</forename><forename type="middle">I</forename><surname>Heywood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Luo</surname></persName>
		</author>
		<idno type="DOI">10.1007/11424918_30</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Canadian Conference on AI (AI &apos;05)</title>
		<title level="s">Lecture Notes in Computer Science (LNCS</title>
		<meeting>the 18th Canadian Conference on AI (AI &apos;05)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3501</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b185">
	<analytic>
		<title level="a" type="main">Defending Against Adversarial Attacks by Randomized Diversification</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Taran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shideh</forename><surname>Rezaeifar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taras</forename><surname>Holotyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slava</forename><surname>Voloshynovskiy</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.01148</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd Conference on Computer Vision and Pattern Recognition. CVPR &apos;19</title>
		<meeting>the 32nd Conference on Computer Vision and Pattern Recognition. CVPR &apos;19</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b186">
	<analytic>
		<title level="a" type="main">Random Projections for Large-Scale Regression</title>
		<author>
			<persName><forename type="first">Gian-Andrea</forename><surname>Thanei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christina</forename><surname>Heinze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolai</forename><surname>Meinshausen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-41573-4_3</idno>
	</analytic>
	<monogr>
		<title level="m">Big and Complex Data Analysis: Methodologies and Applications. Contributions to Statistics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b187">
	<analytic>
		<title level="a" type="main">Improved Analysis of the subsampled Randomized Hadamard Transform</title>
		<author>
			<persName><forename type="first">Joel</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tropp</forename></persName>
		</author>
		<idno type="DOI">10.1142/S1793536911000787</idno>
	</analytic>
	<monogr>
		<title level="j">Advances in Adaptive Data Analysis</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b188">
	<analytic>
		<title level="a" type="main">A Faster Cutting Plane Method and its Implications for Combinatorial and Convex Optimization</title>
		<author>
			<persName><forename type="first">Yin</forename><surname>Tat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Sidford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Wai</forename><surname>Wong</surname></persName>
		</author>
		<idno type="DOI">10.1109/FOCS.2015.68</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Symposium on Foundations of Computer Science. FOCS &apos;15</title>
		<meeting>the 56th Symposium on Foundations of Computer Science. FOCS &apos;15</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b189">
	<analytic>
		<title level="a" type="main">Disclosure Risks of Distance Preserving Data Transformations</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Onur</forename><surname>Turgay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brochmann Pedersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yücel</forename><surname>Saygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erkay</forename><surname>Savas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Levi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Scientific and Statistical Database Management (SSDBM &apos;08)</title>
		<title level="s">Lecture Notes in Computer Science (LNCS</title>
		<meeting>the 20th International Conference on Scientific and Statistical Database Management (SSDBM &apos;08)</meeting>
		<imprint>
			<biblScope unit="volume">5069</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b190">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-69497-7_8</idno>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b191">
	<analytic>
		<title level="a" type="main">Tabulation based 4-universal hashing with applications to second moment estimation</title>
		<author>
			<persName><forename type="first">Mikkel</forename><surname>Thorup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Symposium on Discrete Algorithms. SODA &apos;04</title>
		<meeting>the 15th Symposium on Discrete Algorithms. SODA &apos;04</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page">43</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b192">
	<analytic>
		<title level="a" type="main">Tabulation Based 5-Universal Hashing and Linear Probing</title>
		<author>
			<persName><forename type="first">Mikkel</forename><surname>Thorup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1137/1.9781611972900.7</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Workshop on Algorithm Engineering and Experiments. ALENEX &apos;10. SIAM</title>
		<meeting>the 12th Workshop on Algorithm Engineering and Experiments. ALENEX &apos;10. SIAM</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page">43</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b193">
	<analytic>
		<title level="a" type="main">Tabulation-Based 5-Independent Hashing with Applications to Linear Probing and Second Moment Estimation</title>
		<author>
			<persName><forename type="first">Mikkel</forename><surname>Thorup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1137/100800774</idno>
	</analytic>
	<monogr>
		<title level="j">Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>TZ10; TZ04</note>
</biblStruct>

<biblStruct xml:id="b194">
	<analytic>
		<title level="a" type="main">Clustering by Random Projections</title>
		<author>
			<persName><forename type="first">Chabane</forename><surname>Thierry Urruty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><forename type="middle">A</forename><surname>Djeraba</surname></persName>
		</author>
		<author>
			<persName><surname>Simovici</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-73435-2_9</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Industrial Conference on Data Mining (ICDM &apos;07)</title>
		<title level="s">Lecture Notes in Computer Science (LNCS</title>
		<meeting>the 3rd Industrial Conference on Data Mining (ICDM &apos;07)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">4597</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b195">
	<monogr>
		<title/>
		<author>
			<persName><surname>De Bayeux</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">11</biblScope>
			<pubPlace>Bayeux, France, ca</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b196">
	<analytic>
		<title level="a" type="main">Random Projections, Graph Sparsification, and Differential Privacy</title>
		<author>
			<persName><forename type="first">Jalaj</forename><surname>Upadhyay</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-42033-7_15</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on the Theory and Application of Cryptology and Information Security (ASIACRYPT &apos;13)</title>
		<title level="s">Lecture Notes in Computer Science (LNCS</title>
		<meeting>the 19th International Conference on the Theory and Application of Cryptology and Information Security (ASIACRYPT &apos;13)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">8269</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b197">
	<monogr>
		<title level="m" type="main">Randomness Efficient Fast-Johnson-Lindenstrauss Transform with Applications in Differential Privacy and Compressed Sensing</title>
		<author>
			<persName><forename type="first">Jalaj</forename><surname>Upadhyay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.2470[cs.DS</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>In: arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b198">
	<analytic>
		<title level="a" type="main">The Price of Privacy for Low-rank Factorization</title>
		<author>
			<persName><forename type="first">Jalaj</forename><surname>Upadhyay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31. NeurIPS &apos;18</title>
		<imprint>
			<publisher>Curran Associates</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b199">
	<analytic>
		<author>
			<persName><forename type="first">S</forename><surname>Santosh</surname></persName>
		</author>
		<author>
			<persName><surname>Vempala</surname></persName>
		</author>
		<idno type="DOI">10.1090/dimacs/065</idno>
	</analytic>
	<monogr>
		<title level="m">The Random Projection Method</title>
		<title level="s">DIMACS Series in Discrete Mathematics and Theoretical Computer Science. AMS</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b200">
	<analytic>
		<title level="a" type="main">Random Projection: A New Approach to VLSI Layout</title>
		<author>
			<persName><forename type="first">S</forename><surname>Santosh</surname></persName>
		</author>
		<author>
			<persName><surname>Vempala</surname></persName>
		</author>
		<idno type="DOI">10.1109/SFCS.1998.743489</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th Symposium on Foundations of Computer Science. FOCS &apos;98</title>
		<meeting>the 39th Symposium on Foundations of Computer Science. FOCS &apos;98</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b201">
	<monogr>
		<title level="m" type="main">Using the Johnson-Lindenstrauss Lemma in Linear and Integer Programming</title>
		<author>
			<persName><forename type="first">Khac</forename><surname>Ky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Louis</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leo</forename><surname>Poirion</surname></persName>
		</author>
		<author>
			<persName><surname>Liberti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.00990</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>In: arXiv e-prints math.OC</note>
</biblStruct>

<biblStruct xml:id="b202">
	<analytic>
		<title level="a" type="main">A variant of the Johnson-Lindenstrauss lemma for circulant matrices</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Vybíral</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jfa.2010.11.014</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Functional Analysis</title>
		<imprint>
			<biblScope unit="volume">260</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b203">
	<analytic>
		<title level="a" type="main">New Hash Functions and Their Use in Authentication and Set Equality</title>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">N</forename><surname>Wegman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Lawrence</forename><surname>Carter</surname></persName>
		</author>
		<idno type="DOI">10.1016/0022-0000(81)90033-7</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b204">
	<analytic>
		<title level="a" type="main">A Taxonomy and Performance Model of Data Skew Effects in Parallel Joins</title>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">B</forename><surname>Walton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alfred</forename><forename type="middle">G</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><forename type="middle">M</forename><surname>Jenevein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Very Large Data Bases. VLDB &apos;91</title>
		<meeting>the 17th International Conference on Very Large Data Bases. VLDB &apos;91</meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b205">
	<analytic>
		<title level="a" type="main">Support vector machines resilient against training data integrity attacks</title>
		<author>
			<persName><forename type="first">Sandamal</forename><surname>Weerasinghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><forename type="middle">Monazam</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tansu</forename><surname>Alpcan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Leckie</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2019.106985</idno>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b206">
	<analytic>
		<title level="a" type="main">Feature Hashing for Large Scale Multitask Learning</title>
		<author>
			<persName><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirban</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Attenberg</surname></persName>
		</author>
		<idno type="DOI">10.1145/1553374.1553516</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Machine Learning. ICML &apos;09</title>
		<meeting>the 26th International Conference on Machine Learning. ICML &apos;09</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1113" to="1120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b207">
	<monogr>
		<title level="m" type="main">Feature Hashing for Large Scale Multitask Learning</title>
		<author>
			<persName><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirban</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Attenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:0902.2206v5[cs.AI</idno>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note>In: arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b208">
	<analytic>
		<title level="a" type="main">Multiplying Matrices faster than Coppersmith-Winograd</title>
		<author>
			<persName><forename type="first">Virginia</forename><surname>Vassilevska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Williams</forename></persName>
		</author>
		<idno type="DOI">10.1145/2213977.2214056</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th Symposium on Theory of Computing. STOC &apos;12</title>
		<meeting>the 44th Symposium on Theory of Computing. STOC &apos;12</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b209">
	<analytic>
		<title level="a" type="main">Projecting &quot;Better Than Randomly&quot;: How to Reduce the Dimensionality of Very Large Datasets in a Way that Outperforms Random Projections</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Wojnowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Glenn</forename><surname>Chisholm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Wolff</surname></persName>
		</author>
		<idno type="DOI">10.1109/DSAA.2016.26</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Data Science and Advanced Analytics. DSAA &apos;16</title>
		<meeting>the 3rd International Conference on Data Science and Advanced Analytics. DSAA &apos;16</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b210">
	<analytic>
		<title level="a" type="main">Sketching as a Tool for Numerical Linear Algebra</title>
		<author>
			<persName><forename type="first">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><surname>Woodruff</surname></persName>
		</author>
		<idno type="DOI">10.1561/0400000060</idno>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends® in Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b211">
	<analytic>
		<title level="a" type="main">Searchable words on the Web</title>
		<author>
			<persName><forename type="first">Hugh</forename><forename type="middle">E</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Zobel</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00799-003-0050-z</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal on Digital Libraries</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b212">
	<analytic>
		<title level="a" type="main">Comparison among dimensionality reduction techniques based on Random Projection for cancer classification</title>
		<author>
			<persName><forename type="first">Haozhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaosheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yadong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.compbiolchem.2016.09.010</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Biology and Chemistry</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b213">
	<analytic>
		<title level="a" type="main">DPPro: Differentially Private High-Dimensional Data Release via Random Projection</title>
		<author>
			<persName><forename type="first">Chugui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ju</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaoxue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhan</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kui</forename><surname>Ren</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIFS.2017.2737966</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b214">
	<analytic>
		<title level="a" type="main">Privacy Preserving Collaborative Filtering via the Johnson-Lindenstrauss Transform</title>
		<author>
			<persName><forename type="first">Mengmeng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianqing</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanlei</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1109/Trustcom/BigDataSE/ICESS.2017.266</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom &apos;17) and the 11th International Conference on Big Data Science and Engineering (BigDataSE &apos;17) and the 14th International Conference on Embedded Software and Systems (ICESS&apos;17</title>
		<meeting>the 16th International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom &apos;17) and the 11th International Conference on Big Data Science and Engineering (BigDataSE &apos;17) and the 14th International Conference on Embedded Software and Systems (ICESS&apos;17</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b215">
	<monogr>
		<title level="m" type="main">How to reduce dimension with PCA and random projections?</title>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sifan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edgar</forename><surname>Dobriban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">P</forename><surname>Woodruff</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00511</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note>In: arXiv e-prints math.ST</note>
</biblStruct>

<biblStruct xml:id="b216">
	<analytic>
		<title level="a" type="main">Recovering the Optimal Solution by Dual Random Projection</title>
		<author>
			<persName><forename type="first">Lĳun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehrdad</forename><surname>Mahdavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianbao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenghuo</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual Conference on Learning Theory (COLT &apos;13)</title>
		<meeting>the 26th Annual Conference on Learning Theory (COLT &apos;13)</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research (PMLR)</note>
</biblStruct>

<biblStruct xml:id="b217">
	<analytic>
		<title level="a" type="main">SUOD: Toward Scalable Unsupervised Outlier Detection</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueying</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoping</forename><surname>Bai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.03222[cs.LG</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI-20 Workshop on Artificial Intelligence for Cyber Security. AICS &apos;20</title>
		<meeting>the AAAI-20 Workshop on Artificial Intelligence for Cyber Security. AICS &apos;20</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b218">
	<analytic>
		<title level="a" type="main">Experiments with Random Projections Ensembles: Linear Versus Quadratic Discriminants</title>
		<author>
			<persName><forename type="first">Xi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ata</forename><surname>Kabán</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICDMW.2019.00108</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 International Conference on Data Mining Workshops. ICDMW &apos;19</title>
		<meeting>the 2019 International Conference on Data Mining Workshops. ICDMW &apos;19</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b219">
	<analytic>
		<title level="a" type="main">Compressed Regression</title>
		<author>
			<persName><forename type="first">Shuheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><forename type="middle">A</forename><surname>Wasserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 20. NIPS &apos;07</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page">45</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b220">
	<analytic>
		<title level="a" type="main">Compressed and Privacy-Sensitive Sparse Regression</title>
		<author>
			<persName><forename type="first">Shuheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><forename type="middle">A</forename><surname>Wasserman</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIT.2008.2009605</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="846" to="866" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>Previously published as [ZLW07] (cit. on p. 7</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
