# TRANSFORMER 2 : SELF-ADAPTIVE LLMS

## Abstract

## 

Self-adaptive large language models (LLMs) aim to solve the challenges posed by traditional fine-tuning methods, which are often computationally intensive and static in their ability to handle diverse tasks. We introduce Transformer 2 , a novel self-adaptation framework that adapts LLMs for unseen tasks in real-time by selectively adjusting only the singular components of their weight matrices. During inference, Transformer 2 employs a two-pass mechanism: first, a dispatch system identifies the task properties, and then task-specific "expert" vectors, trained using reinforcement learning, are dynamically mixed to obtain targeted behavior for the incoming prompt. Our method outperforms ubiquitous approaches such as LoRA, with fewer parameters and greater efficiency. Transformer 2 demonstrates versatility across different LLM architectures and modalities, including vision-language tasks. Transformer 2 represents a significant leap forward, offering a scalable, efficient solution for enhancing the adaptability and task-specific performance of LLMs, paving the way for truly dynamic, self-organizing AI systems. Our code is

## 

Figure [1](#): Overview of Transformer 2 . In the training phase, we tune the scales of the singular values of the weight matrices to generate a set of "expert" vectors, each of which specializes in one type of tasks. In the inference phase, a two-pass process is adopted where the first applies the taskspecific expert and the second generates the answer.

Self-adaptive large language models (LLMs) would represent a significant advancement in artificial intelligence, providing a framework where models can adjust to varied tasks and dynamic contexts in real time. While compositionality and scalability are crucial for effective adaptation, current LLM training methodologies fall short of achieving both these properties simultaneously. Our research aims to present a pioneering solution to realize this vision and address these gaps.

Traditionally, LLM post-training has sought to optimize a model for a wide range of capabilities in a single, extensive training session. While this "one-shot" fine-tuning framework is ideal from a simplicity perspective, it is also difficult to achieve in practice. For instance, post-training is still highly resource-intensive, leading to significant computational costs and training times. Additionally, there tends to be notable performance trade-offs when introducing additional breadth to the data, making it challenging to overcome overfitting and task interference at the same time.

In contrast, self-adaptive models offer a more flexible and efficient approach. Rather than attempting to train an LLM for all tasks in one step, expert modules can be developed offline and augmented Preprint to the base LLM on-demand [(Kang et al., 2024)](#b15). This allows the model to dynamically modify its behavior based on the task at hand, without the need for constant re-tuning. In addition to the benefit of having independent components, this modularity also supports continual learning, enabling the model to add new skills over time without catastrophic forgetting. Moreover, self-adaptive LLMs mirror a well-established principle in neuroscience and computational biology, where the brain activates specific regions depending on the task at hand [(Loose et al., 2017)](#b22) and dynamically reconfigures its functional networks in response to changing task demands [(Davison et al., 2015)](#b8).

In principle, the first step toward achieving self-adaptive LLMs can be realized through the development of specialized expert modules, each fine-tuned [(Kaplan et al., 2020)](#b16) via techniques such as low-rank adaptation (LoRA) [(Hu et al., 2021)](#b13). These expert modules can then be dynamically composed at runtime based on the task demands, a process that can be efficiently managed through Mixture of Experts (MoE)-like systems [(Tianlong et al., 2024)](#). However, several challenges need to be addressed to make this approach both scalable and compositional. First, fine-tuning LLMs to create multiple expert modules significantly increases the number of parameters that need to be trained. In practice, even with parameter-efficient methods like LoRA, the cumulative size of these modules can quickly escalate, leading to increased storage and computational demands. Second, these expert modules are often prone to overfitting, a phenomenon especially prevalent when training on smaller datasets or narrow task domains. Third, the flexible composition of these expert modules also presents largely unresolved challenges currently posing as open research problems.

To overcome these limitations, we first propose Singular Value Fine-tuning (SVF), a novel parameter-efficient fine-tuning (PEFT) method to obtain effective building blocks for selfadaptation. SVF works by extracting and tuning only the singular values within the model's weight matrices. By focusing on this principled parameterization, our approach mitigates the risk of overfitting, drastically reduces computational demands, and allows for inherent compositionality. We show these properties enable us to cheaply obtain a set of effective domain-specific "expert" vectors by training on narrow datasets with RL, directly optimizing task performance on individual topics.

We then introduce our full Transformer 2 framework to empower LLMs through the underlying principles of self-adaptation. Given a prompt from an unknown task, Transformer 2 entails a two-pass inference mechanism which we illustrate in Figure [1](#). During the first pass, Transformer 2 executes the model and observes its test-time behavior, gathering the relevant information to understand the necessary skills to tackle the current problem. During the second pass, our framework uses this information to combine the available expert vectors and provide a new modification to the base weights of the LLM specifically tailored to its test-time conditions. We design three different adaptation strategies that can be used within Transformer 2 , which we show provide monotonic performance benefits with increasing access to the test-time conditions.

We evaluate SVF and the full Transformer 2 framework through extensive experiments across a diverse range of LLMs and tasks. First, when trained on domain-specific datasets, we show that SVF consistently outperforms traditional strategies for efficient fine-tuning such as LoRA, and at the same time, with orders of magnitudes fewer parameters. Then we show that Transformer 2 is able to push performance far further, effectively adapting the weights of the base model even in entirely out-of-distribution applications such as visual question answering. Finally, we analyze the properties of our new framework, validating that it provides increasing benefits with additional access to its current test-time conditions and even allow for recycling pre-trained SVF experts across model architectures. In summary, our key technical contributions are the following:

• The development of Transformer 2 as a pivotal self-adaptation framework for LLMs, providing a universal blueprint to dynamically adapt the behavior of LLMs from a growing set of pre-trained skills.

• The introduction of SVF, a novel PEFT method trainable with RL on small datasets, producing compact expert vectors with inherent compositionality, all key properties necessary for our scalable self-adaptation framework.

• The implementation of three adaptation strategies within Transformer 2 , effectively dispatching SVF-trained experts with properties designed to cope with different requirements and deployment scenarios.

## RELATED WORKS

Self-adaptive LLMs We define self-adaptive LLMs as a group of LLMs or a standalone LLM that can evaluate and modify its behavior in response to changes in its operating environment or internal state, without external intervention. This adaptation can be explored from two perspectives: a macroview, where multiple LLMs collaborate and/or compete, and a microview, where internal adaptations allow a single LLM to specialize in different tasks.

Macroview: From this perspective, the system directs queries to LLMs with domain specific expertise, prioritizing outputs from expert models, thereby achieving higher accuracy and task-specific optimization. Such task-specific ensembles can be realized through various mechanisms: multiple LLMs playing distinct roles and coordinate toward a shared goal [(Zhuge et al., 2023)](#b37), engaging in mutual listening and debate [(Du et al., 2023)](#b9), or using meticulously crafted prompt constructions [(Zhang et al., 2024)](#b34) to integrate knowledge library and skill planning. Naturally, the improvement in the specialization and adaptive capabilities of individual LLMs in the ensemble enhances the collective performance. Thus, in this paper, we focus on the microview of self-adaptive LLMs.

Microview: MoE in LLMs plays a critical role in this perspective [(Tianlong et al., 2024)](#). In MoE systems, inputs are dynamically routed to a subset of specialized modules or layers (e.g., MLPs) containing domain-specific knowledge [(Rajbhandari et al., 2022;](#b26)[Fedus et al., 2022)](#b10). To reduce inference time, researchers introduce sparsely activated MoE where only a subset of the experts are selected per token Jiang et al. (2024); Qwen [Team (2024)](#b25). While it is possible to view Transformer 2 loosely as a type of MoE, there are two major differences. In the aforementioned systems, selfadaptation is achieved through token-level routing, whereas Transformer 2 employs a sample-level module selection strategy. The second difference lies in the construction of expert modules. In traditional MoE systems, expert modules are either trained from scratch [(Fedus et al., 2022;](#b10)[Jiang et al., 2024)](#b14) or dense models (e.g., upcycling) (Qwen [Team, 2024;](#b25)[Zhu et al., 2024)](#b36), without an auxiliary loss to ensure module specialization. In contrast, Transformer 2 specifically trains expert vectors with RL to acquire domain specific-knowledge, making them true experts.

Low-rank adaptation PEFT methods such as LoRA [(Hu et al., 2021)](#b13) works by freezing the original model's parameters and introducing small trainable low-rank matrices for task-specific updates. It significantly lowers the computational and memory costs while providing performance comparable to full fine-tuning. Inspired by LoRA's design, various modifications have been proposed [(Zhang et al., 2023;](#b35)[Kopiczko et al., 2023;](#b18)[Liu et al., 2024;](#b21)[Bałazy et al., 2024;](#b2)[Cetoli, 2024)](#b4). Transformer 2 does not rely on low-rank matrices, and instead scales the singular vectors of the original parameter matrix that span the full rank space.

SVD for LLM Fine-tuning SVD is increasingly being used as an inductive bias for PEFT in LLMs. For example, [Wang et al. (2024)](#b31) decompose a weight matrix and use the minor singular components, associated with noisy or long-tail information, to initialize low-rank matrices for LoRA fine-tuning. In a similar vein, SVD is employed to approximate an original weight matrix with the top r singular vectors, corresponding to the highest singular values. A small trainable matrix is then introduced on top of the truncated singular value matrix to adjust the magnitude and orientations within this top-r subspace [(Bałazy et al., 2024;](#b2)[Cetoli, 2024)](#b4). However, the drawback of this approach is that retaining only the top singular components can result in the loss of important information, particularly when the singular values distribution is less skewed. The work most similar to ours is a concurrent effort by [Lingam et al. (2024)](#b19), where they introduce various sparsification methods that utilize the SVD of the weights. However, it is not for self-adaptive LLMs and does not use RL to enhance learning efficiency.

## METHODS

## PRELIMINARIES

Singular value decomposition (SVD) offers a fundamental view of matrix multiplications. In the context of neural networks, each weight matrix W ∈ R n×m can be decomposed into three components W = U ΣV ⊺ , yielding semi-orthogonal matrices U ∈ R m×r and V ∈ R n×r together with an ordered vector of r singular values (in descending order) arranged in the diagonal matrix Σ ∈ R r×r . The linear operation defined by applying W onto x, can be then decomposed into a sum of indepen-Preprint dent terms, derived from mapping each column v i from V into the corresponding column u i from U as y = r i=1 σ i u i v ⊺ i x. Hence, each singular component represented by the rank-1 matrix u i v ⊺ i independently processes the input, providing an orthogonal contribution to the layer's outputs, with the singular values σ i modulating the degree of the contributions.

Cross-entropy method (CEM) is a Monte Carlo method for importance sampling and optimization [(Rubinstein & Kroese, 2004)](#b27). The method is based on the concept of minimizing the KL divergence between two probability distributions D KL (P ∥Q), where P is the target distribution and Q is a maintained distribution. At its core, CEM repeatedly generates a set of samples from Q, evaluates these samples with a performance function, and then updates the distribution Q with the characteristics of the elite samples that have performed best. In the standard setup employed in most applications, Q is set to a diagonal multivariate Gaussian, reducing the problem to simply estimating the empirical mean and standard deviation of the latest elites until a stopping criterion is met. We illustrate a complete CEM step in the Python pseudocode below.

## TRANSFORMER 2

The construction of Transformer 2 comprises two main steps, for which we provide an illustrative overview in Figure [2](#fig_3). First, we introduce Singular Value Fine-tuning (SVF), a method to learn with RL compact and compositional expert vectors based on the SVD of the base model's weights. Then, we describe three different adaptation strategies within Transformer 2 , inspired by three orthogonal principles, which adaptively combine the SVF-trained expert vectors during inference. We motivate how the properties of SVF are highly complementary to our adaptation strategies, making Transformer 2 an effective and scalable framework for the design of new self-adaptive LLMs.



## V |



## V |



## Frozen parameters

## Training Time Inference Time


## ↵1⇥

Replaced with a mixture of the learned vectors Singular value fine-tuning is a key building block in Transformer 2 . It offers an extremely efficient parameterization for fine-tuning and provides inherent compositionality for adaptation. Conventional fine-tuning techniques often aim to augment pre-trained models with new capabilities by modifying their weight matrices. However, in large-scale transformers, these weights are already rich repositories of abstracted knowledge, thanks to the breadth of the pre-training data and expansive architectural design. In fact, as evidenced in much of the prior literature, the requisite capabilities for solving many downstream tasks appear to already exist within these pre-trained models [(Sharma et al., 2023)](#b28). Therefore, instead of seeking to add new features, an efficient fine-tuning approach should focus on making these latent capabilities more expressible. Motivated by these considera-Preprint tions, for any weight matrix W , SVF learns a simple vector z ∈ R r that provides targeted modifications to each singular component of W independently, yielding a new weight matrix W ′ = U Σ ′ V ⊺ , where Σ ′ = Σ ⊗ diag(z). This essential parameterization enjoys several benefits:

Negligible parameters: Learning only a vector z for each weight matrix allows for very efficient fine-tuning with orders of magnitudes fewer optimized parameters even when compared to prior approaches specifically designed for efficiency. For example, the widely popular LoRA approach requires (m+n)×r ′ learnable parameters per weight matrix, where r ′ is a hyper-parameter that generally needs to be set large enough for expressivity. While recent extensions, such LoRA-XS [(Bałazy et al., 2024)](#b2), try to push efficiency even further, they often introduce limiting assumptions that curb applicability in several practical scenarios (see examples in Appendix C). In contrast, while SVF only needs r = min(m, n) parameters, we show it empirically does not display the same shortcomings thanks to working on a highly-meaning space provided by the latent expressiveness compressed in the weights of modern LLMs. SVF's scaling only the singular values may seem to lead to limited expressiveness, we wish to point out that the ability to affect the weight matrix in a full-rank manner technically provides more information than low-rank approaches.

High compositionality: Decomposing the weights in independent singular components makes the learned z vectors highly composable and interpretable, opening numerous possibilities for adaptation via algebraic manipulations. Instead, LoRA-based methods inherently lack these properties. For instance, even if two LoRAs learned on the same task were to learn exactly the same adjustments for each W , directly interpolating between their compressed A and B matrices is unlikely to preserve any of their original behavior, given the countless number of equivalent parameter permutations they might have converged to.

Principled regularization: Exclusively modifying the magnitude of pre-existing singular components provides a principled and effective form of regularization. In practice, this property enables us to fine-tune for arbitrary downstream tasks with only hundreds of data points without the risk of severe collapse or overfitting.

End-to-end optimization with RL. We train a set of SVF vectors θ z = {z 1 , • • • , z N ×M } to finetune an arbitrary language model π θ W parameterized by θ W with RL, optimizing directly for task performance. Here, θ W = {W 1 , • • • , W N ×M } is the set of weight matrices, where N is the number of layers and M is the number of weight matrices to fine-tune per layer. We use the seminal [RE-INFORCE algorithm (Williams, 1992)](#) and label each generated answer y i (for the prompt x i ∈ D) with a unitary reward based on its correctness r ∈ {-1, 1}. Inspired by related applications of RL for optimizing LLMs [(Ouyang et al., 2022)](#b24), we regularize the REINFORCE objective by adding a KL penalty for deviating from the original model's behavior, weighted by a small coefficient λ ∈ R + . Thus, our final objective function can be written as:

$J(θ z ) = E log π θ W ′ (ŷ i | x i ) r(ŷ i , y i ) -λD KL (π θ W ′ ∥π θ W ),(1)$where we use π θ W ′ to denote the resulting language model after substituting the original weight matrices W with W ′ . While RL is generally considered less stable than next-token prediction objectives, we find the regularization properties of SVF avoid many of the failure modes of prior lessconstrained parameterizations (see Section 4.3). Thus, combining these complementary components effectively enables us to avoid relying on expensive fine-tuning procedures with large hand-designed datasets as proxies, and directly maximize task performance end-to-end.

In general, SVF with RL puts lower requirement on the dataset it trains on. For example, LoRA fine-tuning requires "explaining texts" to perform next token predictions, which puts a higher requirement on the dataset (e.g., imagine LoRA fine-tuning on a GSM8K dataset where no reasoning text but only the final number is provided). This benefit allows SVF to be more general and effective. One possible caveat SVF can face is the sparse rewards caused by a weak base model, which we discuss this further in Section 5.

Self-adaptation is a critical mechanism in nature that has established itself as a core guiding principle in modern system design [(Klös et al., 2015)](#b17). Our initial efforts toward self-adaptive foundation models focus on the inference stage of LLMs, where we devise a simple two-pass adaptation strategy that combines K sets of base "expert" vectors z 1:K trained with SVF to provide different kinds of capabilities [(e.g., coding, math, etc)](#). The mapping between a capability and the dataset we train on can be acquired in the dataset's meta data. In the first inference pass, given a task or an individual input prompt, Transformer 2 executes the model and observes its test-time behavior to derive a new z ′ vector tailored to its test-time conditions. This adapted z ′ is then used in the second inference pass to provide an actual response with the newly adapted weights. The interaction between SVF-trained expert vectors and the adaptation strategies ensures seamless integration, where expert vectors provide modular capabilities, and the adaptation strategies dynamically determine and compose the most suitable combination to address the input task. In this first work, we propose three simple approaches to produce the vector z ′ during the first inference pass, implementing selfadaption with distinct methods and requirements. Below, we provide an outline of each method and refer to Appendix A for additional implementation details.

A) Prompt engineering: Our most basic approach involves constructing a new "adaptation" prompt which we use to directly ask the LLM to categorize the input prompt. Based on its response, we then extract one category out of the set of domain topics used to pre-train each SVF expert and, thus, we select the corresponding z ′ directly from z 1:K . In our adaptation prompt, we also explicitly provide the option for a generic "others" category, allowing the model to use its base weights in case no expert provides appropriate capabilities. We show the format used to construct the adaptation prompt in Figure [3](#fig_4).

Analyze the given question and classify it into one of four categories: 'code', 'math', 'reasoning', or 'others'. Follow these guidelines:

1. Code: Questions asking for programming solutions... 2. Math: Questions involving mathematical calculations... 3. Reasoning: Questions requiring logical thinking.... 4. Others: Questions not clearly fit into above categories...

## Instructions:

-Consider the primary focus, skills, and knowledge required to answer the question.

-If a question spans multiple categories, choose the most dominant one.

-Provide your final classification within \\boxed{} notation. Example: \ \boxed{reasoning} Format your response as follows: Classification: \\boxed{category} B) Classification expert: A direct extension of the prompt engineering approach comes from using a specialized system to handle task identification. Following the principles of self-adaptation, we apply SVF to fine-tune the base LLM itself to handle this task. In particular, we collect a dataset

$D = {(x 1,1 , 1), • • • , (x i,k , k), • • • } from the K SVF training tasks$, where x i,k is the i-th example from the k-th expert task. Each tuple (x i,k , k) then forms an example to pre-train an additional job classification expert z c learned in the same fashion as the others. During the first inference pass, we simply load z c , intending to improve the inherent task classification capabilities of the base model to select a more appropriate z ′ to handle the input prompt.

C) Few-shot adaptation: Our third approach leverages additional task information by assuming extended access to its test-time conditions beyond individual prompts. Our approach is inspired by popular few-shot prompting techniques, which have been shown to provide consistent performance improvements and even allow LLMs to "in-context" learn tasks that were entirely unseen prior to inference [(Brown, 2020)](#b16). For each optimized W , our approach entails producing an entirely new z ′ = K k=1 α k z k by linearly interpolating between the K learned SVF vectors, each weighted by the coefficients α k . We employ CEM to search over the possible values of each α k based on the performance on a set of "few-shot prompts", which are specifically held out from the rest of the test prompts and used to evaluate CEM's population samples. In the case of multiple population samples obtaining the same score on these held-out prompts, we break ties by favoring the one with the highest average log-likelihood across its own generated correct answers. Crucially, we only need to perform this process once for each target task, avoiding the need to increase the length of each question prompt, a relevant downside of traditional few-shot prompting. We refer to Section A.4, for additional details and an extended discussion of this final approach.

## EXPERIMENTS

We extensively evaluate Transformer 2 on multiple tasks and models with the purpose of: (1) assessing the efficiency and effectiveness of SVF; (2) demonstrating self-adaptiveness through the three proposed adaptation strategies; (3) conducting in-depth analysis and ablation studies aimed at understanding and interpreting the properties of our new framework.

## EXPERIMENTAL SETUPS

To validate the generality of Transformer 2 we consider three pre-trained LLMs ranging across different model families and architecture sizes: LLAMA3-8B-INSTRUCT, MISTRAL-7B-INSTRUCT-V0.3, and LLAMA3-70B-INSTRUCT. For each model, we obtain three sets of SVF-trained z vectors to maximize performance for GSM8K [(Cobbe et al., 2021)](#b7), MBPP-pro [(Austin et al., 2021)](#b1), In our experiments, we update the parameters at the end of each epoch.

and ARC-Easy [(Clark et al., 2018)](#b6), respectively. Additionally, we also train a set of z vectors for LLAMA3-8B-INSTRUCT, when applied as the language backbone for TextVQA [(Singh et al., 2019)](#b29), in order to assess SVF's applicability to the vision-language modeling (VLM) domain. We provide SVF's main learning curves on each of these tasks in Figure [4](#fig_5). Finally, we evaluate the full Transformer 2 adaptation framework on four unseen tasks: MATH [(Hendrycks et al., 2021)](#b12), Humaneval [(Chen et al., 2021)](#b5), ARC-Challenge [(Clark et al., 2018)](#b6), and OKVQA [(Marino et al., 2019)](#b23). In all our adaptation experiments, we only consider experts obtained in the pure-language settings, assessing its test-time applicability even for the distinctive vision domain. Please refer to the Appendix A for additional details and a summary of the hyper-parameters used in the experiments.

## EXPERIMENTAL RESULTS

## SVF performance

We provide results after training on each considered task with the LLAMA3-8B-INSTRUCT, MISTRAL-7B-INSTRUCT-V0.3, and LLAMA3-70B-INSTRUCT base models in Table 1. Remarkably, we find that SVF provides considerable and consistent performance gains across nearly all tasks and base models. Instead, LoRA experts yield smaller gains and even sporadic performance degradation. (These LoRA experts are trained with next token prediction. While we also have LoRA experts trained with RL in Table [4](#tab_3), RL seems work less well with LoRA than with SVF.) This observed trend extends also to the vision-language domain, as fine-tuning LLAMA3-LLAVA-NEXT-8B with SVF bolsters the base model's performance by over 39% (see Figure [5](#)). To ensure a fair comparison, we provide extensive ablations to both our model and the LoRA baseline considering different architecture and optimization objectives in Appendix 4.3). Due to its essential parameterization, we would like to note that training SVF requires considerably fewer resources, with less than 10% of the training parameters of our LoRA implementation.

Adaptation performance With the SVF trained z vectors, we assess the self-adaptation capability of Transformer 2 on unseen tasks. For a fair comparison with LoRA, we record the performance of this baseline using all checkpoints from the considered training tasks and report only its highest performance for each of the test tasks. As shown in ARC-Challenge task and still significantly deteriorate performance on both MATH and Humaneval. This discrepancy suggests that LoRA's parameterization and optimization might be particularly sensitive to overfitting, especially when trained with the smaller GSM8K and MBPP-Pro datasets, the tasks that provide information most related to MATH and Humaneval. In Figure [5](#), we find a similar dichotomy in the OKVQA task, with the performance of the base LLAMA3-LLAVA-NEXT-8B VLM only improving after applying Transformer 2 . We note that also in this setting, Transformer 2 performs self-adaptation only from the expert vectors from GSM8K, MBPP-Pro, and ARC-Easy. Thus, this result further underscores the high flexibility of self-adaptation, transferring knowledge compressed for tasks entirely based on language even for unrelated vision-based problems.

Comparing the three proposed adaptation strategies, we highlight a clear monotonic trend -with more involved strategies and additional information about the test-time condition, self-adaptation appears to be increasingly effective. In particular, Transformer 2 with few-shot self-adaptation is almost always the highest-scoring method, providing notable improvements across all tested settings except for LLAMA3-70B-INSTRUCT @MATH, where we have only SVF-tuned half of the layers due to our limited GPU resources. This trend shows that providing additional or different kinds of information seems to be highly beneficial to our framework, suggesting that Transformer 2 could provide foundation models with new means to continually improve performance when deployed in lifelong settings. Table [3](#tab_2) reports the inference time required by the prompt adaptation strategy of Transformer 2 , with the time spent on solving the entire problem set presented separately for the 1st and 2nd passes. Notice that the 2nd pass inference time is the time spent on solving the problems, and the 1st pass inference time is the time for self-adaptation, 1st to 2nd pass inference time ratios are in the parentheses. While the additional inference pass might appear to double the overall runtime, it is important to note that inference time primarily depends on the number of tokens generated. In our settings, it is O(n) where n is the length of the input. ARC-challenge's cost ratio is large because they are single choice problems and therefore the cost of the 2nd pass is also O(n). In general settings, we think it is reasonable to assume this ratio to be closer to those of MATH and Humaneval. For a detailed discussion on improving the efficiency of CEM few-shot adaptation methods, please see Appendix D

## ANALYSIS

Lastly, we analyze and discuss the properties of our adaptation strategies for which we provide extensions and further discussion Appendix B.

Analysis 1: Job dispatching accuracy In Figure [6](#) we provide the confusion matrices of our classification-based adaptation strategies. These results validate the effectiveness of both our classification-based adaptation strategies to match each prompt with experts trained in similar domains, as evidenced by the high values along the diagonals. Furthermore, the results from LLAMA3-Figure [6](#): Confusion matrices. These matrices display the classification percentages, where rows represent the task classes (ground truth) and columns indicate the predicted categories. Some samples are misclassified as "Others," which is reflected in rows where the totals do not sum to one. 8B-INSTRUCT and MISTRAL-7B-INSTRUCT-V0.3 also show that using the classification expert consistently provides higher classification accuracy than vanilla prompt engineering. While this difference could explain the higher performance of the relative self-adaptation strategy, we also note that domain similarity might not be the only metric relevant to identifying the best expert for each prompt or task. To this end, we believe many further unexplored extensions could be explored in future work, using heuristics such as past expert performance or token-level analysis to further push our framework's scalability.

Analysis 2: Training tasks adaptation contribution In Figure [7](#fig_6), we show the normalized adaptive coefficients a k interpolating between our SVF vectors learned via CEM for LLAMA3-8B-INSTRUCT and MISTRAL-7B-INSTRUCT-V0.3 across all the unseen downstream tasks. Intuitively, we find that the expert vectors from the training tasks sharing similar topics to the unseen ones are often the highest contributors to the produced adaptive weights. However, we observe that the MATH task appears as an interesting exception, as the a k for the expert obtained from GSM8K training is actually the lowest out of the three in both models. We hypothesize this reflects the different nature of the mathematics competition problems from MATH as compared to the grade-school problems in GSM8K. In fact, not only is the difficulty of the MATH questions far beyond GSM8K, but a large portion of its problems also hinges mainly on logical reasoning, for which a task like ARC might actually be more aligned. Furthermore, we also note that the different z vectors appear to contribute more uniformly to adaptation in the Llama model. This difference might indicate that, due to its higher base performance, the Llama model does not need to rely on any particular set of skills as much as Mistral, and can harness more holistic benefits from self-adaptation. Note that applying a k uniformly is not a universal solution for leveraging expert vectors. This becomes evident when we look at different model and task combinations (e.g. applying a k uniformly on LLAMA3-8B-INSTRUCT for MATH tasks only achieves 24.47, while Transformer 2 (Few-shot) achieves 25.47).

## Analysis 3: Ablation studies

Module sensitivity: We first compare the performance of SVF when it is applied to different modules (see trials 1-3). Under consistent conditions, both individual MLP and attention updates improve performance, with MLP updates resulting in more pronounced gains. Simultaneous updates to both module types yield even more significant enhancements.

Objective function: We are interested in the performance impact from different objective functions, and we compare the RL objective with next-token prediction loss (see trials 2 and 4). For the latter, we use instruction fine-tuning with official GSM8K solutions as target tokens. Results show clear performance gains with RL, demonstrating its effectiveness in task-specific fine-tuning. Conversely, next-token prediction even hinders performance. This highlights RL's ability to handle cases lacking detailed solutions, suggesting its superiority in this context.

SVF vs LoRA: Finally, we also evaluate LoRA using the RL objective (see trials 2 and 5). A significant performance disparity is observed, primarily attributed to the severe instability of the LoRA training process. Despite exploring a wide range of learning rates, LoRA's performance consistently lagged behind. For further illustrations, see Figure [9](#) in the appendix.

Analysis 4: Cross-model compatibility Finally, we explore the potential for our self-adaptation framework to be applied across different LLMs. In particular, we evaluate whether the SVF expert vectors trained on LLAMA3-8B-INSTRUCT can benefit MISTRAL-7B-INSTRUCT-V0.3, and whether we can perform adaptation across the expert vectors of these two models. We present our main findings in Table [5](#tab_4) and refer to Appendix B for additional detailed results. Surprisingly, we find that positive transfer occurs across the two models, with visible benefits in 2 out of 3 tasks. We   This operation leads to notable performance degradation across each task. Finally, by performing few-shot adaptation using the SVF vectors collected from both models, the performance of MISTRAL-7B-INSTRUCT-V0.3 further improves across the board. We observe that these gains even surpass the best score from adapting MISTRAL-7B-INSTRUCT-V0.3 with all the SVF vectors in the ARC-Challenge task reported in Table [2](#tab_1). While these results appear promising, we note that the surprising compatibility discovered through our naive transfer approach is potentially tied to the similarity between the architectures of the two considered LLMs. To this end, whether similar transfer can be replicated with models of different scales remains an open research question that could open the doors to disentangling and recycling task-specific skills for newer/larger models, with important implications for democratization and sustainability.

## CONCLUSION

In this paper, we introduced Transformer 2 , providing a novel blueprint toward realizing self-adaptive LLMs. Within this framework, we first proposed SVF, offering superior performance than prior finetuning recipes, together with reduced costs, high compositionality, and overfitting regularizationall crucial properties to achieve scalable self-adaptation. Leveraging a set of SVF experts as building blocks, we developed three effective strategies for self-adaptation, each offering unique benefits and monotonic performance benefits with increasing access to the test-time conditions. While Transformer 2 demonstrates promising results, there remain exciting opportunities for future work. One limitation is that the capabilities of SVF experts are tied to the latent components of the base model. To address this, model merging offers a promising direction [(Yu et al., 2024;](#b33)[Goddard et al., 2024;](#b11)[Akiba et al., 2024)](#b0), enabling specialized models to be combined into a single, more capable model. Additionally, while our CEM-based adaptation effectively balances performance and efficiency, scaling to a large number of specialized domains may introduce increased one-time computational costs. However, this trade-off is offset by the benefits of improved performance and enhanced self-adaptation capabilities. Advances in model merging and efficient adaptation techniques have produced models dominating open leaderboards, making them strong candidates as base models for Transformer 2 and opening new possibilities for adaptive LLMs. 2 × 10 -4 , 5 × 10 -4 , 2 × 10 -5 , 5 × 10 -5 , 2 × 10 -6 . 5 × 10 -6 , Clip max norm 1 × 10 -3 , 1.0  We investigate the relationship between the number of samples available for fewshot adaptation and downstream performance. Our analysis focused on the test task where LLAMA3-8B-INSTRUCT demonstrates the highest baseline performance, to prevent the potential for a null signal in our CEM-based search.

As Table [8](#tab_7) shows, substantial benefits of our few-shot strategy are evident with as few as 3 to 5 test samples. Moreover, performance appears to plateau beyond 10 samples, underscoring how our essential and inherently regularized SVF pa-

![r S x K r e d b M 4 p O F P Z y + D O I X f 5 M a 5 8 P x y P y 5 3 s Z 6 s b 0]()

![r S x K r e d b M 4 p O F P Z y + D O I X f 5 M a 5 8 P x y P y 5 3 s Z 6 s b 0]()


![Figure 2: Method overview. Left) At training time, we employ SVF and RL to learn the "expert" vectors z's that scale the singular values of the weight matrices. Right) At inference time, we propose three distinct methods to adaptively select/combine the learned expert vectors.]()

![Figure 3: Prompt based adaptation. Selfadaptation prompt used by Transformer 2 to classify the task prompt into pre-defined categories.]()

![Figure 4: SVF learning curves. The dashed lines indicate the performance of LLAMA3-8B-INSTRUCT on the test split of each task. SVF effectively fine-tunes to surpass the base performance. While we use the best validation score to select our checkpoint for evaluation (marked by red dots), we present the entire training curve without early stopping to demonstrate SVF's learning capabilities. Tasks with only hundreds of training samples like Coding and Reasoning were stopped early.In our experiments, we update the parameters at the end of each epoch.]()

![Figure 7: α k learned weights.]()

![Table2, all of our Transformer 2 adaptation strategies demonstrate improvements across all tasks for LLAMA3-8B-INSTRUCT base models, and in at least two out of three tasks for both MISTRAL-7B-INSTRUCT-V0.3 and LLAMA3-70B-INSTRUCT. In contrast, even the best training LoRAs only provide marginal improvements on the Fine-tuning results. LLM performance on the test splits of math, coding and reasoning. Normalized scores are in the parentheses.]()

![Self-adaptation on unseen tasks. Normalized scores are in the parentheses.]()

![Time]()

![Ablation studies. We fine-tune LLAMA3-8B-INSTRUCT on the GSM8K training split with different settings and the results on the test split along with zero-shot transfer results on MATH.]()

![Cross-model z vector transfer. Results from transferring the expert vectors trained on LLAMA3-8B-INSTRUCT to MISTRAL-7B-INSTRUCT-V0.3 with cross model few-shot adaptation. Llama SVF (ordered σ i ) 11.96 (0.92) 45.12 (1.04) 72.01 (1.00) + Llama SVF (shuffled σ i ) 10.52 (0.81) 40.24 (0.93) 70.82 (0.99) + Few-shot adaptation (cross-model) 12.65 (0.97) 46.75 (1.08) 75.64 (1.05)note these improvements are due to the inherent ordering of the SVF parameterization, as randomly shuffling each SVF vector before applying it to the Mistral model consistently degrades performance.]()

![Hyper-parameters used for SVF and LoRA training. We perform a sweep on certain sensitive hyper-parameters across methods for fair comparison.]()

![Additional Comparison Experiment. Normalized scores are in the parentheses. (Cls-expert) 25.18 (1.03) 62.80 (1.03) 81.37 (1.01) + Transformer 2 (Few-shot) 25.47 (1.04) 62.99 (1.03) 82.61 (1.02) B ADDITIONAL RESULTS B.1 BASELINE COMPARISON TO MORE PEFT METHODS We conduct additional comparison studies against more parameter-efficient fine-tuning methods, including IA3Liu et al. (2022), DORA. Liu et al. (2024).As Table7shows, SVF still outperforms other methods and shows promising generalized performance.B.2 IMPACT FROM NUMBER OF FEW-SHOTS]()

![Few-shot adaptation scaling on the Arc-Challenge task. Performance varies with number of examples.]()

