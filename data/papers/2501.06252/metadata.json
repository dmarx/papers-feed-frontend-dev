{
  "arxivId": "2501.06252",
  "title": "$\\text{Transformer}^2$: Self-adaptive LLMs",
  "authors": "Qi Sun, Edoardo Cetin, Yujin Tang",
  "abstract": "Self-adaptive large language models (LLMs) aim to solve the challenges posed\nby traditional fine-tuning methods, which are often computationally intensive\nand static in their ability to handle diverse tasks. We introduce\n$\\text{Transformer}^2$, a novel self-adaptation framework that adapts LLMs for\nunseen tasks in real-time by selectively adjusting only the singular components\nof their weight matrices. During inference, $\\text{Transformer}^2$ employs a\ntwo-pass mechanism: first, a dispatch system identifies the task properties,\nand then task-specific \"expert\" vectors, trained using reinforcement learning,\nare dynamically mixed to obtain targeted behavior for the incoming prompt. Our\nmethod outperforms ubiquitous approaches such as LoRA, with fewer parameters\nand greater efficiency. $\\text{Transformer}^2$ demonstrates versatility across\ndifferent LLM architectures and modalities, including vision-language tasks.\n$\\text{Transformer}^2$ represents a significant leap forward, offering a\nscalable, efficient solution for enhancing the adaptability and task-specific\nperformance of LLMs, paving the way for truly dynamic, self-organizing AI\nsystems.",
  "url": "https://arxiv.org/abs/2501.06252",
  "issue_number": 975,
  "issue_url": "https://github.com/dmarx/papers-feed/issues/975",
  "created_at": "2025-01-15T03:51:49.310517",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 48,
  "last_read": "2025-01-15T05:05:34.185386",
  "last_visited": "2025-01-15T05:04:30.386000+00:00",
  "main_tex_file": null,
  "published_date": "2025-01-09T01:19:21Z",
  "arxiv_tags": [
    "cs.LG",
    "cs.AI",
    "cs.CL"
  ]
}