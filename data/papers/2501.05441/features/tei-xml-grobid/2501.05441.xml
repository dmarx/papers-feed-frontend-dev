<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The GAN is dead; long live the GAN! A Modern Baseline GAN</title>
				<funder>
					<orgName type="full">Brown University Division of Research Seed Award</orgName>
				</funder>
				<funder ref="#_Yrhxp46">
					<orgName type="full">NIH MIRA</orgName>
				</funder>
				<funder ref="#_Pdj4nME #_Jm7zeDT">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yiwen</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Brown University</orgName>
								<orgName type="institution" key="instit2">Cornell University</orgName>
								<orgName type="institution" key="instit3">Cornell University</orgName>
								<orgName type="institution" key="instit4">Brown University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aaron</forename><surname>Gokaslan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Brown University</orgName>
								<orgName type="institution" key="instit2">Cornell University</orgName>
								<orgName type="institution" key="instit3">Cornell University</orgName>
								<orgName type="institution" key="instit4">Brown University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Volodymyr</forename><surname>Kuleshov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Brown University</orgName>
								<orgName type="institution" key="instit2">Cornell University</orgName>
								<orgName type="institution" key="instit3">Cornell University</orgName>
								<orgName type="institution" key="instit4">Brown University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">James</forename><surname>Tompkin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Brown University</orgName>
								<orgName type="institution" key="instit2">Cornell University</orgName>
								<orgName type="institution" key="instit3">Cornell University</orgName>
								<orgName type="institution" key="instit4">Brown University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">The GAN is dead; long live the GAN! A Modern Baseline GAN</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">695AEB0B4AD2FB6B3F392F1F155098D4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>There is a widely-spread claim that GANs are difficult to train, and GAN architectures in the literature are littered with empirical tricks. We provide evidence against this claim and build a modern GAN baseline in a more principled manner. First, we derive a well-behaved regularized relativistic GAN loss that addresses issues of mode dropping and non-convergence that were previously tackled via a bag of ad-hoc tricks. We analyze our loss mathematically and prove that it admits local convergence guarantees, unlike most existing relativistic losses. Second, this loss allows us to discard all ad-hoc tricks and replace outdated backbones used in common GANs with modern architectures. Using StyleGAN2 as an example, we present a roadmap of simplification and modernization that results in a new minimalist baseline-R3GAN ("Re-GAN"). Despite being simple, our approach surpasses StyleGAN2 on FFHQ, ImageNet, CIFAR, and Stacked MNIST datasets, and compares favorably against state-of-the-art GANs and diffusion models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Generative adversarial networks (GANs) let us generate high-quality images in a single forward pass. However, the original objective in Goodfellow et al. <ref type="bibr" target="#b12">[13]</ref>, is notoriously difficult to optimize due to its minimax nature. This leads to a fear that training might diverge at any point due to instability, and a fear that generated images might lose diversity through mode collapse. While there has been progress in GAN objectives <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b81">81,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b64">64]</ref>, practically, the effects of brittle losses are still regularly felt. This notoriety has had a lasting negative impact on GAN research.</p><p>A complementary issue-partly motivated by this instability-is that existing popular GAN backbones like StyleGAN <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32]</ref> use many poorly-understood empirical tricks with little theory. For instance, StyleGAN uses a gradient penalized non-saturating loss <ref type="bibr" target="#b52">[52]</ref> to increase stability (affecting sample diversity), but then employs a minibatch standard deviation trick <ref type="bibr" target="#b27">[28]</ref> to increase sample diversity. Without tricks, the StyleGAN backbone still resembles DCGAN <ref type="bibr" target="#b60">[60]</ref> from 2015, yet it is still the common backbone of SOTA GANs such as GigaGAN <ref type="bibr" target="#b25">[26]</ref> and StyleGAN-T <ref type="bibr" target="#b70">[70]</ref>. Advances in GANs have been conservative compared to other generative models such as diffusion models <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b78">78,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>, where modern computer vision techniques such as multi-headed self attention <ref type="bibr" target="#b87">[87]</ref> and backbones such as preactivated ResNet <ref type="bibr" target="#b16">[17]</ref>, U-Net <ref type="bibr" target="#b63">[63]</ref> and vision transformers (ViTs) <ref type="bibr" target="#b8">[9]</ref> are the norm. Given outdated backbones, it is not surprising that there is a widely-spread belief that GANs do not scale in terms of quantitative metrics like Frechet Inception Distance <ref type="bibr" target="#b18">[19]</ref>.</p><p>We reconsider this situation: we show that by combining progress in objectives into a regularized training loss, GANs gain improved training stability, which allows us to upgrade GANs with modern backbones. First, we propose a novel objective that augments the relativistic pairing GAN loss (RpGAN; <ref type="bibr" target="#b21">[22]</ref>) with zero-centered gradient penalties <ref type="bibr" target="#b52">[52,</ref><ref type="bibr" target="#b64">64]</ref>, improving stability <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b64">64,</ref><ref type="bibr" target="#b52">52]</ref>. We show mathematically that gradient-penalized RpGAN enjoys the same guarantee of local convergence as regularized classic GANs, and that removing our regularization scheme induces non-convergence.</p><p>Once we have a well-behaved loss, none of the GAN tricks are necessary <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b30">31]</ref>, and we are free to engineer a modern SOTA backbone architecture. We strip StyleGAN of all its features, identify those that are essential, then borrow new architecture designs from modern ConvNets and transformers <ref type="bibr" target="#b48">[48,</ref><ref type="bibr" target="#b97">97]</ref>. Briefly, we find that proper ResNet design <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b67">67]</ref>, initialization <ref type="bibr" target="#b99">[99]</ref>, and resampling <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b100">100]</ref> are important, along with grouped convolution <ref type="bibr" target="#b95">[95,</ref><ref type="bibr" target="#b4">5]</ref> and no normalization <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b88">88,</ref><ref type="bibr" target="#b3">4]</ref>. This leads to a design that is simpler than StyleGAN and improves FID performance for the same network capacity (2.75 vs. 3.78 on FFHQ-256).</p><p>In summary, our work first argues mathematically that GANs need not be tricky to train via an improved regularized loss. Then, it empirically develops a simple GAN baseline that, without any tricks, compares favorably by FID to StyleGAN <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref>, other SOTA GANs <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b94">94]</ref>, and diffusion models <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b78">78,</ref><ref type="bibr" target="#b86">86]</ref> across FFHQ, ImageNet, CIFAR, and Stacked MNIST datasets.</p><p>2 Serving Two Masters: Stability and Diversity with RpGAN +R 1 + R 2</p><p>In defining a GAN objective, we tackle two challenges: stability and diversity. Some previous work deals with stability <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref> and other previous work deals with mode collapse <ref type="bibr" target="#b21">[22]</ref>. To make progress in both, we combine a stable method with a simple regularizer that is grounded by theory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Traditional GAN</head><p>A traditional GAN <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b57">57]</ref> is formulated as a minimax game between a discriminator (or critic) D ψ and a generator G θ . Given real data x ∼ p D and fake data x ∼ p θ produced by G θ , the most general form of a GAN is given by:</p><formula xml:id="formula_0">L(θ, ψ) = E z∼pz [f (D ψ (G θ (z)))] + E x∼p D [f (-D ψ (x))]<label>(1)</label></formula><p>where G tries to minimize L while D tries to maximize it. The choice of f is flexible <ref type="bibr" target="#b50">[50,</ref><ref type="bibr" target="#b44">44]</ref>. In particular, f (t) = -log(1 + e -t ) recovers the classic GAN by Goodfellow et al. <ref type="bibr" target="#b12">[13]</ref>. For the rest of this work, this will be our choice of f <ref type="bibr" target="#b57">[57]</ref>.</p><p>It has been shown that Equation 1 has convex properties when p θ can be optimized directly <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b81">81]</ref>. However, in practical implementations, the empirical GAN loss typically shifts fake samples beyond the decision boundary set by D, as opposed to directly updating the density function p θ . This deviation leads to a significantly more challenging problem, characterized by susceptibility to two prevalent failure scenarios: mode collapse/dropping<ref type="foot" target="#foot_1">foot_1</ref> and non-convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Relativistic f -GAN</head><p>We employ a slightly different minimax game named relativistic pairing GAN (RpGAN) by Jolicoeur-Martineau et al. <ref type="bibr" target="#b21">[22]</ref> to address mode dropping. The general RpGAN is defined as:</p><formula xml:id="formula_1">L(θ, ψ) = E z∼pz x∼p D [f (D ψ (G θ (z)) -D ψ (x))]<label>(2)</label></formula><p>Although Eq. 2 differs only slightly from Eq. 1, evaluating this critic difference has a fundamental impact on the landscape of L. Since Eq. 1 merely requires D to separate real and fake data, in the scenario where all real and fake data can be separated by a single decision boundary, the empirical GAN loss encourages G to simply move all fake samples barely past this single boundary-this degenerate solution is what we observe as mode collapse/dropping. Sun et al. <ref type="bibr" target="#b81">[81]</ref> characterize such degenerate solutions as bad local minima in the landscape of L, and show that Eq. 1 has exponentially many bad local minima. The culprit is the existence of a single decision boundary that naturally arises when real and fake data are considered in isolation. RpGAN introduces a simple solution by coupling real and fake data, i.e. a fake sample is critiqued by its realness relative to a real sample, which effectively maintains a decision boundary in the neighborhood of each real sample and hence forbids mode dropping. Sun et al. <ref type="bibr" target="#b81">[81]</ref> show that the landscape of Eq. 2 contains no local minima that correspond to mode dropping solutions, and that every basin is a global minimum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Training Dynamics of RpGAN</head><p>Although the RpGAN landscape result <ref type="bibr" target="#b81">[81]</ref> allows us to address mode dropping, the training dynamics of RpGAN have yet to be studied. The ultimate goal of Eq. 2 is to find the equilibrium (θ * , ψ * ) such that p θ * = p D and D ψ * is constant everywhere on p D . Sun et al. <ref type="bibr" target="#b81">[81]</ref> show that θ * is globally reachable along a non-increasing trajectory in the landscape of Eq. 2 under reasonable assumptions. However, the existence of such a trajectory does not necessarily mean that gradient descent will find it. Jolicoeur-Martineau et al. show empirically that unregularized RpGAN does not perform well <ref type="bibr" target="#b21">[22]</ref>.</p><p>Proposition I. (Informal) Unregularized RpGAN does not always converge using gradient descent.</p><p>We confirm this proposition with a proof in Appendix B. We show analytically that RpGAN does not converge for certain types of p D , such as ones that approach a delta distribution. Thus, further regularization is necessary to fill in the missing piece of a well-behaved loss.</p><p>Zero-centered gradient penalties. To tackle RpGAN non-convergence, we explore gradient penalties as the solution since it is proven that zero-centered gradient penalties (0-GP) facilitate convergent training for classic GANs <ref type="bibr" target="#b52">[52]</ref>. The two most commonly-used 0-GPs are R 1 and R 2 :</p><formula xml:id="formula_2">R 1 (ψ) = γ 2 E x∼p D ∥∇ x D ψ ∥ 2 R 2 (θ, ψ) = γ 2 E x∼p θ ∥∇ x D ψ ∥ 2<label>(3)</label></formula><p>R 1 penalizes the gradient norm of D on real data, and R 2 penalizes the gradient norm of D on fake data. Analysis on the training dynamics of GANs has thus far focused on local convergence <ref type="bibr" target="#b55">[55,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b52">52]</ref>, i.e., whether the training at least converges when (θ, ψ) are in a neighborhood of (θ * , ψ * ). In such a scenario, the convergence behavior can be analyzed <ref type="bibr" target="#b55">[55,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b52">52]</ref> by examining the spectrum of the Jacobian of the gradient vector field</p><formula xml:id="formula_3">(-∇ θ L, ∇ ψ L) at (θ * , ψ * ).</formula><p>The key insight here is that when G already produces the true distribution, we want ∇ x D = 0, so that G is not pushed away from its optimal state, and thus the training does not oscillate. R 1 and R 2 impose such a constraint when p θ = p D . This also explains why earlier attempts at gradient penalties, such as the one-centered gradient penalty (1-GP) in WGAN-GP <ref type="bibr" target="#b13">[14]</ref>, fail to achieve convergent training <ref type="bibr" target="#b52">[52]</ref> as they still encourage D to have a non-zero slope when G has reached optimality.</p><p>Since the same insight also applies to RpGAN, we extend our previous analysis and show that:</p><p>Proposition II. (Informal) RpGAN with R 1 or R 2 regularization is locally convergent subject to similar assumptions as in Mescheder et al. <ref type="bibr" target="#b52">[52]</ref>.</p><p>In Appendix C, our proof similarly analyzes the eigenvalues of the Jacobian of the regularized RpGAN gradient vector field at (θ * , ψ * ). We show that all eigenvalues have a negative real part; thus, regularized RpGAN is convergent in a neighborhood of (θ * , ψ * ) for small enough learning rates <ref type="bibr" target="#b52">[52]</ref>.</p><p>Discussion. Another line of work <ref type="bibr" target="#b64">[64]</ref> links R 1 and R 2 to instance noise <ref type="bibr" target="#b75">[75]</ref> as its analytical approximation. Roth et al. <ref type="bibr" target="#b64">[64]</ref> showed that for the classic GAN <ref type="bibr" target="#b12">[13]</ref> by Goodfellow et al., R 1 approximates convolving p D with the density function of N (0, γI), up to additional weighting and a Laplacian error term. R 2 likewise approximates convolving p θ with N (0, γI) up to similar error terms. The Laplacian error terms from R 1 , R 2 cancel when D ψ approaches D ψ * . We do not extend Roth et al.'s proof <ref type="bibr" target="#b64">[64]</ref> to RpGAN; however, this approach might provide complimentary insights to our work, which follows the strategy of Mescheder et al. <ref type="bibr" target="#b52">[52]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">A Practical Demonstration</head><p>We experiment with how well-behaved our loss is on StackedMNIST <ref type="bibr" target="#b46">[46]</ref> which consists of 1000 uniformly-distributed modes. The network is a small ResNet <ref type="bibr" target="#b16">[17]</ref> for G and D without any normalization layers <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b91">91,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b85">85]</ref>. Through the use of a pretrained MNIST classifier, we can explicitly measure how many modes of p D are recovered by p θ . Furthermore, we can estimate the reverse KL divergence between the fake and real samples D KL (p θ ∥ p D ) via the KL divergence between the categorical distribution of p θ and the true uniform distribution.</p><p>10 0 10 1 10<ref type="foot" target="#foot_2">foot_2</ref> 10<ref type="foot" target="#foot_3">foot_3</ref> Generator loss RpGAN + R1 + R2 GAN + R1 + R2 RpGAN + R1 GAN + R1 Figure 1: Generator G loss for different objectives over training. Regardless of which objective is used, training diverges with only R 1 and succeeded with both R 1 and R 2 . Convergence failure with only R 1 was noted by Lee et al. [42]. Loss # modes↑ D KL ↓ RpGAN +R 1 + R 2 1000 0.0781 GAN +R 1 + R 2 693 0.9270 RpGAN +R 1 Fail Fail GAN +R 1 Fail Fail Table 1: StackedMNIST [46] result for each loss function. The maximum possible mode coverage is 1000. "Fail" indicates that training diverged early on.</p><p>A conventional GAN loss with R 1 , as used by Mescheder et al. <ref type="bibr" target="#b52">[52]</ref> and the StyleGAN series <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref>, diverges quickly (Fig. <ref type="figure">1</ref>). Next, while theoretically sufficient for local convergence, RpGAN with only R 1 regularization is also unstable and diverges quickly 2 . In each case, the gradient of D on fake samples explodes when training diverges. With both R 1 and R 2 , training becomes stable for both the classic GAN and RpGAN. Now stable, we can see that the classic GAN suffers from mode dropping, whereas RpGAN achieves full mode coverage (Tab. 1) and reduces D KL from 0.9270 to 0.0781. As a point of contrast, StyleGAN <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32]</ref> uses the minibatch standard deviation trick to reduce mode dropping, improving mode coverage from 857 to 881 on StackedMNIST 3 and with barely any improvement on D KL <ref type="bibr" target="#b27">[28]</ref>.</p><p>R 1 alone is not sufficient for globally-convergent training. While a theoretical analysis of this is difficult, our small demonstration still provides insights into the assumptions of our convergence proof.</p><p>In particular, the assumption that (θ, ψ) are sufficiently close to (θ * , ψ * ) is highly unlikely early in training. In this scenario, if D is sufficiently powerful, regularizing D solely on real data is not likely to have much effect on D's behavior on fake data and so training can fail due to an ill-behaved D on fake data. This observation has been made by previous studies <ref type="bibr" target="#b84">[84,</ref><ref type="bibr" target="#b83">83]</ref> specifically for empirical GAN training, that regularizing an empirical discriminator with only R 1 leads to gradient explosion on fake data due to the memorization of real samples.</p><p>Thus, the practical solution is to regularize D on both real and fake data. The benefit of doing so can be viewed from the insight of Roth et al. <ref type="bibr" target="#b64">[64]</ref>: that applying R 1 and R 2 in conjunction smooths both p D and p θ which makes learning easier than only smoothing p D . We also find empirically that with both R 1 and R 2 in place, D tends to satisfy</p><formula xml:id="formula_4">E x∼p D ∥∇ x D∥ 2 ≈ E x∼p θ ∥∇ x D∥ 2 even early in</formula><p>the training. Jolicoeur-Martineau et al. <ref type="bibr" target="#b22">[23]</ref> show that in this case D becomes a maximum margin classifier-but if only one regularization term is applied, this does not hold. Additionally, having roughly the same gradient norm on real and fake data potentially reduces discriminator overfitting, as Fang et al. <ref type="bibr" target="#b9">[10]</ref> observe that the gradient norm on real and fake data diverges when D starts to overfit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A Roadmap to a New Baseline -R3GAN</head><p>The well-behaved RpGAN + R 1 + R 2 loss alleviates GAN optimization problems, and lets us proceed to build a minimalist baseline-R3GAN-with recent network backbone advances in mind <ref type="bibr" target="#b48">[48,</ref><ref type="bibr" target="#b97">97]</ref>. Rather than simply state the new approach, we will draw out a roadmap from the StyleGAN2 baseline <ref type="bibr" target="#b29">[30]</ref>. This model (Config A; identical to <ref type="bibr" target="#b29">[30]</ref>) consists of a VGG-like <ref type="bibr" target="#b73">[73]</ref> backbone for G, a ResNet D, a few techniques that facilitate style-based generation, and many tricks that serve as patches to the weak backbone. Then, we remove all non-essential features of StyleGAN2 (Config B), apply our loss function (Config C), and gradually modernize the network backbone (Config D-E). We evaluate each configuration on FFHQ 256 × 256 [29]. Network capacity is kept roughly the same for all configurations-both G and D have about 25 M trainable parameters. Each configuration is trained until D sees 5 M real images. We inherit training hyperparameters (e.g., optimizer settings, batch size, EMA decay length) from Config A unless otherwise specified. We tune the training hyperparameters for our final model and show the converged result in Sec. 4. Configuration FID↓ G #params D #params A StyleGAN2 7.516 24.767M 24.001M B Stripped StyleGAN2 -z normalization -Minibatch stddev -Equalized learning rate -Mapping network -Style injection -Weight mod / demod -Noise injection -Mixing regularization -Path length regularization -Lazy regularization 12.46 18.890M 23.996M C Well-behaved Loss + RpGAN loss 11.77 18.890M 23.996M + R 2 gradient penalty 11.65 D ConvNeXt-ify pt. 1 + ResNet-ify G &amp; D 10.17 23.400M 23.282M -Output skips 9.950 23.378M E ConvNeXt-ify pt. 2 + ResNeXt-ify G &amp; D 7.507 23.188M 23.091M + Inverted bottleneck 7.045 23.058M 23.010M The features fall into three categories:</p><p>• Style-based generation: mapping network <ref type="bibr" target="#b28">[29]</ref>, style injection <ref type="bibr" target="#b28">[29]</ref>, weight modulation/demodulation <ref type="bibr" target="#b30">[31]</ref>, noise injection <ref type="bibr" target="#b28">[29]</ref>. • Image manipulation enhancements:</p><p>mixing regularization <ref type="bibr" target="#b28">[29]</ref>, path length regularization <ref type="bibr" target="#b30">[31]</ref>. • Tricks: z normalization <ref type="bibr" target="#b27">[28]</ref>, minibatch stddev <ref type="bibr" target="#b27">[28]</ref>, equalized learning rate <ref type="bibr" target="#b27">[28]</ref>, lazy regularization <ref type="bibr" target="#b30">[31]</ref>.</p><p>Following <ref type="bibr" target="#b69">[69,</ref><ref type="bibr" target="#b70">70]</ref>, we reduce the dimension of z to 64. The absence of equalized learning rate necessitates a lower learning rate, reduced from 2.5×10 -3 to 5×10 -5 . Despite a higher FID of 12.46 than Config-A, this simplified baseline produces reasonable sample quality and stable training. We compare this with DCGAN <ref type="bibr" target="#b60">[60]</ref>, an early attempt at image generation. Key differences include: Experimental findings from StyleGAN. Violating a), b), or c) often leads to training failures. Gidel et al. <ref type="bibr" target="#b10">[11]</ref> show that negative momentum can improve GAN training dynamics. Since optimal negative momentum is another challenging hyperparameter, we do not use any momentum to avoid worsening GAN training dynamics. Studies suggest normalization layers harm generative models <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b33">34]</ref>. Batch normalization <ref type="bibr" target="#b20">[21]</ref> often cripples training due to dependencies across multiple samples, and is incompatible with R 1 , R 2 , or RpGAN that assume independent handling of each sample. Weaker data-independent normalizations <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b33">34]</ref> might help; we leave this for future work. Early GANs may succeed despite violating a) and c), possibly constituting a full-rank solution <ref type="bibr" target="#b52">[52]</ref> to Eq. 1.</p><p>Violations of d) or e) do not significantly impair training stability but negatively affect sample quality. Improper transposed convolution can cause checkerboard artifacts, unresolved even with subpixel convolution <ref type="bibr" target="#b72">[72]</ref> or carefully tuned transposed convolution unless a low-pass filter is applied. Interpolation methods avoid this issue, varying from nearest neighbor <ref type="bibr" target="#b27">[28]</ref> to Kaiser filters <ref type="bibr" target="#b31">[32]</ref>. We use bilinear interpolation for simplicity. For activation functions, smooth approximations of (leaky) ReLU, such as Swish <ref type="bibr" target="#b61">[61]</ref>, GELU <ref type="bibr" target="#b17">[18]</ref>, and SMU <ref type="bibr" target="#b1">[2]</ref>, worsen FID. PReLU <ref type="bibr" target="#b14">[15]</ref> marginally improves FID but increases VRAM usage, so we use leaky ReLU.</p><p>All subsequent configurations adhere to a) through e). Violation of f) is acceptable as it pertains to the network backbone of StyleGAN2 <ref type="bibr" target="#b30">[31]</ref>, modernized in Config D and E.</p><p>Well-behaved loss function (Config C). We use the loss function proposed in Section 2 and this reduces FID to 11.65. We hypothesize that the network backbone in Config B is the limiting factor. General network modernization (Config D). First, we apply the 1-3-1 bottleneck ResNet architecture <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> to both G and D. This is the direct ancestor of all modern vision backbones <ref type="bibr" target="#b48">[48,</ref><ref type="bibr" target="#b97">97]</ref>. We also incorporate principles discovered in Config B and various modernization efforts from ConvNeXt <ref type="bibr" target="#b48">[48]</ref>. We categorize the roadmap of ConvNeXt as follows:</p><p>i. Consistently beneficial: i.1) increased width with depthwise convolution, i.2) inverted bottleneck, i.3) fewer activation functions, and i.4) separate resampling layers. ii. Negligible performance gain: ii.1) large kernel depthwise conv. with fewer channels, ii.2) swap ReLU with GELU, ii.3) fewer normalization layers, and ii.4) swap batch norm. with layer norm. iii. Irrelevant to our setting: iii.1) improved training recipe, iii.2) stage ratio, and iii.3) 'patchify' stem.</p><p>We aim to apply i) to our model, specifically i.3 and i.4 for the classic ResNet, while reserving i.1 and i.2 for Config E. Many aspects of ii) were introduced merely to mimic vision transformers <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b8">9]</ref> without yielding significant improvements <ref type="bibr" target="#b48">[48]</ref>. ii.3 and ii.4 are inapplicable due to our avoidance of normalization layers following principle c). ii.2 contradicts our finding that GELU deteriorates GAN performance, thus we use leaky ReLU per principle e). Liu et al. emphasize large conv. kernels (ii.1) <ref type="bibr" target="#b48">[48]</ref>, but this results in slightly worse performance compared to wider 3×3 conv. layers, so we do not adopt this ConvNeXt design choice.</p><p>Neural network architecture details. Given i.3, i.4, and principles c), d), and e), we can replace the StyleGAN2 backbone with a modernized ResNet. We use a fully symmetric design for G and D with 25 M parameters each, comparable to Config-A. The architecture is minimalist: each resolution stage has one transition layer and two residual blocks. The transition layer consists of bilinear resampling and an optional 1×1 conv. for changing spatial size and feature map channels. The residual block includes five operations: Conv1×1→ Leaky ReLU → Conv3×3→ Leaky ReLU → Conv1×1, with the final Conv1×1 having no bias term. For the 4×4 resolution stage, the transition layer is replaced by a basis layer for G and a classifier head for D. The basis layer, similar to StyleGAN <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b30">31]</ref>, uses 4×4 learnable feature maps modulated by z via a linear layer. The classifier head uses a global 4×4 depthwise conv. to remove spatial extent, followed by a linear layer to produce D's output. We maintain the width ratio for each resolution stage as in Config A, making the stem width 3× as wide due to the efficient 1×1 conv. The 3×3 conv. in the residual block has a compression ratio of 4, following <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>, making the bottleneck width 0.75× as wide as Config A.</p><p>To avoid variance explosion due to the lack of normalization, we employ fix-up initialization <ref type="bibr" target="#b99">[99]</ref>: We zero-initialize the last convolutional layer in each residual block and scale down the initialization of the other two convolutional layers in the block by L -0.25 , where L is the number of residual blocks. We avoid other fix-up tricks, such as excessive bias terms and a learnable multiplier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bottleneck modernization (Config E)</head><p>. Now that we have settled on the overall architecture, we investigate how the residual block can be modernized, specifically i.1) and i.2). First, we explore i.1 and replace the 3×3 convolution in the residual block with a grouped convolution. We set the group size to 16 rather than 1 (i.e. depthwise convolution as in ConvNeXt) as depthwise convolution is highly inefficient on GPUs and is not much faster than using a larger group size. With grouped convolution, we can reduce the bottleneck compression ratio to two given the same model size. This increases the width of the bottleneck to 1.5× as wide as Config A. Finally, we notice that the compute cost of grouped convolution is negligible compared to 1×1 convolution, and so we seek to enhance the capacity of grouped convolution. We apply i.2), which inverts the bottleneck width and the stem width, and which doubles the width of grouped convolutions without any increase in model size. 4 Experiments Details 4.1 Roadmap Insights on FFHQ-256 <ref type="bibr" target="#b28">[29]</ref> As per Table <ref type="table" target="#tab_2">2</ref>, Config A (vanilla StyleGAN2) achieves an FID of 7.52 using the official implementation on FFHQ-256. Config B with all tricks removed achieves an FID of 12.46-performance drops as expected. Config C, with a well-behaved loss, achieves an FID of 11.65. But, now training is sufficiently stable to improve the architecture.</p><p>Config D, which improves G and D based on the classic ResNet and ConvNeXt findings, achieves an FID of 9.95. The output skips of the StyleGAN2 generator are no longer useful given our new architecture; including them produces a worse FID of 10.17. Karras et al. find that the benefit of output skips is mostly related to gradient magnitude dynamics <ref type="bibr" target="#b31">[32]</ref>, and this has been addressed by our ResNet architecture. For StyleGAN2, Karras et al. conclude that a ResNet architecture is harmful to G <ref type="bibr" target="#b30">[31]</ref>, but this is not true in our case as their ResNet implementation is considerably different from ours: 1) Karras et al. use one 3-3 residual block for each resolution stage, while we have a separate transition layer and two 1-3-1 residual blocks; 2) i.3) and i.4) are violated as they do not have a linear residual block <ref type="bibr" target="#b67">[67]</ref> and the transition layer is placed on the skip branch of the residual block rather than the stem; 3) the essential principle of ResNet <ref type="bibr" target="#b15">[16]</ref>-identity mapping <ref type="bibr" target="#b16">[17]</ref>-is violated as Karras et al. divide the output of the residual block by √ 2 to avoid variance explosion due to the absence of a proper initialization scheme.</p><p>For Config E, we conduct two experiments that ablate i.1 (increased width with depthwise conv.) and i.2 (an inverted bottleneck). We add GroupedConv and reduce the bottleneck compression ratio to two given the same model size. Each bottleneck is now 1.5× the width of Config A, and the FID drops to 7.51, surpassing the performance of StyleGAN2. By inverting the stem and the bottleneck dimensions to enhance the capacity of GroupedConv, our final model achieves an FID of 7.05, exceeding StyleGAN2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Mode</head><p>Recovery -StackedMNIST [53] Model # modes↑ D KL ↓ DCGAN [60] 99 3.40 VEEGAN [80] 150 2.95 WGAN-GP [14] 959 0.73 PacGAN [46] 992 0.28 StyleGAN2 [31] 940 0.42 PresGAN [8] 1000 0.12 Adv. DSM [24] 1000 1.49 VAEBM [93] 1000 0.087 DDGAN [94] 1000 0.071 MEG [39] 1000 0.031 Ours-Config E 1000 0.029</p><p>Table 3: StackedMNIST 1000-mode coverage.</p><p>We repeat the earlier experiment in 1000-mode convergence on StackedMNIST (unconditional generation), but this time with our updated architecture and with comparisons to SOTA GANs and likelihoodbased methods (Tab. 3, Fig. <ref type="figure" target="#fig_5">5</ref>). One advantage brought up of likelihood-based models such as diffusion over GANs is that they achieve mode coverage <ref type="bibr" target="#b6">[7]</ref>. We find that most GANs struggle to find all modes. But, PresGAN <ref type="bibr" target="#b7">[8]</ref>, DDGAN <ref type="bibr" target="#b94">[94]</ref>, and our approach are successful. Further, our method outperforms all other tested GAN models in term of KL divergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">FID -FFHQ-256 [29] (Optimized)</head><p>We train Config E model until convergence and with optimized hyperparameters and training schedule on FFHQ at 256×256 (unconditional generation) (Tab. 4, Figs. 4 and 6). Please see our supplemental material for training details. Our model outperforms existing StyleGAN methods, plus four more Model NFE↓ FID↓ StyleGAN2 [31] 1 3.78 StyleGAN3-T [32] 1 4.81 StyleGAN3-R [32] 1 3.92 LDM [62] 200 4.98 ADM (DDIM) [7, 49] 500 8.41 ADM (DPM-Solver) [7, 49] 500 8.40 Diffusion Autoencoder [59, 49] 500 5.81 Ours-Config E 1 2.75 With ImageNet feature leakage [41]: PolyINR* [74] 1 2.72 StyleGAN-XL* [69] 1 2.19 StyleSAN-XL* [82] 1 1.68 Table 5: FFHQ-64.</p><p>recent diffusion-based methods. On this common dataset experimental setting, many methods (not listed here) use the bCR <ref type="bibr" target="#b101">[101]</ref> trick-this has only been shown to improve performance on FFHQ-256 (not even at different resolutions of FFHQ) <ref type="bibr" target="#b101">[101,</ref><ref type="bibr" target="#b98">98]</ref>. We do not use this trick.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">FID -FFHQ-64 [33]</head><p>To compare with EDM <ref type="bibr" target="#b32">[33]</ref> directly, we evaluate our model on FFHQ at 64×64 resolution. For this, we remove the two highest resolution stages of our 256×256 model, resulting in a generator that is less than half the number of parameters as EDM. Despite this, our model outperforms EDM on this dataset and needs one function evaluation only (Tab. 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">FID -CIFAR-10 [37]</head><p>Model NFE↓ FID↓ BigGAN <ref type="bibr">[</ref>3] 1 14.73 TransGAN [87] 1 9.26 ViTGAN [42] 1 6.66 DDGAN [94] 4 3.75 Diffusion StyleGAN2 [90] 1 3.19 StyleGAN2 + ADA [30] 1 2.42 StyleGAN3-R + ADA [32, 25] 1 10.83 DDPM [20] 1000 3.21 DDIM [76] 50 4.67 VE [78, 33] 35 3.11 VP [78, 33] 35 2.48 Ours-Config E 1 1.96 With ImageNet feature leakage [41]: StyleGAN-XL* [69] 1 1.85</p><p>Table 6: CIFAR-10 performance.</p><p>We train Config E model until convergence and with optimized hyperparameters and training schedule on CIFAR-10 (conditional generation) (Tab. 6, Fig. <ref type="figure" target="#fig_8">8</ref>). Our method outperforms many other GANs by FID even though the model has relatively small capacity. For instance, StyleGAN-XL <ref type="bibr" target="#b69">[69]</ref> has 18 M parameters in the generator and 125 M parameters in the discriminator, while our model has a 40 M parameters between the generator and discriminator combined (Fig. <ref type="figure" target="#fig_3">3</ref>). Compared to diffusion models like LDM or ADM, GAN inference is significantly cheaper as it requires only one network function evaluation compared to the tens or hundreds of network function evaluations for diffusion models without distillation.  Table 8: ImageNet-64. §:deterministic sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">FID -ImageNet-32 [6]</head><p>We train Config E model until convergence and with optimized hyperparameters and training schedule on ImageNet-32 (conditional generation). We compare against recent GAN models and recent diffusion models in Table <ref type="table" target="#tab_10">7</ref>. We adjust the number of parameters in the generator of our model to match StyleGAN-XL <ref type="bibr" target="#b69">[69]</ref>'s generator (84M parameters). Specifically, we make the model significantly wider to match. Our method achieves comparable FID despite using a 60% smaller discriminator (Tab. 7) and despite not using a pre-trained ImageNet classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">FID -ImageNet-64 [6]</head><p>We evaluate our model on ImageNet-64 to test its scalability. We stack another resolution stage on our ImageNet-32 model, resulting in a generator of 104 M parameters. This model is nearly 3× smaller than diffusion-like models <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b79">79,</ref><ref type="bibr" target="#b77">77]</ref> that rely on the ADM backbone, which contains about 300 M parameters. Despite the smaller model size and that our model generates samples in one step, it outperforms larger diffusion models with many NFEs on FID (Tab. 8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Recall</head><p>We evaluate the recall <ref type="bibr" target="#b40">[40]</ref> of our model on each dataset to quantify sample diversity. In general, our model achieves a recall that is similar to or marginally worse than the diffusion model counterpart, yet superior to existing GAN models. For CIFAR-10, the recall of our model peaked at 0.57; as a point of comparison, StyleGAN-XL <ref type="bibr" target="#b69">[69]</ref> has a worse recall of 0.47 despite its lower FID. For FFHQ, we obtain a recall of 0.53 at 64×64 and 0.49 at 256×256, whereas StyleGAN2 <ref type="bibr" target="#b30">[31]</ref> achieved a recall of 0.43 on FFHQ-256. Our ImageNet-32 model achieved a recall of 0.63; comparable to ADM <ref type="bibr" target="#b6">[7]</ref>.</p><p>Our ImageNet-64 model achieved recall 0.59. While this is slightly worse than ≈0.63 that many diffusion models achieve, it is better than BigGAN-deep <ref type="bibr" target="#b2">[3]</ref> which achieved a recall of 0.48.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Limitations</head><p>We have shown that a simplification of GANs is possible for image generation tasks, built upon a more stable RpGAN+R 1 + R 2 objective with mathematically-demonstrated convergence properties that still provides diverse output. This stability is what lets us re-engineer a modern network architecture without the tricks of previous methods, producing the R3GAN model with competitive FID on the common datasets of Stacked-MNIST, FFHQ, CIFAR-10, and ImageNet as an empirical demonstration of the mathematical benefits.</p><p>The focus of our work is to elucidate the essential components of a minimum GAN for image generation. As such, we prioritize simplicity over functionality-we do not claim to beat the performance of every existing model on every dataset or task; merely to provide a new simple baseline that converges easily. While this makes our model a possible backbone for future GANs, it also means that it is not suitable to apply our model directly to downstream applications such as image editing or controllable generation, as our model lacks dedicated features for easy image inversion or disentangled image synthesis. For instance, we remove style injection functionality from StyleGAN even though this has a clear use. We also omitted common techniques that have been shown in previous literature to improve FID considerably. Examples include some form of adaptive normalization modulated by the latent code <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b98">98,</ref><ref type="bibr" target="#b58">58,</ref><ref type="bibr" target="#b89">89,</ref><ref type="bibr" target="#b66">66]</ref>, and using multiheaded self attention at lower resolution stages <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>. We aim to explore these techniques in a future study.</p><p>Further, our work is limited in its evaluation of the scalability of R3GAN models. While they show promising results on 64×64 ImageNet, we are yet to verify the scalability on higher resolution ImageNet data or large-scale text to image generation tasks <ref type="bibr" target="#b11">[12]</ref>.</p><p>Finally, as a method that can improve the quality of generative models, it would be amiss not to mention that generative models-especially of people-can cause direct harm (e.g., through personalized deep fakes) and societal harm through the spread of disinformation (e.g., fake influencers).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This work introduced R3GAN, a new baseline GAN that features increased stability, leverages modern architectures, and does not require ad-hoc tricks that are commonplace in existing GAN models. Central to our approach is a regularized relativistic loss that provably features local convergence and that improves the stability of GAN training. This stable loss enables us to ablate various tricks that were previously necessary in GANs, and incorporate in their place modern deep architectures. The resulting streamlined baseline achieves competitive performance to SOTA models within its parameter size class. We anticipate that our backbone will help to drive future GAN research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices A Local convergence</head><p>Following <ref type="bibr" target="#b52">[52]</ref>, GAN training can be formulated as a dynamical system where the update operator is given by F h (θ, ψ) = (θ, ψ) + hv(θ, ψ). h is the learning rate and v denotes the gradient vector field:</p><formula xml:id="formula_5">v(θ, ψ) = -∇ θ L(θ, ψ) ∇ ψ L(θ, ψ)<label>(4)</label></formula><p>Mescheder et al. <ref type="bibr" target="#b51">[51]</ref> showed that local convergence near (θ * , ψ * ) can be analyzed by examining the spectrum of the Jacobian J F h at the equilibrium: if the Jacobian has eigenvalues with absolute value bigger than 1, then training does not converge. On the other hand, if all eigenvalues have absolute value smaller than 1, then training will converge to (θ * , ψ * ) at a linear rate. If all eigenvalues have absolute value equal to 1, the convergence behavior is undetermined.</p><p>Given some calculations <ref type="bibr" target="#b52">[52]</ref>, we can show that the eigenvalues of the Jacobian of the update operator λ J F h can be determined by λ J v :</p><formula xml:id="formula_6">λ J F h = 1 + hλ J v .<label>(5)</label></formula><p>That is, given small enough h <ref type="bibr" target="#b52">[52]</ref>, the training dynamics can instead be examined using λ J v , i.e., the eigenvalues of the Jacobian of the gradient vector field. If all λ J v have a negative real part, the training will locally converge to (θ * , ψ * ) at a linear rate. On the other hand, if some λ J v have a positive real part, the training is not convergent. If all λ J v have a zero real part, the convergence behavior is inconclusive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B DiracRpGAN: A demonstration of non-convergence</head><p>Summary. To obtain DiracRpGAN, we apply Eq. 2 to the DiracGAN <ref type="bibr" target="#b52">[52]</ref> problem setting. After simplification, DiracRpGAN and DiracGAN are different only by a constant. They have the same gradient vector field, therefore all proofs are identical to Mescheder et al. <ref type="bibr" target="#b52">[52]</ref>.</p><p>Definition B.1. The DiracRpGAN consists of a (univariate) generator distribution p θ = δ θ and a linear discriminator D ψ (x) = ψ • x. The true data distribution p D is given by a Dirac distribution concentrated at 0.</p><p>In this setup, the RpGAN training objective is given by:</p><formula xml:id="formula_7">L(θ, ψ) = f (ψθ) .<label>(6)</label></formula><p>We can now show analytically that DiracRpGAN does not converge without regularzation.</p><p>Lemma B.2. The unique equilibrium point of the training objective in Eq. 6 is given by θ = ψ = 0. Moreover, the Jacobian of the gradient vector field at the equilibrium point has the two eigenvalues ±f ′ (0)i which are both on the imaginary axis.</p><p>The gradient vector field v of Eq. 6 is given by:</p><formula xml:id="formula_8">v(θ, ψ) = -∇ θ L(θ, ψ) ∇ ψ L(θ, ψ) = -ψf ′ (ψθ) θf ′ (ψθ)<label>(7)</label></formula><p>and the Jacobian of v:</p><formula xml:id="formula_9">J v = -ψ 2 f ′′ (ψθ) -f ′ (ψθ) -ψθf ′′ (ψθ) f ′ (ψθ) + ψθf ′′ (ψθ) θ 2 f ′′ (ψθ)<label>(8)</label></formula><p>Evaluating J v at the equilibrium point θ = ψ = 0 gives us:</p><formula xml:id="formula_10">J v (0,0) = 0 -f ′ (0) f ′ (0) 0<label>(9)</label></formula><p>Therefore, the eigenvalues of J v are λ 1/2 = ±f ′ (0)i, both of which have a real part of 0. Thus, the convergence of DiracRpGAN is inconclusive and further analysis is required.</p><p>Lemma B.3. The integral curves of the gradient vector field v(θ, ψ) do not converge to the equilibrium point. More specifically, every integral curve (θ(t), ψ(t)) of the gradient vector field v(θ, ψ) satisfies θ(t) 2 + ψ(t) 2 = const for all t ∈ [0, ∞).</p><p>Let R(θ, ψ) = 1 2 (θ 2 + ψ 2 ), then:</p><formula xml:id="formula_11">d dt R(θ(t), ψ(t)) = -θ(t)ψ(t)f ′ (θ(t)ψ(t)) + ψ(t)θ(t)f ′ (θ(t)ψ(t)) = 0 . (<label>10</label></formula><formula xml:id="formula_12">)</formula><p>We see that the distance between (θ, ψ) and the equilibrium point (0, 0) stays constant. Therefore, training runs in circles and never converges.</p><p>Next, we investigate the convergence behavior of DiracRpGAN with regularization. For DiracRpGAN, both R 1 and R 2 can be reduced to the following form:</p><formula xml:id="formula_13">R(ψ) = γ 2 ψ 2<label>(11)</label></formula><p>Lemma B.4. The eigenvalues of the Jacobian of the gradient vector field for the gradient-regularized DiracRpGAN at the equilibrium point are given by</p><formula xml:id="formula_14">λ 1/2 = - γ 2 ± γ 2 4 -f ′ (0)<label>(12)</label></formula><p>In particular, for γ &gt; 0 all eigenvalues have a negative real part. Hence, gradient descent is locally convergent for small enough learning rates.</p><p>With regularization, the gradient vector field becomes</p><formula xml:id="formula_15">ṽ(θ, ψ) = -ψf ′ (ψθ) θf ′ (ψθ) -γψ<label>(13)</label></formula><p>the Jacobian of ṽ is then given by</p><formula xml:id="formula_16">J ṽ = -ψ 2 f ′′ (ψθ) -f ′ (ψθ) -ψθf ′′ (ψθ) f ′ (ψθ) + ψθf ′′ (ψθ) θ 2 f ′′ (ψθ) -γ<label>(14)</label></formula><p>evaluating the Jacobian at θ = ψ = 0 yields</p><formula xml:id="formula_17">J ṽ (0,0) = 0 -f ′ (0) f ′ (0) -γ<label>(15)</label></formula><p>given some calculations, we arrive at Eq.12.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C General Convergence Results</head><p>Summary. The proofs are largely the same as Mescheder et al. <ref type="bibr" target="#b52">[52]</ref>. We use the same proving techniques, and only slightly modify the assumptions and proof details to adapt Mescheder et al.'s effort to RpGAN. Like in <ref type="bibr" target="#b52">[52]</ref>, our proofs do not rely on unrealistic assumptions such as supp p D = supp p θ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Assumptions</head><p>We closely follow <ref type="bibr" target="#b52">[52]</ref> but modify the assumptions wherever necessary to tailor the proofs for RpGAN. Like in <ref type="bibr" target="#b52">[52]</ref>, we also consider the realizable case where there exists θ such that G θ produces the true data distribution.</p><p>Assumption I. We have p θ * = p D , and D ψ * = C in some local neighborhood of supp p D , where C is some arbitrary constant.</p><p>Since RpGAN is defined on critic difference rather than raw logits, we no longer require D ψ * to produce 0 on supp p D , instead any constant C would suffice.</p><p>Assumption II. We have f ′ (0) ̸ = 0 and f ′′ (0) &lt; 0.</p><p>This assumption is the same as in <ref type="bibr" target="#b52">[52]</ref>. The choice f (t) = -log(1 + e -t ) adopted in the main text satisfies this assumption.</p><p>As discussed in <ref type="bibr" target="#b52">[52]</ref>, there generally is not a single equilibrium point (θ * , ψ * ), but a submanifold of equivalent equilibria corresponding to different parameterizations of the same function. It is therefore necessary to represent the equilibrium as reparameterization manifolds M G and M D . We modify the reparameterization h as follows:</p><formula xml:id="formula_18">h(ψ) = Ex∼pD y∼p D |D ψ (x) -D ψ (y)| 2 + ∥∇ x D ψ (x)∥ 2<label>(16)</label></formula><p>to account for the fact that D ψ * is now allowed to have any constant value on supp p D . The reparameterization manifolds are then given by:</p><formula xml:id="formula_19">M G = {θ | p θ = p D } (17) M D = {ψ | h(ψ) = 0}<label>(18)</label></formula><p>We assume the same regularity properties as in <ref type="bibr" target="#b52">[52]</ref> for M G and M D near the equilibrium. To state these assumptions, we need:</p><formula xml:id="formula_20">g(θ) = E x∼p θ [∇ ψ D ψ | ψ=ψ * ]<label>(19</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>) which leads to:</head><p>Assumption III. There are ϵ-balls B ϵ (θ * ) and B ϵ (ψ * ) around θ * and ψ * so that M G ∩ B ϵ (θ * ) and M D ∩ B ϵ (ψ * ) define C 1 -manifolds. Moreover, the following holds:</p><formula xml:id="formula_21">(i) if v ∈ R n is not in T ψ * M D , then ∂ 2 v h(ψ * ) ̸ = 0. (ii) if w ∈ R m is not in T θ * M G , then ∂ w g(θ * ) ̸ = 0.</formula><p>These two conditions have exactly the same meanings as in <ref type="bibr" target="#b52">[52]</ref>: the first condition indicates the geometry of M D can be locally described by the second derivative of h. The second condition implies that D is strong enough that it can detect any deviation from the equilibrium generator distribution. This is the only assumption we have about the expressiveness of D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Convergence</head><p>We can now show the general convergence result for gradient penalized RpGAN, consider the gradient vector field with either R 1 or R 2 regularization:</p><formula xml:id="formula_22">ṽi (θ, ψ) = -∇ θ L(θ, ψ) ∇ ψ L(θ, ψ) -∇ ψ R i (θ, ψ)<label>(20)</label></formula><p>note that the convergence result can also be trivially extended to the case where both R 1 and R 2 are applied. We omit the proof for this case as it is redundant once the convergence with either regularization is proven.</p><p>Theorem. Assume Assumption I, II and III hold for (θ * , ψ * ). For small enough learning rates, gradient descent for ṽ1 and ṽ2 are both convergent to M G × M D in a neighborhood of (θ * , ψ * ). Moreover, the rate of convergence is at least linear.</p><p>We extend the convergence proof by Mescheder et al. <ref type="bibr" target="#b52">[52]</ref> to our setting. We first prove lemmas necessary to our main proof.</p><p>Lemma C.2.1. Assume J ∈ R (n+m)×(n+m) is of the following form:</p><formula xml:id="formula_23">J = 0 -B ⊤ B -Q (21)</formula><p>where Q ∈ R m×m is a symmetric positive definite matrix and B ∈ R m×n has full column rank. Then all eigenvalues λ of J satisfy ℜ(λ) &lt; 0.</p><p>Proof. See Mescheder et al. <ref type="bibr" target="#b52">[52]</ref>, Theorem A.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Hyperparameters, training configurations, and compute</head><p>We implement our models on top of the official StyleGAN3 code base. While the loss function and the models are implemented from scratch, we reuse support code from the existing implementation whenever possible. This includes exponential moving average (EMA) of generator weights <ref type="bibr" target="#b27">[28]</ref>, non-leaky data augmentation <ref type="bibr" target="#b29">[30]</ref>, and metric evaluation <ref type="bibr" target="#b31">[32]</ref>.</p><p>Training schedule. To speed up the convergence early in training, we specify a cosine schedule for the following hyperparameters before they reach their target values:</p><formula xml:id="formula_24">• Learning rate • γ for R 1 and R 2 regularization • Adam β 2 • EMA half-life • Augmentation probability</formula><p>We call this early training stage the burn-in phase. Burn-in length and schedule for each hyperparameter are listed in Table <ref type="table">9</ref> for each experiment. A schedule for the EMA half-life can already be found in Karras et al. <ref type="bibr" target="#b29">[30]</ref>, albeit they use a linear schedule. A lower initial Adam β 2 is crucial to the initial large learning rate as it allows the optimizer to adapt to the gradient magnitude change much quicker. We use a large initial γ to account for that early in training: p θ and p D are far apart and a large γ smooths both distributions more aggressively which makes learning easier. Augmentation is not necessary until D starts to overfit later on; thus, we set the initial augmentation probability to 0.</p><p>Dataset augmentation. We apply horizontal flips and non-leaky augmentation <ref type="bibr" target="#b29">[30]</ref> to all datasets where augmentation is enabled. Following <ref type="bibr" target="#b29">[30]</ref>, we include pixel blitting, geometric transformations, and color transforms in the augmentation pipeline. We additionally include cutout augmentation which works particularly well with our model, although it does not seem to have much effect on StyleGAN2. We also find it beneficial to apply color transforms less often and thus set their probability multiplier to 0.5 while retaining the multiplier 1 for other types of augmentations. As previously mentioned, we apply a fixed cosine schedule to the augmentation probability rather than adjusting it adaptively as in <ref type="bibr" target="#b29">[30]</ref>. We did not observe any performance degradation with this simplification.</p><p>Network capacity. We keep the capacity distribution for each resolution the same as in <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b31">32]</ref>. We place two residual blocks per resolution which makes our model roughly 3× as deep, 1.5-3× as wide as StyleGAN2 while maintaining the same model size on CIFAR-10 and FFHQ. For the ImageNet model, we double the number of channels which results in roughly 4× as many parameters as the default StyleGAN2 configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mixed precision training.</head><p>We apply mixed precision training as in <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b31">32]</ref> where all parameters are stored in FP32, but cast to lower precision along with the activation maps for the 4 highest resolutions. We notice that using FP16 as the low precision format cripples the training of our model. However, we see no problem when using BFloat16 instead.</p><p>Class conditioning. For class conditional models, we follow the same conditioning scheme as in <ref type="bibr" target="#b29">[30]</ref>. For G, the conditional latent code z ′ is the concatenation of z and the embedding of the class label c, specifically z ′ = concat(z, embed(c)). For D, we use a projection discriminator <ref type="bibr" target="#b54">[54]</ref> which evaluates the dot product of the class embedding and the feature vector D ′ (x) produced by the last layer of D, concretely D(x) = embed(c) • D ′ (x) ⊤ . We do not employ any normalization-based conditioning such as AdaIN <ref type="bibr" target="#b28">[29]</ref>, AdaGN <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b32">33]</ref>, AdaBN <ref type="bibr" target="#b2">[3]</ref> or AdaLN <ref type="bibr" target="#b58">[58]</ref> for simplicity, even though they improve FID considerably.</p><p>Stacked MNIST. We base this model off of the CIFAR-10 model but without class conditioning. We disable all data augmentation and shorten the burn-in phase considerably. We use a constant learning rate and did not observe any benefit of using a lower learning rate later in the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compute resources. We train the</head><p>Stacked MNIST and CIFAR-10 models on an 8× NVIDIA L40 node. Training took 7 hours for Stacked MNIST and 4 days for CIFAR-10. The FFHQ model was trained on an 8× NVIDIA A6000 f0r roughly 3 weeks. The ImageNet model was trained on NVIDIA A100/H100 clusters and training took one day on 32 H100s (about 5000 H100 hours). Hyperparameter Stacked MNIST CIFAR-10 FFHQ ImageNet Resolution 32 × 32 32 × 32 256 × 256 64 × 64 32 × 32 64 × 64 Class conditional -✓ --✓ ✓ Number of GPUs 8 8 8 8 32 64 Duration (Mimg) 10 250 200 100 1000 1000 Burn-in (Mimg) 2 20 20 20 200 200 Minibatch size 512 512 256 256 4096 4096 Learning rate 2 × 10 -4 2 × 10 -4 → 5 × 10 -5 2 × 10 -4 → 5 × 10 -5 2 × 10 -4 → 5 × 10 -5 2 × 10 -4 → 5 × 10 -5 2 × 10 -4 → 5 × 10 -5 γ for R 1 and R 2 1 → 0.1 0.05 → 0.005 150 → 15 2 → 0.2 0.5 → 0.05 1 → 0.1 Adam β 2 0.9 → 0.99 0.9 → 0.99 0.9 → 0.99 0.9 → 0.99 0.9 → 0.99 0.9 → 0.99 EMA half-life (Mimg) 0 → 0.5 0 → 5 0 → 0.5 0 → 0.5 0 → 50 0 → 50 Channels per resolution 768-768-768-768 768-768-768-768 96-192-384-768-768-768-768 384-768-768-768-768 1536-1536-1536-1536 1536-1536-1536-1536-1536 ResBlocks per resolution 2</p><formula xml:id="formula_25">-2-2-2 2-2-2-2 2-2-2-2-2-2-2 2-2-2-2-2 2-2-2-2 2-2-2-2-</formula><p>2 Groups per resolution 96-96-96-96 96-96-96-96 12-24-48-96-96-96-96 48-96-96-96-96 96-96-96-96 96-96-96-96-96 G params 20.73M 20.78M 23.06M 22.43M 82.91M 103.57M D params 20.68M 21.28M 23.01M 22.38M 86.55M 107.21M Dataset x-flips</p><formula xml:id="formula_26">- ✓ ✓ ✓ ✓ ✓ Augment probability - 0 → 0.55 0 → 0.3 0 → 0.3 0 → 0.5 0 → 0.4</formula><p>Table <ref type="table">9</ref>: Hyperparameters for each experiment. The decay factor β of EMA can be obtained using the formula β = 0.5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Minibatch size</head><p>EMA half-life , e.g. for CIFAR-10, EMA β = 0.5 512 5×10 6 ≈ 0.9999.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Negative Results and Future Work</head><p>Following the convention of Brock et al. <ref type="bibr" target="#b2">[3]</ref>, we report alternative design choices that did not make to our final model. Either because they failed to produce any quantitative improvement or because they would considerably complicate our minimalist design which might be better suited for future study.</p><p>• We tried to apply GELU <ref type="bibr" target="#b17">[18]</ref>, Swish <ref type="bibr" target="#b61">[61]</ref>, and SMU <ref type="bibr" target="#b1">[2]</ref> to G and D and found that doing so deteriorates FID considerably. We did not try on G xor D. We posit two independent factors:</p><p>-ConvNeXt in general does not benefit much from GELU (and possibly similar activations). Table <ref type="table">10</ref> and Table <ref type="table">11</ref> in <ref type="bibr" target="#b48">[48]</ref>: replacing ReLU with GELU gives little performance gain to ConvNeXt-T and virtually no performance gain to ConvNeXt-B. -In the context of GANs, GELU and Swish have the same problem as ReLU: that they have little gradient in the negative interval. Since G is updated from the gradient of D, having these activation functions in D could sparsify the gradient of D and as a result G will not receive as much useful information from D compared to using leaky ReLU. This does not explain the strange case of SMU <ref type="bibr" target="#b1">[2]</ref>: SMU is a smooth approximation of leaky ReLU and does not have the sparse gradient problem. It is unclear why it also underperformed and future work awaits.</p><p>• We tried adding group normalization <ref type="bibr" target="#b92">[92]</ref> to G and D and it did not improve FID or training stability. We do not claim that all forms of normalizations are harmful. Our claim in principle c) only extends to normalization layers that explicitly standardizes the mean and standard deviation of the activation maps. This has been verified by prior studies <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b65">65]</ref>. The harm of normalization layers extends to the adjacent field of image restoration <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b88">88]</ref>. To the best of our knowledge, EDM2 <ref type="bibr" target="#b33">[34]</ref> is currently the strongest diffusion UNet and it does not use normalization layers. However, it does apply normalization to the trainable weights and this improves performance considerably. We expect that applying the normalization techniques in EDM2 would improve our model's performance. • We tried removing the activation function after the 3×3 grouped convolution in each residual block as modern architectures <ref type="bibr" target="#b48">[48,</ref><ref type="bibr" target="#b97">97]</ref> typically do not apply non-linearity after depthwise convolution. This worsened FID performance. • We tried Pixel-Shuffle/Unshuffle <ref type="bibr" target="#b71">[71]</ref> for changing the resolution of the activation maps, and found that without low-pass filtering, this led to high frequency artifacts similar to checkerboard artifacts even though Pixel-Shuffle does not have the uneven overlap problem that transposed convolution does. Note that bilinear resampling is equivalent to applying channel duplication/averaging with Pixel-Shuffle/Unshuffle in conjunction with a [1, 2, 1] low-pass kernel. It might be interesting in future studies to explore inplace resampling filters that apply a low-pass filtered Pixel-Shuffle/Unshuffle operation on top of a learned function that changes the number of channels. • We tried scaling up our model size. We found that allocating more model capacity to lower resolution stages generally did not improve FID, but contributed to more rapid overfitting. Increasing model capacity at higher resolution stages always improves FID in our experiments, however scaling up higher resolution stages is very computationally expensive. Capacity distribution for each resolution stage of the model might be an important topic to explore in future studies. • For model simplicity, we did not conduct any experiment with a transformer architecture or attention mechanism in general. We are interested to see whether adding attention blocks to a convolutional network (similar to BigGAN <ref type="bibr" target="#b2">[3]</ref> and diffusion UNet <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>) or using a pure transformer architecture (similar to DiT <ref type="bibr" target="#b58">[58]</ref>) will result in stronger performance.</p><p>Given the impressive results of EDM2 <ref type="bibr" target="#b33">[34]</ref> (which uses UNet), it seems the argument has not yet settled for generative modeling. • We experimented with Adam β 2 = 0.999 following common practice in supervised learning and diffusion models, and found that doing so led to stability issues on our ImageNet models. We expect that introducing proper normalization to our model will resolve this problem. • We tried mixed precision training with IEEE FP16 as this is the low precision format used in StyleGAN2-ADA <ref type="bibr" target="#b29">[30]</ref>, StyleGAN3 <ref type="bibr" target="#b31">[32]</ref>, and EDM2 <ref type="bibr" target="#b33">[34]</ref>. This crippled the training of our model and switching to BFloat16 fixed the problem. We expect that introducing proper normalization to our model will allow us to use IEEE FP16 which offers more precision than BFloat16. • We tried lazy regularization <ref type="bibr" target="#b30">[31]</ref> in our early experiments where R 1 and R 2 were applied once every 8 minibatches. This led to slightly worse FID performance on real world datasets like FFHQ and CIFAR-10. However, it resulted in complete convergence failure on Stacked MNIST and several two dimensional toy datasets (line, circle, 25 Gaussians, etc.), indicating potential concerns regarding the mathematical legitimacy of this trick.      Guidelines:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Qualitative Results</head><p>• The answer NA means that the abstract and introduction do not include the claims made in the paper. • The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. • The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. • It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Limitations</head><p>Question: Does the paper discuss the limitations of the work performed by the authors?</p><p>Answer: <ref type="bibr">[Yes]</ref> Justification: Please see Section 5.</p><p>Guidelines:</p><p>• The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. • The authors are encouraged to create a separate "Limitations" section in their paper.</p><p>• The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. • The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. • The authors should reflect on the factors that influence the performance of the approach.</p><p>For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. • The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. • If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. • While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Theory Assumptions and Proofs</head><p>Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?</p><p>5. Open access to data and code Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [No] Justification: There is no new data. There is no code at submission time. The authors will aim to release this by publication time, with instructions to faithfully reproduce the experiments. Code URL is included in abstract. Guidelines: • The answer NA means that paper does not include experiments requiring code. • Please see the NeurIPS code and data submission guidelines (<ref type="url" target="https://nips.cc/public/guides/CodeSubmissionPolicy">https://nips.cc/ public/guides/CodeSubmissionPolicy</ref>) for more details. • While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). • The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (<ref type="url" target="https://nips.cc/public/guides/CodeSubmissionPolicy">https: //nips.cc/public/guides/CodeSubmissionPolicy</ref>) for more details. • The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. • At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). • Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. 6. Experimental Setting/Details Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Supplemental table lists all hyperparamters, and a supplemental section describes the training configurations. Guidelines: • The answer NA means that the paper does not include experiments. • The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. • The full details can be provided either with the code, in appendix, or as supplemental material. 7. Experiment Statistical Significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: Each experiment takes many days to compute, some take weeks. We do not have the compute time to provide variance bars on training executions. Guidelines: • The answer NA means that the paper does not include experiments. • The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. • The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). • The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) • The assumptions made should be given (e.g., Normally distributed errors). • It should be clear whether the error bar is the standard deviation or the standard error of the mean. • It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. • For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). • If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. 8. Experiments Compute Resources Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes]</p><p>Justification: Please see supplemental section on the experimental setting.</p><p>Guidelines:</p><p>• The answer NA means that the paper does not include experiments.</p><p>• The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. • The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. • The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Code Of Ethics</head><p>Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics <ref type="url" target="https://neurips.cc/public/EthicsGuidelines">https://neurips.cc/public/EthicsGuidelines</ref>?</p><p>Answer: [Yes] Justification: Experimental settings are standard and within the norms of the community.</p><p>Guidelines:</p><p>• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.</p><p>• If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. • The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Broader Impacts</head><p>Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?</p><p>Answer: [Yes]</p><p>Justification: We mention it briefly in Section 5. The paper describes a basic machine learning methodology, and so does not address a specific application with specific societal impacts. But, GANs do have potential social impact; it is clear that face generation has a significant impact (e.g., deep fakes) and our paper does use a face database for evaluation thanks to it being a community norm.</p><p>Guidelines:</p><p>• The answer NA means that there is no societal impact of the work performed.</p><p>• If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. • Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. • The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. • The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. • If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">Safeguards</head><p>Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer: [No]</head><p>Justification: There is no new data and much larger models produce higher fidelity images. The cost of training these large GANs is not prohibitive and is often done by hobbyists.</p><p>As such, it is doubtful that these models will unlock any new capabilities for mis-use or dual-use.</p><p>Guidelines:</p><p>• The answer NA means that the paper poses no such risks.</p><p>• Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. • Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. • We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.</p><p>12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?</p><p>Answer: [Yes]</p><p>Justification: All datasets are cited.</p><p>Guidelines:</p><p>• The answer NA means that the paper does not use existing assets.</p><p>• The authors should cite the original paper that produced the code package or dataset.</p><p>• The authors should state which version of the asset is used and, if possible, include a URL. • The name of the license (e.g., CC-BY 4.0) should be included for each asset.</p><p>• For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. • If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. • For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. • If this information is not available online, the authors are encouraged to reach out to the asset's creators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.">New Assets</head><p>Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?</p><p>Answer: [NA]</p><p>Justification: No new assets are released.</p><p>Guidelines:</p><p>• The answer NA means that the paper does not release new assets.</p><p>• Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. • The paper should discuss whether and how consent was obtained from people whose asset is used. • At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="14.">Crowdsourcing and Research with Human Subjects</head><p>Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?</p><p>Answer: <ref type="bibr">[NA]</ref> Justification: No human subjects are used and no crowdsourcing is used.</p><p>Guidelines:</p><p>• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. • According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="15.">Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects</head><p>Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?</p><p>Answer: <ref type="bibr">[NA]</ref> Justification: No human subjects are used and no crowdsourcing is used.</p><p>Guidelines:</p><p>• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. • We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. • For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>a)</head><figDesc>Convergent training objective with R 1 regularization. b) Smaller learning rate, avoiding momentum optimizer (Adam β 1 = 0). c) No normalization layer in G or D. d) Proper resampling via bilinear interpolation instead of strided (transposed) convolution. e) Leaky ReLU in both G and D, no tanh in the output layer of G. f) 4×4 constant input for G, output skips for G, ResNet D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Architecture comparison. For image generation, G and D are often both deep ConvNets with either partially or fully symmetric architectures. (a) StyleGAN2<ref type="bibr" target="#b30">[31]</ref> G uses a network to map noise vector z to an intermediate style space W. We use a traditional generator as style mapping is not necessary for a minimal working model. (b) StyleGAN2's building blocks have intricate layers but are themselves simple, with a ConvNet architecture from 2015<ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b73">73,</ref><ref type="bibr" target="#b15">16</ref>]. ResNet's identity mapping principle is also violated in the discriminator. (c) We remove tricks and modernize the architecture. Our design has clean layers with a more powerful ConvNet architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2</head><label>2</label><figDesc>depicts our final design, which reflects modern CNN architectures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Millions of parameters vs. FID-50K (log scale) on CIFAR-10. Lower is better.Many state-of-the-art GANs are derived from Projected GAN<ref type="bibr" target="#b68">[68]</ref>, including StyleGAN-XL<ref type="bibr" target="#b69">[69]</ref> and the concurrent work of StyleSAN-XL<ref type="bibr" target="#b82">[82]</ref>. These methods use a pre-trained ImageNet classifier in the discriminator. Prior work has shown that a pretrained ImageNet discriminator can leak ImageNet features into the model<ref type="bibr" target="#b41">[41]</ref>, causing the model to perform better when evaluating on FID since it relies on a pre-trained ImageNet classifier for the loss. But, this does not improve results in perceptual studies<ref type="bibr" target="#b41">[41]</ref>. Our model produces its low FID without any ImageNet pre-training.</figDesc><graphic coords="8,319.75,579.83,184.25,110.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Qualitative examples of sample generation from our Config E on FFHQ-256.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Qualitative examples of sample generation from our Config E on Stacked-MNIST.</figDesc><graphic coords="26,108.00,-1085.30,1584.12,1584.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: More qualitative examples of sample generation from our Config E on FFHQ-256.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Qualitative examples of sample generation from our Config E on FFHQ-64.</figDesc><graphic coords="28,108.00,187.56,396.00,396.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Qualitative examples of sample generation from our Config E on CIFAR-10.</figDesc><graphic coords="29,108.00,72.00,396.00,396.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Qualitative examples of sample generation from our Config E on ImageNet-32.</figDesc><graphic coords="30,108.00,187.56,396.00,396.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 : 50 FIDFigure 12 : 50 FIDFigure 13 : 50 FIDFigure 14 : 50 FIDFigure 15 :</head><label>105012501350145015</label><figDesc>Figure 10: Qualitative examples of sample generation from our Config E on ImageNet-64.</figDesc><graphic coords="31,108.00,187.56,396.00,396.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Effect of our simplification and modernization efforts evaluted on FFHQ-256.</figDesc><table><row><cell>Minimum baseline (Config B). We</cell></row><row><cell>strip away all StyleGAN2 features, re-</cell></row><row><cell>taining only the raw network backbone</cell></row><row><cell>and basic image generation capability.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>FFHQ-256. * denotes models that leak ImageNet features.</figDesc><table><row><cell>Model</cell><cell cols="2">NFE↓ FID↓</cell></row><row><cell>StyleGAN2 [31, 45]</cell><cell>1</cell><cell>3.32</cell></row><row><cell>MSG-GAN [27, 45]</cell><cell>1</cell><cell>2.7</cell></row><row><cell>Anycost GAN [45]</cell><cell>1</cell><cell>2.52</cell></row><row><cell>VE [78, 33]</cell><cell cols="2">79 25.95</cell></row><row><cell>VP [78, 33]</cell><cell>79</cell><cell>3.39</cell></row><row><cell>EDM [33]</cell><cell>79</cell><cell>2.39</cell></row><row><cell>Ours-Config E</cell><cell>1</cell><cell>1.95</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>ImageNet-32.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Model</cell><cell cols="2">NFE↓ FID↓</cell></row><row><cell></cell><cell></cell><cell></cell><cell>BigGAN-deep [3]</cell><cell cols="2">1 4.06</cell></row><row><cell></cell><cell></cell><cell></cell><cell>DDPM [20]</cell><cell>250</cell><cell>11.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell>DDIM [76]</cell><cell>50</cell><cell>13.7</cell></row><row><cell>Model</cell><cell cols="2">NFE↓ FID↓</cell><cell>ADM [7]</cell><cell>§ 250</cell><cell>2.91</cell></row><row><cell>DDPM++ [35]</cell><cell>1000</cell><cell>8.42</cell><cell>EDM [33]</cell><cell>79</cell><cell>2.23</cell></row><row><cell>VDM [36]</cell><cell>1000</cell><cell>7.41</cell><cell>CT [79]</cell><cell>2</cell><cell>11.1</cell></row><row><cell>MSGAN [27, 56]</cell><cell cols="2">1 12.3</cell><cell>CD [79]</cell><cell>3</cell><cell>4.32</cell></row><row><cell>ADM [7]</cell><cell>1000</cell><cell>3.60</cell><cell>iCT-deep [77]</cell><cell>2</cell><cell>2.77</cell></row><row><cell>DDPM-IP [56]</cell><cell>1000</cell><cell>2.87</cell><cell>DMD [96]</cell><cell cols="2">1 2.62</cell></row><row><cell>Ours-Config E</cell><cell>1</cell><cell>1.27</cell><cell>Ours-Config E</cell><cell cols="2">1 2.09</cell></row><row><cell cols="3">With ImageNet feature leakage [41]:</cell><cell cols="3">With ImageNet feature leakage [41]:</cell></row><row><cell>StyleGAN-XL* [69]</cell><cell cols="2">1 1.10</cell><cell>StyleGAN-XL* [69]</cell><cell cols="2">1 1.52</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>38th Conference on Neural Information Processing Systems (NeurIPS 2024).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>While mode collapse and mode dropping are technically distinct issues, they are used interchangeably in this context to describe the common problem where supp(p θ ) does not comprehensively cover supp(pD). Mode collapse refers to the generator producing a limited diversity of samples (i.e., one image for the entire distribution), whereas mode dropping involves the generator failing to represent certain modes of the data distribution (ignoring entire subsets of the training distribution).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>Varying γ from 0.1 to 100 does not stabilize training.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>These numbers are from Karras et al.<ref type="bibr" target="#b27">[28]</ref>, Table</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4"><p>"857" corresponds to a low-capacity version of a progressive GAN and "881" adds the minibatch standard deviation trick. Further comparisons via loss curves are difficult since progressive GAN is a substantially different model than the small ResNet we use for this experiment.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. The authors thank <rs type="person">Xinjie Jayden Yi</rs> for contributing to the proof and <rs type="person">Yu Cheng</rs> for helpful discussion. For compute, the authors thank <rs type="person">Databricks Mosaic Research</rs>. <rs type="person">Yiwen Huang</rs> was supported by a <rs type="funder">Brown University Division of Research Seed Award</rs>, and <rs type="person">James Tompkin</rs> was supported by <rs type="funder">NSF</rs> <rs type="grantNumber">CAREER 2144956</rs>. <rs type="person">Volodymyr Kuleshov</rs> was supported by <rs type="funder">NSF</rs> <rs type="grantNumber">CAREER 2145577</rs> and <rs type="funder">NIH MIRA</rs> <rs type="grantNumber">1R35GM15124301</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Pdj4nME">
					<idno type="grant-number">CAREER 2144956</idno>
				</org>
				<org type="funding" xml:id="_Jm7zeDT">
					<idno type="grant-number">CAREER 2145577</idno>
				</org>
				<org type="funding" xml:id="_Yrhxp46">
					<idno type="grant-number">1R35GM15124301</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Lemma C.2.2. The gradient of L(θ, ψ) w.r.t. θ and ψ are given by:</p><p>Proof. This is just the chain rule.</p><p>Lemma C.2.3. Assume that (θ * , ψ * ) satisfies Assumption I. The Jacobian of the gradient vector field v(θ, ψ) at (θ * , ψ * ) is then</p><p>the terms K DD and K DG are given by</p><p>Proof. Note that</p><p>By Assumption I, D ψ * = C in some neighborhood of supp p D . Therefore we also have ∇ x D ψ * = 0 and ∇ 2 x D ψ * = 0 for x ∈ supp p D . Using these two conditions, we see that ∇ 2 θ L(θ * , ψ * ) = 0. To see Eq.25 and Eq.26, simply take the derivatives of Eq.23 and evaluate at (θ * , ψ * ).</p><p>Proof. See Mescheder et al. <ref type="bibr" target="#b52">[52]</ref>, Lemma D.3.</p><p>Lemma C.2.5. The second derivatives ∇ 2 ψ R i (θ * , ψ * ) of the regularization terms R i , i ∈ {1, 2}, w.r.t. ψ at (θ * , ψ * ) are both given by</p><p>where A = ∇ ψ,x D ψ * . Moreover, both regularization terms satisfy ∇ θ,ψ R i (θ * , ψ * ) = 0.</p><p>Proof. See Mescheder et al. <ref type="bibr" target="#b52">[52]</ref>, Lemma D.4.</p><p>Given Lemma C.2.3, Lemma C.2.5 and Eq.20, we can now show that the Jacobian of the regularized gradient field at the equilibrium point is given by</p><p>where M DD = K DD -L DD . To prove our main theorem, we need to examine J ṽ when restricting it to the space orthogonal to</p><p>Lemma C.2.6. Assume Assumptions II and III hold.</p><p>Proof. By Lemma C.2.3 and Lemma C.2.5, we have</p><p>By Assumption II, we have f ′′ (0</p><p>for all (x, y) ∈ supp p D × supp p D . Recall the definition of h(ψ) from Eq.16. Using the fact that D ψ * = C and ∇ x D ψ * = 0 for x ∈ supp p D , we see that the Hessian of h(ψ) at ψ * is</p><p>The second directional derivative ∂ 2 v h(ψ) is therefore</p><p>By Assumption III, this can only hold if v ∈ T ψ * M D .</p><p>Lemma C.2.7. Assume Assumption III holds. If w ̸ = 0 is not in T θ * M G , then K DG w ̸ = 0.</p><p>Proof. See Mescheder et al. <ref type="bibr" target="#b52">[52]</ref>, Lemma D.6.</p><p>Proof for the main theorem. Given previous lemmas, by choosing local coordinates θ(α, γ G ) and ψ(β, γ D ) for M G and M D such that θ * = 0, ψ * = 0 as well as</p><p>our proof is exactly the same as Mescheder et al. <ref type="bibr" target="#b52">[52]</ref>, Theorem 4.1.</p><p>Answer: [Yes] Justification: Prior knowledge of Mescheder et al. <ref type="bibr" target="#b52">[52]</ref> is required, but this is cited appropriately to help the reader. Guidelines:</p><p>• The answer NA means that the paper does not include theoretical results.</p><p>• All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. • All assumptions should be clearly stated or referenced in the statement of any theorems.</p><p>• The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. • Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. • Theorems and Lemmas that the proof relies upon should be properly referenced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Result Reproducibility</head><p>Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Supplemental table lists all hyperparamters, and a supplemental section describes the training configurations. Guidelines:</p><p>• The answer NA means that the paper does not include experiments.</p><p>• If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. • If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. • Depending on the contribution, reproducibility can be accomplished in various ways.</p><p>For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. • While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. , with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility.</p><p>In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lei Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Smu: smooth activation function for deep networks using smoothing maximum technique</title>
		<author>
			<persName><forename type="first">Koushik</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandeep</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shilpak</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><forename type="middle">Kumar</forename><surname>Pandey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.04682</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.11096</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">High-performance large-scale image recognition without normalization</title>
		<author>
			<persName><forename type="first">Andy</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soham</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1059" to="1071" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A downsampled variant of imagenet as an alternative to the cifar datasets</title>
		<author>
			<persName><forename type="first">Patryk</forename><surname>Chrabaszcz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.08819</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Diffusion models beat gans on image synthesis</title>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Nichol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8780" to="8794" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Prescribed generative adversarial networks</title>
		<author>
			<persName><forename type="first">Francisco Jr</forename><surname>Adji B Dieng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michalis</forename><forename type="middle">K</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><surname>Titsias</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.04302</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Discriminator gradient gap regularization for GAN training with limited data</title>
		<author>
			<persName><forename type="first">Tiantian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoyu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName><surname>Diggan</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=azBVn" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">Alice</forename><forename type="middle">H</forename><surname>Oh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alekh</forename><surname>Agarwal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Danielle</forename><surname>Belgrave</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Negative momentum for improved game dynamics</title>
		<author>
			<persName><surname>Gauthier Gidel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Askari</forename><surname>Reyhane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Hemmat</surname></persName>
		</author>
		<author>
			<persName><surname>Pezeshki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Rémi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Priol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<author>
			<persName><surname>Mitliagkas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 22nd International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1802" to="1811" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Commoncanvas: Open diffusion models trained on creative-commons images</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Gokaslan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasmine</forename><surname>Feder Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Landan</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Austin</forename><surname>Seguin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihir</forename><surname>Jacobson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cory</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volodymyr</forename><surname>Stephenson</surname></persName>
		</author>
		<author>
			<persName><surname>Kuleshov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="8250" to="8260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generative adversarial networks</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="139" to="144" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2016: 14th European Conference</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">October 11-14, 2016. 2016</date>
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
	<note>Proceedings, Part IV 14</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Gaussian error linear units (gelus)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Alexia</forename><surname>Jolicoeur-Martineau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.00734</idno>
		<title level="m">The relativistic discriminator: a key element missing from standard gan</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Alexia</forename><surname>Jolicoeur-Martineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Mitliagkas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.06922</idno>
		<title level="m">Gradient penalty from a maximum margin perspective</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adversarial score matching and improved sampling for image generation</title>
		<author>
			<persName><forename type="first">Alexia</forename><surname>Jolicoeur-Martineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Piché-Taillefer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.05475</idno>
	</analytic>
	<monogr>
		<title level="m">Rémi Tachet des Combes, and Ioannis Mitliagkas</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Studiogan: a taxonomy and benchmark of gans for image synthesis</title>
		<author>
			<persName><forename type="first">Minguk</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joonghyuk</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Scaling up gans for text-to-image synthesis</title>
		<author>
			<persName><forename type="first">Minguk</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="10124" to="10134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Msg-gan: Multi-scale gradients for generative adversarial networks</title>
		<author>
			<persName><forename type="first">Animesh</forename><surname>Karnewar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7799" to="7808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10196</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Training generative adversarial networks with limited data</title>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="12104" to="12114" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of stylegan</title>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8110" to="8119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Alias-free generative adversarial networks</title>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Härkönen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="852" to="863" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Elucidating the design space of diffusion-based generative models</title>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="26565" to="26577" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Analyzing and improving the training dynamics of diffusion models</title>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.02696</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Soft truncation: A universal training technique of score-based diffusion model for high precision score estimation</title>
		<author>
			<persName><forename type="first">Dongjun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungjae</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyungwoo</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanmo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Il-Chul</forename><surname>Moon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05527</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Variational diffusion models</title>
		<author>
			<persName><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="21696" to="21707" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Thesis</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<ptr target="https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862" />
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">25</biblScope>
		</imprint>
	</monogr>
	<note>d3b9d6b76c8436e924a68c45b-Paper.pdf</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Maximum entropy generators for energy-based models</title>
		<author>
			<persName><forename type="first">Rithesh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.08508</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Improved precision and recall metric for assessing generative models</title>
		<author>
			<persName><forename type="first">Tuomas</forename><surname>Kynkäänniemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Tuomas</forename><surname>Kynkäänniemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.06026</idno>
		<title level="m">The role of imagenet classes in fréchet inception distance</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Kwonjoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huiwen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Vitgan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.04589</idno>
		<title level="m">Training gans with vision transformers</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName><forename type="first">Bee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyoung</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="136" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jae</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Hyun</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><surname>Chul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02894</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Geometric gan. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Anycost gans for interactive image synthesis and editing</title>
		<author>
			<persName><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frieder</forename><surname>Ganz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="14986" to="14996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Pacgan: The power of two samples in generative adversarial networks</title>
		<author>
			<persName><forename type="first">Zinan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Khetan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giulia</forename><surname>Fanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewoong</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A convnet for the 2020s</title>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanzi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao-Yuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="11976" to="11986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Compensation sampling for improved convergence in diffusion models</title>
		<author>
			<persName><forename type="first">Hui</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><surname>Poppe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.06285</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName><forename type="first">Xudong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond Yk</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">Paul</forename><surname>Smolley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2794" to="2802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">The numerics of gans</title>
		<author>
			<persName><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Which training methods for gans do actually converge</title>
		<author>
			<persName><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3481" to="3490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Unrolled generative adversarial networks</title>
		<author>
			<persName><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05637</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">cgans with projection discriminator. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Gradient descent gan optimization is locally stable</title>
		<author>
			<persName><forename type="first">Vaishnavh</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kolter</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Input perturbation reduces exposure bias in diffusion models</title>
		<author>
			<persName><forename type="first">Mang</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enver</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelo</forename><surname>Porrello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.11706</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">f-gan: Training generative neural samplers using variational divergence minimization</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Botond</forename><surname>Cseke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryota</forename><surname>Tomioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Scalable diffusion models with transformers</title>
		<author>
			<persName><forename type="first">William</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="4195" to="4205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Diffusion autoencoders: Toward a meaningful and decodable representation</title>
		<author>
			<persName><forename type="first">Konpat</forename><surname>Preechakul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nattanat</forename><surname>Chatthee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suttisak</forename><surname>Wizadwongsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Supasorn</forename><surname>Suwajanakorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022-06">June 2022</date>
			<biblScope unit="page" from="10619" to="10629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<author>
			<persName><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05941</idno>
		<title level="m">Searching for activation functions</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Björn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical image computing and computer-assisted intervention-MICCAI 2015: 18th international conference</title>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">October 5-9, 2015. 2015</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Stabilizing training of generative adversarial networks through regularization</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelien</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Litevae: Lightweight and efficient variational autoencoders for latent diffusion models</title>
		<author>
			<persName><forename type="first">Seyedmorteza</forename><surname>Sadat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Buhmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Otmar</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romann M</forename><surname>Weber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.14477</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<author>
			<persName><forename type="first">Sekhar</forename><surname>Subham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Sahoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">De</forename><surname>Gokaslan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volodymyr</forename><surname>Sa</surname></persName>
		</author>
		<author>
			<persName><surname>Kuleshov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.13236</idno>
		<title level="m">Diffusion models with learned adaptive noise</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Projected gans converge faster</title>
		<author>
			<persName><forename type="first">Axel</forename><surname>Sauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kashyap</forename><surname>Chitta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="17480" to="17492" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">StyleGAN-XL: Scaling stylegan to large diverse datasets</title>
		<author>
			<persName><forename type="first">Axel</forename><surname>Sauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katja</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2022 conference proceedings</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Stylegan-t: Unlocking the power of gans for fast large-scale text-to-image synthesis</title>
		<author>
			<persName><forename type="first">Axel</forename><surname>Sauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="30105" to="30118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient subpixel convolutional neural network</title>
		<author>
			<persName><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ferenc</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Andrew P Aitken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zehan</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient subpixel convolutional neural network</title>
		<author>
			<persName><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ferenc</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Andrew P Aitken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zehan</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Polynomial implicit neural representations for large diverse datasets</title>
		<author>
			<persName><forename type="first">Rajhans</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankita</forename><surname>Shukla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavan</forename><surname>Turaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2041" to="2051" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Amortised map inference for image super-resolution</title>
		<author>
			<persName><forename type="first">Jose</forename><surname>Casper Kaae Sønderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenzhe</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ferenc</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><surname>Huszár</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.04490</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Denoising diffusion implicit models</title>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Improved techniques for training consistency models</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.13456</idno>
		<title level="m">Score-based generative modeling through stochastic differential equations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Consistency models</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="32211" to="32252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Veegan: Reducing mode collapse in gans using implicit variational learning</title>
		<author>
			<persName><forename type="first">Akash</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lazar</forename><surname>Valkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Michael U Gutmann</surname></persName>
		</author>
		<author>
			<persName><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Towards a better global loss landscape of gans</title>
		<author>
			<persName><forename type="first">Ruoyu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiantian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="10186" to="10198" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">SAN: Inducing metrizability of GAN with discriminative normalized linear layer</title>
		<author>
			<persName><forename type="first">Yuhta</forename><surname>Takida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masaaki</forename><surname>Imaizumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takashi</forename><surname>Shibuya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chieh-Hsin</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toshimitsu</forename><surname>Uesaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naoki</forename><surname>Murata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuki</forename><surname>Mitsufuji</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=eiF7TU1E8E" />
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Alleviation of gradient exploding in gans: Fake can be real</title>
		<author>
			<persName><forename type="first">Song</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1191" to="1200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Improving generalization and stability of generative adversarial networks</title>
		<author>
			<persName><forename type="first">Truyen</forename><surname>Hoang Thanh-Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Svetha</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><surname>Venkatesh</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ByxPYjC5KQ" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Instance normalization: The missing ingredient for fast stylization</title>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08022</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Score-based generative modeling in latent space</title>
		<author>
			<persName><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><surname>Kreis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11287" to="11302" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Esrgan: Enhanced super-resolution generative adversarial networks</title>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shixiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV) workshops</title>
		<meeting>the European conference on computer vision (ECCV) workshops</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Infodiffusion: Representation learning using information maximizing diffusion models</title>
		<author>
			<persName><forename type="first">Yingheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yair</forename><surname>Schiff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Gokaslan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weishen</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">De</forename><surname>Sa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volodymyr</forename><surname>Kuleshov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="36336" to="36354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Diffusion-gan: Training gans with diffusion</title>
		<author>
			<persName><forename type="first">Zhendong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huangjie</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyuan</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Vaebm: A symbiosis between variational autoencoders and energy-based models</title>
		<author>
			<persName><forename type="first">Zhisheng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><surname>Kreis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.00654</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">Tackling the generative learning trilemma with denoising diffusion gans</title>
		<author>
			<persName><forename type="first">Zhisheng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><surname>Kreis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.07804</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">One-step diffusion with distribution matching distillation</title>
		<author>
			<persName><forename type="first">Michaël</forename><surname>Tianwei Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Gharbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fredo</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">T</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taesung</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="6613" to="6623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Metaformer is actually what you need for vision</title>
		<author>
			<persName><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyang</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10819" to="10829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Styleswin: Transformer-based gan for high-resolution image generation</title>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuyang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="11304" to="11314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.09321</idno>
		<title level="m">Fixup initialization: Residual learning without normalization</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Making convolutional networks shift-invariant again</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7324" to="7334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Improved consistency regularization for gans</title>
		<author>
			<persName><forename type="first">Zhengli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="11033" to="11041" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
