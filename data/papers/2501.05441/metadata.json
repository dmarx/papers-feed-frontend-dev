{
  "arxivId": "2501.05441",
  "title": "The GAN is dead; long live the GAN! A Modern GAN Baseline",
  "authors": "Yiwen Huang, Aaron Gokaslan, Volodymyr Kuleshov, James Tompkin",
  "abstract": "There is a widely-spread claim that GANs are difficult to train, and GAN\narchitectures in the literature are littered with empirical tricks. We provide\nevidence against this claim and build a modern GAN baseline in a more\nprincipled manner. First, we derive a well-behaved regularized relativistic GAN\nloss that addresses issues of mode dropping and non-convergence that were\npreviously tackled via a bag of ad-hoc tricks. We analyze our loss\nmathematically and prove that it admits local convergence guarantees, unlike\nmost existing relativistic losses. Second, our new loss allows us to discard\nall ad-hoc tricks and replace outdated backbones used in common GANs with\nmodern architectures. Using StyleGAN2 as an example, we present a roadmap of\nsimplification and modernization that results in a new minimalist baseline --\nR3GAN. Despite being simple, our approach surpasses StyleGAN2 on FFHQ,\nImageNet, CIFAR, and Stacked MNIST datasets, and compares favorably against\nstate-of-the-art GANs and diffusion models.",
  "url": "https://arxiv.org/abs/2501.05441",
  "issue_number": 938,
  "issue_url": "https://github.com/dmarx/papers-feed/issues/938",
  "created_at": "2025-01-11T08:07:28.010024",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 0,
  "last_read": null,
  "last_visited": "2025-01-12T17:53:23.123Z",
  "main_tex_file": null,
  "published_date": "2025-01-09T18:53:06Z",
  "arxiv_tags": [
    "cs.LG",
    "cs.CV"
  ]
}