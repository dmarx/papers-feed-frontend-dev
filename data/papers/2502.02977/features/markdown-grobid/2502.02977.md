# Disentangling CLIP Features for Enhanced Localized Understanding

## Abstract

## 

Vision-language models (VLMs) demonstrate impressive capabilities in coarse-grained tasks like image classification and retrieval. However, they struggle with fine-grained tasks that require localized understanding. To investigate this weakness, we comprehensively analyze CLIP features and identify an important issue: semantic features are highly correlated. Specifically, the features of a class encode information about other classes, which we call mutual feature information (MFI). This mutual information becomes evident when we query a specific class and unrelated objects are activated along with the target class. To address this issue, we propose Unmix-CLIP, a novel framework designed to reduce MFI and improve feature disentanglement. We introduce MFI loss, which explicitly separates text features by projecting them into a space where inter-class similarity is minimized. To ensure a corresponding separation in image features, we use multi-label recognition (MLR) to align the image features with the separated text features. This ensures that both image and text features are disentangled and aligned across modalities, improving feature separation for downstream tasks. For the COCO-14 dataset, Unmix-CLIP reduces feature similarity by 24.9%. We demonstrate its effectiveness through extensive evaluations of MLR and zeroshot semantic segmentation (ZS3). In MLR, our method performs competitively on the VOC2007 and surpasses SOTA approaches on the COCO-14 dataset, using fewer training parameters. Additionally, Unmix-CLIP consistently outperforms existing ZS3 methods on COCO and VOC. Input Image Query: Person Class Similarity CLIP Ours Figure 1: Comparison of Activated Regions. When queried for the 'person' class (middle column, highlighted in red), CLIP shows activation in unqueried regions (dogs and horses), while our method maintains focus on the person. The rightmost column displays cosine similarities between class features, showing that reducing the inter-class similarity (person-dog: 0.84 → 0.42, person-horse: 0.80 → 0.28) results in features that are suitable for fine-grained tasks.

## Introduction

Vision-language models (VLMs) have emerged as powerful tools for understanding visual content through natural language supervision. CLIP [(Radford et al., 2021)](#b27), trained on 400 million image-text pairs (WIT-400M), achieves remarkable performance in coarse-grained visual understanding tasks such as image classification [(Zhou et al., 2022b)](#), image retrieval [(Baldrati et al., 2022)](#b0), and visual question answering [(Yu et al., 2022)](#b35). However, these models struggle with fine-grained tasks that require localized understanding, leading to significant performance degradation in multi-label recognition (MLR) [(Zhu & Wu, 2021;](#b39)[Huang et al., 2024)](#b17) and semantic segmentation [(Lüddecke & Ecker, 2022)](#b24). While previous work has attributed these limitations to architectural choices [(Darcet et al., 2023;](#b8)[Zhou et al., 2022a;](#)[Li et al., 2023)](#b20) or training objectives [(Lin et al., 2024;](#b23)[Dong et al., 2023)](#b11), our analysis reveals a more fundamental issue: the entanglement of semantic features in CLIP's feature space.

We systematically analyze CLIP's features and identify two key factors contributing to this issue. First, the spatial pooling operation in the final layer, although effective for global tasks, discards essential localized information necessary for fine-grained understanding. Second, and more importantly, we discover significant interference between class features in the joint vision-language space, which we term mutual feature information (MFI). The mutual information becomes apparent during class-specific queries: regions corresponding to unrelated objects are consistently activated alongside the target class. For example, as illustrated in Figure [1](#), regions containing 'dog' and 'horse' also activate when we query the class 'person.' This activation pattern strongly correlates with the high similarity scores between class text features (0.84 for person-dog and 0.80 for person-horse), indicating substantial feature entanglement in CLIP's representation space.

To address this fundamental limitation, we introduce Unmix-CLIP, a novel framework that disentangles class features in vision-language models. Drawing inspiration from the redundancy reduction principle [(Barlow et al., 1961)](#b2) in neuroscience, we extend this concept to the vision-language domain. While previous approaches have focused on architectural modifications [(Zhou et al., 2022a;](#)[Li et al., 2023;](#b20)[Bousselham et al., 2024)](#b3) or prompt engineering [(Sun et al., 2022;](#b32)[Rawlekar et al., 2024a)](#) to adapt VLMs for fine-grained tasks, Unmix-CLIP directly targets the root cause by minimizing MFI between class representations while preserving task-relevant information. We achieve this through a carefully designed MFI loss function that explicitly disentangles text features by projecting them to minimize inter-class similarity. To achieve a similar separation in image features, we align them with the projected text features using a multilabel recognition framework. The joint training using MFI loss (separates text features) and MLR loss (aligns text and image features) results in disentangled features that align across the image and text domains, leading to improved separation in semantic features.

We train Unmix-CLIP on 80 classes from the COCO-14 [(Lin et al., 2014)](#b21) dataset and evaluate its performance on two fine-grained tasks: multi-label recognition (MLR) and zero-shot semantic segmentation (ZS3). For MLR evaluation, we use the COCO-14 and VOC2007 [(Everingham et al., 2010)](#b12) datasets. For ZS3, we use VOC2012 [(Everingham et al., 2010)](#b12) and COCO-17 [(Lin et al., 2014)](#b21) for seen classes, and VOC Context [(Mottaghi et al., 2014)](#b25) provides 59 classes, 30 of which are unseen during pre-training. Our experimental results demonstrate that Unmix-CLIP achieves competitive performance on VOC and outperforms stateof-the-art (SOTA) methods on the challenging COCO-14 dataset, using only one-third of their training parameters. For ZS3, Unmix-CLIP surpasses SOTA VLMs on datasets with seen classes, demonstrating that reducing mutual feature information (MFI) is crucial for fine-grained tasks. To further assess its segmentation capabilities, we apply Unmix-CLIP to segment objects in the images, recasting the task as single-label recognition, a task more suitable for CLIP. We combine the segment-level and whole-image results to obtain zero-shot MLR predictions. Segmenting objects provides complementary information on top of global image features. The main contributions of this work are:

• We identify a critical challenge in adapting VLMs for fine-grained tasks: mutual information between class features (MFI) degrades fine-grained task performance

• To address this challenge, we propose Unmix-CLIP, a framework that adapts CLIP features for fine-grained tasks by reducing MFI. At its core lies our proposed MFI loss, which explicitly disentangles text features and guides the disentanglement of image features • We show that Unmix-CLIP outperforms SOTA multilabel recognition methods in challenging settings using significantly fewer training parameters. Additionally, it outperforms zero-shot semantic segmentation methods. Moreover, as an object segmenter, Unmix-CLIP enhances CLIP's zero-shot MLR performance.

## Related Work

Recoding information. Shannon proposed that optimal information transmission involves designing codes with minimum entropy [(Shannon, 1948)](#b31). The redundancy reduction principle extended this idea to neuroscience, suggesting that sensory systems recode information to reduce redundancy with minimal loss [(Barlow et al., 1961)](#b2). This principle has since been applied to many recent works, including image compression [(Ballé et al., 2016)](#b1) and more popularly in representation learning [(Oord et al., 2018;](#b26)[Chen et al., 2020;](#b5)[Zbontar et al., 2021;](#b36)[Henaff, 2020;](#b15)[He et al., 2020;](#b14)[Chen & He, 2021)](#b6). While our loss function shares structural similarities with representation learning methods (a similarity and contrastive term), our method differs as follows: (1) Instead of learning features from scratch, we refine learned features (reducing MFI).

(2) We do not rely on augmentation-based learning or batch processing. (3) Unlike contrastive methods that require paired embeddings, our approach operates on a fixed set of text embeddings. Most importantly, our objective is not generic feature learning but targeted feature modification to enhance task-specific utility. Vision-Language Models for Fine-grained Tasks. Visionlanguage models (VLMs) trained with contrastive losses [(Radford et al., 2021;](#b27)[Jia et al., 2021)](#b18) are challenging to adapt for fine-grained tasks due to two reasons: (1) their reliance on global feature aggregation, which ignores local information.

(2) Using the softmax operation in their training loss biases them toward single-object settings.

Recognition. Early efforts to adapt VLMs for recognition centered on learning prompts as classifiers for visual features [(Zhou et al., 2022b)](#). These methods were extended to multi-label settings by learning multiple prompts for each class [(Sun et al., 2022;](#b32)[Hu et al., 2023;](#b16)[Rawlekar et al., 2024a)](#). Subsequent works incorporated co-occurrence information to make predictions interdependent [(Ding et al., 2023;](#b10)[Rawlekar et al., 2024b)](#). In contrast, our approach does not rely on prompt learning or co-occurrence modeling during pre-training. Furthermore, our features are adaptable to tasks beyond multi-label recognition.

Localization. Early approaches addressed localization by training image segmentation models and using VLMs to label the segmented regions [(Kirillov et al., 2023)](#b19). Later methods introduced pre-training setups that combined visionlanguage alignment with mask distillation to enhance localization [(Dong et al., 2023)](#b11). Recent works adapted features for localization without additional training by leveraging the spatial properties preserved in the value projection of CLIP's transformer-style aggregation [(Zhou et al., 2022a)](#). CLIP Surgery [(Li et al., 2023)](#b20) identified consistent noisy activations across classes and reduced them by subtracting average features from class-specific features [(Li et al., 2023)](#b20), though the cause of these activations remains unclear. GEM generalized this concept to vision transformers [(Bousselham et al., 2024)](#b3). We use the finding that value projection preserves spatial information. We further improve value projection by disentangling class features.

## Unmix-CLIP

Given a multi-label dataset D, where D = {x i } |D| i=1 consists of images x i and N class labels {C j } N j=1 , each image x i can contain objects belonging to one or more of these N classes. Additionally, we use CLIP ( f θ ), parameterized by weights θ , consisting of an image encoder ( f θ ,img ) and a text encoder ( f θ ,text ) for feature extraction. Throughout all experiments, we keep the parameters of CLIP ( f θ ) frozen, including both the image and text encoders.

Since the mutual information among class features present in CLIP is detrimental to fine-grained task performance, we analyze CLIP's features from this perspective. Specifically, we focus on two key aspects: (1) spatial preservation in the visual feature maps and (2) the relationship between class To reduce mutual feature information (MFI) between class features, we propose MFI loss that enforces the self-similarity matrix of projected text features to approximate an identity matrix, effectively reducing inter-class feature dependencies (Section 3.2). We propagate the separation in the text features to image space by aligning the image and separated text features using a multi-label recognition setup (Section 3.3). Following [(Sun et al., 2022)](#b32) and as detailed in Section 3.3, we aggregate the projected image and text features to obtain predicted logits. The predicted logits are trained with ground truth labels using the widely used asymmetric loss (ASL) [(Ridnik et al., 2021)](#b30). Our training loss combines the ASL and MFI loss; the only trainable components are the projectors. We freeze both CLIP encoders and projectors during inference for multi-label recognition and downstream tasks such as zero-shot semantic segmentation.

features in the joint vision-language space.

Towards (1), we remove CLIP's final spatial pooling layer to preserve local information in feature maps. We then evaluate class-wise activations by computing the similarity between local visual and text features. For (2), we find that querying an image for a specific class consistently activates unrelated regions. Figure [1](#) shows that querying for 'person' highlights the person regions and activates areas containing dogs and horses. This suggests that CLIP's features for different classes share substantial information.

To quantify this feature entanglement, we analyze the similarity between class text features across multiple datasets (VOC [(Everingham et al., 2010)](#b12), COCO [(Lin et al., 2014)](#b21), and Context [(Mottaghi et al., 2014)](#b25)). Since CLIP learns a joint embedding space, text feature similarities directly reflect the model's ability to distinguish between classes. As illustrated in Figure [2](#fig_0) (top-row), we consistently observe high similarity values between different classes. Specifically, the similarity reaches 0.84 for person-dog pairs and 0.80 for person-horse pairs, far exceeding what one would expect from their semantic relationships. Extending this analysis across various datasets (Table [4](#tab_4)), We observe high average feature similarities of 0.77 in VOC, 0.69 in COCO, and 0.75 in Context, indicating that this is a universal limitation of CLIP's features space. This feature entanglement fundamentally affects CLIP's ability to perform fine-grained tasks. When features intended to represent one class encode significant information about other classes, the model struggles to make precise discrimination necessary for tasks like multi-label recognition and semantic segmentation.

To address this limitation, we propose a framework that reduces mutual information between class features while preserving task-essential semantics. Our approach consists of three components: (1) Feature extraction and Projection, where we extract CLIP features and project them into a disentangled space (Section 3.1), (2) Defining novel MFI Loss for disentangling text features (Section 3.2), and (3) Performing MLR to align image features to the disentangled text features (Section 3.3).

## Feature Extraction and Projection

We use CLIP as our feature extractor. Its image encoder ( f θ ,img ) performs spatial pooling in the final layer, aggregating features from local regions into a d-dimensional vector for the input image x i . However, this pooling step removes spatial details, making it unsuitable for fine-grained tasks where localization is essential. We remove the final pooling layer to preserve class-specific information across local regions. Then the encoder output for input (

$x i ) is f θ ,img (x i ) = z i ∈ R H×W ×d$, where H and W are the spatial dimensions. The text encoder remains unchanged. We use a fixed pair of positive and negative (txt j,+ , txt j,-) prompts for each class j as input to the text encoder. The positive prompt indicates the presence of the class in a local region, while the negative prompt indicates its absence. Passing these prompts through the text encoder produces f θ ,text

$(txt i ) = t i ∈ R d .$The extracted features (image (z i ), text (t i )) lie in CLIP's original feature space and are not suitable for fine-grained tasks as discussed in Section 1. To address this, we introduce learnable projectors (h φ : h φ ,img and h φ ,text ), parameterized by weights φ . These projectors map the image (z i ) and text (t i ) features from their original space (d-dim) to a new disentangled space (d ′ -dim), making them suitable for fine-grained tasks. The image projector transforms z i → z ′ i (R H×W ×d → R H×W ×d ′ ) while preserving the spatial dimensions (H,W ). The text projector maps

$t i → t ′ i (R d → R d ′ ).$
## MFI Loss

We design the projected feature space to reduce mutual feature information (MFI) between class features. Reducing MFI requires obtaining individual class features, as MFI represents the shared information between these individual features. Separating image features into individual class features is non-trivial because multiple classes often co-occur in an image. This leads to mixed features that make classwise feature isolation difficult. Object segmentation models could assist by extracting features from segmented regions, but these models add significant complexity. In contrast, text class features are inherently independent because they are derived from separate class names or prompts inputted to the text encoder. This independence directly gives us individual text class features. We leverage this property of text features and apply MFI reduction to them.

We propose the MFI reduction loss to minimize the mutual information between class text features. This loss is applied to the projected text features (t ′ ) as follows:

$L MFI = ∑ i=1 (S ii -1) 2 Collapse Prevention +λ ∑ i=1 ∑ j=1 j̸ =i S 2 i j MFI Reduction (1$$)$where S is the self-similarity matrix obtained from t ′ . Here, S is defined by

$S i j = t ′ i t ′ ⊤ j ∥t ′ i ∥∥t ′ j ∥ , ∀i, j$where t ′ i , t ′ j are the i-th and j-th column vectors of t ′ (i.e.,

$t ′ i , t ′ j ∈ R d ′ ) and ∥t ′ i ∥ is the L 2 -norm of t i .$In this formulation, λ is the hyperparameter that addresses the imbalance in the loss arising from the larger number of MFI reduction terms in S compared to the collapse prevention terms.

The MFI loss minimizes the inter-class similarity S i j (i ̸ = j) while simultaneously preserving high intra-class S ii to prevent feature collapse. We provide detailed proof of the loss function's connection to the Information Bottleneck principle in supplementary material Appendix A.

## Image-Text Alignment with MLR

MLR Formulation. MLR task involves identifying the subset of classes C i ⊆ {C 1 ,C 2 , . . . ,C N } associated with the image x i . The goal is to learn a mapping function g : x i → {-1, 1} N , that maps input images to 1 if the class is present and -1 if the class is absent in the image.

We train our model to recognize multiple objects in images by learning to align projected image features and text features. For each location (h, w) in the projected image features (z ′ i ), we detect the presence or absence of a class j, by computing the cosine similarity with positive text features (t ′ j,+ ) and negative text features (t ′ j,-). A higher similarity with the positive text features indicates the presence of the class, while a higher similarity with the negative text features indicates its absence. We aggregate these similarity scores from local regions to produce logits p i for the image, following [(Sun et al., 2022;](#b32)[Rawlekar et al., 2024b;](#)[a)](#). We train the setup with the widely used Asymmetric Loss function (ASL) [(Ridnik et al., 2021)](#b30), which addresses the significant imbalance between negative and positive examples in a multi-label recognition dataset. The ASL loss is given by:

$L ASL (p j i ) =    1 -p j i γ + log p j i , if y j i = 1, p j i,δ γ - log 1 -p j i,δ , else(2)$where p j i represents the corresponding prediction associated with label y j i , i represents the image and j represents the class. p j i,δ = max( ŷδ , 0), with δ representing the shifting parameter defined in ASL.

Training. Our training objective is composed of two components: (1) mutual feature information loss that enforces the separation between class text features and (2) Asymmetric loss function [(Ridnik et al., 2021)](#b30), designed for MLR that aligns the image features and text features to obtain predictions for an image.

$L Unmix-CLIP = L ASL + αL MFI (3)$where α controls the relative importance of the two objectives.

## Experiments

Here we describe the datasets, evaluation metrics, implementation details, and performance analysis for multi-label recognition and zero-shot semantic segmentation. 2) Zero-Shot Semantic Segmentation (ZS3): We use image and text projectors trained on the COCO-14 dataset and evaluate ZS3 using the mIoU metric on the following datasets: PASCAL VOC 2012 [(Everingham et al., 2010)](#b12) includes segmentation masks for the 20 classes in VOC2007. Following works [(Li et al., 2023;](#b20)[Bousselham et al., 2024)](#b3), we evaluate on the validation set. PASCAL Context [(Mottaghi et al., 2014)](#b25) extends Pas-calVOC to 59 classes, 30 of which were unseen during our pre-training. These additional classes provide dense annotations for the whole scene. We evaluate the test set, comprising 5,104 images. COCO-2017 [(Lin et al., 2014)](#b21) includes segmentation masks for the 80 classes in COCO-14. Following [(Li et al., 2023;](#b20)[Bousselham et al., 2024)](#b3), we evaluate the validation set.

## Implementation Details

We use CLIP's [(Radford et al., 2021)](#b27) original pre-trained encoder weights for all our experiments and keep them frozen. Consistent with popular MLR and ZS3 literature, we use a ResNets-based visual encoder (RN-101) and the standard transformer for text encoding [(Sun et al., 2022;](#b32)[Ding et al., 2023;](#b10)[Hu et al., 2023;](#b16)[Rawlekar et al., 2024a;](#)[b;](#b2)[Guo et al., 2023;](#b13)[Li et al., 2023;](#b20)[Lin et al., 2023)](#b22). We conduct all experiments on a single RTX A4000 GPU.

During the pre-training stage with the MLR setup (Section 3.3), we follow the settings and hyperparameters from recent works [(Sun et al., 2022;](#b32)[Rawlekar et al., 2024b;](#)[a)](#). This includes resizing images to 448, applying Cutout (De-Vries & Taylor, 2017) and RandAugment [(Cubuk et al., 2020)](#b7) for augmentation. Our projectors (h φ ) are implemented as multi-layer perceptrons (MLPs). Specifically, the image projector follows a [512 → 256] architecture, while the text projector is designed as [512 → 384 → 256] with batch normalization and ReLU. We train both projectors with stochastic gradient descent (SGD) using an initial learning rate of 0.002, which is reduced by cosine annealing. We train the Unmix-CLIP setup (ASL + MFI loss) for 50 epochs with a batch size of 32. We follow [(Sun et al., 2022;](#b32)[Rawlekar et al., 2024b;](#)[a)](#), and use ASL hyperparameters in Equation ( [2](#formula_7)) as γ -= 2, γ + = 1 and δ = 0.05. We set λ = 0.2 and α = 7e-5 when pre-trained with COCO-14 in Equation ( [1](#formula_3)).

For Zero-Shot Semantic Segmentation, we adopt the v-v attention described in [(Li et al., 2023)](#b20) that prevents inversion of activation commonly observed in CLIP. We then add our pre-trained projectors to CLIP. To obtain the segmentation mask, we compute the cosine similarity between locally projected image features (z ′ ) and projected text features for all classes in the dataset. We use the text template "A photo of a {classname}." Lastly, we use bilinear interpolation to upsample the segmentation mask to the input image size.

## Results

Multi-Label Recognition. We primarily compare Unmix-CLIP with other SOTA VLM-based MLR approaches. In Table 1, we present a detailed comparison of the performance (mAP) and the number of training parameters required by each method on the VOC2007 [(Everingham et al., 2010)](#b12)  and COCO-14 [(Lin et al., 2014)](#b21) datasets. For VOC 2007, we observe that our performance is competitive with Du-alCoOp++ [(Hu et al., 2023)](#b16) and requires the same number of parameters. However, on the more challenging COCO-14 dataset, Unmix-CLIP outperforms DualCoOp++ while requiring only one-third of the training parameters.

Zero-Shot Semantic Segmentation. We categorize our comparisons into two main groups. The first group includes approaches that use local annotations (segmentation masks, etc.) to fine-tune the network [(Xian et al., 2019;](#b34)[Bucher et al., 2019;](#b4)[Lin et al., 2023)](#b22). The second comparison is with training-free approaches [(Radford et al., 2021;](#b27)[Li et al., 2023)](#b20). Our approach is closer to the training-free methods, as it does not use any form of local annotations.

Our results are summarized in Table [2](#tab_2). Following prior works [(Xian et al., 2019;](#b34)[Bucher et al., 2019;](#b4)[Lin et al., 2023;](#b22)[Li et al., 2023)](#b20), we report mIoU values for VOC 2012 by including the background as a class. We use a threshold of 0.85 to identify the background, as suggested in [(Bousselham et al., 2024)](#b3). Our approach outperforms CLIP Surgery by 18.5 mIoU and CLIP-VV by 3.4 mIoU on VOC 2012.

For COCO-14, we report results both with and without the background class. When including the background, our method surpasses CLIP Surgery and CLIP-VV by 9.7 mIoU and 2.8 mIoU, respectively. Without the background, we achieve gains of 14.9 mIoU and 2.3 mIoU. Additionally, we evaluate the VOC Context dataset, which contains 30 unseen classes not used during our pre-training. Although our model is not explicitly trained to reduce MFI between these classes (it is designed to minimize MFI among COCO's 80 classes), our approach still outperforms CLIP Surgery. These results demonstrate that our projectors preserve some of the open-vocabulary capabilities of CLIP. We show qualitative results for open-vocabulary tasks in Supplementary Figure [7](#). Segmentation-driven Zero-Shot Multi-Label Recognition (ZS-MLR). We leverage the segmentation capabilities of Unmix-CLIP to reformulate the multi-label recognition problem into a single-label recognition problem, a domain more suitable for CLIP. Specifically, we use two predictions: global and local. We pass the input image directly through CLIP to obtain its global predictions. However, as discussed in Section 1, these predictions are often dominated by more prominent objects in the image, ignoring smaller objects, which leads to poor zero-shot MLR performance.

To address this limitation, we introduce local predictions. We segment the image into multiple regions (ideally corresponding to individual objects) using Unmix-CLIP. Each segment is then processed independently through CLIP, and the predictions from all segments are aggregated. Finally, we combine the global and local predictions to obtain the zero-shot scores for the image. We evaluate the ZS-MLR performance on the VOC2007 and COCO-14 datasets, with results presented in Table [3](#tab_3). The results demonstrate that our method provides meaningful information (segments) to improve CLIP zero-shot capabilities.     [4](#tab_4) quantifies the MFI reduction through the difference in average inter-class similarity between CLIP and Unmix-CLIP. Our framework effectively disentangles representations for both seen and unseen classes, leading to performance gains.

Feature Disentanglement Impact. Figure [5](#fig_3) shows how MFI reduction improves performance in multi-label recognition on COCO-14 dataset and zero-shot semantic segmentation on the COCO-2017 dataset. We observe that as MFI decreases, the performance of both tasks improves.

## Conclusions

In conclusion, this work advances our understanding of CLIP features by identifying and addressing a fundamental challenge in their localized understanding. We first show that reducing mutual information is critical for fine-grained recognition tasks. Motivated by this, we introduce Unmix-CLIP, a novel approach to project CLIP features into a disentangled space by combining our proposed MFI loss and the asymmetric loss for MLR. Our experimental results demonstrate that reducing feature entanglement through Unmix-CLIP significantly enhances the model's ability to perform fine-grained tasks. This improvement is particularly evident in two challenging tasks: multi-label recognition (MLR) and zero-shot semantic segmentation (ZS3). These findings highlight the importance of feature disentanglement in vision-language models and provide a promising direction for future research in improving the localized understanding capabilities of CLIP-based architectures. A limitation of our approach is its reduced capability in zeroshot open-vocabulary segmentation. We constrain some of CLIP's broader semantic capabilities by optimizing feature disentanglement for COCO dataset classes. Training on substantially larger datasets could help mitigate this limitation while preserving the benefits of our feature disentanglement approach.

## A. Objective Function: MFI Loss

This section establishes a connection between MFI loss and the Information Bottleneck (IB) principle [(Tishby & Zaslavsky, 2015)](#b33). As described in Section 3.2, MFI loss explicitly reduces the mutual information between text features to obtain disentangled features.

A.1. Information Bottleneck (IB) Objective Formulation. Let, T i represent the input text (i.e., the prompt with the class label), and let Z i be the extracted features from the CLIP text encoder [(Radford et al., 2021)](#b27). The output is represented by Y i , indicating the class associated with Z i .

As we show in Section 3, mutual information exists between text (class) features Z i , i.e., each class feature contains information about multiple classes rather than only its corresponding class Y i . Our goal is to enforce a one-to-one mapping where Z i retains information only about Y i while discarding information about all other classes Y j ( j ̸ = i).

This aligns naturally with the IB principle, which formulates an optimal trade-off between minimizing the information Z i retains from T i and maximizing the information it preserves for the target class Y i . We extend the IB principle to reduce explicit information about all other classes. We express this formally as:

$IB = I(Z i , T i ) + β I(Z i ,Y i ) -∑ j̸ =i I(Z i ,Y j )(4)$where I represents mutual information. Here, 1. I(Z i ; T i ) ensures that Z i take only the information form T i that is needed to map to Y i . 2. I(Z i ;Y i ) preserves discriminative class information.

3. ∑ j̸ =i I(Z i ;Y j ) reduces information in Z i that map to Y j where j ̸ = i

## A.2. Connection to MFI Loss

To minimize IB, we first express mutual information in terms of entropy:

$I(A; B) = H(A) -H(A|B),(5)$where H(A) is the marginal entropy of A, and H(A|B) is the conditional entropy of A given B.

Substituting this into the IB objective Equation (4): 

$IB = (1 + β -∑ j̸ =i β )H(Z i ) -β H(Z i |Y i ) -∑ j̸ =i H(Z i |Y j ) -H(Z i |T i )(6)$
## Input Text Text Features

## Classes

Figure [6](#): The Information Bottleneck principle is applied for feature disentanglement. Given an input text T i , the text encoder of CLIP [(Radford et al., 2021)](#b27) generates features Z i , which encode information about the output classes Y i . Our objective is to ensure that Z i retains only the information necessary to map to its corresponding class Y i while minimizing its information about other classes Y j ( j ̸ = i) Since the CLIP text encoder is deterministic, the entropy term H(Z i |T i ) = 0. Also, given that text inputs are predefined (i.e., class names in the dataset), Z i is deterministic, implying H(Z i ) = 0. This simplifies the IB objective to:

$IB ∝ -H(Z i |Y i ) + ∑ j̸ =i H(Z i |Y j ).(7)$Assuming Z follows a Gaussian distribution, its entropy is given by:

$H(Z) = 1 2 log |C| + const, (8$$)$where C is the covariance matrix of Z i . Since the constant term does not affect the optimization, we optimize the determinant of the covariance matrix C. In practice, we optimize the covariance matrix. Thus, minimizing IB reduces the covariance between class features, ensuring they are independent.

The IB objective in Equation ( [7](#formula_12)) becomes:

$IB ∝ -C Z i |Y i + ∑ j̸ =i C Z i |Y j ,(9)$Minimizing the IB objective is equivalent to minimizing the MFI Loss. Specifically, maximizing C Z i |Y i is equivalent to collapse prevention term and minimizing ∑ j̸ =i C Z i |Y j is our MFI reduction term in the following equation: 

$L MFI = ∑ i=1 (S ii$![Figure 2: Class Feature Similarity Analysis. Comparison of class-text feature similarities between CLIP (top row) and our method (bottom row) across four datasets: VOC, COCO, Context, and Context (unseen). The heatmaps show cosine similarity between class text features, where darker blue indicates higher similarity. As the reduced off-diagonal similarity values show, our method achieves higher class feature separation. This improved class separation suggests better discrimination capabilities.]()

![Figure3: Unmix-CLIP Overview. Given image and label names in the dataset, CLIP extracts image and text features, which are further processed by respective projectors to embed into a disentangled space while preserving local image information. To reduce mutual feature information (MFI) between class features, we propose MFI loss that enforces the self-similarity matrix of projected text features to approximate an identity matrix, effectively reducing inter-class feature dependencies (Section 3.2). We propagate the separation in the text features to image space by aligning the image and separated text features using a multi-label recognition setup (Section 3.3). Following(Sun et al., 2022) and as detailed in Section 3.3, we aggregate the projected image and text features to obtain predicted logits. The predicted logits are trained with ground truth labels using the widely used asymmetric loss (ASL)(Ridnik et al., 2021). Our training loss combines the ASL and MFI loss; the only trainable components are the projectors. We freeze both CLIP encoders and projectors during inference for multi-label recognition and downstream tasks such as zero-shot semantic segmentation.]()

![Figure 4: Qualitative Comparison on ZS3. Visualization of zero-shot semantic segmentation (ZS3) results for CLIP(Radford et al., 2021), CLIP Surgery (CS)(Li et al., 2023), CLIP-VV(Li et al., 2023), and our approach across multiple categories. The heatmaps show activation regions for each queried class, where darker red indicates strongly activated regions. Our method produces more separated activations, demonstrating improved class localization.]()

![Figure 5: Performance vs. MFI Reduction. Performance of Multi-Label Recognition (MLR, measured by mAP) and Zero-Shot Semantic Segmentation (ZS3, measured by mIoU) on COCO as a function of MFI reduction. As class feature separation increases (i.e., MFI decreases), the model performs better on both tasks.]()

![Disentanglement. We pre-train Unmix-CLIP on COCO-14, which contains 80 classes. As shown in Section 4.3, our approach improves performance even on datasets with previously unseen classes, such as VOC Context. We analyze this improvement by comparing MFI reduction across four datasets: VOC2012 (20 seen classes), COCO-2017 (80 seen classes), Context (59 partially seen classes), and a Context subset (30 unseen classes from COCO-2017). Figure2shows the self-similarity matrices of class text features from CLIP and Unmix-CLIP, demonstrating the class feature disentanglement. Table]()

![Figure 8: mAP vs mIoU. Performance comparison of zero-shot semantic segmentation (mIoU) for VOC2012, COCO 2017 with and without the background, and VOC Context as a function of multi-label recognition (mAP) performance on the COCO-14 dataset. A general trend: higher MLR performance positively correlates with segmentation results.]()

![]()

![Comparison on multi-label recognition (MLR). We compare the performance (mAP) and training efficiency (number of parameters) of our approach with SOTA VLM-based MLR methods on VOC2007 and COCO-14 datasets. Our approach is competitive with SOTA on VOC2007, and on the challenging COCO dataset, it outperforms SOTA while requiring only one-third of the parameters. red and blue indicate the best and the second best performance.]()

![Comparison on zero-shot semantic segmentation (ZS3). We compare Unmix-CLIP with other SOTA baselines across three semantic segmentation datasets using the mIoU metric. The "Dataset" column details the pre-training dataset and the type of annotations used. The abbreviations are as follows: Loc Ann. + FT: local annotations and fine-tuning, SM: segmentation mask, IT: image-text, IC: image classes, Bkgd: include background class, No Bkgd: ignore background class, MR: MFI Reduction(%), red and blue indicate the best and the second best performance]()

![Comparison on Zero-shot Multi-Label Recognition (ZS-MLR). We segment objects from images using Unmix-CLIP and improve CLIP's zero-shot multi-label recognition capabilities by integrating predictions from segmented objects and the entire image.]()

![Quantitative MFI Reduction. MFI values are reported across different dataset datasets with seen (VOC, COCO), partial (Context), and unseen (Context) classes. Our method significantly reduces MFI for all datasets.]()

![MFI Loss Ablation Study. Adding MFI Loss to our method improves multi-label recognition (MLR) performance by 0.5 mAP on the COCO-14 dataset, demonstrating its effectiveness for fine-grained tasks.]()

Department of Electrical and Computer Engineering, University of Illinois Urbana-Champaign, USA

University ofQueensland, Brisbane, Australia

UC Merced, USA

Yonsei University, South Korea. Correspondence to: Samyak Rawlekar <samyakr2@illinois.edu>.

