The research conducted on the multilingual language models (LLMs), specifically Llama-2, aims to investigate whether these models utilize English as an internal pivot language when processing non-English prompts. This inquiry is significant for understanding the operational mechanics of LLMs and the potential biases that may arise from their training data.

### Research Question Justification

The central research question—whether Llama-2 uses English as an internal pivot language—addresses a critical gap in the understanding of multilingual LLMs. Given that Llama-2 was trained predominantly on English data (89.70%), it is essential to explore how this bias influences the model's performance on non-English tasks. The implications of this research extend to the broader field of AI, particularly concerning the fairness and reliability of LLMs in multilingual contexts.

### Key Findings and Their Rationale

1. **Three Phases of Embedding Transformation**:
   - **Input Space**: The initial embeddings are distant from the output token embeddings, indicating that the model starts with raw input representations that do not yet align with any specific language.
   - **Concept Space**: In the middle layers, the model can semantically decode the next token but shows a preference for English versions. This suggests that the model's internal representations may be biased towards English, potentially due to the training data's predominance.
   - **Output Space**: The final layers adjust to produce embeddings specific to the input language, indicating a shift from an English-centric representation to one that aligns with the target language.

   This phased transformation is crucial for understanding how Llama-2 processes multilingual inputs and highlights the potential for Anglocentric biases in its internal representations.

2. **Conceptual Model**:
   - The three phases correspond to distinct operational spaces: raw input, abstract concept, and final output. This model provides a framework for analyzing how Llama-2 transitions from understanding input to generating output, emphasizing the role of English in the intermediate processing stages.

3. **Mechanistic Interpretability**:
   - The use of the logit lens technique allows researchers to analyze intermediate embeddings and their transformations through the model's layers. By applying the language modeling head to earlier layers, the researchers can observe how token probabilities evolve, revealing a tendency to favor English tokens in the concept space.

4. **Logit Lens Technique**:
   - This technique is pivotal for understanding the model's internal workings. By decoding next-token distributions from intermediate layers, researchers can track how the model's predictions shift from English to the target language, providing empirical evidence for the hypothesis of English as a pivot.

### Model Details and Implications

- **Llama-2 Architecture**: The autoregressive, decoder-only transformer architecture with varying parameter sizes (7B, 13B, 70B) allows for a comprehensive analysis of how different model sizes may exhibit similar or varying degrees of Anglocentric bias.
- **Training Data Composition**: The overwhelming presence of English in the training data raises concerns about the model's ability to generalize effectively to non-English languages, potentially leading to biased outputs that reflect English linguistic structures.

### Future Directions

The findings suggest a need for further research into the latent biases present in LLMs and their implications for trustworthy AI. Understanding how these biases manifest in multilingual contexts is crucial for developing more equitable AI systems that can perform reliably across diverse languages and cultures.

### Conclusion

The investigation into whether Llama-2 uses English as an internal pivot language reveals significant insights into the model's operational mechanics and the potential biases that arise from its training data. The phased embedding transformation, coupled with the logit lens technique, provides a robust framework for understanding how LLMs process multilingual inputs and highlights the importance of addressing Anglocentric biases in future AI research.