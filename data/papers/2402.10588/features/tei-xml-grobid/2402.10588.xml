<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Do Llamas Work in English? On the Latent Language of Multilingual Transformers</title>
				<funder ref="#_XRcb63X">
					<orgName type="full">Meta</orgName>
				</funder>
				<funder>
					<orgName type="full">Microsoft</orgName>
				</funder>
				<funder>
					<orgName type="full">Google</orgName>
				</funder>
				<funder ref="#_hXaxHPm">
					<orgName type="full">Swiss Data Science Center</orgName>
				</funder>
				<funder ref="#_kB83N2H #_c3xtRH9">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_BDgaCrW">
					<orgName type="full">Swiss National Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-06-08">8 Jun 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chris</forename><surname>Wendler</surname></persName>
							<email>chris.wendler@epfl.ch</email>
						</author>
						<author>
							<persName><forename type="first">Veniamin</forename><surname>Veselovsky</surname></persName>
							<email>veniamin.veselovsky@epfl.ch</email>
						</author>
						<author>
							<persName><forename type="first">Giovanni</forename><surname>Monea</surname></persName>
							<email>giovanni.monea@epfl.ch</email>
						</author>
						<author>
							<persName><forename type="first">Robert</forename><surname>West</surname></persName>
							<email>robert.west@epfl.ch</email>
						</author>
						<title level="a" type="main">Do Llamas Work in English? On the Latent Language of Multilingual Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-06-08">8 Jun 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">A0622621A8AF6D1A7B47E5510CB3AA7A</idno>
					<idno type="arXiv">arXiv:2402.10588v4[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We ask whether multilingual language models trained on unbalanced, English-dominated corpora use English as an internal pivot languagea question of key importance for understanding how language models function and the origins of linguistic bias. Focusing on the Llama-2 family of transformer models, our study uses carefully constructed non-English prompts with a unique correct single-token continuation. From layer to layer, transformers gradually map an input embedding of the final prompt token to an output embedding from which next-token probabilities are computed. Tracking intermediate embeddings through their high-dimensional space reveals three distinct phases, whereby intermediate embeddings (1) start far away from output token embeddings; (2) already allow for decoding a semantically correct next token in middle layers, but give higher probability to its version in English than in the input language;</p><p>(3) finally move into an input-language-specific region of the embedding space. We cast these results into a conceptual model where the three phases operate in "input space", "concept space", and "output space", respectively. Crucially, our evidence suggests that the abstract "concept space" lies closer to English than to other languages, which may have important consequences regarding the biases held by multilingual language models. Code and data is made available here: <ref type="url" target="https://github.com/epfl-dlab/llm-latent-language">https://github.com/ epfl-dlab/llm-latent-language</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Most modern large language models (LLMs) are trained on massive corpora of mostly English text <ref type="bibr">(Touvron et al., 2023;</ref><ref type="bibr" target="#b35">OpenAI, 2023)</ref>. Despite this, they achieve strong performance on a broad range of downstream tasks, even in non-English languages <ref type="bibr" target="#b44">(Shi et al., 2022)</ref>. This raises a compelling question: How are LLMs able to generalize * Equal contribution.</p><p>Figure <ref type="figure">1</ref>: Illustration of logit lens, which applies language modeling head (here, Llama-2-7B) prematurely to latent embeddings in intermediate layers, yielding one next-token distribution per position (x-axis) and layer (y-axis). We show final tokens of translation prompt (cf. Sec. 3.3) ending with "Français: "fleur" -中文: "" (where "中文" means "Chinese"). Final layer correctly ranks "花" (translation of "fleur") on top, whereas intermediate layers decode English "flower". Color indicates entropy of next-token distributions from low (blue) to high (red). (Plotting tool: <ref type="bibr" target="#b3">Belrose et al. (2023)</ref>.) so well from their mainly English training data to other languages?</p><p>Intuitively, one way to achieve strong performance on non-English data in a data-efficient manner is to use English as a pivot language, by first translating input to English, processing it in English, and then translating the answer back to the input language. This method has been shown to lead to high performance when implemented explicitly <ref type="bibr" target="#b44">(Shi et al., 2022;</ref><ref type="bibr" target="#b0">Ahuja et al., 2023;</ref><ref type="bibr" target="#b21">Huang et al., 2023)</ref>. Our guiding inquiry in this work is whether pivoting to English also occurs implicitly when LLMs are prompted in non-English.</p><p>In the research community as well as the popular press, many seem to assume that the answer is yes, epitomized by claims such as, "The machine, so to say, thinks in English and translates the conversation at the last moment into Estonian" <ref type="bibr" target="#b37">(Piir, 2023)</ref>. In this work, we set out to move beyond such speculation and investigate the question empirically.</p><p>The question is of major importance. On the one hand, implicitly using English as an internal pivot could bias LLMs toward Anglocentric patterns that could predispose the model to certain linguistic elements <ref type="bibr">(lexicon, grammar, metaphors, etc.)</ref>, while also shaping more profound behaviors related to, e.g., emotional stance <ref type="bibr" target="#b5">(Boroditsky et al., 2003)</ref> or temporal reasoning <ref type="bibr" target="#b34">(Núñez and Sweetser, 2006)</ref>. On the other hand, if LLMs do not use English as a pivot, it raises questions of how else they manage to work so remarkably well even in low-resource languages. Overall, the quest for an internal pivot language holds promise to advance our understanding of how LLMs function no matter if we succeed.</p><p>Investigating the existence of an internal LLM language is complicated by the scale and notoriously inscrutable nature of the neural networks behind LLMs, which after the input layer do not operate on discrete tokens, but on high-dimensional floating-point vectors. How to understand if those vectors correspond to English, Estonian, Chinese, etc.-or to no language at all-is an open problem, and the question of whether LLMs use an internal pivot language has therefore, to the best of our knowledge, not been addressed empirically before.</p><p>Summary of contributions. To overcome these hurdles, we draw on, and contribute to, the nascent field of mechanistic interpretability (cf. Sec. 2). In a transformer, each input token's embedding vector is gradually transformed layer by layer without changing its shape. After the final layer, an "unembedding" operation turns the vector into a nexttoken distribution. Focusing on the Llama-2 family of models <ref type="bibr">(Touvron et al., 2023)</ref>-among today's largest open-source LLMs-we find that applying the "unembedding" operation prematurely in intermediate, non-final layers-a technique called logit lens <ref type="bibr" target="#b33">(Nostalgebraist, 2020)</ref>-already decodes a contextually appropriate token early on (Fig. <ref type="figure">1</ref>), giving us a (limited) glimpse at the model's otherwise hard-to-interpret numerical internal state.</p><p>Exploiting this fact, we carefully devise prompts that allow us to determine whether a logit-lens-decoded token is semantically correct and to what language it belongs (e.g., a prompt asking the model to translate French "fleur" ["flower"] to Chinese "花"; cf. Fig. <ref type="figure">1</ref>). Tracking language probabilities across layers, we observe that no contextually appropriate tokens are decoded in the first half of layers, followed by a sudden shift of probability mass onto the English version ("flower") of the correct next token, and finally a shift to the correct next token in the target language ("花").</p><p>Expanding on this first evidence of English as an internal pivot language, we analyze latent embeddings directly as high-dimensional Euclidean points, rather than via the logit lens. This allows us to draw a more nuanced picture of the anatomy of Llama-2's forward pass, suggesting that, in middle layers, the transformer operates in an abstract "concept space" that is partially orthogonal to a language-specific "token space", which is reached only in the final layers. In this interpretation, the latent embeddings' proximity to English tokens observed through the logit lens follows from an English bias in concept space, rather than from the model first translating to English and then "restarting" its forward pass from there.</p><p>We conclude by discussing implications and future directions for studying latent biases and their effects-a crucial step toward trustworthy AI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Multilingual language models. Multilingual language models (LMs) are trained to simultaneously handle multiple input languages. Examples include mBERT <ref type="bibr" target="#b14">(Devlin et al., 2018)</ref>, mBART <ref type="bibr" target="#b27">(Liu et al., 2020)</ref>, XLM-R <ref type="bibr">(Conneau et al., 2020a)</ref>, mT5 <ref type="bibr">(Xue et al., 2021)</ref>, <ref type="bibr">XGLM (Lin et al., 2022</ref><ref type="bibr">), mGPT (Shliazhko et al., 2022)</ref>, <ref type="bibr">BLOOM (Scao et al., 2022)</ref>, and PolyLM <ref type="bibr" target="#b48">(Wei et al., 2023)</ref>. Current frontier models such as GPT-4, PaLM, and Llama-2, despite performing better in English due to their Anglocentric training data <ref type="bibr" target="#b21">(Huang et al., 2023;</ref><ref type="bibr" target="#b2">Bang et al., 2023;</ref><ref type="bibr" target="#b52">Zhang et al., 2023)</ref>, still do well across languages <ref type="bibr" target="#b44">(Shi et al., 2022)</ref>.</p><p>Researchers have devised numerous methods for efficiently transferring LM capabilities across languages, e.g., by aligning contextual embeddings <ref type="bibr" target="#b43">(Schuster et al., 2019;</ref><ref type="bibr" target="#b7">Cao et al., 2020)</ref>, relearning embedding matrices during finetuning on a new language <ref type="bibr" target="#b1">(Artetxe et al., 2020)</ref>, or repeatedly doing so during pretraining <ref type="bibr" target="#b8">(Chen et al., 2023)</ref>.</p><p>Several approaches leverage English as a pivot language. For instance, <ref type="bibr" target="#b54">Zhu et al. (2023)</ref> show that Llama can be efficiently augmented with multilingual instruction-following capabilities thanks to its English representations. Likewise, <ref type="bibr" target="#b53">Zhu et al. (2024)</ref> demonstrate the feasibility of leveraging language models' proficiency in English for non-English contexts by fine-tuning them on translation data and English-only instructional data. They successfully employ this approach to enhance the multilingual reasoning capabilities of Llama-2. Regarding non-Latin low-resource languages, <ref type="bibr" target="#b22">Husain et al. (2024)</ref> illustrate that leveraging both romanized and English data proves to be an effective strategy for efficiently improving multilingual task performance. Prompting strategies, too, can improve multilingual performance by leveraging English as a pivot language, e.g., by simply first translating prompts to English <ref type="bibr" target="#b44">(Shi et al., 2022;</ref><ref type="bibr" target="#b0">Ahuja et al., 2023;</ref><ref type="bibr" target="#b17">Etxaniz et al., 2023)</ref> or by instructing LMs to perform chain-of-thought reasoning <ref type="bibr" target="#b47">(Wei et al., 2022)</ref> in English <ref type="bibr" target="#b21">(Huang et al., 2023)</ref>.</p><p>Although employing high-resource languages can enhance performance on low-resource languages, it might also bias output generation in low-resource languages, e.g., in terms of grammar <ref type="bibr" target="#b36">(Papadimitriou et al., 2022)</ref>.</p><p>Researchers have also investigated how latent representations differ across languages within multilingual models. In the case of encoder-only models such as mBERT, converging evidence suggests the existence of a language-agnostic space in later layers following language-specific early layers <ref type="bibr" target="#b25">(Libovický et al., 2020;</ref><ref type="bibr">Conneau et al., 2020b;</ref><ref type="bibr" target="#b31">Muller et al., 2021;</ref><ref type="bibr" target="#b9">Choenni and Shutova, 2020)</ref>.</p><p>Mechanistic interpretability. The nascent field of mechanistic interpretability (MI) aims to reverse-engineer and thereby understand neural networks, using techniques such as circuit discovery <ref type="bibr" target="#b32">(Nanda et al., 2023;</ref><ref type="bibr" target="#b10">Conmy et al., 2023)</ref>, controlled task-specific training <ref type="bibr" target="#b24">(Li et al., 2022;</ref><ref type="bibr" target="#b28">Marks and Tegmark, 2023)</ref>, and causal tracing <ref type="bibr">(Meng et al., 2022;</ref><ref type="bibr" target="#b30">Monea et al., 2023)</ref>.</p><p>For smaller models, e.g., GPT-2 <ref type="bibr" target="#b40">(Radford et al., 2019)</ref> and Pythia <ref type="bibr" target="#b4">(Biderman et al., 2023)</ref>, MI approaches such as sparse probing <ref type="bibr" target="#b19">(Gurnee et al., 2023)</ref> have revealed monosemantic French <ref type="bibr" target="#b19">(Gurnee et al., 2023)</ref> and German <ref type="bibr" target="#b39">(Quirke et al., 2023)</ref> language neurons and context-dependent German ngram circuits (subnetworks for boosting the probability of German n-grams when the monosemantic German context neuron is active) <ref type="bibr" target="#b39">(Quirke et al., 2023)</ref>.</p><p>The most relevant tools from the MI repertoire in the context of this work are the logit lens (Nos-talgebraist, 2020), tuned lens <ref type="bibr" target="#b3">(Belrose et al., 2023)</ref>, and direct logit attribution <ref type="bibr" target="#b16">(Elhage et al., 2021)</ref>, which decode intermediate token representations from transformer models in different ways. The logit lens does so by using the language modeling head, which is usually only applied in the final layer, prematurely in earlier layers, without any additional training. The more sophisticated tuned lens additionally trains an affine mapping for transforming an intermediate latent state such that it mimics the token predictions made by the final latent state. Finally, direct logit attribution generalizes the logit lens by considering the logit contribution of each individual attention head.</p><p>In this work, we heavily rely on the logit lens, described further in Sec. 3.2, as opposed to the tuned lens. The latter would defeat our purpose of understanding whether Llama-2, when prompted in non-English, takes a detour via English internal states before outputting non-English text. As the tuned lens is specifically trained to map internal states-even if corresponding to English-to the final, non-English next-token prediction, the optimization criterion would "optimize away" our signal of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Materials and methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Language models: Llama-2</head><p>We focus on the Llama-2 family of language models <ref type="bibr">(Touvron et al., 2023)</ref>, some of the largest and most widely used open-source models. The models were trained on a multilingual corpus that is largely dominated by English, which comprises 89.70% of the corpus. However, given the size of the training data (two trillion tokens), even a small percentage of non-English training data still constitutes a large number of tokens in absolute terms (e.g., 0.17% = 3.4B German tokens, 0.13% = 2.6B Chinese tokens). Consequently, Llama-2 is, despite its English bias, considered a multilingual model.</p><p>Versions. Llama-2 comes in three model sizes, with 7B/13B/70B parameters, 32/40/80 layers, and embedding dimension d = 4096/5120/8192, respectively. Across all model sizes, the vocabulary V contains v = 32,000 tokens. Here we study all model sizes, using 8-bit quantization <ref type="bibr" target="#b13">(Dettmers et al., 2022)</ref> in our experiments.</p><p>Architecture. Llama-2 is an autoregressive, decoder-only, residual-based transformer. Such models maintain the shape of the input data throughout the computation process during a forward pass: one embedding vector, a so-called latent, per input token x 1 , . . . , x n ∈ V , where n is the input sequence length. The initial latents h</p><formula xml:id="formula_0">(0) 1 , . . . , h<label>(0)</label></formula><p>n ∈ R d are obtained from a learned embedding dictionary that contains one fixed vector per vocabulary token. Each of these latents is incrementally updated layer by layer by adding a residual. The residual added to the latent at position i in layer j is a function f j of all preceding tokens' latents h</p><formula xml:id="formula_1">( j-1) 1 , . . . , h ( j-1) i : h ( j) i = h ( j-1) i + f j h ( j-1) 1 , . . . , h ( j-1) i , (1)</formula><p>where the resulting vector h ( j) i is still of dimension d. The function f j itself, called a transformer block, is composed of a masked self-attention layer followed by a feed-forward layer with a residual connection and root mean square (RMS) normalization in between <ref type="bibr" target="#b46">(Vaswani et al., 2017;</ref><ref type="bibr">Touvron et al., 2023)</ref>. Due to RMS normalization, all latents lie on a d-dimensional hypersphere of radius √ d. In pretraining, all transformer blocks f 1 , . . . , f m (with m the number of layers) are tuned such that the final latent h (m) i for position i is well-suited for predicting the token at position i + 1. For prediction, the final embedding vector is multiplied with a so-called unembedding matrix U ∈ R v×d , which yields a real vector z i = Uh (m) i ∈ R v containing a so-called logit score z it for each vocabulary token t ∈ V . These scores are then transformed into probabilities P(x i+1 = t | x 1 , . . . , x i ) ∝ e z it via the softmax operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Interpreting latent embeddings: Logit lens</head><p>When transformers are deployed in practice, only the final latent vectors after the last transformer block are turned into token distributions by multiplying them with U and taking a softmax. However, since latents have the same shape in all layers, any latent can in principle be turned into a token distribution, by treating it as though it were a finallayer latent. Prematurely decoding tokens from latents this way, a method called the logit lens (cf. Sec. 2), can facilitate the inspection and interpretation of the internal state of transformers. Using the logit lens, we obtain one next-token distribution P(x i+1 | h ( j) i ) per position i and layer j. We illustrate the logit lens in Fig. <ref type="figure">1</ref>, where every cell shows the most likely next token when applying the logit lens to the latent in that position and layer. As seen, the logit lens decodes contextually appropriate tokens already in intermediate layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Data: Tasks for eliciting latent language</head><p>Our goal is to explore whether Llama-2's internal, latent states correspond to specific natural languages. Although the logit lens allows us to map latent vectors to token distributions, we still require a mapping from token distributions to languages.</p><p>Doing so in general is difficult as many tokens are ambiguous with respect to language; e.g., the token "an" is commonly used in English, French, and German, among others. To circumvent this issue, we construct prompts x 1 . . . x n where the correct next token x n+1 is (1) obvious and ( <ref type="formula" target="#formula_4">2</ref>) can be unambiguously attributed to one language.</p><p>Prompt design. To ensure that the next token is obvious (criterion 1), we design three text completion tasks where the next token x n+1 can be easily inferred from the prompt x 1 . . . x n . In describing the tasks, we use Chinese as an example language.</p><p>Translation task. Here the task is to translate the preceding non-English (e.g., French) word to Chinese. We show the model four words with their correct translations, followed by a fifth word without its translation, and let the model predict the next token ("中文" means "Chinese" below):</p><p>Français: "vertu" -中文: "德" Français: "siège" -中文: "座" Français: "neige" -中文: "雪" Français: "montagne" -中文: "山" Français: "fleur" -中文: " With such a prompt, Llama-2 can readily infer that it should translate the fifth French word. We carefully select words as described below and construct one prompt per word by randomly sampling demonstrations from the remaining words.</p><p>Repetition task. Similarly, we task the model to simply repeat the last word, instead of translating it, by prompting as follows:</p><p>中文: "德" -中文: "德" 中文: "座" -中文: "座" 中文: "雪" -中文: "雪" 中文: "山" -中文: "山" 中文: "花" -中文: " Cloze task. As a slightly harder task, we consider a cloze test, where the model must predict a masked word in a sentence. Given a target word, we construct an English sentence starting with the word by prompting GPT-4, mask the target word, and translate the sentence to the other languages. To construct prompts, we sample two demonstrations from the remaining words. An English example before translation to the other languages follows:</p><p>A "___" is used to play sports like soccer and basketball. Answer: "ball".</p><p>A "___" is a solid mineral material forming part of the surface of the earth. Answer: "rock".</p><p>A "___" is often given as a gift and can be found in gardens. Answer: "</p><p>Word selection. To enable unambiguous language attribution (criterion 2), we construct a closed set of words per language. As a particularly clean case, we focus on Chinese, which has many single-token words and does not use spaces. We scan Llama-2's vocabulary for single-token Chinese words (mostly nouns) that have a single-token English translation. This way, Llama-2's probabilities for the correct next Chinese word and for its English analog can be directly read off the next-token probabilities.</p><p>For robustness, we also run all experiments on German, French, and Russian. For this, we translate the selected Chinese/English words and, for each language, discard words that share a token pre-fix with the English version, as this would render language detection (cf. Sec. 3.4) ambiguous.</p><p>We work with 139 Chinese, 104 German, 56 French, and 115 Russian words (cf. Appendix A.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Measuring latent language probabilities</head><p>To investigate a hypothetical pivot language inside Llama-2, we apply the logit lens to the latents h ( j) n corresponding to the last input token x n for each layer j, obtaining one next-token distribution P(x n+1 | h ( j) n ) per layer. Our prompts (cf. Sec. 3.3) are specifically designed such that an intermediate next-token distribution lets us estimate the probability of the correct next word in the input language as well as English. Since we specifically select single-token words in Chinese (ZH) as well as English (EN), we can simply define the probability of language ℓ ∈ {ZH, EN} as the probability of the next token being ℓ's version t ℓ of the correct singletoken word:</p><formula xml:id="formula_2">P(lang = ℓ | h ( j) n ) := P(x n+1 = t ℓ | h ( j) n ).</formula><p>(For readability we also simply write P(lang = ℓ).) Note that this does not define a distribution over languages, as generally ℓ P(lang = ℓ) &lt; 1.</p><p>In other languages (and in corner cases in Chinese and English), we must account for multiple tokenizations and whitespaces (cf. Appendix A.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>When presenting results, we first (Sec. 4.1) take a probabilistic view via the logit lens (Sec. 3.2), for all tasks and all model sizes. (Since the results are consistent across languages, we focus on Chinese here and refer to Appendix B for French, German, and Russian.) Then (Sec. 4.2) we drill deeper by taking a geometric view of how token embeddings drift as the transformer computes layer by layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Probabilistic view: Logit lens</head><p>The logit lens gives us one set of language probabilities (cf. Sec. 3.4) per input prompt and layer. Fig. <ref type="figure" target="#fig_0">2</ref> tracks the evolution of language probabilities from layer to layer, with one plot per combination of model size (columns) and task 1 (rows). The x-axes show layer indices, and the y-axis the language probabilities P(lang = ZH) and P(lang = EN) averaged over input prompts.</p><p>On the translation and cloze tasks a consistent picture emerges across model sizes. Neither the correct Chinese token nor its English analog garner any noticeable probability mass during the first half of layers. Then, around the middle layer, English begins a sharp rise followed by a decline, while Chinese slowly grows and, after a crossover with English, spikes on the last five layers. On the repetition task, Chinese already rises alongside English (discussed in Sec. 6). This is in contrast to all other languages, where English rises first (Appendix B).</p><p>On top of the language probabilities (Sec. 3.4), the entropy of the full next-token distribution is shown as a heatmap above the plots. We again observe a consistent pattern across tasks and model sizes: high entropy in the first half of layers, while both P(lang = ZH) and P(lang = EN) are close to zero, followed by a sharp drop at the same time that P(lang = EN) rises. From there on, entropy remains low, with a slight rebound as probability mass shifts from English to Chinese.</p><p>With 32,000 ≈ 2 15 tokens in the vocabulary, the early entropy of around 14 bits implies a close-touniform next-token distribution (around 15 bits). 1 In Fig. <ref type="figure" target="#fig_0">2</ref>, translation task uses union of German, French, and Russian as source languages. For individual source languages, as well as all target languages, cf. Appendix B. Path visualization. The plots of Fig. <ref type="figure" target="#fig_0">2</ref> only consider the probability of the correct Chinese next token and its English analog, without speaking to the remaining tokens. To form an intuition of the entire distribution, we use dimensionality reduction to visualize the data. First, we define the distance between a latent h n at position n and a token t via the negative log-likelihood of t given h n , as computed by the logit lens (cf. Sec. 3.4):</p><formula xml:id="formula_3">d(h n ,t) = -log P(x n+1 = t | h n ).</formula><p>Then, we use classical multidimensional scaling to embed tokens and latents in an approximately distance-preserving joint 2D space. (Intra-token and intra-latent distances are set to max h,t d(h,t), which serves as a "spring force" pushing the 2D points apart.)</p><p>A transformer's forward computation for a given final input token x n can now be visualized by connecting the 2D embeddings of the latents h ( j) n in subsequent layers j, as presented and explained in Fig. <ref type="figure" target="#fig_1">3</ref> (German-to-Chinese translation, 70B). We make two observations: (1) An English and a Chinese token cluster emerges, suggesting that the same latent also gives high probability to an entire language, in addition to the language-specific version of the correct next token. (2) Paths first pass through the English cluster, and only later reach the Chinese cluster. Taken together, the emerging picture is that, when translating a German word to Chinese, Llama-2 takes a "detour" through an English subspace.</p><p>So far, we have characterized the transformer's intermediate latent states from a probabilistic perspective, by studying the next-token distributions obtained via the logit lens. For a deeper understanding, we next take a geometric perspective and analyze latents directly as points in Euclidean space, i.e., before mapping them to token probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Geometric view: An 8192D space Odyssey</head><p>Simplistically, the task solved by an autoregressive transformer is to map the input embedding of the current token to the output embedding of the next token. The task is solved incrementally, each layer modifying (by adding a residual) the latent vector produced by the previous layer, a process that, geometrically, describes a path through d-dimensional Euclidean space. We now set out to characterize this path. Since the probabilistic view (Fig. <ref type="figure" target="#fig_0">2</ref>) gave consistent results across tasks and model sizes, we focus on one task (translation) and one model size (70B, i.e., d = 8192).</p><p>Embedding spheres. Output token embeddings (rows of the unembedding matrix U) and latents h cohabitate the same d-dimensional Euclidean space. In fact, due to RMS-normalization (Sec. 3.1), latents by construction live on a hypersphere of radius √ d ≈ 90.1. Additionally, by analyzing the 2-norm of output token embeddings (mean 1.52, SD 0.23), we find that the latter also approximately lie on a sphere, of radius 1.52.</p><p>Token energy. Importantly, token embeddings occupy their sphere unevenly; e.g., the first 25% of the principal components account for 50% of the total variance, and the first 54% for 80%. <ref type="foot" target="#foot_0">2</ref> To build intuition, first consider a hypothetical extreme case where tokens lie in a proper subspace ("token subspace") of the full d-dimensional space (even though, empirically, U has rank d, so the tokens' output embeddings span all of R d ). If a latent h has a component orthogonal to the token subspace, it includes information that is irrelevant for predicting the next token based on h alone (since logits are scalar products of latent and token vectors). The orthogonal component can still be important for the computations carried out by later layers and for predicting the next token in those layers. But the logit lens, which decodes latents into tokens prematurely in intermediate layers, will be blind to the orthogonal component.</p><p>A latent h's angle with the "token subspace" thus measures how much of h is irrelevant for immediately predicting the next token. Concretely, we consider the mean squared cosine between h and the token embeddings (rows of U) to capture how much of h's "energy" translates into logit scores. For interpretability, we normalize by the mean squared cosine among token embeddings themselves,<ref type="foot" target="#foot_1">foot_1</ref> obtaining what we call h's squared token energy</p><formula xml:id="formula_4">E(h) 2 = 1 v ∥ Ûh∥ 2 2 / ∥h∥ 2 2 1 v 2 ∥ Û Û⊤ ∥ 2 F = v d ∥ Ûh∥ 2 2 ∥ Û Û⊤ ∥ 2 F<label>(2)</label></formula><p>( Û being U with 2-normalized rows), which captures h's proximity to "token subspace", compared to a random token's proximity to "token subspace". We visualize token energy and its relation to other key quantities in Fig. <ref type="figure" target="#fig_2">4</ref>. As a function of layer (Fig. <ref type="figure" target="#fig_2">4(b)</ref>), root mean squared token energy is low (around 20%) and mostly flat before layer 70, when it suddenly spikes-just when next-token predictions switch from English to Chinese (Fig. <ref type="figure" target="#fig_2">4(c)</ref>). In sum, Fig. <ref type="figure" target="#fig_2">4(a-c</ref>) reveals three phases:</p><p>1. Phase 1 (layers 1-40): High entropy (14 bits, nearly uniform), low token energy, no language dominates. 2. Phase 2 (layers 41-70): Low entropy (1-2 bits), low token energy, English dominates.</p><p>3. Phase 3 (layers 71-80): Low entropy, high token energy (up from 20% to 30%), Chinese dominates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conceptual model</head><p>Next, we formulate a conceptual model that is consistent with the above observations. In order to predict the next token, the transformer's job essentially consists in mapping the input embedding of the current token to the output embedding of the next token. Phase 1 is focused on building up a better feature representation for the current token from its input embedding, by dealing with tokenization issues (e.g., integrating preceding tokens belonging to the same word), integrating words into larger semantic units, etc. This phase is not yet directly concerned with predicting the next token, with latents remaining largely orthogonal to output token space (low token energy), leading to small dot products between latents and output token embeddings, and thus to high entropy.</p><p>In Phase 2, latents live in an abstract "concept space", which, unlike in Phase 1, is no more orthogonal to the output token space. Rather, latent "concept embeddings" are closer to those output token embeddings that can express the respective concept (across languages, synonyms, etc.), leading to low entropy. Among the concept-relevant tokens, English variants lie closer to the concept embedding than non-English variants (due to the model's overwhelming exposure to English during training), leading to higher probabilities for English than Chinese tokens. Despite the correlation between concept and token embeddings, concept embeddings also carry much information that goes beyond output tokens (including input-specific contextual information and information about the target language), leading to a still-low token energy.</p><p>In Phase 3, the model maps abstract concepts to concrete words/tokens in the target language. Information that is irrelevant for next-token prediction is discarded, leading to a spike in token energy.</p><p>Sketch. This model is illustrated-with a strongly simplified toy-like sketch-in Fig. <ref type="figure" target="#fig_2">4(d)</ref>. In this picture, the model operates in 3D (rather than the actual 8192D) space. All embeddings (output tokens and latents) lie on a sphere around the origin. Token embeddings lie on the equator and are mostly spread out along the x-axis (left/right), which captures language (English left, Chinese right). The y-axis (front/back) captures concepts, in this toy picture along a 1D "sweetness" scale. The z-axis (bottom/top) provides an extra degree of freedom that can be used to store information about context, language, etc. A transformer forward pass moves along the surface of the sphere. In Phase 1, the latent starts out at the north pole, orthogonal to both output token and concept embeddings. Phase 2 rotates the latent into concept space; English tokens are more likely because their embeddings have a stronger concept component y. Finally, Phase 3 rotates the latent along the equator into the target language's hemisphere, onto the output token that best captures the active concept in that language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>In our attempt to answer whether Llama-2 models internally use English as a pivot language, we found that latent embeddings indeed lie further from the correct next token in the input language than from its English analog, leading to overwhelmingly English internal representations as seen through the logit lens. It might thus be tempting to conclude that, yes, Llama-2 uses English as an implicit pivot, similar to researchers' prior use of English as an explicit pivot <ref type="bibr" target="#b44">(Shi et al., 2022;</ref><ref type="bibr" target="#b0">Ahuja et al., 2023;</ref><ref type="bibr" target="#b21">Huang et al., 2023)</ref>. But our answer must be more nuanced, as much of the latents' "energy" points in directions that are largely orthogonal to output token embeddings and thus do not matter for next-token prediction. The model can use these directions as extra degrees of freedom for building rich feature representations from its raw inputs <ref type="bibr">(Yosinski et al., 2014</ref><ref type="bibr" target="#b51">(Yosinski et al., , 2015;;</ref><ref type="bibr">Geva et al., 2022)</ref>, which could be seen as forming an abstract "concept space". In this interpretation, the model's internal lingua franca is not English but conceptsconcepts that are biased toward English. Hence, English could still be seen as a pivot language, but in a semantic, rather than a purely lexical, sense.</p><p>Our experiments involve three text completion tasks. The translation and cloze tasks operate at a semantic level, whereas the word repetition task is purely syntactic. Yet, in most languages (Fig. <ref type="figure">7</ref>) the pattern is similar to that for the two other tasks, with tokens first going through an "English phase"possibly because recognizing that the task is to simply copy a token requires semantic understanding, which is achieved only in concept space, which in turn is closer to English token embeddings.</p><p>This said, note that the English-first pattern is less pronounced on the repetition task (Fig. <ref type="figure">7</ref>), where the input language rises earlier than on the other tasks or, for Chinese (Fig. <ref type="figure">7</ref>(e)) even simultaneously with, or faster than, English. This might be due to tokenization: for Chinese we explicitly chose 100% single-token words, as opposed to only 13% for Russian, 43% for German, and 55% for French (Table <ref type="table" target="#tab_0">1</ref>). Where language-specific tokens are available, the detour through English seems less pronounced. This supports prior concerns about the importance of tokenization, which not only burdens minority languages with more tokens per word <ref type="bibr" target="#b1">(Artetxe et al., 2020)</ref>, but, as we show, also forces latents through an English-biased semantic space.</p><p>Future work should investigate in what ways an English bias in latent space could be problematic, e.g., by biasing downstream model behavior. We see promise in designing experiments building on work from psycholinguistics, which has shown that concepts may carry different emotional values in different languages <ref type="bibr" target="#b5">(Boroditsky et al., 2003)</ref> and that using one word for two concepts (colexification) may affect cognition <ref type="bibr" target="#b15">(Di Natale et al., 2021)</ref>. Future work should also study how English bias changes when decreasing the dominance of English during training, e.g., by applying our method to Llama-2 derivatives with a different language mix <ref type="bibr" target="#b18">(Goddard, 2023;</ref><ref type="bibr" target="#b38">Plüster, 2023;</ref><ref type="bibr" target="#b20">Huang, 2023;</ref><ref type="bibr" target="#b23">Kim, 2023)</ref>, or by using less Anglocentric tokenizers.</p><p>Such work will give important clues for decreasing English bias and enabling more equitable AI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>In this paper, we focus on the Llama-2 family of language models, which limits the claims we can make about other English-dominated models (but see Appendix B.2 for initial evidence that Mistral-7B behaves identically). Moreover, since the proposed method relies on model parameters, little can be said about the more widely used closedsource models. Nonetheless, the methods outlined in this paper can be straightforwardly applied to other autoregressive transformers and generalized to non-autoregressive ones (given their parameters are available), a direction that warrants future exploration.</p><p>Additionally, the tasks outlined in the paper are simple and provide a highly controlled, yet toy-like, context for studying the internal language of LLMs. This is essential as a first step to illustrate existence, but future work should extend to a wider range of tasks; these may include more culturally sensitive problems, popular use-cases (cf. Sec. 6), and technical analyses that go beyond single tokens.</p><p>While we find evidence of a "concept space" in our interpretation (Sec. 5), we have limited understanding of the structure of this space in its original high-dimensional form. We believe that better understanding and mapping out this concept space is an important future direction and will result in a stronger basis for the presented conceptual model.</p><p>Finally, while the logit lens grants us approximate access to the internal beliefs about what should be the output at a given sequence position, everything else contained in the intermediate representations (e.g., information to construct keys, queries, values, or to perform intermediate calculations that do not directly contribute to the output beliefs) remains hidden and only enters the logit lens-based part of our analysis as noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Additional methodological details A.1 Word translation</head><p>A detail that we omitted in the main paper for brevity is how we translate the English words resulting from the procedure outlined in Sec. 3.3 to French, German, and Russian. During these translations we translated both the individual words alongside their cloze sentences using DeepL. <ref type="foot" target="#foot_3">5</ref> For each word translation, we include the context of the cloze task to disambiguate homonyms. We then filter the translations to remove words that have the same prefix token across English and the target language. For example, the French translation of the word "photograph", "photographier", shares the "photo" prefix token. Additionally, we parse through the translations and filter any cloze translations where the target word doesn't align with the expected word from the individual word translation, which was due to failures in the DeepL translation. These filterings result in a different number of final words across the different languages.</p><p>We provide the numbers for the aggregated translation task (Table <ref type="table" target="#tab_0">1</ref>), repetition task (Table <ref type="table" target="#tab_1">2</ref>), cloze-task (Table <ref type="table">3</ref>), and individual translation tasks (Table <ref type="table">4</ref>). Table 3: Cloze task dataset sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Computing language probabilities</head><p>In order to compute language probabilities, we search Llama-2's vocabulary for all tokens that could be the first token of the correct word in the respective language. In particular, we search Llama-2's vocabulary for all prefixes of the word without and with leading space. <ref type="foot" target="#foot_4">6</ref> For Chinese and Russian we also consider tokenizations based on the UTF-8 encodings of their unicode characters. For a language ℓ and its corresponding target word w, we define</p><formula xml:id="formula_5">P(lang = ℓ) := t ℓ ∈Start(w) P(x n+1 = t ℓ ),<label>(3)</label></formula><p>where Start(w) denotes the set of starting tokens of the word w.</p><p>For example, if the correct next Chinese word is "花" ("flower"), which can be tokenized either using the single token "花" or via its UTF-8 encoding "&lt;0xE8&gt;•&lt;0x8A&gt;•&lt;0xB1&gt;", we have P(lang = ZH) = P(x n+1 = "花") + P(x n+1 = "&lt;0xE8&gt;") and P(lang = EN) = P(x n+1 = "f") + P(x n+1 = "fl") + P(x n+1 = "flow") + P(x n+1 = "_f") + P(x n+1 = "_fl") + P(x n+1 = "_flo") + P(x n+1 = "_flow")+P(x n+1 = "_flower") (all the token-level prefixes of "flower" and "_flower").</p><p>Table <ref type="table">4</ref>: Translation statistics between languages, including total numbers and single-token translations (in brackets).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional results</head><p>Here we provide the results for all languages: Chinese, English, French, German, and Russian.</p><p>Language probability. Language probability plots (with entropy heatmaps) for the aggregated translation task are in Fig. <ref type="figure">5</ref>, for the repetition task in Fig. <ref type="figure">7</ref>, and, for the cloze task in Fig. <ref type="figure">9</ref>. Additionally, we provide the translation task results for individual language pairs in <ref type="bibr">Fig. 11,</ref><ref type="bibr">Fig. 13,</ref><ref type="bibr">Fig. 15,</ref><ref type="bibr">Fig. 17,</ref><ref type="bibr">Fig. 19</ref>.</p><p>We observe the same pattern-noise in the early layers, English in the middle, target language in the end-across almost all languages and model sizes. The only exception is the Chinese repetition task.</p><p>Energy. Energy (Sec. 4.2) plots for the aggregated translation task are in Fig. <ref type="figure">6</ref>, for the repetition task in Fig. <ref type="figure">8</ref>, and, for the cloze task in Fig. <ref type="figure">10</ref>. Additionally, we provide the translation task results for individual language pairs in Fig. <ref type="figure" target="#fig_0">12</ref>, <ref type="bibr">Fig. 14,</ref><ref type="bibr">Fig. 16,</ref><ref type="bibr">Fig. 18,</ref><ref type="bibr">Fig. 20</ref>.</p><p>Energy plots are consistent with the theory outlined in Sec. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Low-resource language Estonian</head><p>We also performed our analysis with Llama-2-7B on Estonian, a low-resource language, in Fig. <ref type="figure" target="#fig_0">21</ref>. The fact that Estonian is a low-resource language is already evident in the number of single-token words: only one out of our 99 Estonian words can be represented with a single token.</p><p>Copy task. In the copy task, Estonian behaves the most similarly to Chinese, with the Estonian probability exceeding the English probability already in the intermediate layers.</p><p>Translation task. While the success probability on the translation task after the final layer is significantly smaller than in the languages studied in the main paper, we still observe the same effect as for the other languages: the intermediate next-token distributions decoded via the logit lens concentrate their probability mass on the correct English tokens and only in the final layers transition to Estonian.</p><p>Cloze task. The Estonian cloze task seems too hard, possibly due to the extremely low resources of Estonian in the Llama-2 training data: Llama-2-7B has a 0% success probability after the last layer. Interestingly, the Estonian success probability is slightly greater than 0% in the intermediate layers, when the logit lens decodes to English. The success probability might increase if we included synonyms of the translated words or used human experts for the creation of the cloze examples instead of GPT-4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Other models: Mistral</head><p>We also performed our analysis on Mistral-7B, a model from outside the Llama model family. The results, shown in Fig. <ref type="figure" target="#fig_0">22</ref>, are consistent with those for Llama-2, pointing at the universality of our findings.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Language probabilities for latents during Llama-2 forward pass, for (a) translation task from union of German/French/Russian to Chinese, (b) Chinese repetition task, (c) Chinese cloze task. Each task evaluated for model sizes (columns) 7B, 13B, 70B. On x-axes, layer index; on y-axes, probability (according to logit lens) of correct Chinese next token (blue) or English analog (orange). Error bars show 95% Gaussian confidence intervals over input texts (353 for translation, 139 for repetition and cloze).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Latent trajectories through transformer layers. 2D embedding of latents (•) and output tokens (×) found via multidimensional scaling. Latents for same prompt connected by rainbow-colored path, proceeding from layer 1 (red) to 80 (violet). Labels for correct Chinese next tokens (one per prompt) in blue, for English analogs in orange. Takeaway: latents reach correct Chinese token after detour through English.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Anatomy of transformer forward pass when translating to Chinese (cf. Sec. 3.3). Layer-by-layer evolution of (a) entropy of next-token distribution, (b) token energy, (c) language probabilities. As latents are transformed layer by layer, they go through three phases (Sec. 4.2), (d) traveling on a hypersphere, here in 3D instead of actual 8192D (Sec. 5). "甜" means "sweet".</figDesc><graphic coords="7,306.14,70.87,218.27,134.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :Figure 6 :Figure 7 :Figure 8 :Figure 9 :Figure 10 :Figure 12 :</head><label>567891012</label><figDesc>Figure 5: Figures illustrate the translationtask where Llama-2 7B, 13B, and 70B are tasked with translating a word from all non-English input languages to output language. There is one column per model size. The x-axis shows the layer number of the model, and the y-axis the total probability mass falling on the correct token across languages. The orange line illustrates the probability of the correct target word in English and the blue line shows it for the non-English output language. We do not include the probability the input language since it is zero throughout. Means and 95% Gaussian confidence intervals have been computed over the input examples, numbers in Appendix A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 14 :Figure 16 :Figure 18 :Figure 20 :</head><label>14161820</label><figDesc>Figure 14: Figures illustrate the translation task where Llama-2 7B, 13B, and 70B are tasked with translating a word from English input language to output language. There is one column per model size. The x-axis shows the layer number of the model, and the y-axis the energy. Means and 95% Gaussian confidence intervals have been computed over the input examples, numbers in Appendix A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Aggregated translation task dataset sizes.</figDesc><table><row><cell></cell><cell>Total</cell><cell>Single Token</cell></row><row><cell>de</cell><cell>287</cell><cell>126</cell></row><row><cell>fr</cell><cell>162</cell><cell>88</cell></row><row><cell>ru</cell><cell>324</cell><cell>45</cell></row><row><cell>zh</cell><cell>353</cell><cell>353</cell></row><row><cell></cell><cell>Total</cell><cell>Single Token</cell></row><row><cell>de</cell><cell>104</cell><cell>45</cell></row><row><cell>en</cell><cell>132</cell><cell>132</cell></row><row><cell>fr</cell><cell>56</cell><cell>31</cell></row><row><cell>ru</cell><cell>115</cell><cell>15</cell></row><row><cell>zh</cell><cell>139</cell><cell>139</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Repetition task dataset sizes.</figDesc><table><row><cell></cell><cell>Total</cell><cell>Single Token</cell></row><row><cell>de</cell><cell>104</cell><cell>45</cell></row><row><cell>en</cell><cell>132</cell><cell>132</cell></row><row><cell>fr</cell><cell>56</cell><cell>31</cell></row><row><cell>ru</cell><cell>115</cell><cell>15</cell></row><row><cell>zh</cell><cell>139</cell><cell>139</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>Moreover,<ref type="bibr" target="#b6">Cancedda (2024)</ref> showed that a significant fraction of the principal components can be omitted as long as attention sinking are preserved.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>In practice, we use Û⊤ Û instead of Û Û⊤ in (2), which has equal Frobenius norm but is more efficient to compute.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>https://github.com/nrimsky/LM-exp/blob/main/ intermediate_decoding/intermediate_decoding. ipynb</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>https://www.deepl.com/translator</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>Represented by "_". de en fr ru zh de -120 (120) 56 (31) 105 (15) 120 (120) en 104 (45) -57 (31) 114 (15) 132 (132) fr 93 (40) 118 (118) -104 (15) 118 (118) ru 90 (41) 114 (114) 49 (26) -115 (115) zh 104 (45) 132 (132) 57 (31) 115 (15) -</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We thank Nina <ref type="bibr" target="#b41">Rimsky (2023)</ref> for sharing her Llama-2 wrapper and logit lens implementation; 4 <rs type="person">Lucia Quirke</rs> for inputs on mechanistic interpretability, on our experimental setup, and for a fruitful discussion; <rs type="person">Saibo Geng</rs> for helping us with the Chinese dataset; <rs type="person">Nicola Cancedda</rs>, <rs type="person">David Garcia</rs>, <rs type="person">Eric Horvitz</rs>, <rs type="person">Manoel Horta Ribeiro</rs>, <rs type="person">Maxime Peyrard</rs>, <rs type="person">Saibo Geng</rs>, <rs type="person">Tim Davidson</rs>, <rs type="person">Valentin Hartmann</rs>, and <rs type="person">Zachary Horvitz</rs> for insightful discussions and feedback; and <rs type="institution">Meta</rs> for open-sourcing Llama-2 and thereby helping democratize LLM research. Finally, we thank our anonymous peer reviewers for their productive input, which has led, among others, to Appendices B.1 and B.2. West's lab is partly supported by grants from <rs type="funder">Swiss National Science Foundation</rs> (<rs type="grantNumber">200021_185043</rs>, <rs type="grantNumber">TMSGI2_211379</rs>), <rs type="funder">Swiss Data Science Center</rs> (<rs type="grantNumber">P22_08</rs>), <rs type="grantNumber">H2020 (952215</rs>), and by generous gifts from <rs type="funder">Meta</rs>, <rs type="funder">Google</rs>, and <rs type="funder">Microsoft</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_BDgaCrW">
					<idno type="grant-number">2023</idno>
				</org>
				<org type="funding" xml:id="_kB83N2H">
					<idno type="grant-number">200021_185043</idno>
				</org>
				<org type="funding" xml:id="_hXaxHPm">
					<idno type="grant-number">TMSGI2_211379</idno>
				</org>
				<org type="funding" xml:id="_c3xtRH9">
					<idno type="grant-number">P22_08</idno>
				</org>
				<org type="funding" xml:id="_XRcb63X">
					<idno type="grant-number">H2020 (952215</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>the copy-, translation-, and cloze task for Chinese on Mistral-7B. In the first row, the x-axis shows the layer number of the model, and the y-axis the language probability. In the first row, the x-axis shows the layer number of the model, and the y-axis the token energy. Means and 95% Gaussian confidence intervals have been computed over the input examples.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Kabir</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harshita</forename><surname>Diddee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishav</forename><surname>Hada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Millicent</forename><surname>Ochieng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krithika</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prachi</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshay</forename><surname>Nambi</surname></persName>
		</author>
		<title level="m">Mega: Multilingual evaluation of generative ai</title>
		<editor>
			<persName><forename type="first">Tanuja</forename><surname>Ganu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sameer</forename><surname>Segal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Maxamed</forename><surname>Axmed</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kalika</forename><surname>Bali</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sunayana</forename><surname>Sitaram</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the cross-lingual transferability of monolingual representations</title>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.421</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Cahyawijaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nayeon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Wilie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holy</forename><surname>Lovenia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiezheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Willy</forename><surname>Chung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.04023</idno>
		<title level="m">A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Nora</forename><surname>Belrose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zach</forename><surname>Furman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Logan</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danny</forename><surname>Halawi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Ostrovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lev</forename><surname>Mckinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.08112</idno>
		<title level="m">Eliciting latent predictions from transformers with the tuned lens</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pythia: A suite for analyzing large language models across training and scaling</title>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailey</forename><surname>Schoelkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><forename type="middle">Gregory</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herbie</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Kyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Brien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Hallahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivanshu</forename><surname>Aflah Khan</surname></persName>
		</author>
		<author>
			<persName><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Usvsn Sai Prashanth</surname></persName>
		</author>
		<author>
			<persName><surname>Raff</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2397" to="2430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sex, syntax, and semantics</title>
		<author>
			<persName><forename type="first">Lera</forename><surname>Boroditsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lauren</forename><forename type="middle">A</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Webb</forename><surname>Phillips</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Language in Mind: Advances in the Study of Language and Thought</title>
		<editor>
			<persName><forename type="first">Dedre</forename><surname>Gentner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Susan</forename><surname>Goldin-Meadow</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="61" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Spectral filters, dark signals, and attention sinks</title>
		<author>
			<persName><forename type="first">Nicola</forename><surname>Cancedda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.09221</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Steven</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<title level="m">Multilingual alignment of contextual word representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Improving language plasticity via pretraining with active forgetting</title>
		<author>
			<persName><forename type="first">Yihong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelly</forename><surname>Marchisio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberta</forename><surname>Raileanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Ifeoluwa Adelani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">What does it mean to be language-agnostic? probing multilingual sentence encoders for typological properties</title>
		<author>
			<persName><forename type="first">Rochelle</forename><surname>Choenni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekaterina</forename><surname>Shutova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.12862</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Conmy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Augustine</forename><forename type="middle">N</forename><surname>Mavor-Parker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aengus</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Heimersheim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrià</forename><surname>Garriga-Alonso</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.14997</idno>
		<title level="m">Towards automated circuit discovery for mechanistic interpretability</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Unsupervised cross-lingual representation learning at scale</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Emerging cross-lingual structure in pretrained language models</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6022" to="6034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Younes</forename><surname>Belkada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.07339</idno>
	</analytic>
	<monogr>
		<title level="m">-bit matrix multiplication for transformers at scale</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Colexification networks encode affective meaning</title>
		<author>
			<persName><forename type="first">Anna</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natale</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Pellert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Affective Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="111" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A mathematical framework for transformer circuits</title>
		<author>
			<persName><forename type="first">Nelson</forename><surname>Elhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Nanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuntao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Conerly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transformer Circuits Thread</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Do multilingual language models think better in english? Mor Geva, Avi Caciularu, Kevin Ro Wang, and Yoav Goldberg. 2022. Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space</title>
		<author>
			<persName><forename type="first">Julen</forename><surname>Etxaniz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gorka</forename><surname>Azkune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Soroa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oier</forename><surname>Lopez De Lacalle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Charles</forename><surname>Goddard</surname></persName>
		</author>
		<ptr target="https://huggingface.co/chargoddard/llama-polyglot-13b" />
		<title level="m">Llama-polyglot-13b</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2024" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Wes</forename><surname>Gurnee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Nanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Pauly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitrii</forename><surname>Troitskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Bertsimas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.01610</idno>
		<title level="m">Finding neurons in a haystack: Case studies with sparse probing</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Bofeng</forename><surname>Huang</surname></persName>
		</author>
		<ptr target="https://huggingface.co/bofenghuang/vigogne-2-13b-instruct" />
		<title level="m">vigogne-2-13b-instruct</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2024" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Not all languages are created equal in llms: Improving multilingual capability by cross-lingual-thought prompting</title>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Romansetu: Efficiently unlocking multilingual capabilities of large language models models via romanization</title>
		<author>
			<persName><forename type="first">Raj</forename><surname>Jaavid Aktar Husain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aswanth</forename><surname>Dabre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ratish</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anoop</forename><surname>Puduppully</surname></persName>
		</author>
		<author>
			<persName><surname>Kunchukuttan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Daekeun</forename><surname>Kim</surname></persName>
		</author>
		<ptr target="https://huggingface.co/daekeun-ml/Llama-2-ko-DPO-13B" />
		<title level="m">Llama-2-ko-dpo-13b</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2024" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Aspen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Hopkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernanda</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanspeter</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><surname>Wattenberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.13382</idno>
		<title level="m">Emergent world representations: Exploring a sequence model trained on a synthetic task</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">On the language neutrality of pre-trained multilingual representations</title>
		<author>
			<persName><forename type="first">Jindřich</forename><surname>Libovický</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rudolf</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Fraser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05160</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fewshot learning with multilingual generative language models</title>
		<author>
			<persName><forename type="first">Victoria</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todor</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlu</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Simig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Bhosale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramakanth</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Pasunuru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Punit</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Singh Koura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian O'</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Horo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zornitsa</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.emnlp-main.616</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multilingual denoising pre-training for neural machine translation</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00343</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="726" to="742" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Tegmark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.06824</idno>
		<title level="m">The geometry of truth: Emergent linear structure in large language model representations of true/false datasets</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Alex Andonian, and Yonatan Belinkov. 2022. Locating and editing factual associations in gpt</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Bau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="17359" to="17372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Monea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxime</forename><surname>Peyrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Josifoski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emre</forename><surname>Kıcıman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamid</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barun</forename><surname>Patra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>West</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.02073</idno>
		<title level="m">A glitch in the matrix? locating and detecting language model grounding with fakepedia</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">First align, then predict: Understanding the cross-lingual ability of multilingual bert</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanai</forename><surname>Elazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoît</forename><surname>Sagot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Djamé</forename><surname>Seddah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter</title>
		<meeting>the 16th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>Main Volume</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2214" to="2231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Neel</forename><surname>Nanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Lieberum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jess</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.05217</idno>
		<title level="m">Progress measures for grokking via mechanistic interpretability</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Interpreting gpt: The logit lens</title>
		<author>
			<persName><surname>Nostalgebraist</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Less-Wrong</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">With the future behind them: Convergent evidence from aymara language and gesture in the crosslinguistic comparison of spatial construals of time</title>
		<author>
			<persName><forename type="first">Rafael</forename><forename type="middle">E</forename><surname>Núñez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eve</forename><surname>Sweetser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="401" to="450" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<title level="m">Gpt-4 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Multilingual bert has an accent: Evaluating english influences on fluency in multilingual models</title>
		<author>
			<persName><forename type="first">Isabel</forename><surname>Papadimitriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kezia</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Finland&apos;s chatgpt equivalent begins to think in estonian as well</title>
		<author>
			<persName><forename type="first">Rait</forename><surname>Piir</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>ERR News</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Björn</forename><surname>Plüster</surname></persName>
		</author>
		<ptr target="https://laion.ai/blog-de/leo-lm/" />
		<title level="m">LeoLM: Ein Impuls für Deutschsprachige LLM-Forschung</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2024" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Quirke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lovis</forename><surname>Heindrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wes</forename><surname>Gurnee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Nanda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.00863</idno>
		<title level="m">Training dynamics of contextual n-grams in language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Decoding intermediate activations in Llama-2-7b. LessWrong</title>
		<author>
			<persName><forename type="first">Nina</forename><surname>Rimsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Le</forename><surname>Teven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellie</forename><surname>Akiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suzana</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ilić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Hesslow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Castagné</surname></persName>
		</author>
		<author>
			<persName><forename type="first">François</forename><surname>Sasha Luccioni</surname></persName>
		</author>
		<author>
			<persName><surname>Yvon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.05100</idno>
		<title level="m">Bloom: A 176b-parameter open-access multilingual language model</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Cross-lingual alignment of contextual word embeddings, with applications to zero-shot dependency parsing</title>
		<author>
			<persName><forename type="first">Tal</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ori</forename><surname>Ram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1599" to="1613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Language models are multilingual chain-ofthought reasoners</title>
		<author>
			<persName><forename type="first">Freda</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirac</forename><surname>Suzgun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suraj</forename><surname>Srivats</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soroush</forename><surname>Vosoughi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">mgpt: Few-shot learners go multilingual</title>
		<author>
			<persName><forename type="first">Oleh</forename><surname>Shliazhko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alena</forename><surname>Fenogenova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Tikhonova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladislav</forename><surname>Mikhailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Kozlova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatiana</forename><surname>Shavrina</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.09288</idno>
	</analytic>
	<monogr>
		<title level="m">Open foundation and fine-tuned chat models</title>
		<editor>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kevin</forename><surname>Stone</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Peter</forename><surname>Albert</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yasmine</forename><surname>Babaei</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nikolay</forename><surname>Bashlykov</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Soumya</forename><surname>Batra</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Prajjwal</forename><surname>Bhargava</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Chain-of-thought prompting elicits reasoning in large language models</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="24824" to="24837" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">Xiangpeng</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianhao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingzhang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binbin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianxiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shangjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binyuan</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dayiheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baosong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<title level="m">Polylm: An open source polyglot large language model</title>
		<imprint>
			<date type="published" when="2023-06">Jun Xie. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Aditya Barua, and Colin Raffel. 2021. mt5: A massively multilingual pre-trained text-totext transformer</title>
		<author>
			<persName><forename type="first">Linting</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihir</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.41</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<title level="m">Yoshua Bengio, and Hod Lipson. 2014. How transferable are features in deep neural networks? Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hod</forename><surname>Lipson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.06579</idno>
		<title level="m">Understanding neural networks through deep visualization</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Don&apos;t trust ChatGPT when your question is not in English: A study of multilingual abilities and types of LLMs</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Senyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bradley</forename><surname>Hauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grzegorz</forename><surname>Kondrak</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.emnlp-main.491</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2023 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="7915" to="7927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Question translation training for better multilingual reasoning</title>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuaijie</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Extrapolating large language models to nonenglish by aligning languages</title>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunzhe</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingxiu</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
