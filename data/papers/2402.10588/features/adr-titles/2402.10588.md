- Decision to focus on the Llama-2 family of transformer models
- Choice of using non-English prompts for testing
- Methodology for tracking intermediate embeddings through layers
- Selection of logit lens technique for analyzing model behavior
- Assumption regarding the existence of an internal pivot language
- Framework for categorizing phases of embedding transformation (input space, concept space, output space)
- Decision to analyze latent embeddings as high-dimensional Euclidean points
- Choice of multilingual corpus dominated by English for training
- Decision to utilize specific model sizes (7B, 13B, 70B parameters)
- Use of 8-bit quantization in experiments
- Decision to focus on the implications of English bias in multilingual models
- Assumption about the relationship between concept space and language-specific token space
- Choice of evaluation metrics for determining semantic correctness of decoded tokens
- Decision to make code and data publicly available for reproducibility
- Assumption regarding the impact of training data composition on model performance across languages
- Decision to explore future directions for studying latent biases in LLMs