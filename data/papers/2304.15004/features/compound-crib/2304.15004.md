The paper titled "Are Emergent Abilities of Large Language Models a Mirage?" presents a critical examination of the concept of emergent abilities in large language models (LLMs). The researchers argue that what has been labeled as emergent abilities may not be inherent properties of the models but rather artifacts of the metrics used to evaluate their performance. Below is a detailed technical explanation of the researchers' decisions regarding the definitions, properties, alternative explanations, mathematical models, and implications for AI safety.

### Emergent Abilities Definition
The researchers define emergent abilities as capabilities that are present in large language models but absent in smaller models, and which cannot be predicted by simply extrapolating performance from smaller models. This definition is crucial because it sets the stage for investigating whether these abilities are genuinely emergent or merely a byproduct of how performance is measured.

### Key Properties of Emergent Abilities
1. **Sharpness**: The researchers observe that performance transitions sharply from non-performance to performance at specific scales. This sharpness suggests a discontinuity in the model's capabilities as it scales, which raises questions about the underlying mechanisms driving this behavior.

2. **Unpredictability**: Emergent abilities appear at seemingly unforeseeable model scales, making it difficult to anticipate when a model will acquire a new capability. This unpredictability is concerning, especially in the context of AI safety, as it implies that larger models could unexpectedly develop dangerous capabilities.

### Alternative Explanation
The researchers propose that emergent abilities may be artifacts of metric choice rather than fundamental changes in model behavior. They argue that nonlinear or discontinuous metrics can create the illusion of emergent abilities. For example, metrics like accuracy, which require all tokens in a sequence to be correct, can lead to sharp performance changes that do not reflect the model's actual capabilities.

### Mathematical Model
The researchers introduce a mathematical model to illustrate their points:
- The per-token cross-entropy loss \( L_{CE}(N) \) is modeled as a function of model parameters \( N \):
  \[
  L_{CE}(N) = N^{c \alpha}
  \]
- The probability of selecting the correct token is given by:
  \[
  p(\text{single token correct}) = \exp\left(-L_{CE}(N)\right) = \exp\left(-\frac{N}{c}\alpha\right)
  \]
This model demonstrates how performance can be predicted based on the number of parameters, suggesting that the underlying behavior of the model is smooth and continuous.

### Metric Impact
The researchers analyze how different metrics affect the perception of model performance:
- **Nonlinear Metric (Accuracy)**: The accuracy metric leads to sharp, unpredictable performance changes:
  \[
  \text{Accuracy}(N) \approx p_N(\text{single token correct})^L = \exp\left(-\frac{N}{c}\alpha L\right)
  \]
- **Linear Metric (Token Edit Distance)**: In contrast, using a linear metric results in smooth, continuous performance improvements:
  \[
  \text{Token Edit Distance}(N) \approx L(1 - p_N(\text{single token correct})) = L\left(1 - \exp\left(-\frac{N}{c}\alpha\right)\right)
  \]

### Predictions Tested
The researchers tested several predictions to validate their alternative explanation:
1. Changing from nonlinear to linear metrics reveals smooth performance improvements.
2. Increasing the test dataset size improves resolution, revealing above-chance performance in smaller models.
3. Performance predictably decreases with increasing target string length.

### Meta-Analysis Findings
The meta-analysis of published benchmarks revealed that emergent abilities predominantly appear with specific metrics and not across all task-model family pairs. Changing metrics from nonlinear/discontinuous to linear/continuous removes the appearance of emergent abilities, supporting the researchers' hypothesis.

### Implications for AI Safety
Understanding the nature of emergent abilities is crucial for predicting and controlling model behavior, particularly regarding undesirable capabilities. If emergent abilities are indeed artifacts of metric choice, then researchers and practitioners can better design evaluations to mitigate risks associated with unexpected model behaviors.

### Figures and Diagrams
Figures in the paper illustrate the sharp and unpredictable performance increases in LLMs under different metrics, reinforcing the argument that metric choice significantly influences the perception of model capabilities.

### Conclusion
The researchers conclude that emergent abilities in LLMs may not be inherent properties but rather artifacts of metric selection. This finding emphasizes the importance of careful metric choice in evaluating model performance and understanding the true capabilities of large language models. By challenging the notion of emergent abilities, the paper contributes to a more nuanced understanding of model behavior and its implications for AI safety and alignment.