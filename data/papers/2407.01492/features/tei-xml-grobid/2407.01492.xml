<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">REGMIX: Data Mixture as Regression for Language Model Pre-training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-07-01">1 Jul 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Qian</forename><surname>Liu</surname></persName>
							<email>liuqian@sea.com</email>
							<affiliation key="aff0">
								<orgName type="department">Sea AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaosen</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sea AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sea AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guangtao</forename><surname>Zeng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sea AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Longxu</forename><surname>Dou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sea AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tianyu</forename><surname>Pang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sea AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sea AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Min</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sea AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><surname>Smu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sea AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Contextual</forename><surname>Ai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sea AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><surname>Sutd</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sea AI Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">REGMIX: Data Mixture as Regression for Language Model Pre-training</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-07-01">1 Jul 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">46E5F4DDA22F459946509454A549A110</idno>
					<idno type="arXiv">arXiv:2407.01492v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The data mixture for large language model pre-training significantly impacts performance, yet how to determine an effective mixture remains unclear. We propose REGMIX to automatically identify a high-performing data mixture by formulating it as a regression task. REGMIX involves training a set of small models with diverse data mixtures and fitting a regression model to predict their performance given their respective mixtures. With the fitted regression model, we simulate the top-ranked mixture and use it to train a large-scale model with orders of magnitude more compute. To empirically validate REGMIX, we train 512 models with 1M parameters for 1B tokens of different mixtures to fit the regression model and find the optimal mixture. Using this mixture we train a 1B parameter model for 25B tokens (i.e. 1000× larger and 25× longer) which we find performs best among 64 candidate 1B parameter models with other mixtures. Further, our method demonstrates superior performance compared to human selection and achieves results that match or surpass DoReMi, while utilizing only 10% of the compute budget. Our experiments also show that (1) Data mixtures significantly impact performance with single-task performance variations of up to 14.6%; (2) Web corpora rather than data perceived as high-quality like Wikipedia have the strongest positive correlation with downstream performance; (3) Domains interact in complex ways often contradicting common sense, thus automatic approaches like REGMIX are needed; (4) Data mixture effects transcend scaling laws, and our approach captures the complexity by considering all domains together. Our code is available at <ref type="url" target="https://github.com/sail-sg/regmix">https://github.com/sail-sg/regmix</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The availability of large-scale public datasets has been a key factor enabling the creation of large language models (LLMs). Most data is available on the Internet and includes academic papers (e.g. arXiv), books (e.g. Project Gutenberg), and code (e.g. GitHub). For the creation of one of the first LLMs, GPT-3 <ref type="bibr" target="#b6">[7]</ref>, the authors had already recognized the importance of selecting the best data for training, and thus they decided to upsample Wikipedia due to its perceived high quality. However, such manual data selection is not scalable and may lead to a suboptimal selection <ref type="bibr" target="#b2">[3]</ref>. As the size and diversity of data used for LLM pre-training continue to grow, determining the optimal data mixture becomes increasingly challenging. It gives rise to the critical research question: How can we select the optimal data mixture in a scalable and efficient manner?</p><p>Prior work <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b1">2]</ref> employs small-scale models ("proxy models") to predict the domain weights for large-scale language models. These works train proxy models with a substantial number of  tokens (e.g., 100B), sometimes even the same number as used for training LLMs, and dynamically adjust the data allocation strategy by monitoring the training dynamics. However, these approaches become inefficient as the training data used for pre-training LLMs continues to grow. Training a proxy model for current models, such as Llama-3, would require using up to 15T tokens <ref type="bibr" target="#b0">[1]</ref> with current approaches, which is likely too expensive and too slow to make it worthwhile 2 .</p><p>In this work, we argue that training small models on a limited set of tokens is sufficient to predict an effective data mixture for LLM training. Our key assumption is the rank invariance of data mixtures, which posits that the relative ranking of data mixtures in terms of their impact on model performance is consistent across different model sizes and numbers of training tokens. Under this assumption, the key challenge lies in discovering the top-ranked data mixture from the near-infinite number of potential data mixtures. To do so, we treat the data mixture selection as a regression task. Rather than exhaustively training small models with every possible mixture, we train only a set of small models, each with a unique data mixture. Based on the performance of these models and their mixtures, we fit a regression model to predict the performance of other data mixtures. Our approach is significantly more scalable than prior work, as it allows for parallel training of small proxy models rather than training a single model for a long time. Further, the regression model provides insights into domain interactions that can facilitate understanding and data curation.</p><p>To validate REGMIX, we train models with 1M and 1B parameters 3 with different data mixtures. By training 512 models with 1M parameters on 1B tokens 4 , we are able to predict the optimal data mixture among 64 models that are 1000× larger (1B parameters) and trained 25× longer (25B tokens) as depicted in Figure <ref type="figure" target="#fig_1">1</ref>. Moreover, the optimized data mixture using REGMIX yields a better model than human selection, and achieves performance on par with the flagship DoReMi method <ref type="bibr" target="#b63">[64]</ref> despite it requiring less total compute and allowing for parallel training. We also find that (1) Data mixture significantly impacts downstream performance, resulting in substantial differences of up to 14.6% in single-task performance; (2) General web corpora (e.g., CommonCrawl), rather than Wikipedia, exhibit the strongest positive correlation with improved performance across downstream tasks; <ref type="bibr" target="#b2">(3)</ref> The interactions between domains are complex and often contradict intuition, highlighting the need for automated approaches like REGMIX. (4) Data mixture effects transcend scaling laws, and REGMIX captures the complexity by considering all domains together.</p><p>Hacker News Github Philpapers 9.5% 35.9% 54.6% 87.7% 12.0% 0.3% 24.4% 1.4% 74.2% step Target 5.46 5.57 6.07 step step … … Regression Model Linear Model Tree Model Hacker News Github Philpapers 22.8% 67.0% 10.2% Prediction (Lowest) 5.34 Predicted Best Target Simulated Best Data Mixture  <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b66">67]</ref>. For the pre-training of LLMs, most methods rely on heuristics <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b54">55]</ref>, but there have been some learned approaches using optimization algorithms <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b68">69]</ref>, model perplexity <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b40">41]</ref>, or LLMs to inform the sample selection process <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b71">72]</ref>. (3) Group-level selection assumes the data can be grouped into pools that are then optimally mixed. While early work again relies on manual mixtures <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b6">7]</ref>, learned mixtures have become more common <ref type="bibr" target="#b2">[3]</ref>. Learned approaches either leverage proxy models to determine fixed weights for each group ("offline selection") <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b15">16]</ref> or dynamically adjust the weights during training of the final model ("online selection") <ref type="bibr" target="#b8">[9]</ref>. Our approach, REGMIX, is an offline group-level selection method. Different from the flagship algorithm in this category, DoReMi <ref type="bibr" target="#b63">[64]</ref>, REGMIX does not require training a single model for hundreds of thousands of steps, but instead a few small models for short durations. As these can be trained in parallel, our approach is more scalable, while also yielding better weights leading to a more performant final model.</p><p>Data scaling laws explore interactions of data quantity, quality, and mixing proportions, as LLMs are scaled up. Muennighoff et al. <ref type="bibr" target="#b40">[41]</ref> introduce scaling laws for data-constrained scenarios and Goyal et al. <ref type="bibr" target="#b20">[21]</ref> try to extend this approach to deal with multiple data pools. Prior research has confirmed that different datasets require different scaling <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b41">42]</ref>, thus Ye et al. <ref type="bibr" target="#b67">[68]</ref> and Ge et al. <ref type="bibr" target="#b19">[20]</ref> propose functional relationships to predict the impact of mixtures on language modeling loss. Some work has investigated optimal mixtures during continued pre-training rather than from scratch training <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b13">14]</ref>. While most of these works focus on validation loss, others investigate downstream performance and develop predictive relations with loss <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b61">62]</ref>. Different from data scaling work that attempt to find an analytical scaling function <ref type="bibr" target="#b22">[23]</ref>, REGMIX directly optimizes the target metric using regression models. REGMIX is designed for from-scratch pre-training. In line with previous research, we also find strong correlations between loss and downstream performance, especially for loss on web corpora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">REGMIX: Data mixture as regression</head><p>As illustrated in Figure <ref type="figure">2</ref>, our method involves four key steps: (1) Generate random data mixtures and train small-scale proxy models on these mixtures. (2) Fit a linear regression model using the Table <ref type="table">1</ref>: Overview of the Pile dataset <ref type="bibr" target="#b17">[18]</ref> with datasets that are no longer available due to copyright issues marked in gray. In our experiments, we use the 17 available domains to study the data mixture for language model pre-training.</p><p>Component Effective Size Pile-CC 227.12 GiB PubMed Central 180.55 GiB Books3 151.44 GiB OpenWebText2 125.54 GiB ArXiv 112.42 GiB Github 95.16 GiB FreeLaw 76.73 GiB Stack Exchange 64.39 GiB USPTO Backgrounds 45.81 GiB PubMed Abstracts 38.53 GiB Gutenberg (PG-19) 27.19 GiB Component Effective Size OpenSubtitles 19.47 GiB Wikipedia (en) 19.13 GiB DM Mathematics 15.49 GiB Ubuntu IRC 11.03 GiB BookCorpus2 9.45 GiB EuroParl 9.17 GiB HackerNews 7.80 GiB YoutubeSubtitles 7.47 GiB PhilPapers 4.76 GiB NIH ExPorter 3.79 GiB Enron Emails 1.76 GiB mixtures as features and the target value as the label. (3) Simulate the data mixture space on a larger scale and leverage the regression model to identify the best mixture for the target value. ( <ref type="formula">4</ref>) Train a large-scale model using the simulated best data mixture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Train small-scale proxy models</head><p>The first step is to train a set of small-scale proxy models on multiple different data mixtures. To reduce the required runs, we aim to select a diverse range of data mixtures that cover extreme weights from 0% to 100% for each domain. We achieve this by using a Dirichlet distribution based on the token distribution, which allows us to sample a wide range of values and expose the regression models to various extremes. Simultaneously, basing the distribution on the token distribution ensures that the overall data mixture statistically reflects the availability of data. For example, this prevents any single domain with a token count below 1% from being overly emphasized, which is not feasible for large-scale training since there are not enough available tokens from that domain. In practice, we multiply the token distribution by a value from 0.1 to 5.0 to construct various sparse and near-uniform distributions, then use these distribution vectors as the Dirichlet distribution hyperparameter α.</p><p>After training small-scale proxy models for a few steps, we can obtain several well-trained small models. For example, in our main experiment, each proxy model contains 1M parameters and is trained on 1B tokens. We can then choose to evaluate these trained models on domains or benchmarks to get the target value we want to optimize. Generally, the target value can be the loss on a domain, as shown in Figure <ref type="figure">2</ref> for the StackExchange domain. Once we have obtained these target values, we can use the data mixture as features and the target values as labels to fit a regression model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Fit a regression model</head><p>The second step is to fit a regression model using the data mixture as features, and the target value as labels. The regression task is a conventional supervised learning task that involves predicting a continuous target variable y based on input features X = (x 1 , x 2 , . . . , x n ). The goal is to find a function f that best maps the input features to the target variable, such that y = f (X) + ϵ, where ϵ represents the error or noise in the data. In the context of this paper, the input features X correspond to the domain weights of the data mixture, and the target variable y is the value we want to optimize. Using this data, we train regression models that learn a function to predict the target value based on arbitrary data mixtures without requiring further training.</p><p>Linear regression. The linear regression model is widely used in regression. It assumes a linear relationship between the input features and the target variable, which can be represented as:</p><formula xml:id="formula_0">y = ω 0 + ω 1 x 1 + . . . + ω n x n + ϵ<label>(1)</label></formula><p>where ω 0 is the intercept, and ω = (ω 1 , . . . , ω n ) are the coefficients associated with the respective input features x 1 , . . . , x n . The coefficients ω are typically estimated using techniques such as ordinary least squares, aiming to minimize the sum of squared residuals between the predicted and actual target values. In practice, we employ linear regression with L2 regularization, also known as ridge regression, which applies a penalty to the magnitude of ω to prevent overfitting.</p><p>LightGBM regression. The LightGBM <ref type="bibr" target="#b25">[26]</ref> is a powerful gradient-boosting algorithm that can be used for both regression and classification tasks. In the context of regression, LightGBM learns an ensemble of decision trees to predict the target variable. The process is guided by a gradient-based optimization algorithm, which minimizes a specified loss function (e.g. mean squared error). Moreover, LightGBM is designed to be efficient and scalable, making it suitable for large datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Simulate and predict</head><p>Once we have trained the regression model, we can efficiently explore the entire space of possible data mixtures. By using the trained model to predict the target value for each potential data mixture, we can quickly identify the input that yields the best target value. This simulation-based optimization is relatively cheap, as both the simulation and the regression prediction are computationally fast. For example, running prediction for 1,000,000 data mixtures takes less than 10 CPU seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Large-scale model training</head><p>After identifying the best data mixture with simulation, we generalize the top-ranked data mixture to a large-scale model training with many more tokens. As shown in Figure <ref type="figure">2</ref>, we directly use the best data mixture for training the larger model. In practice, to increase the robustness of our regression prediction, we select the top 100 mixtures and average them as the data mixture for large-scale training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluating on regression prediction</head><p>In this section, we evaluate the ability of REGMIX to predict the effect of unseen data mixtures. First, we fit the regression model using training artifacts of small (i.e., 1M parameter) models and evaluate the loss prediction performance on small models. Then, to verify our rank invariance hypothesis, we test the learned regression on predicting the rank across model sizes and the number of tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental setup</head><p>Datasets and models. We conduct our experiments using the domains of the Pile dataset <ref type="bibr" target="#b17">[18]</ref> depicted in Table <ref type="table">1</ref>. Due to copyright concerns, we utilize the 17 subsets available on HuggingFace<ref type="foot" target="#foot_0">foot_0</ref> that do not violate copyright issues. We consider both linear regression and LightGBM regression models, where the target variable y is set to be the validation loss of the Pile-CC domain.</p><p>Training and evaluation. The regression model is fitted using the training artifacts of 512× 1M models with 1B tokens, and evaluated on 256× unseen data mixtures for 1M, 60M models (each trained with 1B tokens) and 64× unseen data mixtures for 1B models (each trained with 25B tokens).</p><p>Evaluation metrics. We use three different metrics to benchmark our regression models: (1) Spearman Rank Correlation (ρ) is a non-parametric measure of the strength and direction of the association between two ranked variables. (2) Pearson's r is a measure of the linear relationship </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental results</head><p>High correlation across model sizes. As shown in Table <ref type="table" target="#tab_2">2</ref>, the LightGBM model demonstrates superior performance over linear regression models across all three metrics, with its advantage becoming increasingly pronounced when evaluating on larger models with more training tokens. Meanwhile, the fact that 1M models trained with 1B tokens can achieve such a high correlation of 97.12% on unseen mixtures of 1B models with 25B tokens directly validates our rank invariance hypothesis.</p><p>Proxy model count outweighs training token count. Given the same FLOPs budget for smallscale training, we can either increase the token count (i.e., the number of training tokens) or the number of proxy models. Therefore, we study which approach would yield better performance. As shown in Figure <ref type="figure" target="#fig_3">3</ref>, increasing the training tokens of the proxy models saturates after approximately 0.25B tokens. In contrast, increasing the number of proxy models consistently enhances performance, particularly for the LightGBM model. Notably, the performance of 512 models trained on 0.2B tokens surpasses that of 128 models trained on 0.8B tokens, indicating that increasing the number of proxy models is more effective than increasing the training token count beyond a certain token threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluating on downstream tasks</head><p>In this section, we apply our method to demonstrate its effectiveness on realistic downstream tasks. For evaluation, we exclude specific benchmarks that exhibit large performance variance (e.g., RTE) according to the performance traces reported in previous work <ref type="bibr" target="#b35">[36]</ref> and our observations during pre-training. Ultimately, we select the following benchmarks as our downstream tasks: Social IQA <ref type="bibr" target="#b50">[51]</ref>, HellaSwag <ref type="bibr" target="#b69">[70]</ref>, PiQA <ref type="bibr" target="#b4">[5]</ref>, OpenBookQA <ref type="bibr" target="#b38">[39]</ref>, Lambada <ref type="bibr" target="#b42">[43]</ref>, SciQ <ref type="bibr" target="#b59">[60]</ref>, ARC Easy <ref type="bibr" target="#b10">[11]</ref>, COPA <ref type="bibr" target="#b51">[52]</ref>, RACE <ref type="bibr" target="#b29">[30]</ref>, LogiQA <ref type="bibr" target="#b31">[32]</ref>, QQP <ref type="bibr" target="#b58">[59]</ref>, WinoGrande <ref type="bibr" target="#b49">[50]</ref>, and MultiRC <ref type="bibr" target="#b26">[27]</ref>. These benchmarks cover a diverse range of tasks, enabling a comprehensive evaluation of the real-world impact of REGMIX. For each benchmark, we use normalized accuracy as the evaluation metric if provided by lm-eval-harness <ref type="bibr" target="#b18">[19]</ref> else we use regular accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data mixture significantly impacts downstream performance</head><p>Initially, we train 64 models, each with 1B parameters, using different data mixtures. Every model is trained on 25B tokens<ref type="foot" target="#foot_1">foot_1</ref> from the Pile dataset <ref type="bibr" target="#b17">[18]</ref>, with tokens allocated based on their corresponding domain weights. Table <ref type="table">3</ref> presents the performance of the worst and best models on each downstream task. The reported performance is the average from 0-shot to 5-shot evaluations, scored using the lm-eval-harness evaluation framework <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b3">4]</ref>. We find that the data mixture significantly impacts downstream performances, with the largest performance ∆ reaching 14.6 on the Lambada task. This underscores the importance of studying the optimal data mixture. Table <ref type="table">3</ref>: We experiment with 64 models, each with 1B parameters trained on different data mixtures, and evaluate their performance across various benchmarks. The reported performance on each task is the average score from 0-shot to 5-shot settings, following Muennighoff et al. <ref type="bibr" target="#b40">[41]</ref>. Here, we present the worst and best model performances on each task, and detailed experimental results for individual models can be found in Appendix G.</p><p>Benchmark Worst Model Best Model ∆ Social IQA [51] 32.4 33.9 1.5 HellaSwag [70] 33.0 43.4 10.4 PiQA [5] 60.2 69.0 8.8 OpenBookQA [39] 25.8 31.2 5.4 Lambada [43] 18.9 33.5 14.6 SciQ [60] 76.7 82.9 6.2 ARC Easy [11] 44.9 52.2 7.3 COPA [52] 61.5 70.5 9.0 RACE [30] 27.9 32.5 4.6 LogiQA [32] 23.2 27.7 4.5 QQP [59] 48.0 59.7 11.7 WinoGrande [50] 50.3 53.2 2.9 MultiRC [27] 47.6 55.7 8.1 Average Performance 43.7 47.9 4.2 HellaSwag PiQA Lambada ARC Easy RACE OpenBookQA COPA SciQ QQP WinoGrande Social IQA LogiQA MultiRC ArXiv FreeLaw PubMed Central Wikipedia (en) Github Stack Exchange Pile-CC Validation Loss Downstream Performance -1 0 1 (a) Correlation between validation loss by domains of the Pile and downstream performance. HellaSwag PiQA Lambada ARC Easy RACE OpenBookQA COPA SciQ QQP WinoGrande Social IQA LogiQA MultiRC link.springer.com mail-archives.apache.org patents.google.com <ref type="url" target="www.youtube.com">www.youtube.com</ref>  <ref type="url" target="www.latimes.com">www.latimes.com</ref>  <ref type="url" target="www.ign.com">www.ign.com</ref> itunes.apple.com Validation Loss Downstream Performance  The correlation between validation losses across domains and downstream performance for the 64× 1B models. Note that we take the negative of the loss value when calculating the correlation, as this makes the visualization more intuitive. The same applies for Figure <ref type="figure" target="#fig_7">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Web corpora benefits downstream performance the most</head><p>Next, we visualize the correlation between the validation losses of our 64 1B models across different domains and their performance on various downstream tasks in Figure <ref type="figure" target="#fig_5">4 (a)</ref>. Prior to visualization, we hypothesized that the validation loss on the Wikipedia (en) subset would exhibit a strong correlation with most downstream tasks, as it is a high-quality dataset, and many downstream tasks are derived from Wikipedia text. Similarly, previous work often takes WikiText <ref type="bibr" target="#b37">[38]</ref> as a standard benchmark to indicate language model performance.</p><p>However, surprisingly, the validation loss on the Pile-CC dataset shows the strongest correlation with most downstream tasks. For instance, the correlation coefficient between the HellaSwag task and the Pile-CC validation loss is remarkably close to 1.0. This unexpected result challenges the conventional assumption that WikiText is the most representative dataset for evaluating LLMs. Furthermore, this result aligns with the findings of previous studies <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b23">24]</ref>, which discovered that the validation loss on the web dataset closely relates to downstream performance. Moreover, we analyze the correlation between the loss of models on the C4100Domain validation set <ref type="bibr" target="#b33">[34]</ref>, which is taken from the C4 dataset <ref type="bibr" target="#b46">[47]</ref> and supposed to share a similar distribution as Pile-CC since they are all derived from the CommonCrawl corpus. Since CommonCrawl is a collection of diverse domains, we would expect the correlation between the loss of each domain and Table <ref type="table">4</ref>: Performance comparison of different data selection methods. Human refers to the weights put forth in The Pile <ref type="bibr" target="#b17">[18]</ref>, Pile-CC Only to only training on the Pile-CC component, and DoReMi to the weights from Xie et al. <ref type="bibr" target="#b63">[64]</ref>. The reported performance for each task is the average score across 0-shot to 5-shot settings across five different runs, and the standard deviation. We estimate the compute (measured in FLOPs) required to arrive at the training data mixture. Scores significantly outperforming the Human baseline for each task are highlighted in bold, with significance determined using Cohen's d. To provide a comprehensive assessment, we also report the evaluation results using LightEval, following the setup by Penedo et al. <ref type="bibr" target="#b43">[44]</ref> in Appendix C.4. The LightEval results indicate that REGMIX performs slightly better than DoReMi and Pile-CC Only. Benchmark Human DoReMi Pile-CC Only REGMIX Social IQA [51] 33.8 ± 0.4 33.3 ± 0.2 33.4 ± 0.4 33.5 ± 0.2 HellaSwag [70] 37.7 ± 0.2 43.3 ± 0.3 43.2 ± 0.6 44.0 ± 0.2 PiQA [5] 65.5 ± 0.7 68.6 ± 0.4 68.8 ± 0.6 69.1 ± 0.4 OpenBookQA [39] 28.5 ± 0.4 30.0 ± 0.3 30.5 ± 0.4 29.8 ± 0.5 Lambada [43] 28.3 ± 1.5 32.4 ± 0.7 34.2 ± 1.1 32.9 ± 1.4 SciQ [60] 81.5 ± 1.1 83.3 ± 1.9 82.4 ± 1.0 82.8 ± 0.4 ARC Easy [11] 49.9 ± 0.9 52.3 ± 1.1 51.8 ± 0.4 52.1 ± 0.9 COPA [52] 64.6 ± 1.8 69.7 ± 2.7 67.5 ± 2.0 69.9 ± 0.6 RACE [30] 29.5 ± 0.5 31.1 ± 0.2 31.5 ± 0.5 31.2 ± 0.4 LogiQA [32] 25.7 ± 0.8 25.5 ± 0.7 26.6 ± 1.0 25.4 ± 1.2 QQP [59] 55.6 ± 2.9 57.3 ± 1.4 58.0 ± 1.9 55.7 ± 1.9 WinoGrande [50] 52.0 ± 1.0 52.1 ± 0.3 51.8 ± 0.7 52.1 ± 0.7 MultiRC [27] 52.9 ± 1.4 52.9 ± 1.2 51.2 ± 1.5 52.8 ± 1.5 Average Performance 46.6 ± 0.3 48.6 ± 0.3 48.5 ± 0.3 48.6 ± 0.3 Beat Human on -8 / 13 8 / 13 8 / 13 Estimated FLOPs 0 3.7 × 10 19 0 3.5 × 10 18</p><p>the downstream tasks to vary. However, surprisingly more than 85% of the domains exhibit a very strong correlation with Pile-CC (full correlation graph in Appendix D). This is exemplified by the <ref type="url" target="www.ign.com">www.ign.com</ref> domain, which closely mirrors the overall correlation graph of Pile-CC, as illustrated in Figure <ref type="figure" target="#fig_5">4</ref> (b). It also suggests that the high correlation between Pile-CC and downstream task performance may be attributed to its diverse coverage across various topics and domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Data mixture by REGMIX improves downstream performance</head><p>Previous work has shown that the data mixture method can accelerate LLM pre-training by achieving a smaller validation loss (or perplexity) using less training tokens <ref type="bibr" target="#b63">[64]</ref>. However, a key question is which validation loss should be optimized? The most intuitive approach, which is also adopted by previous work, is to minimize the loss across all domains. However, based on our study of 1M training logs, we found this to be nearly impossible to achieve in practice. None of the data mixtures were able to surpass the human selection on all domain validation losses simultaneously. This suggests that a naive approach of minimizing the loss across all domains is likely infeasible. Therefore, we choose to optimize the Pile-CC validation loss to achieve general performance improvement on downstream tasks since it shows the highest correlation with downstream performance.</p><p>We implement two approaches to determine the data mixture. The first approach relies on human intuition. Since Pile-CC and its own distribution should be the closest match, we hypothesized that pre-training solely on Pile-CC might yield better performance than baselines. The second approach leverages REGMIX, using the Pile-CC validation loss as the target variable. We employed LightGBM to predict the data mixture which can minimize the Pile-CC validation loss.</p><p>We compare the performance of our proposed approaches to strong baselines, including selection done by humans for the Pile <ref type="bibr" target="#b17">[18]</ref>, and DoReMi <ref type="bibr" target="#b63">[64]</ref>. For DoReMi we obtain the data mixture directly from their reported best domain weights and re-normalize it across the available 17 domains. This may result in sub-optimal performance for DoReMi compared to the originally reported results. As shown in Table <ref type="table">4</ref>, both Pile-CC Only and REGMIX demonstrate strong performance compared to the baselines. On the widely used HellaSwag benchmark, REGMIX shows an improvement of 6.8 over Human selection. Additionally, REGMIX beats all other three methods on the task performance in 8 ArXiv FreeLaw NIH ExPorter PubMed Central Wikipedia (en) DM Mathematics Github PhilPapers Stack Exchange Enron Emails Gutenberg (PG-19) Pile-CC Ubuntu IRC EuroParl HackerNews PubMed Abstracts USPTO Backgrounds ArXiv FreeLaw PubMed Central Wikipedia (en) DM Mathematics Github Stack Exchange Gutenberg (PG-19) Pile-CC Ubuntu IRC HackerNews PubMed Abstracts USPTO Backgrounds Validation Loss Domain Weight -1 0 1 assembly c c-sharp common-lisp cpp css dart fortran git-commits github-issues go haskell html java javascript json julia jupyter-scripts kotlin lua makefile markdown mathematica pascal perl php powershell python restructuredtext ruby rust scala shell sql tex typescript visual-basic yaml c c-sharp common-lisp cpp css go haskell java javascript jupyter-scripts kotlin lua python Validation Loss Domain Weight out of 14 cases and yields the highest average score. The surprisingly strong performance of Pile-CC Only reinforces the conclusion from our previous section: web corpora benefits on downstream performance. Finally, REGMIX surpasses the Best Model in Table <ref type="table">3</ref>, demonstrating that our automatic data mixture approach is more efficient than random search.</p><p>While the Pile-CC validation loss is an informative indicator for downstream performance, it may not generalize to every task of interest. Sometimes we may not be able to assume that the validation set stems from a similar data distribution as the training set, but rather face an out-of-distribution scenario. To verify the effectiveness of our method in out-of-distribution scenarios, we fully exclude the Pile-CC domain from the pre-training corpus and use the remaining domains to find the optimal data mixture that minimizes Pile-CC validation loss. As illustrated in Figure <ref type="figure" target="#fig_6">5</ref> (right), our proposed method still outperforms baseline approaches. This demonstrates that REGMIX is robust regardless of whether the target domain is in-or out-of-distribution. We additionally provide the results of regression evaluation under this setting in Figure <ref type="figure" target="#fig_6">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Domain interactions are challenging for humans to understand</head><p>To understand the impact of different domains on each other, we visualize the coefficients (ω) of the linear regression model in Figure <ref type="figure" target="#fig_7">6</ref>. The visualization provides insights into how the various data domains contribute to the others, revealing complex interactions among them. We also display code correlation diagrams for each 1M code model trained on The Stack dataset <ref type="bibr" target="#b27">[28]</ref>. Surprisingly, both the domain interaction visualization and the code correlation diagrams display complex relationships that are difficult for human experts to fully comprehend. For example, the PhilPapers domain in the Pile dataset appears to provide gains for all other domains under the linear regression modeling, which is a non-obvious finding that challenges intuitive human understanding. These visualizations highlight the inherent complexity in determining the optimal data mixture, underscoring the value of our automated REGMIX approach in efficiently identifying high-performing mixtures, rather than relying solely on human intuition.</p><p>10 -3 10 -2 10 -1 10 0 Domain Weight 1.4 1.6 1.8 2.0 2.2 Log(Validation Loss) ArXiv 10 -3 10 -2 10 -1 10 0 Domain Weight 1.4 1.6 1.8 Log(Validation Loss) FreeLaw 10 -3 10 -2 10 -1 10 0 Domain Weight 1.4 1.6 1.8 2.0 Log(Validation Loss) PubMed Central 10 -3 10 -2 10 -1 10 0 Domain Weight 1.6 1.7 1.8 1.9 2.0 Log(Validation Loss) Wikipedia (en) 10 -3 10 -2 10 -1 10 0 Domain Weight 0.75 1.00 1.25 1.50 1.75 Log(Validation Loss) DM Mathematics 10 -3 10 -2 10 -1 10 0 Domain Weight 1.4 1.6 1.8 2.0 Log(Validation Loss) Github 10 -3 10 -2 10 -1 10 0 Domain Weight 1.4 1.6 1.8 2.0 Log(Validation Loss) Stack Exchange 10 -3 10 -2 10 -1 10 0 Domain Weight 1.6 1.7 1.8 1.9 Log(Validation Loss) Gutenberg (PG-19) 10 -3 10 -2 10 -1 10 0 Domain Weight 1.7 1.8 1.9 Log(Validation Loss) Pile-CC 10 -3 10 -2 10 -1 10 0 Domain Weight 1.6 1.8 2.0 2.2 Log(Validation Loss) Ubuntu IRC 10 -3 10 -2 10 -1 10 0 Domain Weight 1.6 1.7 1.8 1.9 Log(Validation Loss) HackerNews 10 -3 10 -2 10 -1 10 0 Domain Weight 1.6 1.8 2.0 Log(Validation Loss) PubMed Abstracts </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Data mixture effects transcend scaling laws</head><p>Recent research <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b19">20]</ref> has demonstrated the feasibility of scaling laws for data mixture. However, our findings in Section 5.4 suggest that the relationship between domain weights and validation loss is more complex than scaling laws might imply. To visualize this complexity, we plotted all experimental points of our 1M training logs in Figure <ref type="figure" target="#fig_8">7</ref>. If the scaling law of data mixture held true, we would expect to see a clear log-log linear relationship across all domains. However, our results reveal a more nuanced picture. For example, the DM Mathematics domain, possibly due to its distinct distribution compared to other domains, exhibits a near log-log linear relationship between loss and domain weight. In contrast, for most domains like Pile-CC show more complex patterns, where predicting validation loss is non-trivial. As shown, domain interactions appear to be intricate, making it challenging to predict the validation loss for a domain based solely on its weight in the mixture. These findings suggest that while scaling laws provide valuable insights, they may not fully capture the intricacies of data mixture dynamics. Our approach addresses the challenge by modeling the entire data mixture as input for the regression model, providing a more comprehensive framework for understanding and predicting the validation loss while simultaneously accounting for all domain weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we present a novel approach, REGMIX, for automatically selecting the optimal data mixture for pre-training large language models. REGMIX formulates the data mixture problem as a regression task and trains small models to predict the impact of different data mixtures. This enables efficient identification of the best mixture, which we then generalize to large-scale model training. REGMIX predicts the best data mixture among 64 x 1B models demonstrating its effectiveness. Moreover, our large-scale study provides valuable insights into the impact of data mixture, the relationship between loss and downstream performance, and the domain interaction challenges for human experts in determining the optimal mixture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Limitations</head><p>Despite making progress in understanding and optimizing data mixtures for better performance, our method still has several limitations.</p><p>The maximum model parameters. We have verified that small models can be used to predict the optimal data mixture for large-scale runs with up to 1B parameters. However, much larger models are commonly trained with 7B or 70B parameters <ref type="bibr" target="#b57">[58]</ref>. Due to compute constraints we leave the verification of REGMIX at larger scales to future work.</p><p>The benchmark coverage. Owing to the scarcity of relevant data in the Pile corpus and the relatively small size of our model at 1B scale, their performance on the MMLU benchmark <ref type="bibr" target="#b21">[22]</ref> is nearly random and negligible on GSM8K <ref type="bibr" target="#b11">[12]</ref>. Consequently, we do not compute the correlation between the validation loss and scores on these challenging benchmarks.</p><p>The infinite data assumption. Most existing data mixing methods assume the availability of unlimited data for each domain. Although we consider this issue in our no Pile-CC experiments in Section 5.3, systematically incorporating the effect of available data into the method remains challenging. Combining our method with the decay coefficient of data reuse proposed in Muennighoff et al. <ref type="bibr" target="#b40">[41]</ref> could be an interesting future work to explore, potentially addressing the limited data availability scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The domain assumption.</head><p>A common assumption of existing data mixture methods (including ours) is that the domain each example belongs to is known. However, this may not always be the case and the domain needs to be obtained first. Assigning examples to domains is a hard task, which may make it challenging to apply our methods when the domain boundaries are unclear.</p><p>The tokenizer assumption. All existing data mixture methods require the use of proxy models to obtain domain weights. However, a fundamental assumption of these methods is that the proxy model uses the same tokenizer and vocabulary size as the large model. Generalizing weights across different tokenizers poses significant challenges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Ethic statements</head><p>Optimizing the data mixture for LLM pre-training raises several ethical issues. First, the optimized data mixture might be biased toward certain domains, which is good for achieving better performance. However, certain domains might be underrepresented or misrepresented, leading the trained models to perform poorly or produce biased results for these domains. Second, though our method aims to optimize the data mixture efficiently, searching for the optimal data mixture still requires computational resources, leading to high energy consumption and environmental impact. It is worthwhile to explore how to further reduce the computation cost. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 The regression prediction visualization</head><p>As shown in Figure <ref type="figure" target="#fig_9">8</ref>, we visualize the predicted and true loss pairs of the linear model and LightGBM model on the 1M models. The LightGBM model performs better than the linear model, achieving near 100% Spearman Rank Correlation ρ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Loss and rank prediction on small models for out-of-distribution setting</head><p>In Section 5, we verify the effectiveness of our method in out-of-distribution scenarios where we fully exclude the Pile-CC domain from the pre-training corpus and use the remaining domains to find the optimal data mixture that minimizes Pile-CC validation loss. We also provide the results of regression evaluation under this setting in Figure <ref type="figure" target="#fig_6">5</ref>. Similarly, LightGBM model outperforms the linear model and achieves nearly 100% Spearman Rank Correlation ρ.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 The evaluation results using LightEval</head><p>Following the approach of FineWeb <ref type="bibr" target="#b43">[44]</ref>, we employ the LightEval<ref type="foot" target="#foot_2">foot_2</ref> library to evaluate our models using a suite of benchmarks selected for their stability and suitability. The chosen benchmarks exhibit three key characteristics: low score variance across different data samples, monotonic score improvement during training, and above-random baseline scores for models in the 1B parameter range.</p><p>Table 7 presents the evaluation results. Our method, REGMIX, consistently outperforms the Human baseline on 6 benchmarks. Moreover, REGMIX demonstrates superior average performance compared to the DoReMi and the Pile-CC Only methods. ArXiv FreeLaw NIH ExPorter PubMed Central Wikipedia (en) DM Mathematics Github PhilPapers Stack Exchange Enron Emails Gutenberg (PG-19) Pile-CC Ubuntu IRC EuroParl HackerNews PubMed Abstracts USPTO Backgrounds Domain 10 -3 10 -1 Weight 1M 60M</p><p>Figure <ref type="figure" target="#fig_3">13</ref>: REGMIX yields similar data mixture distributions when using the 1M model and the 60M model as proxy models, demonstrating the stability of our method. Note that the y-axis is in log-scale for visualization purpose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Implementation details</head><p>We utilize the model architecture proposed by Zhang et al. <ref type="bibr" target="#b70">[71]</ref> and create various model variants by modifying the number of layers, the number of attention heads, and the dimensions of token embeddings and hidden states, as illustrated in Figure <ref type="figure" target="#fig_9">8</ref>. For tokenization, we employ the GPTNeoX tokenizer <ref type="bibr" target="#b5">[6]</ref>, which has a vocabulary size of 50,432.</p><p>For models with 1M and 60M parameters, we set the training iterations as 1000 and the batch size as 1M tokens, which means the training budget is 1B tokens. Similarly, we train the larger model with 1B parameters with 25000 training iterations and the same batch size thus consuming 25B tokens in total. We set the learning rate as 4e-4 and use the cosine learning rate scheduler.</p><p>For linear regression, we employ 5-fold cross-validation with ridge regression to determine the optimal ℓ 2 regularization weight from the set [1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3]. For LightGBM, we manually set the number of iterations to 1000 and the learning rate to 1e-2. leaving all other hyperparameters at their default values. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F The stability of our method</head><p>Previous research <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b1">2]</ref> has employed small-scale proxy models, trained on substantial volumes of tokens, to predict optimal data mixtures for large language models. However, these approaches often suffer from instability issues. For example, DoReMi <ref type="bibr" target="#b63">[64]</ref> reported that different proxy model sizes can result in significantly different predicted data mixtures. Their findings (Figure <ref type="figure" target="#fig_9">8</ref>, Appendix) show that using a 280M proxy model resulted in a Pile-CC weight of 0.67, while a 1B proxy model yielded a Pile-CC weight below 0.20. The large discrepancy highlights potential instabilities in previous approaches. To evaluate the robustness of REGMIX against such instabilities, we conducted comparative experiments using two distinct model scales: a 1M proxy model and a 60M proxy model. We used their respective training logs to fit regression models and subsequently simulated the top 1024 predictions. The resulting distributions are plotted in Figure <ref type="figure" target="#fig_3">13</ref>. Our results demonstrate that while the prediction distributions for the 1M and 60M models are not identical, they exhibit remarkably similar patterns. This consistency suggests that REGMIX achieves improved stability compared to previous approaches, even when varying the scale of proxy training models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Detailed experimental results</head><p>To facilitate future research, we share all the data mixtures and the corresponding downstream performances of the 64 trained models with 1B parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Index 1 2 3 4 5 6 7 8 Pre-training Domain Weights ArXiv 0.123 0.066 0.055 0.059 0.201 0.036 0.042 0.126 FreeLaw 0.065 0.071 0.052 0.083 0.004 0.212 0.113 0.21 NIH ExPorter 0.0 0.0 0.004 0.0 0.014 0.0 0.0 0.0 PubMed Central 0.126 0.211 0.177 0.174 0.243 0.153 0.089 0.123 Wikipedia (en) 0.036 0.013 0.02 0.177 0.01 0.005 0.022 0.055 DM Mathematics 0.0 0.0 0.011 0.0 0.03 0.047 0.007 0.008 Github 0.034 0.153 0.095 0.194 0.017 0.205 0.028 0.008 PhilPapers 0.0 0.033 0.0 0.0 0.0 0.0 0.0 0.0 Stack Exchange 0.039 0.097 0.18 0.0 0.103 0.075 0.011 0.129 Enron Emails 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 Gutenberg (PG-19) 0.0 0.0 0.016 0.0 0.002 0.0 0.217 0.035 Pile-CC 0.27 0.101 0.381 0.192 0.359 0.209 0.232 0.288 Ubuntu IRC 0.0 0.0 0.001 0.005 0.0 0.0 0.08 0.0 EuroParl 0.0 0.0 0.0 0.109 0.0 0.001 0.117 0.0 HackerNews 0.0 0.011 0.005 0.0 0.0 0.0 0.018 0.0 PubMed Abstracts 0.0 0.136 0.0 0.005 0.014 0.002 0.011 0.016 USPTO Backgrounds 0.307 0.106 0.003 0.0 0.002 0.055 0.011 0.0 Downstream Performance (%) Social IQA 33.27 33.33 33.62 33.53 33.49 33.56 33.62 33.55 HellaSwag 40.58 36.86 40.58 36.06 40.07 37.85 37.93 39.59 PiQA 67.29 65.14 67.97 64.66 67.03 65.36 66.0 66.55 OpenBookQA 28.63 27.87 29.33 29.1 29.23 28.33 29.13 28.73 Lambada 29.17 26.86 31.55 27.11 29.16 28.92 31.53 30.92 SciQ 80.68 79.98 81.05 80.8 82.4 79.88 78.67 79.7 COPA 70.5 63.83 69.17 65.0 67.5 66.0 66.67 68.67 RACE 29.47 30.0 32.11 28.82 31.13 30.06 29.9 30.75 ARC Easy 50.03 48.72 50.01 46.64 51.06 47.46 46.75 48.39 LogiQA 23.76 24.17 25.29 25.29 24.55 25.96 25.45 26.32 QQP 55.71 55.9 54.84 56.52 54.01 56.34 52.35 54.2 WinoGrande 51.54 51.59 51.39 50.91 53.13 52.26 51.26 51.45 MultiRC 52.65 53.39 51.89 50.92 49.03 53.09 53.64 50.23 Avg 47.18 45.97 47.60 45.80 47.06 46.54 46.38 46.85 Model Index 9 10 11 12 13 14 15 16 Pre-training Domain Weights ArXiv 0.184 0.226 0.107 0.139 0.101 0.099 0.251 0.147 FreeLaw 0.009 0.046 0.276 0.048 0.047 0.002 0.024 0.046 NIH ExPorter 0.0 0.0 0.0 0.0 0.001 0.022 0.0 0.0 PubMed Central 0.094 0.261 0.157 0.184 0.119 0.501 0.101 0.196 Wikipedia (en) 0.035 0.001 0.009 0.032 0.049 0.003 0.17 0.14 DM Mathematics 0.007 0.001 0.0 0.001 0.092 0.0 0.0 0.008 Github 0.106 0.189 0.024 0.055 0.078 0.017 0.048 0.237 PhilPapers 0.0 0.0 0.0 0.0 0.0 0.043 0.019 0.0 Stack Exchange 0.142 0.077 0.051 0.109 0.002 0.065 0.007 0.06 Enron Emails 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 Gutenberg (PG-19) 0.0 0.01 0.001 0.0 0.051 0.091 0.0 0.012 Pile-CC 0.341 0.114 0.273 0.354 0.283 0.055 0.339 0.111 Ubuntu IRC 0.0 0.003 0.0 0.0 0.057 0.0 0.017 0.0 EuroParl 0.0 0.0 0.003 0.003 0.0 0.006 0.0 0.0 HackerNews 0.002 0.0 0.034 0.0 0.0 0.0 0.0 0.001 PubMed Abstracts 0.005 0.039 0.009 0.075 0.061 0.007 0.0 0.01 USPTO Backgrounds 0.075 0.033 0.056 0.0 0.057 0.088 0.024 0.032 Downstream Performance (%) Social IQA 33.43 33.21 33.31 33.17 33.28 32.43 33.57 33.7 HellaSwag 40.05 35.89 39.55 39.89 38.63 36.18 39.52 35.94 PiQA 66.6 64.74 66.29 66.27 66.9 64.05 66.7 64.51 OpenBookQA 28.87 26.6 29.33 28.73 29.4 27.87 29.67 27.83 Lambada 31.39 27.37 30.32 30.31 31.38 26.25 29.86 26.95 SciQ 81.1 79.12 79.97 82.85 79.42 81.4 81.38 81.23 COPA 67.0 64.5 66.83 69.5 67.33 65.83 69.5 66.33 RACE 30.57 29.63 30.49 30.85 30.35 28.66 31.21 29.57 ARC Easy 50.66 47.74 47.47 50.18 49.92 49.52 50.73 48.65 LogiQA 23.6 25.65 26.37 23.81 25.58 26.29 25.86 25.12 QQP 54.89 54.79 54.2 55.23 53.69 57.09 53.95 54.24 WinoGrande 50.83 51.84 51.05 51.83 52.12 52.0 51.01 51.82 MultiRC 54.18 54.48 50.17 52.12 51.42 52.69 51.87 53.48 Avg 47.17 45.81 46.57 47.29 46.88 46.17 47.30 46.11 18 19 20 21 22 23 24 Pre-training Domain Weights ArXiv 0.228 0.0 0.501 0.101 0.047 0.031 0.078 0.068 FreeLaw 0.016 0.019 0.005 0.03 0.014 0.073 0.024 0.181 NIH ExPorter 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 PubMed Central 0.204 0.084 0.156 0.272 0.163 0.053 0.302 0.126 Wikipedia (en) 0.02 0.159 0.17 0.021 0.218 0.129 0.027 0.07 DM Mathematics 0.036 0.009 0.0 0.099 0.0 0.0 0.0 0.001 Github 0.02 0.012 0.022 0.124 0.137 0.066 0.04 0.195 PhilPapers 0.004 0.0 0.017 0.0 0.0 0.0 0.0 0.0 Stack Exchange 0.002 0.052 0.062 0.113 0.173 0.12 0.007 0.24 Enron Emails 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 Gutenberg (PG-19) 0.0 0.001 0.002 0.054 0.001 0.089 0.002 0.0 Pile-CC 0.244 0.361 0.061 0.154 0.19 0.057 0.499 0.023 Ubuntu IRC 0.0 0.296 0.002 0.0 0.029 0.001 0.0 0.0 EuroParl 0.004 0.0 0.0 0.001 0.007 0.0 0.0 0.0 HackerNews 0.0 0.0 0.0 0.0 0.011 0.031 0.0 0.0 PubMed Abstracts 0.196 0.001 0.0 0.011 0.008 0.351 0.0 0.059 USPTO Backgrounds 0.026 0.007 0.002 0.02 0.001 0.001 0.021 0.036 Downstream Performance (%) Social IQA 33.89 33.31 33.53 33.38 33.75 33.24 33.56 33.71 HellaSwag 38.68 39.9 34.67 37.12 37.44 36.07 42.15 34.67 PiQA 66.83 67.39 63.33 64.83 65.0 63.68 67.8 62.99 OpenBookQA 28.13 30.67 28.03 29.4 27.67 27.77 29.37 25.83 Lambada 28.78 28.56 24.13 29.41 27.67 28.03 33.47 24.04 SciQ 79.6 78.83 77.42 78.98 78.95 78.72 81.83 79.12 COPA 65.17 68.17 65.33 67.33 67.67 62.67 69.83 65.83 RACE 28.74 30.03 29.76 29.49 30.77 29.76 31.21 27.91 ARC Easy 48.86 49.42 47.9 48.3 47.88 46.68 50.92 45.24 LogiQA 25.91 26.34 26.24 25.76 26.11 26.24 24.17 25.91 QQP 53.35 53.18 50.61 51.49 54.27 54.99 52.77 55.19 WinoGrande 52.54 51.17 52.01 51.09 52.13 52.03 52.5 50.28 MultiRC 51.49 52.45 55.4 54.87 51.73 49.49 50.61 50.29 Avg 46.30 46.88 45.26 46.27 46.23 45.34 47.71 44.69</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Left: We hypothesize the rank invariance of data mixtures across model sizes and numbers of training tokens. Leveraging this hypothesis, we use small models trained on fewer tokens to predict the effective data mixture for training large models with substantially more tokens. Right: By training 512× 1M models, our method identifies the best data mixture prior to training 64× 1B models. The predicted best data mixture, denoted by the red star, achieves the lowest validation loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Train small-scale proxy models 1 4 Fit a regression model using data mixture as source 2 Figure 2 :</head><label>1422</label><figDesc>Figure 2: The illustration of our method using Hacker News, GitHub, and Philpapers as training domains, with the loss on the StackExchange domain as the Target (where ↓ indicates lower is better). A regression model is fitted using small-scale proxy model training logs and employed to predict the best data mixture within the simulation space, enabling direct prediction of the data mixture for large-scale language model pre-training. Note that the Philpapers domain is omitted in the simulation plot (3) for simplicity. fine-grained level of selection dealing with the filtering of tokens [31]. (2) Sample-level selection is about choosing individual training examples. It is commonly employed for selecting fine-tuning data<ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b66">67]</ref>. For the pre-training of LLMs, most methods rely on heuristics<ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b54">55]</ref>, but there have been some learned approaches using optimization algorithms<ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b68">69]</ref>, model perplexity<ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b40">41]</ref>, or LLMs to inform the sample selection process<ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b71">72]</ref>. (3) Group-level selection assumes the data can be grouped into pools that are then optimally mixed. While early work again relies on manual mixtures<ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b6">7]</ref>, learned mixtures have become more common<ref type="bibr" target="#b2">[3]</ref>. Learned approaches either leverage proxy models to determine fixed weights for each group ("offline selection")<ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b15">16]</ref> or dynamically adjust the weights during training of the final model ("online selection")<ref type="bibr" target="#b8">[9]</ref>. Our approach, REGMIX, is an offline group-level selection method. Different from the flagship algorithm in this category, DoReMi<ref type="bibr" target="#b63">[64]</ref>, REGMIX does not require training a single model for hundreds of thousands of steps, but instead a few small models for short durations. As these can be trained in parallel, our approach is more scalable, while also yielding better weights leading to a more performant final model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The plot of Spearman Rank Correlation ρ between the predicted ranks and true ranks of Linear regression (Left) and LightGBM regression (Right) across different training tokens and different number of proxy models. As shown, increasing the number of proxy models significantly boosts ρ, while adding more training tokens has diminishing returns.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><figDesc>Correlation between validation loss by URL domain within the Pile-CC subset and downstream performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4:  The correlation between validation losses across domains and downstream performance for the 64× 1B models. Note that we take the negative of the loss value when calculating the correlation, as this makes the visualization more intuitive. The same applies for Figure6.</figDesc><graphic coords="7,165.14,363.23,109.49,59.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Left: The validation loss on Pile-CC of different methods with Pile-CC in the pre-training corpus. Right: The validation loss on Pile-CC excluding Pile-CC in the pre-training.</figDesc><graphic coords="9,301.82,272.44,176.27,60.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The visualization of correlations between different target domain validation losses and training domain weights using the linear regression model. Left is on the Pile dataset, and Right is on the Stack dataset. A high correlation indicates that increasing the training domain weight has a positive impact on reducing the target domain validation loss.</figDesc><graphic coords="9,166.72,276.18,75.22,57.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The visualization of 1M training logs across various data mixture. The x-axis represents the weight of each domain in data mixture and the y-axis shows the log value of validation loss for that domain. As seen in the variation along the y-axis, predicting the validation loss solely based on the domain weight is challenging.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: The visualization of loss prediction on small models (e.g., 1M parameters). Left: The scatter plot of predicted and true loss pairs of Linear model. Right: The scatter plot of predicted and true loss pairs of LightGBM model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>We fit the regression model based on the results of the 512× 1M models trained on 1B tokens, and evaluate it on unseen data mixtures for 1M, 60M, and 1B parameter models depicted below. Pearson's r and MSE measure the loss prediction performance, while the Spearman correlation ρ compares the predicted and actual ranks.</figDesc><table><row><cell>Method</cell><cell cols="3">1M models with 1B tokens</cell><cell cols="4">60M models with 1B tokens 1B models with 25B tokens</cell></row><row><cell></cell><cell cols="4">ρ (↑) Pearson's r (↑) MSE (↓) ρ (↑)</cell><cell>Pearson's r (↑)</cell><cell>ρ (↑)</cell><cell>Pearson's r (↑)</cell></row><row><cell>Linear</cell><cell>90.08</cell><cell>87.78</cell><cell>0.13</cell><cell>89.26</cell><cell>86.79</cell><cell>88.01</cell><cell>72.57</cell></row><row><cell cols="2">LightGBM 98.45</cell><cell>98.57</cell><cell>0.04</cell><cell>98.64</cell><cell>98.28</cell><cell>97.12</cell><cell>94.36</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>The regression model is fitted using the training artifacts of 512× 1M models trained with 1B tokens excluding the Pile-CC domain, and evaluated on unseen data mixtures for 1M parameter models. Pearson's r and MSE measure the loss prediction performance, while ρ compares the predicted and actual ranks.Table6presents the derived data mixture weights for different methods. As illustrated, REGMIX assigns a high weight of 0.87 to the Pile-CC dataset, aligning with human intuition.</figDesc><table><row><cell>Method</cell><cell cols="3">1M models with 1B tokens</cell></row><row><cell></cell><cell cols="3">ρ (↑) Pearson's r (↑) MSE (↓)</cell></row><row><cell>Linear</cell><cell>83.00</cell><cell>84.18</cell><cell>0.08</cell></row><row><cell cols="2">LightGBM 95.47</cell><cell>95.48</cell><cell>0.04</cell></row><row><cell>C.3 The derived data mixtures</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>The domain weights of different methods. In our experiments, DoReMi refers to the reported best reference model with 280M parameters and its corresponding domain weights. † Note that the domain weights of Human and DoReMi are re-normalized from the weights reported in Xie et al.<ref type="bibr" target="#b63">[64]</ref> to adapt them to the available domains. The DoReMi weight are derived from the best-performing configuration obtained using a 280M parameter model.</figDesc><table><row><cell>Domain Weights</cell><cell cols="4">Human  † DoReMi  † Pile-CC Only REGMIX</cell></row><row><cell>ArXiv</cell><cell>0.134</cell><cell>0.004</cell><cell>0.0</cell><cell>0.001</cell></row><row><cell>FreeLaw</cell><cell>0.049</cell><cell>0.005</cell><cell>0.0</cell><cell>0.001</cell></row><row><cell>NIH ExPorter</cell><cell>0.007</cell><cell>0.008</cell><cell>0.0</cell><cell>0.001</cell></row><row><cell>PubMed Central</cell><cell>0.136</cell><cell>0.006</cell><cell>0.0</cell><cell>0.003</cell></row><row><cell>Wikipedia (en)</cell><cell>0.117</cell><cell>0.086</cell><cell>0.0</cell><cell>0.016</cell></row><row><cell>DM Mathematics</cell><cell>0.025</cell><cell>0.002</cell><cell>0.0</cell><cell>0.0</cell></row><row><cell>Github</cell><cell>0.054</cell><cell>0.022</cell><cell>0.0</cell><cell>0.0</cell></row><row><cell>PhilPapers</cell><cell>0.003</cell><cell>0.034</cell><cell>0.0</cell><cell>0.0</cell></row><row><cell>Stack Exchange</cell><cell>0.118</cell><cell>0.019</cell><cell>0.0</cell><cell>0.0</cell></row><row><cell>Enron Emails</cell><cell>0.004</cell><cell>0.009</cell><cell>0.0</cell><cell>0.002</cell></row><row><cell>Gutenberg (PG-19)</cell><cell>0.025</cell><cell>0.009</cell><cell>0.0</cell><cell>0.002</cell></row><row><cell>Pile-CC</cell><cell>0.142</cell><cell>0.743</cell><cell>1.0</cell><cell>0.87</cell></row><row><cell>Ubuntu IRC</cell><cell>0.009</cell><cell>0.011</cell><cell>0.0</cell><cell>0.064</cell></row><row><cell>EuroParl</cell><cell>0.005</cell><cell>0.008</cell><cell>0.0</cell><cell>0.0</cell></row><row><cell>HackerNews</cell><cell>0.01</cell><cell>0.016</cell><cell>0.0</cell><cell>0.012</cell></row><row><cell>PubMed Abstracts</cell><cell>0.107</cell><cell>0.014</cell><cell>0.0</cell><cell>0.024</cell></row><row><cell>USPTO Backgrounds</cell><cell>0.053</cell><cell>0.004</cell><cell>0.0</cell><cell>0.002</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Performance comparison of different data selection methods using LightEval following previous work<ref type="bibr" target="#b43">[44]</ref>. Human refers to the weights put forth in The Pile<ref type="bibr" target="#b17">[18]</ref>, Pile-CC Only to only training on the Pile-CC component, and DoReMi to the weights from Xie et al.<ref type="bibr" target="#b63">[64]</ref>. The reported performance for each task is the average zero-shot task performance across five different runs, and the standard deviation. We estimate the compute (measured in FLOPs) required to arrive at the training data mixture. Scores significantly outperforming the Human baseline for each task are highlighted in bold, with significance determined using Cohen's d.</figDesc><table><row><cell>Benchmark</cell><cell>Human</cell><cell>DoReMi</cell><cell cols="2">Pile-CC Only REGMIX</cell></row><row><cell>ARC Easy [11]</cell><cell cols="2">45.3 ± 0.4 46.6 ± 0.7</cell><cell>47.1 ± 0.6</cell><cell>47.2 ± 0.9</cell></row><row><cell>ARC Challenge [11]</cell><cell cols="2">25.5 ± 0.8 25.9 ± 0.8</cell><cell>25.6 ± 0.5</cell><cell>25.6 ± 0.5</cell></row><row><cell cols="3">CommonsenseQA [56] 31.8 ± 1.2 34.1 ± 0.7</cell><cell>34.9 ± 0.3</cell><cell>35.0 ± 0.5</cell></row><row><cell>HellaSwag [70]</cell><cell cols="2">36.5 ± 0.2 41.5 ± 0.3</cell><cell>39.7 ± 0.5</cell><cell>42.1 ± 0.3</cell></row><row><cell>OpenBookQA [39]</cell><cell cols="2">29.8 ± 0.6 31.0 ± 0.8</cell><cell>31.5 ± 0.4</cell><cell>31.8 ± 0.8</cell></row><row><cell>PiQA [5]</cell><cell cols="2">65.4 ± 0.6 68.7 ± 0.3</cell><cell>69.0 ± 0.5</cell><cell>69.4 ± 0.5</cell></row><row><cell>Social IQA [51]</cell><cell cols="2">41.7 ± 0.3 42.0 ± 0.2</cell><cell>42.7 ± 0.3</cell><cell>42.6 ± 0.7</cell></row><row><cell>WinoGrande [50]</cell><cell cols="2">51.1 ± 1.0 51.2 ± 0.4</cell><cell>50.7 ± 1.0</cell><cell>50.9 ± 0.4</cell></row><row><cell>MMLU [22]</cell><cell cols="2">28.6 ± 0.2 28.9 ± 0.4</cell><cell>28.5 ± 0.2</cell><cell>28.7 ± 0.3</cell></row><row><cell>Average Performance</cell><cell cols="2">39.5 ± 0.3 41.1 ± 0.3</cell><cell>41.2 ± 0.3</cell><cell>41.5 ± 0.2</cell></row><row><cell>Beat Human on</cell><cell>-</cell><cell>5 / 9</cell><cell>6 / 9</cell><cell>6 / 9</cell></row><row><cell>Estimated FLOPs</cell><cell>0</cell><cell cols="2">3.7 × 10 19 0</cell><cell>3.5 × 10 18</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>The detailed model configuration for different model sizes.</figDesc><table><row><cell>Model</cell><cell>1M</cell><cell>60M</cell><cell>1B</cell></row><row><cell cols="4">Vocabulary Size 50432 50432 50432</cell></row><row><cell>n layers</cell><cell>2</cell><cell>10</cell><cell>22</cell></row><row><cell>n heads</cell><cell>8</cell><cell>8</cell><cell>16</cell></row><row><cell>d embedding</cell><cell>256</cell><cell>768</cell><cell>2048</cell></row><row><cell>d model</cell><cell>512</cell><cell>1536</cell><cell>5632</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_0"><p>https://huggingface.co/datasets/monology/pile-uncopyrighted</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_1"><p>We set the token quantity such that it is compute-optimal according to the Chinchilla scaling law<ref type="bibr" target="#b22">[23]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_2"><p>https://github.com/huggingface/lighteval</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D URL domain correlation graph</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Introducing meta llama 3: The most capable openly available llm to date</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Meta</surname></persName>
		</author>
		<ptr target="https://ai.meta.com/blog/meta-llama-3/" />
		<imprint>
			<date type="published" when="2024-04">April 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Alon</forename><surname>Albalak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangming</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2312.02406</idno>
		<title level="m">Efficient online data mixing for language model pre-training</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A survey on data selection for language models</title>
		<author>
			<persName><forename type="first">Alon</forename><surname>Albalak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanai</forename><surname>Elazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang</forename><surname>Michael Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bairu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangming</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haewon</forename><surname>Jeong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.16827</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailey</forename><surname>Schoelkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lintang</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Tow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alham</forename><surname>Baber Abbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pawan</forename><surname>Fikri Aji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sidney</forename><surname>Sasanka Ammanamanchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><surname>Clive</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.14782</idno>
		<title level="m">Lessons from the trenches on reproducible evaluation of language models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Reasoning about physical commonsense in natural language</title>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hallahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Leahy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Mcdonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.06745</idno>
		<title level="m">Gpt-neox-20b: An open-source autoregressive language model</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper_files/paper/2020/file/1457" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
	<note>c0d6bfcb4967418bfb8ac142f64a-Paper.pdf</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Data diversity matters for robust instruction tuning</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Bukharin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuo</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.14736</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Skill-it! a data-driven skills framework for understanding and training language models</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Mayee F Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kush</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jue</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederic</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Sala</surname></persName>
		</author>
		<author>
			<persName><surname>Ré</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.14430</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Take the bull by the horns: Hard sample-reweighted continual training improves llm generalization</title>
		<author>
			<persName><forename type="first">Xuxi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhendong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daouda</forename><surname>Sow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingbin</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.14270</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isaac</forename><surname>Cowhey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carissa</forename><surname>Schoenick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05457</idno>
		<title level="m">Think you have solved question answering? try arc, the ai2 reasoning challenge</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Training verifiers to solve math word problems</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Cobbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reiichiro</forename><surname>Nakano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno>CoRR, abs/2110.14168</idno>
		<ptr target="https://arxiv.org/abs/2110.14168" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deft: Data efficient fine-tuning for large language models via unsupervised core-set selection</title>
		<author>
			<persName><forename type="first">Devleena</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Khetan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.16776</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Sailor: Open language models for south-east asia</title>
		<author>
			<persName><forename type="first">Longxu</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangtao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2404.03608</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2404.03608" />
		<imprint>
			<biblScope unit="page">2024</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Axel</forename><surname>Feldmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.12926</idno>
		<title level="m">Dsdm: Model-aware dataset selection with datamodels</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Simin</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Pagliardini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName><surname>Doge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.15393</idno>
		<title level="m">Domain reweighting with generalization estimation</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Language models scale reliably with over-training and on downstream tasks</title>
		<author>
			<persName><forename type="first">Yitzhak</forename><surname>Samir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Gadre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaishaal</forename><surname>Smyrnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suchin</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rulin</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Mercat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sedrick</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Keh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marianna</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Nezhurina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenia</forename><surname>Vasiljevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><forename type="middle">G</forename><surname>Jitsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Dimakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuran</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yair</forename><surname>Kollar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Achal</forename><surname>Carmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reinhard</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Heckel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><surname>Schmidt</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2403.08540</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2403.08540" />
		<imprint>
			<biblScope unit="page">2024</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The pile: An 800gb dataset of diverse text for language modeling</title>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Travis</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anish</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noa</forename><surname>Nabeshima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shawn</forename><surname>Presser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Leahy</surname></persName>
		</author>
		<idno>CoRR, abs/2101.00027</idno>
		<ptr target="https://arxiv.org/abs/2101.00027" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A framework for few-shot language model evaluation</title>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Tow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Baber Abbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sid</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Dipofi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alain</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haonan</forename><surname>Le Noac'h</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Mcdonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Ociepa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laria</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailey</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aviya</forename><surname>Schoelkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lintang</forename><surname>Skowron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anish</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Zou</surname></persName>
		</author>
		<ptr target="https://zenodo.org/records/10256836" />
		<imprint>
			<biblScope unit="page" from="12" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Data mixing made efficient: A bivariate scaling law for language model pretraining</title>
		<author>
			<persName><forename type="first">Ce</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daoyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolin</forename><surname>Ding</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.14908</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Scaling laws for data filtering -data curation cannot be compute agnostic</title>
		<author>
			<persName><forename type="first">Sachin</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pratyush</forename><surname>Maini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditi</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2404.07177</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2404.07177" />
		<imprint>
			<biblScope unit="page">2024</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Measuring massive multitask language understanding</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Collin</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=d7KBjmI3GmQ" />
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">May 3-7, 2021</date>
		</imprint>
	</monogr>
	<note>OpenReview.net, 2021</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Training compute-optimal large language models</title>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>De Las</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.15556</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Yuzhen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinghan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zifei</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.09937</idno>
		<title level="m">Compression represents intelligence linearly</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Get more for less: Principled data selection for warming up fine-tuning in llms</title>
		<author>
			<persName><forename type="first">Feiyang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Just</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Himanshu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Jahagirdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anit</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoxi</forename><surname>Kumar Sahu</surname></persName>
		</author>
		<author>
			<persName><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.02774</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Lightgbm: A highly efficient gradient boosting decision tree. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">Guolin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Finley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weidong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiwei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Looking beyond the surface: A challenge set for reading comprehension over multiple sentences</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Snigdha</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shyam</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="252" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Leandro von Werra, and Harm de Vries. The stack: 3 TB of permissively licensed source code</title>
		<author>
			<persName><forename type="first">Denis</forename><surname>Kocetkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loubna</forename><surname>Ben Allal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenghao</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Muñoz Ferrandis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2211.15533</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2211.15533" />
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Understanding black-box predictions via influence functions</title>
		<author>
			<persName><forename type="first">Pang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koh</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1885" to="1894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Race: Large-scale reading comprehension dataset from examinations</title>
		<author>
			<persName><forename type="first">Guokun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04683</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Zhenghao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhibin</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeyun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruochen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujiu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.07965</idno>
		<title level="m">Rho-1: Not all tokens are what you need</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Logiqa: A challenge dataset for machine reading comprehension with logical reasoning</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanmeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dandan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yile</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08124</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">What makes good data for alignment? a comprehensive study of automatic data selection in instruction tuning</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keqing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Paloma: A benchmark for evaluating language model fit</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Magnusson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshita</forename><surname>Bhagia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Soldaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananya</forename><surname>Harsh Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><forename type="middle">Pete</forename><surname>Walsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanai</forename><surname>Elazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Groeneveld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2312.10523</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2312.10523" />
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">When less is more: Investigating data pruning for pretraining llms at scale</title>
		<author>
			<persName><forename type="first">Max</forename><surname>Marion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmet</forename><surname>Üstün</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luiza</forename><surname>Pozzobon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marzieh</forename><surname>Fadaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Hooker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Openelm: An efficient language model family with open training and inference framework</title>
		<author>
			<persName><forename type="first">Sachin</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Hossein Sekhavat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingqing</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Horton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanzi</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenfan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iman</forename><surname>Mirzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahyar</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Belenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Zatloukal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2404.14619</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2404.14619" />
		<imprint>
			<biblScope unit="page">2024</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Smaller language models are capable of selecting instruction-tuning training data for larger language models</title>
		<author>
			<persName><forename type="first">Dheeraj</forename><surname>Mekala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.10430</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<title level="m">Pointer sentinel mixture models</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.02789</idno>
		<title level="m">Can a suit of armor conduct electricity? a new dataset for open book question answering</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Prioritized training on points that are learnable, worth learning, and not yet learnt</title>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">M</forename><surname>Sören Mindermann</surname></persName>
		</author>
		<author>
			<persName><surname>Brauner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mrinank</forename><surname>Muhammed T Razzak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Winnie</forename><surname>Kirsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benedikt</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Höltgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrien</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Morisot</surname></persName>
		</author>
		<author>
			<persName><surname>Farquhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="15630" to="15649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Aleksandra Piktus, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained language models</title>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boaz</forename><surname>Barak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nouamane</forename><surname>Tazi</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=j5BuTrEj35" />
	</analytic>
	<monogr>
		<title level="m">Thirty-seventh Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">gzip predicts data-dependent scaling laws</title>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Pandey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.16684</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Denis</forename><surname>Paperno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ngoc</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raffaella</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandro</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Pezzelle</surname></persName>
		</author>
		<author>
			<persName><surname>Baroni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.06031</idno>
		<title level="m">Gemma Boleda, and Raquel Fernández. The lambada dataset: Word prediction requiring a broad discourse context</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">The fineweb datasets: Decanting the web for the finest text data at scale</title>
		<author>
			<persName><forename type="first">Guilherme</forename><surname>Penedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hynek</forename><surname>Kydlíček</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loubna</forename><surname>Ben Allal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Lozhkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leandro</forename><surname>Von Werra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2406.17557" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">D-cpt law: Domain-specific continual pre-training scaling law for large language models</title>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Que</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ge</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenchen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingwei</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinghao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feiyu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqi</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiakai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbo</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiamang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jack</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Francis</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Aslanides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Ring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susannah</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Hennigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albin</forename><surname>Cassirer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maribeth</forename><surname>Rauh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amelia</forename><surname>Glaese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumanth</forename><surname>Dathathri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saffron</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Mellor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonia</forename><surname>Creswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nat</forename><surname>Mcaleese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esme</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michela</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Paganini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lena</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorraine</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adhiguna</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aida</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Nematzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Domenic</forename><surname>Gribovskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angeliki</forename><surname>Donato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Lespiau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolai</forename><surname>Tsimpoukelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Grigorev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibault</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mantas</forename><surname>Sottiaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toby</forename><surname>Pajarskas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Pohlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cyprien</forename><surname>Toyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>De Masson D'autume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tayfun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Terzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Mikulik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><surname>De Las</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelia</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blake</forename><forename type="middle">A</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iason</forename><surname>Weidinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Isaac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Rimell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kareem</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Ayoub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorrayne</forename><surname>Stanway</surname></persName>
		</author>
		<author>
			<persName><surname>Bennett</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2112.11446" />
		<imprint/>
	</monogr>
	<note>Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scaling language models: Methods, analysis &amp; insights from training gopher. CoRR, abs/2112.11446, 2021</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Noveen</forename><surname>Sachdeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Coleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang-Cheng</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmo</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Caverlee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><forename type="middle">Zhiyuan</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.09668</idno>
		<title level="m">How to train data-efficient llms</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">Jaehyung</forename><surname>Ritik Sachin Parkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Inn</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyeop</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><surname>Selectllm</surname></persName>
		</author>
		<title level="m">Can llms select important instructions to annotate? arXiv e-prints</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">2401</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Winogrande: An adversarial winograd schema challenge at scale</title>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="99" to="106" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">SocialIQA: Commonsense reasoning about social interactions</title>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Lebras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Superglue: Learning feature matching with graph neural networks</title>
		<author>
			<persName><forename type="first">Paul-Edouard</forename><surname>Sarlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomasz</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4938" to="4947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Balanced data sampling for language model training with clustering</title>
		<author>
			<persName><forename type="first">Yunfan</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoye</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.14526</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Text quality-based pruning for efficient training of language models</title>
		<author>
			<persName><forename type="first">Vasu</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Padthe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Newsha</forename><surname>Ardalani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kushal</forename><surname>Tirumala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><surname>Howes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Po-Yao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shang-Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gargi</forename><surname>Ghosh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.01582</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">Luca</forename><surname>Soldaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodney</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshita</forename><surname>Bhagia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><surname>Authur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Bogin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khyathi</forename><surname>Chandu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Dumas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanai</forename><surname>Elazar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.00159</idno>
		<title level="m">An open corpus of three trillion tokens for language model pretraining research</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">CommonsenseQA: A question answering challenge targeting commonsense knowledge</title>
		<author>
			<persName><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Lourie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1421</idno>
		<ptr target="https://aclanthology.org/N19-1421" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<editor>
			<persName><forename type="first">Jill</forename><surname>Burstein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Christy</forename><surname>Doran</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thamar</forename><surname>Solorio</surname></persName>
		</editor>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06">June 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4149" to="4158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">Megh</forename><surname>Thakkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tolga</forename><surname>Bolukbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sriram</forename><surname>Ganapathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shikhar</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarath</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.00913</idno>
		<title level="m">Self-influence guided data reweighting for language model pre-training</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Llama 2: Open foundation and fine-tuned chat models</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasmine</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prajjwal</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Bikel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Blecher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristian</forename><surname>Canton-Ferrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Esiobu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jude</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyin</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Fuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedanuj</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Hartshorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saghar</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hakan</forename><surname>Inan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Kardas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktor</forename><surname>Kerkez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabel</forename><surname>Kloumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Artem</forename><surname>Korenev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Punit</forename><surname>Singh Koura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenya</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Liskovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinghai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuning</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushkar</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Molybog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Poulton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Reizenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rashi</forename><surname>Rungta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalyan</forename><surname>Saladi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Schelten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruan</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">Michael</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ranjan</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellen</forename><surname>Xiaoqing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binh</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adina</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Puxin</forename><surname>Xiang Kuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iliyan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Zarov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Kambadur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurélien</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Stojnic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><surname>Scialom</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2307.09288</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2307.09288" />
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07461</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Crowdsourcing multiple choice science questions</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nelson</forename><forename type="middle">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06209</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Qurating: Selecting high-quality data for training language models</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Wettig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aatmik</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saumya</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<author>
			<persName><forename type="first">Mengzhou</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Victoria Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramakanth</forename><surname>Pasunuru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ves</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.09803</idno>
		<title level="m">Training trajectories of language models across scales</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<author>
			<persName><forename type="first">Mengzhou</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadhika</forename><surname>Malladi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suchin</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.04333</idno>
		<title level="m">Less: Selecting influential data for targeted instruction tuning</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<author>
			<persName><forename type="first">Sang</forename><surname>Michael Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Doremi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.10429</idno>
		<title level="m">Optimizing data mixtures speeds up language model pretraining</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Data selection for language models via importance resampling</title>
		<author>
			<persName><forename type="first">Sang</forename><surname>Michael Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.03169</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">The fine line: Navigating large language model pretraining with down-streaming capability analysis</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhuo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyao</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinrun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingwei</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruibin</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhi</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.01204</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Smalltolarge (s2l): Scalable data selection for fine-tuning large language models by summarizing training trajectories of small models</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">N</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baharan</forename><surname>Mirzasoleiman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.07384</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Data mixing laws: Optimizing data mixtures by predicting language modeling performance</title>
		<author>
			<persName><forename type="first">Jiasheng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peiju</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianxiang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2403.16952</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2403.16952" />
		<imprint>
			<biblScope unit="page">2024</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Mates: Model-aware data selection for efficient pretraining with data influence models</title>
		<author>
			<persName><forename type="first">Zichun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Spandan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Hellaswag: Can a machine really finish your sentence</title>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.07830</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<author>
			<persName><forename type="first">Peiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangtao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianduo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.02385</idno>
		<title level="m">Tinyllama: An open-source small language model</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Autonomous data selection with language models for mathematical texts</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">C</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2024 Workshop on Navigating and Addressing Data Problems for Foundation Models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
