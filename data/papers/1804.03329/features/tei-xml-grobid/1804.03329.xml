<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Representation Tradeoffs for Hyperbolic Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2018-04-25">April 25, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Christopher</forename><surname>De Sa</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
							<email>albertgu@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Frederic</forename><surname>Sala</surname></persName>
							<email>fredsala@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Representation Tradeoffs for Hyperbolic Embeddings</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-04-25">April 25, 2018</date>
						</imprint>
					</monogr>
					<idno type="MD5">D65877C4FB21D06B480D3F12D943BB35</idno>
					<idno type="arXiv">arXiv:1804.03329v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Hyperbolic embeddings offer excellent quality with few dimensions when embedding hierarchical data structures like synonym or type hierarchies. Given a tree, we give a combinatorial construction that embeds the tree in hyperbolic space with arbitrarily low distortion without using optimization. On WordNet, our combinatorial embedding obtains a mean-average-precision of 0.989 with only two dimensions, while Nickel et al.'s recent construction obtains 0.87 using 200 dimensions. We provide upper and lower bounds that allow us to characterize the precision-dimensionality tradeoff inherent in any hyperbolic embedding. To embed general metric spaces, we propose a hyperbolic generalization of multidimensional scaling (h-MDS). We show how to perform exact recovery of hyperbolic points from distances, provide a perturbation analysis, and give a recovery result that allows us to reduce dimensionality. The h-MDS approach offers consistently low distortion even with few dimensions across several datasets. Finally, we extract lessons from the algorithms and theory above to design a PyTorch-based implementation that can handle incomplete information and is scalable.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, hyperbolic embeddings have been proposed as a way to capture hierarchy information for use in link prediction and natural language processing tasks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b27">28]</ref>. These approaches are an exciting new way to fuse rich structural information (for example, from knowledge graphs or synonym hierarchies) with the continuous representations favored by modern machine learning.</p><p>To understand the intuition behind hyperbolic embeddings' superior capacity, note that trees can be embedded with arbitrarily low distortion into the Poincaré disk, a model of hyperbolic space with only two dimensions <ref type="bibr" target="#b31">[32]</ref>. In contrast, Bourgain's theorem <ref type="bibr" target="#b26">[27]</ref> shows that Euclidean space is unable to obtain comparably low distortion for trees-even using an unbounded number of dimensions. Moreover, hyperbolic space can preserve certain properties; for example, angles between embedded vectors are the same in both Euclidean space and the Poincaré model (the mapping is conformal), which suggests embedded data may be easily able to integrate with downstream tasks.</p><p>Many graphs, such as complex networks <ref type="bibr" target="#b23">[24]</ref>, including the Internet <ref type="bibr" target="#b22">[23]</ref> and social networks <ref type="bibr" target="#b35">[36]</ref>) are known to have hyperbolic structure and thus befit hyperbolic embeddings. Recent works show that hyperbolic representations are indeed suitable for many hierarchies (e.g, the question answering system HyperQA proposed in <ref type="bibr" target="#b34">[35]</ref>, vertex classifiers in <ref type="bibr" target="#b4">[5]</ref>, and link prediction <ref type="bibr" target="#b27">[28]</ref>). However, the optimization problem underlying these embeddings is challenging, and we seek to understand the subtle tradeoffs involved.</p><p>We begin by considering the situation in which we are given an input graph that is a tree or nearly tree-like, and our goal is to produce a low-dimensional hyperbolic embedding that preserves all distances. This leads to a simple strategy that is combinatorial in that it does not minimize a surrogate loss function using gradient descent. It is both fast (nearly linear time) and has formal quality guarantees. The approach proceeds in two phases: <ref type="bibr" target="#b0">(1)</ref> we produce an embedding of a graph into a weighted tree, and (2) we embed that tree into the hyperbolic disk. In particular, we consider an extension of an elegant embedding of trees into the Poincaré disk by Sarkar <ref type="bibr" target="#b31">[32]</ref> and recent work on low-distortion graph embeddings into tree metrics <ref type="bibr" target="#b17">[18]</ref>. For trees, this approach has nearly perfect quality. On the WordNet hypernym graph reconstruction, this obtains nearly perfect mean average precision (MAP) 0.989 using just two dimensions, which outperforms the best published numbers in Nickel and Kiela <ref type="bibr" target="#b27">[28]</ref> by almost 0.12 points with 200 dimensions.</p><p>We analyze this construction to extract fundamental tradeoffs. One tradeoff involves the dimension, the properties of the graph, and the number of bits of precision -an important hidden cost. For example, on the WordNet graph, we require almost 500 bits of precision to store values from the combinatorial embedding. We can reduce this number to 32 bits, but at the cost of using 10 dimensions instead of two. We show that for a fixed precision, the dimension required scales linearly with the length of the longest path. On the other hand, the dimension scales logarithmically with the maximum degree of the tree. This suggests that hyperbolic embeddings should have high quality on hierarchies like WordNet but require large dimensions or high precision on graphs with long chains-which is supported by our experiments. A second observation is that in contrast to Euclidean embeddings, hyperbolic embeddings are not scale invariant. This motivates us to add a learnable scale term into a stochastic gradient descent-based Pytorch algorithm described below, and we show that it allows us to empirically improve the quality of embeddings.</p><p>To understand how hyperbolic embeddings perform for metrics that are far from tree-like, we consider a more general problem: given a matrix of distances that arise from points that are embeddable in hyperbolic space of dimension d (not necessarily from a graph), find a set of points that produces these distances. In Euclidean space, the problem is known as multidimensional scaling (MDS) which is solvable using PCA. <ref type="foot" target="#foot_0">1</ref> A key step is a transformation that effectively centers the points-without knowledge of their exact coordinates. It is not obvious how to center points in hyperbolic space, which is curved. We show that in hyperbolic space, a centering operation is still possible with respect to a non-standard mean. In turn, this allows us to reduce the hyperbolic MDS problem (h-MDS) to a standard eigenvalue problem, and so it can be solved with scalable power methods. Further, we extend classical perturbation analysis <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>. When applied to distances from real data, h-MDS obtains low distortion on graphs that are far from tree like. However, we observe that these solutions may require high precision, which is not surprising in light of our previous analysis.</p><p>Finally, we consider handling increasing amounts of noise in the model, which leads naturally into new SGD-based formulations. In traditional PCA, one may discard eigenvectors that have correspondingly small eigenvalues to cope with noise. In hyperbolic space, this approach may produce suboptimal results. Like PCA, the underlying problem is nonconvex. In contrast to PCA, the optimization problem is more challenging: the underlying problem has local minima that are not global minima. Our main technical result is that an SGD-based algorithm initialized with a h-MDS solution can recover the submanifold the data is on-even in some cases in which the data is perturbed by noise that can be full dimensional. Our algorithm essentially provides new recovery results for convergence for Principal Geodesic Analysis (PGA) in hyperbolic space <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17]</ref>. We discuss the nuances between our optimization algorithms and previous attempts at these problems in Appendix B.</p><p>All of our results can handle incomplete distance information through standard techniques. Using the observations above, we implemented an SGD algorithm that minimizes the loss derived from the PGA loss using PyTorch. <ref type="foot" target="#foot_1">2</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>We provide intuition connecting hyperbolic space and tree distances, discuss the metrics used to measure embedding fidelity, and provide the relationship between reconstruction and learning for graph embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyperbolic spaces</head><p>The Poincaré disk H 2 is a two-dimensional model of hyperbolic geometry with points located in the interior of the unit disk, as shown in Figure <ref type="figure" target="#fig_1">1</ref>. A natural generalization of H 2 is the Poincaré ball H r , with elements inside the unit ball. The Poincaré models offer several useful properties, chief among which is mapping conformally to Euclidean space. That is, angles are preserved between hyperbolic and Euclidean space. Distances, on the other hand,  are not preserved, but are given by</p><formula xml:id="formula_0">d H (x, y) = acosh 1 + 2 x -y 2 (1 -x 2 )(1 -y 2 )</formula><p>.</p><p>There are some potentially unexpected consequences of this formula, and a simple example gives intuition about a key technical property that allows hyperbolic space to embed trees. Consider three points: the origin 0, and points x and y with x = y = t for some t &gt; 0. As shown on the right of Figure <ref type="figure" target="#fig_1">1</ref>, as t → 1 (i.e., the points move towards the outside of the disk), in flat Euclidean space, the ratio</p><formula xml:id="formula_1">d E (x,y) d E (x,0</formula><p>)+d E (0,y) is constant with respect to t (blue curve). In contrast, the ratio</p><formula xml:id="formula_2">d H (x,y) d H (x,0</formula><p>)+d H (0,y) approaches 1, or, equivalently, the distance d H (x, y) approaches d H (x, 0) + d H (0, y) (red and pink curves). That is, the shortest path between x and y is almost the same as the path through the origin. This is analogous to the property of trees in which the shortest path between two sibling nodes is the path through their parent. This tree-like nature of hyperbolic space is the key property exploited by embeddings. Moreover, this property holds for arbitrarily small angles between x and y.</p><p>Lines and geodesics There are two types of geodesics (shortest paths) in the Poincaré disk model of hyperbolic space: segments of circles that are orthogonal to the disk surface, and disk diameters <ref type="bibr" target="#b2">[3]</ref>. Our algorithms and proofs make use of a simple geometric fact: isometric reflection across geodesics (preserving hyperbolic distances) is represented in this Euclidean model as a circle inversion. A particularly important reflection associated with each point x is the one that takes x to the origin <ref type="bibr">[3, p. 268</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Embeddings and fidelity measures An embedding is a mapping</head><formula xml:id="formula_3">f : U → V for spaces U, V with distances d U , d V .</formula><p>We measure the quality of embeddings with several fidelity measures, presented here from most local to most global.</p><p>Recent work <ref type="bibr" target="#b27">[28]</ref> proposes using the mean average precision (MAP). For a graph</p><formula xml:id="formula_4">G = (V, E), let a ∈ V have neighborhood N a = {b 1 , b 2 , . . . , b deg(a) },</formula><p>where deg(a) denotes the degree of a. In the embedding f , consider the points closest to f (a), and define R a,bi to be the smallest set of such points that contains b i (that is, R a,bi is the smallest set of nearest points required to retrieve the ith neighbor of a in f ). Then, the MAP is defined to be</p><formula xml:id="formula_5">MAP(f ) = 1 |V | a∈V 1 |N a | |Na| i=1 Precision(R a,bi ) = 1 |V | a∈V 1 deg(a) |Na| i=1 |N a ∩ R a,bi | |R a,bi | .</formula><p>We have MAP(f ) ≤ 1, with equality as the best case. Note that MAP is not concerned with the underlying distances at all, but only the ranks between the distances of immediate neighbors. It is a local metric.</p><p>The standard metric for graph embeddings is distortion D. For an n point embedding,</p><formula xml:id="formula_6">D(f ) = 1 n 2   u,v∈U :u =v |d V (f (u), f (v)) -d U (u, v)| d U (u, v)   .</formula><p>The best distortion is D(f ) = 0, preserving the edge lengths exactly. This is a global metric, as it depends directly on the underlying distances rather than the local relationships between distances. A variant of this, the worst-case distortion D wc , is the metric defined by</p><formula xml:id="formula_7">D wc (f ) = max u,v∈U :u =v d V (f (u), f (v))/d U (u, v) min u,v∈U :u =v d V (f (u), f (v))/d U (u, v) .</formula><p>That is, the wost-case distortion is the ratio of the maximal expansion and the minimal contraction of distances. Note that scaling the unit distance does not affect D wc . The best worst-case distortion is D wc (f ) = 1.</p><p>The intended application of the embedding informs the choice of metric. For applications where the underlying distances are important, distortion is useful. On the other hand, if only rankings matter, MAP may suffice. This choice is important: as we shall see, different embedding algorithms implicitly target different metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reconstruction and learning</head><p>In the case where we lack a full set of distances, we can deal with the missing data in one of two ways. First, we can use the triangle inequality to recover the missing distances. Second, we can access the scaled Euclidean distances (the inside of the acosh in d H (x, y)), and then the resulting matrix can be recovered with standard matrix completion techniques <ref type="bibr" target="#b3">[4]</ref>. Afterwards, we can proceed to compute an embedding using any of the approaches discussed in this paper. We quantify the error introduced by this process experimentally in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Combinatorial Constructions</head><p>We first focus on hyperbolic tree embeddings-a natural approach considering the tree-like behavior of hyperbolic space. We review the embedding of Sarkar <ref type="bibr" target="#b31">[32]</ref> to higher dimensions. We then provide novel analysis about the precision of the embeddings that reveals fundamental limits of hyperbolic embeddings. In particular, we characterize the bits of precision needed for hyperbolic representations. We then extend the construction to r dimensions, and we propose to use Steiner nodes to better embed general graphs as trees building on a condition from I. Abraham et al. <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Embedding trees</head><p>The nature of hyperbolic space lends itself towards excellent tree embeddings. In fact, it is possible to embed trees into the Poincaré disk H 2 with arbitrarily low distortion <ref type="bibr" target="#b31">[32]</ref>. Remarkably, trees cannot be embedded into Euclidean space with arbitrarily low distortion for any number of dimensions. These notions motivate the following two-step process for embedding hierarchies into hyperbolic space.</p><p>1. Embed the graph G = (V, E) into a tree T , 2. Embed T into the Poincaré ball H d .</p><p>We refer to this process as the combinatorial construction. Note that we are not required to minimize a loss function. We begin by describing the second stage, where we extend an elegant construction from Sarkar <ref type="bibr" target="#b31">[32]</ref>. First, f (a) and f (b) are reflected across a geodesic (using circle inversion) so that f (a) is mapped onto the origin 0 and f (b) is mapped onto some point z. Next, we place the children nodes to vectors y 1 , . . . , y d-1 equally spaced around a circle with radius e τ -1 e τ +1 (which is a circle of radius τ in the hyperbolic metric), and maximally separated from the reflected parent node embedding z. Lastly, we reflect all of the points back across the geodesic. Note that the isometric properties of reflections imply that all children are now at hyperbolic distance exactly τ from f (a). To embed the entire tree, we place the root at the origin O and its children in a circle around it (as in Step 5 of Algorithm 1), then recursively place their children until all nodes have been placed. Notice this construction runs in linear time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sarkar's Construction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Analyzing Sarkar's Construction</head><p>The Voronoi cell around a node a ∈ T consists of points</p><formula xml:id="formula_8">x ∈ H 2 such that d H (f (a), x) ≤ d H (f (b), x) for all b ∈ T distinct from a.</formula><p>That is, the cell around a includes all points closer to f (a) than to any other embedded node of the tree. Sarkar's construction produces Delauney embeddings: embeddings where the Voronoi cells for points a and b touch only if a and b are neighbors in T . Thus this embedding will preserve neighborhoods.</p><p>A key technical idea exploited by Sarkar <ref type="bibr" target="#b31">[32]</ref> is to scale all the edges by a factor τ before embedding. We can then recover the original distances by dividing by τ . This transformation exploits the fact that hyperbolic space is not scale invariant. Sarkar's construction always captures neighbors perfectly, but Figure <ref type="figure" target="#fig_1">1</ref> implies that increasing the scale preserves the distances between farther nodes better. Indeed, if one sets</p><formula xml:id="formula_9">τ = 1+ε ε 2 log deg max π/2</formula><p>, then the worst-case distortion D of the resulting embedding is no more than 1 + ε. For trees, Sarkar's construction has arbitrarily high fidelity. However, this comes at a cost: the scaling τ affects the bits of precision required. In fact, we will show that the precision scales logarithmically with the degree of the tree-but linearly with the maximum path length. We use this to better understand the situations in which hyperbolic embeddings obtain high quality.</p><p>How many bits of precision do we need to represent points in H 2 ? If x ∈ H 2 , then x &lt; 1, so we need sufficiently many bits so that 1 -x will not be rounded to zero. This requires roughly -log(1 -x ) = log 1  1-x bits. Say we are embedding two points x, y at distance d. As described in the background, there is an isometric reflection that takes a pair of points (x, y) in H 2 to (0, z) while preserving their distance, so without loss of generality we have that</p><formula xml:id="formula_10">d = d H (x, y) = d H (0, z) = acosh 1 + 2 z 2 1 -z 2 .</formula><p>Rearranging the terms, we have</p><formula xml:id="formula_11">cosh(d) + 1 2 = 1 1 -z 2 ≥ 1/2 1 -z .</formula><p>Thus, the number of bits we want so that 1 -z will not be rounded to zero is log(cosh(d) + 1). Since cosh(d) = (exp(d) + exp(-d))/2, this is roughly d bits. That is, in hyperbolic space, we need about d bits to express distances of d (rather than log d as we would in Euclidean space). <ref type="foot" target="#foot_2">3</ref> This result will be of use below.</p><p>Now we consider the largest distance in the embeddings produced by Algorithm 1. If the longest path in the tree is , and each edge has length τ = 1 ε 2 log</p><formula xml:id="formula_12">deg max π/2</formula><p>, the largest distance is O( ε log deg max ), and we require this number of bits for the representation. We interpret this expression. Note that deg max is inside the log term, so that a bushy tree is not penalized much in precision. On the other hand, the longest path length is not, so that hyperbolic embeddings struggle with long paths. Moreover, by selecting an explicit graph, we derive a matching lower bound, concluding that to achieve a distortion ε, any construction requires Ω ε log(deg max ) bits, which matches the upper bound of the combinatorial construction. The argument follows from selecting a graph consisting of m(deg max + 1) nodes in a tree with a single root and deg max chains each of length m. The proof of this result is described in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Improving the Construction</head><p>Our next contribution is a generalization of the construction from the disk H 2 to the ball H r . Our construction follows the same line as Algorithm 1, but since we have r dimensions, the step where we place children spaced out on a circle around their parent now uses a hypersphere.</p><p>Spacing out points on the hypersphere is a classic problem known as spherical coding <ref type="bibr" target="#b6">[7]</ref>. As we shall see, the number of children that we can place for a particular angle grows with the dimension. Since the required scaling factor τ gets larger as the angle decreases, we can reduce τ for a particular embedding by increasing the dimension. Note that increasing the dimension helps with bushy trees (large deg max ), but has limited effect on tall trees with small deg max .</p><p>We show Proposition 3.1. The generalized H r combinatorial construction has distortion at most 1 + ε and requires at most O( 1 ε r log deg max ) bits to represent a node component for r ≤ (log deg max )+1, and O( 1 ε ) bits for r &gt; (log deg max )+ 1.</p><p>The algorithm for the generalized H r combinatorial construction replaces Step 5 in Algorithm 1 with a node placement step based on ideas from coding theory. The children are placed at the vertices of a hypercube inscribed into the unit hypersphere (and afterwards scaled by τ ). Each component of a hypercube vertex has the form ±1 √ r . We index these points using binary sequences a ∈ {0, 1} r in the following way:</p><formula xml:id="formula_13">x a = (-1) a1 √ r , (-1) a2 √ r , . . . , (-1) ar √ r .</formula><p>We can space out the children by controlling the distances between the children. This is done in turn by selecting a set of binary sequences with a prescribed minimum Hamming distance-a binary error-correcting code-and placing the children at the resulting hypercube vertices. We provide more details on this technique and our choice of code in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Embedding into Trees</head><p>We revisit the first step of the construction: embedding graphs into trees. There are fundamental limits to how well graphs can be embedded into trees; in general, breaking long cycles inevitably adds distortion, as shown in Figure <ref type="figure" target="#fig_2">2</ref>.</p><p>We are inspired by a measure of this limit, the δ-4 points condition introduced in I. Abraham et al. <ref type="bibr" target="#b17">[18]</ref>. A graph on n nodes that satisfies the δ-4 points condition has distortion at most (1 + δ) c1 log n for some constant c 1 . This result enables our end-to-end embedding to achieve a distortion of at most</p><formula xml:id="formula_14">D(f ) ≤ (1 + δ) c1 log n (1 + ε).</formula><p>The result in I. Abraham et al. <ref type="bibr" target="#b17">[18]</ref> builds a tree with Steiner nodes. These additional nodes can help control the distances in the resulting tree. Example 3.1. Embed a complete graph on {1, 2, . . . , n} into a tree. The tree will have a central node, say 1, w.l.o.g., connected to every other node; the shortest paths between pairs of nodes in {2, . . . , n} go from distance 1 in the graph to distance 2 in the tree. However, we can introduce a Steiner node n + 1 and connect it to all of the nodes, with edge weights of 1 2 . This is shown in Figure <ref type="figure" target="#fig_2">2</ref>. The distance between any pair of nodes in {1, . . . , n} remains 1. Note that introducing Steiner nodes can produce a weighted tree, but Algorithm 1 readily extends to the case of weighted trees by modifying Step 5. We propose using the Steiner tree algorithm in I. Abraham et al. <ref type="bibr" target="#b17">[18]</ref> (used to achieve the distortion bound) for real embeddings, and we rely on it for our experiments in Section 5. In summary, the key takeaways of our analysis in this section are:</p><p>• There is a fundamental tension between precision and quality in hyperbolic embeddings.</p><p>• Hyperbolic embeddings have an exponential advantage in space compared to Euclidean embeddings for short, bushy hierarchies, but will have less of an advantage for graphs that contain long paths.</p><p>• Choosing an appropriate scaling factor τ is critical for quality. Later, we will propose to learn this scale factor automatically for computing embeddings in PyTorch.</p><p>• Steiner nodes can help improve embeddings of graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Hyperbolic Multidimensional Scaling</head><p>In this section, we explore a fundamental and more general question than we did in the previous section: if we are given the pairwise distances arising from a set of points in hyperbolic space, can we recover the points? The equivalent problem for Euclidean distances is solved with multidimensional scaling (MDS). The goal of this section is to analyze the hyperbolic MDS (h-MDS) problem. We describe and overcome the additional technical challenges imposed by hyperbolic distances, and show that exact recovery is possible and interpretable. Afterwards we propose a technique for dimensionality reduction using principal geodesics analysis (PGA) that provides optimization guarantees. In particular, this addresses the shortcomings of h-MDS when recovering points that do not exactly lie on a hyperbolic manifold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Exact Hyperbolic MDS</head><p>Suppose that there is a set of hyperbolic points x 1 , . . . , x n ∈ H r , embedded in the Poincaré ball and written X ∈ R n×r in matrix form. We observe all the pairwise distances d i,j = d H (x i , x j ), but do not observe X: our goal is use the observed d i,j 's to recover X (or some other set of points with the same pairwise distances d i,j ).</p><p>The MDS algorithm in the Euclidean setting makes an important centering<ref type="foot" target="#foot_3">foot_3</ref> assumption. That is it assumes the points have mean 0, and it turns out that if an exact embedding for the distances exists, it can be recovered from a matrix factorization. In other words, Euclidean MDS always recovers a centered embedding.</p><p>In hyperbolic space, the same algorithm does not work, but we show that it is possible to find an embedding centered at a different mean. More precisely, we introduce a new mean which we call the pseudo-Euclidean mean, that behaves like the Euclidean mean in that it enables recovery through matrix factorization. Once the points are recovered in hyperbolic space, they can be recentered around a more canonical mean by translating it to the origin.</p><p>Algorithm 2 is our complete algorithm, and for the remainder of this section we will describe how and why it works.</p><p>We first describe the hyperboloid model, an alternate but equivalent model of hyperbolic geometry in which h-MDS is simpler. Of course, we can easily convert between the hyperboloid model and the Poincaré ball model we have used thus far. Next, we show how to reduce the problem to a standard PCA problem, which recovers an embedding centered at the points' pseudo-Euclidean mean. Finally, we discuss the meaning and implications of centering and prove that the algorithm preserves submanifolds as well-that is, if there is an exact embedding in k &lt; r dimensions centered at their canonical mean, then our algorithm will recover them.</p><p>The hyperboloid model Define Q to be the diagonal matrix in R r+1 where Q 00 = 1 and</p><formula xml:id="formula_15">Q ii = -1 for i &gt; 0. For a vector x ∈ R r+1 , x T Qx is called the Minkowski quadratic form.</formula><p>The hyperboloid model is defined as</p><formula xml:id="formula_16">M r = x ∈ R r+1 x T Qx = 1 ∧ x 0 &gt; 0 .</formula><p>This manifold is endowed with a distance measure</p><formula xml:id="formula_17">d H (x, y) = acosh(x T Qy).</formula><p>As a notational convenience, for a point x ∈ M r we will let x 0 denote 0th coordinate e T 0 x, and let x ∈ R r denote the rest of the coordinates. Notice that x 0 is just a function of x (in fact, x 0 = 1 + x 2 ), and so we can equivalently consider just x as being a member of a model of hyperbolic space: this model is sometimes known as the Gans model. With this notation, the Minkowski quadratic form can be simplified to x T Qy = x 0 y 0 -x T y.</p><p>A new mean We introduce the new mean that we will use. Given points x 1 , x 2 , . . . , x n ∈ M r in hyperbolic space, define a variance term</p><formula xml:id="formula_18">Ψ(z; x 1 , x 2 , . . . , x n ) = n i=1 sinh 2 (d H (x i , z)).</formula><p>Using this, we define a pseudo-Euclidean mean to be any local minimum of this expression. Notice that this average is independent of the model of hyperbolic space that we are using, since it only is defined in terms of the hyperbolic distance function d H . Lemma 4.1. Define the matrix X ∈ R n×r such that X T e i = x i and the vector u ∈ R n such that u i = x 0,i . Then</p><formula xml:id="formula_19">∇ z Ψ(z; x 1 , x 2 , . . . , x n )| z=0 = -2 n i=1 x 0,i x i = -2X T u.</formula><p>This means that 0 is a pseudo-Euclidean mean if and only if 0 = X T u. Call some hyperbolic points x 1 , . . . , x n pseudo-Euclidean centered if their average is 0 in this sense: i.e. if X T u = 0. We can always center a set of points without affecting their pairwise distances by simply finding their average, and then sending it to 0 through an isometry.</p><p>Recovery via matrix factorization Suppose that there exist points x 1 , x 2 , . . . , x n ∈ M r for which we observe their pairwise distances d H (x i , x j ). From these, we can compute the matrix Y such that</p><formula xml:id="formula_20">Y i,j = cosh (d H (x i , x j )) = x T i Qx j = x 0,i x 0,j -x i T x j .<label>(1)</label></formula><p>Furthermore, defining X and u as in Lemma 4.1, then we can write Y in matrix form as</p><formula xml:id="formula_21">Y = uu T -XX T .<label>(2)</label></formula><p>Without loss of generality, we can suppose that the points we are trying to recover, x 1 , . . . , x n , are centered at their pseudo-Euclidean mean, so that X T u = 0 by Lemma 4.1.</p><p>Algorithm 2</p><p>1: Input: Distance matrix d i,j and rank r 2: Compute scaled distance matrix Y i,j = cosh(d i,j ) 3: X → PCA(-Y, r) 4: Project X from hyperboloid model to Poincaré model: x → x 1+ √ 1+ x 2 5: If desired, center X at a different mean (e.g. the Karcher mean) 6: return X</p><p>This implies that u is an eigenvector of Y with positive eigenvalue, and the rest of Y 's eigenvalues are negative. Therefore an eigendecomposition of Y will find u, X such that Y = uu T -X XT , i.e. it will directly recover X up to rotation.</p><p>In fact, running PCA on -Y = X T X -uu T to find the n most significant non-negative eigenvectors will recover X up to rotation, and then u can be found by leveraging the fact that</p><formula xml:id="formula_22">x 0 = 1 + x 2 .</formula><p>This leads to Algorithm 2, with optional post-processing steps for converting the embedding to the Poincaré ball model and for re-centering the points.</p><p>A word on centering The MDS algorithm in Euclidean geometry returns points centered at their Karcher mean z, which is a point minimizing d 2 (z, x i ) (where d is the distance metric). The Karcher center is particularly useful for interpreting dimensionality reduction; for example, we use the analogous hyperbolic Karcher mean to perform PGA in Section 4.2.</p><p>Although Algorithm 2 returns points centered at their pseudo-Euclidean mean instead of their Karcher mean, they can be easily recentered by finding their Karcher mean and reflecting it onto the origin. Furthermore, we show that Algorithm 2 preserves the dimension of the embedding. More precisely, we prove Lemma 4.2 in Appendix E. Lemma 4.2. If a set of points lie in a dimension-k geodesic submanifold, then both their Karcher mean and their pseudo-Euclidean mean lie in the same submanifold.</p><p>This implies that centering with the pseudo-Euclidean mean preserves geodesic submanifolds: If it is possible to embed distances in a dimension-k geodesic submanifold centered and rooted at a Karcher mean, then it is also possible to embed the distances in a dimension-k submanifold centered and rooted at a pseudo-Euclidean mean, and vice versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Reducing Dimensionality with PGA</head><p>Sometimes we are given a high-rank embedding (resulting from h-MDS, for example), and wish to find a lower-rank version. In Euclidean space, one can get the optimal lower rank embedding by simply discarding components. However, this may not be the case in hyperbolic space. Motivated by this, we study dimensionality reduction in hyperbolic space.</p><p>As hyperbolic space does not have a linear subspace structure like Euclidean space, we need to define what we mean by lower-dimensional. We follow Principal Geodesic Analysis <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b16">[17]</ref>. Consider an initial embedding with points x 1 , . . . , x n ∈ H 2 and let d H : H 2 × H 2 → R + be the hyperbolic distance. Suppose we want to map this embedding onto a one-dimensional subspace. (Note that we are considering a two-dimensional embedding and one-dimensional subspace here for simplicity, and these results immediately extend to higher dimensions.) In this case, the goal of PGA is to find a geodesic γ : [0, 1] → H 2 that passes through the mean of the points and that minimizes the squared error (or variance):</p><formula xml:id="formula_23">f (γ) = n i=1 min t∈[0,1] d H (γ(t), x i ) 2 .</formula><p>This expression can be simplified significantly and reduced to a minimization in Euclidean space. First, we find the mean of the points, the point x which minimizes n i=1 d H (x, x i ) 2 ; this definition in terms of distances generalizes the mean in Euclidean space. <ref type="foot" target="#foot_4">5</ref> Next, we reflect all the points x i so that their mean is 0 in the Poincaré disk model; we The PGA objective of an example task where the input dataset in the Poincaré disk is x 1 = (0.8, 0), x 2 = (-0.8, 0), x 3 = (0, 0.7) and x 4 = (0, -0.7). Note the presence of non-optimal local minima, unlike PCA. can do this using a circle inversion that maps x onto 0. In the Poincaré disk model, a geodesic through the origin is a Euclidean line, and the action of the reflection across this line is the same in both Euclidean and hyperbolic space. Coupled with the fact that reflections are isometric, if γ is a line through 0 and R γ is the reflection across γ, we have</p><formula xml:id="formula_24">d H (γ, x) = min t∈[0,1] d H (γ(t), x) = 1 2 d H (R l x, x).</formula><p>Combining this with the Euclidean reflection formula and the hyperbolic metric produces</p><formula xml:id="formula_25">f (γ) = 1 4 n i=1 acosh 2 1 + 8d E (γ, x i ) 2 (1 -x i 2 ) 2 ,</formula><p>in which d E is the Euclidean distance from a point to a line. If we define</p><formula xml:id="formula_26">w i = √ 8x i /(1 -x i 2 )</formula><p>this reduces to the simplified expression</p><formula xml:id="formula_27">f (γ) = 1 4 n i=1 acosh 2 1 + d E (γ, w i ) 2 .</formula><p>Notice that the loss function is not convex. We observe that there can be multiple local minima that are attractive and stable, in contrast to PCA. Figure <ref type="figure">3</ref> illustrates this nonconvexity on a simple dataset in H 2 with only four examples. This makes globally optimizing the objective difficult.</p><p>Nevertheless, there will always be a region Ω containing a global optimum γ * that is convex and admits an efficient projection, and where f is convex when restricted to Ω. Thus it is possible to build a gradient descent-based algorithm to recover lower-dimensional subspaces: for example, we built a simple optimizer in PyTorch. We also give a sufficient condition on the data for f above to be convex. Lemma 4.3. For hyperbolic PGA if for all i,</p><formula xml:id="formula_28">acosh 2 1 + d E (γ, w i ) 2 &lt; min 1, 1 3 w i 2 then f is locally convex at γ.</formula><p>As a result, if we initialize in and optimize over a region that contains γ * and where the condition of Lemma 4.3 holds, then gradient descent will be guaranteed to converge to γ * . We can turn this result around and read it as a recovery result: if the noise is bounded in this regime, then we are able to provably recover the correct low-dimensional embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We evaluate the proposed approaches and compare against existing methods. We hypothesize that for tree-like data, the combinatorial construction offers the best performance. For general data, we expect h-MDS to produce the lowest Table 2: MAP measure for WordNet embedding compared to values in Nickel and Kiela [28]. Closer to 1 is better.</p><p>distortion, while it may have low MAP due to precision limitations. We anticipate that dimension is a critical factor (outside of the combinatorial construction). Finally, we expect that the MAP of h-MDS techniques can be improved by learning the correct scale and weighting the loss function as suggested in earlier sections. In the Appendix, we report on additional datasets, parameters found by the combinatorial construction, and the effect of hyperparameters.</p><p>Datasets We consider trees, tree-like hierarchies, and graphs that are not tree-like. First, we consider hierarchies that form trees: fully-balanced trees along with phylogenetic trees expressing genetic heritage (of mosses growing in urban environments <ref type="bibr" target="#b15">[16]</ref>, available from <ref type="bibr" target="#b30">[31]</ref>). Similarly, we used hierarchies that are nearly tree-like: WordNet hypernym (the largest connected component from Nickel and Kiela <ref type="bibr" target="#b27">[28]</ref>) and a graph of Ph.D. advisor-advisee relationships <ref type="bibr" target="#b9">[10]</ref>. Also included are datasets that vary in their tree nearness, such as biological sets involving disease relationships <ref type="bibr" target="#b13">[14]</ref> and protein interactions in yeast bacteria <ref type="bibr" target="#b19">[20]</ref>, both available from <ref type="bibr" target="#b29">[30]</ref>. We also include the collaboration network formed by authorship relations for papers submitted to the general relativity and quantum cosmology (Gr-QC) arXiv <ref type="bibr" target="#b25">[26]</ref>.</p><p>Approaches Combinatorial embeddings into H 2 are done using Steiner trees generated from a randomly selected root for the ε = 0.1 precision setting; others are considered in the Appendix. We performed h-MDS in floating point precision. We also include results for our PyTorch implementation of an SGD-based algorithm (described later), as well as a warm start version initialized with the high-dimensional combinatorial construction. We compare against classical MDS (i.e., PCA), and the optimization-based approach Nickel and Kiela <ref type="bibr" target="#b27">[28]</ref>, which we call FB. The experiments for h-MDS, PyTorch SGD, PCA, and FB used dimensions of 2,5,10,50,100,200; we recorded the best resulting MAP and distortion. Due to the large scale, we did not replicate the best FB numbers on large graphs (i.e., Gr-QC and WordNet).</p><p>As a result, we report their best published MAP numbers for comparison (their work does not report distortion). These  Table 4: MAP measures using combinatorial and h-MDS techniques, compared against PCA. Closer to 1 is better.</p><p>entries are marked with an asterisk. For the WordNet graph we use a random BFS tree rather than a Steiner tree. Moreover, for comparison against FB (which computes the transitive closure), we can use a weighted version of the graph that captures the ancestor relationships. The full details are in Appendix.</p><p>Quality In Table <ref type="table" target="#tab_4">3</ref>, we report the distortion. As expected, when the graph is a tree or tree-like the combinatorial construction has exceedingly low distortion. Because h-MDS is meant to recover points exactly, we hypothesized that h-MDS would offer very low distortion on these datasets. We confirm this hypothesis: among h-MDS, PCA, and FB, h-MDS consistently offers the best (lowest) distortion, producing, for example, a distortion of 0.039 on the phylogenetic tree dataset. We observe that the optimization-based approach works quite well for reducing distortion, and on tree-like datasets it is bolstered by appropriate initialization from the combinatorial construction.</p><p>Table <ref type="table">4</ref> reports the MAP measure (this is shown in Table <ref type="table">2</ref> for WordNet), which is a local measure. We expect that the combinatorial construction performs well for tree-like hierarchies. This is indeed the case: on trees and tree-like graphs, the MAP is close to 1, improving on approaches such as FB that rely on optimization. On larger graphs like WordNet, our approach yields a MAP of 0.989-improving on the FB MAP result of 0.870 at 200 dimensions. This is exciting because the combinatorial approach is deterministic and linear-time. In addition, it suggests that this refined understanding of hyperbolic embeddings may be used to improve the quality and runtime state of the art constructions.</p><p>As expected, the MAP of the combinatorial construction decreases as the graphs are less tree-like. Interestingly, h-MDS solved in floating point indeed struggles with MAP. We separately confirmed that it is indeed due to precision using a high-precision solver, which obtains a perfect MAP-but uses 512 bits of precision. It may be possible to compensate for this with scaling, but we did not explore this possibility.</p><p>Rank No Scale Learned Scale Exp. Weighting 50 0.481 0.508 0.775 100 0.688 0.681 0.882 200 0.894 0.907 0.963 Table 5: Ph.D. dataset. Improved MAP performance of PyTorch implementation using a modified PGA-like loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SGD-Based Algorithm</head><p>We also built an SGD-based algorithm implemented in PyTorch. Here the loss function is equivalent to the PGA loss, and so is continuously differentiable. We use this to verify two claims:</p><p>Learned Scale. In Table <ref type="table">5</ref>, we verify the importance of scaling that our analysis suggests; our implementation has a simple learned scale parameter. Moreover, we added an exponential weighting to the distances in order to penalize long paths, thus improving the local reconstruction. These techniques indeed improve the MAP; in particular, the learned scale provides a better MAP at lower rank. We hope these techniques can be useful in other embedding techniques.</p><p>Incomplete Information. To evaluate our algorithm's ability to deal with incomplete information, we examine the quality of recovered solutions as we sample the distance matrix. We set the sampling rate of non-edges to edges at 10 : 1 following Nickel and Kiela <ref type="bibr" target="#b27">[28]</ref>. We examine the phylogenetic tree, which is full rank in Euclidean space. In Figure <ref type="figure" target="#fig_6">4</ref>, we are able to recover a good solution with a small fraction of the entries for the phylogenetic tree dataset; for example, we sampled approximately 4% of the graph but provide a MAP of 0.74 and distortion of less than 0.6.</p><p>Understanding the sample complexity for this problem is an interesting theoretical question. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Glossary of Symbols</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Related Work</head><p>Our study of representation tradeoffs for hyperbolic embeddings was motivated by exciting recent approaches towards such embeddings in Nickel and Kiela <ref type="bibr" target="#b27">[28]</ref> and Chamberlain et al. <ref type="bibr" target="#b4">[5]</ref>. Earlier efforts proposed using hyperbolic spaces for routing, starting with Kleinberg's work on geographic routing <ref type="bibr" target="#b21">[22]</ref>. Cvetkovski and Crovella <ref type="bibr" target="#b7">[8]</ref> performed hyperbolic embeddings and routing for dynamic networks. Recognizing that the use of hyperbolic space for routing required a large number of bits to store the vertex coordinates, Eppstein and Goodrich <ref type="bibr" target="#b10">[11]</ref> introduced a scheme for succinct embedding and routing in the hyperbolic plane. Another very recent effort also proposes using hyperbolic cones (similar to the cones that are the fundamental building block used in Sarkar <ref type="bibr" target="#b31">[32]</ref> and our work) as a heuristic for embedding entailment relations, i.e. directed acyclic graphs <ref type="bibr" target="#b12">[13]</ref>. The authors also propose to optimize on the hyperbolic manifold using its exponential map, as opposed to our approach of finding a closed form for the embedding should it exist (Section 4). An interesting avenue for future work is to compare both optimization methods empirically and theoretically, i.e., to understand the types of recovery guarantees under noise that such methods have.</p><p>There have been previous efforts to perform multidimensional scaling in hyperbolic space (the h-MDS problem), often in the context of visualization <ref type="bibr" target="#b24">[25]</ref>. Most propose descent methods in hyperbolic space (e.g. <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b36">[37]</ref>) and fundamentally differ from ours. Arguably the most relevant is Wilson et al. <ref type="bibr" target="#b37">[38]</ref>, which mentions exact recovery as an intermediate result, but ultimately suggests a heuristic optimization. Our h-MDS analysis characterizes the recovered embedding and manifold and obtains the correctly centered one-a key issue in MDS. For example, this allows us to properly find the components of maximal variation. Furthermore, we discuss robustness to noise and produce optimization guarantees when a perfect embedding doesn't exist.</p><p>Several papers have studied the notion of hyperbolicity of networks, starting with the seminal work on hyperbolic graphs Gromov <ref type="bibr" target="#b14">[15]</ref>. More recently, Chen et al. <ref type="bibr" target="#b5">[6]</ref> considered the hyperbolicity of small world graphs and tree-like random graphs. Abu-Ata and Dragan <ref type="bibr" target="#b0">[1]</ref> performed a survey that examines how well real-world networks can be approximated by trees using a variety of tree measures and tree embedding algorithms. To motivate their study of tree metrics, I. Abraham et al. <ref type="bibr" target="#b17">[18]</ref> computed a measure of tree likeness on a Internet infrastructure network.</p><p>We use matrix completion (closure) to perform embeddings with incomplete data. Matrix completion is a celebrated problem. Candes and Tao <ref type="bibr" target="#b3">[4]</ref> derive bounds on the minimum number of entries needed for completion for a fixed rank matrix; they also introduce a convex program for matrix completion operating at near the optimal rate.</p><p>Principal geodesic analysis (PGA) generalizes principal components analysis (PCA) for the manifold setting. It was introduced and applied to shape analysis in <ref type="bibr" target="#b11">[12]</ref> and extended to a probabilistic setting in <ref type="bibr" target="#b38">[39]</ref>. There are other variants; the geodesic principal components analysis (GPCA) of Huckemann et al. <ref type="bibr" target="#b16">[17]</ref> uses our loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Low-Level Formulation Details</head><p>We plan to release our PyTorch code, high precision solver, and other routines on Github. A few comments are helpful to understand the reformulation. In particular, we simply minimize the squared hyperbolic distance with a learned scale parameter, τ , e.g., :</p><formula xml:id="formula_29">min x1,...,xn,τ 1≤i&lt;j≤n (τ d H (x i , x j ) -d i,j )<label>2</label></formula><p>We typically require that τ ≥ 0.1.</p><p>• On continuity of the derivative of the loss: Note that</p><formula xml:id="formula_30">∂ x acosh(1 + x) = 1 (1 + x) 2 -1 = 1 x(x + 2) hence lim x→0 ∂ x acosh(1 + x) = ∞.</formula><p>Thus, lim y→x ∂ x d H (x, y) = ∞. In particular, if two points happen to get near to one another during execution, gradient-based optimization becomes unstable. Note that exp{acosh(1 + x)} suffers from a similar issue, and is used in both <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b27">28]</ref>. This change may increase numerical instability, and the public code for these approaches does indeed take steps like masking out updates to mitigate NANs. In contrast, the following may be more stable:</p><formula xml:id="formula_31">∂ x acosh(1 + x) 2 = 2 acosh(1 + x) x(x + 2)</formula><p>and in particular lim</p><formula xml:id="formula_32">x→0 ∂ x acosh(1 + x) 2 = 2</formula><p>The limits follows by simply applying L'Hopital's rule. In turn, this implies the square formulation is continuously differentiable. Note that it is not convex.</p><p>• One challenge is to make sure the gradient computed by PyTorch has the appropriate curvature correction (the Riemannian metric), as is well explained by Nickel and Kiela <ref type="bibr" target="#b27">[28]</ref>. The modification is straightforward: we create a subclass of NN.PARAMETER called HYPERBOLIC PARAMETER. This wrapper class allows us to walk the tree to apply the appropriate correction to the metric (which amounts to multiplying ∇ w f (w) by 1  4 (1 -w 2 ) 2 . After calling the BACKWARD function, we call a routine to walk the autodiff tree to find such parameters and correct them. This allows HYPERBOLIC PARAMETER and traditional parameters to be freely mixed.</p><p>• We project back on the hypercube following Nickel and Kiela <ref type="bibr" target="#b27">[28]</ref> and use gradient clipping with bounds of [-10</p><p>5 , 10 5 ]. This allows larger batch sizes to more fully utilize the GPU. a 0 a 1 a 2 a 3 . . . a deg max a 0 a 1,1 a 1,2 a 1,3 . . . a 1,deg max a 2,1 a 2,2 a 2,3 . . . a 2,deg max . . . . . . . . . . . . . . . a m,1 a m,2 a m,3 . . . a m,deg max </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Combinatorial Construction Proofs</head><p>Precision vs. model We first provide a simple justification of the fact (used in Section 3.2) that representing distances d requires about d bits in hyperbolic space -independent of the model of the space. Formally, we show that the number of bits needed to represent a space depends only on the maximal and minimal desired distances and the geometry of the space. Thus although the bulk of our results are presented in the Poincaré sphere, our discussion on precision tradeoffs is fundamental to hyperbolic space.</p><p>A representation using b bits can distinguish 2 b distinct points in a space S. Suppose we wish to capture distances up to d with error tolerance ε -concretely, say every point in the ball B(0, d) must be within distance ε of a represented point. By a sphere covering argument, this requires at least V S (d) V S (ε) points to be represented, where V S (r) is the volume of a ball of radius r in the geometry. Thus at least b = log V S (d)</p><p>V S (ε) bits are needed for the representation. Notice that V E (d) ∼ d n in Euclidean R n space, so this gives the correct bit complexity of n log(d/ε). In hyperbolic space, V H is exponential instead of polynomial in d, so O(d) bits are needed in the representation (for any constant tolerance). In particular, this is independent of the model of the space.</p><p>Graph embedding lower bound Now, we derive a lower bound on the bits of precision required for embedding a graph into H 2 . Afterwards we prove a result bounding the precision for our extension of Sarkar's construction for the r-dimensional Poincaré ball H r . Finally, we give some details on the algorithm for this extension.</p><p>We derive the lower bound by exhibiting an explicit graph and lower bounding the precision needed to represent its nodes (for any embedding of the graph into H 2 ). The explicit graph G m we consider consists of a root node with deg max chains attached to it. Each of these chains has m nodes for a total of 1 + m(deg max ) nodes, as shown in Figure <ref type="figure" target="#fig_7">5</ref>. Lemma D.1. The bits of precision needed to embed a graph with longest path is Ω ε log(deg max ) .</p><p>Proof. We first consider the case where m = 1. Then G 1 is a star with 1+deg max children a 1 , a 2 , . . . , a deg max . Without loss of generality, we can place the root a 0 at the origin 0.</p><p>Let x i = f (a i ) be the embedding into H 2 for vertex a i for 0 ≤ i ≤ deg max . We begin by showing that the distortion does not increase if we equalize the distances between the origin and each child x i . Let us write max = max i d H (0, x i ) and min = min i d H (0, x i ).</p><p>What is the worst-case distortion? We must consider the maximal expansion and the maximal contraction of graph distances. Our graph distances are either 1 or 2, corresponding to edges (a 0 to a i ) and paths of length 2 (a i to a 0 to a j ). By triangle inequality,</p><formula xml:id="formula_33">d H (xi,xj ) 2 ≤ d H (0,xi) 2 + d H (0,xj ) 2</formula><p>≤ max . This implies that the maximal expan- Equalizing the origin-to-child distances (that is, taking max = min ) reduces the distortion. Moreover, these distances are a function of the norms x i , so we set x i = v for each child.</p><formula xml:id="formula_34">sion max i =j d H (f (a i ), f (a j ))/d G (a i , a j ) is max</formula><p>Next, observe that since there are deg max children to place, there exists a pair of children x, y so that the angle formed by x, 0, y is no larger than θ = 2π deg max</p><p>. In order to get a worst-case distortion of 1 + ε, we need the product of the maximum expansion and maximum contraction to be no more than 1 + ε. The maximum expansion is simply d H (0, x) while the maximum contraction is 2 d H (x,y) , so we wan</p><formula xml:id="formula_35">2d H (0, x) ≤ (1 + ε)d H (x, y).</formula><p>We use the log-based expressions for hyperbolic distance:</p><formula xml:id="formula_36">d H (0, x) = log 1 + v 1 -v ,</formula><p>and</p><formula xml:id="formula_37">d H (x, y) = 2 log x -y + x 2 y 2 -2 x, y + 1 (1 -x 2 )(1 -y 2 ) = 2 log 2v 2 (1 -cos θ) + √ v 4 -2v 2 cos θ + 1 1 -v 2 .</formula><p>This leaves us with</p><formula xml:id="formula_38">log 2v 2 (1 -cos θ) + √ v 4 -2v 2 cos θ + 1 1 -v 2 (1 + ε) ≥ log 1 + v 1 -v .</formula><p>Now, since 1 &gt; v 2 , we have that 2(1 -cos θ) ≥ 2v 2 (1 -cos θ). Some algebra shows that 3(1 -cos θ) ≥ √ v 4 -2v 2 cos θ + 1, so that we can upper bound the left-hand side to write</p><formula xml:id="formula_39">log   (1 + 3 2 ) 2(1 -cos θ) 1 -v 2   (1 + ε) ≥ log 1 + v 1 -v .</formula><p>Next we use the small angle approximation cos</p><formula xml:id="formula_40">(θ) = 1 -θ 2 /2 to get 2(1 -cos θ) = θ. Now we have log   (1 + 3 2 )θ 1 -v 2   (1 + ε) ≥ log 1 + v 1 -v . Since v &lt; 1, 1 1-v &gt; 1 1-v 2 and 1+v 1-v ≥ 1 1-v</formula><p>, so we can upper bound the left-hand side and lower bound the right-hand side:</p><formula xml:id="formula_41">log   (1 + 3 2 )θ 1 -v   (1 + ε) ≥ log 1 1 -v .</formula><p>Rearranging,</p><formula xml:id="formula_42">-log(1 -v) ≥ -log 1 + 3 2 θ 1 + ε ε .</formula><p>Recall that θ = 2π deg max</p><p>. Then we have that</p><formula xml:id="formula_43">-log(1 -v) ≥ 1 + ε ε log(deg max ) -log((2 + √ 6)π) , so that -log(1 -v) = Ω 1 ε log(deg max ) . Since v = x = y , -log(1 -v)</formula><p>is precisely the required number of bits of precision, so we have our lower bound for the m = 1 case.</p><p>Next we analyze the m &gt; 1 case. Consider the embedded vertices x 1 , x 2 , . . . , x m corresponding to one chain and y 1 , y 2 , . . . , y m corresponding to another. There exists a pair of chains such that the angle formed by x m , 0,</p><formula xml:id="formula_44">y 1 is at most θ = 2π deg max . Let u = x m and v = y 1 .</formula><p>From the m = 1 case, we have a lower bound on -log(1 -v); we will now lower bound -log(1 -u). The worst-case distortion we consider uses the contraction given by the path</p><formula xml:id="formula_45">x m → x m-1 → • • • → x 1 → 0 → y 1 ; this path has length m + 1.</formula><p>The expansion is just the edge between 0 and y 1 . Then, to satisfy the worst-case distortion 1 + ε, we need</p><formula xml:id="formula_46">(m + 1)d H (0, y 1 ) ≤ (1 + ε)d H (x m , y 1 ).</formula><p>Using the hyperbolic distance formulas, we can rewrite this as</p><formula xml:id="formula_47">2 log x m -y 1 + x m 2 y 1 2 -2 x m , y 1 + 1 (1 -x m 2 )(1 -y 1 2 ) (1 + ε) ≥ (m + 1) log 1 + v 1 -v ,</formula><p>or,</p><formula xml:id="formula_48">2 log √ u 2 + v 2 -2uv cos θ + √ u 2 v 2 -2uv cos θ + 1 (1 -u 2 )(1 -v 2 ) (1 + ε) ≥ (m + 1) log 1 + v 1 -v .</formula><p>Next,</p><formula xml:id="formula_49">2 log √ u 2 + v 2 -2uv cos θ + √ u 2 v 2 -2uv cos θ + 1 (1 -u 2 )(1 -v 2 ) ≤ 2 log   (1 + 3 2 )θ (1 -u 2 )(1 -v 2 )   = log   (1 + 3 2 ) 2 θ 2 (1 -u 2 )(1 -v 2 )   ≤ log   (1 + 3 2 ) 2 θ 2 (1 -u)(1 -v)   .</formula><p>In the first step, we used the same arguments as earlier. Applying this result and using 1+v</p><formula xml:id="formula_50">1-v ≥ 1 1-v , we have log   (1 + 3 2 ) 2 θ 2 (1 -u)(1 -v)   (1 + ε) ≥ (m + 1) log 1 1 -v , or, log   (1 + 3 2 ) 2 θ 2 1 -u   (1 + ε) ≥ (m -ε) log 1 1 -v .</formula><p>Next we can apply the bound on -log(1 -v).</p><formula xml:id="formula_51">log 1 1 -u ≥ -log (1 + 3 2 ) 2 θ 2 + m -ε 1 + ε log 1 1 -v ≥ -log (1 + 3 2 ) 2 θ 2 + m -ε 1 + ε 1 + ε ε log(deg max ) -log((2 + √ 6)π) ) = m -ε ε log(deg max ) - m -ε ε log((2 + √ 6)π) - 1 2 log(deg max ) -log((2 + √ 6)π) .</formula><p>Here, we applied the relationship between θ and deg max we derived earlier. To conclude, note that the longest path in our graph is = 2m. Then, we have that</p><formula xml:id="formula_52">-log(1 -u) = Ω ε log(deg max ) ,</formula><p>as desired.</p><p>Combinatorial construction upper bounds Next, we prove our extension of Sarkar's construction for H r , restated below. Proof. The combinatorial construction achieves worst-case distortion bounded by 1 + ε in two steps <ref type="bibr" target="#b31">[32]</ref>. First, it is necessary to scale the embedded edges by a factor of τ sufficiently large to enable each child of a parent node to be placed in a disjoint cone. Note that there will be a cone with angle α less than π deg max . The connection between this angle and the scaling factor τ is governed by τ = -log(tan α/2). As expected, as deg max increases, α decreases, and the necessary scale τ increases.</p><p>This initial step provides a Delaunay embedding (and thus a MAP of 1.0), but perhaps not sufficient distortion. The second step is to further scale the points by a factor of 1+ε ε ; this ensures the distortion upper bound. Our generalization to the Poincaré ball of dimension r will modify the first step by showing that we can pack more children around a parent while maintaining the same angle. In other words, for a fixed number of children we can increase the angle between them, correspondingly decreasing the scale. We use the following generalization of cones for H r , defined by the maximum angle α ∈ [0, π/2] between the axis and any point in the cone. Let cone C(X, Y, α) be the cone at point X with axis XY and cone angle</p><formula xml:id="formula_53">α: C(X, Y, α) = {Z ∈ H r : Z -X, Y -X ≥ Z -X Y -X cos α} .</formula><p>We seek the maximum angle α for which deg max disjoint cones can be fit around a sphere. Supposing r -1 ≤ log deg max , we use the following lower bound <ref type="bibr" target="#b18">[19]</ref> on the number of unit vectors A(r, θ) that can be placed on the unit sphere of dimension r with pairwise angle at least θ:</p><formula xml:id="formula_54">A(r, θ) ≥ (1 + o(1)) √ 2πr cos θ (sin θ) r-1 .</formula><p>Consider taking angle θ = asin(deg max</p><formula xml:id="formula_55">-1 r-1 ).</formula><p>Note that</p><formula xml:id="formula_56">deg max -1 r-1 = exp log deg max -1 r-1 = exp - log d r -1 ≤ 1/e,</formula><p>which implies that θ is bounded from above and cos θ is bounded from below. Therefore</p><formula xml:id="formula_57">deg max = 1 (sin θ) r-1 ≤ O(1) cos θ (sin θ) r-1 ≤ A(r, θ).</formula><p>So it is possible to place deg max children around the sphere with pairwise angle θ, or equivalently place deg max disjoint cones with cone angle α = θ/2. Note the key difference compared to the two-dimensional case where α = π deg max ; here we reduce the angle's dependence on the degree by an exponent of 1 r-1 . It remains to compute the explicit scaling factor τ that this angle yields; recall that τ = -log(tan α/2) suffices <ref type="bibr" target="#b31">[32]</ref>. We then have τ = -log(tan(θ/4)) = -log sin(θ/2) 1 + cos(θ/2) = log 1 + cos(θ/2) sin(θ/2)</p><formula xml:id="formula_58">≤ log 2 sin(θ/2) = log 4 cos(θ/2) sin θ ≤ log 4 deg max -1 r-1 = O 1 r log deg max .</formula><p>This quantity tells us the scaling factor without considering distortion (the first step). To yield the 1 + ε distortion, we just increase the scaling by a factor of 1+ε ε . The longest distance in the graph is the longest path multiplied by this quantity.</p><p>Putting it all together, for a tree with longest path , maximum degree deg max and distortion at most 1 + ε, the components of the embedding require (using the fact that distances d require d bits), is a trivial upper bound. Note that this cannot be improved asymptotically: As deg max grows, the minimum pairwise angle approaches π/2,<ref type="foot" target="#foot_5">foot_5</ref> so that τ = Ω(1) irrespective of the dimension r.</p><p>Next, we provide more details on the coding-theoretic child placement construction for r-dimensional embeddings. Recall that children are placed at the vertices of a hypercube inscribed into the unit hypersphere, with components in ±1 √ r . These points are indexed by sequences a ∈ {0, 1} r so that . Therefore, we can control the distances between the children by selecting a set of binary sequences with a prescribed minimum Hamming distance-a binary error-correcting code-and placing the children at the resulting hypercube vertices.</p><formula xml:id="formula_59">x a = (-1) a1 √ r , (<label>-1)</label></formula><p>We introduce a small amount of terminology from coding theory. A binary code C is a set of sequences a ∈ {0, 1} r . A [r, k, h] 2 code C is a binary linear code with length r (i.e., the sequences are of length r), size 2 k (there are 2 k sequences), and minimum Hamming distance h (the minimum Hamming distance between two distinct members of the code is h).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Hadamard code C has parameters</head><formula xml:id="formula_60">[2 k , k, 2 k-1 ]. If r = 2 k</formula><p>is the dimension of the space, the Hamming distance between two members of C is at least 2 k-1 = r/2. Then, the distance between two distinct vertices of the hypercube x a and x b is 2 r/<ref type="foot" target="#foot_6">foot_6</ref> r = 2 1/2 = √ 2. Moreover, we can place up to 2 k = r points at least at this distance.</p><p>To build intuition, consider placing children on the unit circle (r = 2) compared to the r = 128-dimensional unit sphere. For r = 2, we can place up to 4 points with pairwise distance at least √ 2. However, for r = 128, we can place up to 128 children while maintaining this distance.</p><p>We briefly describe a few more practical details. Note that the Hadamard code is parametrized by k. To place c + 1 children, take k = log 2 (c + 1) . However, the desired dimension r of the embedding might be larger than the resulting code length r = 2 k . We can deal with this by repeating the codeword. If there are r dimensions and r|r , then the distance between the resulting vertices is still at least √ 2. Also, recall that when placing children, the parent node has already been placed. Therefore, we perform the placement using the hypercube, and rotate the hypersphere so that one of the c + 1 placed nodes is located at this parent.</p><p>Embedding the ancestor transitive closure Prior work embeds the transitive closure of the WordNet noun hypernym graph <ref type="bibr" target="#b27">[28]</ref>. Here, edges are placed between each word and its hypernym ancestors; MAP is computed over edges of the form (word, hypernym), or, equivalently, edges (a, b) where b ∈ A(a) is an ancestor of a.</p><p>In this section, we show how to achieve arbitrarily good MAP on these types of transitive closures of a tree by embedding a weighted version of the tree (which we can do using the combinatorial construction with arbitrarily low distortion for any number dimensions). The weights are simply selected to ensure that nodes are always nearer to their ancestors than to any other node.</p><p>Let T = (V, E) be our original graph. We recursively produce a weighted version of the graph called T that satisfies the desired property. Let s be the depth of node a ∈ V . We weight each of the edges (a, c), where c is a child of a with weight 2 s . Now we show the following property: Proposition D.2. Let b ∈ A(a) be an ancestor of a and e ∈ A(a) be some node not an ancestor of a. Then,</p><formula xml:id="formula_61">d G (a, b) &lt; d G (a, e).</formula><p>Proof. Let a be at depth s. First, the farthest ancestor from a is the root, at distance 2 s-1 + 2 s-2 + . . .</p><formula xml:id="formula_62">+ 2 + 1 = 2 s -1. Thus d G (a, b) ≤ 2 s -1.</formula><p>If e is a descendant of a, then d G (a, e) is at least 2 s Next, if e is neither a descendant nor an ancestor of a, let f be their nearest common ancestor, and let the depths of a, e, f be s, s 2 , s 3 , respectively, where s 3 &lt; min{s 1 , s 2 }. We have that d G (a, e) = (2 s-1 + . . .</p><formula xml:id="formula_63">+ 2 s3 ) + (2 s2-1 + . . . + 2 s3 ) = 2 s -2 s3 + 2 s2 -2 s3 = 2 s + 2 s2 -2 s3+1 ≥ 2 s &gt; d G (a, b).</formula><p>The fourth line follows from s 2 &gt; s 3 . This concludes the argument. Therefore, embedding the weighted tree T with the combinatorial construction enables us to keep all of a word's ancestors nearer to it than any other word. This enables us to embed a transitive closure hierarchy (like WordNet's) while still embedding a nearly tree-like graph. <ref type="foot" target="#foot_7">7</ref> Furthermore, the desirable properties of the construction still carry through (perfect MAP on trees, linear-time, etc).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Proof of h-MDS Results</head><p>We first prove the condition that X T u = 0 is equivalent to pseudo-Euclidean centering.</p><p>Proof of Lemma 4.1. In the hyperboloid model, the variance term Ψ can be written as</p><formula xml:id="formula_64">Ψ(z; x 1 , x 2 , . . . , x n ) = k i=1 sinh 2 (d H (x i , z)) = k i=1 cosh 2 (d H (x i , z)) -1 = k i=1 (x T i Qz) 2 -1 = k i=1 (x 0,i z 0 -x T i z) 2 -1 = k i=1 x 0,i 1 + z 2 -x T i z 2 -1 .</formula><p>The derivative of this with respect to z is</p><formula xml:id="formula_65">∇ z Ψ(z; x 1 , x 2 , . . . , x n ) = 2 k i=1 x 0,i 1 + z 2 -x T i z x 0,i z 1 + z 2 -x i .</formula><p>At z = 0 (or equivalently z = e 0 ), this becomes</p><formula xml:id="formula_66">∇ z Ψ(z; x 1 , x 2 , . . . , x n )| z=0 = 2 k i=1 x 0,i √ 1 + 0 -0 x 0,i 0 √ 1 + 0 -x i = -2 k i=1 x 0,i x i .</formula><p>If we define the matrix X ∈ R n×k such that X T e i = x i and the vector u ∈ R k such that u i = x 0,i , then</p><formula xml:id="formula_67">∇ z Ψ(z; x 1 , x 2 , . . . , x n )| z=0 = -2 k i=1 X T e i e T i u = -2X T u.</formula><p>Centering and Geodesic Submanifolds A well-known property of the hyperboloid model is that the geodesic submanifolds on M r are exactly the linear subspaces of R r+1 intersected with the hyperboloid model (Corollary A.5.5. from <ref type="bibr" target="#b1">[2]</ref>). This is analogous to how the affine subspaces of R r are the linear subspaces of R r+1 intersected with the homogeneous-coordinates model of R r . Notice that this directly implies that any geodesic submanifold can be written as a geodesic submanifold centered on any of the points in that manifold. To be explicit with the definitions: Definition E.1. A geodesic submanifold is a subset S of a manifold such that for any two points x, y ∈ S, the geodesic from x to y is fully contained within S. Definition E.2. A geodesic submanifold rooted at a point x, given some local subspace of its tangent bundle T , is the subset S of the manifold that is the union of all the geodesics through x that are tangent at x in a direction contained in T .</p><p>Now we prove that centering with the pseudo-Euclidean mean preserves geodesic submanifolds.</p><p>First, we need the following technical lemma showing that projection to a manifold decreases distances. Lemma E.3. Consider a dimension-r geodesic submanifold S and point x outside of it. Let z be the projection of x onto S. Then for any point x ∈ S, d H (x, x) &gt; d H (x, z).</p><p>Proof. As a consequence of the projection, the points x, z, x form a right angle. From the hyperbolic Pythagorean theorem, we know that cosh(d H (x, x)) = cosh(d H (x, z)) cosh(d H (z, x)).</p><p>Since cosh is increasing and at least 1 (with equality only at cosh(0) = 1), this implies that</p><formula xml:id="formula_68">d H (x, x) &gt; d H (x, z).</formula><p>Lemma E.4. If some points x 1 , . . . , x k lie in a dimension-r geodesic submanifold S, then both a Karcher mean and a pseudo-Euclidean mean lie in this submanifold. Equivalently, if the points lie in a submanifold, then this submanifold can be written as centered at the Karcher mean or the pseudo-Euclidean mean.</p><p>Proof. Suppose by way of contradiction that there is a Karcher mean x that lies outside this submanifold S. Then, consider the projection z of x onto S. From Lemma E.3, projecting onto S has strictly decreased the distance to all the points on S.</p><p>As a result, the Frechet variance</p><formula xml:id="formula_69">k i=1 d 2 H (x i , x)</formula><p>also decreases when x is projected onto S. From this, it follows that there is a minimum value of the Frechet variance (a Karcher mean) that lies on S. An identical argument works for the pseudo-Euclidean distance, since the pseudo-Euclidean distance uses a variance that is just the sum of monotonically increasing functions of the hyperbolic distance.</p><p>Lemma E.5. Given some pairwise distances d i,j , if it is possible to embed the distances in a dimension-r geodesic submanifold rooted and centered at a pseudo-Euclidean mean, then it is possible to embed the distances in a dimension-r geodesic submanifold rooted and centered at a Karcher mean, and vice versa.</p><p>Proof. Suppose that it is possible to embed the distances as some points x 1 , . . . , x k in a dimension-r geodesic submanifold S. Then, by Lemma E.4, S contains both a Karcher mean x and a pseudo-Euclidean mean xP of these points. If we reflect all the points such that x is reflected to the origin, then the new reflected points will also be an embedding of the distances (since reflection is isometric) and they will also be centered at the origin. Furthermore, we know that they will still lie in a dimension-r submanifold (now containing the origin) since reflection also preserves the dimension of geodesic submanifolds. So the reflected points that we have constructed are an embedding of d i,j into a dimension-r geodesic submanifold rooted and centered at a Karcher mean. The same argument will show that (by reflecting xP to the origin instead of x) we can construct an embedding of d i,j into a dimension-r geodesic submanifold rooted and centered at the pseudo-Euclidean mean. This proves the lemma.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Perturbation Analysis F.1 Handling Perturbations</head><p>Now that we have shown that h-MDS recovers an embedding exactly, we consider the impact of perturbations on the data. Given the necessity of high precision for some embeddings, we expect that in some regimes the algorithm should be very sensitive. Our results identify the scaling of those perturbations.</p><p>First, we consider how to measure the effect of a perturbation on the resulting embedding. We measure the gap between two configurations of points, written as matrices in R n×r , by the sum of squared differences D(X, Y ) = trace((X -Y ) T (X -Y )). Of course, this is not immediately useful, since X and Y can be rotated or reflected without affecting the distance matrix used for MDS-as these are isometries, while scalings and Euclidean translations are not. Instead, we measure the gap by D E (X, Y ) = inf{D(X, P Y ) : P T P = I}.</p><p>In other words, we look for the configuration of Y with the smallest gap relative to X. For Euclidean MDS, Sibson <ref type="bibr" target="#b32">[33]</ref> provides an explicit formula for D E (X, Y ) and uses this formulation to build a perturbation analysis for the case where Y is a configuration recovered by performing MDS on the perturbed matrix XX T + ∆(E), with ∆(E) symmetric.</p><p>Problem setup In our case, the perturbations affect the hyperbolic distances. Let H ∈ R n×n be the distance matrix for a set of points in hyperbolic space. Let ∆(H) ∈ R n×n be the perturbation, with H i,i = 0 and ∆(H) symmetric (so that Ĥ = H + ∆ H remains symmetric). The goal of our analysis is to estimate the gap D E (X, Y ) between X recovered from H with h-MDS and X recovered from the perturbed distances H + ∆(H). Lemma F.1. Under the above conditions, if λ min denotes the smallest nonzero eigenvalue of XX T then up to second order in ∆(H),</p><formula xml:id="formula_70">D E (X, X) ≤ 2n 2 λ min sinh 2 ( H ∞ ) ∆(H) 2 ∞ .</formula><p>The key takeaway is that this upperbound matches our intuition for the scaling: if all points are close to one another, then H ∞ is small and the space is approximately flat (since sinh 2 (z) is dominated by 2z 2 close to the origin). On the other hand, points at great distance are sensitive to perturbations in an absolute sense.</p><p>Proof of Lemma F.1. Similarly to our development of h-MDS, we proceed by accessing the underlying Euclidean distance matrix, and then apply the perturbation analysis from Sibson <ref type="bibr" target="#b33">[34]</ref>. There are three steps: first, we get rid of the acosh in the distances to leave us with scaled Euclidean distances. Next, we remove the scaling factors, and apply Sibson's result. Finally, we bound the gap when projecting to the Poincaré sphere.</p><p>Hyperbolic to scaled Euclidean distortion Let Y denote the scaled-Euclidean distance matrix, as in <ref type="bibr" target="#b0">(1)</ref>, so that Y i,j = cosh(H i,j ). Let Ŷi,j = cosh(H i,j + ∆(H) i,j ). We write ∆(Y ) = Ŷ -Y for the scaled Euclidean version of the perturbation. We can use the hyperbolic-cosine difference formula on each term to write ∆(Y ) i,j = cosh( Ĥi,j ) -cosh(H i,j ) = (cosh(H i,j + ∆(H) i,j ) -cosh(H i,j ))</p><p>= 2 sinh 2H i,j + ∆(H) i,j 2 sinh ∆(H) i,j 2 .</p><p>In terms of the infinity norm, as long as H ∞ ≥ ∆(H) ∞ (it is fine to assume this because we are only deriving a bound up to second order, so we can suppose that ∆(H) is small), we can simplify this to ∆(Y ) ∞ ≤ 2 sinh ( H ∞ ) sinh ( ∆(H) ∞ /2) .</p><p>Scaled Euclidean to Euclidean inner product. Recall that if X is the embedding in the hyperboloid model, then Y = uu T -XX T from equation (2), and furthermore X T u = 0 so that X can be recovered through PCA. Now we are in the Euclidean setting, and can thus measure the result of the perturbation on the recovered X. The proof of Theorem 4.1 in Sibson <ref type="bibr" target="#b33">[34]</ref> transfers to this setting. This result states that if X is the configuration recovered from the perturbed inner products, then, the lowest-order term of the expansion of the error D E (X, X) in the perturbation ∆(Y ) is</p><formula xml:id="formula_71">D E (X, X) = 1 2 j,k (v T j ∆(Y )v k ) 2 λ j + λ k .</formula><p>Here, the λ i and v i are the eigenvalues and corresponding orthonormal eigenvectors of XX T and the sum is taken over pairs of λ j , λ k that are not both 0. Let λ min be the smallest nonzero eigenvalue of XX T . Then,</p><formula xml:id="formula_72">D E (X, X) ≤ 1 2λ min j,k (v T j ∆(Y )v k ) 2 ≤ 1 2λ min ∆(Y ) 2 F ≤ n 2 2λ min ∆(Y ) 2 ∞ .</formula><p>Combining this with the previous bounds, and restricting to second-order terms in ∆(H) 2 ∞ proves Lemma F.1 for the embedding X in the hyperboloid model.</p><p>Proof of Lemma 4.3. We begin by considering the component function</p><formula xml:id="formula_73">f i (γ) = acosh 2 (1 + d 2 E (γ, v i )).</formula><p>Here, the γ is a geodesic through the origin. We can identify this geodesic on the Poincaré disk with a unit vector u such that γ(t) = (2t -1)u. In this case, simple Euclidean projection gives us</p><formula xml:id="formula_74">d 2 E (γ, v i ) = (I -uu T )v i 2 .</formula><p>Optimizing over γ is equivalent to optimizing over u, and so</p><formula xml:id="formula_75">f i (u) = acosh 2 1 + (I -uu T )v i 2 .</formula><p>If we define the functions h(γ) = acosh 2 (1 + γ)</p><p>and</p><formula xml:id="formula_76">R(u) = (I -uu T )v i 2 = v i 2 -(u T v i ) 2</formula><p>then we can rewrite f i as f i (u) = h(R(u)).</p><p>Now, optimizing over u is an geodesic optimization problem on the hypersphere. Every goedesic on the hypersphere can be isometrically parameterized in terms of an angle θ as u(θ) = x cos(θ) + y sin(θ) for orthogonal unit vectors x and y. Without loss of generality, suppose that y T v i = 0 (we can always choose such a y because there will always be some point on the geodesic that is orthogonal to v i ). Then, we can write Differentiating again, d 2 dθ 2 h(R(θ)) = 4h (R(θ)) • (v T i x) 4 • sin 2 (θ) cos 2 (θ) + 2h (R(θ)) • (v T i x) 2 • cos 2 (θ) -sin 2 (θ) .</p><formula xml:id="formula_77">R(θ) = v i 2 -(x T v i ) 2 cos 2 (θ) = v i 2 -(x T v i ) 2 + (x T v i ) 2 sin 2 (θ).</formula><p>Now, suppose that we are interested in the Hessian at a point z = x cos(θ) + y sin(θ) for some fixed angle θ. Here, R(θ) = R(z), and as always v T i z = v T i x cos(θ), so</p><formula xml:id="formula_78">d 2 dθ 2 h(R(θ)) u(θ)=z = 4h (R(θ)) • (v T i x) 4 • sin 2 (θ) cos 2 (θ) + 2h (R(θ)) • (v T i x) 2 • cos 2 (θ) -sin 2 (θ) = 4h (R(z)) • (v T i z) 4 cos 4 (θ)</formula><p>• sin 2 (θ) cos 2 (θ) + 2h (R(z)) • (v T i x) 2 cos 2 (θ)</p><p>• cos 2 (θ) -sin 2 (θ) = 4h (R(z))</p><formula xml:id="formula_79">• (v T i z) 4 • tan 2 (θ) + 2h (R(z)) • (v T i z) 2 • 1 -tan 2 (θ) = 2h (R(z)) • (v T i z) 2 + 4h (R(z)) • (v T i z) 4 -2h (R(z)) • (v T i z) 2 tan 2 (θ).</formula><p>But we know that since h is concave and increasing, this last expression in parenthesis must be negative. It follows that a lower bound on this expression for fixed z will be attained when tan<ref type="foot" target="#foot_8">foot_8</ref> (θ) is maximized. For any geodesic through z, the angle θ is the distance along the geodesic to the point that is (angularly) closest to v i . By the Triangle inequality, this will be no greater than the distance θ along the Geodesic that connects z with the normalization of v i . On this worst-case geodesic, v T i z = v i cos(θ), and so 2 . Thus, for any geodesic, for the worst-case angle θ,</p><formula xml:id="formula_80">cos 2 (θ) = (v T i z) 2 v i 2 and tan 2 (θ) = sec 2 (θ) -1 = v i 2 (v T i z) 2 -1 = R(z) (v T i z)</formula><formula xml:id="formula_81">d 2 dθ 2 h(R(θ)) u(θ)=z ≥ 2h (R(z)) • (v T i z) 2 + 4h (R(z)) • (v T i z) 4 -2h (R(z)) • (v T i z) 2 tan 2 (θ) = 2h (R(z)) • (v T i z) 2 + 4h (R(z)) • (v T i z) 2 -2h (R(z)) R(z).</formula><p>From here, it is clear that this lower bound on the second derivative (and as a consequence local convexity) is a function solely of the norm of v i and the residual to z. From simple evaluation, we can compute that For any γ that satisfies 0 ≤ γ ≤ 1, 4γ γ 2 + 2γ ≥ (3γ 2 + 2γ) acosh(1 + γ)</p><p>and so 4γh (γ) + h (γ) ≥ 0.</p><p>Thus, if 0 ≤ R(z) ≤ 1,</p><formula xml:id="formula_82">d 2 dθ 2 h(R(θ)) u(θ)=z ≥ 2h (R(z)) • (v T i z) 2 + 4h (R(z)) • (v T i z) 2 -2h (R(z)) R(z) = h (R(z)) • (v T i z) 2 + (4h (R(z)) • R(z) + h (R(z))) • (v T i z) 2 -2h (R(z)) • R(z) ≥ h (R(z)) • (v T i z) 2 -2h (R(z)) • R(z) = h (R(z)) • v i 2 -R(z) -2h (R(z)) • R(z) = h (R(z)) • v i 2 -3R(z) .</formula><p>Thus, a sufficient condition for convexity is for (as we assumed above) R(z) ≤ 1 and</p><formula xml:id="formula_83">v i 2 ≥ 3R(z).</formula><p>Combining these together shows that if</p><formula xml:id="formula_84">acosh 2 1 + d E (γ, v i ) 2 = R(z) ≤ min 1, 1 3 v i</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Geodesics and distances in the Poincaré disk. As x and y move towards the outside of the disk (i.e., letting x , y → 1), the distance d H (x, y) approaches d H (x, O) + d H (O, y).</figDesc><graphic coords="3,272.00,74.70,210.60,111.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1 implements a simple embedding of trees into H 2 .</head><label>2</label><figDesc>The algorithm takes as input a scaling factor τ a node a (of degree deg(a)) from the tree with parent node b. Suppose a and b have already been embedded into H 2 and have corresponding embedded vectors f (a) and f (b). The algorithm places the children c 1 , c 2 , . . . , c deg(a)-1 into H 2 through a two-step process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 1 Sarkar's Construction 1 : 8 :</head><label>118</label><figDesc>Input: Node a with parent b, children to place c 1 , c 2 , . . . , c deg(a)-1 , partial embedding f containing an embedding for a and b, scaling factor τ2: (0, z) ← reflect f (a)→0 (f (a), f (b)) 3: θ ← arg(z){angle of z from x-axis in the plane} 4: for i ∈ {1, . . . , deg(a) -1} do 5: y i ← e τ -1 e τ +1 • cos θ + 2πi deg(a) , e τ -1 e τ +1 • sin θ + 2πi deg(a) 6: end for 7: (f (a), f (b), f (c 1 ), . . . , f (c deg(a)-1 )) ← reflect 0→f (a) (0, z, y 1 , . . . , y deg(x)-1 ) Output: Embedded H 2 vectors f (c 1 ), f (c 2 ), . . . , f (c deg(a)-1 )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Top. Cycles are a challenge for tree embeddings: d G (a, b) goes from 1 to 5. Bottom. Steiner nodes can help: adding a node (blue) and weighting edges maintains the pairwise distances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><figDesc>Figure3: The PGA objective of an example task where the input dataset in the Poincaré disk is x 1 = (0.8, 0), x 2 = (-0.8, 0), x 3 = (0, 0.7) and x 4 = (0, -0.7). Note the presence of non-optimal local minima, unlike PCA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Learning from incomplete information. The distance matrix is sampled, completed, and embedded.</figDesc><graphic coords="11,307.25,566.82,187.20,127.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Explicit graphs G m used to derive precision lower bound. Left: m = 1 case (star graph). Right: m &gt; 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>1 =</head><label>1</label><figDesc>max occuring at a parent-child edge. Similarly, the maximal contraction is at least 1 min . With this, D wc (f ) ≥ max min .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Proposition 3 . 1 .</head><label>31</label><figDesc>The generalized H r combinatorial construction has distortion at most 1 + ε and requires at most O( 1 ε r log deg max ) bits to represent a node component for r ≤ (log deg max )+1, and O( 1 ε ) bits for r &gt; (log deg max )+ 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><figDesc>bits per component. This big-O is with respect to deg max and any r ≤ log deg max +1. When r &gt; log deg max +1, O 1 ε</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><figDesc>The Euclidean distance between x a and x b is a function of the Hamming distance d Hamming (a, b) between a and b. The Euclidean distance is exactly 2 dHamming(a,b) r</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><figDesc>Differentiating the objective with respect to θ,d dθ h(R(θ)) = h (R(θ))R (θ) = 2h (R(θ)) • (v T i x)2 • sin(θ) cos(θ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>γ 2 2 = 2 4γ γ 2 2 = 2 4γ γ 2</head><label>2222222</label><figDesc>+ 2γ -(1 + γ) acosh(1 + γ) (γ 2 + 2γ) 3/2 .As a result4γh (γ) + h (γ) = 8 γ γ 2 + 2γ -(γ 2 + γ) acosh(1 + γ) (γ 2 + 2γ) 3/2 + 2 (γ 2 + 2γ) acosh(1 + γ) (γ 2 + 2γ) 3/+ 2γ -4(γ 2 + γ) acosh(1 + γ) + (γ 2 + 2γ) acosh(1 + γ) (γ 2 + 2γ) 3/+ 2γ -(3γ 2 + 2γ) acosh(1 + γ) (γ 2 + 2γ) 3/2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Datasets Statistics.</figDesc><table><row><cell>Dataset</cell><cell cols="3">Nodes Edges Comment</cell></row><row><cell>Bal. Tree</cell><cell>40</cell><cell>39</cell><cell>Tree</cell></row><row><cell>Phy. Tree</cell><cell>344</cell><cell>343</cell><cell>Tree</cell></row><row><cell>CS PhDs</cell><cell>1025</cell><cell>1043</cell><cell>Tree-like</cell></row><row><cell>WordNet</cell><cell cols="3">74374 75834 Tree-like</cell></row><row><cell>Diseases</cell><cell>516</cell><cell>1188</cell><cell>Dense</cell></row><row><cell>Gr-QC</cell><cell>4158</cell><cell>13428</cell><cell>Dense</cell></row><row><cell>Dataset</cell><cell cols="3">C-H 2 FB H 5 FB H 200</cell></row><row><cell cols="3">WordNet 0.989 0.823*</cell><cell>0.87*</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Distortion measures using combinatorial and h-MDS techniques, compared against PCA and results from Nickel and Kiela<ref type="bibr" target="#b27">[28]</ref>. Closer to 0 is better.</figDesc><table><row><cell>Dataset</cell><cell cols="2">C-H 2 FB H 2</cell><cell cols="4">h-MDS PyTorch PWS PCA</cell><cell>FB</cell></row><row><cell>Bal. Tree</cell><cell cols="2">0.013 0.425</cell><cell>0.077</cell><cell>0.034</cell><cell cols="3">0.020 0.496 0.236</cell></row><row><cell cols="3">Phy. Tree 0.006 0.832</cell><cell>0.039</cell><cell>0.237</cell><cell cols="3">0.092 0.746 0.583</cell></row><row><cell>CS PhDs</cell><cell cols="2">0.286 0.542</cell><cell>0.149</cell><cell>0.298</cell><cell cols="3">0.187 0.708 0.336</cell></row><row><cell>Diseases</cell><cell cols="2">0.147 0.410</cell><cell>0.111</cell><cell>0.080</cell><cell cols="3">0.108 0.595 0.764</cell></row><row><cell>Gr-QC</cell><cell>0.354</cell><cell>-</cell><cell>0.530</cell><cell>0.125</cell><cell cols="2">0.134 0.546</cell><cell>-</cell></row><row><cell>Dataset</cell><cell cols="2">C-H 2 FB H 2</cell><cell cols="4">h-MDS PyTorch PWS PCA</cell><cell>FB</cell></row><row><cell>Bal. Tree</cell><cell>1.0</cell><cell>0.846</cell><cell>1.0</cell><cell>1.0</cell><cell>1.0</cell><cell>1.0</cell><cell>0.859</cell></row><row><cell>Phy. Tree</cell><cell>1.0</cell><cell>0.718</cell><cell>0.675</cell><cell>0.951</cell><cell>0.998</cell><cell>1.0</cell><cell>0.811</cell></row><row><cell>CS PhDs</cell><cell cols="2">0.991 0.567</cell><cell>0.463</cell><cell>0.799</cell><cell cols="2">0.945 0.541</cell><cell>0.78</cell></row><row><cell>Diseases</cell><cell cols="2">0.822 0.788</cell><cell>0.949</cell><cell>0.995</cell><cell cols="3">0.897 0.999 0.934</cell></row><row><cell>Gr-QC</cell><cell>0.696</cell><cell>-</cell><cell>0.710</cell><cell>0.733</cell><cell cols="3">0.504 0.738 0.999  *</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Glossary of variables and symbols used in this paper.</figDesc><table><row><cell>Symbol</cell><cell>Used for</cell></row><row><cell>x, y, z</cell><cell>vectors in the Poincaré ball model of hyperbolic space</cell></row><row><cell>d H</cell><cell>metric distance between two points in hyperbolic space</cell></row><row><cell>d E</cell><cell>metric distance between two points in Euclidean space</cell></row><row><cell>d U</cell><cell>metric distance between two points in metric space U</cell></row><row><cell>d</cell><cell>a particular distance value</cell></row><row><cell>d i,j</cell><cell>the distance between the ith and jth points in an embedding</cell></row><row><cell>H r</cell><cell>the Poincaré ball model of r-dimensional Hyperbolic space</cell></row><row><cell>r</cell><cell>the dimension of a Hyperbolic space</cell></row><row><cell>H</cell><cell>Hyperbolic space of an unspecified or arbitrary dimension</cell></row><row><cell>M r</cell><cell>the Minkowski (hyperboloid) model of r-dimensional Hyperbolic space</cell></row><row><cell>f</cell><cell>an embedding</cell></row><row><cell>N a</cell><cell>neighborhood around node a in a graph</cell></row><row><cell>R a,b</cell><cell>the smallest set of closest points to node a in an embedding f that contains node b</cell></row><row><cell>MAP(f )</cell><cell>the mean average precision fidelity measure of the embedding f</cell></row><row><cell>D(f )</cell><cell>the distortion fidelity measure of the embedding f</cell></row><row><cell>D wc (f )</cell><cell>the worst-case distortion fidelity measure of the embedding f</cell></row><row><cell>G</cell><cell>a graph, typically with node set V and edge set E</cell></row><row><cell>T</cell><cell>a tree</cell></row><row><cell>a, b, c</cell><cell>nodes in a graph or tree</cell></row><row><cell>deg(a)</cell><cell>the degree of node a</cell></row><row><cell>deg max</cell><cell>maximum degree of a node in a graph</cell></row><row><cell></cell><cell>the longest path length in a graph</cell></row><row><cell>τ</cell><cell>the scaling factor of an embedding</cell></row><row><cell cols="2">reflect x→y a reflection of x onto y in hyperbolic space</cell></row><row><cell>arg(z)</cell><cell>the angle that the point z in the plane makes with the x-axis</cell></row><row><cell>X</cell><cell>matrix of points in hyperbolic space</cell></row><row><cell>Y</cell><cell>matrix of transformed distances</cell></row><row><cell>γ</cell><cell>geodesic used in PGA</cell></row><row><cell>w i</cell><cell>transformed points used in PGA</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>There is no perfect analogue of PCA in hyperbolic space<ref type="bibr" target="#b28">[29]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>A minor instability with Chamberlain et al.<ref type="bibr" target="#b4">[5]</ref>, Nickel and Kiela<ref type="bibr" target="#b27">[28]</ref>'s formulation is that one must guard against NANs. This instability may be unavoidable in formulations that minimize hyperbolic distance with gradient descent, as the derivative of the hyperbolic distance has a singularity, that is, limy→x ∂x|d H (x, y)| → ∞ for any x ∈ H in which d H is the hyperbolic distance function. This issue can be mitigated by minimizing d 2 H , which does have a continuous derivative throughout H. We propose to do so in Section 4.2 and discuss this further in the Appendix.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Although it is particularly easy to bound precision in the Poincaré model, this fact holds generally for hyperbolic space independent of model. See Appendix D for a general lower bound argument.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>We say that points are centered at a particular mean if this mean is at 0. The act of centering refers to applying an isometry that makes the mean of the points 0.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>As we noted earlier, considering the distances without squares leads to a non-continuously-differentiable formulation.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>Given points x 1 , . . . , xn on the unit sphere, 0 ≤x i</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_6"><p>= n + i =j x i , x j implies there is a pair such that x i • x j ≥ -1 n-1 , i.e. an angle bounded by cos -1 (-1/(n -1)).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_7"><p>Note that further separation can be achieved by picking weights with a base larger than 2.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_8"><p>then f i is locally convex at z. The result of the lemma now follows from the fact that f is the sum of many f i and the sum of convex functions is also convex.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6">Conclusion and Future Work</head><p>Hyperbolic embeddings embed hierarchical information with high fidelity and few dimensions. We explored the limits of this approach by describing scalable, high quality algorithms. We hope the techniques here encourage more follow-on work on the exciting techniques of Chamberlain et al. <ref type="bibr" target="#b4">[5]</ref>, <rs type="person">Nickel and Kiela</rs> <ref type="bibr" target="#b27">[28]</ref>. As future work, we hope to explore how hyperbolic embeddings can be most effectively incorporated into downstream tasks and applications.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Projecting to the Poincaré disk Algorithm 2 initially finds an embedding in M r , but optionally converts it to the Poincaré disk. To convert a point x in the hyperboloid model to z in the Poincaré disk, take z =</p><p>. Let Z ∈ R n×r be the projected embedding. Now we show that the same perturbation bound holds after projection. Lemma F.2. For any x and y,</p><p>Proof. Let u x = 1 + x 2 and define u y analogously. Note that u x ≥ 2, u x ≥ x , and</p><p>Combining these facts leads to the bound</p><p>Lemma F.2 is equivalent to the statement that D(z, ẑ) ≤ D(x, x) where z, ẑ are the projections of x, x. Since orthogonal matrices P preserve 2 norm, P ẑ is the projection of P x so D(z, P ẑ) ≤ D(x, P x) for any P . Finally, D(Z, P Ẑ) is just a sum over all columns and therefore D(Z, P Ẑ) ≤ D(X, P X). This implies that D E (Z, Ẑ) ≤ D E (X, X) as desired.</p><p>The hyperbolic gap The gap D(X, X) can be written as a sum d E (x i , xi ) 2 over the vectors (columns) of X, X. We can instead ask about the hyperbolic gap</p><p>which is a better interpretation of the perturbation error when recovering hyperbolic distances.</p><p>Note that for any points x, y in the Gans model, we have</p><p>Furthermore, the function acosh(1 + t 2 /2) -t is always negative except in a tiny region around t = 0 (and attains a maximum here on the order of 10 -10 ), so effectively acosh 1 + 1 2 x -y 2 ≤ x -y = d E (x, y), and the same bound in Lemma F.1 carries over to the hyperbolic gap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Proof of Lemma 4.3</head><p>In this section, we prove Lemma 4.3, which gives a setting under which we can guarantee that the hyperbolic PGA objective is locally convex.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Experimental Results</head><p>In this section, we provide some additional experimental results. We also present results on an additional less tree-like graph (a search engine query response graph for the search term 'California' <ref type="bibr" target="#b20">[21]</ref>.)</p><p>Combinatorial Construction: Parameters To improve the intuition behind the combinatorial construction, we report some additional parameters used by the construction. For each of the graphs, we report the maximum degree, the scaling factor ν that the construction used (note how these vary with the size of the graph and the maximal degree), the time it took to perform the embedding, in seconds, and the number of bits needed to store a component for ε = 0.1 and ε = 1.0. Hyperparameter: Effect of Rank We also considered the influence of the dimension on the perfomance of h-MDS, PCA, and FB. On the Phylogenetic tree dataset, we measured distortion and MAP metrics for dimensions of 2,5,10,50,100, and 200. The results are shown in Table <ref type="table">8</ref>. We expected all of the techniques to improve with better rank, and this was the case as well. Here, the optimization-based approach typically produces the best MAP, optimizing the fine details accurately. We observe that the gap is closed when considering 2-MAP (that is, MAP where the retrieved neighbors are at distance up to 2 away). In particular we see that the main limitation of h-MDS is at the finest layer, confirming the idea MAP is heavily influenced by local changes. In terms of distortion, we found that h-MDS offers good performance even at a very low dimension (0.083 at 5 dimensions).</p><p>Precision Experiment (cf Table <ref type="table">9</ref>). Finally, we considered the effect of precision on h-MDS for a balanced tree and fixed dimension 10 </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Metric tree-like structures in real-world networks: an empirical study</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abu-Ata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">F</forename><surname>Dragan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Networks</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="68" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Benedetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Petronio</surname></persName>
		</author>
		<title level="m">Lectures on Hyperbolic Geometry</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">D</forename><surname>Brannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Esplen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><surname>Geometry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The power of convex relaxation: Near-optimal matrix completion</title>
		<author>
			<persName><forename type="first">E</forename><surname>Candes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2053" to="2080" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Deisenroth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.10359</idno>
		<title level="m">Neural embeddings of graphs in hyperbolic space</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On the hyperbolicity of small-world and tree-like random graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Algorithms and Computation (ISAAC) 2012</title>
		<meeting><address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="278" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Conway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J A</forename><surname>Sloane</surname></persName>
		</author>
		<title level="m">Sphere Packings, Lattices and Groups</title>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hyperbolic embedding and routing for dynamic graphs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cvetkovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Crovella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE INFOCOM</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multidimensional scaling in the poincaré disk</title>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Cvetkovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Crovella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applied mathematics &amp; information sciences</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Exploratory social network analysis with Pajek</title>
		<author>
			<persName><forename type="first">W</forename><surname>De Nooy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mrvar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Batagelj</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Succinct greedy graph drawing in the hyperbolic plane</title>
		<author>
			<persName><forename type="first">D</forename><surname>Eppstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goodrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Symposium on Graph Drawing (GD 2011)</title>
		<meeting>of the International Symposium on Graph Drawing (GD 2011)<address><addrLine>Eindhoven, Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="355" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Principal geodesic analysis for the study of nonlinear statistics of shape</title>
		<author>
			<persName><forename type="first">P</forename><surname>Fletcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pizer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="995" to="1005" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Hyperbolic entailment cones for learning hierarchical embeddings</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bécigneul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.01882</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The human disease network</title>
		<author>
			<persName><forename type="first">K</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cusick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Childs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Barabási</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">21</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hyperbolic groups</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gromov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Essays in group theory</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">First insights in the diversity of certain mosses colonising modern building surfaces by use of genetic barcoding</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hofbauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Forrest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hollingsworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hart</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Intrinsic shape analysis: Geodesic pca for riemannian manifolds modulo isometric lie group actions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Huckemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hotz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Munk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistica Sinica</title>
		<imprint>
			<biblScope unit="page" from="1" to="58" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Reconstructing approximate tree metrics</title>
		<author>
			<persName><forename type="first">I</forename><surname>Abraham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-sixth annual ACM symposium on Principles of Distributed Computing (PODC)</title>
		<meeting>the twenty-sixth annual ACM symposium on Principles of Distributed Computing (PODC)<address><addrLine>Portland, Oregon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">On kissing numbers and spherical codes in high dimensions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jenssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Joos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Perkin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02702</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Mason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Barabasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">N</forename><surname>Oltvai</surname></persName>
		</author>
		<idno>cond-mat/0105306</idno>
		<title level="m">Lethality and centrality in protein networks</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Authoritative sources in a hyperlinked environment</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
		<ptr target="http://www.cs.cornell.edu/courses/cs685/2002fa/" />
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Geographic routing using hyperbolic space</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">26th IEEE International Conference on Computer Communications (ICC)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1902" to="1909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Curvature and temperature of complex networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Krioukov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Boguná</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">35101</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hyperbolic geometry of complex networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Krioukov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kitsak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Boguná</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">36106</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Laying out and visualizing large trees using a hyperbolic space</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lamping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th annual ACM symposium on User interface software and technology</title>
		<meeting>the 7th annual ACM symposium on User interface software and technology</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="13" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Graph evolution: Densification and shrinking diameters</title>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The geometry of graphs and some of its algorithmic applications</title>
		<author>
			<persName><forename type="first">N</forename><surname>Linial</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>London</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Combinatorica</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="215" to="245" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Poincaré embeddings for learning hierarchical representations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Barycentric subspace analysis on manifolds</title>
		<author>
			<persName><forename type="first">X</forename><surname>Pennec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint/>
	</monogr>
	<note>to appear 2017</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The network data repository with interactive graph analytics and visualization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Ahmed</surname></persName>
		</author>
		<ptr target="http://networkrepository.com" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">TreeBASE: a prototype database of phylogenetic analyses and an interactive tool for browsing the phylogeny of life</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Donoghue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Piel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eriksson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Botany</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Low distortion Delaunay embedding of trees in hyperbolic plane</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Symposium on Graph Drawing (GD 2011)</title>
		<meeting>of the International Symposium on Graph Drawing (GD 2011)<address><addrLine>Eindhoven, Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="355" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Studies in the robustness of multidimensional scaling: Procrustes statistics</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sibson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series B</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="234" to="238" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Studies in the robustness of multidimensional scaling: Perturbational analysis of classical scaling</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sibson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series B</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="217" to="229" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Hyperbolic representation learning for fast and efficient neural question answering</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Hui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Eleventh ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="583" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Metric embedding, hyperbolic space, and social networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Suri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Geometry</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">H-MDS: a new approach for interactive visualization with multidimensional scaling in the hyperbolic space</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Walter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="273" to="292" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Spherical and hyperbolic embeddings of data</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pekalska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Duin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2255" to="2269" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Probabilistic principal geodesic analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fletcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26 (NIPS 2013)</title>
		<meeting><address><addrLine>Lake Tahoe, NV</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
