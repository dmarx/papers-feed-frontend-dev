- Decision to create the MATH dataset for measuring mathematical problem-solving ability
- Choice of problem sources from high school math competitions
- Inclusion of step-by-step solutions for each problem
- Decision to categorize problems by difficulty levels (1 to 5)
- Selection of subjects covered in the dataset (e.g., Algebra, Geometry, etc.)
- Use of L A T E X for problem and solution formatting
- Decision to provide an auxiliary pretraining dataset (AMPS)
- Choice of training methodology for models (e.g., training on solutions vs. direct questions)
- Decision to evaluate model performance based on exact match scoring
- Choice to analyze the effectiveness of hints and partial solutions
- Decision to assess human performance on the MATH dataset for benchmarking
- Choice to explore algorithmic advancements for improving model performance
- Decision to document the limitations of scaling Transformer models for mathematical reasoning
- Choice to compare MATH dataset performance with existing benchmarks
- Decision to utilize "scratch space" in model training for intermediate computations
- Choice to focus on natural language and common mathematical notation for problem representation