<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Musings About the Future of Search: A Return to the Past?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-12-25">25 Dec 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pankaj</forename><surname>Gupta</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">BSL AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Will</forename><surname>Horn</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">BSL AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gilad</forename><surname>Mishne</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">BSL AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Musings About the Future of Search: A Return to the Past?</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-12-25">25 Dec 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">075E4351D9246E34B44EA9ACDDEC639D</idno>
					<idno type="arXiv">arXiv:2412.18956v1[cs.IR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>When you have a question, the most effective way to have the question answered is to directly connect with experts on the topic and have a conversation with them. Prior to the invention of writing, this was the only way. Although effective, this solution exhibits scalability challenges. Writing allowed knowledge to be materialized, preserved, and replicated, enabling the development of different technologies over the centuries to connect information seekers with relevant information. This progression ultimately culminated in the ten-blue-links web search paradigm we're familiar with, just before the recent emergence of generative AI. However, we often forget that consuming static content is an imperfect solution. With the advent of large language models, it has become possible to develop a superior experience by allowing users to directly engage with experts. These interactions can of course satisfy information needs, but expert models can do so much more. This coming future requires reimagining search.</p><p>Having questions and seeking answers to them are as old as humanity. The information science and information retrieval literature broadly refers to this process as information seeking, which is the terminology we adopt here.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The most effective information seeking solution is for the information seeker to directly connect with the best known expert<ref type="foot" target="#foot_0">foot_0</ref> on the topic of interest and have a conversation, interacting with the expert until the information need has been satisfied <ref type="bibr" target="#b8">[Taylor, 1962]</ref>. <ref type="foot" target="#foot_1">2</ref> In our distant preliterate past, this was the only way. It is our hypothesis that the future of search is to return to this past, but aided by yet-to-be-developed technologies and millions of large language models (LLMs) that overcome fundamental challenges that have plagued humanity for millennia.</p><p>Writing is a relatively late invention in human history, a development that occurred roughly 5000 years ago <ref type="bibr" target="#b6">[Robinson, 1995]</ref>. In preliterate societies, seeking information by finding relevant experts was the only way. If you had a question, you went to the people in your village who were the most knowledgeable about the topic and simply asked them-or, more accurately, had a conversation with them until your question was answered.</p><p>Although effective, this "ask-the-experts" approach exhibits two fundamental limitations, both around its scalability:</p><p>1. The limited bandwidth of experts, who could practically have only a small number of conversations with information seekers daily. Quite simply, these experts had other things to do (e.g., hunting, farming, etc.) and couldn't spend every waking moment answering questions. However, as societies developed specialization of roles, these experts evolved to fill the niche of sages or village elders, sanctioned by the community to devote significant time to transmitting knowledge orally.</p><p>2. The problem of finding relevant experts and their willingness to engage, especially as populations grew larger and became more geographically distributed. Dunbar's number has been suggested as the cognitive limit of the number of people that an individual can maintain stable social relationships with <ref type="bibr" target="#b2">[Dunbar, 1962]</ref>. Some limit like this sets the upper bound on the pool of experts one could know, who might also be willing to share their time.</p><p>Beyond this limit, some type of referral is likely necessary.</p><p>Before the invention of writing, knowledge was locked in the heads of individuals, and "connecting directly with experts" was the only way to "search", to address information needs. The invention of writing, however, provided a mechanism for materializing knowledge in forms that became increasingly easier to preserve and replicate.</p><p>Worth repeating: the invention of writing enabled the convenient materialization of knowledge.</p><p>This invention solved the bandwidth problem. The speed of information dissemination was no longer bounded by the physical limitations associated with verbal transmission by experts. Instead, they could write down (i.e., materialize) what they knew, whether on clay tablets, tortoise shells, vellum, papyrus, or any other medium-to be consumed by others, across time and space, with a degree of parallelization.<ref type="foot" target="#foot_2">foot_2</ref> </p><p>The materialization of knowledge solved a number of practical problems, for example:</p><p>• The experts don't have the time or the desire to converse with every information seeker.</p><p>• The experts aren't geographically close to the information seekers.</p><p>• The experts aren't "temporally" close to the information seekers. In fact, the experts may not even be alive anymore! Furthermore, it became possible to replicate materialized knowledge (i.e., make copies), thus leveraging network effects and parallelizing dissemination to a degree that would not have been possible with human experts transmitting knowledge orally. Centuries ago, replicating materialized knowledge was an expensive proposition, when human scribes had to meticulously make copies by hand. The invention of the printing press (mad props to Gutenberg) dramatically lowered costs, which steadily declined over time, until it was pushed even lower by the invention of the web (hats off to timbl). Today, the marginal cost of making another copy of something is near zero.</p><p>However, we must recognize that such mechanisms lead to a sub-optimal "user experience". The information seeker cannot engage with a "dead" medium (i.e., static content that is unable to support interactions). Often, the materialized knowledge is not exactly what the information seeker desired, for example, differing in a key assumption or detail. The information seeker cannot engage in follow-up exchanges to clarify a point, to drill down into detail, etc. Materialized knowledge captured in a static medium represents at best a proxy. Nevertheless, this imperfect solution represented a huge advance for civilization. Writing provided a practical workaround to the first challenge of scalability: experts had limited bandwidth, so we materialized their knowledge.</p><p>The invention of writing was followed by numerous attempts over the centuries to tackle the second challenge: finding the right experts who held the knowledge sought after. Since humanity now dealt with proxies of knowledge (e.g., scrolls and books), the challenge was reformulated into that of finding the relevant materialized proxies. Today, we call this the search problem. Over the centuries, great minds proposed different solutions: The Great Library of Alexandria had Callimachus' Pinakes, a catalog system that connected scholars with the library's holdings <ref type="bibr" target="#b3">[El-Abbadi, 1984]</ref>. Dewey took a stab at the problem in the 19th century <ref type="bibr" target="#b1">[Dewey, 1876]</ref> (whose solution remains in prod today).</p><p>Fast forward to the web just before the advent of large language models (LLMs): search engines can be understood as facilitating a "connection" across time and space by surfacing materialized proxies of knowledge (i.e., the ten blue links) that were relevant, thus potentially satisfying a user's information need (again, hearkening back to <ref type="bibr" target="#b8">Taylor [1962]</ref>). These are webpages, blog posts, videos, papers, etc., the things we "search for". The experience remained sub-optimal because the information seeker was still limited to consuming "dead" content. The first challenge remains unaddressed, as it is still not possible to have conversations with the best experts in the world on a topic of interest. Thus, consuming materialized proxies of their knowledge (reading the webpages and blog posts they wrote, watching videos they produced, etc.) remains the next best option.</p><p>But lest we forget that these materialized proxies are at best dancing shadows in Plato's cave. Can we develop a better solution to the first problem? For example:</p><p>• Instead of reading Jeff's guide on restaurants in London because you're looking for dining options while on vacation, you just want to talk to Jeff directly and ask him for a suggestion in Southwark. (But does Jeff have the patience to listen as you describe your idiosyncratic palate and your fondness of high-end sake?)</p><p>• Instead of watching Jane's YouTube tutorial on repairing stucco, you'd like to send her a picture of the damage you're facing and ask her to walk you through repairs with "turn-byturn" instructions calibrated to your skill level. (Of course, she needs to first listen to you ramble about existing DIY experience and the tools that you have access to.) With LLMs, this becomes possible. You will be able to converse directly with Jeff or Jane. Well, not exactly the humans Jeff and Jane, but rather their representative LLMs. We can call these avatars, agents, digital twins, or any other similar name. Admittedly, we're still not connecting information seekers to human experts, but this is probably the best we can do for the foreseeable future.</p><p>Economics does not pose a barrier to the massive proliferation of these LLMs. Once created, the marginal cost of replicating knowledge in this form is primarily dictated by the computation costs of LLM inference, which according to one estimate has been decreasing by 10× every year <ref type="bibr" target="#b0">[Appenzeller, 2024]</ref>. Jevons would predict a commensurate increase in the number of available LLMs and their use <ref type="bibr">[Jevons, 1866]</ref>. There is emerging consensus that the future does not lie in "one model to rule them all" <ref type="bibr" target="#b4">[Huang and Grady, 2024]</ref>. Rather, we will witness a proliferation of LLMs-thousands, perhaps even millions. <ref type="foot" target="#foot_3">4</ref> The continued popularity of open-weight models will ensure this future, barring any unexpected regulatory surprises.</p><p>In tomorrow's world populated by millions of LLMs, each with distinct expertise, the first challenge is solved. Expertise has become abundant, and the bandwidth of experts is only limited by the amount of compute capacity we can muster (a separate challenge). Interacting with agents, avatars, or whatever we want to call them provides a superior "user experience" to consuming static materialized content (books, webpages, papers, blog posts, etc.), which is our only option today.</p><p>In fact, user interactions with these LLM experts can go far beyond the answering of questions.</p><p>"Find examples of verses in iambic pentameter about love" is an information need. "Compose a poem for my wife about our first date in iambic pentameter" seems like much more? One might argue that underlying every information need is a desire for some sort of action; the literature has given "the ultimate goal" of information seeking different names, including what information science researchers have called the "outcome" or "use" <ref type="bibr" target="#b9">[Wilson, 1999]</ref>, and what information retrieval researchers have more recently described as the underlying task <ref type="bibr" target="#b7">[Shah et al., 2023]</ref>. In this case, searching for examples of verses in iambic pentameter fulfills an instructional purpose, to aid in the task of composing poetry. But here the LLM can directly assist in the task! Perhaps this is not qualitatively different from inferring the intent to purchase an item (and providing online shopping assistance) based on a search query comparing two alternatives. Nevertheless, technology linking information to action so explicitly has never been available before, and LLMs enable interactions that would be impossible with static content.</p><p>However, a world with millions of LLMs completely breaks the working solution to the second challenge. Search techniques and systems today for connecting users to relevant materialized knowledge do not work for this coming future. We're no longer searching for documents, webpages, images, and video; it's more than just content created on the fly in a personalized manner by generative AI techniques. Ultimately, we're looking for potential interactions with relevant experts that can go far beyond simply answering questions. In fact, the very term "search" is perhaps insufficient to describe this future world when LLM experts can "just do it", like composing a poem or fixing a bug. Or perhaps we should reconceive search as not merely seeking information, but finding the right assistant to fulfill my "wants".</p><p>Either way, search needs to be reimagined in a world populated by millions of expert LLMs (dare we call them AIs?). Our first step is to start a conversation on what this future might look like.</p></div>			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The best expert or a small panel of experts? We're agnostic on this distinction, although for expository purposes writing in plural allows us to avoid the use of gendered pronouns.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>In his words: "We assume that man talking with man is the best possible form of communication."</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Also from<ref type="bibr" target="#b8">Taylor [1962]</ref>: "Our ideal is to bring two people together through the printed page."</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>These millions of LLMs may very well derive from a relatively small set of model backbones, but adapted (post-training) in myriad ways.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Welcome to LLMflation -LLM inference cost is going down fast</title>
		<author>
			<persName><forename type="first">G</forename><surname>Appenzeller</surname></persName>
		</author>
		<ptr target="https://a16z.com/llmflation-llm-inference-cost/" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Classification and subject index for cataloguing and arranging the books and pamphlets of a library</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dewey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1876">1876</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neocortex size as a constraint on group size in primates</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">I M</forename><surname>Dunbar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Human Evolution</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="469" to="493" />
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The Life and Fate of the Ancient Library of Alexandria</title>
		<author>
			<persName><forename type="first">M</forename><surname>El-Abbadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<publisher>UNESCO/UNDP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Generative AI&apos;s act o1</title>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Grady</surname></persName>
		</author>
		<ptr target="https://www.sequoiacap.com/article/generative-ais-act-o1/" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The Coal Question</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Jevons</surname></persName>
		</author>
		<imprint>
			<publisher>Macmillan and Company</publisher>
			<biblScope unit="page">1866</biblScope>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The Story of Writing: Alphabets, Hieroglyphs and Pictograms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Robinson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Thames &amp; Hudson</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Taking search to task</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Belkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2023 Conference on Human Information Interaction and Retrieval (CHIIR 2023)</title>
		<meeting>the 2023 Conference on Human Information Interaction and Retrieval (CHIIR 2023)<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The process of asking questions</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Documentation</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="391" to="396" />
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Models in information behaviour research</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Documentation</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="249" to="270" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
