# Improving Factuality with Explicit Working Memory

## Abstract

## Meta FAIR

Large language models can generate factually inaccurate content, a problem known as hallucination. Recent works have built upon retrieved-augmented generation to improve factuality through iterative prompting but these methods are limited by the traditional RAG design. To address these challenges, we introduce Ewe (Explicit Working Memory), a novel approach that enhances factuality in long-form text generation by integrating a working memory that receives real-time feedback from external resources. The memory is refreshed based on online fact-checking and retrieval feedback, allowing Ewe to rectify false claims during the generation process and ensure more accurate and reliable outputs. Our experiments demonstrate that Ewe outperforms strong baselines on four fact-seeking long-form generation datasets, increasing the factuality metric, VeriScore, by 2 to 10 points absolute without sacrificing the helpfulness of the responses. Further analysis reveals that the design of rules for memory updates, configurations of memory units, and the quality of the retrieval datastore are crucial factors for influencing model performance.

## Introduction

In the realm of long-form text generation, a notable vulnerability of large language models (LLMs) is their propensity for hallucination, wherein the generated text contains factually inaccurate information. By prepending the input prompt with relevant documents from trustworthy sources, retrieved-augmented generation (RAG) [(Lewis et al., 2020;](#b11)[Shi et al., 2024)](#b21) has been shown to be a simple yet effective approach that substantially mitigates the hallucination issue. To further enhance the factual accuracy of model output, various iterative prompting methods have been proposed that build upon RAG. For instance, FLARE [(Jiang et al., 2023)](#b8) generates responses sentence by sentence, and if a newly generated sentence contains low-probability tokens, it retrieves a new set of documents and re-runs RAG to regenerate the sentence. Alternatively, Self-RAG [(Asai et al., 2024)](#b0) employs a self-critic component to verify the correctness of each partial generation and repeatedly queries a retrieval system to update the background knowledge, thereby producing more accurate and faithful responses. While these systems demonstrate significant empirical improvement, they are restricted in the traditional RAG design. Context-relevant knowledge through retrieval is the only online feedback to the model, incorporated as part of the input string.

In this work, we propose Ewe (Explicit Working mEmory), an iterative framework that aims to provide more factual responses for knowledge-intensive long-form generation, with the help of an auxiliary fact-checking module. Ewe augments an existing language model with an explicit working memory, which keeps track of the knowledge that is most relevant and useful at the current generation timestep. The memory is initially filled with the latent representation of some retrieved passages relevant to the input prompt. During the generation process, Ewe actively monitors the newly generated partial response and pauses occasionally to refresh the memory with knowledge from retrieval and to check the output statement. If the statement is factually incorrect, it then refreshes the memory with the fact-checking feedback. With the updated memory, Ewe first removes the incorrect statement and backtracks to the previous timestep, and then continues the generation process from there.

We assume that the main text generation model used here is a Transformer-based large language model, such as Llama [(Dubey et al., 2024)](#b2). Similar to the standard RAG setting, given an input prompt, we first retrieve Figure [1](#) Example pipeline illustrating how Ewe pauses, receives feedback from retrievers and fact-checkers, and then re-generate to correct factual errors in its outputs. Ewe handles knowledge from fact-checkers and retrievers separately as they tend to provide information with distinct properties. Retrieval offers more general background information, while fact-checkers focus more on specific details, targeting particular aspects of the output text.

k relevant text chunks of the same number of tokens, as the background knowledge. Unlike RAG, which directly prepends the input prompt with these k chunks, we apply the language model to them separately and store the KV cache of each chunk in a memory of k units. When predicting the next token, the language model effectively attends the current token to all k chunks in parallel, using their KV caches stored in the memory, and have the average as the attention value. When Ewe pauses the generation and checks the newly generated, partial response, it has the opportunity to update the memory in multiple ways to guide the language model. For instance, if some claims in the new sentence are not supported, this feedback along with additional supporting documents can be used as a new unit appended to the existing memory. In addition, if the knowledge from an initial retrieved passage is no longer relevant, its corresponding memory unit can be removed or updated with embeddings of a new passage retrieved using the generated partial response as query.

Ewe can be seen as a more general framework that subsumes many existing approaches. For example, if there is no stopping in generation and if the memory contains only one unit (i.e., k=1), then Ewe degenerates to the simple vanilla RAG. If Ewe pauses at the end of generation of every sentence and checks whether the new sentence contains any token with low probability as a proxy of factuality measure, then this particular instantiation, with one memory unit, is effectively FLARE. Notice that in a typical, more general configuration of Ewe, the memory module consists of multiple units. When the memory is refreshed, not all the units need to be updated. If some knowledge is still required, their original raw data (e.g., passages) will not be reprocessed to create the embeddings, saving some redundant computational cost at inference time. While conceptually simple, the working memory design in Ewe provides a more flexible and yet efficient way to incorporate various of types of external online information, as different forms of feedback are encoded in parallel and stored in memory units (e.g., see Figure [1](#)). We notice that the design of leveraging working memory is also very related to some recently proposed methods for long-content models (e.g., Memory 3 [(Yang et al., 2024)](#b27)). If our memory is used only for encoding the knowledge from the passages in our corpus, then this can be viewed as the whole corpus is used as the context, along with the prompt, as the input to the model. The key differences are that Ewe does not pre-encode every passage (although the KV caches of some frequently retrieved passages can certainly be precomputed in advance) and its memory can be dynamically updated as the generation progresses.

We demonstrate empirically that Ewe generates more factual responses without sacrificing the relevance to the input questions, using four fact-seeking long-form generation datasets. In general, with the feedback from online fact-checking and targeted retrieval, Ewe increases VeriScore [(Song et al., 2024)](#b22), the factuality metric we use, by 2 to 10 points absolute and has a similar helpfulness in terms of instruction following compared to the base model Llama-3.1 70B .

## Related work

Aiming to reduce hallucination and make the LLMs generate more factual responses, our proposed framework, Ewe, detects knowledge gaps and acquires relevant information as needed, incorporating feedback from auxiliary models when available. Unlike chain-of-verification approaches [(Dhuliawala et al., 2024, CoVe)](#), which rely solely on the LLM for reasoning, Ewe combines adaptive retrieval augmentation and explicit memories with a focus on factuality. This section discusses related work on these two aspects.

## Iterative and Adaptive Retrieval Augmentation

Retrieval-augmented generation (RAG) typically involves a single retrieval step, followed by the language model generating a complete response. However, iterative retrieval methods [(Gao et al., 2024, ยง5)](#) have been proposed to generate responses in multiple segments, with each segment generated using different additional information retrieved through iterative retrieval. One such approach is ITER-RETGEN [(Shao et al., 2023)](#b20), which uses the model output of the previous iteration to formulate the query and retrieve more relevant knowledge for the current generation. Extending iterative retrieval, the process of adaptive retrieval [(Gao et al., 2024, ยง5)](#) examines partially generated responses in previous iterations to decide whether retrieving new information or regenerating a segment response is needed. For instance, FLARE [(Jiang et al., 2023)](#b8) follows a simple sentence-by-sentence generation process to answer fact-seeking questions. In each step, it generates a temporary next sentence and examines its acceptability based on model confidence. If the sentence is deemed questionable, it retrieves new text chunks using a query based on the temporary sentence and re-generates the next sentence using standard RAG. DRAGIN [(Su et al., 2024)](#b23) improves upon FLARE by introducing a new model confidence measure that combines attention scores and entropy values. This allows the model to pause the generation immediately after the confidence score of a token falls below a threshold. Additionally, DRAGIN uses preceding tokens with high attention scores on the stopping token to form a keyword-based query, which helps the model make a more confident next-token prediction.

Our work shares similarities with Self-RAG [(Asai et al., 2024)](#b0), particularly in the use of an auxiliary model to provide feedback. Unlike confidence measures based on token probability or attention score, Self-RAG fine-tunes the model to introspectively decide when to pause generation by outputting a special retrieve token. This triggers the retrieval of multiple passages, which are then used separately to generate candidate segments via standard RAG. Each segment is evaluated by a "critique" model for relevance, usefulness to the original prompt, and support from the retrieved passage. The critique model's output determines whether a candidate segment is included in the final output.

Our approach, Ewe, differs from existing iterative and adaptive retrieval augmentation methods in two key aspects. Firstly, traditional retrieval augmentation is replaced with memory augmentation, where the representation is the KV cache (similar to TurboRAG [(Lu et al., 2024)](#b14)) instead of the raw text, and different memory chunks that encode different passages are processed in parallel. This design allows for greater flexibility in incorporating diverse information types and improves efficiency when only part of the memory is updated, as the remaining portion can be reused. Secondly, feedback from the auxiliary model is passed to the language model through memory, enabling the core language model to naturally incorporate multiple streams of information and produce better responses. This design difference sets our approach apart from existing methods and allows for more effective integration of factuality feedback from the auxiliary model.

## Memories in Long-context LLMs

Incorporating a large-scale corpus as additional knowledge can be achieved by prepending the given prompt with all documents in the corpus as an extremely long context input [(Lee et al., 2024)](#b10) to language models. It is thus natural to see that long-context LLMs share some technical components that apply to retrieval augmentation. The memory module in Ewe is analogous to the explicit memory design in Memory 3 [(Yang et al., 2024)](#b27). Instead of encoding the knowledge in the training corpus completely in model parameters, or incorporating the knowledge primarily through retrieval augmentation, Memory 3 encodes 128-token chunks of the training corpus using their KV caches as memories. During inference, the model generates segments of 64 tokens. At the generation of each segment, it first uses the previous segment as query to retrieve 5 most relevant memories, and attends to them when generating the next segment. Retrieving memories of KV caches has been proposed in earlier work. For instance, Memorizing Transformers [(Wu et al., 2022)](#b26) effectively extends the context of the language model by k nearest neighbor lookup of the past key-value pairs (i.e., long-range memory) and attends them in the last layer of the models. LongMem [(Wang et al., 2023)](#b24) proposed a decoupled network architecture, using the backbone language model as memory encoder and a trained residual side-network as memory retriever and reader. The top-k attention key-value pairs stored in the memory are retrieved and incorporated at inference. While we also use explicit memories to store KV caches in Ewe, our goal is to pass new information at each step in the iterative decoding process, such as new information relevant to the current context via online retrieval and feedback from auxiliary models. We allow different operations on existing memories, including update, append, or delete, providing more flexibility for various downstream tasks.

## Method

The overall generation process of Ewe is similar to the decoding process of typical Transformer-based models, with two differences: ( ยง3.1) Ewe pauses generation periodically. When a new complete sentence has been generated, Ewe uses the current context to retrieve a new set of passages as knowledge feedback. In addition, it runs a fact-checking model to judge whether the sentence contains any factually incorrect statements. If the sentence does contain factual errors, the correct facts will be used as the fact-checking feedback. Both types of feedback will be added to memories, and the sentence will be regenerated if the original one has factual errors. ( ยง3.2) The generation is memory-augmented. In addition to the typical context like the input sentence and tokens generated in previous timesteps, embeddings of various forms of feedback stored in the memories will influence the generated tokens through self-attention.

## Real-time Feedback

Following the design of recently proposed evaluation metrics on factuality [(Min et al., 2023;](#b15)[Wei et al., 2024;](#b25)[Song et al., 2024)](#b22), we determine whether a sentence is factually correct by checking if all of the claims extracted from this sentence are supported. While in general, Ewe can use any textual knowledge as feedback, we focus on providing two types of feedback when the newly generated sentence contains factual errors: fact-checking outcomes and relevant knowledge.

Fact-checking outcomes This feedback consists of the correct information that refutes the inaccurate claims, such as "Strelitzia thrives in a tropical-like 60%-70% humidity." that proves "Bird of Paradise prefers a dry atmosphere." wrong in the example in Figure [1](#). In this work, we adapt the claim extraction model and verification model in VeriScore [(Song et al., 2024)](#b22) as the fact-checking model, where the factual knowledge is derived from the Google snippets when using the extracted claim as query.

Relevant knowledge Using the original input question and the sentence being fact-checked as query, we use Contriever [(Izacard et al., 2022)](#b7) to retrieve passages from C4 [(Raffel et al., 2020)](#b17) and Wikipedia, following the setting in MassiveDS [(Shao et al., 2024)](#b19). Passages with retrieval scores exceeding a certain threshold will be viewed as knowledge relevant to the current context and used to update the working memories.

We pause at every T r timesteps to gather feedback from retrievers, and T v timesteps from fact-checkers. However, if no new sentence is generated, the feedback collection process will be skipped.

Figure [2](#) Diagram illustrating self-attention computations performed at each layer in Ewe. We concatenate each memory with the context (except for the last hidden vector where we only use context), apply standard self-attention and then aggregate the resulting hidden vectors to produce the final hidden vectors.

## Refreshing Working Memories

The working memory in Ewe consists of k memory units, where each unit is designed to store the representations of each feedback message of M tokens. When updating working memories, we follow the first in, first out (FIFO) rule. Given refreshed text chunks of the same length from fact-checkers and retrievers, our model encodes them into the KV cache in parallel using the same positional IDs. Working memories in Ewe are stored as part of the language models' context preceding the model's own output text and prompts, allowing for flexible updates without reprocessing generated content. As shown in Figure [2](#), a separate embedding store is used for preserving these memories, which are then processed at each layer by concatenating them with the context. We then apply regular self-attention and aggregate the resulting hidden vectors using normalization terms from self-attention for each memory unit. Empirically, we find that adding hidden vectors produced by context only improves the fluency of long outputs, so we keep it in our model architectures. More formally,

$โ h n = k+1 i=1 ฮฑ i โ h ni k+1 j=1 ฮฑ j ,(1)$where โ h n is the output vectors for self-attention at n-th layer in LMs, โ h ni is the hidden vectors produced by memories concatenated with the context and by only the context vectors, and ฮฑ i is the normalization term from self-attention that leads to โ h ni .

## Experiments

We present the main experimental results of Ewe in this section, along with details of the datasets and evaluation metrics we used, and the baseline we compared with. In this set of experiments, we set the retrieval and verification timesteps, T r and T v , to be 1 and 8, respectively.

## Evaluation Datasets

We evaluated Ewe and baseline models using four fact-seeking long-form generation datasets: LongFact [(Wei et al., 2024)](#b25), Fava [(Mishra et al., 2024)](#b16), AlpacaFact [(Dubois et al., 2023;](#b3)[Lin et al., 2024)](#b13) and Biography [(Min et al., 2023)](#b15).

LongFact Designed to probe the factuality of a model of which response consists of at least several paragraphs, LongFact was created by prompting GPT-4 to generate questions regarding a specific concept or object within a given topic. In our experiments, we use the 250 prompts from the LongFact-Objects dataset, selected by [Wei et al. (2024)](#b25).

Fava As a new fine-grained hallucination benchmark, Fava constructed 200 information-seeking queries that require factual knowledge to give accurate long-form answers from multiple sources, including Open Assistant [(Kรถpf et al., 2023)](#b9), No Robots [(Rajani et al., 2023)](#b18), WebNLG [(Gardent et al., 2017)](#b6) and instructions written by the authors [(Mishra et al., 2024)](#b16). Following [Lin et al. (2024)](#b13), we selected 141 prompts from this collection for our experiments.

AlpacaFact Originally collected from real-world interactions with various users, the 805 instructions in AlpacaFarm [(Dubois et al., 2023)](#b3) was used for evaluating the instruction-following ability of different LLMs.

To focus our evaluation on factuality, we used a subset of 241 fact-seeking instructions selected by [Lin et al. (2024)](#b13) in this work.

Biography To demonstrate the effectiveness of the factuality metric FActScore, [Min et al. (2023)](#b15) selected 183 names of famous people found in Wikipedia, and applied the "Tell me a bio of [Person Name]" template to create a collection of prompts called Biography. As this set of prompts have been used extensively in several recent papers, we include it in our study as well.

When using these prompts, we appended the instruction "Provide as many specific details and examples as possible (such as names of people, numbers, events, locations, dates, times, etc.)" to encourage models to generate more detailed responses that cover multiple factoids, following [Wei et al. (2024)](#b25).

## Evaluation Metrics

We assess the quality of model responses to fact-seeking questions based on two key axes: factuality and helpfulness. For evaluating factuality, we considered multiple automatic metrics, such as FActScore [(Min et al., 2023)](#b15) and SAFE [(Wei et al., 2024)](#b25), but ultimately chose VeriScore [(Song et al., 2024)](#b22) as our primary evaluation metric. Although these metrics share a similar design that decomposes sentences into "atomic claims" and checks their support against an external knowledge source, VeriScore focuses on extracting more sensible verifiable claims and uses Google search snippets instead of Wikipedia as the knowledge source. As a result, VeriScore can be applied to responses on more diverse topics and is also more efficient, requiring fewer but more meaningful claims to be checked. We report the F 1 score from VeriScore, which is the harmonic mean of the precision and recall of the claims. Following [Song et al. (2024)](#b22), we set the minimum number of facts required for a model's response to achieve perfect recall as the median number of extracted claims per dataset[foot_0](#foot_0) . We also used their fine-tuned models for claim extraction and verification, provided in their package[foot_1](#foot_1) .

To make sure that a model with a high factuality score does not simply give irrelevant but correct factual statements, we also need to check whether the response is helpful to the user. Following [Lin et al. (2024)](#b13), we use AlpacaEval [(Dubois et al., 2024)](#b4) to compare the target model and baseline model in terms of their instruction-following ability. For the responses to the same input prompt, a large language model is used as judge to determine which of the two is better[foot_2](#foot_2) , and the win rate is thus used as a measure of helpfulness[foot_3](#foot_3) .

## Baselines

We used instruction-tuned Llama-3.1 70B and 8B as the base models and compared Ewe with five baselines: base model only, retrieval augmentation (RA), Chain of verification (CoVe)[foot_4](#foot_4) , an iterative retrieval approach DRAGIN [(Su et al., 2024)](#b23)[foot_5](#foot_5) , and a recently proposed semi-parametric decoding method Nest [(Li et al., 2024)](#b12). For base model only, Llama-3.1 70B or Llama-3.1 8B , we simply gave the language model the prompt in the dataset and the instruction of requesting detailed information, without other additional information.

With retrieval augmentation, we retrieved 20 passages using the input prompts as queries and then prepended the passages to the input[foot_6](#foot_6) . Nest is a strong retrieval-based decoding algorithm. Following the original setup, we retrieved 100 passages to use as candidates. For CoVe, we employ the "factor+revise" method, which For each dataset, we report F1 scores from VeriScore and win rates (WR) from AlpacaEval. We use Llama-3.170B as the baseline method in AlpacaEval win rate experiments. [Dhuliawala et al. (2024)](#b1) demonstrated to be the most effective. Additionally, we improve CoVe by integrating retrieved passages from our retrieval datastore during the verification step. This augmentation helps us establish a stronger and more comparable baseline method, considering that most other baseline methods also utilize retrieval. For all our experiments, the maximum generation step was set to 1024. Llama-3.1 70B is used as the baseline method for all AlpacaEval comparisons.

## Results

Our main results are shown in Table [1](#tab_0). For the Llama-3.1 70B base model, we find that in terms of factuality, retrieval augmentation generally improves the results consistently across different datasets. This is expected as for fact-seeking prompts, specifically conditioning generation on relevant factual knowledge has been demonstrated to be an effective way to mitigate hallucinations. Nest performs better than the base model on the Biography dataset, but not on others, and it appears that the VeriScore F 1 is lower than the standard retrieval augmentation. It might suggest that the configuration or hyperparameter settings of Nest need to be further optimized, as Nest was originally evaluated by Biography with Llama-2. DRAGIN performs similarly to RA, likely because their query formulation method is not optimized for long-form generation, resulting in less useful retrieved passages. Similarly, with CoVe, we notice that it often produces shorter model responses, leading to significantly lower recall performance despite high precision, which results in a less favorable VeriScore F 1 . While augmenting CoVe with retrieval slightly alleviates this issue, it still lags behind. Perhaps more interestingly, with online fact-checking feedback and refreshed knowledge from retrieval, Ewe achieves the highest VeriScore F 1 on all datasets. On the helpfulness of the responses, it appears that AlpacaEval generally prefers the output from the base model, except for Ewe, where the win rates are roughly 50%.

When using Llama-3.1 8B as the base model, we have observed a similar trend. Retrieval augmentation improves factuality in terms of VeriScore F 1 and Ewe still gives the best factuality results. However, compared to the models based on Llama-3.1 70B , we notice that the improvement is generally smaller. We hypothesize that the smaller base language model is less capable in leveraging feedback, and may not always regenerate a sentence that is factually correct. In terms of helpfulness, we can see that Ewe generally performs comparably to its base model Llama-3.1 8B , as they have similar win rates when judged against the output of the same Llama-3.1 70B base model. Table [2](#) VeriScore F1 over 50 prompts from LongFact with different shapes of working memory. We allocate an equal number of memory units for both retrieval and fact-checking feedback.

## Analysis

We provide some insights based on different ablations in this section. Llama-3.1 70B were used as the base model for all the experiments in this section.

## Memory Configurations

In this analysis, we explore the influence of memory configurations on factuality, based on experiments on 50 randomly sampled prompts from LongFact. We examine this impact through two dimensions: the number of memory units and the of memory units.

In Figure [3](#fig_0), we investigate how varying the numbers of memory units used for storing fact-checking feedback and retrieved passages may impact factuality. When adjusting the configuration for one, we keep the other constant to facilitate easier interpretation of the results. Overall, we observe that having a large amount of memory units for either fact-checking feedback or retrieved passages negatively impacts factuality. This is likely because a significant amount of stale information remains in working memory for an extended period without being updated, as we adhere to the FIFO rule for updating working memory. Consequently, this information becomes outdated as the generation process continues.

In Table [2](#), we examine the impact of varying memory unit shapes on factuality. To ensure a fair comparison, we maintained a consistent total number of tokens in working memory across different experimental setups. Notably, our findings suggest that models favor shorter, more memory units over longer, fewer ones. We hypothesize that this preference arises because 128 tokens approximately match the length of a retrieved passage, allowing the attention mechanism to effectively cover one individual passage at a time. In contrast, longer memory units combine multiple passages into a single unit, which may compel the attention to focus on less relevant passages when they are grouped with more relevant ones.

## Feedback Forms

In this analysis, we explore various feedback formats utilized by fact-checkers. The models in VeriScore offers 2 types of information: a list of both factual and nonfactual claims, along with relevant passages that support these factual and nonfactual judgments. To examine the impact of these feedback formats, we conduct experiments using different combinations of these information types in the working memory. For the

Passages determining a claim is incorrect Passages determining a claim is correct Instructions for nonfactual claims Precision Recall F 1 โ โ โ 77.3 64.0 66.8 โ 76.4 67.4 67.9 โ 77.5 67.2 69.4 โ 67.1 66.2 66.7 โ โ 80.8 66.1 69.3 72.5 65.9 66.2 Llama-3.1 70B 65.8 67.1 65.5 Llama-3.1 70B + RA 70.1 66.1 65.9

Table 3 Comparing different feedback forms for fact-checkers. We report VeriScore over 50 prompts from LongFact.

supporting passages, we combine them using new line symbols. For the list of claims, we apply an instruction template as follows to encode nonfactual claims:

Please refrain from including the following imprecise statements: (1) nonfactual claim 1 (2) nonfactual claim 2 ...

Our results are shown in Table [3](#). Overall, fact-checking feedback is beneficial compared to the base model with and without retrieval augmentation. The specific types of feedback also play a crucial role. Incorporating all feedback forms does not enhance model performance, with supporting passages proving more effective than instructions. We notice that instructing models not to generate specific details often results in misunderstanding. Models might rephrase the instruction, include the nonfactual statement in their response, and then add a clarification indicating the previous statement is nonfactual, such as "(Note: This is a nonfactual claim and may not be accurate.)". We leave a better design of feedback forms to future work. Interestingly, when we exclude all the textual feedback from fact-checkers and only pause and regenerate in the presence of nonfactual sentences, performance still slightly improves.

## Model Confidence

One important question remains is when to refresh the working memory. To study it, we conducted a comparative analysis of different criteria for refreshing working memory and regeneration. Since the working memory consists of the retrieval memory and fact-checking memory, which can have interacting effects, we first investigate when to trigger the retriever alone (without fact-checking memory) and then investigate when to trigger the fact-checker (when retrieval interval T r is set to 1).

Fixed intervals for refreshing working memory As shown in Figure [4a](#), when using a fixed retrieval interval, an intermediate interval seems to perform well. This may be due to the fact that overly frequent retrieval can add irrelevant and conflicting information to the memory. With fact-checking feedback in memory, it seems frequent verification and regeneration is not always beneficial due to the fact that we only regenerate once and the regenerated sentence is not always better.

Model confidence for refreshing working memory In practice, fixed retrieval and verification intervals may be unnecessary and lead to sub-optimal performance. We explore whether model-confidence can serve as a signal for refreshing working memory. Specifically, we compare two different metrics for model confidence: (1) Entropy: average entropy of generated tokens in a sentence, and (2) Min-prob: minimum probability of tokens in a sentence. A higher threshold for entropy results in less frequent memory update, and a higher threshold for min-prob results in more frequent memory update. Since external fact-checkers can be computationally expensive, we first examine if we can use model confidence as a signal for retrieval and regeneration, without using an auxiliary fact-checking model to provide feedback. As shown in Figure [4b](#) and 4c (blue line), we observe empirically intermediate thresholds for retrieval perform well, leading to to better F 1 when compared to the settings in Figure [4a](#), where we use different fixed intervals for retrieval. With external fact-checkers, we investigate if we can use model confidence as a signal to trigger verification and regeneration to improve generation efficiency. In Figure [4b](#) and 4c (red line), when chosen at an appropriate threshold, both entropy and min-prob can outperform the baseline (using fixed verification interval T v = 8) despite with less frequent verification.

## Knowledge from Retrieval

We present the results of using different retrieval corpora in Table [4](#), including Wikipedia, C4, or both of them together. Likely due to its broader coverage, C4 is more effective than Wikipedia in helping the model produce more factual responses. Combining C4 with Wikipedia further enhances the factual accuracy (except for Biography), probably because they offer complementary sets of knowledge.

## Conclusion

We present Ewe, a novel system that incorporates a working memory mechanism during the generation process. Ewe pauses at given intervals and refreshes its working memory based on feedback from retrieval and fact-checking models, ensuring that the generated content remains accurate and relevant. By integrating this working memory into each attention layer of the Transformer architectures, Ewe can be easily adapted to various large language models. Our experiments demonstrate the effectiveness of Ewe by benchmarking it on 8B and 70B Llama-3.1 models, resulting in significant improvements in both factuality and helpfulness across four fact-seeking long-form generation datasets. Furthermore, our analysis reveals that updating the working memory with more relevant information at each timestep, allowing attention to focus on each passage, and utilizing high-quality retrieval datastores with extensive knowledge coverage are crucial factors for improving factuality of models.

![Figure 3 VeriScore F1 over 50 prompts from LongFact when varying number of memory units used for storing retrieved passages and fact-checking feedback. Each memory unit stores 128 tokens.]()

![]()

![Evaluation on factuality and helpfulness of the model responses to prompts provided in four long-form question answering datasets.]()

The median numbers of extracted facts for LongFact, Fava, AlpacaFact, Biography are 55,49, 31, 43, respectively.   

https://github.com/Yixiao-Song/VeriScore

We used GPT-4o as the judge.

We found that the length-controlled win rates in AlpacaEval could conflate hallucinations and length effects, and thus report the version without length normalization.

We adapted an implementation from https://github.com/ritun16/chain-of-verification

We used the authors' implementation https://github.com/oneal2000/DRAGIN

Using more than 20 passages does not provide significant benefits in our preliminary experiments, so we limit our retrieval to the top 20 passages.

Table 4 VeriScore F1 over 50 prompts from LongFact, AlpacaFact, Fava and Biography with different retrieval datastores.

