{
  "arxivId": "2412.18069",
  "title": "Improving Factuality with Explicit Working Memory",
  "authors": "Mingda Chen, Yang Li, Karthik Padthe, Rulin Shao, Alicia Sun, Luke Zettlemoyer, Gargi Gosh, Wen-tau Yih",
  "abstract": "Large language models can generate factually inaccurate content, a problem\nknown as hallucination. Recent works have built upon retrieved-augmented\ngeneration to improve factuality through iterative prompting but these methods\nare limited by the traditional RAG design. To address these challenges, we\nintroduce EWE (Explicit Working Memory), a novel approach that enhances\nfactuality in long-form text generation by integrating a working memory that\nreceives real-time feedback from external resources. The memory is refreshed\nbased on online fact-checking and retrieval feedback, allowing EWE to rectify\nfalse claims during the generation process and ensure more accurate and\nreliable outputs. Our experiments demonstrate that Ewe outperforms strong\nbaselines on four fact-seeking long-form generation datasets, increasing the\nfactuality metric, VeriScore, by 2 to 10 points absolute without sacrificing\nthe helpfulness of the responses. Further analysis reveals that the design of\nrules for memory updates, configurations of memory units, and the quality of\nthe retrieval datastore are crucial factors for influencing model performance.",
  "url": "https://arxiv.org/abs/2412.18069",
  "issue_number": 805,
  "issue_url": "https://github.com/dmarx/papers-feed/issues/805",
  "created_at": "2025-01-05T19:04:47.334344",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 27,
  "last_read": "2025-01-05T19:04:47.338057",
  "last_visited": "2025-01-05T19:03:17.946Z",
  "main_tex_file": null,
  "published_date": "2024-12-24T00:55:59Z",
  "arxiv_tags": [
    "cs.CL"
  ]
}