# Sampling weights of deep neural networks

## Abstract

## 

We introduce a probability distribution, combined with an efficient sampling algorithm, for weights and biases of fully-connected neural networks. In a supervised learning context, no iterative optimization or gradient computations of internal network parameters are needed to obtain a trained network. The sampling is based on the idea of random feature models. However, instead of a data-agnostic distribution, e.g., a normal distribution, we use both the input and the output training data to sample shallow and deep networks. We prove that sampled networks are universal approximators. For Barron functions, we show that the L 2 -approximation error of sampled shallow networks decreases with the square root of the number of neurons. Our sampling scheme is invariant to rigid body transformations and scaling of the input data, which implies many popular pre-processing techniques are not required. In numerical experiments, we demonstrate that sampled networks achieve accuracy comparable to iteratively trained ones, but can be constructed orders of magnitude faster. Our test cases involve a classification benchmark from OpenML, sampling of neural operators to represent maps in function spaces, and transfer learning using well-known architectures.

## Introduction

Training deep neural networks involves finding all weights and biases. Typically, iterative, gradientbased methods are employed to solve this high-dimensional optimization problem. Randomly sampling all weights and biases before the last, linear layer circumvents this optimization and results in much shorter training time. However, the drawback of this approach is that the probability distribution of the parameters must be chosen. Random projection networks [[54]](#b53) or extreme learning machines [[30]](#b29) involve weight distributions that are completely problem-and data-agnostic, e.g., a normal distribution. In this work, we introduce a data-driven sampling scheme to construct weights and biases close to gradients of the target function (cf. Figure [1](#)). This idea provides a solution to three main challenges that have prevented randomly sampled networks to compete successfully against iterative training in the setting of supervised learning: deep networks, accuracy, and interpretability.

Figure [1](#): Random feature models choose weights in a data-agnostic way, compared to sampling them where it matters: at large gradients. The arrows illustrate where the network weights are placed.

Deep neural networks. Random feature models and extreme learning machines are typically defined for networks with a single hidden layer. Our sampling scheme accounts for the high-dimensional ambient space that is introduced after this layer, and thus deep networks can be constructed efficiently.

Approximation accuracy. Gradient-based, iterative approximation can find accurate solutions with a relatively small number of neurons. Randomly sampling weights using a data-agnostic distribution often requires thousands of neurons to compete. Our sampling scheme takes into account the given training data points and function values, leading to accurate and width-efficient approximations. The distribution also leads to invariance to orthogonal transformations and scaling of the input data, which makes many common pre-processing techniques redundant.

Interpretability. Sampling weights and biases completely randomly, i.e., without taking into account the given data, leads to networks that do not incorporate any information about the given problem. We analyze and extend a recently introduced weight construction technique [[27]](#b26) that uses the direction between pairs of data points to construct individual weights and biases. In addition, we propose a sampling distribution over these data pairs that leads to efficient use of weights; cf. Figure [1](#).

## Related work

Regarding random Fourier features, Li et al. [[41]](#b40) and Liu et al. [[43]](#b42) review and unify theory and algorithms of this approach. Random features have been used to approximate input-output maps in Banach spaces [[50]](#b49) and solve partial differential equations [[16,](#b15)[48,](#b47)[10]](#b9). Gallicchio and Scardapane [[28]](#b27) provide a review of deep random feature models, and discuss autoencoders and reservoir computing (resp. echo-state networks [[34]](#b33)). The latter are randomly sampled, recurrent networks to model dynamical systems [[6]](#b5). Regarding construction of features, Monte Carlo approximation of data-dependent parameter distributions is used towards faster kernel approximation [[1,](#b0)[59,](#b58)[47]](#b46). Our work differs in that we do not start with a kernel and decompose it into random features, but we start with a practical and interpretable construction of random features and then discuss their approximation properties. This may also help to construct activation functions similar to collocation [[62]](#b61). Fiedler et al. [[23]](#b22) and Fornasier et al. [[24]](#b23) prove that for given, comparatively small networks with one hidden layer, all weights and biases can be recovered exactly by evaluating the network at specific points in the input space. The work of Spek et al. [[60]](#b59) showed a certain duality between weight spaces and data spaces, albeit in a purely theoretical setting. Recent work from Bollt [[7]](#b6) analyzes individual weights in networks by visualizing the placement of ReLU activation functions in space. Regarding approximation errors and convergence rates of networks, Barron spaces are very useful [[2,](#b1)[20]](#b19), also to study regularization techniques, esp. Tikohnov and Total Variation [[40]](#b39). A lot of work [[54,](#b53)[19,](#b18)[17,](#b16)[57,](#b56)[65]](#b64) surrounds the approximation rate of O(m -1/2 ) for neural networks with one hidden layer of width m, originally proved by Barron [[2]](#b1). The rate, but not the constant, is independent of the input space dimension. This implies that neural networks can mitigate the curse of dimensionality, as opposed to many approximation methods with fixed, non-trained basis functions [[14]](#b13), including random feature models with data-agnostic probability distributions. The convergence rates of over-parameterized networks with one hidden layer is considered in [[19]](#b18), with a comparison to the Monte Carlo approximation. In our work, we prove the same convergence rate for our networks. Regarding deep networks, E and Wojtowytsch [[17,](#b16)[18]](#b17) discuss simple examples that are not Barron functions, i.e., cannot be represented by shallow networks. Shallow [[15]](#b14) and deep random feature networks [[29]](#b28) have also been analyzed regarding classification accuracy. Regarding different sampling techniques, Bayesian neural networks are prominent examples [[49,](#b48)[5,](#b4)[26,](#b25)[61]](#b60). The goal is to learn a good posterior distribution and ultimately express uncertainty around both weights and the output of the network. These methods are computationally often on par with or worse than iterative optimization. In this work, we directly relate data points and weights, while Bayesian neural networks mostly employ distributions only over the weights. Generative modeling has been proposed as a way to sample weights from existing, trained networks [[56,](#b55)[51]](#b50). It may be interesting to consider our sampled weights as training set in this context. In the lottery ticket hypothesis [[25,](#b24)[8]](#b7), "winning" subnetworks are often not trained, but selected from a randomly initialized starting network, which is similar to our approach. Still, the score computation during selection requires gradient updates. Most relevant to our work is the weight construction method by Galaris et al. [[27]](#b26), who proposed to use pairs of data points to construct weights. Their primary goal was to randomly sample weights that capture low-dimensional structures. No further analysis was provided, and only a uniform distribution over the data pairs was used. We expand and analyze their setting here.

## Mathematical framework

We introduce sampled networks, which are neural networks where each pair of weight and bias of all hidden layers is completely determined by two points from the input space. This duality between weights and data has been shown theoretically [[60]](#b59), here, we provide an explicit relation. The weights are constructed using the difference between the two points, and the bias is the inner product between the weight and one of the two points. After all hidden layers are constructed, we must only solve an optimization problem for the coefficients of a linear layer, mapping the output from the last hidden layer to the final output. We start to formalize this construction by introducing some notation.

Let X ⊆ R D be the input space with ∥•∥ being the Euclidean norm with inner product ⟨•, •⟩. Further, let Φ be a neural network with L hidden layers, parameters {W l , b l } L+1 l=1 , and activation function ϕ : R → R. For x ∈ X , we write Φ (l) (x) = ϕ(W l Φ (l-1) (x) -b l ) as the output of the lth layer, with Φ (0) (x) = x. The two activation functions we focus on are the rectified linear unit (ReLU), ϕ(x) = max{x, 0}, and the hyperbolic tangent (tanh). We set N l to be the number of neurons in the lth layer, with N 0 = D and N L+1 as the output dimension. We write w l,i for the ith row of W l and b l,i for the ith entry of b l . Building upon work of Galaris et al. [[27]](#b26), we now introduce sampled networks. The probability distribution to sample pairs of data points is arbitrary here, but we will refine it in Definition 2. We use L to denote the loss of our network we would like to minimize. Definition 1. Let Φ be a neural network with L hidden layers. For l = 1, . . . , L, let (x

$(1) 0,i , x (2) 0,i ) N l i=1$be pairs of points sampled over X × X . We say Φ is a sampled network if the weights and biases of every layer l = 1, 2, . . . , L and neurons i = 1, 2, . . . , N l , are of the form

$w l,i = s 1 x (2) l-1,i -x (1) l-1,i ∥x (2) l-1,i -x(1)$$l-1,i ∥ 2 , b l,i = ⟨w l,i , x(1)$$l-1,i ⟩ + s 2 ,(1)$where

$s 1 , s 2 ∈ R are constants, x(j)$l-1,i = Φ (l-1) (x

$(j) 0,i ) for j = 1, 2,and x (1)$$l-1,i ̸ = x(2)$l-1,i . The last set of weights and biases are

$W L+1 , b L+1 = arg min L(W L+1 Φ (L) (•) -b L+1 ).$The constants s 1 , s 2 are used to fix what values the activation function takes on when it is applied to the points x (1) , x (2) ; cf. Figure [2](#). For ReLU, we set s 1 = 1 and s 2 = 0, so that ϕ x (1) = 0 and ϕ x (2) = 1. For tanh, we set s 1 = 2s 2 and s 2 = ln(3)/2, which implies ϕ x (1) = 1/2 and ϕ x (2) = -1/2, respectively, and ϕ 1/2 x (1) + x (2) = 0. This means that in a regression problem with ReLU, we linearly interpolate values between the two points. For classification, the tanh construction introduces a boundary if x (1) belongs to a different class than x (2) . We will use this idea later to define a useful distribution over pairs of points (cf. Definition 2).

Figure [2](#): Placement of the point pairs x (1) , x (2) for activation functions ReLU (left) and tanh (right). Two data pairs are chosen in each subfigure, resulting in two activation functions on each data domain.

The space of functions that sampled networks can approximate is not immediately clear. First, we are only using points in the input space to construct both the weights and the biases, instead of letting them take on any value. Second, there is a dependence between the bias and the direction of the weight. Third, for deep networks, the sampling space changes after each layer. These apparent restrictions require investigation into which functions we can approximate. We assume that the input space in Theorem 1 and Theorem 2 extends into its ambient space R D as follows. Let X ′ be any compact subset of R D with finite reach τ > 0. Informally, such a set has a boundary that does not change too quickly [[12]](#b11). We then set the input space X to be the space of points including X ′ and those that are at most ϵ I away from X ′ , given the canonical distance function in R D , where 0 < ϵ I < τ . In Theorem 1, we also consider L layers to show that the construction of weights in deep networks does not destroy the space of functions that networks with one hidden layer can approximate, even though we alter the space of weights we can construct when L > 1.

Theorem 1. For any number of layers L ∈ N >0 , the space of sampled networks with L hidden layers, Φ : X → R N L+1 , with activation function ReLU, is dense in C(X , R N L+1 ).

## Sketch of proof:

We show that the possible biases that sampled networks can produce are all we need inside a neuron, and the rest can be added in the last linear part and with additional neurons. We then show that we can approximate any neural network with one hidden layer with at most 6 • N 1 neurons -which is not much, considering the cost of sampling versus backpropagation. We then show that we can construct weights so that we preserve the information of X through the first L -1 layers, and then we use the arbitrary width result applied to the last hidden layer. The full proof can be found in Appendix A.1. We also prove a similar theorem for networks with tanh activation function and one hidden layer. The proof differs fundamentally, because tanh is not positive homogeneous.

We now show existence of sampled networks for which the L 2 approximation error of Barron functions is bounded (cf. Theorem 2). We later demonstrate that we can actually construct such networks (cf. Section 4.1). The Barron space [[2,](#b1)[20]](#b19) is defined as

$B = {f : f (x) = Ω w 2 ϕ(⟨w 1 , x⟩ -b)dµ(b, w 1 , w 2 ) and ∥f ∥ B < ∞} with ϕ being the ReLU function, Ω = R × R D × R,$and µ being a probability measure over Ω. The Barron norm is defined as

$∥f ∥ B = inf µ max (b,w1,w2)∈supp(µ) {|w 2 |(∥w 1 ∥ 1 + |b|)}.$Theorem 2. Let f ∈ B and X = [0, 1] D . For any N 1 ∈ N >0 , ϵ > 0, and an arbitrary probability measure π, there exist sampled networks Φ with one hidden layer, N 1 neurons, and ReLU activation function, such that

$∥f -Φ∥ 2 2 = X |f (x) -Φ(x)| 2 dπ(x) < (3 + ϵ)∥f ∥ 2 B N 1 .$Sketch of proof: It quickly follows from the results of E et al. [[20]](#b19), which showed it for regular neural networks, and Theorem 1. The full proof can be found in Appendix A.2.

Up until now, we have been concerned with the space of sampling networks, but not with the distribution of the parameters. We found that putting emphasis on points that are close and differ a lot with respect to the output of the true function works well. As we want to sample layers sequentially, and neurons in each layer independently from each other, we define a layer-wise conditional definition underneath. The norms ∥•∥ X l-1 and ∥•∥ Y , that defines the following densities, are arbitrary over their respective space, denoted by the subscript.

Definition 2. Let f : R D → R N L+1 be Lipschitz-continuous and set Y = f (X ). For any l ∈ {1, 2, . . . , L}, setting ϵ = 0 when l = 1 and otherwise ϵ > 0, we define

$q ϵ l x (1) 0 , x(2)$$0 | {W j , b j } l-1 j=1 =      ∥f (x (2) 0 ) -f (x (1) 0 )∥ Y max{∥x (2) l-1 -x (1) l-1 ∥ X l-1 , ϵ} , x(1)$$l-1 ̸ = x (2) l-1 0, otherwise,(2)$where x

(1)

$0 , x (2) 0 ∈ X , x(1)$$l-1 = Φ (l-1) (x(1)$0 ), and x

(2)

$l-1 = Φ (l-1) (x(2)$0 ), with the network Φ (l-1) parameterized by sampled {W j , b j } l-1 j=1 . Then, using λ as the Lebesgue measure, we define the integration constant C l = X ×X q ϵ l dλ. The density p ϵ l to sample pairs of points for weights and biases in layer l is equal to q ϵ l /C l if C l > 0, and uniform over X × X otherwise.

Note that here, a distribution over pair of points is equivalent to a distribution over weights and biases, and the additional ϵ is a regularization term. Now we can sample for each layer sequentially, starting with l = 1, using the conditional density p ϵ l . This induces a probability distribution P over the full parameter space, which, with the given regularity conditions on X and f , is a valid probability distribution. For a complete definition of P and proof of validity, see Appendix A.3.

Using this distribution also comes with the benefit that sampling and training are not affected by rigid body transformations (affine transformation with orthogonal matrix) and scaling, as long as the true function f is equivariant w.r.t. to the transformation. That is, if H is such a transformation, we say f is equivariant with respect to H, if there exists a scalar and rigid body transformation H ′ such that H ′ (f (x)) = f (H(x)) for all x ∈ X , and invariant if H ′ is the identity function. We also assume that norms ∥•∥ Y and ∥•∥ X0 in Equation ( [2](#formula_6)) are chosen such that orthogonal matrix multiplication is an isometry. Theorem 3. Let f be the target function and equivariant w.r.t. to a scalar and rigid body transformation H. If we have two sampled networks, Φ, Φ, with the same number of hidden layers L and neurons N 1 , . . . , N L , where Φ : X → R N L+1 and Φ : H(X ) → R N L+1 , then the following statements hold for all x ∈ X :

(1) If

x(1)

$0,i = H(x(1)$0,i ) and x(2)

$0,i = H(x (2) 0,i ) for all i = 1, 2, . . . , N 1 , then Φ (1) (x) = Φ(1) (H(x)).$(2) If f is invariant w.r.t. H, then for any parameters of Φ, there exist parameters of Φ such that Φ(x) = Φ(H(x)), and vice versa.

(3) The probability measure P over the parameters is invariant under H.

## Sketch of proof:

Any neuron in the sampled network can be written as ϕ(⟨s 1 w ∥w∥ 2 , x -x (1) ⟩ -s 2 ). As we divide by the square of the norm of w, the scalar in H cancels. There is a difference between two points in both inputs of ⟨•, •⟩, which means the translation cancels. Orthogonal matrices cancel due to isometry. When f is invariant with respect to H, the loss function is also unchanged and lead to the same output. Similar argument is made for P , and the theorem follows (cf. Appendix A.3).

If the input is embedded in a higher-dimensional ambient space R D ′ , with D < D ′ , we sample from a subspace with dimension D = dim(span{X }) ≤ D ′ . All the results presented in this section still hold due to orthogonal decomposition. However, the standard approach of backpropagation and initialization allows the weights to take on any value in R D ′ , which implies a lot of redundancy when D ≪ D ′ . The biases are also more relevant to the input space than when initialized to zero -potentially avoiding the issues highlighted by Holzmüller and Steinwart [[32]](#b31). For these reasons, we have named the proposed method Sampling Where It Matters (SWIM), which is summarized in Algorithm 1. For computational reasons, we consider a random subset of all possible pairs of training points when sampling weights and biases.

We end this section with a time and memory complexity analysis of Algorithm 1. In Table [1](#), we list runtime and memory usage for three increasingly strict assumptions. The main assumption is that the dimension of the output is less than or equal to the largest dimension of the hidden layers. This is true for the problems we consider, and the difference in runtime without this assumption is only reflected in the linear optimization part. The term ⌈N/M ⌉, i.e., integer ceiling of N/M , is required because only a subset of points are considered when sampling. For the full analysis, see Appendix F.

## Numerical experiments

We now demonstrate the performance of Algorithm 1 on numerical examples. Our implementation is based on the numpy and scipy Python libraries, and we run all experiments on a machine with 32GB system RAM (256GB in Section 4.3 and Section 4.4) and a GeForce 4x RTX 3080 Turbo GPU with 10GB RAM. The Appendix contains detailed information on all experiments. In Section 4.1 we compare sampling to random Fourier feature models regarding the approximation of a Barron function. In Section 4.2 we compare classification accuracy of sampled networks to iterative, gradientbased optimization in a classification benchmark with real datasets. In Section 4.3 we demonstrate that more specialized architectures can be sampled, by constructing deep neural architectures as PDE solution operators. In Section 4.4 we show how to use sampling of fully-connected layers for transfer learning. For the probability distribution over the pairs in Algorithm 1, we always choose the L ∞ norm for ∥•∥ Y and for l = 1, 2, . . . , L, we choose the L 2 norm for ∥•∥ X l-1 . The code to reproduce the experiments from the paper, and an up-to-date code base, can be found at [https://gitlab.com/felix.dietrich/swimnetworks-paper](https://gitlab.com/felix.dietrich/swimnetworks-paper), [https://gitlab.com/felix.dietrich/swimnetworks](https://gitlab.com/felix.dietrich/swimnetworks).

Algorithm 1: The SWIM algorithm, for activation function ϕ, and norms on input, output of the hidden layers, and output space, ∥•∥ X0 ,∥•∥ X l , and ∥•∥ Y respectively. Also, L is a loss function, which in our case is always L 2 loss, and arg min L(•, •) becomes a linear optimization problem.

$Constant :ϵ ∈ R >0 , ς ∈ N >0 , L ∈ N >0 , {N l ∈ N >0 } L+1 l=1 , and s 1 , s 2 ∈ R Data: X = {x i : x i ∈ R D , i = 1, 2, . . . , M }, Y = {y i : f (x i ) = y i ∈ R N L+1 , i = 1, 2, . . . , M } Φ (0) (x) = x; for l = 1, 2, . . . , L do M ← ς • ⌈ N l M ⌉ • M ; P (l) ∈ R M ; P (l) i ← 0 ∀i; X = {(x (1) i , x (2) i ) : Φ (l-1) (x (1) i ) ̸ = Φ (l-1) (x (2) i )} M i=1 ∼ Uniform(X × X); for i = 1, 2, . . . , M do x(1) i , x(2)$$i ← Φ (l-1) (x(1)$i ), Φ (l-1) (x

$(2) i ); ỹ(1) i , ỹ(2)$$i = f (x (1) i ), f (x(2)$i );

$P (l) i ← ∥ỹ (2) i -ỹ (1) i ∥ Y max{∥x (2) i -x (1) i ∥ X l-1 ,ϵ} ; end W l ∈ R N l ,N l-1 , b l ∈ R N l ; for k = 1, 2, . . . , N l do$Sample (x (1) , x (2) ) from X, with replacement and with probability proportional to P (l) ;

x(1) , x(2) ← Φ (l-1) (x (1) ), Φ (l-1) (x (2) );

$W (k,:) l ← s 1 x(2) -x (1) ∥x (2) -x (1) ∥ 2 ; b (k) l ← ⟨W (k,:) l , x(1) ⟩ + s 2 ; end Φ (l) (•) ← ϕ(W l Φ (l-1) (•) -b l ); end W L+1 , b L+1 ← arg min L(Φ (L) (X), Y ); return {W l , b l } L+1 l=1$Table [1](#): Runtime and memory requirements for training sampled neural networks with the SWIM algorithm, where N = max{N 0 , N 1 , N 2 , . . . , N L }. Assumption (I) is that the output dimension is less than or equal to N . Assumption (II) adds that N < M 2 , i.e., number of neurons and input dimension is less than the size of dataset squared. Assumption (III) requires a fixed architecture.

## Runtime

Memory

$Assumption (I) O(L • M (min{⌈N/M ⌉, M } + N 2 )) O(M • min{⌈N/M ⌉, M } + LN 2 ) Assumption (II) O(L • M (⌈N/M ⌉ + N 2 )) O(M • ⌈N/M ⌉ + LN 2 ) Assumption (III) O(M ) O(M )$
## Illustrative example: approximating a Barron function

We compare random Fourier features and our sampling procedure on a test function for neural networks [[64]](#b63): f (x) = 3/2(∥x -a∥ -∥x + a∥), with x ∈ R D and the vector a ∈ R D defined by a j = 2j/D -1, j = 1, 2, . . . , D. The Barron norm of f is equal to one for all input dimensions, and it can be represented exactly with a network with one infinitely wide hidden layer, ReLU activation, and weights uniformly distributed on a sphere of radius D 1/2 . We approximate f using networks f of up to three hidden layers. The error is defined by e 2 rel = x∈X (f (x) -f (x)) 2 / x∈X f (x) 2 . We compare this error over the domain X = [-1, 1] 2 , with 10, 000 points sampled uniformly, separately for training and test sets. For random features, we use w ∼ N (0, 1), b ∼ U (-π, π), as proposed in [[54]](#b53), and ϕ = sin. For sampling, we also use ϕ = sin to obtain a fair comparison. We also observed similar accuracy results when repeating the experiment with the tanh function. The number of neurons m is the same in each hidden layer and ranges from m = 64 up to m = 4096. Figure [3](#fig_0) shows results for D = 5, 10 (results are similar for D = 2, 3, 4, and sampled networks can be constructed as fast as the random feature method, cf. Appendix B). Random features here have

10 2 10 3 Network width 10 3 10 2 10 1 10 0 Rel. L 2 error, dim=5 randomfeatures, L=1 sampling, L=1 randomfeatures, L=2 sampling, L=2 randomfeatures, L=3 sampling, L=3 reference m 1/2 reference m 1 10 2 10 3 Network width 10 3 10 2 10 1 10 0 Rel. L 2 error, dim=10 randomfeatures, L=1 sampling, L=1 randomfeatures, L=2 sampling, L=2 randomfeatures, L=3 sampling, L=3 reference m 1/2 reference m 1 comparable accuracy for networks with one hidden layer, but very poor performance for deeper networks. This may be explained by the much larger ambient space dimension of the data after it is processed through the first hidden layer. With our sampling method, we obtain accurate results even with more layers. The convergence rate for D < 10 seems to be faster than the theoretical rate.

## Classification benchmark from OpenML

We use the "OpenML-CC18 Curated Classification benchmark" [[4]](#b3) with all its 72 tasks to compare our sampling method to the Adam optimizer [[36]](#b35). With both methods, we separately perform neural architecture search, changing the number of hidden layers from 1 to 5. All layers always have 500 neurons. Details of the training are in Appendix C. Figure [4](#fig_1) shows the benchmark results. On all tasks, sampling networks is faster than training them iteratively (on average, 30 times faster). The classification accuracy is comparable (cf. Figure [4](#fig_1), second and third plot). The best number of layers for each problem is slightly higher for the Adam optimizer (cf. Figure [4](#fig_1), fourth plot).

2 1 0 1 2 Fit time (log 10) 0 2 4 6 8 10 12 14 16 18 Count Adam Sampling 0.0 0.2 0.4 0.6 0.8 1.0 Accuracy Adam 0.0 0.2 0.4 0.6 0.8 1.0 Accuracy Sampling 0.2 0.0 0.2 Accuracy (Adam -Sampling) 0 2 4 6 8 10 12 14 16 18 Count Count Average 4 2 0 2 4 Layer difference (Adam -Sampling) 0 2 4 6 8 10 12 14 16 18 Count 

## Deep neural operators

We sample deep neural operators and compare their speed and accuracy to iterative gradient-based training of the same architectures. As a test problem, we consider Burgers' equation, ∂u ∂t + u ∂u ∂x = ν ∂ 2 u ∂ 2 x , x ∈ (0, 2π), t ∈ (0, 1], with periodic boundary conditions and viscosity ν = 0.1. The goal is to predict the solution at t = 1 from the initial condition at t = 0. Thus, we construct neural operators that represent the map G : u(x, 0) → u(x, 1). We generate initial conditions by sampling five Fourier coefficients of lowest frequency and restoring the function values from these coefficients. Using a classical numerical solver, we generate 15000 pairs of (u(x, 0), u(x, 1)), and split them into the train (60%), validation (20%), and test sets (20%). Figure [5](#fig_2) shows samples from the generated dataset.

## Fully-connected network in signal space

The first baseline for the task is a fully-connected network (FCN) trained with tanh activation to predict the discretized solution from the discretized initial condition. We trained the classical version using the Adam optimizer and the mean squared error as a loss function. We also performed early stopping based on the mean relative L 2 -error on the validation set. For sampling, we use Algorithm 1 to construct a fully-connected network with tanh as the activation function.

## Fully-connected network in Fourier space

Similarly to Poli et al. [[53]](#b52), we train a fully-connected network in Fourier space. For training, we perform a Fourier transform on the initial condition and the solution, keeping only the lowest frequencies. We always split complex coefficients into real and imaginary parts, and train a standard FCN on the transformed data. The reported metrics are in signal space, i.e., after inverse Fourier transform. For sampling, we perform exactly the same pre-processing steps.

## POD-DeepONet

The third architecture considered here is a variation of a deep operator network (DeepONet) architecture [[44]](#b43). The original DeepONet consists of two trainable components: the trunk net, which transforms the coordinates of an evaluation point, and the branch net, which transforms the function values on some grid. The outputs of these nets are then combined into the predictions of the whole network G(u)(y) = p k=1 b k (u)t k (y) + b 0 , where u is a discretized input function; y is an evaluation point; [t 1 , . . . , t p ] T ∈ R p are the p outputs of the trunk net; [b 1 , . . . , b p ] T ∈ R p are the p outputs of the branch net; and b 0 is a bias. DeepONet sets no restrictions on the architecture of the two nets, but often fully-connected networks are used for one-dimensional input. POD-DeepONet proposed by Lu et al. [[46]](#b45) first assumes that evaluation points lie on the input grid. It performs proper orthogonal decomposition (POD) of discretized solutions in the train data and uses its components instead of the trunk net to compute the outputs

$G(u)(ξ j ) = p k=1 b k (u)ψ k (ξ j ) + ψ 0 (ξ j ).$Here [ψ 1 (ξ j ), . . . , ψ p (ξ j )] are p precomputed POD components for a point ξ j , and ψ 0 (ξ j ) is the mean of discretized solutions evaluated at ξ j . Hence, only the branch net is trained in POD-DeepONet. We followed Lu et al. [[46]](#b45) and applied scaling of 1/p to the network output. For sampling, we employ orthogonality of the components and turn POD-DeepONet into a fully-connected network. Let ξ = [ξ 1 , . . . , ξ n ] be the grid used to discretize the input function u and evaluate the output function G(u). Then the POD components of the training data are

$Ψ(ξ) = [ψ 1 (ξ), . . . , ψ p (ξ)] ∈ R n×p . If b(u) ∈ R p is the output vector of the trunk net, the POD-DeepONet transformation can be written G(u)(ξ) = Ψb(u) + ψ 0 (ξ). As Ψ T Ψ = I p , we can express the output of the trunk net as b(u) = Ψ T (G(u)(ξ) -ψ 0 (ξ)).$Using this equation, we can transform the training data to sample a fully-connected network for b(u). We again use tanh as the activation function for sampling.

## Fourier Neural Operator

The concept of a Fourier Neural Operator (FNO) was introduced by Li et al. [[42]](#b41) to represent maps in function spaces. An FNO consists of Fourier blocks, combining a linear operator in the Fourier space and a skip connection in signal space. As a first step, FNO lifts an input signal to a higher dimensional channel space. Let v t ∈ R n×dv be an input to a Fourier block having d v channels and discretized with n points. Then, the output v t+1 of the Fourier block is computed as

$v t+1 = ϕ(F -1 k (R•F k (v t ))+W •v t ) ∈ R n×dv .$Here, F k is a discrete Fourier transform keeping only Model width layers mean rel. L 2 error Time Adam FCN; signal space 1024 2 4.48 × 10 -3 644s GPU FCN; Fourier space 1024 1 3.29 × 10 -3 1725s POD-DeepONet 2048 4 1.62 × 10 -3 4217s FNO n/a 4 0.38 × 10 -3 3119s Sampling FCN; signal space 4096 1 0.92 × 10 -3 20s CPU FCN; Fourier space 4096 1 1.08 × 10 -3 16s POD-DeepONet 4096 1 0.85 × 10 -3 21s FNO 4096 1 0.94 × 10 -3 387s the k lowest frequencies and F -1 k is the corresponding inverse transform; ϕ is an activation function; • is a spectral convolution; • is a 1 × 1 convolution with bias; and R ∈ C k×dv×dv , W ∈ R dv×dv are learnable parameters. FNO stacks several Fourier blocks and then projects the output signal to the target dimension. The projection and the lifting operators are parameterized with neural networks. For sampling, the construction of convolution kernels is not possible yet, so we cannot sample FNO directly. Instead, we use the idea of FNO to construct a neural operator with comparable accuracy. Similar to the original FNO, we normalize the input data and append grid coordinates to it before lifting. Then, we draw the weights from a uniform distribution on [-1, 1] to compute the 1 × 1 lifting convolution. We first apply the Fourier transform to both input and target data, and then train a fully-connected network for each channel in Fourier space. We use skip connections, as in the original FNO, by removing the input data from the lifted target function during training, and then add it before moving to the output of the block. After sampling and transforming the input data with the sampled networks, we apply the inverse Fourier transform. After the Fourier block(s), we sample a fully-connected network that maps the signal to the solution.

The results of the experiments in Figure [5](#fig_2) show that sampled models are comparable to the Adamtrained ones. The sampled FNO model does not directly follow the original FNO architecture, as we are able to only sample fully-connected layers. This shows the advantage of gradient-based methods: as of now, they are applicable to much broader use cases. These experiments showcase one of the main advantages of sampled networks: speed of training. We run sampling on the CPU; nevertheless, we see a significant speed-up compared to Adam training performed on GPU.

## Transfer learning

Training deep neural networks from scratch involves finding a suitable neural network architecture [[21]](#b20) and hyper-parameters [[3,](#b2)[66]](#b65). Transfer learning aims to improve performance on the target task by leveraging learned feature representations from the source task. This has been successful in image classification [[35]](#b34), multi-language text classification, and sentiment analysis [[63,](#b62)[9]](#b8). Here, we compare the performance of sampling with iterative training on an image classification transfer learning task. We choose the CIFAR-10 dataset [[39]](#b38), with 50000 training and 10000 test images. Each image has dimension 32 × 32 × 3 and must be classified into one of the ten classes. We consider ResNet50 [[31]](#b30), VGG19 [[58]](#b57), and Xception [[11]](#b10), all pre-trained on the ImageNet dataset [[55]](#b54). We freeze the weights of all convolutional layers and append one fully connected hidden layer and one output layer. We refer to these two layers as the classification head, which we sample with Algorithm 1 and compare the classification accuracy against iterative training with the Adam optimizer.

Figure [6](#fig_4) (left) shows that for a pre-trained ResNet50, the test accuracy using the sampling approach is higher than the Adam training approach for a width greater than 1024. We observe similar qualitative behavior for VGG19 and Xception (figures in Appendix E). Figure [6](#fig_4) (middle) shows that the sampling approach results in a higher test accuracy with all three pre-trained models. Furthermore, the deviation in test accuracy obtained with the sampling algorithm is very low, demonstrating that sampling is more robust to changing random seeds than iterative training. After fine-tuning the whole neural network with the Adam optimizer with a learning rate of 10 -5 , the test accuracies of sampled networks are close to the iterative approach. Thus, sampling provide a good starting point for fine-tuning the entire model. A comparison for the three models before and after fine-tuning is contained in Appendix E. Figure [6](#fig_4) (right) shows that sampling is up to two orders of magnitude faster than iterative training for  smaller widths, and around ten times faster for a width of 2048. In summary, Algorithm 1 is much faster than iterative training, yields a higher test accuracy for certain widths before fine-tuning, and is more robust with respect to changing random seeds. The sampled weights also provide a good starting point for fine-tuning of the entire model.

## Broader Impact

Sampling weights through data pairs at large gradients of the target function offers improvements over random feature models. In terms of accuracy, networks with relatively large widths can even be competitive to iterative, gradient-based optimization. Constructing the weights through pairs of points also allows to sample deep architectures efficiently. Sampling networks offers a straightforward interpretation of the internal weights and biases, namely, which data pairs are important. Given the recent critical discussions around fast advancement in artificial intelligence, and calls to slow it down, publishing work that potentially speeds up the development (concretely, training speed) in this area by orders of magnitude may seem irresponsible. The solid mathematical underpinning of random feature models and, now, sampled networks, combined with much greater interpretability of the individual steps during network construction, should mitigate some of these concerns.

## Conclusion

We present a data-driven sampling method for fully-connected neural networks that outperforms random feature models in terms of accuracy, and in many cases is competitive to gradient-based optimization. The time to obtain a trained network is orders of magnitude faster compared to gradientbased optimization. In addition, much fewer hyperparameters need to be optimized, as opposed to learning rate, number of training epochs, and type of optimizer.

Several open issues remain, we list the most pressing here. Many architectures like convolutional or transformer networks cannot be sampled with our method yet, and thus must still be trained with iterative methods. Implicit problems, such as the solution to PDE without any training data, are a challenge, as our distribution over the data pairs relies on known function values from a supervised learning setting. Iteratively refining a random initial guess may prove useful here. On the theory side, convergence rates for Algorithm 1 beyond the default Monte-Carlo estimate are not available yet, but are important for robust applications in engineering.

In the future, hyperparameter optimization, including neural architecture search, could benefit from the fast training time of sampled networks. We already demonstrate benefits for transfer learning here, which may be exploited for other pre-trained models and tasks. Analyzing which data pairs are sampled during training may help to understand the datasets better. We did not show that our sampling distribution results in optimal weights, so there is a possibility of even more efficient heuristics. Applications in scientific computing may benefit most from sampling networks, as accuracy and speed requirements are much higher than for many tasks in machine learning.

where x l is defined recursively as x 0 = x ∈ X and

$x l = ϕ(W l x l-1 -b l ), l = 1, 2, . . . , L.(4)$The image after l layers is denoted as X l = Φ (l) (X ).

As we study the space of these networks, we find it useful to introduce the following notation. The space of all neural networks with L hidden layers, with N = [N 1 , . . . , N L ] neurons in the separate layers, is denoted as F L,N , assuming the input dimension and the output dimension are implicitly given. If each hidden layer has N neurons, and we let the input dimension and the output dimension be fixed, we write the space of all such neural networks as F L,N . We then let

$F ∞,N = ∞ L=1 F L,N ,$meaning the set of neural networks with N width, and arbitrary depth. Similarly,

$F L,∞ = ∞ N =1 F L,N ,$for arbitrary width at fixed depth. Finally,

$F ∞,∞ = ∞ L=1 ∞ N =1$F L,N .

We will now introduce sampled networks again, and go into depth the choice of constants. The input space X is a subset of Euclidean space, and we will work with canonical inner products ⟨•, •⟩ and their induced norms ∥•∥ if we do not explicitly specify the norm.

Definition 4. Let Φ be an neural network with L hidden layers. For l = 1, . . . , L, let

$0,i ) N l i=1(x (1) 0,i , x (2)$be pairs of points sampled over X × X . We say Φ is a sampled network if the weights and biases of every layer l = 1, 2, . . . , L and neurons i = 1, 2, . . . , N l , are of the form

$w l,i = s 1 x (2) l-1,i -x (1) l-1,i ∥x (2) l-1,i -x (1) l-1,i ∥ 2 , b l,i = ⟨w l,i , x(1)$$l-1,i ⟩ + s 2(5)$where s 1 , s 2 ∈ R are constants related to the activation function, and x

$(j) l-1,i = Φ (l-1) (x (j) 0,i ) for j = 1, 2, assuming x (1) l-1,i ̸ = x (2)$l-1,i . The last set of weights and biases are

$W L+1 , b L+1 = arg min L(W L+1 Φ (L) (•) -b L+1 )$, where L is a suitable loss.

Remark 1. The constants s 1 , s 2 can be fixed such that for a given activation function, we can specify values at specific points, e.g., we can specify what value to map the two points x (1) and x (2) to; cf. Figure [7](#). Also note that the points we sampled from X × X are sampled such that we have unique points after mapping the two sampled points through the first l -1 layers. This is enforced by constructing the density of the distribution we use to sample the points so that zero density is assigned to points that map to the same output of Φ (l-1) (see Definition 2).

Figure [7](#): Illustration of the placement of point pairs x (1) , x (2) for activation functions ReLU (left) and tanh (right).

Example A.1. We consider the construction of the constants s 1 , s 2 for ReLU and tanh, as they are the two activation functions used for both the proofs and the experiments. We start by considering the activation function tanh, i.e.,

$ϕ(x) = exp 2x -1 exp 2x +1 .$Consider an arbitrary weight w, where we drop the subscripts for simplicity. Setting s 1 = 2 • s 2 , the input of the corresponding activation function, at the mid-point

$x = x (1) + x (2) -x (1) 2 = x (2) +x (1)$
## 2

, is

$⟨w, x⟩ -b = w, x (2) + x (1) 2 -⟨w, x (1) ⟩ -s 2 = w, x (2) -x (1) 2 -s 2 = 2s 2 x (2) -x (1) 2 • 1 2 x (2) -x (1) , x (2) -x (1) -s 2 = s 2 -s 2 = 0.$The output of the neuron corresponding to the input above is then zero, regardless of what the constant s 2 is. Another aspect of the constants are that we can decide activation values for the two sampled points. This can be seen with a similar calculation as above, ⟨w,

$x (1) ⟩ -b = -s 2 .$Letting s 2 = ln 3 2 , we see that the output of the corresponding neuron at point x (1) is

$ϕ(-s 2 ) = exp ln(3) -1 exp ln(3) +1 = - 1 2 ,$and for x (2) we get 1 2 . We can also consider the ReLU activation function where we choose to set s 1 = 1 and s 2 = 0. The center of the real line is then placed at x (1) , and x (2) is mapped to one.

## A.1 Universality of sampled networks

In this section we show that universal approximation results also holds for our sampled networks. We start by considering the ReLU function. To separate it from tanh, which is the second activation function we consider, we denote ϕ to be ReLU and ψ to be tanh. Networks with ReLU as activation function are then denoted by Φ, and Ψ are networks with tanh activation function. When we use ReLU, we set s 1 = 1 and s 2 = 0, as already discussed. We provide the notation that is used throughout Appendix A in Table [2](#).

The rest of this section is structured as follows:

1. We introduce the type of input spaces we are working with. 2. We then aim to rewrite an arbitrary fully connected neural network with one hidden layer into a sampled network. 3. This is done by first constructing neurons that add a constant to parts of the input space while leaving the rest untouched. 4. We then show that we can translate between an arbitrary neural network and a sampled network, if the weights of the former are given. This gives us the first universal approximator result, by combining this step with the former. 5. We go on to construct deep sampled networks with arbitrary width, showing that we can contain the information needed through the first L -1 layers, and then apply the result for sampled networks with one hidden layer, to the last hidden layer. 6. We conclude the section by showing that sampled networks with tanh activation functions are also universal approximators. Concretely, we show that it is dense in the space of sampled networks with ReLU activation function. The reason we need a different proof for the tanh case is that it is not positive homogeneous, a property of ReLU that we heavily depend upon for the proof of sampled networks with ReLU as activation function.

Before we can start with the proof that F 1,∞ is dense in the space of continuous functions, we start by specifying the domain/input space for the functions.

Table 2: Notation used through the theory section of this appendix. X ′ Pre-noise input space. Subset of R D and compact for Appendix A.1 and Appendix A.2. X Input space. Subset of R D , X ′ ⊂ X , and compact for Appendix A.1 and Appendix A.2. ϵ I Noise level for X ′ . Definition 5. ϕ Activation function ReLU, ϕ(x) = max{x, 0}. Φ Neural network with activation function ReLU. ψ Activation function tanh. Ψ Neural network with activation function tanh.

Φ c Constant block. 5 neurons that combined add a constant value c to parts of the input space X , while leaving the rest untouched. Definition 6. N l Number of neurons in layer l of a neural network. N 0 = D and N L+1 is the output dimension of network. w l,i The weight at the ith neuron, of the lth layer. b l,i The bias at the ith neuron, of the lth layer. Φ (l) Function that maps x ∈ X to the output of the lth hidden layer. Φ (l,i) The ith neuron for the lth layer.

X l The image of X after l layers, i.e., X l = Φ(X ). We set X 0 = X .

x l x l = Φ (l) (x). F L,[N1,N2,...,N L ] Space of neural networks with L hidden layers and N l neurons in layer l.

F 1,∞ Space of all neural networks with one hidden layer, i.e., networks with arbitrary width.

$F S 1,∞$Space of all sampled networks with one hidden layer and arbitrary width. ζ Function that maps points from X to X ′ , mapping to the closest point in X ′ , which is unique.

$η Maps x ∈ X × [0, 1] to X , by η(x, δ) = x + δ(ζ(x) -x). ∥•∥ Canonical norm of R D , with inner product ⟨•⟩. B ϵ (x) The ball B ϵ (x) = {x ′ ∈ R D : ∥x -x ′ ∥ < ϵ}. B ϵ (x) The closed ball B ϵ (x) = {x ′ ∈ R D : ∥x -x ′ ∥ ≤ ϵ}.$τ Reach of the set X ′ . λ Lebesgue measure.

## A.1.1 Input space

We again assume that the input space X is a subset of R D . We also assume that X contains some additional additive noise. Before we can define this noisy input space, we recall two concepts.

Let (Z, d) be a metric space, and the distance between a subset A ⊆ Z and a point z ∈ Z be defined as

$d Z (z, A) = inf{d(z, a) : a ∈ A}.$If Z is also a normed space, the medial axis is then defined as

$Med(A) = {z ∈ Z : ∃p ̸ = q ∈ A, ∥p -z∥ = ∥q -z∥ = d Z (z, A)} The reach of A is defined as τ A = inf a∈A d Z (a, Med(A)).$Informally, the reach is the smallest distance from a point in the subset A to a non-unique projection of it in the complement A c . This means that the reach of convex subsets is infinite (all projections are unique), while other sets can have zero reach, which means 0 ≤ τ A ≤ ∞.

Let Z = R D , and d be the canonical Euclidean distance. Definition 5. Let X ′ be a nonempty compact subset of R D with reach τ X ′ > 0. The input space X is defined as

$X = {x ∈ R D : d Z (x, X ′ ) ≤ ϵ I },$where 0 < ϵ I < min{τ X ′ , 1}. We refer to X ′ as the pre-noise input space.

Remark 2. As d Z (x, X ′ ) = 0 for all x ∈ X ′ , we have that X ′ ⊂ X . Due to d Z (x, X ′ ) ≤ ϵ I we preserve that X is closed, and as ϵ I < ∞ means it is bounded, and hence compact. Informally, we have enlarged the input space with new points at most ϵ I distance away from X ′ . This preserves many properties of the pre-noise input space X ′ , while also being helpful in the proofs to come. We also argue that in practice, we are often faced with "noisy" datasets X anyway, rather than non-perturbed data X ′ .

We also define a function that maps elements in X \ X ′ down to X ′ , using the uniqueness property given by the medial axis. That is, ζ : X → X ′ is the mapping defined as ζ(x) = x ′ , for x ∈ X and x ′ ∈ X ′ , such that d(x, x ′ ) = d Z (x, X ′ ). As we also want to work along the line between these two points, we set η : X × [0, 1] → X , where η(x, δ) = x + δ(ζ(x) -x). We conclude this part with an elementary result. Lemma 1. Let x ∈ X , δ ∈ [0, 1], and x ′ = η(x, δ) ∈ X , then ∥x ′ -x∥ ≤ δ, with strict inequality when δ > 0. Furthermore, when δ > 0, we have x ′ / ∈ ∂X .

Proof. We start by noticing

$d(ζ(x), x ′ ) = ∥ζ(x) -x ′ ∥ = (1 -δ)∥ζ(x) -x∥ ≤ (1 -δ)ϵ I ≤ ϵ I ,$which means x ′ ∈ X . When δ > 0, we have strict inequality, which implies

$x ′ ∈ int X = X \ ∂X . Furthermore, d(x, x ′ ) = ∥x -x ′ ∥ = δ∥ζ(x) -x∥ ≤ δ • ϵ I ≤ δ,$where the last inequality holds due to ϵ I < 1, and is equal only when δ = 0.

## A.1.2 Networks with a single hidden layer

We can now proceed to the proof that sampled networks with one hidden layer are indeed universal approximators. The main idea is to start off with an arbitrary network with a single hidden layer, and show that we can approximate this network arbitrarily well. Then we can rely on previous universal approximation theorems [[13,](#b12)[33,](#b32)[52]](#b51) to finalize the proof. We start by showing some results for a different type of neural networks, but very similar in form. We consider networks where the bias is of the same form as a sampled network. However, the weight is normalized to be a unit weight, that is, divided by the norm, and not the square of the norm. We show in Lemma 4 that the results also hold for any positive scalar multiple of the unit weight, and therefore our sampled network, where we divide by the norm of the weight squared. To be more precise, the weights are of the form

$w l,i = x (2) l-1,i -x (1) l-1,i ∥x(2)$l-1,i -x

l-1,i ∥ , and biases b l,i = ⟨w l,i , x

l-1,i ⟩. Networks with weights/biases in the hidden layers of this form is referred to as unit sampled network. In addition, when the output dimension is N L+1 = 1, we split the bias of the last layer, b L+1 into N L parts, to make the proof easier to follow. This is of no consequence for the final result, as we can always sum up the parts to form the original bias. We write the different parts of the split as b L+1,i for i = 1, 2, . . . , N L .

We start by defining a constant block. This is crucial for handling the bias, as we can add constants to the output of certain parts of the input space, while leaving the rest untouched. This is important when proving Lemma 2. Definition 6. Let c 1 < c 2 < c 3 , and c ∈ R + . A constant block Φ c is defined as five neurons summed together as follows. For x ∈ R,

$Φ c (x) = 5 i=1 f i (x),$where

$f 1 (x) = a 1 ϕ(x -c 2 ), f 2 (x) = a 1 ϕ(-(x -c 3 )), f 3 (x) = -a 1 ϕ(x -c 3 ), f 4 (x) = -a 2 ϕ(-(x -c 1 )) f 5 (x) = a 3 ϕ(x -c 1 ),$and

$a 1 = c c3-c2 , a 2 = a 1 c1-c3 c1-c2$, and a 3 = a 2 -a 1 .

Remark 3. The function Φ c are constructed using neurons, but can also be written as the continuous function,

$Φ c (x) =    0, x ≤ c 1 a 3 • x + d, c 1 < x ≤ c 2 c, x > c 2 ,$where d = a 1 c 3 + a 2 c 2 . Obviously, if c needs to be negative, one can simply swap the sign on each of the three parameters, a i .

We can see by the construction of Φ c that we might need the negative of some original weight. That is, the input to Φ c is of the form ⟨w 1,i , x⟩, and for f 2 and f 4 , we require ⟨-w 1,i , x⟩. In Lemma 3, we shall see that this is not an issue and that we can construct neurons such that they approximate constant blocks arbitrarily well, as long as c 1 , c 2 , c 3 can be produced by the inner product between a weight and points in X , that is, if we can produce the biases equal to the constants c 1 , c 2 , c 3 .

Let Φ ∈ F 1,K , with parameters { ŵl,i , bl,i } 2,K l=1,i=1 , be an arbitrary neural network. Unless otherwise stated, the weights in this arbitrary network Φ are always nonzero. Sampled networks cannot have zero weights, as the point pairs used to construct weights, both for unit and regular sampled networks, are distinct. However, one can always construct the same output as neurons with zero weights by setting certain weights in W L+1 to zero.

We start by showing that, given all weights of a network, we can construct all biases in a unit sampled network so that the function values agree. More precisely, we want to construct a network with weights ŵl,i , and show that we can find points in X to construct the biases of the form in unit sampled networks, such that the resulting neural network output on X equals exactly the values of the arbitrary network Φ. Lemma 2. Let Φ ∈ F 1,K : X → R N2 . There exists a set of at most 6 • N 1 points x i ∈ X and biases b 2,i ∈ R, such that a network Φ with weights w 1,i = ŵ1,i , w 2,i = ŵ2,i , and biases b 2,i ∈ R, b 1,i = ⟨w 1,i , x i ⟩, for i = 1, 2, . . . , N 1 , satisfies Φ(x) -Φ(x) = 0 for all x ∈ X . Proof. W.l.o.g., we assume N 2 = 1. For any weight/bias pair ŵ1,i , b1,i , we let w 1,i = ŵ1,i ,

$w 2,i = ŵ2,i , B i = {⟨w 1,i , x⟩ : x ∈ X }, b (i) ∧ = inf B i , and b (i) ∨ = sup B i . As ⟨w 1,i , •⟩ is continuous, means B i is compact and b (i) ∧ , b (i) ∨ ∈ B i .$We have four different cases, depending on b1,i .

(1) If b1,i ∈ B i , then we simply choose a corresponding x i ∈ X such that b 1,i = ⟨w 1,i , x i ⟩ = b1,i .

Letting b 2,i = b2,i , we have

$w 2,i ϕ(⟨w 1,i , x⟩ -b 1,i ) -b 2,i = ŵ2,i ϕ(⟨ ŵ1,i , x⟩ -b1,i ) -b2,i . (2) If b1,i > b (i) ∨ , we choose x i such that b 1,i = ⟨w 1,i , x i ⟩ = b (i) ∨ and b 2,i = b2,i . As ϕ(⟨w 1,i , x⟩ - b 1,i ) = ϕ(⟨ ŵ1,i , x⟩ -b1,i ) = 0, for all x ∈ X , we are done. (3) If b1,i < b (i) ∧ , we choose corresponding x i such that b 1,i = ⟨w 1,i , x i ⟩ = b (i) ∧ , and set b 2,i = b2,i + w 2,i b1,i -b (i) ∧ . We then have w 2,i ϕ(⟨w 1,i , x⟩ -b 1,i ) -b 2,i = w 2,i ⟨w 1,i , x⟩ -w 2,i b 1,i -b 2,i = w 2,i ⟨w 1,i , x⟩ -b2,i -w 2,i b1,i ± w 2,i b (i) ∧ = ŵ2,i ⟨ ŵ1,i , x⟩ -b2,i -ŵ2,i b1,i = ŵ2,i ϕ(⟨ ŵ1,i , x⟩ -b1,i ) -b2,i ,$where first and last equality holds due to ⟨w 1,i , x⟩ > b 1,i > b1,i for all x ∈ X . 

$= B i ∩ [b (i) ∧ , b1,i ] and B (2) i = B i ∩ [ b1,i , b (i)$∨ ] are both non-empty compact sets. We therefore have that supremum and infimum of both sets are members of their respective set, and thus also part of B i . We therefore choose x i such that b 1,i = ⟨w 1,i , x i ⟩ = inf B

(2) i . To make up for the difference between b1,i < b 1,i , we add a constant to all x ∈ X where ⟨w 1,i , x⟩ > b 1,i .

To do this we add some additional neurons, using our constant block

$Φ (i) c (⟨w 1,i , •⟩). Letting c = w 2,i b 1,i -b1,i , c 1 = sup B (1) i , c 2 = b 1,i , and c 3 = b (i)$∨ . We have now added five more neurons, and the weights and bias in second layer corresponding to the neurons is set to be ±a and 0, respectively, where both a and the sign of a depends on Definition 6. In case we require a negative sign in front of ⟨w 1,i , •⟩, we simply set it as we are only concerned with finding biases given weights. We then have that for all x ∈ X ,

$Φ (i) c (⟨w 1,i , x⟩) = c, ⟨w 1,i , x⟩ > b 1,i 0, otherwise.$Finally, by letting b 2,i = b2,i , we have

$w 2,i ϕ(⟨w 1,i , x⟩ -b 1,i ) -b 2,i + Φ (i) c (⟨w 1,i , x⟩) =w 2,i ⟨w 1,i , x⟩ -w 2,i b 1,i -b2,i + w 2,i b 1,i -ŵ2,i b1,i =⟨ ŵ1,i , x⟩ -ŵ2,i b1,i -b2,i = ŵ2,i ϕ(⟨ ŵ1,i , x⟩ b1,i ) -b2,i , when ⟨w 1,i , x⟩ > b 1,i , and$$w 2,i ϕ(⟨w 1,i , x⟩ -b 1,i ) -b 2,i + Φ (i) c (⟨w 1,i , x⟩) = b 2,i = b2,i = ŵ2,i ϕ(⟨w 1,i , x⟩ b1,i ) -b2,i ,$otherwise. And thus, Φ(x) = Φ(x) for all x ∈ X . As we add five additional neurons for each constant block, and we may need one for each neuron, means we need to construct our network with at most 6 • N 1 neurons. Now that we know we can construct suitable biases in our unit sampled networks for all types of biases bl,i in the arbitrary network Φ, we show how to construct the weights. Lemma 3. Let Φ ∈ F 1,K : X → R N2 , with biases of the form b1,i = ⟨ ŵ1,i , x i ⟩, where x i ∈ X for i = 1, 2, . . . , N 1 . For any ϵ > 0, there exist unit sampled network Φ, such that ∥Φ -Φ∥ ∞ < ϵ.

Proof. W.l.o.g., we assume N 2 = 1. For all i = 1, 2, . . . , N 1 , we construct the weights and biases as follows:

$If x i / ∈ ∂X , then we set ϵ ′ > 0 such that B ϵ ′ (x i ) ⊂ X . Let x (1) 0,i = x i and x (2) 0,i = x (1) 0,i + ϵ ′ 2 w 1,i ∈ B ϵ ′ (x i ) ⊂ X . Setting w 2,i = ∥ ŵ1,i ∥ ŵ2,i , b 2,i = b2,i , and w 1,i = x (2) 0,i -x (1) 0,i ∥x (2) 0,i -x (1) 0,i ∥ = ŵ1,i ∥ ŵ1,i∥ , implies w 2,i ϕ(⟨w 1,i , x -x (1) 0,i ⟩) -b 2,i = ∥ ŵ1,i ∥ ŵ2,i ϕ ŵ1,i ∥ ŵ1,i ∥ , x -x i -b2,i = ŵ2,i ϕ(⟨ ŵi , x -x i ⟩) -b2,i ,$where last equality follows by ϕ being positive homogeneous.

If x i ∈ ∂X , by continuity we find δ > 0 such that for all i = 1, 2, . . . , N 1 and x ′ , x ∈ X , where ∥x ′ -x i ∥ < δ, we have

$|ϕ(⟨ ŵ1,i , x -x ′ ⟩) -ϕ(⟨ ŵ1,i , x -x i ⟩)| < ϵ N 1 w 2 ,$where w 2 = max{| ŵ2,i |} N1 i=1 . We set x

(1) 0,i = η(x i , min{δ, 1}), with x (1) 0,i ∈ int X and ∥x i -x

(1) 0,i ∥ < δ, due to δ > 0 and Lemma 1. We may now proceed by constructing x (2) 0,i as above, and similarly setting w 2,i = ∥ ŵ1,i ∥ ŵ2,i , b 2,i = b2,i , we have

$N1 i=1 w 2,i ϕ(⟨w 1,i , x -x (1) 0,i ⟩) -b 2,i - N1 i=1 ŵ2,i ϕ(⟨ ŵ1,i , x -x i ⟩) -b2,i = N1 i=1 ŵ2,i ϕ(⟨ ŵ1,i , x -x (1) 0,i ⟩) -ϕ(⟨ ŵ1,i , x -x i ⟩) ≤ N1 i=1 w 2 ϕ(⟨ ŵ1,i , x -x (1) 0,i ⟩) -ϕ(⟨ ŵ1,i , x -x i ⟩) < N1 i=1 w 2 ϵ N 1 w 2 = ϵ,$and thus ∥Φ -Φ∥ ∞ < ϵ.

Until now, we have worked with weights of the form w = x (2) -x (1) ∥x (2) -x (1) ∥ , however, the weights in a sampled network are divided by the norm squared, not just the norm. We now show that for all the results so far, and also for any other results later on, differing by a positive scalar (such as this norm) is irrelevant when ReLU is the activation function. Lemma 4. Let Φ be a network with one hidden layer, with weights and biases of the form

$w 1,i = x (2) 0,i -x(1) 0,i ∥x (2) 0,i -x (1) 0$$,i ∥ , b 1,i = ⟨w 1,i , x(1)$0,i ⟩, for i = 1, 2, . . . , N 1 . For any weights and biases in the last layer, {w 2,i , b 2,i } N2 i=1 , and set of strictly positive scalars {ω i } N1 i=1 , there exist sampled networks Φ ω where weights and biases in the hidden layer {w ′ 1,i , b ′ 1,i } N1 i=1 are of the form

$w ′ 1,i = ω i w 1,i , b ′ 1,i = ⟨w ′ 1,i , x(1)$0,i ⟩, such that Φ ω (x) = Φ(x) for all x ∈ X .

Proof. We set w ′ 2,i = w2,i ωi and b ′ 2,i = b 2,i . As ReLU is a positive homogeneous function, we have for all x ∈ X ,

$w ′ 2,i ϕ(⟨w ′ 1,i , x⟩ -b ′ 1,i ) -b ′ 1,i = ω i w 2,i ω i ϕ(⟨w 1,i , x⟩ -b 1,i ) w 2,i ϕ(⟨w 1,i , x⟩ -b 1,i ).$The result itself is not too exciting, but it allows us to use the results proven earlier, applying them to sampled networks by setting the scalar to be

$ω i = 1 ∥x (2) 0,i -x (1) 0,i ∥ .$We are now ready to show the universal approximation property for sampled networks with one hidden layer. We let F S 1,∞ be defined similarly to F 1,∞ , with every Φ ∈ F S 1,∞ being a sampled neural network. Theorem 4. Let g ∈ C(X , R N L+1 ). Then, for any ϵ > 0, there exist Φ ∈ F S 1,∞ with ReLU activation function, such that

$∥g -Φ∥ ∞ < ϵ. That is, F S 1,∞ is dense in C(X , R N L+1 ).$Proof. W.l.o.g., let N L+1 = 1, and in addition, let ϵ > 0 and g ∈ C(X , R N L+1 ). Using the universal approximation theorem of Pinkus [[52]](#b51), we have that for ϵ > 0, there exist a network Φ ∈ F 1,∞ , such that ∥g -Φ∥ ∞ < ϵ 2 . Let K be the number of neurons in Φ. We then create a new network Φ, by first keeping the weights fixed to the original ones, { ŵ1,i , ŵ2,i } K i=1 , and setting the biases of Φ according to Lemma 2 using { b1,i , b2,i } K i=1 , adding constant blocks if necessary. We then change the weights of our Φ with the respect to the new biases, according to Lemma 3 (with the epsilon set to ϵ/2). It follows from the two lemmas that ∥Φ -

$Φ∥ ∞ < ϵ 2 . That means ∥g -Φ∥ ∞ = ∥g -Φ + Φ -Φ∥ ∞ ≤ ∥g -Φ∥ ∞ + ∥ Φ -Φ∥ ∞ < ϵ.$As the weights of the first layer of Φ can be written as

$w 1,i = x (2) 0,i -x (1) 0,i ∥x (2) 0,i -x (1) 0,i ∥ 2 and bias b 1,i = ⟨w 1,i , x (1) 0,i ⟩, both guaranteed by Lemma 4, means Φ ∈ F S 1,∞ . Thus, F S 1,∞ is dense in C(X , R N L+1$). Remark 4. By the same two lemmas, Lemma 2 and Lemma 3, one can show that other results regarding networks with one hidden layer with at most K neurons, also hold for sampled networks, but with 6 • K neurons, due to the possibility of one constant block for each neuron. When X is a connected set, we only need K neurons, as no additional constant blocks must be added; see proof of Lemma 2 for details. after this first hidden layer Ψ (l) (X ) is injective, and therefore bijective. To show this, let u 1 , u 2 ∈ X such that u 1 ̸ = u 2 . As the vectors in v spans X , means there exists two unique set of coefficients,

$c 1 , c 2 ∈ R D , such that u 1 = c (i) 1 v i and u 2 = c (i) 2 v i .$We then have

$Φ (l) (u 1 ) = V c 1 -b l ̸ = V c 2 -b l = Φ (l) (u 2 ),$where V is the Gram matrix of v. As vectors in v are linearly independent, V is positive definite, and combined with c 1 ̸ = c 2 , this implies the inequality. That means Φ (l) is a bijective mapping of X . As the mapping is bijective and continuous, we have that for any x ∈ X ′ , there is an ϵ (l)

$I > 0, such that B ϵ ′ I (Φ (l) (x)$) ⊆ X . For 1 < l < L, we repeat the procedure, but swap X with X l-1 . As Φ (l-1) is a bijective mapping, we may find similar linear independent vectors and construct similar points x

$(1) l-1,i , x(2)$l-1,i , but now with noise level ϵ (l) I . For l = L, as we have a subset of X that is a closed ball around each point in Φ l-1 (x), for every x ∈ X ′ , means we can proceed by constructing the last hidden layer and the last layer in the same way as explained when proving T heorem 4. The only caveat is that we are approximating a network with one hidden layer with the domain X l-1 , and the function we approximate is g = g • [Φ (L-1) ] -1 . Given this, denoting Φ (L:L+1) as the function of last hidden layer and the last layer, there exists a number of nodes, weights, and biases in the last hidden layer and the last layer, such that ∥g -Φ∥ ∞ = ∥g -Φ (L:L+1) ∥ ∞ < ϵ, due to construction above and Theorem 4. As Φ is a unit sampled network, it follows by Lemma 4 that

$∞ N L =1 F S L,N L is dense in C(X , R N L+1 ).$We can now prove that sampled networks with L layers, and different dimensions in all neurons, with arbitrary width in the last hidden layer, are universal approximators, with the obvious caveat that each hidden layer l = 1, 2, . . . , L -1 needs at least D neurons, otherwise we will lose some information regardless of how we construct the network.

$Theorem 5. Let N L = [N 1 , N 2 , . . . , N L-1 , N L ],$where min{N l : l = 1, 2, . . . , L -1} ≥ D, and

$L ≥ 1. Then ∞ N L =1 F S L,N L is dense in C(X , R N L+1 ).$Proof. Let ϵ > 0 and g ∈ C(X , R N L+1 ). For L = 1, Theorem 4 is enough, and we therefore assume

$L > 1. We start by constructing a network Φ ∈ ∞ N L =1 F S L, ÑL$, where ÑL = [D, D, . . . , D, N L ]

$according to Lemma 5, such that ∥ Φ -g∥ ∞ < ϵ. To construct Φ ∈ ∞ N L =1 F S L,N L , let l = 1$, and start by constructing weights/biases for the first D nodes according to Φ. For the additional nodes, in the first hidden layer, select an arbitrary direction w. Let X 1 = {x (j) 1,i : i = 1, 2, . . . , D and j = 1, 2} be the set of all points needed to construct the D neurons in the next layer of Φ. Then for each additional node i = D + 1, . . . , N 1 , we set

$x (1) 0,i = arg max{⟨w, x⟩ : x ∈ X and Φ(1) (x) ∈ X 1 }. and choose x (2) 0,i ∈ X such that x (2) 0,i -x (1)$0,i = aw, where a ∈ R >0 , similar to what is done in the proof for Lemma 3. Using these points to define the weights and biases of the last N 1 -D nodes, the following space X 1 now contains points [x (j) 1,i , 0, 0, . . . , 0], for j = 1, 2 and i = 1, 2, . . . , D. For 1 < l < L repeat the process above, setting x

$(j) l,i = [x (j)$l,i , 0, 0, . . . , 0] for the first D nodes, and construct the weights of the additional nodes as described above, but with sampling space being X l-1 . When l = L, set number of nodes to the same as in Φ, and choose the points to construct the weights and biases as x

$(j) L-1,i = [x (j)$L-1,i , 0, 0, . . . , 0], for j = 1, 2 and i = 1, 2, . . . , N L . The weights and biases in the last layer are the same as in Φ. This implies,

$∥Φ -g∥ ∞ = ∥ Φ -g∥ ∞ < ϵ,$and thus

$∞ N L =1 F S L,N L is dense in C(X , R N L+1 ).$Remark 5. Important to note that the proof is only showing existence, and that we expect networks to have a more interesting representation after the first L -1 layers. With this theorem, we can conclude that stacking layers is not necessarily detrimental for the expressiveness of the networks, even though it may alter the sampling space in non-trivial ways. Empirically, we also confirm this, with several cases performing better under deep networks -very similar to iteratively trained neural networks.

$Corollary 1. F S ∞,∞ is dense in C(X , R N L+1 ).$A.1.4 Networks with a single hidden layer, tanh activation

We now turn to using tanh as activation function, which we find useful for both prediction tasks, and if we need the activation function to be smooth. We will use the results for sampled networks with ReLU as activation function, and show we can arbitrarily well approximate these. The reason for this, instead of using arbitrary network with ReLU as activation function, is that we are using weights and biases of the correct form in the former, such that the tanh networks we construct will more easily have the correct form. We set s 2 = ln(3) 2 , s 1 = 2 • s 2 -as already discussed -and let ψ be the tanh function, with Ψ being neural networks with ψ as activation function -simply to separate from the ReLU ϕ, as we are using both in this section. Note that Φ is still network with ReLU as activation function and s 1 = 1, s 2 = 0.

We start by showing how a sum of tanh functions can approximate a set of particular functions.

$Lemma 6. Let f : [c 0 , c M +1 ] → R + , defined as f (x) = M i=1 a i 1 [ci,c M +1 ] (x), with 1 being the indicator function, c 0 < c 1 < • • • < c M < c M +1$, and for all i = 0, 1, . . . , M + 1, c i ∈ R and a i ∈ R >0 . Then there exists strictly positive scalars ω = {ω i } M i=1 such that

$g(x) = M i=1 g i (x) = M i=1 a i 2 [ψ(ω i (x -c i ) -s 2 ) + 1] , fulfills f (c i-1 ) < g(x) < f (c i+1 ) whenever x ∈ [c i-1 , c i+1 ]$, for all i = 1, 2, . . . , M .

Proof. We start by observing that both functions, f and g, are increasing, with the latter strictly increasing. We also have that f

$(c 0 ) = 0 < g(x) < M i=1 a i = f (c M ), for all x ∈ [c 0 , c M +1$], regardless choice of ω. We then fix constants

$0 < δ i < 3 4 a i M -i 0 < ϵ i+1 < ai 4$i , for i = 1, 2, . . . , M -1. We have, due to s 2 , that g i (c i ) = ai 4 , for all i = 1, 2, . . . , M . In addition, we can always increase ω i to make sure g i (c i+1 ) is large enough, and g i (c i-1 ) small enough for our purposes, as ψ is bijective and strictly increasing. We set ω 1 large enough such that g 1 (c j ) > a 1 -ϵ j , for all j = 2, 3, . . . , M -1. For i = 2, 3, . . . , M -1, we set ω i large enough such that g i (c j ) < δ j , where j = 1, 2, . . . , i -1, and g i (c j ) > a i -ϵ j , where j = i + 1, i + 2, . . . , M -1. Finally, let ω M be large enough such that g M (c j ) < δ j , for j = 1, 2, . . . , M -1. With the strictly increasing property of every g i , we see that

$g(c i ) = i-1 j=1 g j (c i ) + a i 4 + M j=i+1 g j (c i ) < i-1 j=1 a j + a i 4 + M j=i+1 δ j = i-1 j=1 a j + a i 4 + 3a i 4 = f (c i ),and$$g(c i ) = i-1 j=1 g j (c i ) + a i 4 + M j=i+1 g j (c i ) > i-1 j=1 (a j -ϵ i ) + a i 4 = i-1 j=1 a j - a i 4 + a i 4 = f (c i-1 ).$Combing the observations at the start with f (c i-1 ) < g(c i ) < f (c i ), for i = 1, 2, . . . , M , and the property sought after follows quickly.

We can now show that we can approximate a neuron with ReLU ϕ activation function and with unit sampled weights arbitrarily well.

Lemma 7. Let x(1) , x(2) ∈ X and ŵ2 ∈ R. For any ϵ > 0, there exist a M ∈ N >0 , and M pairs of distinct points {(x

$(2) i , x(1) i$$) ∈ X × X } M i=1 , such that ŵ2 ϕ(⟨ ŵ1 , x -x(1) ⟩) - M i=1 wi ψ w i , x -x (1) i -s 2 + 1 < ϵ,where ŵ1 = x(2) -x (1) ∥x (2) -x (1)$∥ , wi ∈ R, and

$w i = s 1 x(2) i -x (1) i ∥x (2) i -x (1) i ∥ 2 .$Proof. Let ϵ > 0 and, w.l.o.g., ŵ2 > 0.

$Let B = {⟨ ŵ1 , x⟩ : x ∈ X }, as well as f (x) = ŵ2 ϕ(⟨ ŵ1 , x- x(1)$). We start by partitioning f into ϵ/4-chunks. More specifically, let c = max B, and

$M ′ = 4 ŵ2 |c| ϵ . Set d k = (k-1)c$M , for k = 1, 2, . . . , M ′ , M ′ + 1. We will now define points c j , with the goal of constructing a function f as in Lemma 6. Still, because we require c j ∈ B to define biases in our tanh functions later, we must define the c j s iteratively, by setting c 1 = 0, k = j = 2, and define every other c j as follows:

1. If d k = c, we are done, otherwise proceed to 2. We have M < 2 • M ′ points, and can now construct the a j s of f . For j = 1, 2, . . . , M , with c M +1 = c, let

## Set

$d ′ k = sup B ∩ [c j-1 , d k ] d ′′ k = inf B ∩ [d k , c]. 3. If d ′ k = d ′′ k , set c j = d k ,$$a j = f (c j+1 ) -f (c j ), c j+1 -c j ≤ c M ′ f (ρ(c j )) -f (c j ), otherwise,$where ρ(c j ) = arg min{d k -c j : d k -c j ≥ 0 and k = 1, 2, . . . , M ′ +1}. Note that 0 < c-c M ≤ c M ′ , by Definition 5 and continuity of the inner product ⟨ ŵ1 , • -x (1) ⟩. We then construct f as in Lemma 6, namely,

$f (x) = M i=1 a i 1 [ci,c M +1 ] (x). Letting C = {[c j , c j+1 ] : j = 0, 1, . . . , M and c j+1 -c j ≤ c M ′ }, it is easy to see |f (x) -f (x)| < ϵ 4 ,$for all x ∈ c ′ ∈C c ′ . For any x outside said set, we are not concerned with, as it is not part of B, and hence nothing from X is mapped to said points.

Construct ω = {ω i } M i=1 according to Lemma 6. We will now construct a sum of tanh functions, using only weights/biases allowed in sampled networks. For all i = 1, 2, . . . , M , define wi = ai 2 and set x

(1) i = η(x, δ i ), with δ i ≥ 0 and x ∈ X , such that ⟨ ŵ1 , x⟩ = c i -where δ i = 0 iff / ∈ ∂X . We specify δ i and ϵ

$′ i > 0 such that s1 ϵ ′ i ≥ ω i , B 2ϵ ′ i (x(1)$i ) ⊆ X , and

$|ψ(⟨w i , x -x (1) i ⟩ -s 2 ) -ψ(⟨w i , x⟩ -c i -s 2 )| < ϵ 4| wi |M , with w i = s 1 x (2) i -x (1) i ∥x (2) -x (1) ∥ 2 , x(2) i = x (1)$i + ϵ ′ i ŵ1 , and for all x ∈ X . It is clear that

$x (1) i , x(2) i$∈ X . We may now rewrite the sum of tanh functions as

$g(x) = M i=1 wi ψ ⟨w i , x -x (1) i ⟩ -s 2 + wi = M i=1 wi ψ s 1 ∥ϵ ′ i ŵ1 ∥ ϵ ′ i ŵ1 ∥ϵ ′ i ŵ1 ∥ , x -x (1) i -s 2 + wi = M i=1 wi ψ s 1 ϵ ′ i ⟨ ŵ1 , x⟩ -ci -s 2 + wi = M i=1 a i 2 [ψ (ω i ⟨ ŵ1 , x⟩ -ci -s 2 ) + 1],$where ci = ⟨ ŵ1 , x

i ⟩. As ω i ≤ ωi for all i = 1, 2, . . . , M , it follows from Lemma 6 and the construction above that

$|g(x) -f (x)| ≤ |g(x) - M i=1 a i 2 [ψ (ω i ⟨ ŵ1 , x⟩ -c i -s 2 ) + 1]| + | M i=1 a i 2 [ψ (ω i ⟨ ŵ1 , x⟩ -c i -s 2 ) + 1] -f (x)| + |f (x) -f (x)| < ϵ 4 + ϵ 2 + ϵ 4 = ϵ.$We are now ready to prove that sampled networks with one hidden layer with tanh as activation function are universal approximators. Theorem 6. Let F S 1,∞ be the set of all sampled networks with one hidden layer of arbitrary width and activation function ψ. F S 1,∞ is dense in C(X , R N2 ), with respect to the uniform norm.

Proof. Let g ∈ C(X, R N2 ), ϵ > 0, and w.l.o.g., N 2 = 1. By Theorem 4, we know there exists a network Φ, with N1 neurons and parameters { ŵ1,i , ŵ2,i , b1,i , b2,i } N1 i=1 , and ReLU as activation function, such that ∥Φ -g∥ ∞ < ϵ 2 . We can then construct a new network Ψ, with ψ as activation function, where for each neuron Φ (1,n) , we construct M i neurons in Ψ, according to Lemma 7, with

$ϵ 2 N1$. Setting the biases in last layer of Ψ based on Φ, i.e., for every i = 1, 2, . . . , N1 , b 2,j = b2,i Mi , where j = 1, 2, . . . , M i . We then have, letting N 1 be the number of neurons in Ψ,

$|Ψ(x) -Φ(x)| = N1 i=1 w 2,i Ψ (1,i) (x) -b 2,i -   N1 i=1 ŵ2,i Φ (1,i) (x) -b2,i   = N1 i=1   Mi j=1 w 2,j ψ(⟨w 1,j , x⟩ -b 1,j )   -ŵ2,i ϕ(⟨ ŵ1,i , x⟩ -b1,i ) ≤ N1 i=1   Mi j=1 w 2,j ψ(⟨w 1,j , x⟩ -b 1,j )   -ŵ2,i ϕ(⟨ ŵ1,i , x⟩ -b1,i ) < N1 i=1 ϵ 2 N1 = ϵ 2 ,$for all x ∈ X . The last inequality follows from Lemma 7. This implies that

$∥Ψ -g∥ ∞ ≤ ∥Ψ -Φ∥ ∞ + ∥Φ -g∥ ∞ < ϵ 2 + ϵ 2 = ϵ, and F S 1,∞ is dense in C(X , R N2 ).$
## A.2 Barron spaces

Working with neural networks and sampling makes it very natural to connect our theory to Barron spaces [[2,](#b1)[20]](#b19). This space of functions can be considered a continuum analog of neural networks with one hidden layer of arbitrary width. We start by considering all functions f : X → R that can be written as

$f (x) = Ω w 2 ϕ(⟨w 1 , x⟩ -b)dµ(b, w 1 , w 2 ),$where µ is a probability measure over (Ω, Σ Ω ), with Ω = R × R D × R. A Barron space B p is equipped with a norm of the form,

$∥f ∥ Bp = inf µ {E µ [|w 2 | p (∥w 1 ∥ 1 + |b|) p ] 1/p }, 1 ≤ p ≤ ∞,$taken over the space of probability measure µ over (Ω, Σ Ω ). When p = ∞, we have

$∥f ∥ B∞ = inf µ max (b,w1,w2)∈supp(µ) {|w 2 |(∥w 1 ∥ 1 + |b|)}.$The Barron space can then be defined as

$B p = {f : f (x) = Ω w 2 ϕ(⟨w 1 , x⟩ -b)dµ(b, w 1 , w 2 ) and ∥f ∥ Bp < ∞}.$As for any 1 ≤ p ≤ ∞, we have B p = B ∞ , and so we may drop the subscript p [[20]](#b19). Given our previous results, we can easily show approximation bounds between our sampled networks and Barron functions.

Theorem 7. Let f ∈ B and X = [0, 1] D . For any N 1 ∈ N >0 , ϵ > 0, and an arbitrary probability measure π, there exist sampled networks Φ with one hidden layer, N 1 neurons, and ReLU activation function, such that

$∥f -Φ∥ 2 2 = X |f (x) -Φ(x)| 2 dπ(x) < (3 + ϵ)∥f ∥ 2 B N 1 .$Proof. Let N 1 ∈ N >0 and ϵ > 0. By E et al. [[20]](#b19), we know there exists a network Φ ∈ F 1,N1 ,

where

$Φ(•) = N1 i=1 ŵ2,i ϕ(⟨ ŵ1,i , •⟩ -b1,i ), such that ∥f -Φ∥ 2 2 ≤ 3∥f ∥ 2 B N1$. By Theorem 4, letting It is not immediately clear from above that P is valid distribution, and in particular, that the density is integrable. This is what we show next. Proposition 1. Let X ⊆ R D be compact, λ D (X ) > 0, and f be Lipschitz continuous w.r.t. the metric spaces induced by ∥•∥ Y and ∥•∥ X . For fixed architecture {N l } L l=1 , the proposed function p is integrable and

$X p dλ D = 1.$It therefore follows that P is a valid probability distribution.

Proof. We will show for each l = 1, 2, . . . , L that p l is bounded a.e. and is nonzero for at least one subset with nonzero measure. There exist A ⊆ X × X such that p l (A) ̸ = 0 and λ 2D (A) > 0, as either C l > 0 or p l is the uniform density by Definition 7 and λ 2D (X × X ) > 0 by assumption and the product topology.

For l = 1, let K l > 0 be the Lipschitz constant. Then q l (x (1) , x (2) ) by assumption of f for all x (1) , x (2) ∈ X . When l > 1, for all X l-1 ∈ X 2 Nl-1 , we have that there exist a constant K l > 0 due to continuity and compactness, such that

$p l (x (1) , x (2) | X l ) ≤ max K l ϵ , 1 λ 2D (X × X ) .$As p is a multiplication of a finite set of elements, we end up with

$0 < X p dλ < X L l=1 N l K l ϵ + K l + 1 λ 2D (X × X ) dλ D < L max N l K l ϵ + K l + 1 λ 2D (X × X ) L l=1 λ D ( X ) < ∞.$using the fact that 0 < λ D (X ) < ∞ and λ D being the product measure, implies 0 < λ D ( X ) < ∞. Due to the normalization constants added to q l , we see p l integrates to one. This means P is a valid distribution of X , with implied independence between the neurons in the same layer.

One special property of sampled networks, and in particular of the distribution P , is their invariance under both linear isometries and translation (together forming the Euclidean group, i.e., rigid body transformations), as well as scaling. We denote the set of possible transformations as H(D) = R \ {0} × O(D) × R, with O(D) being the orthogonal group of dimension D. We then denote (a, A, c) = H ∈ H(D) as H(x) = aAx + c, where x ∈ X . The space H f (D) ⊆ H are all transformations such that f : X → R N L+1 is equivariant with respect to the transformations, with the underlying metric space given by ∥•∥ Y . That is, for any H ∈ H f (D), there exists a H ′ ∈ H(N L+1 ), such that f (H(x)) = H ′ (f (x)), where x ∈ X , and the orthogonal matrix part of H ′ is isometric w.r.t. ∥•∥ Y . Note that often the H ′ will be the identity transformation, for example by having the same labels for the transformed data. When H ′ is the identity function, we say f is invariant with respect to H. In the next result, we assume we choose norms ∥•∥ X0 and ∥•∥ Y , such that the orthogonal matrix part of H is isometric w.r.t. those norms and the canonical norm ∥•∥, as well as continue the assumption of Lipschitz-continuous f . Theorem 8. Let H ∈ H f (D), Φ, Φ be two sampled networks with the same number of layers L and neurons N 1 , . . . , N L , where Φ : X → R N L+1 and Φ : H(X ) → R N L+1 , and f : X → R N L+1 is the true function. Then the following statements hold:

(1) If

x(1)

$0,i = H(x(1)$0,i ) and x(2)

$0,i = H(x(2)$0,i ), for all i = 1, 2, . . . , N 1 , then Φ (1) (x) = Φ(1) (H(x)), for all x ∈ X .

(2) If f is invariant w.r.t. H: Φ ∈ F S L,[N1,...N L ] (X ) if and only if Φ ∈ F S L,[N1,...N L ] (H(X )), such that Φ(x) = Φ(x) for all x ∈ X .

(3) The probability measure P over the parameters is invariant under H.

Proof. Let H = (a, A, c). Assume we have sampled

$x(1) 0,i = H(x(1)$0,i ) and x(2)

$0,i = H(x(2)$0,i ), for all i = 1, 2, . . . , N 1 . The points sampled determines the weights and biases in the usual way, giving

$ϕ w 1,i , x -x (1) 0,i = ϕ x (2) 0,i -x (1) 0,i ∥x (2) 0,i -x (1) 0,i ∥ 2 , x -x (1) 0,i = ϕ A x (2) 0,i -x (1) 0,i ∥A(x (2) 0,i -x (1) 0,i )∥ 2 , A(x -x(1)$$0,i ) = a • a a 2 ϕ A x (2) 0,i -x (1) 0,i ∥A(x (2) 0,i -x (1) 0,i )∥ 2 , A(x -x(1)$$0,i ) = ϕ aA x (2) 0,i -x (1) 0,i ∥aA(x (2) 0,i -x (1) 0,i )∥ 2 , aA(x -x(1)$$0,i ) = ϕ aA x (2) 0,i + c -x (1) 0,i -c ∥aA(x (2) 0,i + c -x (1) 0,i -c)∥ 2 , aA(x + c -x (1) 0,i -c) = ϕ H(x (2) 0,i ) -H(x(1)$$0,i ) ∥H(x (2) 0,i ) -H(x (1) 0,i )∥ 2 , H(x) -H(x (1) 0,i ) = ϕ ŵ1,i , x - x(1)$0,i for all x ∈ X , x ∈ H(X ), and i = 1, 2, . . . , N 1 . Which implies that (1) holds.

Assuming f is invariant w.r.t. H, then for any Φ ∈ F S L,[N1,...N L ] (X ), let X = {x

1,i } N1 i=1 , we can then choose H(X) as points to construct weights and biases in the first layer of Φ, and by (1), we have X 1 = Φ (1) (X ) = Φ(1) (H(X )) = X1 . As the sampling space is the same for the next layer, we see that we can choose the points for the weights and biases of Φ, such that X l = Xl , where l = 1, 2, . . . , L. As the input after the final hidden layer is also the same, by the same argument, means the weights in the last layer must be the same, due to the loss function L in Definition 4 is the same due to the invariance assumption. Thus, Φ(x) = Φ(H(x)) for all x ∈ X . As H is bijective, means the same must hold true starting with Φ, and constructing Φ, and we conclude that (2) holds.

For any distinct points x (1) , x (2) ∈ X , letting (a ′ , A ′ , c ′ ) be the set such that g(aAx + c) = a ′ A ′ g(x) + c ′ , and Ĉl , C l be the normalization constants over the conditional density p l , for H(X ) and X resp. We have for the conditional density when l = 1 is, p 1 (H(x (1) ), H(x (2) )) = 1 Ĉ1

∥f (aAx (2) + c) -f (aAx (1)  1) , x (2) ).

$+ c)∥ Y ∥aAx (2) + c -aAx (1) -c∥ X = 1 Ĉ1 ∥a ′ A ′ f (x (2) ) + c ′ -a ′ A ′ f (x (1) ) -c ′ ∥ Y ∥aAx (2) -aAx (1) ∥ X = |a ′ | |a| Ĉ1 ∥f (x (2) ) -f (x (1) )∥ Y ∥x (2) -x (1) ∥ X = |a ′ | |a| Ĉ1 p 1 (x($With similar calculations, we have

$|a| |a ′ | Ĉ1 = |a| |a ′ | X ×X p 1 (H(x), H(z))dxdz = |a| |a ′ | |a ′ | |a| X ×X p 1 (x, z)dxdz = C 1 .$Hence, the conditional probability distribution for the first layer is invariant under H, and then by (1) and (2), the possible sampling spaces are equal for the following layers, and therefore the conditional distributions for each layer is the same, and therefore P is invariant under H.

Remark 7. Point (2) in the theorem requires f to be invariant w.r.t. H. This is due to the fact that the parameters in the last layer minimizes the implicitly given loss function L, seen in Definition 4.

We have rarely discussed the loss function, as it depends on what function we are approximating.

Technically, (2) also holds as long as the loss function is not altered by H in a way that the final layer alters, but we simplified it to be invariant, as this is also the most likely case to stumble upon in practice. The loss function also appears in the proofs given in Appendix A.1 and Appendix A.2. Here both uses, implicitly, the loss function based on the uniform norm.

## B Illustrative example: approximating a Barron function

All computations for this experiment were performed on the Intel Core i7-7700 CPU @ 3.60GHz × 8 with 32GB RAM.

We compare random Fourier features and our sampling procedure on a test function for neural networks [[64]](#b63): f (x) = 3/2(∥x -a∥ ℓ 2 -∥x + a∥ ℓ 2 ), with the constant vector a ∈ R d defined by a j = 2j/d -1. For all dimensions d, we sample 10,000 points uniformly at random in the cube [-1, 1] d for training (i.e., sampling and solving the last, linear layer problem), and another 10,000 points for evaluating the error. We re-run the same experiment with five different random seeds, but with the same train/test datasets. This means the random seed only influences the weight distributions in sampled networks and random feature models, not the data. Figure [8](#) shows the relative L 2 error in the full hyperparameter study, i.e. in dimensions d ∈ {1, 2, 3, 4, 5, 10} (rows) and for tanh (left column) and sine activation functions (right column). For the random feature method, we always use sine activation, because we did not find any data-agnostic probability distributions for tanh. Figure [9](#) shows the fit times for the same experiments, demonstrating that sampling networks are not slower than random feature models. This is obvious from the complexity analysis in Appendix F, and confirmed experimentally here. Interesting observations regarding the full hyperparameter study are that for one dimension, d = 1, random feature models outperform sampled networks and can even use up to two hidden layers. In higher dimensions, and with larger widths / more layers, sampled networks consistently outperform random features. In d = 10, a sampled network with even a single hidden layer is about one order of magnitude more accurate than the same network with normally distributed weights. The convergence rate of the error of sampled networks with respect to increasing layer widths is consistent over all dimensions and layers, even sometimes outperforming the theoretical bound of O(N -1/2 ).

## C Classification benchmark from OpenML

The Adam training was done on the GeForce 4x RTX 3080 Turbo GPU with 10 GB RAM, while sampling was performed on the Intel Core i7-7700 CPU @ 3.60GHz × 8 with 32GB RAM.

We use all 72 datasets in the "OpenML-CC18 Curated Classification benchmark" from the "OpenML Benchmarking Suites" (CC-BY) [[4]](#b3), which is available on openml.org: [https://openml.org/ search?type=benchmark&sort=tasks_included&study_type=task&id=99](https://openml.org/search?type=benchmark&sort=tasks_included&study_type=task&id=99).

We use the OpenML Python API (BSD-3 Clause) [[22]](#b21) to download the benchmark datasets. The hyperparameters used in the study are listed in Table [3](#). From all datasets, we use at most 5000 points. This was done for all datasets before applying any sampled or gradient-based approach, because we wanted to reduce the training time when using the Adam optimizer. We pre-process the input features using one-hot encoding for categorical variables, and robust whitening (removal of mean, division by standard deviation using the RobustScaler class from scikit-learn). Missing features are imputed with the median of the feature. All layers of all networks always have 500 neurons. For Adam, we employ early stopping with patience=3, monitoring the loss value. The least squares regularization constant for sampling is 10 -10 . We split the data sets for 10-fold cross-validation using stratified k-fold (StratifiedKFold class from scikit-learn), and report the average of the accuracy scores and fit times over the ten folds (Figure [4](#fig_1) in the main paper).

## D Deep neural operators D.1 Dataset

To generate an initial condition u 0 , we first sample five Fourier coefficients for the lowest frequencies. We sample both real and imaginary parts from a normal distribution with zero mean and standard keep in the POD. To compute the POD, we first center the data and then use np.linalg.eigh to obtain the modes. We add two callbacks to the standard training: the early stopping and the model checkpoint. We use the default parameters for the optimization and only set the learning rate. The default training loop uses the number of iterations instead of the number of epochs, and we train for 90000 iterations with patience set to 4500. By default, POD-DeepONet uses all the data available as a batch. We ran several experiments with the batch size set to 64, but the results were significantly worse compared to the default setting. Hence, we stick to using the full data as one batch.

FNO We use the neuralop ([https://github.com/neuraloperator/neuraloperator](https://github.com/neuraloperator/neuraloperator), MIT license, [[42,](#b41)[38]](#b37)) package with slight modifications. We changed the trainer.py to add the early stopping and to store the results of experiments. Apart from these changes, we use the functionality provided by the library to define and run experiments. We use batch size 64 and train for 2000 epochs with patience 100. In addition to the hyperparameters considered for other architecture, we also did a search over the number of hidden channels (32 or 64). We use the same number of channels for lifting and projection operators.

FCN For both fully-connected networks in our experiments, we use PyTorch framework to define the models. For training a network in Fourier space, we prepend a forward Fourier transform before starting the training. During the inference, we applied the inverse Fourier transform to compute the validation metric. We considered using the full data as a batch, but several experiments indicated that this change worsens the final metric. Hence, we use batch size 64 for both architectures.

Sampling FNO Here, we want to give more details about sampled FNO. First, we start with input u 0 and append the coordinate information to it as an additional channel. Then, we lift this input to a higher dimensional channel space by drawing 1 × 1 convolution filters from a normal distribution. Then, we apply a Fourier block which consists of fully-connected networks in Fourier space for each channel. After applying the inverse Fourier transform, we add the input of the block to the restored signal as a skip connection. After applying several Fourier blocks, we learn another FCN in signal space to obtain the final prediction. We can see the schematic depiction of the model in Figure [10](#fig_7). We note that to train fully-connected networks in the Fourier space, we apply Fourier transform to labels u 1 and use it during sampling as target functions. Similarly to the Adam-trained FNO, we search over the number of hidden channels we use for lifting.

## D.3 Results

We can see the results of the hyperparameter search in Figure [11](#fig_8). We see that sampled FNO and FCN in Fourier space perform well even with a smaller number of frequencies available, while DeepONet requires more modes to achieve comparable accuracy. We also see that adding more layers is not beneficial for sampled networks. A more important factor is the width of the layers: all of the sampled networks achieved the best results with the largest number of neurons.

For the Adam-trained models, we note that we could not significantly improve the results by increasing the widths of the layers. Possibly, this is due to a limited range of learning rates considered in the experiments. FNO with Adam has no dependency on a layer width. Thus, we show results for FNO trained with Adam as a straight line in all rows. For the hyperparameters not present in the plot, we chose the values that produce the lowest error on the validation set. We repeat each experiment three times and average the resulting metrics for the plotting. The error is computed on the validation set.

## E Transfer learning

This section contains details about the results shown in the main paper and the supporting hyperparameter search concerning the transfer learning task. The source and target tasks of transfer learning, train-test split, and pre-trained models are the same as in the main paper. We describe the software, hardware, data pre-processing, and details of the sampling and the Adam training approach. Note that the results shown here are not averaged over five runs, as was done for the plots in the main paper.

## Software and hardware

We performed all the experiments concerning the transfer learning task in Python, using the TensorFlow framework. We used a GeForce 4x RTX 3080 Turbo GPU with 10 GB RAM for the Adam training and a 1x AMD EPYC 7402 socket with 24 cores @ 2.80GHz for sampling. We use the pre-trained models from Keras Applications which are licensed under the Apache 2.0 License which can be found at [https://github.com/keras-team/keras/blob/v2. 12.0/LICENSE](https://github.com/keras-team/keras/blob/v2.12.0/LICENSE).

Pre-processing of data The size of images in the ImageNet data set is larger than the ones in CIFAR-10, so we use a bicubic interpolation to upscale the CIFAR-10 images to dimensions 224 × 224 × 3.

We pre-process the input data for each model the same way it was pre-processed during the pretraining on ImageNet. Moreover, we apply some standard data augmentation techniques before training, such as vertical and horizontal shifts, rotation, and horizontal flips. After pre-processing, the images are first passed through the pre-trained classification layers and a global average pooling layer. The output of the global average pooling layer and classification labels serve as the input and output data for the classification head respectively.

## Details of the Adam training and sampling approaches

We find the weights and biases of the hidden layer of the classification head using the proposed sampling algorithm and the usual gradient-based training algorithm.

First, in the Adam-training approach, we find the parameters of the classification head by iterative training with the Adam optimizer. We use a learning rate of 10 -3 , batch size of 32, and train for 20 epochs with early-stopping patience of 10 epochs. We store the parameters that yield the lowest loss on test data. We use the tanh activation function for the hidden layer, the softmax activation function for the output layer, and the categorical cross-entropy loss function with no regularization.

Second, in the sampling approach, we use the proposed sampling algorithm to sample parameters of the hidden layer of the classification head. We use the tanh activation function for the hidden layer.

Once the parameters of the hidden layers are sampled, an optimization problem for the coefficients of a linear output layer is solved. For this, the mean squared error loss function without regularization is used.

Unless mentioned otherwise, the above-mentioned setup is used for all the experiments in this section.

## E.1 Sampling Vs Adam training

Figure [12](#fig_9) compares the train and test accuracy for different widths (number of neurons in the hidden layer of the classification head) using three pre-trained neural network architectures for the Adam training and sampling approaches. As in the main paper, we observe that for all the models, the sampling approach results in a higher test accuracy than the Adam training approach for sufficiently higher widths.

The sampling algorithm tends to over-fit for very high widths. The loss on the train data for the iterative training approach decreases with width, particularly for the Xception network. This suggests that an extensive hyper-parameter search for the iterative training approach could yield higher classification accuracy on the train data. However, as shown in the main paper, the iterative training approach can be orders of magnitude slower than the sampling approach. 

## E.2 Sampling with tanh Vs ReLU activation functions

This sub-section compares the performance of the sampling algorithm used to sample the weights of the hidden layer of the classification head for tanh and ReLu activation functions.

Figure [13](#fig_10) shows that for ResNet50, the test accuracy with tanh is similar to that obtained with ReLU for a width smaller than 2048. As the width increases beyond 4096, test accuracy with ReLU is slightly better than with tanh. However, for VGG19 and Xception, test accuracy with tanh is higher than or equal to that with ReLU for all widths. Thus, we find the sampling algorithm with the tanh activation function yields better results for classification tasks than with ReLU.

On the training dataset, tanh and ReLU yield similar accuracies for all widths for ResNet50 and Xception. For VGG19, using the ReLU activation function yields much lower accuracies on the train and test data sets, especially as the width of the fully connected layer is increased. Thus, we observe that the tanh activation function is more suitable for classification tasks.

## E.3 Fine-tuning

There are two typical approaches to transfer learning: feature extraction and fine-tuning. In feature extraction, there is no need to retrain the pre-trained model. The pre-trained models capture the In fine-tuning, after the feature extraction, some or all the parameters of the pre-trained model (a few of the last layers or the entire model) are re-trained with a much smaller learning rate using typical gradient-based optimization algorithms. Fine-tuning is not the focus of this work. Nevertheless, it is essential to verify that the weights of the last layer sampled by the proposed sampling algorithm can also be trained effectively in the fine-tuning phase. The results are shown in Figure [14](#fig_5).

In Figure [14](#fig_5), we observe that for certain widths (512, 1024, and 4096), sampling the last layer followed by fine-tuning the entire model yields slightly higher test accuracies than training the last layer followed by fine-tuning. For widths 2048, 6144, and 8192, for some models, sampling the last layer, followed by fine-tuning, is better; for others, training the last layer, followed by fine-tuning, is better.

Nevertheless, these experiments validate that the parameters of the last layer sampled with the proposed algorithm serve as a good starting point for fine-tuning. Moreover, the test accuracy after fine-tuning is comparable irrespective of whether the last layer was sampled or trained. However, as we show in the main paper, sampling the weights in the feature extraction phase takes much less time and gives better accuracy than training with the Adam optimizer for appropriate widths.

## E.4 One vs two hidden layers in the classification head

The goal of this sub-section is to explore whether adding an additional hidden layer in the classification head leads to an improvement in classification accuracy. We keep the same width for the extra layer in this experiment.

Figure [15](#fig_12) compares the train and test accuracies obtained with one and two hidden layers in the classification head for different widths.

Figure [15](#fig_12) (left) shows that unless one chooses a very high width with the sampling approach >= 6148, adding an extra hidden layer yields a lower test accuracy. On the train data, the sampling approach with 2 hidden layers results in lower accuracy for all widths in consideration.

Figure [15](#fig_12) (right) shows that with the Adam training approach, adding an extra hidden layer yields a lower test accuracy for all the widths in consideration. For lower widths, adding more layers results in over-fitting. We believe that the accuracy on the train data for higher widths could be improved with an extensive hyper-parameter search. However, adding an extra layer increases the training time too.  When the size of the hidden layer is less than the number of training points -which is often the case -we compute 2 • M probabilities -depending on a scaling constant. On the other hand, when the size of the layer is greater than or equal to M 2 , we compute in worst case all possible probabilities, that is, M 2 probabilities. The last contributing factor is to sample N l pair of points, that in the expression is dominated by the term N l • N l-1 • M . We are often interested in a fixed architecture, or at least to bound the number of neurons in each hidden layer to be less than the square of the number of training points, i.e., N < M 2 . Adding the latter as an assumption, we end up with the runtime O(L • M (⌈N/M ⌉ + N 2 )).

For the second part, we optimize the weights/biases W L+1 , b L+1 to map from the last hidden layer to the output. Assume that we use the SVD decomposition to compute the pseudoinverse and subsequently solve the least squares problem. The time complexity in general is then

$O(M • N L • min{M, N L } + N L • M • N L+1 ).$Again, if we make the reasonable assumption that the number of training points is larger than the number of hidden layers, and that the output dimension is smaller than the dimension of the last hidden layer, we find O(M N 2 L ), which can be rewritten to O(M N 2 ). With these assumptions in mind, the run time for the full training procedure, from start to finish, is O(L • M (⌈N/M ⌉ + N 2 )), and when the architecture is fixed, we have O(M ) runtime for the full training procedure.

In terms of memory, at every layer l, we need to store the probability matrix, the output of the training set when mapped through the previous l -1 layers, and the number of points we sample. This means the memory required is O(M • ⌈N l /M ⌉ + N L+1 • N L ). In the last layer, we only need the image of the data passed through all the hidden layers, as well as the weights/biases, which leads to O(M + N L+1 • N L ). We end up with the required memory for the SWIM algorithm is O(M • ⌈N/M ⌉ + LN 2 ).

![Figure 3: Relative L 2 approximation error of a Barron function (test set), using random features and sampling, both with sine activation. Left: input dimension D = 5. Right: input dimension D = 10.]()

![Figure4: Fitting time, accuracy, and number of layers using weight sampling, compared to training with the Adam optimizer. The best architecture is chosen separately for each method and each problem, by evaluating 10-fold cross-validation error over 1-5 layers with 500 neurons each.]()

![Figure 5: Left: Samples of initial conditions u 0 and corresponding solutions u 1 for Burgers' equation. Right: Parameters of the best model for each architecture, the mean relative L 2 error on the test set, and the training time. We average the metrics across three runs with different random seeds.]()

![Figure 6: Left: Train and test accuracies with different widths for ResNet50 (averaged over 5 random seeds). Middle: Test accuracy with different models with and without fine-tuning (width = 2048). Right: Adam training and sampling times of the classification head (averaged over 5 experiments).]()

![If b(i) ∧ < b1,i < b (i) ∨ ,and b1,i / ∈ B i , things are a bit more involved. First notice that B (1) i]()

![and increase j and k, and go to 1.4. If d′ k > c j-1 , set c j = d ′ k ,and increase j, otherwise discard the point d ′ k . Set c j = d ′′ k , and increase j. Set k = arg min{d k -c j : d k -c j > 0} and go to 1.]()

![Figure 10: The architecture of the sampled FNO.]()

![Figure 11: Comparison of different deep neural operators trained with Adam and with SWIM algorithm.FNO with Adam has no dependency on a layer width. Thus, we show results for FNO trained with Adam as a straight line in all rows. For the hyperparameters not present in the plot, we chose the values that produce the lowest error on the validation set. We repeat each experiment three times and average the resulting metrics for the plotting. The error is computed on the validation set.]()

![Figure 12: Left: ResNet50. Middle: VGG19. Right: Xception.]()

![Figure 13: Left: ResNet50. Middle: VGG19. Right:Xception]()

![Figure 15: Left: Sampling, Right: Adam training]()

