The decisions made by the researchers in the introduction of sampling weights for deep neural networks are grounded in a combination of theoretical insights and practical considerations. Below is a detailed technical explanation and rationale for each of the key points mentioned:

### 1. Introduction of Sampling Weights
The researchers propose a novel approach to weight and bias initialization in fully-connected neural networks by introducing a probability distribution that is informed by the training data. This method avoids the need for iterative optimization, which is typically computationally expensive and time-consuming. The rationale behind this decision is to leverage the structure of the data to inform the initialization of the network parameters, thereby enhancing the network's ability to learn effectively from the start. By sampling weights based on the input-output relationships in the training data, the researchers aim to create a more efficient training process that can lead to faster convergence and improved performance.

### 2. Key Contribution
The key contribution of this work is the development of a data-driven sampling scheme that utilizes both input and output training data. This approach enhances the accuracy and efficiency of the network by ensuring that the sampled weights and biases are closely aligned with the gradients of the target function. The rationale is that by directly incorporating information from the training data into the sampling process, the network can achieve better performance with fewer neurons, thus addressing the common issue of over-parameterization in deep learning.

### 3. Universal Approximation
The researchers prove that the sampled networks are universal approximators, meaning they can approximate any continuous function to any desired degree of accuracy. This is a critical theoretical result that justifies the use of the proposed sampling method. The rationale is that if the sampled networks can approximate a wide range of functions, they can be effectively applied to various tasks in supervised learning, making them versatile and powerful tools in machine learning.

### 4. L2-Approximation Error
The researchers demonstrate that for Barron functions, the L2-approximation error of sampled shallow networks decreases with the square root of the number of neurons. This finding indicates that the proposed method is efficient in network design, as it suggests that fewer neurons can achieve a desired level of accuracy. The rationale behind this result is that the sampling method effectively captures the essential features of the target function, allowing for a more compact representation without sacrificing performance.

### 5. Invariance Properties
The sampling scheme is shown to be invariant to rigid body transformations and scaling of input data. This property reduces the need for extensive pre-processing techniques, which are often required in traditional machine learning workflows. The rationale is that by ensuring invariance to these transformations, the proposed method can be applied more broadly without the need for additional data manipulation, simplifying the overall modeling process.

### 6. Numerical Experiments
The numerical experiments conducted by the researchers demonstrate that sampled networks achieve accuracy comparable to iteratively trained networks while being constructed orders of magnitude faster. This empirical evidence supports the theoretical claims made earlier and provides a practical justification for the proposed method. The rationale is that if the sampled networks can deliver similar performance with significantly reduced computational costs, they represent a valuable alternative to traditional training methods.

### 7. Mathematical Framework
The mathematical framework introduced by the researchers defines how weights and biases are constructed based on pairs of input points. The specific formulas for weight and bias construction are designed to ensure that the network captures the relationships between data points effectively. The rationale is that by using the differences between pairs of points to define weights and the inner product to define biases, the network can learn meaningful representations that are closely tied to the underlying data distribution.

### 8. Activation Functions
The focus on specific activation functions (ReLU and tanh) is motivated by their widespread use and effectiveness in deep learning. The researchers provide specific values for the constants used in the weight and bias construction for these functions, ensuring that the network behaves appropriately during training. The rationale is that by tailoring the sampling method to well-known activation functions, the researchers can leverage existing theoretical insights while also ensuring practical applicability.

### 9. Theorems on Density and Approximation Error
Theorems 1 and 2 establish the density of sampled networks in the space of continuous functions and bound the L2 approximation error for Barron functions, respectively. These results provide a strong theoretical foundation for the proposed method, confirming that it can achieve the desired approximation properties. The rationale is that by proving these theorems, the researchers can assure practitioners of the robustness and reliability of their approach.

### 10. Comparison with Existing Methods
The researchers highlight the differences between their method and existing approaches, such as random Fourier features and Bayesian neural networks. By emphasizing the direct relationship between data points and weights in their method, they argue for its interpretability and efficiency. The rationale is that by clearly delineating how their approach differs from and improves upon existing methods, they can position their work as a significant advancement in the field.

### 11. Efficiency of Sampling
The construction of weights based on data pairs allows for efficient use of neurons, leading to reduced computational costs compared to traditional gradient-based methods. The rationale is that by minimizing the