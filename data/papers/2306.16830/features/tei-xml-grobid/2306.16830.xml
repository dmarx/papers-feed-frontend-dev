<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sampling weights of deep neural networks</title>
				<funder ref="#_TSbSeF6">
					<orgName type="full">Institute for Advanced Study</orgName>
					<orgName type="abbreviated">IAS</orgName>
				</funder>
				<funder ref="#_kJzG4tv">
					<orgName type="full">Deutsche Forschungsgemeinschaft (DFG, German Research Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-11-12">12 Nov 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Erik</forename><surname>Lien Bolager</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computation, Information and Technology</orgName>
								<orgName type="department" key="dep2">Institute for Advanced Study</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Iryna</forename><surname>Burak</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computation, Information and Technology</orgName>
								<orgName type="department" key="dep2">Institute for Advanced Study</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">+</forename><surname>Chinmay Datar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computation, Information and Technology</orgName>
								<orgName type="department" key="dep2">Institute for Advanced Study</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qing</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computation, Information and Technology</orgName>
								<orgName type="department" key="dep2">Institute for Advanced Study</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">+</forename><surname>Felix</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computation, Information and Technology</orgName>
								<orgName type="department" key="dep2">Institute for Advanced Study</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Sampling weights of deep neural networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-11-12">12 Nov 2023</date>
						</imprint>
					</monogr>
					<idno type="MD5">06AD1436239551E5F7C8608EDF20DE2E</idno>
					<idno type="arXiv">arXiv:2306.16830v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a probability distribution, combined with an efficient sampling algorithm, for weights and biases of fully-connected neural networks. In a supervised learning context, no iterative optimization or gradient computations of internal network parameters are needed to obtain a trained network. The sampling is based on the idea of random feature models. However, instead of a data-agnostic distribution, e.g., a normal distribution, we use both the input and the output training data to sample shallow and deep networks. We prove that sampled networks are universal approximators. For Barron functions, we show that the L 2 -approximation error of sampled shallow networks decreases with the square root of the number of neurons. Our sampling scheme is invariant to rigid body transformations and scaling of the input data, which implies many popular pre-processing techniques are not required. In numerical experiments, we demonstrate that sampled networks achieve accuracy comparable to iteratively trained ones, but can be constructed orders of magnitude faster. Our test cases involve a classification benchmark from OpenML, sampling of neural operators to represent maps in function spaces, and transfer learning using well-known architectures.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Training deep neural networks involves finding all weights and biases. Typically, iterative, gradientbased methods are employed to solve this high-dimensional optimization problem. Randomly sampling all weights and biases before the last, linear layer circumvents this optimization and results in much shorter training time. However, the drawback of this approach is that the probability distribution of the parameters must be chosen. Random projection networks <ref type="bibr" target="#b53">[54]</ref> or extreme learning machines <ref type="bibr" target="#b29">[30]</ref> involve weight distributions that are completely problem-and data-agnostic, e.g., a normal distribution. In this work, we introduce a data-driven sampling scheme to construct weights and biases close to gradients of the target function (cf. Figure <ref type="figure">1</ref>). This idea provides a solution to three main challenges that have prevented randomly sampled networks to compete successfully against iterative training in the setting of supervised learning: deep networks, accuracy, and interpretability.</p><p>Figure <ref type="figure">1</ref>: Random feature models choose weights in a data-agnostic way, compared to sampling them where it matters: at large gradients. The arrows illustrate where the network weights are placed.</p><p>Deep neural networks. Random feature models and extreme learning machines are typically defined for networks with a single hidden layer. Our sampling scheme accounts for the high-dimensional ambient space that is introduced after this layer, and thus deep networks can be constructed efficiently.</p><p>Approximation accuracy. Gradient-based, iterative approximation can find accurate solutions with a relatively small number of neurons. Randomly sampling weights using a data-agnostic distribution often requires thousands of neurons to compete. Our sampling scheme takes into account the given training data points and function values, leading to accurate and width-efficient approximations. The distribution also leads to invariance to orthogonal transformations and scaling of the input data, which makes many common pre-processing techniques redundant.</p><p>Interpretability. Sampling weights and biases completely randomly, i.e., without taking into account the given data, leads to networks that do not incorporate any information about the given problem. We analyze and extend a recently introduced weight construction technique <ref type="bibr" target="#b26">[27]</ref> that uses the direction between pairs of data points to construct individual weights and biases. In addition, we propose a sampling distribution over these data pairs that leads to efficient use of weights; cf. Figure <ref type="figure">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Regarding random Fourier features, Li et al. <ref type="bibr" target="#b40">[41]</ref> and Liu et al. <ref type="bibr" target="#b42">[43]</ref> review and unify theory and algorithms of this approach. Random features have been used to approximate input-output maps in Banach spaces <ref type="bibr" target="#b49">[50]</ref> and solve partial differential equations <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b9">10]</ref>. Gallicchio and Scardapane <ref type="bibr" target="#b27">[28]</ref> provide a review of deep random feature models, and discuss autoencoders and reservoir computing (resp. echo-state networks <ref type="bibr" target="#b33">[34]</ref>). The latter are randomly sampled, recurrent networks to model dynamical systems <ref type="bibr" target="#b5">[6]</ref>. Regarding construction of features, Monte Carlo approximation of data-dependent parameter distributions is used towards faster kernel approximation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b46">47]</ref>. Our work differs in that we do not start with a kernel and decompose it into random features, but we start with a practical and interpretable construction of random features and then discuss their approximation properties. This may also help to construct activation functions similar to collocation <ref type="bibr" target="#b61">[62]</ref>. Fiedler et al. <ref type="bibr" target="#b22">[23]</ref> and Fornasier et al. <ref type="bibr" target="#b23">[24]</ref> prove that for given, comparatively small networks with one hidden layer, all weights and biases can be recovered exactly by evaluating the network at specific points in the input space. The work of Spek et al. <ref type="bibr" target="#b59">[60]</ref> showed a certain duality between weight spaces and data spaces, albeit in a purely theoretical setting. Recent work from Bollt <ref type="bibr" target="#b6">[7]</ref> analyzes individual weights in networks by visualizing the placement of ReLU activation functions in space. Regarding approximation errors and convergence rates of networks, Barron spaces are very useful <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b19">20]</ref>, also to study regularization techniques, esp. Tikohnov and Total Variation <ref type="bibr" target="#b39">[40]</ref>. A lot of work <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b64">65]</ref> surrounds the approximation rate of O(m -1/2 ) for neural networks with one hidden layer of width m, originally proved by Barron <ref type="bibr" target="#b1">[2]</ref>. The rate, but not the constant, is independent of the input space dimension. This implies that neural networks can mitigate the curse of dimensionality, as opposed to many approximation methods with fixed, non-trained basis functions <ref type="bibr" target="#b13">[14]</ref>, including random feature models with data-agnostic probability distributions. The convergence rates of over-parameterized networks with one hidden layer is considered in <ref type="bibr" target="#b18">[19]</ref>, with a comparison to the Monte Carlo approximation. In our work, we prove the same convergence rate for our networks. Regarding deep networks, E and Wojtowytsch <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> discuss simple examples that are not Barron functions, i.e., cannot be represented by shallow networks. Shallow <ref type="bibr" target="#b14">[15]</ref> and deep random feature networks <ref type="bibr" target="#b28">[29]</ref> have also been analyzed regarding classification accuracy. Regarding different sampling techniques, Bayesian neural networks are prominent examples <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b60">61]</ref>. The goal is to learn a good posterior distribution and ultimately express uncertainty around both weights and the output of the network. These methods are computationally often on par with or worse than iterative optimization. In this work, we directly relate data points and weights, while Bayesian neural networks mostly employ distributions only over the weights. Generative modeling has been proposed as a way to sample weights from existing, trained networks <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b50">51]</ref>. It may be interesting to consider our sampled weights as training set in this context. In the lottery ticket hypothesis <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b7">8]</ref>, "winning" subnetworks are often not trained, but selected from a randomly initialized starting network, which is similar to our approach. Still, the score computation during selection requires gradient updates. Most relevant to our work is the weight construction method by Galaris et al. <ref type="bibr" target="#b26">[27]</ref>, who proposed to use pairs of data points to construct weights. Their primary goal was to randomly sample weights that capture low-dimensional structures. No further analysis was provided, and only a uniform distribution over the data pairs was used. We expand and analyze their setting here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Mathematical framework</head><p>We introduce sampled networks, which are neural networks where each pair of weight and bias of all hidden layers is completely determined by two points from the input space. This duality between weights and data has been shown theoretically <ref type="bibr" target="#b59">[60]</ref>, here, we provide an explicit relation. The weights are constructed using the difference between the two points, and the bias is the inner product between the weight and one of the two points. After all hidden layers are constructed, we must only solve an optimization problem for the coefficients of a linear layer, mapping the output from the last hidden layer to the final output. We start to formalize this construction by introducing some notation.</p><p>Let X ⊆ R D be the input space with ∥•∥ being the Euclidean norm with inner product ⟨•, •⟩. Further, let Φ be a neural network with L hidden layers, parameters {W l , b l } L+1 l=1 , and activation function ϕ : R → R. For x ∈ X , we write Φ (l) (x) = ϕ(W l Φ (l-1) (x) -b l ) as the output of the lth layer, with Φ (0) (x) = x. The two activation functions we focus on are the rectified linear unit (ReLU), ϕ(x) = max{x, 0}, and the hyperbolic tangent (tanh). We set N l to be the number of neurons in the lth layer, with N 0 = D and N L+1 as the output dimension. We write w l,i for the ith row of W l and b l,i for the ith entry of b l . Building upon work of Galaris et al. <ref type="bibr" target="#b26">[27]</ref>, we now introduce sampled networks. The probability distribution to sample pairs of data points is arbitrary here, but we will refine it in Definition 2. We use L to denote the loss of our network we would like to minimize. Definition 1. Let Φ be a neural network with L hidden layers. For l = 1, . . . , L, let (x</p><formula xml:id="formula_0">(1) 0,i , x (2) 0,i ) N l i=1</formula><p>be pairs of points sampled over X × X . We say Φ is a sampled network if the weights and biases of every layer l = 1, 2, . . . , L and neurons i = 1, 2, . . . , N l , are of the form</p><formula xml:id="formula_1">w l,i = s 1 x (2) l-1,i -x (1) l-1,i ∥x (2) l-1,i -x<label>(1)</label></formula><formula xml:id="formula_2">l-1,i ∥ 2 , b l,i = ⟨w l,i , x<label>(1)</label></formula><formula xml:id="formula_3">l-1,i ⟩ + s 2 ,<label>(1)</label></formula><p>where</p><formula xml:id="formula_4">s 1 , s 2 ∈ R are constants, x<label>(j)</label></formula><p>l-1,i = Φ (l-1) (x</p><formula xml:id="formula_5">(j) 0,i ) for j = 1, 2,<label>and x (1)</label></formula><formula xml:id="formula_6">l-1,i ̸ = x<label>(2)</label></formula><p>l-1,i . The last set of weights and biases are</p><formula xml:id="formula_7">W L+1 , b L+1 = arg min L(W L+1 Φ (L) (•) -b L+1 ).</formula><p>The constants s 1 , s 2 are used to fix what values the activation function takes on when it is applied to the points x (1) , x (2) ; cf. Figure <ref type="figure">2</ref>. For ReLU, we set s 1 = 1 and s 2 = 0, so that ϕ x (1) = 0 and ϕ x (2) = 1. For tanh, we set s 1 = 2s 2 and s 2 = ln(3)/2, which implies ϕ x (1) = 1/2 and ϕ x (2) = -1/2, respectively, and ϕ 1/2 x (1) + x (2) = 0. This means that in a regression problem with ReLU, we linearly interpolate values between the two points. For classification, the tanh construction introduces a boundary if x (1) belongs to a different class than x (2) . We will use this idea later to define a useful distribution over pairs of points (cf. Definition 2).</p><p>Figure <ref type="figure">2</ref>: Placement of the point pairs x (1) , x (2) for activation functions ReLU (left) and tanh (right). Two data pairs are chosen in each subfigure, resulting in two activation functions on each data domain.</p><p>The space of functions that sampled networks can approximate is not immediately clear. First, we are only using points in the input space to construct both the weights and the biases, instead of letting them take on any value. Second, there is a dependence between the bias and the direction of the weight. Third, for deep networks, the sampling space changes after each layer. These apparent restrictions require investigation into which functions we can approximate. We assume that the input space in Theorem 1 and Theorem 2 extends into its ambient space R D as follows. Let X ′ be any compact subset of R D with finite reach τ &gt; 0. Informally, such a set has a boundary that does not change too quickly <ref type="bibr" target="#b11">[12]</ref>. We then set the input space X to be the space of points including X ′ and those that are at most ϵ I away from X ′ , given the canonical distance function in R D , where 0 &lt; ϵ I &lt; τ . In Theorem 1, we also consider L layers to show that the construction of weights in deep networks does not destroy the space of functions that networks with one hidden layer can approximate, even though we alter the space of weights we can construct when L &gt; 1.</p><p>Theorem 1. For any number of layers L ∈ N &gt;0 , the space of sampled networks with L hidden layers, Φ : X → R N L+1 , with activation function ReLU, is dense in C(X , R N L+1 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sketch of proof:</head><p>We show that the possible biases that sampled networks can produce are all we need inside a neuron, and the rest can be added in the last linear part and with additional neurons. We then show that we can approximate any neural network with one hidden layer with at most 6 • N 1 neurons -which is not much, considering the cost of sampling versus backpropagation. We then show that we can construct weights so that we preserve the information of X through the first L -1 layers, and then we use the arbitrary width result applied to the last hidden layer. The full proof can be found in Appendix A.1. We also prove a similar theorem for networks with tanh activation function and one hidden layer. The proof differs fundamentally, because tanh is not positive homogeneous.</p><p>We now show existence of sampled networks for which the L 2 approximation error of Barron functions is bounded (cf. Theorem 2). We later demonstrate that we can actually construct such networks (cf. Section 4.1). The Barron space <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b19">20]</ref> is defined as</p><formula xml:id="formula_8">B = {f : f (x) = Ω w 2 ϕ(⟨w 1 , x⟩ -b)dµ(b, w 1 , w 2 ) and ∥f ∥ B &lt; ∞} with ϕ being the ReLU function, Ω = R × R D × R,</formula><p>and µ being a probability measure over Ω. The Barron norm is defined as</p><formula xml:id="formula_9">∥f ∥ B = inf µ max (b,w1,w2)∈supp(µ) {|w 2 |(∥w 1 ∥ 1 + |b|)}.</formula><p>Theorem 2. Let f ∈ B and X = [0, 1] D . For any N 1 ∈ N &gt;0 , ϵ &gt; 0, and an arbitrary probability measure π, there exist sampled networks Φ with one hidden layer, N 1 neurons, and ReLU activation function, such that</p><formula xml:id="formula_10">∥f -Φ∥ 2 2 = X |f (x) -Φ(x)| 2 dπ(x) &lt; (3 + ϵ)∥f ∥ 2 B N 1 .</formula><p>Sketch of proof: It quickly follows from the results of E et al. <ref type="bibr" target="#b19">[20]</ref>, which showed it for regular neural networks, and Theorem 1. The full proof can be found in Appendix A.2.</p><p>Up until now, we have been concerned with the space of sampling networks, but not with the distribution of the parameters. We found that putting emphasis on points that are close and differ a lot with respect to the output of the true function works well. As we want to sample layers sequentially, and neurons in each layer independently from each other, we define a layer-wise conditional definition underneath. The norms ∥•∥ X l-1 and ∥•∥ Y , that defines the following densities, are arbitrary over their respective space, denoted by the subscript.</p><p>Definition 2. Let f : R D → R N L+1 be Lipschitz-continuous and set Y = f (X ). For any l ∈ {1, 2, . . . , L}, setting ϵ = 0 when l = 1 and otherwise ϵ &gt; 0, we define</p><formula xml:id="formula_11">q ϵ l x (1) 0 , x<label>(2)</label></formula><formula xml:id="formula_12">0 | {W j , b j } l-1 j=1 =      ∥f (x (2) 0 ) -f (x (1) 0 )∥ Y max{∥x (2) l-1 -x (1) l-1 ∥ X l-1 , ϵ} , x<label>(1)</label></formula><formula xml:id="formula_13">l-1 ̸ = x (2) l-1 0, otherwise,<label>(2)</label></formula><p>where x</p><p>(1)</p><formula xml:id="formula_14">0 , x (2) 0 ∈ X , x<label>(1)</label></formula><formula xml:id="formula_15">l-1 = Φ (l-1) (x<label>(1)</label></formula><p>0 ), and x</p><p>(2)</p><formula xml:id="formula_16">l-1 = Φ (l-1) (x<label>(2)</label></formula><p>0 ), with the network Φ (l-1) parameterized by sampled {W j , b j } l-1 j=1 . Then, using λ as the Lebesgue measure, we define the integration constant C l = X ×X q ϵ l dλ. The density p ϵ l to sample pairs of points for weights and biases in layer l is equal to q ϵ l /C l if C l &gt; 0, and uniform over X × X otherwise.</p><p>Note that here, a distribution over pair of points is equivalent to a distribution over weights and biases, and the additional ϵ is a regularization term. Now we can sample for each layer sequentially, starting with l = 1, using the conditional density p ϵ l . This induces a probability distribution P over the full parameter space, which, with the given regularity conditions on X and f , is a valid probability distribution. For a complete definition of P and proof of validity, see Appendix A.3.</p><p>Using this distribution also comes with the benefit that sampling and training are not affected by rigid body transformations (affine transformation with orthogonal matrix) and scaling, as long as the true function f is equivariant w.r.t. to the transformation. That is, if H is such a transformation, we say f is equivariant with respect to H, if there exists a scalar and rigid body transformation H ′ such that H ′ (f (x)) = f (H(x)) for all x ∈ X , and invariant if H ′ is the identity function. We also assume that norms ∥•∥ Y and ∥•∥ X0 in Equation ( <ref type="formula" target="#formula_6">2</ref>) are chosen such that orthogonal matrix multiplication is an isometry. Theorem 3. Let f be the target function and equivariant w.r.t. to a scalar and rigid body transformation H. If we have two sampled networks, Φ, Φ, with the same number of hidden layers L and neurons N 1 , . . . , N L , where Φ : X → R N L+1 and Φ : H(X ) → R N L+1 , then the following statements hold for all x ∈ X :</p><p>(1) If</p><p>x(1)</p><formula xml:id="formula_17">0,i = H(x<label>(1)</label></formula><p>0,i ) and x(2)</p><formula xml:id="formula_18">0,i = H(x (2) 0,i ) for all i = 1, 2, . . . , N 1 , then Φ (1) (x) = Φ(1) (H(x)).</formula><p>(2) If f is invariant w.r.t. H, then for any parameters of Φ, there exist parameters of Φ such that Φ(x) = Φ(H(x)), and vice versa.</p><p>(3) The probability measure P over the parameters is invariant under H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sketch of proof:</head><p>Any neuron in the sampled network can be written as ϕ(⟨s 1 w ∥w∥ 2 , x -x (1) ⟩ -s 2 ). As we divide by the square of the norm of w, the scalar in H cancels. There is a difference between two points in both inputs of ⟨•, •⟩, which means the translation cancels. Orthogonal matrices cancel due to isometry. When f is invariant with respect to H, the loss function is also unchanged and lead to the same output. Similar argument is made for P , and the theorem follows (cf. Appendix A.3).</p><p>If the input is embedded in a higher-dimensional ambient space R D ′ , with D &lt; D ′ , we sample from a subspace with dimension D = dim(span{X }) ≤ D ′ . All the results presented in this section still hold due to orthogonal decomposition. However, the standard approach of backpropagation and initialization allows the weights to take on any value in R D ′ , which implies a lot of redundancy when D ≪ D ′ . The biases are also more relevant to the input space than when initialized to zero -potentially avoiding the issues highlighted by Holzmüller and Steinwart <ref type="bibr" target="#b31">[32]</ref>. For these reasons, we have named the proposed method Sampling Where It Matters (SWIM), which is summarized in Algorithm 1. For computational reasons, we consider a random subset of all possible pairs of training points when sampling weights and biases.</p><p>We end this section with a time and memory complexity analysis of Algorithm 1. In Table <ref type="table">1</ref>, we list runtime and memory usage for three increasingly strict assumptions. The main assumption is that the dimension of the output is less than or equal to the largest dimension of the hidden layers. This is true for the problems we consider, and the difference in runtime without this assumption is only reflected in the linear optimization part. The term ⌈N/M ⌉, i.e., integer ceiling of N/M , is required because only a subset of points are considered when sampling. For the full analysis, see Appendix F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Numerical experiments</head><p>We now demonstrate the performance of Algorithm 1 on numerical examples. Our implementation is based on the numpy and scipy Python libraries, and we run all experiments on a machine with 32GB system RAM (256GB in Section 4.3 and Section 4.4) and a GeForce 4x RTX 3080 Turbo GPU with 10GB RAM. The Appendix contains detailed information on all experiments. In Section 4.1 we compare sampling to random Fourier feature models regarding the approximation of a Barron function. In Section 4.2 we compare classification accuracy of sampled networks to iterative, gradientbased optimization in a classification benchmark with real datasets. In Section 4.3 we demonstrate that more specialized architectures can be sampled, by constructing deep neural architectures as PDE solution operators. In Section 4.4 we show how to use sampling of fully-connected layers for transfer learning. For the probability distribution over the pairs in Algorithm 1, we always choose the L ∞ norm for ∥•∥ Y and for l = 1, 2, . . . , L, we choose the L 2 norm for ∥•∥ X l-1 . The code to reproduce the experiments from the paper, and an up-to-date code base, can be found at <ref type="url" target="https://gitlab.com/felix.dietrich/swimnetworks-paper">https://gitlab.com/felix.dietrich/swimnetworks-paper</ref>, <ref type="url" target="https://gitlab.com/felix.dietrich/swimnetworks">https://gitlab.com/felix.dietrich/swimnetworks</ref>.</p><p>Algorithm 1: The SWIM algorithm, for activation function ϕ, and norms on input, output of the hidden layers, and output space, ∥•∥ X0 ,∥•∥ X l , and ∥•∥ Y respectively. Also, L is a loss function, which in our case is always L 2 loss, and arg min L(•, •) becomes a linear optimization problem.</p><formula xml:id="formula_19">Constant :ϵ ∈ R &gt;0 , ς ∈ N &gt;0 , L ∈ N &gt;0 , {N l ∈ N &gt;0 } L+1 l=1 , and s 1 , s 2 ∈ R Data: X = {x i : x i ∈ R D , i = 1, 2, . . . , M }, Y = {y i : f (x i ) = y i ∈ R N L+1 , i = 1, 2, . . . , M } Φ (0) (x) = x; for l = 1, 2, . . . , L do M ← ς • ⌈ N l M ⌉ • M ; P (l) ∈ R M ; P (l) i ← 0 ∀i; X = {(x (1) i , x (2) i ) : Φ (l-1) (x (1) i ) ̸ = Φ (l-1) (x (2) i )} M i=1 ∼ Uniform(X × X); for i = 1, 2, . . . , M do x(1) i , x<label>(2)</label></formula><formula xml:id="formula_20">i ← Φ (l-1) (x<label>(1)</label></formula><p>i ), Φ (l-1) (x</p><formula xml:id="formula_21">(2) i ); ỹ(1) i , ỹ<label>(2)</label></formula><formula xml:id="formula_22">i = f (x (1) i ), f (x<label>(2)</label></formula><p>i );</p><formula xml:id="formula_23">P (l) i ← ∥ỹ (2) i -ỹ (1) i ∥ Y max{∥x (2) i -x (1) i ∥ X l-1 ,ϵ} ; end W l ∈ R N l ,N l-1 , b l ∈ R N l ; for k = 1, 2, . . . , N l do</formula><p>Sample (x (1) , x (2) ) from X, with replacement and with probability proportional to P (l) ;</p><p>x(1) , x(2) ← Φ (l-1) (x (1) ), Φ (l-1) (x (2) );</p><formula xml:id="formula_24">W (k,:) l ← s 1 x(2) -x (1) ∥x (2) -x (1) ∥ 2 ; b (k) l ← ⟨W (k,:) l , x(1) ⟩ + s 2 ; end Φ (l) (•) ← ϕ(W l Φ (l-1) (•) -b l ); end W L+1 , b L+1 ← arg min L(Φ (L) (X), Y ); return {W l , b l } L+1 l=1</formula><p>Table <ref type="table">1</ref>: Runtime and memory requirements for training sampled neural networks with the SWIM algorithm, where N = max{N 0 , N 1 , N 2 , . . . , N L }. Assumption (I) is that the output dimension is less than or equal to N . Assumption (II) adds that N &lt; M 2 , i.e., number of neurons and input dimension is less than the size of dataset squared. Assumption (III) requires a fixed architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Runtime</head><p>Memory</p><formula xml:id="formula_25">Assumption (I) O(L • M (min{⌈N/M ⌉, M } + N 2 )) O(M • min{⌈N/M ⌉, M } + LN 2 ) Assumption (II) O(L • M (⌈N/M ⌉ + N 2 )) O(M • ⌈N/M ⌉ + LN 2 ) Assumption (III) O(M ) O(M )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Illustrative example: approximating a Barron function</head><p>We compare random Fourier features and our sampling procedure on a test function for neural networks <ref type="bibr" target="#b63">[64]</ref>: f (x) = 3/2(∥x -a∥ -∥x + a∥), with x ∈ R D and the vector a ∈ R D defined by a j = 2j/D -1, j = 1, 2, . . . , D. The Barron norm of f is equal to one for all input dimensions, and it can be represented exactly with a network with one infinitely wide hidden layer, ReLU activation, and weights uniformly distributed on a sphere of radius D 1/2 . We approximate f using networks f of up to three hidden layers. The error is defined by e 2 rel = x∈X (f (x) -f (x)) 2 / x∈X f (x) 2 . We compare this error over the domain X = [-1, 1] 2 , with 10, 000 points sampled uniformly, separately for training and test sets. For random features, we use w ∼ N (0, 1), b ∼ U (-π, π), as proposed in <ref type="bibr" target="#b53">[54]</ref>, and ϕ = sin. For sampling, we also use ϕ = sin to obtain a fair comparison. We also observed similar accuracy results when repeating the experiment with the tanh function. The number of neurons m is the same in each hidden layer and ranges from m = 64 up to m = 4096. Figure <ref type="figure" target="#fig_0">3</ref> shows results for D = 5, 10 (results are similar for D = 2, 3, 4, and sampled networks can be constructed as fast as the random feature method, cf. Appendix B). Random features here have</p><p>10 2 10 3 Network width 10 3 10 2 10 1 10 0 Rel. L 2 error, dim=5 randomfeatures, L=1 sampling, L=1 randomfeatures, L=2 sampling, L=2 randomfeatures, L=3 sampling, L=3 reference m 1/2 reference m 1 10 2 10 3 Network width 10 3 10 2 10 1 10 0 Rel. L 2 error, dim=10 randomfeatures, L=1 sampling, L=1 randomfeatures, L=2 sampling, L=2 randomfeatures, L=3 sampling, L=3 reference m 1/2 reference m 1 comparable accuracy for networks with one hidden layer, but very poor performance for deeper networks. This may be explained by the much larger ambient space dimension of the data after it is processed through the first hidden layer. With our sampling method, we obtain accurate results even with more layers. The convergence rate for D &lt; 10 seems to be faster than the theoretical rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Classification benchmark from OpenML</head><p>We use the "OpenML-CC18 Curated Classification benchmark" <ref type="bibr" target="#b3">[4]</ref> with all its 72 tasks to compare our sampling method to the Adam optimizer <ref type="bibr" target="#b35">[36]</ref>. With both methods, we separately perform neural architecture search, changing the number of hidden layers from 1 to 5. All layers always have 500 neurons. Details of the training are in Appendix C. Figure <ref type="figure" target="#fig_1">4</ref> shows the benchmark results. On all tasks, sampling networks is faster than training them iteratively (on average, 30 times faster). The classification accuracy is comparable (cf. Figure <ref type="figure" target="#fig_1">4</ref>, second and third plot). The best number of layers for each problem is slightly higher for the Adam optimizer (cf. Figure <ref type="figure" target="#fig_1">4</ref>, fourth plot).</p><p>2 1 0 1 2 Fit time (log 10) 0 2 4 6 8 10 12 14 16 18 Count Adam Sampling 0.0 0.2 0.4 0.6 0.8 1.0 Accuracy Adam 0.0 0.2 0.4 0.6 0.8 1.0 Accuracy Sampling 0.2 0.0 0.2 Accuracy (Adam -Sampling) 0 2 4 6 8 10 12 14 16 18 Count Count Average 4 2 0 2 4 Layer difference (Adam -Sampling) 0 2 4 6 8 10 12 14 16 18 Count </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Deep neural operators</head><p>We sample deep neural operators and compare their speed and accuracy to iterative gradient-based training of the same architectures. As a test problem, we consider Burgers' equation, ∂u ∂t + u ∂u ∂x = ν ∂ 2 u ∂ 2 x , x ∈ (0, 2π), t ∈ (0, 1], with periodic boundary conditions and viscosity ν = 0.1. The goal is to predict the solution at t = 1 from the initial condition at t = 0. Thus, we construct neural operators that represent the map G : u(x, 0) → u(x, 1). We generate initial conditions by sampling five Fourier coefficients of lowest frequency and restoring the function values from these coefficients. Using a classical numerical solver, we generate 15000 pairs of (u(x, 0), u(x, 1)), and split them into the train (60%), validation (20%), and test sets (20%). Figure <ref type="figure" target="#fig_2">5</ref> shows samples from the generated dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Fully-connected network in signal space</head><p>The first baseline for the task is a fully-connected network (FCN) trained with tanh activation to predict the discretized solution from the discretized initial condition. We trained the classical version using the Adam optimizer and the mean squared error as a loss function. We also performed early stopping based on the mean relative L 2 -error on the validation set. For sampling, we use Algorithm 1 to construct a fully-connected network with tanh as the activation function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Fully-connected network in Fourier space</head><p>Similarly to Poli et al. <ref type="bibr" target="#b52">[53]</ref>, we train a fully-connected network in Fourier space. For training, we perform a Fourier transform on the initial condition and the solution, keeping only the lowest frequencies. We always split complex coefficients into real and imaginary parts, and train a standard FCN on the transformed data. The reported metrics are in signal space, i.e., after inverse Fourier transform. For sampling, we perform exactly the same pre-processing steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">POD-DeepONet</head><p>The third architecture considered here is a variation of a deep operator network (DeepONet) architecture <ref type="bibr" target="#b43">[44]</ref>. The original DeepONet consists of two trainable components: the trunk net, which transforms the coordinates of an evaluation point, and the branch net, which transforms the function values on some grid. The outputs of these nets are then combined into the predictions of the whole network G(u)(y) = p k=1 b k (u)t k (y) + b 0 , where u is a discretized input function; y is an evaluation point; [t 1 , . . . , t p ] T ∈ R p are the p outputs of the trunk net; [b 1 , . . . , b p ] T ∈ R p are the p outputs of the branch net; and b 0 is a bias. DeepONet sets no restrictions on the architecture of the two nets, but often fully-connected networks are used for one-dimensional input. POD-DeepONet proposed by Lu et al. <ref type="bibr" target="#b45">[46]</ref> first assumes that evaluation points lie on the input grid. It performs proper orthogonal decomposition (POD) of discretized solutions in the train data and uses its components instead of the trunk net to compute the outputs</p><formula xml:id="formula_26">G(u)(ξ j ) = p k=1 b k (u)ψ k (ξ j ) + ψ 0 (ξ j ).</formula><p>Here [ψ 1 (ξ j ), . . . , ψ p (ξ j )] are p precomputed POD components for a point ξ j , and ψ 0 (ξ j ) is the mean of discretized solutions evaluated at ξ j . Hence, only the branch net is trained in POD-DeepONet. We followed Lu et al. <ref type="bibr" target="#b45">[46]</ref> and applied scaling of 1/p to the network output. For sampling, we employ orthogonality of the components and turn POD-DeepONet into a fully-connected network. Let ξ = [ξ 1 , . . . , ξ n ] be the grid used to discretize the input function u and evaluate the output function G(u). Then the POD components of the training data are</p><formula xml:id="formula_27">Ψ(ξ) = [ψ 1 (ξ), . . . , ψ p (ξ)] ∈ R n×p . If b(u) ∈ R p is the output vector of the trunk net, the POD-DeepONet transformation can be written G(u)(ξ) = Ψb(u) + ψ 0 (ξ). As Ψ T Ψ = I p , we can express the output of the trunk net as b(u) = Ψ T (G(u)(ξ) -ψ 0 (ξ)).</formula><p>Using this equation, we can transform the training data to sample a fully-connected network for b(u). We again use tanh as the activation function for sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Fourier Neural Operator</head><p>The concept of a Fourier Neural Operator (FNO) was introduced by Li et al. <ref type="bibr" target="#b41">[42]</ref> to represent maps in function spaces. An FNO consists of Fourier blocks, combining a linear operator in the Fourier space and a skip connection in signal space. As a first step, FNO lifts an input signal to a higher dimensional channel space. Let v t ∈ R n×dv be an input to a Fourier block having d v channels and discretized with n points. Then, the output v t+1 of the Fourier block is computed as</p><formula xml:id="formula_28">v t+1 = ϕ(F -1 k (R•F k (v t ))+W •v t ) ∈ R n×dv .</formula><p>Here, F k is a discrete Fourier transform keeping only Model width layers mean rel. L 2 error Time Adam FCN; signal space 1024 2 4.48 × 10 -3 644s GPU FCN; Fourier space 1024 1 3.29 × 10 -3 1725s POD-DeepONet 2048 4 1.62 × 10 -3 4217s FNO n/a 4 0.38 × 10 -3 3119s Sampling FCN; signal space 4096 1 0.92 × 10 -3 20s CPU FCN; Fourier space 4096 1 1.08 × 10 -3 16s POD-DeepONet 4096 1 0.85 × 10 -3 21s FNO 4096 1 0.94 × 10 -3 387s the k lowest frequencies and F -1 k is the corresponding inverse transform; ϕ is an activation function; • is a spectral convolution; • is a 1 × 1 convolution with bias; and R ∈ C k×dv×dv , W ∈ R dv×dv are learnable parameters. FNO stacks several Fourier blocks and then projects the output signal to the target dimension. The projection and the lifting operators are parameterized with neural networks. For sampling, the construction of convolution kernels is not possible yet, so we cannot sample FNO directly. Instead, we use the idea of FNO to construct a neural operator with comparable accuracy. Similar to the original FNO, we normalize the input data and append grid coordinates to it before lifting. Then, we draw the weights from a uniform distribution on [-1, 1] to compute the 1 × 1 lifting convolution. We first apply the Fourier transform to both input and target data, and then train a fully-connected network for each channel in Fourier space. We use skip connections, as in the original FNO, by removing the input data from the lifted target function during training, and then add it before moving to the output of the block. After sampling and transforming the input data with the sampled networks, we apply the inverse Fourier transform. After the Fourier block(s), we sample a fully-connected network that maps the signal to the solution.</p><p>The results of the experiments in Figure <ref type="figure" target="#fig_2">5</ref> show that sampled models are comparable to the Adamtrained ones. The sampled FNO model does not directly follow the original FNO architecture, as we are able to only sample fully-connected layers. This shows the advantage of gradient-based methods: as of now, they are applicable to much broader use cases. These experiments showcase one of the main advantages of sampled networks: speed of training. We run sampling on the CPU; nevertheless, we see a significant speed-up compared to Adam training performed on GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Transfer learning</head><p>Training deep neural networks from scratch involves finding a suitable neural network architecture <ref type="bibr" target="#b20">[21]</ref> and hyper-parameters <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b65">66]</ref>. Transfer learning aims to improve performance on the target task by leveraging learned feature representations from the source task. This has been successful in image classification <ref type="bibr" target="#b34">[35]</ref>, multi-language text classification, and sentiment analysis <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b8">9]</ref>. Here, we compare the performance of sampling with iterative training on an image classification transfer learning task. We choose the CIFAR-10 dataset <ref type="bibr" target="#b38">[39]</ref>, with 50000 training and 10000 test images. Each image has dimension 32 × 32 × 3 and must be classified into one of the ten classes. We consider ResNet50 <ref type="bibr" target="#b30">[31]</ref>, VGG19 <ref type="bibr" target="#b57">[58]</ref>, and Xception <ref type="bibr" target="#b10">[11]</ref>, all pre-trained on the ImageNet dataset <ref type="bibr" target="#b54">[55]</ref>. We freeze the weights of all convolutional layers and append one fully connected hidden layer and one output layer. We refer to these two layers as the classification head, which we sample with Algorithm 1 and compare the classification accuracy against iterative training with the Adam optimizer.</p><p>Figure <ref type="figure" target="#fig_4">6</ref> (left) shows that for a pre-trained ResNet50, the test accuracy using the sampling approach is higher than the Adam training approach for a width greater than 1024. We observe similar qualitative behavior for VGG19 and Xception (figures in Appendix E). Figure <ref type="figure" target="#fig_4">6</ref> (middle) shows that the sampling approach results in a higher test accuracy with all three pre-trained models. Furthermore, the deviation in test accuracy obtained with the sampling algorithm is very low, demonstrating that sampling is more robust to changing random seeds than iterative training. After fine-tuning the whole neural network with the Adam optimizer with a learning rate of 10 -5 , the test accuracies of sampled networks are close to the iterative approach. Thus, sampling provide a good starting point for fine-tuning the entire model. A comparison for the three models before and after fine-tuning is contained in Appendix E. Figure <ref type="figure" target="#fig_4">6</ref> (right) shows that sampling is up to two orders of magnitude faster than iterative training for  smaller widths, and around ten times faster for a width of 2048. In summary, Algorithm 1 is much faster than iterative training, yields a higher test accuracy for certain widths before fine-tuning, and is more robust with respect to changing random seeds. The sampled weights also provide a good starting point for fine-tuning of the entire model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Broader Impact</head><p>Sampling weights through data pairs at large gradients of the target function offers improvements over random feature models. In terms of accuracy, networks with relatively large widths can even be competitive to iterative, gradient-based optimization. Constructing the weights through pairs of points also allows to sample deep architectures efficiently. Sampling networks offers a straightforward interpretation of the internal weights and biases, namely, which data pairs are important. Given the recent critical discussions around fast advancement in artificial intelligence, and calls to slow it down, publishing work that potentially speeds up the development (concretely, training speed) in this area by orders of magnitude may seem irresponsible. The solid mathematical underpinning of random feature models and, now, sampled networks, combined with much greater interpretability of the individual steps during network construction, should mitigate some of these concerns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We present a data-driven sampling method for fully-connected neural networks that outperforms random feature models in terms of accuracy, and in many cases is competitive to gradient-based optimization. The time to obtain a trained network is orders of magnitude faster compared to gradientbased optimization. In addition, much fewer hyperparameters need to be optimized, as opposed to learning rate, number of training epochs, and type of optimizer.</p><p>Several open issues remain, we list the most pressing here. Many architectures like convolutional or transformer networks cannot be sampled with our method yet, and thus must still be trained with iterative methods. Implicit problems, such as the solution to PDE without any training data, are a challenge, as our distribution over the data pairs relies on known function values from a supervised learning setting. Iteratively refining a random initial guess may prove useful here. On the theory side, convergence rates for Algorithm 1 beyond the default Monte-Carlo estimate are not available yet, but are important for robust applications in engineering.</p><p>In the future, hyperparameter optimization, including neural architecture search, could benefit from the fast training time of sampled networks. We already demonstrate benefits for transfer learning here, which may be exploited for other pre-trained models and tasks. Analyzing which data pairs are sampled during training may help to understand the datasets better. We did not show that our sampling distribution results in optimal weights, so there is a possibility of even more efficient heuristics. Applications in scientific computing may benefit most from sampling networks, as accuracy and speed requirements are much higher than for many tasks in machine learning.</p><p>where x l is defined recursively as x 0 = x ∈ X and</p><formula xml:id="formula_29">x l = ϕ(W l x l-1 -b l ), l = 1, 2, . . . , L.<label>(4)</label></formula><p>The image after l layers is denoted as X l = Φ (l) (X ).</p><p>As we study the space of these networks, we find it useful to introduce the following notation. The space of all neural networks with L hidden layers, with N = [N 1 , . . . , N L ] neurons in the separate layers, is denoted as F L,N , assuming the input dimension and the output dimension are implicitly given. If each hidden layer has N neurons, and we let the input dimension and the output dimension be fixed, we write the space of all such neural networks as F L,N . We then let</p><formula xml:id="formula_30">F ∞,N = ∞ L=1 F L,N ,</formula><p>meaning the set of neural networks with N width, and arbitrary depth. Similarly,</p><formula xml:id="formula_31">F L,∞ = ∞ N =1 F L,N ,</formula><p>for arbitrary width at fixed depth. Finally,</p><formula xml:id="formula_32">F ∞,∞ = ∞ L=1 ∞ N =1</formula><p>F L,N .</p><p>We will now introduce sampled networks again, and go into depth the choice of constants. The input space X is a subset of Euclidean space, and we will work with canonical inner products ⟨•, •⟩ and their induced norms ∥•∥ if we do not explicitly specify the norm.</p><p>Definition 4. Let Φ be an neural network with L hidden layers. For l = 1, . . . , L, let</p><formula xml:id="formula_33">0,i ) N l i=1<label>(x (1) 0,i , x (2)</label></formula><p>be pairs of points sampled over X × X . We say Φ is a sampled network if the weights and biases of every layer l = 1, 2, . . . , L and neurons i = 1, 2, . . . , N l , are of the form</p><formula xml:id="formula_34">w l,i = s 1 x (2) l-1,i -x (1) l-1,i ∥x (2) l-1,i -x (1) l-1,i ∥ 2 , b l,i = ⟨w l,i , x<label>(1)</label></formula><formula xml:id="formula_35">l-1,i ⟩ + s 2<label>(5)</label></formula><p>where s 1 , s 2 ∈ R are constants related to the activation function, and x</p><formula xml:id="formula_36">(j) l-1,i = Φ (l-1) (x (j) 0,i ) for j = 1, 2, assuming x (1) l-1,i ̸ = x (2)</formula><p>l-1,i . The last set of weights and biases are</p><formula xml:id="formula_37">W L+1 , b L+1 = arg min L(W L+1 Φ (L) (•) -b L+1 )</formula><p>, where L is a suitable loss.</p><p>Remark 1. The constants s 1 , s 2 can be fixed such that for a given activation function, we can specify values at specific points, e.g., we can specify what value to map the two points x (1) and x (2) to; cf. Figure <ref type="figure">7</ref>. Also note that the points we sampled from X × X are sampled such that we have unique points after mapping the two sampled points through the first l -1 layers. This is enforced by constructing the density of the distribution we use to sample the points so that zero density is assigned to points that map to the same output of Φ (l-1) (see Definition 2).</p><p>Figure <ref type="figure">7</ref>: Illustration of the placement of point pairs x (1) , x (2) for activation functions ReLU (left) and tanh (right).</p><p>Example A.1. We consider the construction of the constants s 1 , s 2 for ReLU and tanh, as they are the two activation functions used for both the proofs and the experiments. We start by considering the activation function tanh, i.e.,</p><formula xml:id="formula_38">ϕ(x) = exp 2x -1 exp 2x +1 .</formula><p>Consider an arbitrary weight w, where we drop the subscripts for simplicity. Setting s 1 = 2 • s 2 , the input of the corresponding activation function, at the mid-point</p><formula xml:id="formula_39">x = x (1) + x (2) -x (1) 2 = x (2) +x (1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>, is</p><formula xml:id="formula_40">⟨w, x⟩ -b = w, x (2) + x (1) 2 -⟨w, x (1) ⟩ -s 2 = w, x (2) -x (1) 2 -s 2 = 2s 2 x (2) -x (1) 2 • 1 2 x (2) -x (1) , x (2) -x (1) -s 2 = s 2 -s 2 = 0.</formula><p>The output of the neuron corresponding to the input above is then zero, regardless of what the constant s 2 is. Another aspect of the constants are that we can decide activation values for the two sampled points. This can be seen with a similar calculation as above, ⟨w,</p><formula xml:id="formula_41">x (1) ⟩ -b = -s 2 .</formula><p>Letting s 2 = ln 3 2 , we see that the output of the corresponding neuron at point x (1) is</p><formula xml:id="formula_42">ϕ(-s 2 ) = exp ln(3) -1 exp ln(3) +1 = - 1 2 ,</formula><p>and for x (2) we get 1 2 . We can also consider the ReLU activation function where we choose to set s 1 = 1 and s 2 = 0. The center of the real line is then placed at x (1) , and x (2) is mapped to one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Universality of sampled networks</head><p>In this section we show that universal approximation results also holds for our sampled networks. We start by considering the ReLU function. To separate it from tanh, which is the second activation function we consider, we denote ϕ to be ReLU and ψ to be tanh. Networks with ReLU as activation function are then denoted by Φ, and Ψ are networks with tanh activation function. When we use ReLU, we set s 1 = 1 and s 2 = 0, as already discussed. We provide the notation that is used throughout Appendix A in Table <ref type="table">2</ref>.</p><p>The rest of this section is structured as follows:</p><p>1. We introduce the type of input spaces we are working with. 2. We then aim to rewrite an arbitrary fully connected neural network with one hidden layer into a sampled network. 3. This is done by first constructing neurons that add a constant to parts of the input space while leaving the rest untouched. 4. We then show that we can translate between an arbitrary neural network and a sampled network, if the weights of the former are given. This gives us the first universal approximator result, by combining this step with the former. 5. We go on to construct deep sampled networks with arbitrary width, showing that we can contain the information needed through the first L -1 layers, and then apply the result for sampled networks with one hidden layer, to the last hidden layer. 6. We conclude the section by showing that sampled networks with tanh activation functions are also universal approximators. Concretely, we show that it is dense in the space of sampled networks with ReLU activation function. The reason we need a different proof for the tanh case is that it is not positive homogeneous, a property of ReLU that we heavily depend upon for the proof of sampled networks with ReLU as activation function.</p><p>Before we can start with the proof that F 1,∞ is dense in the space of continuous functions, we start by specifying the domain/input space for the functions.</p><p>Table 2: Notation used through the theory section of this appendix. X ′ Pre-noise input space. Subset of R D and compact for Appendix A.1 and Appendix A.2. X Input space. Subset of R D , X ′ ⊂ X , and compact for Appendix A.1 and Appendix A.2. ϵ I Noise level for X ′ . Definition 5. ϕ Activation function ReLU, ϕ(x) = max{x, 0}. Φ Neural network with activation function ReLU. ψ Activation function tanh. Ψ Neural network with activation function tanh.</p><p>Φ c Constant block. 5 neurons that combined add a constant value c to parts of the input space X , while leaving the rest untouched. Definition 6. N l Number of neurons in layer l of a neural network. N 0 = D and N L+1 is the output dimension of network. w l,i The weight at the ith neuron, of the lth layer. b l,i The bias at the ith neuron, of the lth layer. Φ (l) Function that maps x ∈ X to the output of the lth hidden layer. Φ (l,i) The ith neuron for the lth layer.</p><p>X l The image of X after l layers, i.e., X l = Φ(X ). We set X 0 = X .</p><p>x l x l = Φ (l) (x). F L,[N1,N2,...,N L ] Space of neural networks with L hidden layers and N l neurons in layer l.</p><p>F 1,∞ Space of all neural networks with one hidden layer, i.e., networks with arbitrary width.</p><formula xml:id="formula_43">F S 1,∞</formula><p>Space of all sampled networks with one hidden layer and arbitrary width. ζ Function that maps points from X to X ′ , mapping to the closest point in X ′ , which is unique.</p><formula xml:id="formula_44">η Maps x ∈ X × [0, 1] to X , by η(x, δ) = x + δ(ζ(x) -x). ∥•∥ Canonical norm of R D , with inner product ⟨•⟩. B ϵ (x) The ball B ϵ (x) = {x ′ ∈ R D : ∥x -x ′ ∥ &lt; ϵ}. B ϵ (x) The closed ball B ϵ (x) = {x ′ ∈ R D : ∥x -x ′ ∥ ≤ ϵ}.</formula><p>τ Reach of the set X ′ . λ Lebesgue measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.1 Input space</head><p>We again assume that the input space X is a subset of R D . We also assume that X contains some additional additive noise. Before we can define this noisy input space, we recall two concepts.</p><p>Let (Z, d) be a metric space, and the distance between a subset A ⊆ Z and a point z ∈ Z be defined as</p><formula xml:id="formula_45">d Z (z, A) = inf{d(z, a) : a ∈ A}.</formula><p>If Z is also a normed space, the medial axis is then defined as</p><formula xml:id="formula_46">Med(A) = {z ∈ Z : ∃p ̸ = q ∈ A, ∥p -z∥ = ∥q -z∥ = d Z (z, A)} The reach of A is defined as τ A = inf a∈A d Z (a, Med(A)).</formula><p>Informally, the reach is the smallest distance from a point in the subset A to a non-unique projection of it in the complement A c . This means that the reach of convex subsets is infinite (all projections are unique), while other sets can have zero reach, which means 0 ≤ τ A ≤ ∞.</p><p>Let Z = R D , and d be the canonical Euclidean distance. Definition 5. Let X ′ be a nonempty compact subset of R D with reach τ X ′ &gt; 0. The input space X is defined as</p><formula xml:id="formula_47">X = {x ∈ R D : d Z (x, X ′ ) ≤ ϵ I },</formula><p>where 0 &lt; ϵ I &lt; min{τ X ′ , 1}. We refer to X ′ as the pre-noise input space.</p><p>Remark 2. As d Z (x, X ′ ) = 0 for all x ∈ X ′ , we have that X ′ ⊂ X . Due to d Z (x, X ′ ) ≤ ϵ I we preserve that X is closed, and as ϵ I &lt; ∞ means it is bounded, and hence compact. Informally, we have enlarged the input space with new points at most ϵ I distance away from X ′ . This preserves many properties of the pre-noise input space X ′ , while also being helpful in the proofs to come. We also argue that in practice, we are often faced with "noisy" datasets X anyway, rather than non-perturbed data X ′ .</p><p>We also define a function that maps elements in X \ X ′ down to X ′ , using the uniqueness property given by the medial axis. That is, ζ : X → X ′ is the mapping defined as ζ(x) = x ′ , for x ∈ X and x ′ ∈ X ′ , such that d(x, x ′ ) = d Z (x, X ′ ). As we also want to work along the line between these two points, we set η : X × [0, 1] → X , where η(x, δ) = x + δ(ζ(x) -x). We conclude this part with an elementary result. Lemma 1. Let x ∈ X , δ ∈ [0, 1], and x ′ = η(x, δ) ∈ X , then ∥x ′ -x∥ ≤ δ, with strict inequality when δ &gt; 0. Furthermore, when δ &gt; 0, we have x ′ / ∈ ∂X .</p><p>Proof. We start by noticing</p><formula xml:id="formula_48">d(ζ(x), x ′ ) = ∥ζ(x) -x ′ ∥ = (1 -δ)∥ζ(x) -x∥ ≤ (1 -δ)ϵ I ≤ ϵ I ,</formula><p>which means x ′ ∈ X . When δ &gt; 0, we have strict inequality, which implies</p><formula xml:id="formula_49">x ′ ∈ int X = X \ ∂X . Furthermore, d(x, x ′ ) = ∥x -x ′ ∥ = δ∥ζ(x) -x∥ ≤ δ • ϵ I ≤ δ,</formula><p>where the last inequality holds due to ϵ I &lt; 1, and is equal only when δ = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.2 Networks with a single hidden layer</head><p>We can now proceed to the proof that sampled networks with one hidden layer are indeed universal approximators. The main idea is to start off with an arbitrary network with a single hidden layer, and show that we can approximate this network arbitrarily well. Then we can rely on previous universal approximation theorems <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b51">52]</ref> to finalize the proof. We start by showing some results for a different type of neural networks, but very similar in form. We consider networks where the bias is of the same form as a sampled network. However, the weight is normalized to be a unit weight, that is, divided by the norm, and not the square of the norm. We show in Lemma 4 that the results also hold for any positive scalar multiple of the unit weight, and therefore our sampled network, where we divide by the norm of the weight squared. To be more precise, the weights are of the form</p><formula xml:id="formula_50">w l,i = x (2) l-1,i -x (1) l-1,i ∥x<label>(2)</label></formula><p>l-1,i -x</p><p>l-1,i ∥ , and biases b l,i = ⟨w l,i , x</p><p>l-1,i ⟩. Networks with weights/biases in the hidden layers of this form is referred to as unit sampled network. In addition, when the output dimension is N L+1 = 1, we split the bias of the last layer, b L+1 into N L parts, to make the proof easier to follow. This is of no consequence for the final result, as we can always sum up the parts to form the original bias. We write the different parts of the split as b L+1,i for i = 1, 2, . . . , N L .</p><p>We start by defining a constant block. This is crucial for handling the bias, as we can add constants to the output of certain parts of the input space, while leaving the rest untouched. This is important when proving Lemma 2. Definition 6. Let c 1 &lt; c 2 &lt; c 3 , and c ∈ R + . A constant block Φ c is defined as five neurons summed together as follows. For x ∈ R,</p><formula xml:id="formula_53">Φ c (x) = 5 i=1 f i (x),</formula><p>where</p><formula xml:id="formula_54">f 1 (x) = a 1 ϕ(x -c 2 ), f 2 (x) = a 1 ϕ(-(x -c 3 )), f 3 (x) = -a 1 ϕ(x -c 3 ), f 4 (x) = -a 2 ϕ(-(x -c 1 )) f 5 (x) = a 3 ϕ(x -c 1 ),</formula><p>and</p><formula xml:id="formula_55">a 1 = c c3-c2 , a 2 = a 1 c1-c3 c1-c2</formula><p>, and a 3 = a 2 -a 1 .</p><p>Remark 3. The function Φ c are constructed using neurons, but can also be written as the continuous function,</p><formula xml:id="formula_56">Φ c (x) =    0, x ≤ c 1 a 3 • x + d, c 1 &lt; x ≤ c 2 c, x &gt; c 2 ,</formula><p>where d = a 1 c 3 + a 2 c 2 . Obviously, if c needs to be negative, one can simply swap the sign on each of the three parameters, a i .</p><p>We can see by the construction of Φ c that we might need the negative of some original weight. That is, the input to Φ c is of the form ⟨w 1,i , x⟩, and for f 2 and f 4 , we require ⟨-w 1,i , x⟩. In Lemma 3, we shall see that this is not an issue and that we can construct neurons such that they approximate constant blocks arbitrarily well, as long as c 1 , c 2 , c 3 can be produced by the inner product between a weight and points in X , that is, if we can produce the biases equal to the constants c 1 , c 2 , c 3 .</p><p>Let Φ ∈ F 1,K , with parameters { ŵl,i , bl,i } 2,K l=1,i=1 , be an arbitrary neural network. Unless otherwise stated, the weights in this arbitrary network Φ are always nonzero. Sampled networks cannot have zero weights, as the point pairs used to construct weights, both for unit and regular sampled networks, are distinct. However, one can always construct the same output as neurons with zero weights by setting certain weights in W L+1 to zero.</p><p>We start by showing that, given all weights of a network, we can construct all biases in a unit sampled network so that the function values agree. More precisely, we want to construct a network with weights ŵl,i , and show that we can find points in X to construct the biases of the form in unit sampled networks, such that the resulting neural network output on X equals exactly the values of the arbitrary network Φ. Lemma 2. Let Φ ∈ F 1,K : X → R N2 . There exists a set of at most 6 • N 1 points x i ∈ X and biases b 2,i ∈ R, such that a network Φ with weights w 1,i = ŵ1,i , w 2,i = ŵ2,i , and biases b 2,i ∈ R, b 1,i = ⟨w 1,i , x i ⟩, for i = 1, 2, . . . , N 1 , satisfies Φ(x) -Φ(x) = 0 for all x ∈ X . Proof. W.l.o.g., we assume N 2 = 1. For any weight/bias pair ŵ1,i , b1,i , we let w 1,i = ŵ1,i ,</p><formula xml:id="formula_57">w 2,i = ŵ2,i , B i = {⟨w 1,i , x⟩ : x ∈ X }, b (i) ∧ = inf B i , and b (i) ∨ = sup B i . As ⟨w 1,i , •⟩ is continuous, means B i is compact and b (i) ∧ , b (i) ∨ ∈ B i .</formula><p>We have four different cases, depending on b1,i .</p><p>(1) If b1,i ∈ B i , then we simply choose a corresponding x i ∈ X such that b 1,i = ⟨w 1,i , x i ⟩ = b1,i .</p><p>Letting b 2,i = b2,i , we have</p><formula xml:id="formula_58">w 2,i ϕ(⟨w 1,i , x⟩ -b 1,i ) -b 2,i = ŵ2,i ϕ(⟨ ŵ1,i , x⟩ -b1,i ) -b2,i . (2) If b1,i &gt; b (i) ∨ , we choose x i such that b 1,i = ⟨w 1,i , x i ⟩ = b (i) ∨ and b 2,i = b2,i . As ϕ(⟨w 1,i , x⟩ - b 1,i ) = ϕ(⟨ ŵ1,i , x⟩ -b1,i ) = 0, for all x ∈ X , we are done. (3) If b1,i &lt; b (i) ∧ , we choose corresponding x i such that b 1,i = ⟨w 1,i , x i ⟩ = b (i) ∧ , and set b 2,i = b2,i + w 2,i b1,i -b (i) ∧ . We then have w 2,i ϕ(⟨w 1,i , x⟩ -b 1,i ) -b 2,i = w 2,i ⟨w 1,i , x⟩ -w 2,i b 1,i -b 2,i = w 2,i ⟨w 1,i , x⟩ -b2,i -w 2,i b1,i ± w 2,i b (i) ∧ = ŵ2,i ⟨ ŵ1,i , x⟩ -b2,i -ŵ2,i b1,i = ŵ2,i ϕ(⟨ ŵ1,i , x⟩ -b1,i ) -b2,i ,</formula><p>where first and last equality holds due to ⟨w 1,i , x⟩ &gt; b 1,i &gt; b1,i for all x ∈ X . </p><formula xml:id="formula_59">= B i ∩ [b (i) ∧ , b1,i ] and B (2) i = B i ∩ [ b1,i , b (i)</formula><p>∨ ] are both non-empty compact sets. We therefore have that supremum and infimum of both sets are members of their respective set, and thus also part of B i . We therefore choose x i such that b 1,i = ⟨w 1,i , x i ⟩ = inf B</p><p>(2) i . To make up for the difference between b1,i &lt; b 1,i , we add a constant to all x ∈ X where ⟨w 1,i , x⟩ &gt; b 1,i .</p><p>To do this we add some additional neurons, using our constant block</p><formula xml:id="formula_60">Φ (i) c (⟨w 1,i , •⟩). Letting c = w 2,i b 1,i -b1,i , c 1 = sup B (1) i , c 2 = b 1,i , and c 3 = b (i)</formula><p>∨ . We have now added five more neurons, and the weights and bias in second layer corresponding to the neurons is set to be ±a and 0, respectively, where both a and the sign of a depends on Definition 6. In case we require a negative sign in front of ⟨w 1,i , •⟩, we simply set it as we are only concerned with finding biases given weights. We then have that for all x ∈ X ,</p><formula xml:id="formula_61">Φ (i) c (⟨w 1,i , x⟩) = c, ⟨w 1,i , x⟩ &gt; b 1,i 0, otherwise.</formula><p>Finally, by letting b 2,i = b2,i , we have</p><formula xml:id="formula_62">w 2,i ϕ(⟨w 1,i , x⟩ -b 1,i ) -b 2,i + Φ (i) c (⟨w 1,i , x⟩) =w 2,i ⟨w 1,i , x⟩ -w 2,i b 1,i -b2,i + w 2,i b 1,i -ŵ2,i b1,i =⟨ ŵ1,i , x⟩ -ŵ2,i b1,i -b2,i = ŵ2,i ϕ(⟨ ŵ1,i , x⟩ b1,i ) -b2,i , when ⟨w 1,i , x⟩ &gt; b 1,i , and</formula><formula xml:id="formula_63">w 2,i ϕ(⟨w 1,i , x⟩ -b 1,i ) -b 2,i + Φ (i) c (⟨w 1,i , x⟩) = b 2,i = b2,i = ŵ2,i ϕ(⟨w 1,i , x⟩ b1,i ) -b2,i ,</formula><p>otherwise. And thus, Φ(x) = Φ(x) for all x ∈ X . As we add five additional neurons for each constant block, and we may need one for each neuron, means we need to construct our network with at most 6 • N 1 neurons. Now that we know we can construct suitable biases in our unit sampled networks for all types of biases bl,i in the arbitrary network Φ, we show how to construct the weights. Lemma 3. Let Φ ∈ F 1,K : X → R N2 , with biases of the form b1,i = ⟨ ŵ1,i , x i ⟩, where x i ∈ X for i = 1, 2, . . . , N 1 . For any ϵ &gt; 0, there exist unit sampled network Φ, such that ∥Φ -Φ∥ ∞ &lt; ϵ.</p><p>Proof. W.l.o.g., we assume N 2 = 1. For all i = 1, 2, . . . , N 1 , we construct the weights and biases as follows:</p><formula xml:id="formula_64">If x i / ∈ ∂X , then we set ϵ ′ &gt; 0 such that B ϵ ′ (x i ) ⊂ X . Let x (1) 0,i = x i and x (2) 0,i = x (1) 0,i + ϵ ′ 2 w 1,i ∈ B ϵ ′ (x i ) ⊂ X . Setting w 2,i = ∥ ŵ1,i ∥ ŵ2,i , b 2,i = b2,i , and w 1,i = x (2) 0,i -x (1) 0,i ∥x (2) 0,i -x (1) 0,i ∥ = ŵ1,i ∥ ŵ1,i∥ , implies w 2,i ϕ(⟨w 1,i , x -x (1) 0,i ⟩) -b 2,i = ∥ ŵ1,i ∥ ŵ2,i ϕ ŵ1,i ∥ ŵ1,i ∥ , x -x i -b2,i = ŵ2,i ϕ(⟨ ŵi , x -x i ⟩) -b2,i ,</formula><p>where last equality follows by ϕ being positive homogeneous.</p><p>If x i ∈ ∂X , by continuity we find δ &gt; 0 such that for all i = 1, 2, . . . , N 1 and x ′ , x ∈ X , where ∥x ′ -x i ∥ &lt; δ, we have</p><formula xml:id="formula_65">|ϕ(⟨ ŵ1,i , x -x ′ ⟩) -ϕ(⟨ ŵ1,i , x -x i ⟩)| &lt; ϵ N 1 w 2 ,</formula><p>where w 2 = max{| ŵ2,i |} N1 i=1 . We set x</p><p>(1) 0,i = η(x i , min{δ, 1}), with x (1) 0,i ∈ int X and ∥x i -x</p><p>(1) 0,i ∥ &lt; δ, due to δ &gt; 0 and Lemma 1. We may now proceed by constructing x (2) 0,i as above, and similarly setting w 2,i = ∥ ŵ1,i ∥ ŵ2,i , b 2,i = b2,i , we have</p><formula xml:id="formula_66">N1 i=1 w 2,i ϕ(⟨w 1,i , x -x (1) 0,i ⟩) -b 2,i - N1 i=1 ŵ2,i ϕ(⟨ ŵ1,i , x -x i ⟩) -b2,i = N1 i=1 ŵ2,i ϕ(⟨ ŵ1,i , x -x (1) 0,i ⟩) -ϕ(⟨ ŵ1,i , x -x i ⟩) ≤ N1 i=1 w 2 ϕ(⟨ ŵ1,i , x -x (1) 0,i ⟩) -ϕ(⟨ ŵ1,i , x -x i ⟩) &lt; N1 i=1 w 2 ϵ N 1 w 2 = ϵ,</formula><p>and thus ∥Φ -Φ∥ ∞ &lt; ϵ.</p><p>Until now, we have worked with weights of the form w = x (2) -x (1) ∥x (2) -x (1) ∥ , however, the weights in a sampled network are divided by the norm squared, not just the norm. We now show that for all the results so far, and also for any other results later on, differing by a positive scalar (such as this norm) is irrelevant when ReLU is the activation function. Lemma 4. Let Φ be a network with one hidden layer, with weights and biases of the form</p><formula xml:id="formula_67">w 1,i = x (2) 0,i -x<label>(1) 0,i ∥x (2) 0,i -x (1) 0</label></formula><formula xml:id="formula_68">,i ∥ , b 1,i = ⟨w 1,i , x<label>(1)</label></formula><p>0,i ⟩, for i = 1, 2, . . . , N 1 . For any weights and biases in the last layer, {w 2,i , b 2,i } N2 i=1 , and set of strictly positive scalars {ω i } N1 i=1 , there exist sampled networks Φ ω where weights and biases in the hidden layer {w ′ 1,i , b ′ 1,i } N1 i=1 are of the form</p><formula xml:id="formula_69">w ′ 1,i = ω i w 1,i , b ′ 1,i = ⟨w ′ 1,i , x<label>(1)</label></formula><p>0,i ⟩, such that Φ ω (x) = Φ(x) for all x ∈ X .</p><p>Proof. We set w ′ 2,i = w2,i ωi and b ′ 2,i = b 2,i . As ReLU is a positive homogeneous function, we have for all x ∈ X ,</p><formula xml:id="formula_70">w ′ 2,i ϕ(⟨w ′ 1,i , x⟩ -b ′ 1,i ) -b ′ 1,i = ω i w 2,i ω i ϕ(⟨w 1,i , x⟩ -b 1,i ) w 2,i ϕ(⟨w 1,i , x⟩ -b 1,i ).</formula><p>The result itself is not too exciting, but it allows us to use the results proven earlier, applying them to sampled networks by setting the scalar to be</p><formula xml:id="formula_71">ω i = 1 ∥x (2) 0,i -x (1) 0,i ∥ .</formula><p>We are now ready to show the universal approximation property for sampled networks with one hidden layer. We let F S 1,∞ be defined similarly to F 1,∞ , with every Φ ∈ F S 1,∞ being a sampled neural network. Theorem 4. Let g ∈ C(X , R N L+1 ). Then, for any ϵ &gt; 0, there exist Φ ∈ F S 1,∞ with ReLU activation function, such that</p><formula xml:id="formula_72">∥g -Φ∥ ∞ &lt; ϵ. That is, F S 1,∞ is dense in C(X , R N L+1 ).</formula><p>Proof. W.l.o.g., let N L+1 = 1, and in addition, let ϵ &gt; 0 and g ∈ C(X , R N L+1 ). Using the universal approximation theorem of Pinkus <ref type="bibr" target="#b51">[52]</ref>, we have that for ϵ &gt; 0, there exist a network Φ ∈ F 1,∞ , such that ∥g -Φ∥ ∞ &lt; ϵ 2 . Let K be the number of neurons in Φ. We then create a new network Φ, by first keeping the weights fixed to the original ones, { ŵ1,i , ŵ2,i } K i=1 , and setting the biases of Φ according to Lemma 2 using { b1,i , b2,i } K i=1 , adding constant blocks if necessary. We then change the weights of our Φ with the respect to the new biases, according to Lemma 3 (with the epsilon set to ϵ/2). It follows from the two lemmas that ∥Φ -</p><formula xml:id="formula_73">Φ∥ ∞ &lt; ϵ 2 . That means ∥g -Φ∥ ∞ = ∥g -Φ + Φ -Φ∥ ∞ ≤ ∥g -Φ∥ ∞ + ∥ Φ -Φ∥ ∞ &lt; ϵ.</formula><p>As the weights of the first layer of Φ can be written as</p><formula xml:id="formula_74">w 1,i = x (2) 0,i -x (1) 0,i ∥x (2) 0,i -x (1) 0,i ∥ 2 and bias b 1,i = ⟨w 1,i , x (1) 0,i ⟩, both guaranteed by Lemma 4, means Φ ∈ F S 1,∞ . Thus, F S 1,∞ is dense in C(X , R N L+1</formula><p>). Remark 4. By the same two lemmas, Lemma 2 and Lemma 3, one can show that other results regarding networks with one hidden layer with at most K neurons, also hold for sampled networks, but with 6 • K neurons, due to the possibility of one constant block for each neuron. When X is a connected set, we only need K neurons, as no additional constant blocks must be added; see proof of Lemma 2 for details. after this first hidden layer Ψ (l) (X ) is injective, and therefore bijective. To show this, let u 1 , u 2 ∈ X such that u 1 ̸ = u 2 . As the vectors in v spans X , means there exists two unique set of coefficients,</p><formula xml:id="formula_75">c 1 , c 2 ∈ R D , such that u 1 = c (i) 1 v i and u 2 = c (i) 2 v i .</formula><p>We then have</p><formula xml:id="formula_76">Φ (l) (u 1 ) = V c 1 -b l ̸ = V c 2 -b l = Φ (l) (u 2 ),</formula><p>where V is the Gram matrix of v. As vectors in v are linearly independent, V is positive definite, and combined with c 1 ̸ = c 2 , this implies the inequality. That means Φ (l) is a bijective mapping of X . As the mapping is bijective and continuous, we have that for any x ∈ X ′ , there is an ϵ (l)</p><formula xml:id="formula_77">I &gt; 0, such that B ϵ ′ I (Φ (l) (x)</formula><p>) ⊆ X . For 1 &lt; l &lt; L, we repeat the procedure, but swap X with X l-1 . As Φ (l-1) is a bijective mapping, we may find similar linear independent vectors and construct similar points x</p><formula xml:id="formula_78">(1) l-1,i , x<label>(2)</label></formula><p>l-1,i , but now with noise level ϵ (l) I . For l = L, as we have a subset of X that is a closed ball around each point in Φ l-1 (x), for every x ∈ X ′ , means we can proceed by constructing the last hidden layer and the last layer in the same way as explained when proving T heorem 4. The only caveat is that we are approximating a network with one hidden layer with the domain X l-1 , and the function we approximate is g = g • [Φ (L-1) ] -1 . Given this, denoting Φ (L:L+1) as the function of last hidden layer and the last layer, there exists a number of nodes, weights, and biases in the last hidden layer and the last layer, such that ∥g -Φ∥ ∞ = ∥g -Φ (L:L+1) ∥ ∞ &lt; ϵ, due to construction above and Theorem 4. As Φ is a unit sampled network, it follows by Lemma 4 that</p><formula xml:id="formula_79">∞ N L =1 F S L,N L is dense in C(X , R N L+1 ).</formula><p>We can now prove that sampled networks with L layers, and different dimensions in all neurons, with arbitrary width in the last hidden layer, are universal approximators, with the obvious caveat that each hidden layer l = 1, 2, . . . , L -1 needs at least D neurons, otherwise we will lose some information regardless of how we construct the network.</p><formula xml:id="formula_80">Theorem 5. Let N L = [N 1 , N 2 , . . . , N L-1 , N L ],</formula><p>where min{N l : l = 1, 2, . . . , L -1} ≥ D, and</p><formula xml:id="formula_81">L ≥ 1. Then ∞ N L =1 F S L,N L is dense in C(X , R N L+1 ).</formula><p>Proof. Let ϵ &gt; 0 and g ∈ C(X , R N L+1 ). For L = 1, Theorem 4 is enough, and we therefore assume</p><formula xml:id="formula_82">L &gt; 1. We start by constructing a network Φ ∈ ∞ N L =1 F S L, ÑL</formula><p>, where ÑL = [D, D, . . . , D, N L ]</p><formula xml:id="formula_83">according to Lemma 5, such that ∥ Φ -g∥ ∞ &lt; ϵ. To construct Φ ∈ ∞ N L =1 F S L,N L , let l = 1</formula><p>, and start by constructing weights/biases for the first D nodes according to Φ. For the additional nodes, in the first hidden layer, select an arbitrary direction w. Let X 1 = {x (j) 1,i : i = 1, 2, . . . , D and j = 1, 2} be the set of all points needed to construct the D neurons in the next layer of Φ. Then for each additional node i = D + 1, . . . , N 1 , we set</p><formula xml:id="formula_84">x (1) 0,i = arg max{⟨w, x⟩ : x ∈ X and Φ(1) (x) ∈ X 1 }. and choose x (2) 0,i ∈ X such that x (2) 0,i -x (1)</formula><p>0,i = aw, where a ∈ R &gt;0 , similar to what is done in the proof for Lemma 3. Using these points to define the weights and biases of the last N 1 -D nodes, the following space X 1 now contains points [x (j) 1,i , 0, 0, . . . , 0], for j = 1, 2 and i = 1, 2, . . . , D. For 1 &lt; l &lt; L repeat the process above, setting x</p><formula xml:id="formula_85">(j) l,i = [x (j)</formula><p>l,i , 0, 0, . . . , 0] for the first D nodes, and construct the weights of the additional nodes as described above, but with sampling space being X l-1 . When l = L, set number of nodes to the same as in Φ, and choose the points to construct the weights and biases as x</p><formula xml:id="formula_86">(j) L-1,i = [x (j)</formula><p>L-1,i , 0, 0, . . . , 0], for j = 1, 2 and i = 1, 2, . . . , N L . The weights and biases in the last layer are the same as in Φ. This implies,</p><formula xml:id="formula_87">∥Φ -g∥ ∞ = ∥ Φ -g∥ ∞ &lt; ϵ,</formula><p>and thus</p><formula xml:id="formula_88">∞ N L =1 F S L,N L is dense in C(X , R N L+1 ).</formula><p>Remark 5. Important to note that the proof is only showing existence, and that we expect networks to have a more interesting representation after the first L -1 layers. With this theorem, we can conclude that stacking layers is not necessarily detrimental for the expressiveness of the networks, even though it may alter the sampling space in non-trivial ways. Empirically, we also confirm this, with several cases performing better under deep networks -very similar to iteratively trained neural networks.</p><formula xml:id="formula_89">Corollary 1. F S ∞,∞ is dense in C(X , R N L+1 ).</formula><p>A.1.4 Networks with a single hidden layer, tanh activation</p><p>We now turn to using tanh as activation function, which we find useful for both prediction tasks, and if we need the activation function to be smooth. We will use the results for sampled networks with ReLU as activation function, and show we can arbitrarily well approximate these. The reason for this, instead of using arbitrary network with ReLU as activation function, is that we are using weights and biases of the correct form in the former, such that the tanh networks we construct will more easily have the correct form. We set s 2 = ln(3) 2 , s 1 = 2 • s 2 -as already discussed -and let ψ be the tanh function, with Ψ being neural networks with ψ as activation function -simply to separate from the ReLU ϕ, as we are using both in this section. Note that Φ is still network with ReLU as activation function and s 1 = 1, s 2 = 0.</p><p>We start by showing how a sum of tanh functions can approximate a set of particular functions.</p><formula xml:id="formula_90">Lemma 6. Let f : [c 0 , c M +1 ] → R + , defined as f (x) = M i=1 a i 1 [ci,c M +1 ] (x), with 1 being the indicator function, c 0 &lt; c 1 &lt; • • • &lt; c M &lt; c M +1</formula><p>, and for all i = 0, 1, . . . , M + 1, c i ∈ R and a i ∈ R &gt;0 . Then there exists strictly positive scalars ω = {ω i } M i=1 such that</p><formula xml:id="formula_91">g(x) = M i=1 g i (x) = M i=1 a i 2 [ψ(ω i (x -c i ) -s 2 ) + 1] , fulfills f (c i-1 ) &lt; g(x) &lt; f (c i+1 ) whenever x ∈ [c i-1 , c i+1 ]</formula><p>, for all i = 1, 2, . . . , M .</p><p>Proof. We start by observing that both functions, f and g, are increasing, with the latter strictly increasing. We also have that f</p><formula xml:id="formula_92">(c 0 ) = 0 &lt; g(x) &lt; M i=1 a i = f (c M ), for all x ∈ [c 0 , c M +1</formula><p>], regardless choice of ω. We then fix constants</p><formula xml:id="formula_93">0 &lt; δ i &lt; 3 4 a i M -i 0 &lt; ϵ i+1 &lt; ai 4</formula><p>i , for i = 1, 2, . . . , M -1. We have, due to s 2 , that g i (c i ) = ai 4 , for all i = 1, 2, . . . , M . In addition, we can always increase ω i to make sure g i (c i+1 ) is large enough, and g i (c i-1 ) small enough for our purposes, as ψ is bijective and strictly increasing. We set ω 1 large enough such that g 1 (c j ) &gt; a 1 -ϵ j , for all j = 2, 3, . . . , M -1. For i = 2, 3, . . . , M -1, we set ω i large enough such that g i (c j ) &lt; δ j , where j = 1, 2, . . . , i -1, and g i (c j ) &gt; a i -ϵ j , where j = i + 1, i + 2, . . . , M -1. Finally, let ω M be large enough such that g M (c j ) &lt; δ j , for j = 1, 2, . . . , M -1. With the strictly increasing property of every g i , we see that</p><formula xml:id="formula_94">g(c i ) = i-1 j=1 g j (c i ) + a i 4 + M j=i+1 g j (c i ) &lt; i-1 j=1 a j + a i 4 + M j=i+1 δ j = i-1 j=1 a j + a i 4 + 3a i 4 = f (c i ),<label>and</label></formula><formula xml:id="formula_95">g(c i ) = i-1 j=1 g j (c i ) + a i 4 + M j=i+1 g j (c i ) &gt; i-1 j=1 (a j -ϵ i ) + a i 4 = i-1 j=1 a j - a i 4 + a i 4 = f (c i-1 ).</formula><p>Combing the observations at the start with f (c i-1 ) &lt; g(c i ) &lt; f (c i ), for i = 1, 2, . . . , M , and the property sought after follows quickly.</p><p>We can now show that we can approximate a neuron with ReLU ϕ activation function and with unit sampled weights arbitrarily well.</p><p>Lemma 7. Let x(1) , x(2) ∈ X and ŵ2 ∈ R. For any ϵ &gt; 0, there exist a M ∈ N &gt;0 , and M pairs of distinct points {(x</p><formula xml:id="formula_96">(2) i , x<label>(1) i</label></formula><formula xml:id="formula_97">) ∈ X × X } M i=1 , such that ŵ2 ϕ(⟨ ŵ1 , x -x(1) ⟩) - M i=1 wi ψ w i , x -x (1) i -s 2 + 1 &lt; ϵ,<label>where ŵ1 = x(2) -x (1) ∥x (2) -x (1)</label></formula><p>∥ , wi ∈ R, and</p><formula xml:id="formula_98">w i = s 1 x<label>(2) i -x (1) i ∥x (2) i -x (1) i ∥ 2 .</label></formula><p>Proof. Let ϵ &gt; 0 and, w.l.o.g., ŵ2 &gt; 0.</p><formula xml:id="formula_99">Let B = {⟨ ŵ1 , x⟩ : x ∈ X }, as well as f (x) = ŵ2 ϕ(⟨ ŵ1 , x- x<label>(1)</label></formula><p>). We start by partitioning f into ϵ/4-chunks. More specifically, let c = max B, and</p><formula xml:id="formula_100">M ′ = 4 ŵ2 |c| ϵ . Set d k = (k-1)c</formula><p>M , for k = 1, 2, . . . , M ′ , M ′ + 1. We will now define points c j , with the goal of constructing a function f as in Lemma 6. Still, because we require c j ∈ B to define biases in our tanh functions later, we must define the c j s iteratively, by setting c 1 = 0, k = j = 2, and define every other c j as follows:</p><p>1. If d k = c, we are done, otherwise proceed to 2. We have M &lt; 2 • M ′ points, and can now construct the a j s of f . For j = 1, 2, . . . , M , with c M +1 = c, let</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Set</head><formula xml:id="formula_101">d ′ k = sup B ∩ [c j-1 , d k ] d ′′ k = inf B ∩ [d k , c]. 3. If d ′ k = d ′′ k , set c j = d k ,</formula><formula xml:id="formula_102">a j = f (c j+1 ) -f (c j ), c j+1 -c j ≤ c M ′ f (ρ(c j )) -f (c j ), otherwise,</formula><p>where ρ(c j ) = arg min{d k -c j : d k -c j ≥ 0 and k = 1, 2, . . . , M ′ +1}. Note that 0 &lt; c-c M ≤ c M ′ , by Definition 5 and continuity of the inner product ⟨ ŵ1 , • -x (1) ⟩. We then construct f as in Lemma 6, namely,</p><formula xml:id="formula_103">f (x) = M i=1 a i 1 [ci,c M +1 ] (x). Letting C = {[c j , c j+1 ] : j = 0, 1, . . . , M and c j+1 -c j ≤ c M ′ }, it is easy to see |f (x) -f (x)| &lt; ϵ 4 ,</formula><p>for all x ∈ c ′ ∈C c ′ . For any x outside said set, we are not concerned with, as it is not part of B, and hence nothing from X is mapped to said points.</p><p>Construct ω = {ω i } M i=1 according to Lemma 6. We will now construct a sum of tanh functions, using only weights/biases allowed in sampled networks. For all i = 1, 2, . . . , M , define wi = ai 2 and set x</p><p>(1) i = η(x, δ i ), with δ i ≥ 0 and x ∈ X , such that ⟨ ŵ1 , x⟩ = c i -where δ i = 0 iff / ∈ ∂X . We specify δ i and ϵ</p><formula xml:id="formula_104">′ i &gt; 0 such that s1 ϵ ′ i ≥ ω i , B 2ϵ ′ i (x<label>(1)</label></formula><p>i ) ⊆ X , and</p><formula xml:id="formula_105">|ψ(⟨w i , x -x (1) i ⟩ -s 2 ) -ψ(⟨w i , x⟩ -c i -s 2 )| &lt; ϵ 4| wi |M , with w i = s 1 x (2) i -x (1) i ∥x (2) -x (1) ∥ 2 , x<label>(2) i = x (1)</label></formula><p>i + ϵ ′ i ŵ1 , and for all x ∈ X . It is clear that</p><formula xml:id="formula_106">x (1) i , x<label>(2) i</label></formula><p>∈ X . We may now rewrite the sum of tanh functions as</p><formula xml:id="formula_107">g(x) = M i=1 wi ψ ⟨w i , x -x (1) i ⟩ -s 2 + wi = M i=1 wi ψ s 1 ∥ϵ ′ i ŵ1 ∥ ϵ ′ i ŵ1 ∥ϵ ′ i ŵ1 ∥ , x -x (1) i -s 2 + wi = M i=1 wi ψ s 1 ϵ ′ i ⟨ ŵ1 , x⟩ -ci -s 2 + wi = M i=1 a i 2 [ψ (ω i ⟨ ŵ1 , x⟩ -ci -s 2 ) + 1],</formula><p>where ci = ⟨ ŵ1 , x</p><p>i ⟩. As ω i ≤ ωi for all i = 1, 2, . . . , M , it follows from Lemma 6 and the construction above that</p><formula xml:id="formula_109">|g(x) -f (x)| ≤ |g(x) - M i=1 a i 2 [ψ (ω i ⟨ ŵ1 , x⟩ -c i -s 2 ) + 1]| + | M i=1 a i 2 [ψ (ω i ⟨ ŵ1 , x⟩ -c i -s 2 ) + 1] -f (x)| + |f (x) -f (x)| &lt; ϵ 4 + ϵ 2 + ϵ 4 = ϵ.</formula><p>We are now ready to prove that sampled networks with one hidden layer with tanh as activation function are universal approximators. Theorem 6. Let F S 1,∞ be the set of all sampled networks with one hidden layer of arbitrary width and activation function ψ. F S 1,∞ is dense in C(X , R N2 ), with respect to the uniform norm.</p><p>Proof. Let g ∈ C(X, R N2 ), ϵ &gt; 0, and w.l.o.g., N 2 = 1. By Theorem 4, we know there exists a network Φ, with N1 neurons and parameters { ŵ1,i , ŵ2,i , b1,i , b2,i } N1 i=1 , and ReLU as activation function, such that ∥Φ -g∥ ∞ &lt; ϵ 2 . We can then construct a new network Ψ, with ψ as activation function, where for each neuron Φ (1,n) , we construct M i neurons in Ψ, according to Lemma 7, with</p><formula xml:id="formula_110">ϵ 2 N1</formula><p>. Setting the biases in last layer of Ψ based on Φ, i.e., for every i = 1, 2, . . . , N1 , b 2,j = b2,i Mi , where j = 1, 2, . . . , M i . We then have, letting N 1 be the number of neurons in Ψ,</p><formula xml:id="formula_111">|Ψ(x) -Φ(x)| = N1 i=1 w 2,i Ψ (1,i) (x) -b 2,i -   N1 i=1 ŵ2,i Φ (1,i) (x) -b2,i   = N1 i=1   Mi j=1 w 2,j ψ(⟨w 1,j , x⟩ -b 1,j )   -ŵ2,i ϕ(⟨ ŵ1,i , x⟩ -b1,i ) ≤ N1 i=1   Mi j=1 w 2,j ψ(⟨w 1,j , x⟩ -b 1,j )   -ŵ2,i ϕ(⟨ ŵ1,i , x⟩ -b1,i ) &lt; N1 i=1 ϵ 2 N1 = ϵ 2 ,</formula><p>for all x ∈ X . The last inequality follows from Lemma 7. This implies that</p><formula xml:id="formula_112">∥Ψ -g∥ ∞ ≤ ∥Ψ -Φ∥ ∞ + ∥Φ -g∥ ∞ &lt; ϵ 2 + ϵ 2 = ϵ, and F S 1,∞ is dense in C(X , R N2 ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Barron spaces</head><p>Working with neural networks and sampling makes it very natural to connect our theory to Barron spaces <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b19">20]</ref>. This space of functions can be considered a continuum analog of neural networks with one hidden layer of arbitrary width. We start by considering all functions f : X → R that can be written as</p><formula xml:id="formula_113">f (x) = Ω w 2 ϕ(⟨w 1 , x⟩ -b)dµ(b, w 1 , w 2 ),</formula><p>where µ is a probability measure over (Ω, Σ Ω ), with Ω = R × R D × R. A Barron space B p is equipped with a norm of the form,</p><formula xml:id="formula_114">∥f ∥ Bp = inf µ {E µ [|w 2 | p (∥w 1 ∥ 1 + |b|) p ] 1/p }, 1 ≤ p ≤ ∞,</formula><p>taken over the space of probability measure µ over (Ω, Σ Ω ). When p = ∞, we have</p><formula xml:id="formula_115">∥f ∥ B∞ = inf µ max (b,w1,w2)∈supp(µ) {|w 2 |(∥w 1 ∥ 1 + |b|)}.</formula><p>The Barron space can then be defined as</p><formula xml:id="formula_116">B p = {f : f (x) = Ω w 2 ϕ(⟨w 1 , x⟩ -b)dµ(b, w 1 , w 2 ) and ∥f ∥ Bp &lt; ∞}.</formula><p>As for any 1 ≤ p ≤ ∞, we have B p = B ∞ , and so we may drop the subscript p <ref type="bibr" target="#b19">[20]</ref>. Given our previous results, we can easily show approximation bounds between our sampled networks and Barron functions.</p><p>Theorem 7. Let f ∈ B and X = [0, 1] D . For any N 1 ∈ N &gt;0 , ϵ &gt; 0, and an arbitrary probability measure π, there exist sampled networks Φ with one hidden layer, N 1 neurons, and ReLU activation function, such that</p><formula xml:id="formula_117">∥f -Φ∥ 2 2 = X |f (x) -Φ(x)| 2 dπ(x) &lt; (3 + ϵ)∥f ∥ 2 B N 1 .</formula><p>Proof. Let N 1 ∈ N &gt;0 and ϵ &gt; 0. By E et al. <ref type="bibr" target="#b19">[20]</ref>, we know there exists a network Φ ∈ F 1,N1 ,</p><p>where</p><formula xml:id="formula_118">Φ(•) = N1 i=1 ŵ2,i ϕ(⟨ ŵ1,i , •⟩ -b1,i ), such that ∥f -Φ∥ 2 2 ≤ 3∥f ∥ 2 B N1</formula><p>. By Theorem 4, letting It is not immediately clear from above that P is valid distribution, and in particular, that the density is integrable. This is what we show next. Proposition 1. Let X ⊆ R D be compact, λ D (X ) &gt; 0, and f be Lipschitz continuous w.r.t. the metric spaces induced by ∥•∥ Y and ∥•∥ X . For fixed architecture {N l } L l=1 , the proposed function p is integrable and</p><formula xml:id="formula_119">X p dλ D = 1.</formula><p>It therefore follows that P is a valid probability distribution.</p><p>Proof. We will show for each l = 1, 2, . . . , L that p l is bounded a.e. and is nonzero for at least one subset with nonzero measure. There exist A ⊆ X × X such that p l (A) ̸ = 0 and λ 2D (A) &gt; 0, as either C l &gt; 0 or p l is the uniform density by Definition 7 and λ 2D (X × X ) &gt; 0 by assumption and the product topology.</p><p>For l = 1, let K l &gt; 0 be the Lipschitz constant. Then q l (x (1) , x (2) ) by assumption of f for all x (1) , x (2) ∈ X . When l &gt; 1, for all X l-1 ∈ X 2 Nl-1 , we have that there exist a constant K l &gt; 0 due to continuity and compactness, such that</p><formula xml:id="formula_120">p l (x (1) , x (2) | X l ) ≤ max K l ϵ , 1 λ 2D (X × X ) .</formula><p>As p is a multiplication of a finite set of elements, we end up with</p><formula xml:id="formula_121">0 &lt; X p dλ &lt; X L l=1 N l K l ϵ + K l + 1 λ 2D (X × X ) dλ D &lt; L max N l K l ϵ + K l + 1 λ 2D (X × X ) L l=1 λ D ( X ) &lt; ∞.</formula><p>using the fact that 0 &lt; λ D (X ) &lt; ∞ and λ D being the product measure, implies 0 &lt; λ D ( X ) &lt; ∞. Due to the normalization constants added to q l , we see p l integrates to one. This means P is a valid distribution of X , with implied independence between the neurons in the same layer.</p><p>One special property of sampled networks, and in particular of the distribution P , is their invariance under both linear isometries and translation (together forming the Euclidean group, i.e., rigid body transformations), as well as scaling. We denote the set of possible transformations as H(D) = R \ {0} × O(D) × R, with O(D) being the orthogonal group of dimension D. We then denote (a, A, c) = H ∈ H(D) as H(x) = aAx + c, where x ∈ X . The space H f (D) ⊆ H are all transformations such that f : X → R N L+1 is equivariant with respect to the transformations, with the underlying metric space given by ∥•∥ Y . That is, for any H ∈ H f (D), there exists a H ′ ∈ H(N L+1 ), such that f (H(x)) = H ′ (f (x)), where x ∈ X , and the orthogonal matrix part of H ′ is isometric w.r.t. ∥•∥ Y . Note that often the H ′ will be the identity transformation, for example by having the same labels for the transformed data. When H ′ is the identity function, we say f is invariant with respect to H. In the next result, we assume we choose norms ∥•∥ X0 and ∥•∥ Y , such that the orthogonal matrix part of H is isometric w.r.t. those norms and the canonical norm ∥•∥, as well as continue the assumption of Lipschitz-continuous f . Theorem 8. Let H ∈ H f (D), Φ, Φ be two sampled networks with the same number of layers L and neurons N 1 , . . . , N L , where Φ : X → R N L+1 and Φ : H(X ) → R N L+1 , and f : X → R N L+1 is the true function. Then the following statements hold:</p><p>(1) If</p><p>x(1)</p><formula xml:id="formula_122">0,i = H(x<label>(1)</label></formula><p>0,i ) and x(2)</p><formula xml:id="formula_123">0,i = H(x<label>(2)</label></formula><p>0,i ), for all i = 1, 2, . . . , N 1 , then Φ (1) (x) = Φ(1) (H(x)), for all x ∈ X .</p><p>(2) If f is invariant w.r.t. H: Φ ∈ F S L,[N1,...N L ] (X ) if and only if Φ ∈ F S L,[N1,...N L ] (H(X )), such that Φ(x) = Φ(x) for all x ∈ X .</p><p>(3) The probability measure P over the parameters is invariant under H.</p><p>Proof. Let H = (a, A, c). Assume we have sampled</p><formula xml:id="formula_124">x(1) 0,i = H(x<label>(1)</label></formula><p>0,i ) and x(2)</p><formula xml:id="formula_125">0,i = H(x<label>(2)</label></formula><p>0,i ), for all i = 1, 2, . . . , N 1 . The points sampled determines the weights and biases in the usual way, giving</p><formula xml:id="formula_126">ϕ w 1,i , x -x (1) 0,i = ϕ x (2) 0,i -x (1) 0,i ∥x (2) 0,i -x (1) 0,i ∥ 2 , x -x (1) 0,i = ϕ A x (2) 0,i -x (1) 0,i ∥A(x (2) 0,i -x (1) 0,i )∥ 2 , A(x -x<label>(1)</label></formula><formula xml:id="formula_127">0,i ) = a • a a 2 ϕ A x (2) 0,i -x (1) 0,i ∥A(x (2) 0,i -x (1) 0,i )∥ 2 , A(x -x<label>(1)</label></formula><formula xml:id="formula_128">0,i ) = ϕ aA x (2) 0,i -x (1) 0,i ∥aA(x (2) 0,i -x (1) 0,i )∥ 2 , aA(x -x<label>(1)</label></formula><formula xml:id="formula_129">0,i ) = ϕ aA x (2) 0,i + c -x (1) 0,i -c ∥aA(x (2) 0,i + c -x (1) 0,i -c)∥ 2 , aA(x + c -x (1) 0,i -c) = ϕ H(x (2) 0,i ) -H(x<label>(1)</label></formula><formula xml:id="formula_130">0,i ) ∥H(x (2) 0,i ) -H(x (1) 0,i )∥ 2 , H(x) -H(x (1) 0,i ) = ϕ ŵ1,i , x - x<label>(1)</label></formula><p>0,i for all x ∈ X , x ∈ H(X ), and i = 1, 2, . . . , N 1 . Which implies that (1) holds.</p><p>Assuming f is invariant w.r.t. H, then for any Φ ∈ F S L,[N1,...N L ] (X ), let X = {x</p><p>1,i } N1 i=1 , we can then choose H(X) as points to construct weights and biases in the first layer of Φ, and by (1), we have X 1 = Φ (1) (X ) = Φ(1) (H(X )) = X1 . As the sampling space is the same for the next layer, we see that we can choose the points for the weights and biases of Φ, such that X l = Xl , where l = 1, 2, . . . , L. As the input after the final hidden layer is also the same, by the same argument, means the weights in the last layer must be the same, due to the loss function L in Definition 4 is the same due to the invariance assumption. Thus, Φ(x) = Φ(H(x)) for all x ∈ X . As H is bijective, means the same must hold true starting with Φ, and constructing Φ, and we conclude that (2) holds.</p><p>For any distinct points x (1) , x (2) ∈ X , letting (a ′ , A ′ , c ′ ) be the set such that g(aAx + c) = a ′ A ′ g(x) + c ′ , and Ĉl , C l be the normalization constants over the conditional density p l , for H(X ) and X resp. We have for the conditional density when l = 1 is, p 1 (H(x (1) ), H(x (2) )) = 1 Ĉ1</p><p>∥f (aAx (2) + c) -f (aAx (1)  1) , x (2) ).</p><formula xml:id="formula_132">+ c)∥ Y ∥aAx (2) + c -aAx (1) -c∥ X = 1 Ĉ1 ∥a ′ A ′ f (x (2) ) + c ′ -a ′ A ′ f (x (1) ) -c ′ ∥ Y ∥aAx (2) -aAx (1) ∥ X = |a ′ | |a| Ĉ1 ∥f (x (2) ) -f (x (1) )∥ Y ∥x (2) -x (1) ∥ X = |a ′ | |a| Ĉ1 p 1 (x<label>(</label></formula><p>With similar calculations, we have</p><formula xml:id="formula_133">|a| |a ′ | Ĉ1 = |a| |a ′ | X ×X p 1 (H(x), H(z))dxdz = |a| |a ′ | |a ′ | |a| X ×X p 1 (x, z)dxdz = C 1 .</formula><p>Hence, the conditional probability distribution for the first layer is invariant under H, and then by (1) and (2), the possible sampling spaces are equal for the following layers, and therefore the conditional distributions for each layer is the same, and therefore P is invariant under H.</p><p>Remark 7. Point (2) in the theorem requires f to be invariant w.r.t. H. This is due to the fact that the parameters in the last layer minimizes the implicitly given loss function L, seen in Definition 4.</p><p>We have rarely discussed the loss function, as it depends on what function we are approximating.</p><p>Technically, (2) also holds as long as the loss function is not altered by H in a way that the final layer alters, but we simplified it to be invariant, as this is also the most likely case to stumble upon in practice. The loss function also appears in the proofs given in Appendix A.1 and Appendix A.2. Here both uses, implicitly, the loss function based on the uniform norm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Illustrative example: approximating a Barron function</head><p>All computations for this experiment were performed on the Intel Core i7-7700 CPU @ 3.60GHz × 8 with 32GB RAM.</p><p>We compare random Fourier features and our sampling procedure on a test function for neural networks <ref type="bibr" target="#b63">[64]</ref>: f (x) = 3/2(∥x -a∥ ℓ 2 -∥x + a∥ ℓ 2 ), with the constant vector a ∈ R d defined by a j = 2j/d -1. For all dimensions d, we sample 10,000 points uniformly at random in the cube [-1, 1] d for training (i.e., sampling and solving the last, linear layer problem), and another 10,000 points for evaluating the error. We re-run the same experiment with five different random seeds, but with the same train/test datasets. This means the random seed only influences the weight distributions in sampled networks and random feature models, not the data. Figure <ref type="figure">8</ref> shows the relative L 2 error in the full hyperparameter study, i.e. in dimensions d ∈ {1, 2, 3, 4, 5, 10} (rows) and for tanh (left column) and sine activation functions (right column). For the random feature method, we always use sine activation, because we did not find any data-agnostic probability distributions for tanh. Figure <ref type="figure">9</ref> shows the fit times for the same experiments, demonstrating that sampling networks are not slower than random feature models. This is obvious from the complexity analysis in Appendix F, and confirmed experimentally here. Interesting observations regarding the full hyperparameter study are that for one dimension, d = 1, random feature models outperform sampled networks and can even use up to two hidden layers. In higher dimensions, and with larger widths / more layers, sampled networks consistently outperform random features. In d = 10, a sampled network with even a single hidden layer is about one order of magnitude more accurate than the same network with normally distributed weights. The convergence rate of the error of sampled networks with respect to increasing layer widths is consistent over all dimensions and layers, even sometimes outperforming the theoretical bound of O(N -1/2 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Classification benchmark from OpenML</head><p>The Adam training was done on the GeForce 4x RTX 3080 Turbo GPU with 10 GB RAM, while sampling was performed on the Intel Core i7-7700 CPU @ 3.60GHz × 8 with 32GB RAM.</p><p>We use all 72 datasets in the "OpenML-CC18 Curated Classification benchmark" from the "OpenML Benchmarking Suites" (CC-BY) <ref type="bibr" target="#b3">[4]</ref>, which is available on openml.org: <ref type="url" target="https://openml.org/search?type=benchmark&amp;sort=tasks_included&amp;study_type=task&amp;id=99">https://openml.org/ search?type=benchmark&amp;sort=tasks_included&amp;study_type=task&amp;id=99</ref>.</p><p>We use the OpenML Python API (BSD-3 Clause) <ref type="bibr" target="#b21">[22]</ref> to download the benchmark datasets. The hyperparameters used in the study are listed in Table <ref type="table">3</ref>. From all datasets, we use at most 5000 points. This was done for all datasets before applying any sampled or gradient-based approach, because we wanted to reduce the training time when using the Adam optimizer. We pre-process the input features using one-hot encoding for categorical variables, and robust whitening (removal of mean, division by standard deviation using the RobustScaler class from scikit-learn). Missing features are imputed with the median of the feature. All layers of all networks always have 500 neurons. For Adam, we employ early stopping with patience=3, monitoring the loss value. The least squares regularization constant for sampling is 10 -10 . We split the data sets for 10-fold cross-validation using stratified k-fold (StratifiedKFold class from scikit-learn), and report the average of the accuracy scores and fit times over the ten folds (Figure <ref type="figure" target="#fig_1">4</ref> in the main paper).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Deep neural operators D.1 Dataset</head><p>To generate an initial condition u 0 , we first sample five Fourier coefficients for the lowest frequencies. We sample both real and imaginary parts from a normal distribution with zero mean and standard keep in the POD. To compute the POD, we first center the data and then use np.linalg.eigh to obtain the modes. We add two callbacks to the standard training: the early stopping and the model checkpoint. We use the default parameters for the optimization and only set the learning rate. The default training loop uses the number of iterations instead of the number of epochs, and we train for 90000 iterations with patience set to 4500. By default, POD-DeepONet uses all the data available as a batch. We ran several experiments with the batch size set to 64, but the results were significantly worse compared to the default setting. Hence, we stick to using the full data as one batch.</p><p>FNO We use the neuralop (<ref type="url" target="https://github.com/neuraloperator/neuraloperator">https://github.com/neuraloperator/neuraloperator</ref>, MIT license, <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b37">38]</ref>) package with slight modifications. We changed the trainer.py to add the early stopping and to store the results of experiments. Apart from these changes, we use the functionality provided by the library to define and run experiments. We use batch size 64 and train for 2000 epochs with patience 100. In addition to the hyperparameters considered for other architecture, we also did a search over the number of hidden channels (32 or 64). We use the same number of channels for lifting and projection operators.</p><p>FCN For both fully-connected networks in our experiments, we use PyTorch framework to define the models. For training a network in Fourier space, we prepend a forward Fourier transform before starting the training. During the inference, we applied the inverse Fourier transform to compute the validation metric. We considered using the full data as a batch, but several experiments indicated that this change worsens the final metric. Hence, we use batch size 64 for both architectures.</p><p>Sampling FNO Here, we want to give more details about sampled FNO. First, we start with input u 0 and append the coordinate information to it as an additional channel. Then, we lift this input to a higher dimensional channel space by drawing 1 × 1 convolution filters from a normal distribution. Then, we apply a Fourier block which consists of fully-connected networks in Fourier space for each channel. After applying the inverse Fourier transform, we add the input of the block to the restored signal as a skip connection. After applying several Fourier blocks, we learn another FCN in signal space to obtain the final prediction. We can see the schematic depiction of the model in Figure <ref type="figure" target="#fig_7">10</ref>. We note that to train fully-connected networks in the Fourier space, we apply Fourier transform to labels u 1 and use it during sampling as target functions. Similarly to the Adam-trained FNO, we search over the number of hidden channels we use for lifting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Results</head><p>We can see the results of the hyperparameter search in Figure <ref type="figure" target="#fig_8">11</ref>. We see that sampled FNO and FCN in Fourier space perform well even with a smaller number of frequencies available, while DeepONet requires more modes to achieve comparable accuracy. We also see that adding more layers is not beneficial for sampled networks. A more important factor is the width of the layers: all of the sampled networks achieved the best results with the largest number of neurons.</p><p>For the Adam-trained models, we note that we could not significantly improve the results by increasing the widths of the layers. Possibly, this is due to a limited range of learning rates considered in the experiments. FNO with Adam has no dependency on a layer width. Thus, we show results for FNO trained with Adam as a straight line in all rows. For the hyperparameters not present in the plot, we chose the values that produce the lowest error on the validation set. We repeat each experiment three times and average the resulting metrics for the plotting. The error is computed on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Transfer learning</head><p>This section contains details about the results shown in the main paper and the supporting hyperparameter search concerning the transfer learning task. The source and target tasks of transfer learning, train-test split, and pre-trained models are the same as in the main paper. We describe the software, hardware, data pre-processing, and details of the sampling and the Adam training approach. Note that the results shown here are not averaged over five runs, as was done for the plots in the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Software and hardware</head><p>We performed all the experiments concerning the transfer learning task in Python, using the TensorFlow framework. We used a GeForce 4x RTX 3080 Turbo GPU with 10 GB RAM for the Adam training and a 1x AMD EPYC 7402 socket with 24 cores @ 2.80GHz for sampling. We use the pre-trained models from Keras Applications which are licensed under the Apache 2.0 License which can be found at <ref type="url" target="https://github.com/keras-team/keras/blob/v2.12.0/LICENSE">https://github.com/keras-team/keras/blob/v2. 12.0/LICENSE</ref>.</p><p>Pre-processing of data The size of images in the ImageNet data set is larger than the ones in CIFAR-10, so we use a bicubic interpolation to upscale the CIFAR-10 images to dimensions 224 × 224 × 3.</p><p>We pre-process the input data for each model the same way it was pre-processed during the pretraining on ImageNet. Moreover, we apply some standard data augmentation techniques before training, such as vertical and horizontal shifts, rotation, and horizontal flips. After pre-processing, the images are first passed through the pre-trained classification layers and a global average pooling layer. The output of the global average pooling layer and classification labels serve as the input and output data for the classification head respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Details of the Adam training and sampling approaches</head><p>We find the weights and biases of the hidden layer of the classification head using the proposed sampling algorithm and the usual gradient-based training algorithm.</p><p>First, in the Adam-training approach, we find the parameters of the classification head by iterative training with the Adam optimizer. We use a learning rate of 10 -3 , batch size of 32, and train for 20 epochs with early-stopping patience of 10 epochs. We store the parameters that yield the lowest loss on test data. We use the tanh activation function for the hidden layer, the softmax activation function for the output layer, and the categorical cross-entropy loss function with no regularization.</p><p>Second, in the sampling approach, we use the proposed sampling algorithm to sample parameters of the hidden layer of the classification head. We use the tanh activation function for the hidden layer.</p><p>Once the parameters of the hidden layers are sampled, an optimization problem for the coefficients of a linear output layer is solved. For this, the mean squared error loss function without regularization is used.</p><p>Unless mentioned otherwise, the above-mentioned setup is used for all the experiments in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 Sampling Vs Adam training</head><p>Figure <ref type="figure" target="#fig_9">12</ref> compares the train and test accuracy for different widths (number of neurons in the hidden layer of the classification head) using three pre-trained neural network architectures for the Adam training and sampling approaches. As in the main paper, we observe that for all the models, the sampling approach results in a higher test accuracy than the Adam training approach for sufficiently higher widths.</p><p>The sampling algorithm tends to over-fit for very high widths. The loss on the train data for the iterative training approach decreases with width, particularly for the Xception network. This suggests that an extensive hyper-parameter search for the iterative training approach could yield higher classification accuracy on the train data. However, as shown in the main paper, the iterative training approach can be orders of magnitude slower than the sampling approach. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Sampling with tanh Vs ReLU activation functions</head><p>This sub-section compares the performance of the sampling algorithm used to sample the weights of the hidden layer of the classification head for tanh and ReLu activation functions.</p><p>Figure <ref type="figure" target="#fig_10">13</ref> shows that for ResNet50, the test accuracy with tanh is similar to that obtained with ReLU for a width smaller than 2048. As the width increases beyond 4096, test accuracy with ReLU is slightly better than with tanh. However, for VGG19 and Xception, test accuracy with tanh is higher than or equal to that with ReLU for all widths. Thus, we find the sampling algorithm with the tanh activation function yields better results for classification tasks than with ReLU.</p><p>On the training dataset, tanh and ReLU yield similar accuracies for all widths for ResNet50 and Xception. For VGG19, using the ReLU activation function yields much lower accuracies on the train and test data sets, especially as the width of the fully connected layer is increased. Thus, we observe that the tanh activation function is more suitable for classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Fine-tuning</head><p>There are two typical approaches to transfer learning: feature extraction and fine-tuning. In feature extraction, there is no need to retrain the pre-trained model. The pre-trained models capture the In fine-tuning, after the feature extraction, some or all the parameters of the pre-trained model (a few of the last layers or the entire model) are re-trained with a much smaller learning rate using typical gradient-based optimization algorithms. Fine-tuning is not the focus of this work. Nevertheless, it is essential to verify that the weights of the last layer sampled by the proposed sampling algorithm can also be trained effectively in the fine-tuning phase. The results are shown in Figure <ref type="figure" target="#fig_5">14</ref>.</p><p>In Figure <ref type="figure" target="#fig_5">14</ref>, we observe that for certain widths (512, 1024, and 4096), sampling the last layer followed by fine-tuning the entire model yields slightly higher test accuracies than training the last layer followed by fine-tuning. For widths 2048, 6144, and 8192, for some models, sampling the last layer, followed by fine-tuning, is better; for others, training the last layer, followed by fine-tuning, is better.</p><p>Nevertheless, these experiments validate that the parameters of the last layer sampled with the proposed algorithm serve as a good starting point for fine-tuning. Moreover, the test accuracy after fine-tuning is comparable irrespective of whether the last layer was sampled or trained. However, as we show in the main paper, sampling the weights in the feature extraction phase takes much less time and gives better accuracy than training with the Adam optimizer for appropriate widths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4 One vs two hidden layers in the classification head</head><p>The goal of this sub-section is to explore whether adding an additional hidden layer in the classification head leads to an improvement in classification accuracy. We keep the same width for the extra layer in this experiment.</p><p>Figure <ref type="figure" target="#fig_12">15</ref> compares the train and test accuracies obtained with one and two hidden layers in the classification head for different widths.</p><p>Figure <ref type="figure" target="#fig_12">15</ref> (left) shows that unless one chooses a very high width with the sampling approach &gt;= 6148, adding an extra hidden layer yields a lower test accuracy. On the train data, the sampling approach with 2 hidden layers results in lower accuracy for all widths in consideration.</p><p>Figure <ref type="figure" target="#fig_12">15</ref> (right) shows that with the Adam training approach, adding an extra hidden layer yields a lower test accuracy for all the widths in consideration. For lower widths, adding more layers results in over-fitting. We believe that the accuracy on the train data for higher widths could be improved with an extensive hyper-parameter search. However, adding an extra layer increases the training time too.  When the size of the hidden layer is less than the number of training points -which is often the case -we compute 2 • M probabilities -depending on a scaling constant. On the other hand, when the size of the layer is greater than or equal to M 2 , we compute in worst case all possible probabilities, that is, M 2 probabilities. The last contributing factor is to sample N l pair of points, that in the expression is dominated by the term N l • N l-1 • M . We are often interested in a fixed architecture, or at least to bound the number of neurons in each hidden layer to be less than the square of the number of training points, i.e., N &lt; M 2 . Adding the latter as an assumption, we end up with the runtime O(L • M (⌈N/M ⌉ + N 2 )).</p><p>For the second part, we optimize the weights/biases W L+1 , b L+1 to map from the last hidden layer to the output. Assume that we use the SVD decomposition to compute the pseudoinverse and subsequently solve the least squares problem. The time complexity in general is then</p><formula xml:id="formula_134">O(M • N L • min{M, N L } + N L • M • N L+1 ).</formula><p>Again, if we make the reasonable assumption that the number of training points is larger than the number of hidden layers, and that the output dimension is smaller than the dimension of the last hidden layer, we find O(M N 2 L ), which can be rewritten to O(M N 2 ). With these assumptions in mind, the run time for the full training procedure, from start to finish, is O(L • M (⌈N/M ⌉ + N 2 )), and when the architecture is fixed, we have O(M ) runtime for the full training procedure.</p><p>In terms of memory, at every layer l, we need to store the probability matrix, the output of the training set when mapped through the previous l -1 layers, and the number of points we sample. This means the memory required is O(M • ⌈N l /M ⌉ + N L+1 • N L ). In the last layer, we only need the image of the data passed through all the hidden layers, as well as the weights/biases, which leads to O(M + N L+1 • N L ). We end up with the required memory for the SWIM algorithm is O(M • ⌈N/M ⌉ + LN 2 ).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Relative L 2 approximation error of a Barron function (test set), using random features and sampling, both with sine activation. Left: input dimension D = 5. Right: input dimension D = 10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: Fitting time, accuracy, and number of layers using weight sampling, compared to training with the Adam optimizer. The best architecture is chosen separately for each method and each problem, by evaluating 10-fold cross-validation error over 1-5 layers with 500 neurons each.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Left: Samples of initial conditions u 0 and corresponding solutions u 1 for Burgers' equation. Right: Parameters of the best model for each architecture, the mean relative L 2 error on the test set, and the training time. We average the metrics across three runs with different random seeds.</figDesc><graphic coords="9,110.72,72.00,112.86,93.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Left: Train and test accuracies with different widths for ResNet50 (averaged over 5 random seeds). Middle: Test accuracy with different models with and without fine-tuning (width = 2048). Right: Adam training and sampling times of the classification head (averaged over 5 experiments).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>( 4 )</head><label>4</label><figDesc>If b(i) ∧ &lt; b1,i &lt; b (i) ∨ ,and b1,i / ∈ B i , things are a bit more involved. First notice that B (1) i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>5 .</head><label>5</label><figDesc>and increase j and k, and go to 1.4. If d′ k &gt; c j-1 , set c j = d ′ k ,and increase j, otherwise discard the point d ′ k . Set c j = d ′′ k , and increase j. Set k = arg min{d k -c j : d k -c j &gt; 0} and go to 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: The architecture of the sampled FNO.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Comparison of different deep neural operators trained with Adam and with SWIM algorithm.FNO with Adam has no dependency on a layer width. Thus, we show results for FNO trained with Adam as a straight line in all rows. For the hyperparameters not present in the plot, we chose the values that produce the lowest error on the validation set. We repeat each experiment three times and average the resulting metrics for the plotting. The error is computed on the validation set.</figDesc><graphic coords="37,108.00,72.00,395.99,165.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Left: ResNet50. Middle: VGG19. Right: Xception.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Left: ResNet50. Middle: VGG19. Right:Xception</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Left: Sampling, Right: Adam training</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments and Disclosure of Funding</head><p>We are grateful for discussions with <rs type="person">Erik Bollt</rs>, <rs type="person">Constantinos Siettos</rs>, <rs type="person">Edmilson Roque Dos Santos</rs>, <rs type="person">Anna Veselovska</rs>, and <rs type="person">Massimo Fornasier</rs>. We also thank the anonymous reviewers at <rs type="institution">NeurIPS</rs> for their constructive feedback. E.B., I.B., and F.D. are funded by the <rs type="funder">Deutsche Forschungsgemeinschaft (DFG, German Research Foundation</rs>) -project no. <rs type="grantNumber">468830823</rs>, and also acknowledge association to DFG-SPP-229. C.D. is partially funded by the <rs type="funder">Institute for Advanced Study (IAS) at the Technical University of Munich</rs>. F.D. and Q.S. are supported by the <rs type="projectName">TUM Georg Nemetschek Institute -Artificial Intelligence for the Built World</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_kJzG4tv">
					<idno type="grant-number">468830823</idno>
				</org>
				<org type="funded-project" xml:id="_TSbSeF6">
					<orgName type="project" subtype="full">TUM Georg Nemetschek Institute -Artificial Intelligence for the Built World</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>Here we include full proofs of all theorems in the paper, and then details on all numerical experiments. In the last section, we briefly explain the code repository (URL in section Appendix F) and provide the analysis of the algorithm runtime and memory complexity. A Sampled networks theory</p><p>In this section, we provide complete proofs of the results stated in the main paper. In Appendix A.1 we consider sampled networks and the universal approximation property, followed by the result bounding L 2 approximation error between sampled networks and Barron functions in Appendix A.2. We end with the proof regarding equivariance and invariance in Appendix A.3.</p><p>We start by defining neural networks and sampled neural networks, and show more in depth the choices of the scalars in sampled networks for ReLU and tanh. Let the input space X be a subset of R D . We now define a fully connected, feed forward neural network; mostly to introduce some notation. Definition 3. Let ϕ : R → R be non-polynomial, L ≥ 1, N 0 = D, and N 1 , . . . , N L+1 ∈ N &gt;0 . A (fully connected) neural network with L hidden layers, activation function ϕ, and weight space</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.3 Deep networks</head><p>The extension of sampled networks into several layers is not obvious, as the choice of weights in the first layer affects the sampling space for the weights in the next layer. This additional complexity raises the question, letting</p><p>We aim to answer this question in this section. With dimensions in each layer being N L = [D, D, . . . , D, N L ], we start by showing it holds for ∞ N L =1 F S L,N L , i.e., the space of sampled networks with L hidden layers, and D neurons in the first L -1 layers and arbitrary width in the last layer.</p><p>Proof. Let ϵ &gt; 0, and g ∈ C(X , R N L+1 ). Basic linear algebra implies there exists a set of D linearly independent unit vectors v = {v j } D j=1 , such that X ⊆ span{v j } D j=1 , with v j ∈ X for all v j ∈ v. For l = 1 and i ∈ {1, 2, . . . , N l }, the bias is set to b l,i = inf{⟨v i , x⟩ : x ∈ X }. Due to continuity and compactness, we can set x (1) l-1,i to correspond to an x ∈ X such that ⟨v i , x</p><p>l-1,i ⟩ = b l,i . We must have x</p><p>(1) l-1,i ∈ ∂X -otherwise the inner product between some points in X and v i is smaller than the bias, which contradicts the construction of b l,i . We now need to show that ζ(x</p><p>l-1,i in similar fashion as in Lemma 3.</p><p>l-1,i } = U ∩ ∂X -otherwise ζ would not give a unique element, which is guaranteed by Definition 5. We define the hyperplane Ker</p><p>l-1,i ⟩ = 0}, and the projection matrix P from R D onto Ker. Let</p><p>l-1,i ∈ ∂U and P is a unique projection minimizing the distance. As x ′ ∈ int U , means there is an open ball around x ′ , where there are points in</p><p>l-1,i ⟩ is not the minimum. This is a contradiction, and hence</p><p>l-1,i . This means Ker is a tangent hyperplane, and as any vectors along the hyperplane is orthogonal to ζ(x</p><p>l,i , implies the angle between v i and ζ(x</p><p>We may now construct x</p><p>l-1,i ∈ X due to the last paragraph. We then have w l,i = vi ∥vi∥ and b l,i = ⟨v i , x</p><p>l-1,i ⟩ for all neurons in the first hidden layer. The image</p><p>N1 . We do not need constant blocks, because X is connected. We then have</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Distribution of sampled networks</head><p>In this section we prove certain invariance properties for sampled networks and our proposed distribution. First, we define the distribution to sample the pair of points, or equivalently, the parameters. Note that X is now any compact subset of R D , as long as λ D (X ) &gt; 0, where λ D the D-dimensional Lebesgue measure.</p><p>As we are in a supervised setting, we assume access to values of the true function f , and define Y = f (X ). We also choose a norm ∥•∥ Y over R N L+1 , and norms ∥•∥ X l-1 over R N l-1 , for each l = 1, 2, . . . , L. For the experimental part of the paper, we choose the L ∞ norm for ∥•∥ Y and for l = 1, 2, . . . , L, we choose the L 2 norm for ∥•∥ X l-1 . We also denote N = L l=1 N l as the total number of neurons in a given network, and Nl = l k=1 N k . Due to the nature of sampled networks and because we sample each layer sequentially, we start by giving a more precise definition of the conditional density given in Definition 2. As a pair of points from X identifies a weight and bias, we need a distribution over X 2 N , and for each layer l condition on sets of 2 Nl-1 points, which then parameterize the network Φ (l-1) by constructing weights and biases according to Definition 4. Definition 7. Let X be compact, λ D (X ) &gt; 0, and f : R D → R N L+1 be Lipschitz-continuous w.r.t. the metric spaces induced by ∥•∥ Y and ∥•∥ X . For any l ∈ {1, 2, . . . , L}, setting ϵ = 0 when l = 1 and otherwise ϵ &gt; 0, we define</p><p>l-1 = Φ (l-1) (x</p><p>(1) 0 ), and x</p><p>(2)</p><p>0 ), with the network Φ (l-1) parameterized by pairs of points in X Nl-1 . Then, we define the integration constant C l = X ×X q ϵ l dλ. The l-layered density p ϵ l is defined as</p><p>Remark 6. The added ϵ is there to ensure the density is bounded, but is not needed when considering the first layer, due to the Lipschitz assumption. Adding ϵ &gt; 0 for l = 1 is both unnecessary and affects equivariant/invariant results in Theorem 8. We drop the ϵ superscript wherever it is unambiguously included.</p><p>We can now use this definition to define the probability of the whole parameter space X 2 N , i.e., given an architecture provide a distribution over all weights and biases the network require. Let X = X 2 N , i.e., the sampling space of P with the product topology, and D = 2 N D as the dimension of the space. Since all the parameters is defined through points of X × X , we may choose an arbitrary ordering of the points, which means one set of weights and biases for the whole network can be written as {x</p><p>i } N i=1 . Definition 8. The probability distribution P over X have density p, p x</p><p>N , x</p><p>Nk +j }.  deviation equal to five. We then scale the k-th coefficient by 1/(k + 1) 2 to create a smoother initial condition. For the first coefficient corresponding to the zero frequency, we also set the imaginary part to zero as our initial condition should be real-valued. Then, we use real inverse Fourier transform with orthonormal normalization (np.fft.irrft) to generate an initial condition with the  discretization over 256 grid points. To solve the Burgers' equation, we use a classical numerical solver (scipy.integrate.solve_ivp) and obtain a solution u 1 at t = 1. This way, we generate 15000 pairs of (u 0 , u 1 ) and split them between train (9000 pairs), validation (3000 pairs), and test sets (3000 pairs). The Adam training was done on the GeForce 4x RTX 3080 Turbo GPU with 10 GB RAM, while sampling was performed on the 1x AMD EPYC 7402 @ 2.80GHz × 8 with 256GB RAM. We use the sacred package (<ref type="url" target="https://github.com/IDSIA/sacred">https://github.com/IDSIA/sacred</ref>, MIT license, <ref type="bibr" target="#b36">[37]</ref>) to conduct the hyperparameter search.</p><p>We perform a grid search over several hyperparameters (where applicable): the number of layers/Fourier blocks, the width of layers, the number of hidden channels, the regularization scale, the learning rate, and the number of frequencies/modes to keep. The full search grid is listed in Table <ref type="table">4</ref>.</p><p>Adam With the exception of FNO, we use the mean squared error as the loss function when training with Adam. For FNO, we train with the default option, that is, with the mean relative H 1 -loss. This loss is based on the H 1 -norm for a one-dimensional function f defined as</p><p>For all the experiments, we use the mean relative L 2 error as the validation metric. We perform an early stopping on the validation set and restore the model with the best metric.</p><p>DeepONet We use the deepxde package (<ref type="url" target="https://github.com/lululxvi/deepxde">https://github.com/lululxvi/deepxde</ref>, LGPL-2.1 License, <ref type="bibr" target="#b44">[45]</ref>) to define and train POD-DeepONet for our dataset. After defining the model, we apply an output transform by centering and scaling the output with 1/p, where p is the number of modes we </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Code repository and algorithm complexity</head><p>The code to reproduce the experiments from the paper can be found at <ref type="url" target="https://gitlab.com/felix.dietrich/swimnetworks-paper">https://gitlab.com/felix.dietrich/swimnetworks-paper</ref>.</p><p>An up-to-date code base is maintained at <ref type="url" target="https://gitlab.com/felix.dietrich/swimnetworks">https://gitlab.com/felix.dietrich/swimnetworks</ref>.</p><p>Python 3.8 is required to run the computational experiments and install the software. Installation works using "pip install -e ." in the main code repository. The python packages required can be installed using the file requirements.txt file, using pip. The code base is contained in the folder swimnetworks, the experiments and code for hyperparameter studies in the folder experiments.</p><p>We now include a short complexity analysis of the SWIM algorithm (Algorithm 1 in the main paper), which is implemented in the code used in the experiments. The main points of the analysis can be found in Table <ref type="table">5</ref>.</p><p>There are two separate parts that contribute to the runtime. First, we consider sampling the parameters of the hidden layers. Letting N = max{N 0 , N 1 , N 2 , . . . , N L }, i.e., the maximum of the number of neurons in any given layer and the input dimension, and M being the size of the training set. The size of the training set also limits the number of possible weights / biases that we can sample, namely M  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On the equivalence between kernel quadrature rules and random feature expansions</title>
		<author>
			<persName><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v18/15-178.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Universal approximation bounds for superpositions of a sigmoidal function</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Barron</surname></persName>
		</author>
		<idno type="DOI">10.1109/18.256500</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<idno type="ISSN">0018-9448</idno>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1557" to="9654" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Algorithms for hyper-parameter optimization</title>
		<author>
			<persName><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Bardenet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balázs</forename><surname>Kégl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">OpenML benchmarking suites</title>
		<author>
			<persName><forename type="first">Bernd</forename><surname>Bischl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Casalicchio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Feurer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Gijsbers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Gomes Mantovani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">N</forename><surname>Van Rijn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joaquin</forename><surname>Vanschoren</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=OCrD8ycKjG" />
	</analytic>
	<monogr>
		<title level="m">Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Weight uncertainty in neural network</title>
		<author>
			<persName><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Cornebise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v37/blundell15.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning, ICML</title>
		<meeting>the 32nd International Conference on Machine Learning, ICML</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On explaining the surprising success of reservoir computing forecaster of chaos? The universal machine learning dynamical system with contrast to VAR and DMD</title>
		<author>
			<persName><forename type="first">Erik</forename><surname>Bollt</surname></persName>
		</author>
		<idno type="DOI">10.1063/5.0024890</idno>
	</analytic>
	<monogr>
		<title level="j">Chaos: An Interdisciplinary Journal of Nonlinear Science</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">How Neural Networks Work: Unraveling the Mystery of Randomized Neural Networks For Functions and Chaotic Dynamical Systems</title>
		<author>
			<persName><forename type="first">Erik</forename><forename type="middle">M</forename><surname>Bollt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>unpublished, submitted</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On the Existence of Universal Lottery Tickets</title>
		<author>
			<persName><forename type="first">Rebekka</forename><surname>Burkholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nilanjana</forename><surname>Laha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajarshi</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alkis</forename><surname>Gotovos</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SYB4WrJql1n" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">State of the art: a review of sentiment analysis based on sequential transfer learning</title>
		<author>
			<persName><forename type="first">Jireh</forename><surname>Yi-Le Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khean</forename><surname>Thye Bea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Mun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Leow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seuk</forename><surname>Wai Phoong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wai</forename><forename type="middle">Khuen</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="749" to="780" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bridging Traditional and Machine Learningbased Algorithms for Solving PDEs</title>
		<author>
			<persName><forename type="first">Jingrun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xurong</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Weinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhouwang</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.13380</idno>
	</analytic>
	<monogr>
		<title level="m">The Random Feature Method</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On Statistical Properties of Sets Fulfilling Rolling-Type Conditions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cuevas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fraiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pateiro-López</surname></persName>
		</author>
		<idno type="DOI">10.1239/aap/1339878713</idno>
	</analytic>
	<monogr>
		<title level="j">Advances in Applied Probability</title>
		<idno type="ISSN">0001-8678</idno>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1475" to="6064" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Approximation by superpositions of a sigmoidal function</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cybenko</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF02551274</idno>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Control, Signals and Systems</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="303" to="314" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural network approximation</title>
		<author>
			<persName><forename type="first">Ronald</forename><surname>Devore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Hanin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guergana</forename><surname>Petrova</surname></persName>
		</author>
		<idno type="DOI">10.1017/S0962492921000052</idno>
	</analytic>
	<monogr>
		<title level="j">Acta Numerica</title>
		<idno type="ISSN">0962-4929</idno>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="327" to="444" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The separation capacity of random neural networks</title>
		<author>
			<persName><forename type="first">Sjoerd</forename><surname>Dirksen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Genzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Jacques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Stollenwerk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">309</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Local extreme learning machines and domain decomposition for solving linear and nonlinear partial differential equations</title>
		<author>
			<persName><forename type="first">Suchuan</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongwei</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cma.2021.114129</idno>
		<ptr target="https://linkinghub.elsevier.com/retrieve/pii/S0045782521004606" />
	</analytic>
	<monogr>
		<title level="j">Computer Methods in Applied Mechanics and Engineering</title>
		<imprint>
			<biblScope unit="volume">387</biblScope>
			<biblScope unit="page">114129</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Towards a Mathematical Understanding of Neural Network-Based Machine Learning: What We Know and What We Don&apos;t</title>
		<author>
			<persName><forename type="first">E</forename><surname>Weinan</surname></persName>
		</author>
		<idno type="DOI">10.4208/csiam-am.SO-2020-0002</idno>
	</analytic>
	<monogr>
		<title level="j">CSIAM Transactions on Applied Mathematics</title>
		<idno type="ISSN">2708-0560</idno>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2708" to="0579" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Representation formulas and pointwise properties for Barron functions</title>
		<author>
			<persName><forename type="first">E</forename><surname>Weinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Wojtowytsch</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00526-021-02156-6</idno>
	</analytic>
	<monogr>
		<title level="j">Calculus of Variations and Partial Differential Equations</title>
		<idno type="ISSN">0944-2669</idno>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">46</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A Priori estimates of the population risk for two-layer neural networks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Weinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.4310/CMS.2019.v17.n5.a11</idno>
	</analytic>
	<monogr>
		<title level="j">Communications in Mathematical Sciences</title>
		<idno type="ISSN">15396746</idno>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1407" to="1425" />
			<date type="published" when="2019">2019. 19450796</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The Barron Space and the Flow-Induced Function Spaces for Neural Network Models</title>
		<author>
			<persName><forename type="first">E</forename><surname>Weinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00365-021-09549-y</idno>
	</analytic>
	<monogr>
		<title level="j">Constructive Approximation</title>
		<idno type="ISSN">0176-4276</idno>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="369" to="406" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural architecture search: A survey</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Hendrik Metzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1997" to="2017" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Feurer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">N</forename><surname>Van Rijn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arlind</forename><surname>Kadra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Gijsbers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neeratyoy</forename><surname>Mallik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sahithya</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joaquin</forename><surname>Vanschoren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.02490</idno>
		<title level="m">OpenML-Python: An extensible Python API for OpenML</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Christian</forename><surname>Fiedler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimo</forename><surname>Fornasier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Klock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Rauchensteiner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.07150</idno>
		<title level="m">Stable Recovery of Entangled Weights: Towards Robust Identification of Deep Neural Networks from Minimal Samples</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Massimo</forename><surname>Fornasier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Klock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Mondelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Rauchensteiner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.04589</idno>
		<title level="m">Finite Sample Identification of Wide Shallow Neural Networks with Biases</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The lottery ticket hypothesis: Finding sparse, trainable neural networks</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Carbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning</title>
		<author>
			<persName><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
		<meeting>The 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Numerical Bifurcation Analysis of PDEs From Lattice Boltzmann Model Simulations: A Parsimonious Machine Learning Approach</title>
		<author>
			<persName><forename type="first">Evangelos</forename><surname>Galaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gianluca</forename><surname>Fabiani</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10915-022-01883-y</idno>
	</analytic>
	<monogr>
		<title level="m">Ioannis Gallos, Ioannis Kevrekidis, and Constantinos Siettos</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="page" from="1573" to="7691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep Randomized Neural Networks</title>
		<author>
			<persName><forename type="first">Claudio</forename><surname>Gallicchio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Scardapane</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-43883-8_3</idno>
	</analytic>
	<monogr>
		<title level="m">Recent Trends in Learning From Data</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">896</biblScope>
			<biblScope unit="page" from="43" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Raja</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<idno type="DOI">10.1109/TSP.2016.2546221</idno>
		<title level="m">Deep Neural Networks with Random Gaussian Weights: A Universal Classification Strategy? IEEE Transactions on Signal Processing</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="3444" to="3457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Extreme learning machine: A new learning scheme of feedforward neural networks</title>
		<author>
			<persName><forename type="first">Guang-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qin-Yu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chee-Kheong</forename><surname>Siew</surname></persName>
		</author>
		<idno type="DOI">10.1109/IJCNN.2004.1380068</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE International Joint Conference on Neural Networks</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="985" to="990" />
		</imprint>
	</monogr>
	<note>IEEE Cat. No.04CH37541</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Training two-layer ReLU networks with gradient descent is inconsistent</title>
		<author>
			<persName><forename type="first">David</forename><surname>Holzmüller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ingo</forename><surname>Steinwart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">181</biblScope>
			<biblScope unit="page" from="1" to="82" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks are universal approximators</title>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Halbert</forename><surname>White</surname></persName>
		</author>
		<idno type="DOI">10.1016/0893-6080(89)90020-8</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="359" to="366" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Harnessing Nonlinearity: Predicting Chaotic Systems and Saving Energy in Wireless Communication</title>
		<author>
			<persName><forename type="first">Herbert</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harald</forename><surname>Haas</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.1091277</idno>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<idno type="ISSN">0036-8075</idno>
		<imprint>
			<biblScope unit="volume">304</biblScope>
			<biblScope unit="issue">5667</biblScope>
			<biblScope unit="page" from="1095" to="9203" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Transfer learning for medical image classification: A literature review</title>
		<author>
			<persName><forename type="first">Hee</forename><forename type="middle">E</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alejandro</forename><surname>Cosa-Linan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nandhini</forename><surname>Santhanam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahboubeh</forename><surname>Jannesari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mate</forename><forename type="middle">E</forename><surname>Maros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Ganslandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC medical imaging</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">69</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The Sacred Infrastructure for Computational Research</title>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Chovanec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.25080/shinma-7f4c6e7-008</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Python in Science Conference</title>
		<meeting>the 16th Python in Science Conference</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Nikola</forename><surname>Kovachki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Burigede</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamyar</forename><surname>Azizzadenesheli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaushik</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.08481</idno>
		<title level="m">Neural Operator: Learning Maps Between Function Spaces</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Learning Multiple Layers of Features from Tiny Images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Generalization Error Analysis of Neural Networks with Gradient Based Regularization</title>
		<author>
			<persName><forename type="first">Lingfeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue-Cheng Tai</forename><surname>Null</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.4208/cicp.OA-2021-0211</idno>
	</analytic>
	<monogr>
		<title level="j">Communications in Computational Physics</title>
		<idno type="ISSN">1815-2406</idno>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1007" to="1038" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Towards a unified analysis of random fourier features</title>
		<author>
			<persName><forename type="first">Zhu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Francois</forename><surname>Ton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dino</forename><surname>Oglic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dino</forename><surname>Sejdinovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">108</biblScope>
			<biblScope unit="page" from="1" to="51" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Fourier Neural Operator for Parametric Partial Differential Equations</title>
		<author>
			<persName><forename type="first">Zongyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikola</forename><surname>Kovachki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamyar</forename><surname>Azizzadenesheli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Burigede</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaushik</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2010.08895</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Random Features for Kernel Approximation: A Survey on Algorithms, Theory, and Beyond</title>
		<author>
			<persName><forename type="first">Fanghui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yudong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johan</forename><forename type="middle">A K</forename><surname>Suykens</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2021.3097011</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<idno type="ISSN">1939-3539</idno>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="7128" to="7148" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">DeepONet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators</title>
		<author>
			<persName><forename type="first">Lu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengzhan</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">Em</forename><surname>Karniadakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03193</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">DeepXDE: A Deep Learning Library for Solving Differential Equations</title>
		<author>
			<persName><forename type="first">Lu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuhui</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiping</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">Em</forename><surname>Karniadakis</surname></persName>
		</author>
		<idno type="DOI">10.1137/19M1274067</idno>
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<idno type="ISSN">0036-1445</idno>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1095" to="7200" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A comprehensive and fair comparison of two neural operators (with practical extensions) based on FAIR data</title>
		<author>
			<persName><forename type="first">Lu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuhui</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengze</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiping</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somdatta</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">Em</forename><surname>Karniadakis</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cma.2022.114778</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Methods in Applied Mechanics and Engineering</title>
		<imprint>
			<biblScope unit="volume">393</biblScope>
			<biblScope unit="page">114778</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The ridgelet prior: A covariance function approach to prior specification for bayesian neural networks</title>
		<author>
			<persName><forename type="first">Takuo</forename><surname>Matsubara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Oates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">François-Xavier</forename><surname>Briol</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v22/20-1300.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">157</biblScope>
			<biblScope unit="page" from="1" to="57" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Solution of nonlinear ordinary differential equations by feedforward neural networks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Meade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Fernandez</surname></persName>
		</author>
		<idno type="DOI">10.1016/0895-7177(94)00160-X</idno>
	</analytic>
	<monogr>
		<title level="j">Mathematical and Computer Modelling</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="19" to="44" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Bayesian Learning for Neural Networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><surname>Neal</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-1-4612-0745-0</idno>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Statistics</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<date type="published" when="1996">1996</date>
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The Random Feature Model for Input-Output Maps between Banach Spaces</title>
		<author>
			<persName><forename type="first">Nicholas</forename><forename type="middle">H</forename><surname>Nelsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Stuart</surname></persName>
		</author>
		<idno type="DOI">10.1137/20M133957X</idno>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Scientific Computing</title>
		<idno type="ISSN">1064-8275</idno>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1095" to="7197" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Learning to Learn with Generative Models of Neural Network Checkpoints</title>
		<author>
			<persName><forename type="first">William</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.12892</idno>
		<ptr target="http://arxiv.org/abs/2209.12892" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Approximation theory of the MLP model in neural networks</title>
		<author>
			<persName><forename type="first">Allan</forename><surname>Pinkus</surname></persName>
		</author>
		<idno type="DOI">10.1017/S0962492900002919</idno>
	</analytic>
	<monogr>
		<title level="j">Acta Numerica</title>
		<idno type="ISSN">0962-4929</idno>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="143" to="195" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Transform once: Efficient operator learning in frequency domain</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Poli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Massaroli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Berto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinkyoo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="7947" to="7959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Uniform approximation of functions with random bases</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<idno type="DOI">10.1109/ALLERTON.2008.4797607</idno>
	</analytic>
	<monogr>
		<title level="m">46th Annual Allerton Conference on Communication, Control, and Computing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="555" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Hyper-representations as generative models: Sampling unseen neural network weights</title>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Schürholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Knyazev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Giró-I Nieto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damian</forename><surname>Borth</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper_files/paper/2022/file" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="27906" to="27920" />
		</imprint>
	</monogr>
	<note>d34b3d96b9dc12f7bce424b7ae-Paper-Conference.pdf</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Approximation rates for neural networks with general activation functions</title>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">W</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinchao</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neunet.2020.05.019</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page" from="313" to="321" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">Sho</forename><surname>Sonoda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.00648</idno>
		<title level="m">Fast Approximation and Estimation Bounds of Kernel Quadrature for Infinitely Wide Models</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">Len</forename><surname>Spek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tjeerd</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Heeringa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Brune</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.05020</idno>
		<title level="m">Duality for neural networks through Reproducing Kernel Banach Spaces</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Functional variational Bayesian neural networks</title>
		<author>
			<persName><forename type="first">Shengyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A representer theorem for deep neural networks</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Unser</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v20/18-418.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">110</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">A survey of transfer learning</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Taghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingding</forename><surname>Khoshgoftaar</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1186/s40537-016-0043-6</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Big Data</title>
		<idno type="ISSN">2196-1115</idno>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Can Shallow Neural Networks Beat the Curse of Dimensionality? A Mean Field Training Perspective</title>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Wojtowytsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Weinan</surname></persName>
		</author>
		<idno type="DOI">10.1109/TAI.2021.3051357</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Artificial Intelligence</title>
		<idno type="ISSN">2691-4581</idno>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="121" to="129" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">A Spectral-Based Analysis of the Separation between Two-Layer Neural Networks and Linear Methods</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jihao</forename><surname>Long</surname></persName>
		</author>
		<idno type="DOI">10.5555/3586589.3586708</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<idno type="ISSN">1532-4435</idno>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">On hyperparameter optimization of machine learning algorithms: Theory and practice</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdallah</forename><surname>Shami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">415</biblScope>
			<biblScope unit="page" from="295" to="316" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
