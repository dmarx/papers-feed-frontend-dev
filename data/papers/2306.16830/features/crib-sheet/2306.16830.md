- **Introduction of Sampling Weights**: Introduces a probability distribution for weights and biases in fully-connected neural networks, avoiding iterative optimization.
  
- **Key Contribution**: Proposes a data-driven sampling scheme that utilizes both input and output training data, enhancing accuracy and efficiency.

- **Universal Approximation**: Proves that sampled networks are universal approximators, capable of approximating any continuous function.

- **L2-Approximation Error**: For Barron functions, the L2-approximation error of sampled shallow networks decreases with the square root of the number of neurons, indicating efficiency in network design.

- **Invariance Properties**: The sampling scheme is invariant to rigid body transformations and scaling of input data, reducing the need for pre-processing techniques.

- **Numerical Experiments**: Demonstrates that sampled networks achieve comparable accuracy to iteratively trained networks while being constructed orders of magnitude faster.

- **Mathematical Framework**: Defines sampled networks where weights and biases are determined by pairs of input points:
  - **Weight Construction**: 
    \[
    w_{l,i} = s_1 \frac{x^{(2)}_{l-1,i} - x^{(1)}_{l-1,i}}{\|x^{(2)}_{l-1,i} - x^{(1)}_{l-1,i}\|^2}
    \]
  - **Bias Construction**: 
    \[
    b_{l,i} = \langle w_{l,i}, x^{(1)}_{l-1,i} \rangle + s_2
    \]

- **Activation Functions**: Focuses on ReLU and tanh:
  - For ReLU: \(s_1 = 1\), \(s_2 = 0\)
  - For tanh: \(s_1 = 2s_2\), \(s_2 = \ln(3)/2\)

- **Theorem 1**: States that sampled networks with L hidden layers are dense in \(C(X, \mathbb{R}^{N_{L+1}})\), ensuring they can approximate a wide range of functions.

- **Theorem 2**: Establishes that the L2 approximation error of Barron functions is bounded, confirming the effectiveness of the proposed sampling method.

- **Comparison with Existing Methods**: Highlights differences from random Fourier features and Bayesian neural networks, emphasizing the direct relationship between data points and weights in the proposed method.

- **Efficiency of Sampling**: The construction of weights allows for efficient use of neurons, leading to reduced computational costs compared to traditional gradient-based methods.