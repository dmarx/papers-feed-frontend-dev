# Hallo3: Highly Dynamic and Realistic Portrait Image Animation with Diffusion Transformer Networks

## Abstract

## 

Existing methodologies for animating portrait images face significant challenges, particularly in handling nonfrontal perspectives, rendering dynamic objects around the portrait, and generating immersive, realistic backgrounds. In this paper, we introduce the first application of a pretrained transformer-based video generative model that demonstrates strong generalization capabilities and generates highly dynamic, realistic videos for portrait animation, effectively addressing these challenges. The adoption of a new video backbone model makes previous U-Netbased methods for identity maintenance, audio conditioning, and video extrapolation inapplicable. To address this limitation, we design an identity reference network consisting of a causal 3D VAE combined with a stacked series of transformer layers, ensuring consistent facial identity across video sequences. Additionally, we investigate various speech audio conditioning and motion frame mechanisms to enable the generation of continuous video driven by speech audio. Our method is validated through experiments on benchmark and newly proposed wild datasets, demonstrating substantial improvements over prior methods in generating realistic portraits characterized by diverse orientations within dynamic and immersive scenes. Further visualizations and the source code are available at: [https://fudan-generative-vision.github.io/hallo3](https://fudan-generative-vision.github.io/hallo3).

## Introduction

Portrait image animation refers to the process of generating realistic facial expressions, lip movements, and head poses based on portrait images. This technique leverages various motion signals, including audio, textual prompts, facial keypoints, and dense motion flow. As a crossdisciplinary research task within the realms of computer vision and computer graphics, this area has garnered increasing attention from both academic and industrial communities. Furthermore, portrait image animation has critical ap-plications across several sectors, including film and animation production, game development, social media content creation, and online education and training.

In recent years, the field of portrait image animation has witnessed rapid advancements. Early methodologies predominantly employed facial landmarks-key points [[31,](#b30)[43,](#b42)[45]](#b44) on the face utilized for the localization and representation of critical regions such as the mouth, eyes, eyebrows, nose, and jawline. Additionally, these methods [[10,](#b9)[26,](#b25)[44,](#b43)[51]](#b50) incorporated 3D parametric models, notably the 3D Morphable Model (3DMM) [[3]](#b2), which captures variability in human faces through a statistical shape model integrated with a texture model. However, the application of explicit approaches grounded in intermediate facial representations is constrained by the accuracy of expression and head pose reconstruction, as well as the richness and precision of the resultant expressions. Simultaneously, significant advancements in Generative Adversarial Networks (GANs) and diffusion models have notably benefited portrait image animation. These advancements [[7,](#b6)[17,](#b16)[35,](#b34)[38,](#b37)[40,](#b39)[48,](#b47)[51]](#b50) enhance the high-resolution and high-quality generation of realistic facial details, facilitate generalized character animation, and enable longterm identity preservation. Recent contributions to the field-including Live Portrait [[11]](#b10), which leverages GAN technology for portrait animation with stitching and retargeting control, as well as various end-to-end methods such as VASA-1 [[40]](#b39), EMO [[35]](#b34), and Hallo [[8,](#b7)[39]](#b38) employing diffusion models-exemplify these advancements.

Despite these improvements, existing methodologies encounter substantial limitations. First, many current facial animation techniques emphasize eye gaze, lip synchronization, and head posture while often depending on reference portrait images that present a frontal, centered view of the subject. This reliance presents challenges in handling profile, overhead, or low-angle perspectives for portrait animation. Secondly, accounting for significant accessories, such as holding a smartphone, microphone, or wearing closely fitted objects, presents challenges in generating realistic motion for the associated objects within video sequences.

Figure [1](#). Demonstration of the proposed approach. Given a reference image, an audio sequence, and a textual prompt, the method generates animated portraits from frontal or different perspectives while preserving the portrait identity over extended durations. Additionally, it incorporates dynamic foreground and background elements, with temporal consistency and high visual fidelity.

Third, existing methods often assume static backgrounds, undermining their ability to generate authentic video effects in dynamic scenarios, such as those with campfires in the foreground or crowded street scenes in the background.

Recent advancements in diffusion transformer (DiT)based video generation models [[2,](#b1)[18,](#b17)[22,](#b21)[41]](#b40) have addressed several challenges associated with traditional video generation techniques, including issues of realism, dynamic movement, and subject generalization. In this paper, we present the first application of a pretrained DiT-based video generative model to the task of portrait image animation. The introduction of this new video backbone model renders previous U-Net-based methods for identity maintenance, audio conditioning, and video extrapolation impractical. We tackle these issues from three distinct perspectives. (1) Identity preservation: We employ a 3D VAE in conjunction with a stack of transformer layers as an identity reference network, enabling the embedding and injection of identity information into the denoising latent codes for self-attention. This facilitates accurate representation and long-term preservation of the facial subject's identity.

(2) Speech audio conditioning: We achieve high alignment between speech audio-serving as motion control information-and facial expression dynamics during training, which allows for precise control during inference. We investigate the use of adaptive layer normalization and crossattention strategies, effectively integrating audio embeddings through the latter. (3) Video extrapolation: Addressing the limitations of the DiT-based model in generating continuous videos, which is constrained to a maximum of several tens of frames, we propose a strategy for longduration video extrapolation. This approach uses motion frames as conditional information, wherein the final frames of each generated video serve as inputs for subsequent clip generation.

We validate our approach using benchmark datasets, including HTDF and Celeb-V, demonstrating results comparable to previous methods that are constrained to limited datasets characterized by frontal, centered faces, static backgrounds, and defined expressions. Furthermore, our method successfully generates dynamic foregrounds and backgrounds, accommodating complex poses, such as profile views or interactions involving devices like smartphones and microphones, yielding realistic and smoothly animated motion, thereby addressing challenges that previous methodologies have struggled to resolve effectively.

## Related Work

Portrait Image Animation. Recent advancements in the domain of portrait image animation have been significantly propelled by innovations in audio-driven techniques. Notable frameworks, such as LipSyncExpert [[23]](#b22) and SadTalker [[46]](#b45), have tackled challenges related to facial synchronization and expression modulation, achieving dynamic lip movements and coherent head motions. Concurrently, DiffTalk [[30]](#b29) and VividTalk [[33]](#b32) have integrated latent diffusion models, enhancing output quality while generalizing across diverse identities without the necessity for extensive fine-tuning. Furthermore, studies such as DreamTalk [[19]](#b18) and EMO [[35]](#b34) underscore the importance of emotional expressiveness by showcasing the integration of audio cues with facial dynamics. AniPortrait [[38]](#b37) and VASA-1 [[40]](#b39) propose methodologies that facilitate the generation of high-fidelity animations, emphasizing temporal consistency along with effective exploitation of static images and audio clips. In addition, recent innovations like LivePortrait [[11]](#b10) and Loopy [[14]](#b13) focus on enhancing computational efficiency while ensuring realism and fluid motion. Furthermore, the works of Hallo [[39]](#b38) and Hallo2 [[8]](#b7) have made significant progress in extending capabilities to facilitate long-duration video synthesis and integrating adjustable semantic inputs, thereby marking a step towards richer and more controllable content generation. Nevertheless, existing facial animation techniques still encounter limitations in addressing extreme facial poses, accommodating background motion in dynamic environments, and incorporating camera movements dictated by textual prompts. Diffusion-Based Video Generation. Unet-based diffusion model has made notable strides, exemplified by frameworks such as Make-A-Video and MagicVideo [[49]](#b48). Specifically, Make-A-Video [[32]](#b31) capitalizes on pre-existing Textto-Image (T2I) models to enhance training efficiency without necessitating paired text-video data, thereby achieving state-of-the-art results across a variety of qualitative and quantitative metrics. Simultaneously, MagicVideo [[49]](#b48) employs an innovative 3D U-Net architecture to operate within a low-dimensional latent space, achieving efficient video synthesis while significantly reducing computational requirements. Building upon these foundational principles, AnimateDiff [[12]](#b11) introduces a motion module that integrates seamlessly with personalized T2I models, allowing for the generation of temporally coherent animations without the need for model-specific adjustments. Additionally, VideoComposer [[37]](#b36) enhances the controllability of video synthesis by incorporating spatial, temporal, and textual conditions, which facilitates improved inter-frame consistency. The development of diffusion models continues with the advent of DiT-based approaches such as CogVideoX [[41]](#b40) and Movie Gen [[22]](#b21). CogVideoX employs a 3D Variational Autoencoder to improve video fidelity and narrative coherence, whereas Movie Gen establishes a robust foundation for high-quality video generation complemented by advanced editing capabilities. In the present study, we adopt the DiT diffusion formulation to optimize the generalization capabilities of the generated video.

## Methodology

This methodology section systematically outlines the approaches employed in our study. Section 3.1 describes the baseline transformer diffusion network, detailing its architecture and functionality. Section 3.2 focuses on the integra-tion of speech audio conditions via a cross-attention mechanism. Section 3.3 discusses the implementation of the identity reference network, which is crucial for preserving facial identity coherence throughout extended video sequences. Section 3.4 reviews the training and inference procedures used for the transformer diffusion network. Finally, Section 3.5 details the comprehensive strategies for data sourcing and preprocessing.

## Baseline Transformer Diffusion Network

Baseline Network. The CogVideoX model [[41]](#b40) serves as the foundational architecture for our transformer diffusion network, employing a 3D VAE for the compression of video data. In this framework, latent variables are concatenated and reshaped into a sequential format, denoted as z t . Concurrently, the model utilizes the T5 architecture [[24]](#b23) to encode textual inputs into embeddings, represented as c text . The combined sequences of video latent representations z t and textual embeddings c text are subsequently processed through an expert transformer network. To address discrepancies in feature space between text and video, we implement expert adaptive layer normalization techniques, which facilitate the effective utilization of temporal information and ensure robust alignment between visual and semantic data. Following this integration, a repair mechanism is applied to restore the original latent variable, after which the output is decoded through the 3D causal VAE decoder to reconstruct the video. Furthermore, the incorporation of 3D Rotational Positional Encoding (3D RoPE) [[41]](#b40) enhances the model's capacity to capture inter-frame relationships across the temporal dimension, thereby establishing long-range dependencies within the video framework. Conditioning in Diffusion Transformer. In addition to the textual prompt c text , we introduce two supplementary conditions: the speech audio condition c audio and the identity appearance condition c id .

Within diffusion transformers, four primary conditioning mechanisms are identified: in-context conditioning, cross-attention, adaptive layer normalization (adaLN), and adaLN-zero [[20]](#b19). Our investigation primarily focuses on cross-attention and adaptive layer normalization (adaLN). Cross-attention enhances the model's focus on conditional information by treating condition embeddings as keys and values, while latent representations serve as queries. Although adaLN is effective in simpler conditioning scenarios, it may not be optimal for more complex conditional embeddings that incorporate richer semantic details, such as sequential speech audio. Relevant comparative analyses will be elaborated upon in the experimental section.

## Audio-Driven Transformer Diffusion

Speech Audio Embedding. To extract salient audio features for our proposed model, we utilize the wav2vec frame-Figure [2](#). The overview of the proposed method. Specifically, the method takes a reference image, an audio sequence, and a textual prompt as inputs to generate a video output with temporal consistency and high visual fidelity. We leverage the casual 3D VAE, T5, and Wav2Vec models to process the visual, textual, and audio features, respectively. The Identity Reference Network extracts identity features from the input reference image and textual prompt, enabling controllable animation while preserving the subject's appearance. The audio encoder generates motion information for lip synchronization, while the face encoder extracts facial features to maintain consistency in facial expressions. The 3D Full Attention and Audio-Attention Modules combine identity and motion data within a denoising network, producing high-fidelity, temporally consistent, and controllable animated videos. work developed by Schneider et al. [[28]](#b27). The audio representation is defined as c audio . Specifically, we concatenate the audio embeddings generated by the final twelve layers of the wav2vec network, resulting in a comprehensive semantic representation capable of capturing various audio hierarchies. This concatenation emphasizes the significance of phonetic elements, such as pronunciation and prosody, which are crucial as driving signals for character generation.

To transform the audio embeddings obtained from the pretrained model into frame-specific representations, we apply three successive linear transformation layers, mathematically expressed as: c

$(f ) audio = L 3 (L 2 (L 1 (c audio )))$, where L 1 , L 2 , and L 3 represent the respective linear transformation functions. This systematic approach ensures that the resulting frame-specific representations effectively encapsulate the nuanced audio features essential for the performance of our model. Speech Audio Conditioning.

We explore three fusion strategies-self-attention, adaptive normalization, and cross-attention-as illustrated in Figure [3](#) to integrate audio condition into the DiT-based video generation model. Our experiments show that the cross-attention strategy delivers the best performance in our model. For more details, please refer to Section 4.3.

Following this, we integrate audio attention layers after each face-attention layer within the denoising network, employing a cross-attention mechanism that facilitates interaction between the latent encodings and the audio embeddings. Specifically, within the DiT block, the mo-tion patches function as keys and values in the crossattention computation with the hidden states z t :

$z t = CrossAttention(z t , c (f )$audio ). This methodology leverages the conditional information from the audio embeddings to enhance the coherence and relevance of the generated outputs, ensuring that the model effectively captures the intricacies of the audio signals that drive character generation.

## Identity Consistent Transformer Diffusion

Identity Reference Network. Diffusion transformer-based video generation models encounter significant challenges in maintaining facial identity coherence, particularly as the length of the generated video increases. While incorporating speech audio embeddings as conditional features can establish a correspondence between audio speech and facial movements, prolonged generation often leads to rapid degradation of facial identity characteristics.

To address this issue, we introduce a control condition within the existing diffusion transformer architecture to ensure long-term consistency of facial identity appearance. We explore four strategies (as shown in Figure [4](#)) for appearance conditioning: 1) Face attention, where identity features are encoded by the face encoder and combined with a cross-attention module; 2) Face adaptive norm, which integrates features from the face encoder with an adaptive layer normalization technique; 3) Identity reference network, where identity features are captured by a 3D VAE and combined with some transformer layers; and 4) Face attention and Identity reference network, which encodes identity features using both the face encoder and 3D VAE, combining them with self-attention and cross-attention. Our experiments show that the combination with Face attention and Identity reference net achieves the best performance in our model. For further details, please refer to Section 4.3.

We treat a reference image as a single frame and input it into a causal 3D VAE to obtain latent features, which are then processed through a reference network consisting of 42 transformer layers. Mathematically, if I ref denotes the reference image, the encoder function of the 3D VAE is defined as: z id = E 3D (I ref ), where z id represents the latent features associated with the reference image.

During the operation of the reference network, we extract vision tokens from the input of the 3D full attention mechanism for each transformer layer, which serve as reference features z id . These features are integrated into corresponding layers of the denoising network to enhance its capability, expressed as: z t,enhanced = SelfAttention(z t , z id ), where z t is the latent representation at time step t. Given that both the reference network and denoising network leverage the same causal 3D VAE with identical weights and comprise the same number of transformer layers (42 layers in our implementation), the visual features generated from both networks maintain semantic and scale consistency. This consistency allows the reference network's features to incorporate the appearance characteristics of facial identity from the reference image while minimizing disruption to the original feature representations of the denoising network, thereby reinforcing the model's capacity to generate coherent and identity-consistent facial animations across longer video sequences. Temporal Motion Frames. To facilitate long video inference, we introduce the last n frames of the previously generated video, referred to as motion frames, as additional conditions. Given a generated video length of L and the corresponding latent representation of l frames, we denote the motion frames as N . The motion frames are processed through the 3D VAE to obtain n frames of latent codes. We apply zero padding to the subsequent (l -n) frames and concatenate them with l frames of Gaussian noise. This concatenated representation is then patchified to yield vision tokens, which are subsequently input into the denoising network. By repeatedly utilizing motion frames, we achieve temporally consistent long video inference.

## Training and Inference

Training. The training process consists of two phases:

(1) Identity Consistency Phase. In this initial phase, we train the model to generate videos with consistent identity. The parameters of the 3D Variational Autoencoder (VAE) and face image encoder remain fixed, while the parameters of the 3D full attention blocks in both the reference and denoising networks, along with the face attention blocks in the denoising network, are updated during training. The model's input includes a randomly sampled reference image from the training video, a textual prompt, and the face embedding. The textual prompt is generated using MiniCPM [[42]](#b41), which describes human appearance, actions, and detailed environmental background. The face embedding is extracted via InsightFace [[9]](#b8). With these inputs, the model generates a video comprising 49 frames.

(2) Audio-Driven Video Generation Phase. In the second phase, we extend the training to include audio-driven video generation. We integrate audio attention modules into each transformer block of the denoising network, while fixing the parameters of other components and updating only those of the audio attention modules. Here, the model's input consists of a reference image, an audio embedding, and a textual prompt, resulting in a sequence of 49 video frames driven by audio. Inference. During inference, the model receives a reference image, a segment of driving audio, a textual prompt, and motion frames as inputs. The model then generates a video that exhibits identity consistency and lip synchronization based on the driving audio. To produce long videos, we utilize the last two frames of the preceding video as motion frames, thereby achieving temporally consistent video generation.

## Dataset

In this section, we will give a detailed introduction of our data curation, including data sources, filtering strategy and data statistics. Figure [5](#fig_1) shows the data pipeline and the statistical analysis of the final data. Data Sources The training data used in this work is prepared from three distinct sources to ensure diversity and generalization. Specifically, the sources are: (1) HDTF dataset [[47]](#b46), which contains 8 hours of raw video footage;

(2) YouTube data, which consists of 1,200 hours of public raw videos; (3) a large scale movie dataset, which contains film videos of 2,346 hours. Our dataset contains a large scale of human identities and, however, we find that YouTube and movie dataset contains a large amount of noised data. Therefore, we design a data curation pipeline as follows to construct a high-quality and diverse talking dataset, as shown in Figure [5(a)](#fig_1). Video Filtering. During the data pre-processing phase, we implement a series of meticulous filtering steps to ensure the quality and applicability of the dataset. The workflow includes three stages: extraction of single-speaker, motion filter and post-processing. Firstly, we select video of single-speaker. This stage aims to clean the video content to solve camera shot, background noise, etc, using existing tools [[4,](#b3)[21]](#b20). After that, we apply several filtering techniques to ensure the quality of head motion, head pose, camera motion, etc [[6,](#b5)[15,](#b14)[16]](#b15). In this stage, we compute all metric scores for each clip, therefore, we can flexibly adjust data screening strategies to satisfy different data requirement of our multiple training stages or strategies. Finally, based on the facial positions detected in previous steps, we crop the videos to a 3:2 aspect ratio to meet the model's input requirements. We then select a random frame from each video and use InsightFace [[25]](#b24) to encode the face into embeddings, providing essential facial feature information for the model. Additionally, we extract the audio from the videos and encode it into embeddings using Wav2Vec2 model [[1]](#b0), facilitating the incorporation of audio conditions during model training. Data Statistics. Following the data cleaning and filtering processes, we conducted a detailed analysis of the final dataset to assess its quality and suitability for the intended modeling tasks. Finally, our training data contains about 134 hours of videos, including 6 hours of high-quality data from HDTF dataset, 72 hours of YouTube videos, and 56 hours of movie videos. 

## Experiment

## Experimental Setups

Implementation. We initialize the identity reference and denoising networks with weights derived from CogVideoX-5B-I2V [[41]](#b40). During both training phases, we employ the vprediction diffusion loss [[27]](#b26) for optimization. Each training phase comprises 20,000 steps, utilizing 64 NVIDIA H100 GPUs. The batch size per GPU is set to 1, with a learning rate of 480 × 720 pixels. To enhance video generation variability, the reference image, guidance audio and textual prompt are dropped with a probability of 0.05 during training. Evaluation Metrics. We employed a range of evaluation metrics for generated videos across benchmark datasets, including HDTF [[47]](#b46) and Celeb-V [[50]](#b49). These metrics comprise Fréchet Inception Distance (FID) [[29]](#b28), Fréchet Video Distance (FVD) [[36]](#b35), Synchronization-C (Sync-C) [[6]](#b5), Synchronization-D (Sync-D) [[6]](#b5), and E-FID [[35]](#b34). FID and FVD quantify the similarity between generated images and real data, while Sync-C and Sync-D assess lip synchronization accuracy. E-FID evaluates the image quality based on features extracted from the Inception network. Besides, we introduced VBench [13] metrics to enhance evaluation, focusing on dynamic degree and subject consistency. Dynamic degree is measured using RAFT [[34]](#b33) to quantify the extent of motion in generated videos, providing a comprehensive assessment of temporal quality. Subject consistency is measured through DINO [[5]](#b4) feature similarity, ensuring uniformity of a subject's appearance across frames. Baseline Approaches. We considered several representative audio-driven talking face generation methods for comparison, all of which have publicly available source code or implementations. These methods include SadTalker [[45]](#b44), DreamTalk [[19]](#b18), AniPortrait [[38]](#b37), and Hallo [[8,](#b7)[39]](#b38). The selected approaches encompass both GANs and diffusion models, as well as techniques utilizing intermediate facial representations alongside end-to-end frameworks. This diversity in methodologies allows for a comprehensive evaluation of the effectiveness of our proposed approach compared to existing solutions.

## Comparison with State-of-the-art

Comparison on HDTF and Celeb-V Dataset. As shown in Table [1](#tab_0) and 2, our method achieves best results on FID, FVD on both datasets. Although our approach shows some disparity compared to the state-of-the-art in lip synchronization, it still demonstrates promising results as illustrated in Figure [6](#fig_3). This is because, to generate animated portraits from different perspectives, our training data primarily consists of talking videos with significant head and body movements, as well as diverse dynamic scenes, unlike static scenes with minimal motion. While this may lead to some performance degradation on lip synchronization, it better reflects realistic application scenarios. Table 3. Comparison with other methods on our proposed wild dataset. the performance of the general talking portrait video generation, we carefully collect 34 representative cases for evaluation. This dataset consists of portrait images with various head proportions, head poses, static and dynamic scenes and complex headwears and clothing. To achieve comprehensive assessment, we evaluate the performance on lip synchronization (Sync-C and Sync-D), motion strength (subject and background dynamic degree) and video quality (subject and background FVD). As shown in Table [3](#), our method generates videos with largest head and background dynamic degree (13.286 and 4.481) while keeping lip synchronization of highest accuracy.

Figure [7](#fig_4) provides a qualitative comparison of different portrait methods on a "wild" dataset. The results reveal that other methods struggle to animate side-face portrait images, often resulting in static poses or facial distortions. Additionally, these methods tend to focus solely on animating the face, overlooking interactions with other objects in the foreground-such as the dog next to the elderly, or the dy-namic movement of the background-like the ostrich behind the girl. In contrast, as shown in Figure [8](#fig_5) our method produces realistic portraits with diverse orientations and complex foreground and background scenes.

## Ablation Study and Discussion

Audio Conditioning. Table [4](#tab_3) and Figure [9](#fig_6) illustrate the effects of various strategies for incorporating audio conditioning. The results demonstrate that using cross-attention to integrate audio improves lip synchronization by enhancing the local alignment between visual and audio features, particularly around the lips. This is evident from the improvements in Sync-C and Sync-D, and it also contributes to a degree of enhancement in video quality. Identity Reference Network. Table [5](#) and Figure [10](#fig_7) evaluate different identity conditioning strategies. The results indicate that without an identity condition, the model fails to preserve the portrait appearance. When using face embedding alone, the model introduces blur and distortion, as  it focuses solely on facial features and disrupts the global visual context. To address this, we introduce an identity reference network to preserve global features while making facial motion more controllable through identity-based facial embeddings. Thus, the proposed method achieves a lower FID of 23.458 and FVD of 242.602, while maintaining lip synchronization. Temporal Motion Frames. Table [6](#) presents an analysis of varying temporal motion frames. One motion frame achieves the highest Sync-C score (6.889) and the lowest Sync-D score (8.695), indicating substantial lip synchronization. CFG Scales for Diffusion Model. Table [7](#) provides a quantitative analysis of video generations using various CFG scales for audio, text, and reference images. A comparison between the second and fourth rows demonstrates that increasing the audio CFG scale enhances the model's ability to synchronize lip movements. The text CFG scale significantly influences the video's dynamism, as indicated in the first three rows, where both the subject's and the background's dynamics increase with higher text CFG scales. Conversely, the reference image CFG scale primarily governs the subject's appearance; higher values improve subject consistency, as illustrated by the second and fifth rows. Among the tested configurations, setting λ a = 3.5, λ t = 3.5, and λ i = 1.0 yields a balanced performance. This interplay between visual fidelity and dynamics underscores the effectiveness of CFG configurations in generating realistic portrait animations.

Limitations and Future Works. Despite the advancements in portrait image animation techniques presented in this study, several limitations warrant acknowledgment. While   Table 6. Ablation on the number of motion frames.

the proposed methods improve identity preservation and lip synchronization, the model's ability to realistically represent intricate facial expressions in dynamic environments still requires refinement, especially under varying illumination conditions. Future work will focus on enhancing the model's robustness to diverse perspectives and interactions, incorporating more comprehensive datasets that include varied backgrounds and facial accessories. Furthermore, investigating the integration of real-time feedback mechanisms could significantly enhance the interactivity and realism of portrait animations, paving the way for broader applications in live media and augmented reality. Safety Considerations. The advancement of portrait image animation technologies, particularly those driven by audio inputs, presents several social risks, most notably concerning the ethical implications associated with the creation of highly realistic portraits that may be misused for deepfake purposes. To address these concerns, it is essential to develop comprehensive ethical guidelines and responsible use practices. Moreover, issues surrounding privacy and consent are prominent when utilizing individuals' images and voices. It is imperative to establish transparent data usage policies, ensuring that individuals provide informed consent and that their privacy rights are fully protected. By acknowledging these risks and implementing appropriate mitigation strategies, this research aims to promote the responsible and ethical development of portrait image animation technology.

## Generation Controllability

Textual Prompt for Subject Animation. To evaluate whether textual conditional controllability is effectively preserved, we conducted a series of experiments comparing the performance of our method to that of the baseline model, CogVideoX [[41]](#b40), using same text prompts. As shown in Figure [11](#), the results shows that our model maintains its ability for textual control, and effectively captures the interaction between different subjects as dictated by the textual prompts.

Textual Prompt for Foreground and Background Animation. We also explore model's ability to follow the foreground and background textual prompt. As illustrated in Figure [12](#), our method animates the foreground and background subjects naturally, such as the ocean waves and flickering candlelight. The results demonstrates the model's ability to control foreground, and background with the textual caption, which is maintained even after introducing the audio condition.  [7](#). Quantitative study of audio, text and image CFG scales on our proposed wild dataset.

## Ours CogVideoX

A young girl with blonde hair embraces her beautiful brown horse in a tender moment. An elderly man enjoys the company of vibrant birds perched on his arms in nature.

## Reference Prompt

A young girl with a charming smile wearing a straw hat cuddles her white rabbit.

Figure [11](#). Condition on interacting with subjects. Our method achieves alignment comparable to that of CogVideX, maintaining the controllability of interactive subjects even after introducing the audio condition.

## Ours CogVideoX

A cheerful man wearing a floral shirt and sunglasses enjoys the beautiful beachside scenery, with waves gently lapping against the shore.

A radiant young woman smiles brightly, surrounded by a vibrant field of blooming sunflowers, their golden petals swaying gently in the breeze.

## Ref. Image Prompt

A young boy glows with happiness as he prepares to blow out candles on his birthday cake, their flames dancing and flickering gently in the soft glow of the room. A stylish woman in a floral dress and sunglasses enjoys a serene moment on a sunny ocean, as gentle waves ripple against the shore, sparkling under the sunlight.

Figure [12](#). Textual condition on foreground and background. Our method achieves alignment comparable to that of CogVideX, maintaining the controllability of foreground and background after incorporating the audio condition.

## Conclusion

This paper introduces advancements in portrait image animation utilizing the enhanced capabilities of a transformer-based diffusion model. By integrating audio conditioning through cross-attention mechanisms, our approach effectively captures the intricate relationship between audio signals and facial expressions, achieving sub-stantial lip synchronization. To preserve facial identity across video sequences, we incorporate an identity reference network. Additionally, we utilize motion frames to enable the model to generate long-duration video extrapolations. Our model produces animated portraits from diverse perspectives, seamlessly blending dynamic foreground and background elements while maintaining temporal consistency and high fidelity.

![Figure 3. Different strategies of audio conditioning. (a) self-attention; (b) adaptive norm; (c) cross-attention.]()

![Figure 5. Illustration of the dataset, including the flow of data processing, data distribution across different metric, and the visualization of some representative portrait images for inference.]()

![Figure 5(b) also shows other statistics, such as Lip Sync score (Sync-C and Sync-D), face rotation, face ratio (the ratio of face height to video height).]()

![Figure 6. Qualitative comparison on the HTDF (left) and CelebV (right) data-set.]()

![Figure 7. Complex facial identity with dynamic accessories subjects and different pose orientation.]()

![Figure 8. Complex scenes with dynamic foreground or background and various head poses.]()

![Figure 9. Qualitative comparison of different strategies for audio conditioning.]()

![Figure 10. Qualitative comparison of different strategies (as in Table 5) for identity conditioning. (a) No identity condition; (b) Face attention; (c) Face adaptive norm; (d) Identity reference network; (e) Face attention and Identity reference network.]()

![t ↓ λ a = 3.5 λ t = 1.0 λ i = 1Base λ a = 3.5 λ t = 3.5 λ i = 1λ t ↑ λ a = 3.5 λ t = 6.0 λ i = 1λ a ↑ λ a = 6.0 λ t = 3.5 λ i = 1λ i ↑ λ a = 3.5 λ t = 3.5 λ i = 3]()

![]()

![Comparison on Wild Dataset. To effectively demonstrateComparison with the other methods on HDTF dataset.]()

![Comparison with other methods on Celeb-V dataset.]()

![Comparison on the different strategy of audio conditioning.]()

