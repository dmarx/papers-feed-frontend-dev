<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hallo3: Highly Dynamic and Realistic Portrait Image Animation with Diffusion Transformer Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-01-04">4 Jan 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiahao</forename><surname>Cui</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University</orgName>
								<address>
									<addrLine>2 Baidu Inc</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hui</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University</orgName>
								<address>
									<addrLine>2 Baidu Inc</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yun</forename><surname>Zhan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University</orgName>
								<address>
									<addrLine>2 Baidu Inc</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hanlin</forename><surname>Shang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University</orgName>
								<address>
									<addrLine>2 Baidu Inc</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kaihui</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University</orgName>
								<address>
									<addrLine>2 Baidu Inc</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuqi</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University</orgName>
								<address>
									<addrLine>2 Baidu Inc</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shan</forename><surname>Mu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University</orgName>
								<address>
									<addrLine>2 Baidu Inc</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hang</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Siyu</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University</orgName>
								<address>
									<addrLine>2 Baidu Inc</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shanghai Academy of AI for Science</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Hallo3: Highly Dynamic and Realistic Portrait Image Animation with Diffusion Transformer Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-01-04">4 Jan 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">BBC0BB019FC974250234FF718580FD55</idno>
					<idno type="arXiv">arXiv:2412.00733v3[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing methodologies for animating portrait images face significant challenges, particularly in handling nonfrontal perspectives, rendering dynamic objects around the portrait, and generating immersive, realistic backgrounds. In this paper, we introduce the first application of a pretrained transformer-based video generative model that demonstrates strong generalization capabilities and generates highly dynamic, realistic videos for portrait animation, effectively addressing these challenges. The adoption of a new video backbone model makes previous U-Netbased methods for identity maintenance, audio conditioning, and video extrapolation inapplicable. To address this limitation, we design an identity reference network consisting of a causal 3D VAE combined with a stacked series of transformer layers, ensuring consistent facial identity across video sequences. Additionally, we investigate various speech audio conditioning and motion frame mechanisms to enable the generation of continuous video driven by speech audio. Our method is validated through experiments on benchmark and newly proposed wild datasets, demonstrating substantial improvements over prior methods in generating realistic portraits characterized by diverse orientations within dynamic and immersive scenes. Further visualizations and the source code are available at: <ref type="url" target="https://fudan-generative-vision.github.io/hallo3">https://fudan-generative-vision.github.io/hallo3</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Portrait image animation refers to the process of generating realistic facial expressions, lip movements, and head poses based on portrait images. This technique leverages various motion signals, including audio, textual prompts, facial keypoints, and dense motion flow. As a crossdisciplinary research task within the realms of computer vision and computer graphics, this area has garnered increasing attention from both academic and industrial communities. Furthermore, portrait image animation has critical ap-plications across several sectors, including film and animation production, game development, social media content creation, and online education and training.</p><p>In recent years, the field of portrait image animation has witnessed rapid advancements. Early methodologies predominantly employed facial landmarks-key points <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b44">45]</ref> on the face utilized for the localization and representation of critical regions such as the mouth, eyes, eyebrows, nose, and jawline. Additionally, these methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b50">51]</ref> incorporated 3D parametric models, notably the 3D Morphable Model (3DMM) <ref type="bibr" target="#b2">[3]</ref>, which captures variability in human faces through a statistical shape model integrated with a texture model. However, the application of explicit approaches grounded in intermediate facial representations is constrained by the accuracy of expression and head pose reconstruction, as well as the richness and precision of the resultant expressions. Simultaneously, significant advancements in Generative Adversarial Networks (GANs) and diffusion models have notably benefited portrait image animation. These advancements <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b50">51]</ref> enhance the high-resolution and high-quality generation of realistic facial details, facilitate generalized character animation, and enable longterm identity preservation. Recent contributions to the field-including Live Portrait <ref type="bibr" target="#b10">[11]</ref>, which leverages GAN technology for portrait animation with stitching and retargeting control, as well as various end-to-end methods such as VASA-1 <ref type="bibr" target="#b39">[40]</ref>, EMO <ref type="bibr" target="#b34">[35]</ref>, and Hallo <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b38">39]</ref> employing diffusion models-exemplify these advancements.</p><p>Despite these improvements, existing methodologies encounter substantial limitations. First, many current facial animation techniques emphasize eye gaze, lip synchronization, and head posture while often depending on reference portrait images that present a frontal, centered view of the subject. This reliance presents challenges in handling profile, overhead, or low-angle perspectives for portrait animation. Secondly, accounting for significant accessories, such as holding a smartphone, microphone, or wearing closely fitted objects, presents challenges in generating realistic motion for the associated objects within video sequences.</p><p>Figure <ref type="figure">1</ref>. Demonstration of the proposed approach. Given a reference image, an audio sequence, and a textual prompt, the method generates animated portraits from frontal or different perspectives while preserving the portrait identity over extended durations. Additionally, it incorporates dynamic foreground and background elements, with temporal consistency and high visual fidelity.</p><p>Third, existing methods often assume static backgrounds, undermining their ability to generate authentic video effects in dynamic scenarios, such as those with campfires in the foreground or crowded street scenes in the background.</p><p>Recent advancements in diffusion transformer (DiT)based video generation models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b40">41]</ref> have addressed several challenges associated with traditional video generation techniques, including issues of realism, dynamic movement, and subject generalization. In this paper, we present the first application of a pretrained DiT-based video generative model to the task of portrait image animation. The introduction of this new video backbone model renders previous U-Net-based methods for identity maintenance, audio conditioning, and video extrapolation impractical. We tackle these issues from three distinct perspectives. (1) Identity preservation: We employ a 3D VAE in conjunction with a stack of transformer layers as an identity reference network, enabling the embedding and injection of identity information into the denoising latent codes for self-attention. This facilitates accurate representation and long-term preservation of the facial subject's identity.</p><p>(2) Speech audio conditioning: We achieve high alignment between speech audio-serving as motion control information-and facial expression dynamics during training, which allows for precise control during inference. We investigate the use of adaptive layer normalization and crossattention strategies, effectively integrating audio embeddings through the latter. (3) Video extrapolation: Addressing the limitations of the DiT-based model in generating continuous videos, which is constrained to a maximum of several tens of frames, we propose a strategy for longduration video extrapolation. This approach uses motion frames as conditional information, wherein the final frames of each generated video serve as inputs for subsequent clip generation.</p><p>We validate our approach using benchmark datasets, including HTDF and Celeb-V, demonstrating results comparable to previous methods that are constrained to limited datasets characterized by frontal, centered faces, static backgrounds, and defined expressions. Furthermore, our method successfully generates dynamic foregrounds and backgrounds, accommodating complex poses, such as profile views or interactions involving devices like smartphones and microphones, yielding realistic and smoothly animated motion, thereby addressing challenges that previous methodologies have struggled to resolve effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Portrait Image Animation. Recent advancements in the domain of portrait image animation have been significantly propelled by innovations in audio-driven techniques. Notable frameworks, such as LipSyncExpert <ref type="bibr" target="#b22">[23]</ref> and SadTalker <ref type="bibr" target="#b45">[46]</ref>, have tackled challenges related to facial synchronization and expression modulation, achieving dynamic lip movements and coherent head motions. Concurrently, DiffTalk <ref type="bibr" target="#b29">[30]</ref> and VividTalk <ref type="bibr" target="#b32">[33]</ref> have integrated latent diffusion models, enhancing output quality while generalizing across diverse identities without the necessity for extensive fine-tuning. Furthermore, studies such as DreamTalk <ref type="bibr" target="#b18">[19]</ref> and EMO <ref type="bibr" target="#b34">[35]</ref> underscore the importance of emotional expressiveness by showcasing the integration of audio cues with facial dynamics. AniPortrait <ref type="bibr" target="#b37">[38]</ref> and VASA-1 <ref type="bibr" target="#b39">[40]</ref> propose methodologies that facilitate the generation of high-fidelity animations, emphasizing temporal consistency along with effective exploitation of static images and audio clips. In addition, recent innovations like LivePortrait <ref type="bibr" target="#b10">[11]</ref> and Loopy <ref type="bibr" target="#b13">[14]</ref> focus on enhancing computational efficiency while ensuring realism and fluid motion. Furthermore, the works of Hallo <ref type="bibr" target="#b38">[39]</ref> and Hallo2 <ref type="bibr" target="#b7">[8]</ref> have made significant progress in extending capabilities to facilitate long-duration video synthesis and integrating adjustable semantic inputs, thereby marking a step towards richer and more controllable content generation. Nevertheless, existing facial animation techniques still encounter limitations in addressing extreme facial poses, accommodating background motion in dynamic environments, and incorporating camera movements dictated by textual prompts. Diffusion-Based Video Generation. Unet-based diffusion model has made notable strides, exemplified by frameworks such as Make-A-Video and MagicVideo <ref type="bibr" target="#b48">[49]</ref>. Specifically, Make-A-Video <ref type="bibr" target="#b31">[32]</ref> capitalizes on pre-existing Textto-Image (T2I) models to enhance training efficiency without necessitating paired text-video data, thereby achieving state-of-the-art results across a variety of qualitative and quantitative metrics. Simultaneously, MagicVideo <ref type="bibr" target="#b48">[49]</ref> employs an innovative 3D U-Net architecture to operate within a low-dimensional latent space, achieving efficient video synthesis while significantly reducing computational requirements. Building upon these foundational principles, AnimateDiff <ref type="bibr" target="#b11">[12]</ref> introduces a motion module that integrates seamlessly with personalized T2I models, allowing for the generation of temporally coherent animations without the need for model-specific adjustments. Additionally, VideoComposer <ref type="bibr" target="#b36">[37]</ref> enhances the controllability of video synthesis by incorporating spatial, temporal, and textual conditions, which facilitates improved inter-frame consistency. The development of diffusion models continues with the advent of DiT-based approaches such as CogVideoX <ref type="bibr" target="#b40">[41]</ref> and Movie Gen <ref type="bibr" target="#b21">[22]</ref>. CogVideoX employs a 3D Variational Autoencoder to improve video fidelity and narrative coherence, whereas Movie Gen establishes a robust foundation for high-quality video generation complemented by advanced editing capabilities. In the present study, we adopt the DiT diffusion formulation to optimize the generalization capabilities of the generated video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>This methodology section systematically outlines the approaches employed in our study. Section 3.1 describes the baseline transformer diffusion network, detailing its architecture and functionality. Section 3.2 focuses on the integra-tion of speech audio conditions via a cross-attention mechanism. Section 3.3 discusses the implementation of the identity reference network, which is crucial for preserving facial identity coherence throughout extended video sequences. Section 3.4 reviews the training and inference procedures used for the transformer diffusion network. Finally, Section 3.5 details the comprehensive strategies for data sourcing and preprocessing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Baseline Transformer Diffusion Network</head><p>Baseline Network. The CogVideoX model <ref type="bibr" target="#b40">[41]</ref> serves as the foundational architecture for our transformer diffusion network, employing a 3D VAE for the compression of video data. In this framework, latent variables are concatenated and reshaped into a sequential format, denoted as z t . Concurrently, the model utilizes the T5 architecture <ref type="bibr" target="#b23">[24]</ref> to encode textual inputs into embeddings, represented as c text . The combined sequences of video latent representations z t and textual embeddings c text are subsequently processed through an expert transformer network. To address discrepancies in feature space between text and video, we implement expert adaptive layer normalization techniques, which facilitate the effective utilization of temporal information and ensure robust alignment between visual and semantic data. Following this integration, a repair mechanism is applied to restore the original latent variable, after which the output is decoded through the 3D causal VAE decoder to reconstruct the video. Furthermore, the incorporation of 3D Rotational Positional Encoding (3D RoPE) <ref type="bibr" target="#b40">[41]</ref> enhances the model's capacity to capture inter-frame relationships across the temporal dimension, thereby establishing long-range dependencies within the video framework. Conditioning in Diffusion Transformer. In addition to the textual prompt c text , we introduce two supplementary conditions: the speech audio condition c audio and the identity appearance condition c id .</p><p>Within diffusion transformers, four primary conditioning mechanisms are identified: in-context conditioning, cross-attention, adaptive layer normalization (adaLN), and adaLN-zero <ref type="bibr" target="#b19">[20]</ref>. Our investigation primarily focuses on cross-attention and adaptive layer normalization (adaLN). Cross-attention enhances the model's focus on conditional information by treating condition embeddings as keys and values, while latent representations serve as queries. Although adaLN is effective in simpler conditioning scenarios, it may not be optimal for more complex conditional embeddings that incorporate richer semantic details, such as sequential speech audio. Relevant comparative analyses will be elaborated upon in the experimental section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Audio-Driven Transformer Diffusion</head><p>Speech Audio Embedding. To extract salient audio features for our proposed model, we utilize the wav2vec frame-Figure <ref type="figure">2</ref>. The overview of the proposed method. Specifically, the method takes a reference image, an audio sequence, and a textual prompt as inputs to generate a video output with temporal consistency and high visual fidelity. We leverage the casual 3D VAE, T5, and Wav2Vec models to process the visual, textual, and audio features, respectively. The Identity Reference Network extracts identity features from the input reference image and textual prompt, enabling controllable animation while preserving the subject's appearance. The audio encoder generates motion information for lip synchronization, while the face encoder extracts facial features to maintain consistency in facial expressions. The 3D Full Attention and Audio-Attention Modules combine identity and motion data within a denoising network, producing high-fidelity, temporally consistent, and controllable animated videos. work developed by Schneider et al. <ref type="bibr" target="#b27">[28]</ref>. The audio representation is defined as c audio . Specifically, we concatenate the audio embeddings generated by the final twelve layers of the wav2vec network, resulting in a comprehensive semantic representation capable of capturing various audio hierarchies. This concatenation emphasizes the significance of phonetic elements, such as pronunciation and prosody, which are crucial as driving signals for character generation.</p><p>To transform the audio embeddings obtained from the pretrained model into frame-specific representations, we apply three successive linear transformation layers, mathematically expressed as: c</p><formula xml:id="formula_0">(f ) audio = L 3 (L 2 (L 1 (c audio )))</formula><p>, where L 1 , L 2 , and L 3 represent the respective linear transformation functions. This systematic approach ensures that the resulting frame-specific representations effectively encapsulate the nuanced audio features essential for the performance of our model. Speech Audio Conditioning.</p><p>We explore three fusion strategies-self-attention, adaptive normalization, and cross-attention-as illustrated in Figure <ref type="figure">3</ref> to integrate audio condition into the DiT-based video generation model. Our experiments show that the cross-attention strategy delivers the best performance in our model. For more details, please refer to Section 4.3.</p><p>Following this, we integrate audio attention layers after each face-attention layer within the denoising network, employing a cross-attention mechanism that facilitates interaction between the latent encodings and the audio embeddings. Specifically, within the DiT block, the mo-tion patches function as keys and values in the crossattention computation with the hidden states z t :</p><formula xml:id="formula_1">z t = CrossAttention(z t , c (f )</formula><p>audio ). This methodology leverages the conditional information from the audio embeddings to enhance the coherence and relevance of the generated outputs, ensuring that the model effectively captures the intricacies of the audio signals that drive character generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Identity Consistent Transformer Diffusion</head><p>Identity Reference Network. Diffusion transformer-based video generation models encounter significant challenges in maintaining facial identity coherence, particularly as the length of the generated video increases. While incorporating speech audio embeddings as conditional features can establish a correspondence between audio speech and facial movements, prolonged generation often leads to rapid degradation of facial identity characteristics.</p><p>To address this issue, we introduce a control condition within the existing diffusion transformer architecture to ensure long-term consistency of facial identity appearance. We explore four strategies (as shown in Figure <ref type="figure">4</ref>) for appearance conditioning: 1) Face attention, where identity features are encoded by the face encoder and combined with a cross-attention module; 2) Face adaptive norm, which integrates features from the face encoder with an adaptive layer normalization technique; 3) Identity reference network, where identity features are captured by a 3D VAE and combined with some transformer layers; and 4) Face attention and Identity reference network, which encodes identity features using both the face encoder and 3D VAE, combining them with self-attention and cross-attention. Our experiments show that the combination with Face attention and Identity reference net achieves the best performance in our model. For further details, please refer to Section 4.3.</p><p>We treat a reference image as a single frame and input it into a causal 3D VAE to obtain latent features, which are then processed through a reference network consisting of 42 transformer layers. Mathematically, if I ref denotes the reference image, the encoder function of the 3D VAE is defined as: z id = E 3D (I ref ), where z id represents the latent features associated with the reference image.</p><p>During the operation of the reference network, we extract vision tokens from the input of the 3D full attention mechanism for each transformer layer, which serve as reference features z id . These features are integrated into corresponding layers of the denoising network to enhance its capability, expressed as: z t,enhanced = SelfAttention(z t , z id ), where z t is the latent representation at time step t. Given that both the reference network and denoising network leverage the same causal 3D VAE with identical weights and comprise the same number of transformer layers (42 layers in our implementation), the visual features generated from both networks maintain semantic and scale consistency. This consistency allows the reference network's features to incorporate the appearance characteristics of facial identity from the reference image while minimizing disruption to the original feature representations of the denoising network, thereby reinforcing the model's capacity to generate coherent and identity-consistent facial animations across longer video sequences. Temporal Motion Frames. To facilitate long video inference, we introduce the last n frames of the previously generated video, referred to as motion frames, as additional conditions. Given a generated video length of L and the corresponding latent representation of l frames, we denote the motion frames as N . The motion frames are processed through the 3D VAE to obtain n frames of latent codes. We apply zero padding to the subsequent (l -n) frames and concatenate them with l frames of Gaussian noise. This concatenated representation is then patchified to yield vision tokens, which are subsequently input into the denoising network. By repeatedly utilizing motion frames, we achieve temporally consistent long video inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training and Inference</head><p>Training. The training process consists of two phases:</p><p>(1) Identity Consistency Phase. In this initial phase, we train the model to generate videos with consistent identity. The parameters of the 3D Variational Autoencoder (VAE) and face image encoder remain fixed, while the parameters of the 3D full attention blocks in both the reference and denoising networks, along with the face attention blocks in the denoising network, are updated during training. The model's input includes a randomly sampled reference image from the training video, a textual prompt, and the face embedding. The textual prompt is generated using MiniCPM <ref type="bibr" target="#b41">[42]</ref>, which describes human appearance, actions, and detailed environmental background. The face embedding is extracted via InsightFace <ref type="bibr" target="#b8">[9]</ref>. With these inputs, the model generates a video comprising 49 frames.</p><p>(2) Audio-Driven Video Generation Phase. In the second phase, we extend the training to include audio-driven video generation. We integrate audio attention modules into each transformer block of the denoising network, while fixing the parameters of other components and updating only those of the audio attention modules. Here, the model's input consists of a reference image, an audio embedding, and a textual prompt, resulting in a sequence of 49 video frames driven by audio. Inference. During inference, the model receives a reference image, a segment of driving audio, a textual prompt, and motion frames as inputs. The model then generates a video that exhibits identity consistency and lip synchronization based on the driving audio. To produce long videos, we utilize the last two frames of the preceding video as motion frames, thereby achieving temporally consistent video generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Dataset</head><p>In this section, we will give a detailed introduction of our data curation, including data sources, filtering strategy and data statistics. Figure <ref type="figure" target="#fig_1">5</ref> shows the data pipeline and the statistical analysis of the final data. Data Sources The training data used in this work is prepared from three distinct sources to ensure diversity and generalization. Specifically, the sources are: (1) HDTF dataset <ref type="bibr" target="#b46">[47]</ref>, which contains 8 hours of raw video footage;</p><p>(2) YouTube data, which consists of 1,200 hours of public raw videos; (3) a large scale movie dataset, which contains film videos of 2,346 hours. Our dataset contains a large scale of human identities and, however, we find that YouTube and movie dataset contains a large amount of noised data. Therefore, we design a data curation pipeline as follows to construct a high-quality and diverse talking dataset, as shown in Figure <ref type="figure" target="#fig_1">5(a)</ref>. Video Filtering. During the data pre-processing phase, we implement a series of meticulous filtering steps to ensure the quality and applicability of the dataset. The workflow includes three stages: extraction of single-speaker, motion filter and post-processing. Firstly, we select video of single-speaker. This stage aims to clean the video content to solve camera shot, background noise, etc, using existing tools <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b20">21]</ref>. After that, we apply several filtering techniques to ensure the quality of head motion, head pose, camera motion, etc <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>. In this stage, we compute all metric scores for each clip, therefore, we can flexibly adjust data screening strategies to satisfy different data requirement of our multiple training stages or strategies. Finally, based on the facial positions detected in previous steps, we crop the videos to a 3:2 aspect ratio to meet the model's input requirements. We then select a random frame from each video and use InsightFace <ref type="bibr" target="#b24">[25]</ref> to encode the face into embeddings, providing essential facial feature information for the model. Additionally, we extract the audio from the videos and encode it into embeddings using Wav2Vec2 model <ref type="bibr" target="#b0">[1]</ref>, facilitating the incorporation of audio conditions during model training. Data Statistics. Following the data cleaning and filtering processes, we conducted a detailed analysis of the final dataset to assess its quality and suitability for the intended modeling tasks. Finally, our training data contains about 134 hours of videos, including 6 hours of high-quality data from HDTF dataset, 72 hours of YouTube videos, and 56 hours of movie videos. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setups</head><p>Implementation. We initialize the identity reference and denoising networks with weights derived from CogVideoX-5B-I2V <ref type="bibr" target="#b40">[41]</ref>. During both training phases, we employ the vprediction diffusion loss <ref type="bibr" target="#b26">[27]</ref> for optimization. Each training phase comprises 20,000 steps, utilizing 64 NVIDIA H100 GPUs. The batch size per GPU is set to 1, with a learning rate of 480 × 720 pixels. To enhance video generation variability, the reference image, guidance audio and textual prompt are dropped with a probability of 0.05 during training. Evaluation Metrics. We employed a range of evaluation metrics for generated videos across benchmark datasets, including HDTF <ref type="bibr" target="#b46">[47]</ref> and Celeb-V <ref type="bibr" target="#b49">[50]</ref>. These metrics comprise Fréchet Inception Distance (FID) <ref type="bibr" target="#b28">[29]</ref>, Fréchet Video Distance (FVD) <ref type="bibr" target="#b35">[36]</ref>, Synchronization-C (Sync-C) <ref type="bibr" target="#b5">[6]</ref>, Synchronization-D (Sync-D) <ref type="bibr" target="#b5">[6]</ref>, and E-FID <ref type="bibr" target="#b34">[35]</ref>. FID and FVD quantify the similarity between generated images and real data, while Sync-C and Sync-D assess lip synchronization accuracy. E-FID evaluates the image quality based on features extracted from the Inception network. Besides, we introduced VBench [13] metrics to enhance evaluation, focusing on dynamic degree and subject consistency. Dynamic degree is measured using RAFT <ref type="bibr" target="#b33">[34]</ref> to quantify the extent of motion in generated videos, providing a comprehensive assessment of temporal quality. Subject consistency is measured through DINO <ref type="bibr" target="#b4">[5]</ref> feature similarity, ensuring uniformity of a subject's appearance across frames. Baseline Approaches. We considered several representative audio-driven talking face generation methods for comparison, all of which have publicly available source code or implementations. These methods include SadTalker <ref type="bibr" target="#b44">[45]</ref>, DreamTalk <ref type="bibr" target="#b18">[19]</ref>, AniPortrait <ref type="bibr" target="#b37">[38]</ref>, and Hallo <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b38">39]</ref>. The selected approaches encompass both GANs and diffusion models, as well as techniques utilizing intermediate facial representations alongside end-to-end frameworks. This diversity in methodologies allows for a comprehensive evaluation of the effectiveness of our proposed approach compared to existing solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with State-of-the-art</head><p>Comparison on HDTF and Celeb-V Dataset. As shown in Table <ref type="table" target="#tab_0">1</ref> and 2, our method achieves best results on FID, FVD on both datasets. Although our approach shows some disparity compared to the state-of-the-art in lip synchronization, it still demonstrates promising results as illustrated in Figure <ref type="figure" target="#fig_3">6</ref>. This is because, to generate animated portraits from different perspectives, our training data primarily consists of talking videos with significant head and body movements, as well as diverse dynamic scenes, unlike static scenes with minimal motion. While this may lead to some performance degradation on lip synchronization, it better reflects realistic application scenarios. Table 3. Comparison with other methods on our proposed wild dataset. the performance of the general talking portrait video generation, we carefully collect 34 representative cases for evaluation. This dataset consists of portrait images with various head proportions, head poses, static and dynamic scenes and complex headwears and clothing. To achieve comprehensive assessment, we evaluate the performance on lip synchronization (Sync-C and Sync-D), motion strength (subject and background dynamic degree) and video quality (subject and background FVD). As shown in Table <ref type="table">3</ref>, our method generates videos with largest head and background dynamic degree (13.286 and 4.481) while keeping lip synchronization of highest accuracy.</p><p>Figure <ref type="figure" target="#fig_4">7</ref> provides a qualitative comparison of different portrait methods on a "wild" dataset. The results reveal that other methods struggle to animate side-face portrait images, often resulting in static poses or facial distortions. Additionally, these methods tend to focus solely on animating the face, overlooking interactions with other objects in the foreground-such as the dog next to the elderly, or the dy-namic movement of the background-like the ostrich behind the girl. In contrast, as shown in Figure <ref type="figure" target="#fig_5">8</ref> our method produces realistic portraits with diverse orientations and complex foreground and background scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study and Discussion</head><p>Audio Conditioning. Table <ref type="table" target="#tab_3">4</ref> and Figure <ref type="figure" target="#fig_6">9</ref> illustrate the effects of various strategies for incorporating audio conditioning. The results demonstrate that using cross-attention to integrate audio improves lip synchronization by enhancing the local alignment between visual and audio features, particularly around the lips. This is evident from the improvements in Sync-C and Sync-D, and it also contributes to a degree of enhancement in video quality. Identity Reference Network. Table <ref type="table">5</ref> and Figure <ref type="figure" target="#fig_7">10</ref> evaluate different identity conditioning strategies. The results indicate that without an identity condition, the model fails to preserve the portrait appearance. When using face embedding alone, the model introduces blur and distortion, as  it focuses solely on facial features and disrupts the global visual context. To address this, we introduce an identity reference network to preserve global features while making facial motion more controllable through identity-based facial embeddings. Thus, the proposed method achieves a lower FID of 23.458 and FVD of 242.602, while maintaining lip synchronization. Temporal Motion Frames. Table <ref type="table">6</ref> presents an analysis of varying temporal motion frames. One motion frame achieves the highest Sync-C score (6.889) and the lowest Sync-D score (8.695), indicating substantial lip synchronization. CFG Scales for Diffusion Model. Table <ref type="table">7</ref> provides a quantitative analysis of video generations using various CFG scales for audio, text, and reference images. A comparison between the second and fourth rows demonstrates that increasing the audio CFG scale enhances the model's ability to synchronize lip movements. The text CFG scale significantly influences the video's dynamism, as indicated in the first three rows, where both the subject's and the background's dynamics increase with higher text CFG scales. Conversely, the reference image CFG scale primarily governs the subject's appearance; higher values improve subject consistency, as illustrated by the second and fifth rows. Among the tested configurations, setting λ a = 3.5, λ t = 3.5, and λ i = 1.0 yields a balanced performance. This interplay between visual fidelity and dynamics underscores the effectiveness of CFG configurations in generating realistic portrait animations.</p><p>Limitations and Future Works. Despite the advancements in portrait image animation techniques presented in this study, several limitations warrant acknowledgment. While   Table 6. Ablation on the number of motion frames.</p><p>the proposed methods improve identity preservation and lip synchronization, the model's ability to realistically represent intricate facial expressions in dynamic environments still requires refinement, especially under varying illumination conditions. Future work will focus on enhancing the model's robustness to diverse perspectives and interactions, incorporating more comprehensive datasets that include varied backgrounds and facial accessories. Furthermore, investigating the integration of real-time feedback mechanisms could significantly enhance the interactivity and realism of portrait animations, paving the way for broader applications in live media and augmented reality. Safety Considerations. The advancement of portrait image animation technologies, particularly those driven by audio inputs, presents several social risks, most notably concerning the ethical implications associated with the creation of highly realistic portraits that may be misused for deepfake purposes. To address these concerns, it is essential to develop comprehensive ethical guidelines and responsible use practices. Moreover, issues surrounding privacy and consent are prominent when utilizing individuals' images and voices. It is imperative to establish transparent data usage policies, ensuring that individuals provide informed consent and that their privacy rights are fully protected. By acknowledging these risks and implementing appropriate mitigation strategies, this research aims to promote the responsible and ethical development of portrait image animation technology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Generation Controllability</head><p>Textual Prompt for Subject Animation. To evaluate whether textual conditional controllability is effectively preserved, we conducted a series of experiments comparing the performance of our method to that of the baseline model, CogVideoX <ref type="bibr" target="#b40">[41]</ref>, using same text prompts. As shown in Figure <ref type="figure">11</ref>, the results shows that our model maintains its ability for textual control, and effectively captures the interaction between different subjects as dictated by the textual prompts.</p><p>Textual Prompt for Foreground and Background Animation. We also explore model's ability to follow the foreground and background textual prompt. As illustrated in Figure <ref type="figure">12</ref>, our method animates the foreground and background subjects naturally, such as the ocean waves and flickering candlelight. The results demonstrates the model's ability to control foreground, and background with the textual caption, which is maintained even after introducing the audio condition.  <ref type="table">7</ref>. Quantitative study of audio, text and image CFG scales on our proposed wild dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours CogVideoX</head><p>A young girl with blonde hair embraces her beautiful brown horse in a tender moment. An elderly man enjoys the company of vibrant birds perched on his arms in nature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reference Prompt</head><p>A young girl with a charming smile wearing a straw hat cuddles her white rabbit.</p><p>Figure <ref type="figure">11</ref>. Condition on interacting with subjects. Our method achieves alignment comparable to that of CogVideX, maintaining the controllability of interactive subjects even after introducing the audio condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours CogVideoX</head><p>A cheerful man wearing a floral shirt and sunglasses enjoys the beautiful beachside scenery, with waves gently lapping against the shore.</p><p>A radiant young woman smiles brightly, surrounded by a vibrant field of blooming sunflowers, their golden petals swaying gently in the breeze.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ref. Image Prompt</head><p>A young boy glows with happiness as he prepares to blow out candles on his birthday cake, their flames dancing and flickering gently in the soft glow of the room. A stylish woman in a floral dress and sunglasses enjoys a serene moment on a sunny ocean, as gentle waves ripple against the shore, sparkling under the sunlight.</p><p>Figure <ref type="figure">12</ref>. Textual condition on foreground and background. Our method achieves alignment comparable to that of CogVideX, maintaining the controllability of foreground and background after incorporating the audio condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper introduces advancements in portrait image animation utilizing the enhanced capabilities of a transformer-based diffusion model. By integrating audio conditioning through cross-attention mechanisms, our approach effectively captures the intricate relationship between audio signals and facial expressions, achieving sub-stantial lip synchronization. To preserve facial identity across video sequences, we incorporate an identity reference network. Additionally, we utilize motion frames to enable the model to generate long-duration video extrapolations. Our model produces animated portraits from diverse perspectives, seamlessly blending dynamic foreground and background elements while maintaining temporal consistency and high fidelity.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .Figure 4 .</head><label>34</label><figDesc>Figure 3. Different strategies of audio conditioning. (a) self-attention; (b) adaptive norm; (c) cross-attention.</figDesc><graphic coords="4,50.11,72.27,321.75,136.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Illustration of the dataset, including the flow of data processing, data distribution across different metric, and the visualization of some representative portrait images for inference.</figDesc><graphic coords="6,50.11,72.00,494.97,127.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><figDesc>Figure 5(b) also shows other statistics, such as Lip Sync score (Sync-C and Sync-D), face rotation, face ratio (the ratio of face height to video height).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Qualitative comparison on the HTDF (left) and CelebV (right) data-set.</figDesc><graphic coords="7,50.11,165.04,494.82,280.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Complex facial identity with dynamic accessories subjects and different pose orientation.</figDesc><graphic coords="8,45.17,72.00,499.94,201.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Complex scenes with dynamic foreground or background and various head poses.</figDesc><graphic coords="8,50.11,304.65,494.99,181.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Qualitative comparison of different strategies for audio conditioning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Qualitative comparison of different strategies (as in Table 5) for identity conditioning. (a) No identity condition; (b) Face attention; (c) Face adaptive norm; (d) Identity reference network; (e) Face attention and Identity reference network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><figDesc>t ↓ λ a = 3.5 λ t = 1.0 λ i = 1Base λ a = 3.5 λ t = 3.5 λ i = 1λ t ↑ λ a = 3.5 λ t = 6.0 λ i = 1λ a ↑ λ a = 6.0 λ t = 3.5 λ i = 1λ i ↑ λ a = 3.5 λ t = 3.5 λ i = 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="2,50.11,72.00,494.98,230.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison on Wild Dataset. To effectively demonstrateComparison with the other methods on HDTF dataset.</figDesc><table><row><cell></cell><cell>FID↓</cell><cell>FVD↓</cell><cell cols="2">Sync-C↑ Sync-D↓</cell><cell></cell><cell>FID↓</cell><cell>FVD↓</cell><cell cols="3">Sync-C↑ Sync-D↓ E-FID↓</cell></row><row><cell cols="3">SadTalker [45] 22.340 203.860</cell><cell>7.885</cell><cell>7.545</cell><cell>SadTalker [45]</cell><cell cols="2">50.015 471.163</cell><cell>6.922</cell><cell>7.921</cell><cell>95.194</cell></row><row><cell cols="3">DreamTalk [19] 78.147 790.660</cell><cell>6.376</cell><cell>8.364</cell><cell cols="3">DreamTalk [19] 109.011 988.539</cell><cell>5.709</cell><cell>8.743</cell><cell>153.450</cell></row><row><cell cols="3">AniPortrait [38] 26.561 234.666</cell><cell>4.015</cell><cell>10.548</cell><cell cols="3">AniPortrait [38] 46.915 477.179</cell><cell>2.853</cell><cell>11.709</cell><cell>88.986</cell></row><row><cell>Hallo [39]</cell><cell cols="2">20.545 173.497</cell><cell>7.750</cell><cell>7.659</cell><cell>Hallo [39]</cell><cell cols="2">44.578 377.117</cell><cell>7.191</cell><cell>7.984</cell><cell>78.495</cell></row><row><cell>Ours</cell><cell cols="2">20.359 160.838</cell><cell>7.252</cell><cell>8.106</cell><cell>Ours</cell><cell cols="2">43.271 355.272</cell><cell>6.527</cell><cell>9.113</cell><cell>71.210</cell></row><row><cell>Real video</cell><cell>-</cell><cell>-</cell><cell>8.700</cell><cell>6.597</cell><cell>Real video</cell><cell>-</cell><cell>-</cell><cell>7.372</cell><cell>7.518</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison with other methods on Celeb-V dataset.</figDesc><table><row><cell></cell><cell cols="2">Sync-C↑ Sync-D↓</cell><cell>Subject Dynamic↑</cell><cell>Background Dynamic↑</cell><cell>Subject FVD↓</cell><cell>Background FVD↓</cell></row><row><cell>SadTalker [45]</cell><cell>3.845</cell><cell>10.378</cell><cell>2.953</cell><cell>0.220</cell><cell>470.377</cell><cell>313.758</cell></row><row><cell>DreamTalk [19]</cell><cell>4.498</cell><cell>11.005</cell><cell>6.958</cell><cell>1.806</cell><cell>835.480</cell><cell>744.177</cell></row><row><cell>AniPortrait [38]</cell><cell>1.685</cell><cell>12.025</cell><cell>3.351</cell><cell>1.769</cell><cell>473.173</cell><cell>302.716</cell></row><row><cell>Hallo [39]</cell><cell>4.654</cell><cell>10.202</cell><cell>5.268</cell><cell>1.272</cell><cell>394.627</cell><cell>291.052</cell></row><row><cell>Ours</cell><cell>6.154</cell><cell>8.574</cell><cell>13.286</cell><cell>4.481</cell><cell>359.493</cell><cell>248.283</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Comparison on the different strategy of audio conditioning.</figDesc><table><row><cell>Audio Injection Method</cell><cell></cell><cell>FID↓</cell><cell cols="2">FVD↓</cell><cell></cell><cell>Sync-C↑</cell><cell>Sync-D↓</cell></row><row><cell>adaLN</cell><cell cols="2">24.159</cell><cell cols="2">264.331</cell><cell></cell><cell>1.374</cell><cell>13.524</cell></row><row><cell>adaLN-zero</cell><cell cols="2">24.029</cell><cell cols="2">276.403</cell><cell></cell><cell>1.398</cell><cell>13.553</cell></row><row><cell>Self Attn.</cell><cell cols="2">24.748</cell><cell cols="2">270.101</cell><cell></cell><cell>1.345</cell><cell>13.456</cell></row><row><cell>Cross Attn. (Ours)</cell><cell cols="2">23.458</cell><cell cols="2">242.602</cell><cell></cell><cell>4.601</cell><cell>10.416</cell></row><row><cell>Identity Injection Method</cell><cell></cell><cell>FID↓</cell><cell>FVD↓</cell><cell cols="2">Sync-C↑</cell><cell>Sync-D↓</cell><cell>Subject Consistency↑</cell></row><row><cell>(a) No identity condition</cell><cell></cell><cell>32.304</cell><cell>371.820</cell><cell>3.183</cell><cell></cell><cell>11.732</cell><cell>0.977</cell></row><row><cell>(b) Face attention</cell><cell></cell><cell>57.541</cell><cell>740.536</cell><cell>4.042</cell><cell></cell><cell>10.682</cell><cell>0.974</cell></row><row><cell>(c) Face adaptive norm</cell><cell></cell><cell>150.720</cell><cell>1587.395</cell><cell>3.822</cell><cell></cell><cell>12.324</cell><cell>0.904</cell></row><row><cell>(d) Identity reference network</cell><cell></cell><cell>28.789</cell><cell>291.863</cell><cell>4.553</cell><cell></cell><cell>10.317</cell><cell>0.984</cell></row><row><cell cols="2">(e) Face attention and Identity reference network</cell><cell>23.458</cell><cell>242.602</cell><cell>4.601</cell><cell></cell><cell>10.416</cell><cell>0.988</cell></row><row><cell cols="7">Table 5. Comparison of different identity injection method. "No</cell></row><row><cell cols="7">identity condition" refers to the absence of any conditioning re-</cell></row><row><cell cols="7">lated to identity; "Face attention" and "Face adaptive norm"</cell></row><row><cell cols="7">involve incorporating face embeddings using self-attention and</cell></row><row><cell cols="7">adaptive layer normalization, respectively. "Identity reference net-</cell></row><row><cell cols="7">work" refers to the introduction of identity features using a refer-</cell></row><row><cell>ence network.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Motion Frame Number</cell><cell></cell><cell>FID↓</cell><cell cols="2">FVD↓</cell><cell cols="2">Sync-C↑</cell><cell>Sync-D↓</cell></row><row><cell>n = 1</cell><cell cols="2">24.040</cell><cell cols="2">242.708</cell><cell></cell><cell>6.889</cell><cell>8.695</cell></row><row><cell>n = 2</cell><cell cols="2">23.458</cell><cell cols="2">242.602</cell><cell></cell><cell>4.601</cell><cell>10.416</cell></row><row><cell>n = 4</cell><cell cols="2">24.459</cell><cell cols="2">269.904</cell><cell></cell><cell>5.109</cell><cell>10.489</cell></row><row><cell>n = 8</cell><cell cols="2">27.303</cell><cell cols="2">265.396</cell><cell></cell><cell>5.114</cell><cell>10.464</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">wav2vec 2.0: A framework for self-supervised learning of speech representations</title>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="12449" to="12460" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Vidu: a highly consistent, dynamic and skilled text-to-video generator with diffusion models</title>
		<author>
			<persName><forename type="first">Fan</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chendong</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guande</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiwen</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shilong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaole</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.04233</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Face recognition based on fitting a 3d morphable model</title>
		<author>
			<persName><forename type="first">Volker</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1063" to="1074" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">pyannote.audio 2.1 speaker diarization pipeline: principle, benchmark, and recipe</title>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Bredin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH 2023</title>
		<meeting>INTERSPEECH 2023</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<biblScope unit="page" from="2021" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Out of time: automated lip sync in the wild</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Multi-view Lip-reading, ACCV</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Vlogger: Multimodal diffusion for embodied avatar synthesis</title>
		<author>
			<persName><forename type="first">Enric</forename><surname>Corona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Gabriel Bazavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thiemo</forename><surname>Alldieck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.08764</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Hallo2: Long-duration and high-resolution audio-driven portrait image animation</title>
		<author>
			<persName><forename type="first">Jiahao</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanlin</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaihui</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.07718</idno>
		<imprint>
			<date type="published" when="2006">2024. 1, 3, 6</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Insightface: An open-source 2d and 3d deep face analysis toolkit</title>
		<author>
			<persName><surname>Deepinsight</surname></persName>
		</author>
		<ptr target="https://github.com/deepinsight/insightface,2024.5" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">High-fidelity and freely controllable talking head video generation</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinglu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="5609" to="5619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Jianzhu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingyun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhizhou</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.03168</idno>
		<title level="m">Liveportrait: Efficient portrait animation with stitching and retargeting control</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Animatediff: Animate your personalized text-to-image diffusion models without specific tuning</title>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anyi</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaohui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.04725</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">VBench: Comprehensive benchmark suite for video generative models</title>
		<author>
			<persName><forename type="first">Ziqi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashuo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyang</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianxing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyang</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nattapol</forename><surname>Chanpaisit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaohui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Jianwen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaojie</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanbo</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><surname>Loopy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.02634</idno>
		<title level="m">Taming audio-driven portrait avatar with long-term motion dependency</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cotracker3: Simpler and better point tracking by pseudolabelling real videos</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Karaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iurii</forename><surname>Makarov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.11831</idno>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cotracker: It is better to track together</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Karaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ignacio</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Anitalker: Animate vivid and diverse talking faces through identity-decoupled facial motion encoding</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feilong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenpeng</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.03121</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiling</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chujie</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoxi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengqing</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanchi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.17177</idno>
		<title level="m">A review on background, technology, limitations, and opportunities of large vision models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Dreamtalk: When expressive talking head generation meets diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Yifeng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhidong</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.09767</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Scalable diffusion models with transformers</title>
		<author>
			<persName><forename type="first">William</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="4195" to="4205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Powerset multi-class cross entropy loss for neural speaker diarization</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Plaquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Bredin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH 2023</title>
		<meeting>INTERSPEECH 2023</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Movie gen: A cast of media foundation models</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Zohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andros</forename><surname>Tjandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Animesh</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ann</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chih-Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ching-Yao</forename><surname>Chuang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.13720</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A lip sync expert is all you need for speech to lip generation in the wild</title>
		<author>
			<persName><forename type="first">Rudrabha</forename><surname>Kr Prajwal</surname></persName>
		</author>
		<author>
			<persName><surname>Mukhopadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vinay</surname></persName>
		</author>
		<author>
			<persName><surname>Namboodiri</surname></persName>
		</author>
		<author>
			<persName><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM international conference on multimedia (ACM MM)</title>
		<meeting>the 28th ACM international conference on multimedia (ACM MM)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="484" to="492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Facial geometric detail recovery via implicit representation</title>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Xingyu Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baris</forename><surname>Lattas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiankang</forename><surname>Gecer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaokang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2023 IEEE 17th International Conference on Automatic Face and Gesture Recognition (FG)</title>
		<imprint>
			<biblScope unit="page" from="2023" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pirenderer: Controllable portrait image generation via semantic neural rendering</title>
		<author>
			<persName><forename type="first">Ge</forename><surname>Yurui Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="13759" to="13768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Progressive distillation for fast sampling of diffusion models</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.00512</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">wav2vec: Unsupervised pre-training for speech recognition</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.05862</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Seitzer</surname></persName>
		</author>
		<idno>Version 0.3.0. 6</idno>
		<ptr target="https://github.com/mseitzer/pytorch-fid" />
		<title level="m">pytorch-fid: FID Score for PyTorch</title>
		<imprint>
			<date type="published" when="2020-08">August 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Difftalk: Crafting diffusion models for generalized audio-driven portraits animation</title>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenliang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zibin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanhua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1982">1982-1991, 2023. 2</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">First order motion model for image animation</title>
		<author>
			<persName><forename type="first">Aliaksandr</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stéphane</forename><surname>Lathuilière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Make-a-video: Text-to-video generation without text-video data</title>
		<author>
			<persName><forename type="first">Uriel</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiyuan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harry</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oron</forename><surname>Ashual</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oran</forename><surname>Gafni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.14792</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Vividtalk: One-shot audio-driven talking head generation based on 3d hybrid prior</title>
		<author>
			<persName><forename type="first">Xusen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinya</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kangneng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daiheng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liefeng</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xun</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.01841</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Raft: Recurrent all-pairs field transforms for optical flow</title>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Teed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">August 23-28, 2020. 2020</date>
			<biblScope unit="page" from="402" to="419" />
		</imprint>
	</monogr>
	<note>Proceedings, Part II 16</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Emo: Emote portrait alive-generating expressive portrait videos with audio2video diffusion model under weak conditions</title>
		<author>
			<persName><forename type="first">Linrui</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liefeng</forename><surname>Bo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.17485</idno>
		<imprint>
			<date type="published" when="2006">2024. 1, 3, 6</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karol</forename><surname>Sjoerd Van Steenkiste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raphael</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Marinier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01717</idno>
		<title level="m">Towards accurate generative models of video: A new metric &amp; challenges</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Compositional video synthesis with motion controllability</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hangjie</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dayou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiuniu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><surname>Videocomposer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Aniportrait: Audio-driven synthesis of photorealistic portrait animation</title>
		<author>
			<persName><forename type="first">Huawei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zejun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhisheng</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.17694</idno>
		<imprint>
			<date type="published" when="2007">2024. 1, 3, 6, 7</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Hallo: Hierarchical audio-driven visual synthesis for portrait image animation</title>
		<author>
			<persName><forename type="first">Mingwang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingkun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanlin</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyu</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2024. 1, 3, 6, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Vasa-1: Lifelike audio-driven talking faces generated in real time</title>
		<author>
			<persName><forename type="first">Sicheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guojun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Xiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaolong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.10667</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Cogvideox: Text-to-video diffusion models with an expert transformer</title>
		<author>
			<persName><forename type="first">Zhuoyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayan</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wendi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiazheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyi</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanyu</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.06072</idno>
		<imprint>
			<date type="published" when="2009">2024. 2, 3, 6, 9</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbo</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongji</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianchi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihui</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.01800</idno>
		<title level="m">Minicpm-v: A gpt-4v level mllm on your phone</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Fast bi-layer neural synthesis of oneshot realistic head avatars</title>
		<author>
			<persName><forename type="first">Egor</forename><surname>Zakharov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksei</forename><surname>Ivakhnenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aliaksandra</forename><surname>Shysheya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">August 23-28, 2020. 2020</date>
			<biblScope unit="page" from="524" to="540" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XII 16</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Metaportrait: Identity-preserving talking head generation with fast personalized adaptation</title>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiang-Tao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="22096" to="22105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning realistic 3d motion coefficients for stylized audiodriven single image talking face animation</title>
		<author>
			<persName><forename type="first">Wenxuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Sadtalker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning realistic 3d motion coefficients for stylized audiodriven single image talking face animation</title>
		<author>
			<persName><forename type="first">Wenxuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Sadtalker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="8652" to="8661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Flow-guided one-shot talking face generation with a high-resolution audio-visual dataset</title>
		<author>
			<persName><forename type="first">Zhimeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lincheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changjie</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Tora: Trajectory-oriented diffusion transformer for video generation</title>
		<author>
			<persName><forename type="first">Zhenghao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchao</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhi</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.21705</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Magicvideo: Efficient video generation with latent diffusion models</title>
		<author>
			<persName><forename type="first">Daquan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weimin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanshu</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhe</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.11018</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">CelebV-HQ: A large-scale video facial attributes dataset</title>
		<author>
			<persName><forename type="first">Wayne</forename><surname>Hao Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liming</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siwei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Champ: Controllable and consistent human image animation with 3d parametric guidance</title>
		<author>
			<persName><forename type="first">Shenhao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junming</forename><surname>Leo Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuozhuo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinghui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyu</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
