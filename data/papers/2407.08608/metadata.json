{
  "arxivId": "2407.08608",
  "title": "FlashAttention-3: Fast and Accurate Attention with Asynchrony and\n  Low-precision",
  "authors": "Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, Tri Dao",
  "abstract": "Attention, as a core layer of the ubiquitous Transformer architecture, is the\nbottleneck for large language models and long-context applications.\nFlashAttention elaborated an approach to speed up attention on GPUs through\nminimizing memory reads/writes. However, it has yet to take advantage of new\ncapabilities present in recent hardware, with FlashAttention-2 achieving only\n35% utilization on the H100 GPU. We develop three main techniques to speed up\nattention on Hopper GPUs: exploiting asynchrony of the Tensor Cores and TMA to\n(1) overlap overall computation and data movement via warp-specialization and\n(2) interleave block-wise matmul and softmax operations, and (3) block\nquantization and incoherent processing that leverages hardware support for FP8\nlow-precision. We demonstrate that our method, FlashAttention-3, achieves\nspeedup on H100 GPUs by 1.5-2.0$\\times$ with FP16 reaching up to 740 TFLOPs/s\n(75% utilization), and with FP8 reaching close to 1.2 PFLOPs/s. We validate\nthat FP8 FlashAttention-3 achieves 2.6$\\times$ lower numerical error than a\nbaseline FP8 attention.",
  "url": "https://arxiv.org/abs/2407.08608",
  "issue_number": 343,
  "issue_url": "https://github.com/dmarx/papers-feed/issues/343",
  "created_at": "2025-01-04T14:49:48.246613",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 39,
  "last_read": "2025-01-04T15:02:45.957021",
  "last_visited": "2024-12-28T06:10:45.698Z",
  "main_tex_file": null,
  "published_date": "2024-07-11T15:44:48Z",
  "arxiv_tags": [
    "cs.LG",
    "cs.AI"
  ]
}