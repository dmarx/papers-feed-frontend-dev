- Decision to use a lightweight vanilla Vision Transformer (ViT) as the image encoder
- Choice of EfficientTAMs architecture for video object segmentation
- Implementation of an efficient memory cross-attention mechanism
- Selection of SA-1B and SA-V datasets for training
- Evaluation benchmarks chosen for performance comparison
- Decision to prioritize mobile device efficiency in model design
- Trade-off considerations between model size and segmentation quality
- Adoption of a non-hierarchical architecture over hierarchical models
- Strategy for reducing computational complexity in memory module
- Decision to benchmark performance on specific mobile devices (e.g., iPhone 15 Pro Max)
- Assumptions regarding the locality of memory spatial tokens
- Choice of semi-supervised and promptable video segmentation tasks for evaluation
- Decision to compare performance against existing models like SAM 2 and EfficientSAM
- Approach to handling memory tokens and their representation in the model
- Decision to focus on real-world application scenarios for model deployment
- Assumptions about the scalability of the proposed model for various tasks