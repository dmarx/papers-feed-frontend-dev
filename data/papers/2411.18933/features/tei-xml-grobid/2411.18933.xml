<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Track Anything</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-11-28">28 Nov 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Yunyang</forename><surname>Xiong</surname></persName>
							<email>yunyang@meta.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chong</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaoyu</forename><surname>Xiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lemeng</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zechun</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Saksham</forename><surname>Suri</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Balakrishnan</forename><surname>Varadarajan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ramya</forename><surname>Akula</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Forrest</forename><surname>Iandola</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Raghuraman</forename><surname>Krishnamoorthi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bilge</forename><surname>Soran</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vikas</forename><surname>Chandra</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Meta</forename><surname>Ai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Track Anything</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-11-28">28 Nov 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">38D4B825798F7C561DC537399D0C81F7</idno>
					<idno type="arXiv">arXiv:2411.18933v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Segment Anything Model 2 (SAM 2) has emerged as a powerful tool for video object segmentation and tracking anything. Key components of SAM 2 that drive the impressive video object segmentation performance include a large multistage image encoder for frame feature extraction and a memory mechanism that stores memory contexts from past frames to help current frame segmentation. The high computation complexity of multistage image encoder and memory module has limited its applications in real-world tasks, e.g., video object segmentation on mobile devices. To address this limitation, we propose EfficientTAMs, lightweight track anything models that produce high-quality results with low latency and model size. Our idea is based on revisiting the plain, nonhierarchical Vision Transformer (ViT) as an image encoder for video object segmentation, and introducing an efficient memory module, which reduces the complexity for both frame feature extraction and memory computation for current frame segmentation. We take vanilla lightweight ViTs and efficient memory module to build EfficientTAMs, and train the models on SA-1B and SA-V datasets for video object segmentation and track anything tasks. We evaluate on multiple video segmentation benchmarks including semisupervised VOS and promptable video segmentation, and find that our proposed EfficientTAM with vanilla ViT perform comparably to SAM 2 model (HieraB+SAM 2) with ∼2x speedup on A100 and ∼2.4x parameter reduction. On segment anything image tasks, our EfficientTAMs also perform favorably over original SAM with ∼20x speedup on A100 and ∼20x parameter reduction. On mobile devices such as iPhone 15 Pro Max, our EfficientTAMs can run at ∼10 FPS for performing video object segmentation with reasonable quality, highlighting the capability of small models for on-device video object segmentation applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Segment Anything Model 2 (SAM 2) <ref type="bibr" target="#b57">(Ravi et al., 2024</ref>) is a foundational model for unified image and video object segmentation, achieving state-of-the-art performance in various segmentation tasks such as zero-shot image segmentation <ref type="bibr" target="#b35">(Kirillov et al., 2023;</ref><ref type="bibr">Chen et al., 2023a;</ref><ref type="bibr" target="#b18">Deng et al., 2023;</ref><ref type="bibr">Chen et al., 2023b)</ref>, semi-supervised video object segmentation <ref type="bibr" target="#b53">(Pont-Tuset et al., 2017;</ref><ref type="bibr" target="#b79">Xu et al., 2018;</ref><ref type="bibr" target="#b49">Oh et al., 2019;</ref><ref type="bibr" target="#b1">Bhat et al., 2020;</ref><ref type="bibr" target="#b58">Robinson et al., 2020;</ref><ref type="bibr">Li et al., 2022b;</ref><ref type="bibr" target="#b81">Yang and Yang, 2022;</ref><ref type="bibr" target="#b8">Cheng and Schwing, 2022;</ref><ref type="bibr">Zhang et al., 2023b;</ref><ref type="bibr" target="#b69">Wang et al., 2023;</ref><ref type="bibr" target="#b74">Wu et al., 2023;</ref><ref type="bibr">Cheng et al., 2024;</ref><ref type="bibr" target="#b82">Yang et al., 2024)</ref>, interactive video segmentation <ref type="bibr" target="#b3">(Caelles et al., 2018;</ref><ref type="bibr" target="#b30">Heo et al., 2020;</ref><ref type="bibr">Cheng et al., 2021a;</ref><ref type="bibr" target="#b31">Homayounfar et al., 2021;</ref><ref type="bibr" target="#b14">Yang et al., 2023;</ref><ref type="bibr">Cheng et al., 2023b;</ref><ref type="bibr" target="#b56">Rajič et al., 2023;</ref><ref type="bibr">Cheng et al., 2024;</ref><ref type="bibr" target="#b17">Delatolas et al., 2024)</ref>, and other real-world applications <ref type="bibr">(Zhang et al., 2024b;</ref><ref type="bibr">Xiong et al., 2024a;</ref><ref type="bibr" target="#b60">Shen et al., 2024;</ref><ref type="bibr">Zhang et al., 2024a;</ref><ref type="bibr" target="#b20">Ding et al., 2024;</ref><ref type="bibr" target="#b55">Qiu et al., 2024;</ref><ref type="bibr" target="#b63">Tang et al., 2024;</ref><ref type="bibr" target="#b94">Zhou et al., 2024)</ref>. SAM 2 uses a multistage image encoder to extract hierarchical frame features and introduces a memory module to cross-attend to both current frame features and stored memories from observed frames for consistent object segmentation across frames and interactive tracking in videos. Despite these advantages, SAM 2 is not efficient for mobile deployment, particularly because the large image encoder (e.g., HieraB+) and memory module are expensive. The default image encoder of SAM 2, HieraB+ <ref type="bibr" target="#b59">(Ryali et al., 2023)</ref>, is parameter inefficient, e.g., ∼80M parameters. While SAM 2 provides a tiny version, it has a running time of 43.8 FPS comparable to 47.2 FPS of the default SAM 2 model, due to the hierarchical image encoder. Additionally, that the memory tokens (e.g., a concatenation of spatial memory tokens and object pointer tokens) are long, e.g., ∼30K, which hurts the efficiency of the memory module with cross-attention.</p><p>In this paper, we revisit plain, nonhierarchical image encoder for video object segmentation and tracking anything. We propose using a lightweight vanilla ViT image encoder (e.g., ViT-Tiny/-Small <ref type="bibr" target="#b67">(Touvron et al., 2021)</ref>) as EfficientSAMs <ref type="bibr">(Xiong et al., 2024b)</ref> did to reduce the complexity of SAM 2 while maintaining decent performance. Further, we propose an efficient cross-attention method for accelerating the memory module. This is achieved by leveraging the underlying structure of memory spatial tokens. We observed that the memory spatial tokens have strong locality and a coarser representation of memory spatial tokens can be a good proxy for performing cross-attention. We show that this yields a good alternative to the original memory module.</p><p>To evaluate our method, we conduct extensive experiments across video and image segmentation benchmarks, including MOSE, DAVIS, LVOS, and SA-V for video segmentation, and SA-23 for image segmentation. Our EfficientTAM outperforms strong semi-supervised video object segmentation methods such as Cutie-base, XMem, and DEVA while being more efficient. Compared with SAM 2, our EfficientTAM is comparable, e.g., 74.5% vs 74.7% on SA-V test dataset, with ∼ 2x reduced FPS. On image segmentation benchmark, SA-23, our EfficientTAM achieves 60.7% accuracy for zero-shot image segmentation compared to 59.1% accuracy for SAM and and 61.9% for SAM 2. We also benchmarked our EfficientTAM model on iPhone 15 Pro Max, which can run ∼ 10 frames per second with reasonable video segmentation performance.</p><p>Our main contributions can be summarized as follows:</p><p>• We revisit using plain, non-hierarchical image encoder, ViT-Tiny/-Small for video object segmentation and show that vanilla ViT can achieve competing performance comparing to SAM 2 with hierarchical image encoder. • We propose an efficient memory cross-attention by exploiting the underlying memory spatial token structure and demonstrate the favorable performance. • We deliver EfficientTAMs, lightweight video object segmentation and track anything models with state-ofthe-art quality-efficiency tradeoffs (figure <ref type="figure" target="#fig_0">1</ref>), which is complementary to SAM 2 for practical deployment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Video Object Segmentation (VOS) is a fundamental task in computer vision, segments objects of interest from the background and tracks target objects in a video. In the unsupervised setting <ref type="bibr" target="#b28">(Grundmann et al., 2010;</ref><ref type="bibr" target="#b2">Brox and Malik, 2010;</ref><ref type="bibr" target="#b37">Lee et al., 2011;</ref><ref type="bibr" target="#b78">Xu and Corso, 2012;</ref><ref type="bibr" target="#b25">Fragkiadaki et al., 2012;</ref><ref type="bibr" target="#b51">Perazzi et al., 2012;</ref><ref type="bibr" target="#b88">Zhang et al., 2013;</ref><ref type="bibr" target="#b38">Li et al., 2013;</ref><ref type="bibr" target="#b50">Papazoglou and Ferrari, 2013;</ref><ref type="bibr" target="#b23">Faktor and Irani, 2014;</ref><ref type="bibr" target="#b71">Wang et al., 2015;</ref><ref type="bibr" target="#b66">Taylor et al., 2015;</ref><ref type="bibr" target="#b52">Perazzi et al., 2016)</ref>, VOS models segment salient objects without a reference mask. In the semi-supervised setting <ref type="bibr" target="#b53">(Pont-Tuset et al., 2017;</ref><ref type="bibr" target="#b79">Xu et al., 2018;</ref><ref type="bibr" target="#b49">Oh et al., 2019;</ref><ref type="bibr" target="#b1">Bhat et al., 2020;</ref><ref type="bibr" target="#b58">Robinson et al., 2020;</ref><ref type="bibr">Li et al., 2022b;</ref><ref type="bibr" target="#b81">Yang and Yang, 2022;</ref><ref type="bibr" target="#b8">Cheng and Schwing, 2022;</ref><ref type="bibr">Zhang et al., 2023b;</ref><ref type="bibr" target="#b69">Wang et al., 2023;</ref><ref type="bibr" target="#b74">Wu et al., 2023;</ref><ref type="bibr">Cheng et al., 2024;</ref><ref type="bibr" target="#b82">Yang et al., 2024)</ref>, VOS requires tracking and segmenting objects based on a first-frame mask of target objects. For interactive video object segmentation (iVOS) <ref type="bibr" target="#b3">(Caelles et al., 2018;</ref><ref type="bibr" target="#b30">Heo et al., 2020;</ref><ref type="bibr">Cheng et al., 2021a;</ref><ref type="bibr" target="#b31">Homayounfar et al., 2021;</ref><ref type="bibr" target="#b14">Yang et al., 2023;</ref><ref type="bibr">Cheng et al., 2023b;</ref><ref type="bibr" target="#b56">Rajič et al., 2023;</ref><ref type="bibr">Cheng et al., 2024;</ref><ref type="bibr" target="#b17">Delatolas et al., 2024)</ref>, iVOS models perform object segmentation in videos (masklets) with user guidance, e.g., clicks, bounding boxes, scribbles. In SAM 2 <ref type="bibr" target="#b57">(Ravi et al., 2024)</ref>. Semi-supervised VOS and iVOS have been extended to promptable visual segmentation (PVS), where the model can be interactively prompted with different types of inputs such as clicks, boxes, and masks on any frame in a video for segmenting and tracking a valid object.</p><p>Vision Transformers (ViTs) have achieved huge success on various vision tasks including image classification <ref type="bibr" target="#b21">(Dosovitskiy et al., 2020)</ref>, object detection <ref type="bibr">(Li et al., 2022c)</ref>, image segmentation <ref type="bibr" target="#b8">Cheng et al. (2022)</ref>; <ref type="bibr" target="#b35">Kirillov et al. (2023</ref><ref type="bibr">), video classification (Fan et al., 2021)</ref>, and video object segmentation <ref type="bibr" target="#b22">(Duke et al., 2021;</ref><ref type="bibr" target="#b14">Yang et al., 2023)</ref>. The original ViT family scales from the efficient ViT-Tiny up to ViT-Huge, with a plain, non-hierarchical architecture. There are also hierarchical vision transformers that combine transformers with hierarchical stage structure, such as Swin <ref type="bibr" target="#b46">(Liu et al., 2021)</ref>, <ref type="bibr">MViT (Fan et al., 2021;</ref><ref type="bibr">Li et al., 2022d)</ref>, PViT <ref type="bibr" target="#b72">(Wang et al., 2021)</ref>, and Hiera <ref type="bibr" target="#b59">(Ryali et al., 2023)</ref>. While being successful, hierarchical models are usually slower than their plain ViT counterparts for practical deployment <ref type="bibr" target="#b59">(Ryali et al., 2023)</ref>. Combining ViT with convolutions <ref type="bibr" target="#b36">(LeCun et al., 1989)</ref> has been explored for fast hybrid models such as MobileViT <ref type="bibr" target="#b48">(Mehta and Rastegari, 2021)</ref>, LeViT <ref type="bibr" target="#b27">(Graham et al., 2021)</ref>, EfficientFormer <ref type="bibr">(Li et al., 2022e)</ref>, Next-ViT <ref type="bibr">(Li et al., 2022a)</ref>, Tiny-ViT <ref type="bibr" target="#b73">(Wu et al., 2022)</ref>, Castling-ViT <ref type="bibr" target="#b83">(You et al., 2023)</ref>, EfficientViT <ref type="bibr">(Liu et al., 2023b)</ref>, and MobileNetv4 <ref type="bibr" target="#b54">(Qin et al., 2024)</ref>. This line of progression towards building efficient ViTs is orthogonal to our EfficientTAM work towards building efficient video object segmentation. Following SAM <ref type="bibr" target="#b35">(Kirillov et al., 2023)</ref> and EfficientSAMs <ref type="bibr">(Xiong et al., 2024b)</ref>, we are pursuing plain ViT backbones for efficient video object segmentation and track anything tasks.</p><p>Efficient Attention. The field has developed methods to reduce the quadratic cost of standard self-attention with respect to input sequence length <ref type="bibr" target="#b68">Vaswani et al. (2017)</ref>. Local windowed attention has been applied in <ref type="bibr" target="#b0">Beltagy et al. (2020)</ref>; <ref type="bibr" target="#b85">Zaheer et al. (2020)</ref> for reducing the complexity of self-attention. In <ref type="bibr" target="#b61">Shen et al. (2018)</ref>; <ref type="bibr" target="#b34">Katharopoulos et al. (2020)</ref>, a linear dot product approximation is proposed to linearize the softmax matrix in self-attention by heuristically separating keys and queries. In <ref type="bibr" target="#b15">Choromanski et al. (2020)</ref>, the Performer model uses random features to approximate self-attention, achieving linear time and memory cost. Nyströmformer in <ref type="bibr" target="#b76">Xiong et al. (2021)</ref> makes use of the Nyström method to approximate self-attention with a linear cost. Linformer <ref type="bibr" target="#b70">Wang et al. (2020)</ref> shows that self-attention is low-rank, which can be approximated by learning linear projection matrices for the keys and values. The approach of <ref type="bibr">(Liu et al., 2023b;</ref><ref type="bibr" target="#b83">You et al., 2023)</ref> leverages the associative property of matrix multiplication for efficient attentions in vision transformers. This direction has shown success and has achieved decent performance on vision tasks. However, in preliminary experiments we found that these methods underperformed in a memory cross-attention module when adapted for efficiency improvement.</p><p>Segment Anything Model. SAM <ref type="bibr" target="#b35">(Kirillov et al., 2023)</ref> is a vision foundation model that can segment any object in an image using interactive prompts such as points and bounding boxes. SAM has demonstrated remarkable zero-shot transfer performance and high versatility for many vision tasks including a broad range of segmentation applications <ref type="bibr">(Chen et al., 2023a;</ref><ref type="bibr" target="#b5">Cen et al., 2023;</ref><ref type="bibr" target="#b18">Deng et al., 2023;</ref><ref type="bibr">Chen et al., 2023b)</ref>, in-painting <ref type="bibr" target="#b84">(Yu et al., 2023)</ref>, image restoration <ref type="bibr" target="#b33">(Jiang and Holz, 2023)</ref>, image editing <ref type="bibr" target="#b26">(Gao et al., 2023)</ref>, image shadow removal <ref type="bibr">(Zhang et al., 2023c)</ref>, medical image segmentation (Ma and <ref type="bibr">Wang, 2023)</ref>, camouflaged object detection <ref type="bibr" target="#b64">(Tang et al., 2023)</ref>, transparent object detection <ref type="bibr" target="#b4">(Han et al., 2023)</ref>, concept-based explanation <ref type="bibr" target="#b62">(Sun et al., 2023)</ref>, semantic communication <ref type="bibr" target="#b65">(Tariq et al., 2023)</ref>, and object tracking <ref type="bibr">(Cheng et al., 2023b;</ref><ref type="bibr" target="#b14">Yang et al., 2023)</ref>. The strong ability on image segmentation with flexible prompts motivates the extension of SAM for video object segmentation and track anything. Track Anything Model (TAM) <ref type="bibr" target="#b14">(Yang et al., 2023)</ref> combines SAM and XMem <ref type="bibr" target="#b8">Cheng and Schwing (2022)</ref> for interactive video object tracking and segmentation with SAM for frame segmentation and XMem for tracking. SAM-Track <ref type="bibr">(Cheng et al., 2023b)</ref> perform object tracking and segmentation in videos by combining SAM <ref type="bibr" target="#b35">(Kirillov et al., 2023)</ref>, DeAOT <ref type="bibr" target="#b81">(Yang and Yang, 2022)</ref>, and Grounding-Dino <ref type="bibr">(Liu et al., 2023a)</ref>. The latest SAM 2 <ref type="bibr" target="#b57">(Ravi et al., 2024)</ref> extended SAM for video segmentation through a hierarchical image encoder for frame embeddings and a memory module that conditions current frame embeddings on past frames. Motivated by mobile app use-cases and computationally-constrained applications, recent works have reduced the computational cost of SAM, such as MobileSAM <ref type="bibr">(Zhang et al., 2023a)</ref>, FastSAM <ref type="bibr" target="#b93">(Zhao et al., 2023)</ref>, and EfficientSAM <ref type="bibr">(Xiong et al., 2024b)</ref>. The present paper focuses on improving the efficiency challenges of SAM 2 for practical deployment of video object segmentation and track anything.</p><p>3 Approach</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries</head><p>Segment Anything. SAM <ref type="bibr" target="#b35">(Kirillov et al., 2023)</ref> contains a ViT image encoder and a prompt-guided mask decoder. The encoder takes an image and outputs image embeddings. Then the decoder takes the image embeddings and a prompt, which allows cutting out any object from the background in an image. SAM is trained on an image dataset of over 1B masks.</p><p>Segment Anything 2. The architecture of segment anything 2 (SAM 2) <ref type="bibr" target="#b57">(Ravi et al., 2024)</ref> largely follows SAM, which consists of a hierarchical image encoder, a prompt-guided lightweight mask decoder, and a new memory mechanism. SAM 2 uses a hierarchical image encoder, Hiera <ref type="bibr" target="#b59">(Ryali et al., 2023)</ref>, to produce image embeddings for each frame. The stride 16 and 32 features from Stage 3 and 4 are used for the memory module.</p><p>The stride 4 and 8 features from Stage 1 and Stage 2 are not used in the memory module but are fed to upsampling layers in the mask decoder for generating segmentation masks. For stable object tracking, SAM 2 employs a memory mechanism consisting of a lightweight memory encoder, a lightweight memory bank, and a memory attention module. It stores information from past frames and uses the memory attention module to perform cross-attention between the stored memory in the memory bank and current frame features, thereby understanding temporal dependencies in video.</p><p>The memory attention module consists of a stack of transformer blocks. Each block contains self-attention, cross-attention, and MLP. The first transformer block takes the image embedding from the current frame as an input. The core component of each transformer block, cross-attention, integrates the current frame embedding and the memory stored in memory bank to produce an embedding with temporal correspondence information. For memory tokens, it includes two parts, the spatial embedding tokens from memory encoder and the object-level pointer tokens from mask decoder. Let us assume the number of spatial tokens is n, the number of object-level pointer tokens is P , and d m is the channel dimension, memory tokens can be formulated as</p><formula xml:id="formula_0">M b = M s ∈ R n×dm M p ∈ R P ×dm .</formula><p>Let L be the number of tokens and d q be the dimension of each token for input frame features after selfattention, X ∈ R L×dq . The input sequence X ∈ R L×dq is linearly projected to input queries Q ∈ R L×d , and the memory tokens, M b ∈ R (n+P )×dm are linearly projected to keys K ∈ R (n+P )×d , and values V ∈ R (n+P )×d respectively, where d is the embedding dimension of queries, keys, and values. The scaled dot-product cross attention mechanism applied on the queries Q, keys K, values V can be formally written as,</p><formula xml:id="formula_1">C(Q, K, V ) = softmax QK T √ d V,<label>(1)</label></formula><p>where the softmax operation is applied row-wise. A single head cross attention is used in the memory module.</p><p>In later discussion, we also consider keys and values as memory tokens for simplification.</p><p>Efficiency Bottleneck. Despite the advantages of the hierarchical image encoder for multiscale frame feature extraction and cross-attention for integrating current frame features with stored memory, it poses the challenges for practical deployment of SAM 2. The inefficient SAM 2 (tiny) even shows comparable FPS to the base SAM 2, 47.2 FPS vs 43.8 FPS due to the hierarchical design of the image encoder and the use of hierarchical features, which also makes SAM 2 challenging to deploy on mobile devices. Moreover, the number of tokens in keys and values for performing cross-attention in the memory module are super long, e.g., 30K. It leads to a large computation and memory cost when performing cross-attention, which becomes the efficiency bottleneck of the memory module for real-world deployment.</p><p>Mask Decoder Memory Bank Pooling Object Pointer Spatial Embedding xL Prompt Encoder Prompt Memory encoder Vanilla ViT Encoder Efficient Memory Cross-Attention Video sequence  At right, we visualize the difference between original cross-attention of equation ( <ref type="formula" target="#formula_1">1</ref>) and efficient cross-attention of equation ( <ref type="formula" target="#formula_7">5</ref>); the relative error w.r.t original cross-attention is 0.03 under Frobenius norm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Efficient Video Object Segmentation and Track Anything</head><p>We now address the efficiency issue of SAM 2 for building efficient video object segmentation and track anything model, EfficientTAM. Motivated by the high quality segmentation performance of SAM and EfficientSAM, we revisit using plain, non-hierarchical lightweight image encoders such as ViT-Small/ViT-Tiny, for frame feature extraction. We found that the use of vanilla ViT for frame feature extraction makes EfficientTAM highly efficient and deployable on mobile devices. Further, we introduce an efficient memory module to reduce the computation and memory cost by proposing an efficient cross-attention operation. Based on these two designs, we build efficient video object segmentation and track anything model by largely following SAM2. figure <ref type="figure" target="#fig_1">2</ref> illusrates an overview of our proposed EfficientTAM.</p><p>Efficient Image Encoder. The image encoder's role is to produce feature embeddings for each high-resolution frame. We use a SAMI <ref type="bibr">(Xiong et al., 2024b)</ref> pretrained vanilla ViT image encoder <ref type="bibr" target="#b21">(Dosovitskiy et al., 2020;</ref><ref type="bibr" target="#b67">Touvron et al., 2021)</ref> to extract frame features. Differing from the image encoder of SAM 2, our image encoder provides a single-scale feature map and no other features in the mask decoder are added to the upsampling layers during decoding for segmentation mask generation. We adopt the lightweight image encoders ViT-Small and ViT-Tiny with a 16 × 16 patch size. Following <ref type="bibr">(Li et al., 2022c)</ref>, we use 14 × 14 non-overlapping windowed attention and 4 equally-spaced global attention blocks to efficiently extract features from high-resolution frames. Our image encoder outputs a single-scale feature embedding with a 16x reduced resolution, which takes high-resolution (e.g., 1024 × 1024) frames as input and transforms it into a dense embedding of downscaled size 64 × 64.</p><p>Efficient Memory Module. The memory module leverages information from previous frames to facilitate consistent object tracking. Cross-attention is a major efficiency bottleneck of the memory module in SAM 2 <ref type="bibr" target="#b57">(Ravi et al., 2024)</ref> due to its long memory token sequence. We now discuss how exploiting the underlying structure of memory tokens -local smoothness (strong locality) within spatial memory tokens -can yield a more efficient cross-attention.</p><p>Consider two consecutive memory spatial tokens, k i and k i+1 , local smoothness implies that ||k i -k i+1 || 2 2 ≤ c K n 2 , for i = 1, . . . , n -1, where c K is a positive constant. This suggests that given a sufficient small local window, l w × l h , using a single token to represent other tokens in the homogeneous window may provide a coarser representation of the full set of memory spatial tokens K s as Ks . We can construct a good surrogate of K s with the same size, Ks , from Ks by repeating the single token in each window l w × l h times. Under the smoothness assumption, Ks will not be far from K s . Empirically, we observed that a coarser representation of spatial memory tokens is good surrogate of the full spatial memory tokens. figure <ref type="figure" target="#fig_2">3</ref> confirms the coarser representation of input keys and values are close to the original keys and values of cross-attention in the memory module.</p><p>Utilizing highly correlated neighboring tokens in cross-attention, we perform average pooling to efficiently compute a coarser representation for keys K and values V in our model. For input spatial tokens K s = [k 11 , . . . , k 1h ; . . . ; k w1 , . . . , k wh ] where w ×h is the resolution size, we divide the n = w ×h tokens into k = w × h rectangular pooling regions and compute the average token of each region. For simplicity, we assume w is divisible by w and h is divisible by h. Denote l w = w w , l h = h h . Ks and Ṽs can be computed by averaging each region as,</p><formula xml:id="formula_2">kij = (i+1)×lw p=i×lw+1 (j+1)×l h q=j×l h +1 k pq l w × l h , ṽij = (i+1)×lw p=i×lw+1 (j+1)×l h q=j×l h +1 v pq l w × l h ,<label>(2)</label></formula><formula xml:id="formula_3">where i = 1, • • • , w, j = 1, • • • , h.</formula><p>This token-pooling scheme requires a single scan of the tokens leading to an efficient coarse token generation. We find that using averaging pooling with window size, 2 × 2, is sufficient to ensure a good approximation for spatial memory tokens.</p><p>Assume Ks is a coarser representation of memory spatial keys, K s , we can construct a good surrogate of K s ∈ R n×d with the same size, Ks ∈ R n×d from Ks ∈ R wh ×d by stacking each ki , i = 1, . . . , wh , l w × l h times, which can be written as, Ks = [ k1 ; . . . ; k1</p><p>lw×l h ; k2 ; . . . ; k2 lw×l h ; . . . ; k wh ; . . . ; k wh lw×l h ] Similarly, we stack each ṽi , i = 1, . . . , wh , l w × l h times to construct Vs ∈ R n×d as a good surrogate of values, V s ∈ R n×d , which can be written as, Vs = [ṽ1; . . . ; ṽ1 lw×l h ; ṽ2; . . . ; ṽ2 lw×l h ; . . . ; ṽ wh; . . . ; ṽ wh lw×l h</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>]</head><p>Then we concatenate this coarse spatial tokens with object pointer tokens, K = [ Ks ; K p ] ∈ R (n+P )×d and V = [ Vs ; K p ] ∈ R (n+P )×d , for a good surrogate of original memory tokens, K and V . For the coarse memory tokens, K and V , we have,</p><formula xml:id="formula_4">softmax Q KT √ d V = softmax (A) Ṽ ,<label>(3)</label></formula><p>where A = [</p><formula xml:id="formula_5">Q KT s √ d + ln (l w × l h ), QK T p √ d ] ∈ R L×( wh +P ) , Ṽ = [ Ṽs ; V p ] ∈ R ( wh +P )×d .</formula><p>We provide a proof of equation ( <ref type="formula">7</ref>) in the appendix. Since K and V are good surrogate of K and V respectively, we obtain a good surrogate of the original cross-attention, softmax</p><formula xml:id="formula_6">QK T √ d V in equation (1), C(Q, K, V ) = softmax Q KT √ d V .<label>(4)</label></formula><p>With equation ( <ref type="formula">7</ref>), we have an efficient version of cross-attention,</p><formula xml:id="formula_7">C(Q, K, V ) = softmax(A) Ṽ . (<label>5</label></formula><formula xml:id="formula_8">)</formula><p>Link to efficient cross-attention variants. Interestingly, we can find some cross-attention variants based on our proposed efficient cross-attention in equation ( <ref type="formula" target="#formula_7">5</ref>). We notice there is a constant for balancing the attention score between coarse spatial tokens and object pointer tokens, avoiding reducing the attention to spatial tokens after pooling. If we remove this constant, it can lead to a linformer variant using averaging pooling to replace the learnable projection. Instead of removing the constant, we add it to keys for regularizing the attention between coarse spatial tokens and object pointer tokens in equation ( <ref type="formula" target="#formula_9">6</ref>), for obtaining another variant.</p><formula xml:id="formula_9">C(Q, K, V ) = softmax Q KT √ d Ṽ ,<label>(6)</label></formula><p>where</p><formula xml:id="formula_10">K = [ Ks + ln (l w × l h ), K p ] ∈ R ( wh +P )×d .</formula><p>It is feasible to achieve a good surrogate of the original cross-attention because spatial memory embeddings have strong locality. Our efficient cross-attention is close to the original cross-attention, visualized in figure <ref type="figure" target="#fig_2">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setting</head><p>Pretraining. The SA-1B dataset consists of 11M diverse, high resolution images with 1.1B high-quality segmentation masks. Similar to <ref type="bibr" target="#b57">(Ravi et al., 2024)</ref>, we pretrain our EfficientTAM without memory components on SA-1B dataset <ref type="bibr" target="#b35">(Kirillov et al., 2023)</ref> for 90k steps. Our ViT image encoder is initialized from pre-trained ViTs <ref type="bibr">(Xiong et al., 2024b)</ref> . We use the AdamW optimizer <ref type="bibr" target="#b47">(Loshchilov and Hutter, 2019)</ref> with a momentum, (β 1 = 0.9, β 2 = 0.999), a global batch size of 256, and a initial learning rate of 4e -4. The learning rate is decayed by a reciprocal square root learning rate schedule <ref type="bibr" target="#b86">(Zhai et al., 2022)</ref> with 1k iterations linear warmup and 5k iterations linear cooldown. We set weight decay to 0.1. We do not apply drop path for our image encoder. Layer-wise decay <ref type="bibr" target="#b16">(Clark et al., 2020)</ref> is set to 0.8. We apply horizontal flip augmentation and resize the input image resolution to 1024 × 1024. We restrict our training to 64 masks per image. Our models are pre-trained on 256 A100 GPUs with 80GB GPU memory with a linear combination of focal and dice loss for mask prediction (e.g., a ratio of 20:1). Bfloat16 is used during the training.</p><p>Full Training Datasets. Following <ref type="bibr" target="#b57">(Ravi et al., 2024)</ref>, we train our EfficientTAM including memory components on SA-V dataset <ref type="bibr" target="#b57">(Ravi et al., 2024)</ref> and a 10% subset of SA-1B <ref type="bibr" target="#b35">(Kirillov et al., 2023)</ref>. SA-V is a large-scale and diverse video segmentation dataset, including 51K videos captured across 47 countries and 600K mask annotations covering whole objects and parts. SA-V video resolution ranges from 240p to 4K and duration ranges from 4 seconds to 138 seconds. Unlike SAM 2, we do not use other open-source datasets or internal datasets during our training for a fair comparison with baselines.</p><p>Full Training Implementation Details. Similar to <ref type="bibr" target="#b57">(Ravi et al., 2024)</ref>, we train our EfficientTAM for 300k steps after pretraining. We use the AdamW optimizer <ref type="bibr" target="#b47">(Loshchilov and Hutter, 2019)</ref> with a momentum, (β 1 = 0.9, β 2 = 0.999), a batch size of 256, and a initial learning rate of 6e -5 for image encoder and 3e -4 for other components of the model. The learning rate is decayed by a cosine schedule with 15k iterations linear warmup. We set weight decay to 0.1. We do not apply drop path for our image encoder. Layer-wise decay <ref type="bibr" target="#b16">(Clark et al., 2020)</ref> is set to 0.8. We apply horizontal flip image augmentation and resize the input image resolution to 1024 × 1024. For video, we apply horizontal flip augmentation, affine transformation with degree 25 and shear 20, color jittering with brightness 0.1, contrast 0.03, saturation 0.03, gray scale augmentation with a probability of 0.05, We restrict our training to 64 masks per image and 3 masks per frame for video. Our models are trained on 256 A100-80G GPUs with a linear combination of focal and dice losses for mask prediction, mean-absolution-error loss for IoU prediction, and cross-entropy loss for object prediction. The ratio for the linear combination loss is 20:1:1:1. Bfloat16 is used for training. demonstrate the competing capabilities of EfficientTAM on image and video segmentation. For zero-shot image tasks, we evaluate EfficientTAM on 37 datasets including 23 datasets of SA-23 <ref type="bibr" target="#b35">(Kirillov et al., 2023)</ref> and 14 video datasets introduced in <ref type="bibr" target="#b57">(Ravi et al., 2024)</ref>. For zero-shot video tasks, we evaluate our EfficientTAM on 9 densely annotated datasets for promptable video segmentation. We use 17 video datasets to evaluate zero-shot accuracy under interactive semi-supervised VOS setting using different prompts. For the standard semi-supervised VOS setting where a ground-truth mask on the first frame is provided, MOSE <ref type="bibr" target="#b19">(Ding et al., 2023)</ref>, DAVIS2017 <ref type="bibr" target="#b53">(Pont-Tuset et al., 2017)</ref>, LVOS <ref type="bibr" target="#b32">(Hong et al., 2024)</ref>, SA-V <ref type="bibr" target="#b57">(Ravi et al., 2024)</ref>, and YTVOS <ref type="bibr" target="#b79">(Xu et al., 2018)</ref> are used to measure the VOS accuracy. We refer readers to <ref type="bibr" target="#b35">(Kirillov et al., 2023;</ref><ref type="bibr" target="#b57">Ravi et al., 2024)</ref> for the details of these datasets. Models. We use our EfficientTAM for zero-shot image and video tasks.</p><p>Baselines and Evaluation Metrics. Baselines. For the standard semi-supervised VOS task, where the first-frame mask is provided, we compare the performance of our EfficientTAM with SAM 2 <ref type="bibr" target="#b57">(Ravi et al., 2024)</ref>, Cutiebase <ref type="bibr">(Cheng et al., 2024)</ref>, DEVA <ref type="bibr">(Cheng et al., 2023a)</ref>, XMem <ref type="bibr" target="#b8">(Cheng and Schwing, 2022)</ref>, etc. For the zero-shot promptable video segmentation task and the interactive semi-supervised video object segmentation task using different prompts, we compare our method with SAM2 <ref type="bibr" target="#b57">(Ravi et al., 2024)</ref>, SAM+XMem++ <ref type="bibr" target="#b57">(Ravi et al., 2024)</ref>, and SAM+Cutie <ref type="bibr" target="#b57">(Ravi et al., 2024)</ref>. For zero-shot image segmentation task, we compare with SAM <ref type="bibr" target="#b35">(Kirillov et al., 2023)</ref> and SAM2 <ref type="bibr" target="#b57">(Ravi et al., 2024)</ref>. Note that we use the opensource version of SAM 2 (without training on MOSE/LVOS/YTVOS) for comparison. We also acknowledge the very recent release of SAM 2.1 trained with long memory contexts. Evaluation Metrics. We evaluate our method and all baselines using the accuracy metrics of the combined J (region similarity)&amp;F(contour accuracy), for zero-shot video segmentation tasks; mIoU (mean intersection over union) for zero-shot image segmentation tasks. For efficiency metrics, we compare the number of model parameters or inference throughput on GPU (e.g, A100) and latency on mobile devices (e.g., iPhone 15 Pro Max). We follow SAM 2 <ref type="bibr" target="#b57">(Ravi et al., 2024)</ref> to report metrics. When providing main results on MOSE, LVOS and YTVOS, we submit to their benchmarking servers to evaluate on, MOSE val, LVOS val, and YTVOS2019 val, for final performance. For ablation studies, we evaluate on a MOSE development set, MOSE dev with 200 randomly-sampled videos from the MOSE training split <ref type="bibr" target="#b57">(Ravi et al., 2024)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main Results</head><p>Standard Semi-Supervised Video Object Segmentation. Semi-supervised video object segmentation is the process of object segmentation and tracking in a video based on a ground-truth mask on the first frame. We follow SAM 2 <ref type="bibr" target="#b57">(Ravi et al., 2024)</ref> and report accuracy of our methods on this standard semi-supervised video object segmentation task. We also report latency on a single A100 GPU with a batch size of 1. We evaluate EfficientTAMs with different image encoders, ViT-Tiny and ViT-Small, and memory modules, original memory block and efficient memory block with a 2 × 2 window pooling for a trade-off between efficiency and accuracy. EfficientTAM-S denotes EfficientTAM using a ViT-Small image encoder and the original memory block, and EfficientTAM-S/2 denotes EfficientTAM with a ViT-Small image encoder and efficiency memory block with a 2 × 2 window pooling. table 1 compares our EfficientTAM with VOS baselines including SAM 2 <ref type="bibr" target="#b57">(Ravi et al., 2024)</ref>, Cutie-base <ref type="bibr">(Cheng et al., 2024)</ref>, and XMem <ref type="bibr" target="#b8">(Cheng and Schwing, 2022)</ref>. On SA-V test, our EfficientTAM-S achieves 74.5 J &amp;F, outperforming Cutie-base, Cutie-base+, and XMem by 12.2, 12.9, and 14.4, respectively. On long-term video object segmentation benchmark, LVOS, we can also see that Our EfficientTAM-S outperform Cutie-base and XMem by a large margin. Notice that our EfficientTAM-S only underperforms SAM 2 by &lt; 2 J &amp;F or G across 5 video benchmarks with ∼2x speedup and ∼2.4x fewer parameters. Further, EfficientTAM with efficient memory attention performs slightly worse than the one with original memory attention, but with much speedup, especially on mobile devices, &gt;2x reduced latency on iPhone 15. For example, EfficientSAM-S achieves 74.5 J &amp;F on SA-V test with 1010.8 ms running time per frame on iPhone 15. EfficientSAM-S/2 with efficient cross-memory attention obtain 74.0 J &amp;F with only 450 ms. These results show the extraordinary benefits of EfficientTAMs for semi-supervised video object segmentation and validate the advantages of our methods for practical deployment. Promptable Video Segmentation. Similar to SAM 2 <ref type="bibr" target="#b57">(Ravi et al., 2024)</ref>, we evaluate promptable video segmentation using two settings, offline evaluation and online evaluation. For offline evaluation, we make multiple passes through a video to annotate frames w.r.t. the largest model error. For online evaluation, we make a single pass through the video to annotate frames. 3 clicks per frame are used for the evaluations on 9 densely annotated video datasets including EndoVis, ESD, LVOSv2, LV-VIS, UVO, VOST, PUMaVOS, Virtual KITTI 2, and VIPSeg. Average J &amp;F accuracy over 1, . . . , 8 interacted frames is reported. figure <ref type="figure" target="#fig_3">4</ref> shows the comparison between our method and strong baselines including SAM 2, SAM + XMem++, and SAM + Cutie. EfficientTAM outperforms SAM + XMem++ and SAM + Cutie for both evaluation settings. EfficientTAM also reduces the gap between SAM 2 for offline and online settings. Specifically, with 8 annotated frames with 3-click, EfficientTAM-S and EfficientTAM-S/2 achieve ∼ 82 J &amp;F in average for offline evaluation setting and ∼ 81 J &amp;F in average for online evaluation, outperforming SAM + XMem++, and SAM + Cutie by &gt;3 J &amp;F and reducing the gap of SAM 2. This set of experiments further validate the effectiveness of our EfficientTAM on promptable video segmentation.</p><p>Interactive Semi-Supervised Video Object Segmentation. We also evaluate our method on the interactive semi-supervised video object segmentation task with click, box, or mask prompts provided only on the first Table <ref type="table">3</ref> Segment anything results on SA-23 benchmark <ref type="bibr" target="#b35">(Kirillov et al., 2023)</ref> and 14 new video benchmark <ref type="bibr" target="#b57">(Ravi et al., 2024)</ref>. The average 1-click (5-click) mIoU is reported.</p><p>frame by following SAM 2. In table 2, we report the average J &amp;F accuracy over 17 video datasets for each type of prompt. We observe that EfficientTAM outperforms SAM + XMem++, and SAM + Cutie with different input prompts. We also notice the reduced gap between EfficientTAM and SAM 2. With 1 click, our EfficientTAM-S obtain 63 J &amp;F accuracy, with a 6 J &amp;F gain over SAM + XMem++ and SAM + Cutie and a slight loss, 1.3 J &amp;F comparing to SAM 2. In summary, EfficientTAM performs favorably on the interactive semi-supervised VOS task using different prompts.</p><p>Segment Anything on Images. We now evaluate our model for the segment anything task on images. In Table <ref type="table">table</ref> 3, we report 1-click and 5-click mIoU accuracy on both SA-23 benchmark, plus the new benchmark introduced in SAM 2 <ref type="bibr" target="#b57">(Ravi et al., 2024)</ref> with 14 video datasets from video domain. We compare our EfficientTAMs with SAM (ViT-H) and HQ-SAM (ViT-H). Our EfficientTAM-S obtains a 2.6 mIoU improvement over SAM (ViT-H) and 1.6 mIoU improvement over HQ-SAM (ViT-H) on 1-click accuracy. For 5-click, we observe consistent improvement over SAM (ViT-H) and HQ-SAM (ViT-H). We also notice a significant improvement on the video benchmarks of SA-23 and the one with 14 new videos. This indicates our EfficientTAMs are strong for both image and video segmentation.</p><p>Qualitative Evaluation. figure 5 shows two video examples. We compare EfficientTAM and SAM 2 with a mask in the first frame prompted. We find that our EfficientTAM can generate high-quality masklet for the target object as SAM 2. More video examples are in the appendix. These results suggest that our EfficientTAMs have similar abilities to SAM 2, while EfficientTAM is more efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Studies</head><p>Impact of the object pointer tokens. We study the effect of the object pointer tokens when performing cross-attention in the memory module. We ablate the cross-attention with or without the object pointer tokens. We find that object pointers significantly improve the performance on SA-V test dataset, 74.5 vs 72.1 J &amp;F, consistent with SAM 2 <ref type="bibr" target="#b57">(Ravi et al., 2024)</ref>. This demonstrates that object pointer tokens need to be cross-attended with spatial tokens from the memory bank.</p><p>Structure of memory tokens. We ablate the impact of memory tokens for efficient cross-attention in the memory module. In our efficient cross-attention, we leverage the locality of memory spatial tokens for a coarser representation, and we concatenate the coarser embedding with object pointer tokens. We observe that naively pooling the entire memory tokens instead of only the spatial tokens yields a large performance Table <ref type="table">5</ref> Ablation study on the effect of input resolution.</p><p>drop, 2.3 J &amp;F on SA-V test.</p><p>Impact of window size. We perform an averaging pooling for a good surrogate in equation ( <ref type="formula" target="#formula_7">5</ref>). We experiment with window sizes 2 × 2 and 4 × 4. We find increasing the window from 2 × 2 to 4 × 4 for efficient cross-attention will lead to ∼ 1 J &amp;F accuracy drop with marginal speed improvement. Therefore, we use window size 2 × 2 to achieve a trade-off between accuracy and efficiency.</p><p>Linear cross-attention. We explore adapting one representative efficient attention method such as linear attention <ref type="bibr" target="#b15">(Choromanski et al., 2020;</ref><ref type="bibr" target="#b4">Cai et al., 2023;</ref><ref type="bibr" target="#b83">You et al., 2023)</ref> by leveraging the associative property of matrix multiplication. We find that linear attention using associative property of matrix multiplication leads to significant performance drop, &gt; 10 J &amp;F accuracy on SA-V test, comparing to our proposed efficient cross-attention. Therefore, leveraging the underlying token structure for efficient cross-attention is more effective.</p><p>Efficient cross-attention variants. We compare efficient cross-attention variants. We find that the Linformer variant underperforms the efficient cross-attention in equation ( <ref type="formula" target="#formula_7">5</ref>), 73.4 vs 74 J &amp;F on SA-V test. However, we find that equation ( <ref type="formula" target="#formula_9">6</ref>), can achieve comparable performance, shown in table <ref type="table" target="#tab_4">4</ref>.</p><p>Impact of input resolution. We ablate the impact of input resolution for video object segmentation. By default, we used 1024 × 1024. We experiment with different input resolution, e.g., 512 × 512. table <ref type="table">5</ref> shows that decreasing the input resolution leads to some performance drop. But it improves the efficiency, especially on mobile device, 12.5x speedup on iPhone 15. This gives flexibility for practical deployments with different latency and quality needs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We revisited using a </p><formula xml:id="formula_11">q i √ d K T p )V p ) = D Sii (l w × l h × (e( q i √ d kT 1 )ṽ 1 + • • • + e( q i √ d</formula><p>kT 1 )ṽ wh ) + e(</p><formula xml:id="formula_12">q i √ d K T p )V p ) = D Sii (l w × l h × e( q i √ d KT s ) Ṽ T s + e( q i √ d K T p )V p ) = D Sii (e(ln(l w × l h ) + q i √ d KT s ) Ṽs + e( q i √ d K T p )V p ) = softmax[ q i KT s √ d + ln (l w × l h ), q i KT p √ d ][ Ṽs ; V p ]<label>(8)</label></formula><p>where D Sii is the i th diagonal element of the matrix D S . Note that the right side of equation ( <ref type="formula" target="#formula_12">8</ref>) is the i th row of softmax (A) Ṽ . It concludes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Ablation Studies</head><p>Impact of the object pointer tokens. We study the effect of the object pointer tokens when performing crossattention in the memory module. We ablate the cross-attention with or without the object pointer tokens. When performing cross-attention, we find that object pointers significantly improve the performance on SA-V test dataset, 74.5 vs 72.1 J &amp;F, shown in table <ref type="table" target="#tab_5">6</ref>. The observations are consistent with SAM 2 <ref type="bibr" target="#b57">(Ravi et al., 2024)</ref>. This demonstrates that object pointer tokens need to be cross-attended with spatial tokens.</p><p>Structure of memory tokens. We ablate the impact of memory tokens for efficient cross-attention in the memory module. In our efficient cross-attention, we leverage the locality of memory spatial tokens for a coarser representation, and we concatenate the coarser embedding with object pointer tokens. In table 7, we observe that naively pooling the entire memory tokens instead of only the spatial tokens yields a large performance drop, 2.3 J &amp;F on SA-V test.</p><p>Local windowed cross-attention. We adapt local windowed attention for efficient cross-attention by partitioning input tokens into 4 non-overlapping segments (windows), within which we conduct cross-attention. In table <ref type="table" target="#tab_7">8</ref>, we find that local windowed cross-attention underperforms our proposed efficient cross-attention using averaging pooling, 72.4 vs 74.0 J &amp;F on SA-V test dataset. These results demonstrate the effectiveness of our efficient cross-attention by leveraging the strong locality of spatial memory tokens.</p><p>Efficient cross-attention variant. We observe that equation (6) in the main paper is close to original crossattention, visualized in figure <ref type="figure" target="#fig_5">7</ref>. This suggests that equation ( <ref type="formula" target="#formula_9">6</ref>) can also serve as a surrogate of the original cross-attention.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Qualitative Evaluation</head><p>We provide more qualitative results of EfficientTAMs for video and image instance segmentation. figure <ref type="figure">6</ref> shows two challenging video examples with occluded objects. We compare EfficientTAM and SAM 2 with a mask in the first frame prompted. We find that our EfficientTAM can generate high-quality masklet for the target occluded object as SAM 2. For image segmentation, we also observe that our EfficientTAM can generate quality image segmentation results as SAM and SAM 2, shown in figure <ref type="figure">8</ref>. We report the predicted masks with two types of prompts, point and box, and also segment everything results. These results suggest that our EfficientTAMs have similar abilities to SAM 2, while EfficientTAM is more efficient.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1</head><label>1</label><figDesc>Figure 1 Comparative analysis. (Left) Speed comparison between EfficientTAM and SAM 2 on a single NVIDIA A100 GPU. While SAM 2 is challenging for on-device deployment, our EfficientTAM can run 261 ms per frame on iPhone 15 Pro Max. (Right) FPS/Parameter/Performance comparison of EfficientTAM, SAM 2, and other efficient models for zero-shot video object segmentation on SA-V test. We benchmark FPS (frames per second) of all models with 1024 × 1024 input resolution on a single NVIDIA A100.</figDesc><graphic coords="2,70.87,63.78,470.26,113.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2</head><label>2</label><figDesc>Figure 2 EfficientTAM architecture. Our proposed EfficientTAM takes a vanilla lightweight ViT image encoder for frame feature extraction. An efficient memory cross-attention is proposed to further improve the efficiency of EfficientTAM by leveraging the strong locality of memory spatial embeddings. EfficientTAM is fully trained on SA-1B (image) and SA-V (video) for unified image and video segmentation.</figDesc><graphic coords="5,70.87,240.52,470.27,87.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3</head><label>3</label><figDesc>Figure 3 An example to show strong locality of the Keys and Values in the cross-attention of the memory module. Keys and Values are a matrix of size 28700 × 256. Cross-attention is a matrix of size 4096 × 256. For simplicity of visualizing and comparison, we only draw the top matrix of size 320 × 256. We use a single averaged token to represent other tokens in the homogeneous window with a 2 × 2 size, for Keys and Values to obtain coarse Keys and Values. At right, we visualize the difference between original cross-attention of equation (1) and efficient cross-attention of equation (5); the relative error w.r.t original cross-attention is 0.03 under Frobenius norm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4</head><label>4</label><figDesc>Figure4Promptable video segmentation results across 9 video segmentation datasets under interactive offline (left) and online (right) evaluation settings. The average J &amp;F over 1, . . . , 8 interacted frames is reported.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5</head><label>5</label><figDesc>Figure5Visualization results on video segmentation and tracking with SAM 2, and our EfficientTAM model. We sampled a subset of frames for visualization. The segmented objects, e.g., the goose and the camel, are colored in red.</figDesc><graphic coords="9,106.14,376.02,399.73,118.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7</head><label>7</label><figDesc>Figure7Visualization of the difference between original cross-attention and efficient cross-attention of equation (6).</figDesc><graphic coords="20,186.77,135.38,235.14,230.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc>Standard semi-supervised video object segmentation results across video object segmentation benchmarks.</figDesc><table><row><cell>Method</cell><cell>MOSE val</cell><cell>J &amp;F DAVIS 2017 val</cell><cell>LVOS val</cell><cell>SA-V test</cell><cell>G YTVOS 2019 val</cell><cell>Parameters (M)</cell><cell cols="2">FPS Latency (ms) A100 iPhone15</cell></row><row><cell>STCN (Cheng et al., 2021b)</cell><cell>52.5</cell><cell>85.4</cell><cell>-</cell><cell>57.3</cell><cell>82.7</cell><cell>54</cell><cell>62.8</cell><cell>-</cell></row><row><cell>RDE (Li et al., 2022b)</cell><cell>46.8</cell><cell>84.2</cell><cell>-</cell><cell>48.4</cell><cell>81.9</cell><cell>64</cell><cell>88.8</cell><cell>-</cell></row><row><cell>XMem (Cheng and Schwing, 2022)</cell><cell>59.6</cell><cell>86.0</cell><cell>-</cell><cell>60.1</cell><cell>85.6</cell><cell>62</cell><cell>61.2</cell><cell>-</cell></row><row><cell>DEVA (Cheng et al., 2023a)</cell><cell>66.0</cell><cell>87.0</cell><cell>55.9</cell><cell>53.8</cell><cell>85.4</cell><cell>69</cell><cell>65.2</cell><cell>-</cell></row><row><cell>Cutie-base (Cheng et al., 2024)</cell><cell>69.9</cell><cell>87.9</cell><cell>66.0</cell><cell>61.6</cell><cell>87.0</cell><cell>35</cell><cell>65</cell><cell>-</cell></row><row><cell>Cutie-base+ (Cheng et al., 2024)</cell><cell>71.7</cell><cell>88.1</cell><cell>-</cell><cell>62.3</cell><cell>87.5</cell><cell>35</cell><cell>57.2</cell><cell>-</cell></row><row><cell>SAM 2 (Ravi et al., 2024)</cell><cell>72.8</cell><cell>88.9</cell><cell>76.2</cell><cell>74.7</cell><cell>87.9</cell><cell>81</cell><cell>43.8</cell><cell>-</cell></row><row><cell>EfficientTAM-Ti/2 (ours)</cell><cell>68.4</cell><cell>88.4</cell><cell>66.1</cell><cell>70.8</cell><cell>87.1</cell><cell>18</cell><cell>109.4</cell><cell>261.4</cell></row><row><cell>EfficientTAM-Ti (ours)</cell><cell>69.3</cell><cell>89.1</cell><cell>69.6</cell><cell>70.7</cell><cell>86.7</cell><cell>18</cell><cell>96.2</cell><cell>840.5</cell></row><row><cell>EfficientTAM-S/2 (ours)</cell><cell>70.8</cell><cell>88.6</cell><cell>72.1</cell><cell>74.0</cell><cell>87.2</cell><cell>34</cell><cell>109.4</cell><cell>450</cell></row><row><cell>EfficientTAM-S (ours)</cell><cell>71.4</cell><cell>89.2</cell><cell>73.4</cell><cell>74.5</cell><cell>87.2</cell><cell>34</cell><cell>85.0</cell><cell>1010.8</cell></row></table><note><p>Downstream Tasks/Datasets/Models. Tasks and Datasets. We consider zero-shot video tasks including promptable video segmentation and semi-supervised video object segmentation, and zero-shot image tasks to</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc>Interactive semi-supervised video object segmentation results with different prompts. We report averaged J &amp;F zero-shot accuracy across 17 video datasets for each type of prompt.</figDesc><table><row><cell>Method</cell><cell cols="5">1-click 3-click 5-click bounding box ground-truth mask</cell></row><row><cell>SAM+XMem++</cell><cell>56.9</cell><cell>68.4</cell><cell>70.6</cell><cell>67.6</cell><cell>72.7</cell></row><row><cell>SAM+Cutie</cell><cell>56.7</cell><cell>70.1</cell><cell>72.2</cell><cell>69.4</cell><cell>74.1</cell></row><row><cell>SAM 2</cell><cell>64.3</cell><cell>73.2</cell><cell>75.4</cell><cell>72.9</cell><cell>77.6</cell></row><row><cell>EfficientTAM-S/2</cell><cell>60.5</cell><cell>72.8</cell><cell>75.4</cell><cell>71.2</cell><cell>76.8</cell></row><row><cell>EfficientTAM-S</cell><cell>63</cell><cell>74.1</cell><cell>75.7</cell><cell>73.2</cell><cell>77.8</cell></row><row><cell>Model</cell><cell></cell><cell cols="4">SA-23 All SA-23 Image SA-23 Video 14 new Video</cell></row><row><cell>SAM (ViT-B)</cell><cell></cell><cell>55.9 (80.9)</cell><cell>57.4 (81.3)</cell><cell>54.0 (80.4)</cell><cell>54.5 (82.6)</cell></row><row><cell>SAM (ViT-H)</cell><cell></cell><cell>58.1 (81.3)</cell><cell>60.8 (82.1)</cell><cell>54.5 (80.3)</cell><cell>59.1 (83.4)</cell></row><row><cell cols="2">HQ-SAM (ViT-B)</cell><cell>53.9 (72.1)</cell><cell>56.3 (73.9)</cell><cell>50.7 (69.9)</cell><cell>54.5 (75.0)</cell></row><row><cell cols="2">HQ-SAM (ViT-H)</cell><cell>59.1 (79.8)</cell><cell>61.8 (80.5)</cell><cell>55.7 (78.9)</cell><cell>58.9 (81.6)</cell></row><row><cell>SAM 2</cell><cell></cell><cell>61.9 (83.6)</cell><cell>63.2 (83.8)</cell><cell>60.3 (83.3)</cell><cell>69.9 (85.9)</cell></row><row><cell cols="3">EfficientTAM-Ti/2 (ours) 58.6 (82.5)</cell><cell>59.6 (82.8)</cell><cell>57.4 (82.1)</cell><cell>63.4 (84.9)</cell></row><row><cell cols="2">EfficientTAM-Ti (ours)</cell><cell>58.2 (82.6)</cell><cell>59.5 (82.9)</cell><cell>56.5 (82.1)</cell><cell>62.7 (85.0)</cell></row><row><cell cols="3">EfficientTAM-S/2 (ours) 60.5 (82.9)</cell><cell>61.6 (83.2)</cell><cell>59.1 (82.4)</cell><cell>67.8 (85.4)</cell></row><row><cell cols="2">EfficientTAM-S (ours)</cell><cell>60.7 (83.0)</cell><cell>61.7 (83.3)</cell><cell>59.5 (82.6)</cell><cell>67.7 (85.4)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc>Efficient cross-attention variants.</figDesc><table><row><cell cols="6">Cross-Attention MOSE dev DAVIS 2017 val SA-V test</cell></row><row><cell cols="2">equation (6)</cell><cell>76.4</cell><cell cols="2">88.7</cell><cell>73.9</cell></row><row><cell cols="2">equation (5)</cell><cell>76.5</cell><cell cols="2">88.6</cell><cell>74.0</cell></row><row><cell>Resolution</cell><cell>MOSE dev</cell><cell>DAVIS 2017 val</cell><cell>SA-V test</cell><cell>FPS A100</cell><cell>Latency (ms) iPhone 15</cell></row><row><cell>1024 × 1024</cell><cell>76.5</cell><cell>89.2</cell><cell>74.5</cell><cell>85</cell><cell>1010.8</cell></row><row><cell>512 × 512</cell><cell>74.8</cell><cell>87.2</cell><cell>71.5</cell><cell>134</cell><cell>80.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6</head><label>6</label><figDesc>plain, non-hierachical image encoder for building efficient video object segmentation and track anything model, EfficientTAM. With a vanilla lightweight ViT image encoder, EfficientTAM demonstrated competing image and video segmentation capabilities as hierarchical image encoder while being more efficient and deployable on mobile devices. We also proposed an efficient memory module with faster cross-attention, leveraging the locality of spatial memory embeddings. The efficient memory module further improves EfficientTAM's accuracy-efficiency tradeoff on video segmentation and tracking anything. Extensive experiments on semi-supervised video object segmentation, promptable video segmentation, and the segment anything tasks consistently validate the advantages of our EfficientTAM. Our preliminary work suggests that EfficientTAM has many potential applications for on-device tracking anything.Ablation study on the design of memory cross-attention in EfficientTAM.</figDesc><table><row><cell cols="4">Object Pointers MOSE dev DAVIS 2017 val SA-V test</cell></row><row><cell>No</cell><cell>75.8</cell><cell>89.0</cell><cell>72.1</cell></row><row><cell>Yes</cell><cell>76.5</cell><cell>89.2</cell><cell>74.5</cell></row><row><cell>Pooling</cell><cell cols="3">MOSE dev DAVIS 2017 val SA-V test</cell></row><row><cell>Memory tokens</cell><cell>74.5</cell><cell>87.6</cell><cell>71.7</cell></row><row><cell>Spatial tokens only</cell><cell>76.5</cell><cell>88.6</cell><cell>74.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7</head><label>7</label><figDesc>Ablation study on taking care of the memory token structure for efficient cross-attention in EfficientTAM.sum up to 1, and e(•) denotes exp(•). For each row of the cross-attention matrix, we have,</figDesc><table><row><cell>Cij = D Sii (e(</cell><cell>q i √ d</cell><cell>kT 1 )ṽ 1 + . . . e(</cell><cell>q i √ d</cell><cell>kT 1 )ṽ 1</cell><cell>+ • • • + e(</cell><cell>q i √ d</cell><cell>kT 1 )ṽ wh + . . . e(</cell><cell>q i √ d</cell><cell>kT 1 )ṽ wh</cell><cell>+e(</cell></row><row><cell></cell><cell></cell><cell>lw×l h</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>lw×l h</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8</head><label>8</label><figDesc>Cross-Attention MOSE dev DAVIS 2017 val SA-V testComparing with local windowed attention.</figDesc><table><row><cell>Local-windowed</cell><cell>75.4</cell><cell>88.6</cell><cell>72.4</cell></row><row><cell>Pooling</cell><cell>76.5</cell><cell>88.6</cell><cell>74.0</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6">Acknowledgments</head><p>We thank <rs type="person">Chaitanya Ryali</rs> for valuable discussions and data access support. We thank <rs type="person">Ronghang Hu</rs> for suggestions. Thanks to <rs type="person">Nikhila Ravi</rs> for supporting 1 node of A100 for benchmarking.</p></div>
			</div>
			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p><ref type="url" target="https://yformer.github.io/efficient-track-anything/">https://yformer.github.io/efficient-track-anything/</ref> </p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Efficient Cross-Attention</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>]</head><p>The concatenation of coarse spatial tokens with object pointer tokens is, K = [ Ks ; K p ] ∈ R (n+P )×d and V = [ Vs ; V p ] ∈ R (n+P )×d .</p><p>Lemma 1. For the coarse memory tokens, K and V , queries</p><p>where A = [</p><p>where D S is a L × L diagonal matrix, which normalizes each row of the S matrix such that the row entries </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><surname>Longformer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<title level="m">The long-document transformer</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning what to learn for video object segmentation</title>
		<author>
			<persName><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Järemo Lawin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">August 23-28, 2020. 2020</date>
			<biblScope unit="page" from="777" to="794" />
		</imprint>
	</monogr>
	<note>Proceedings, Part II 16</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Object segmentation by long term analysis of point trajectories</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="282" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Sergi Caelles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevis-Kokitsi</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhua</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordi</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName><surname>Pont-Tuset</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.00557</idno>
		<title level="m">The 2018 davis challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Efficientvit: Lightweight multi-scale attention for high-resolution dense prediction</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muyan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="17302" to="17313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Jun</forename><surname>Cen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kewei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingdong</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.14207</idno>
		<title level="m">Sad: Segment any rgbd</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Semantic segment anything</title>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://github.com/fudan-zvg/Semantic-Segment-Anything" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sam-adapter: Adapting segment anything in underperformed scenes</title>
		<author>
			<persName><forename type="first">Tianrun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lanyun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaotao</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runlong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shangzhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zejian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingyun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Papa</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="3367" to="3375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Masked-attention mask transformer for universal image segmentation</title>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1290" to="1299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Xmem: Long-term video object segmentation with an atkinson-shiffrin memory model</title>
		<author>
			<persName><forename type="first">Kei</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="640" to="658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Modular interactive video object segmentation: Interaction-to-mask, propagation and difference-aware fusion</title>
		<author>
			<persName><forename type="first">Kei</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Wing</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi-Keung</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5559" to="5568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rethinking space-time networks with improved memory coverage for efficient video object segmentation</title>
		<author>
			<persName><forename type="first">Kei</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Wing</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi-Keung</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11781" to="11794" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Tracking anything with decoupled video segmentation</title>
		<author>
			<persName><forename type="first">Seoung</forename><forename type="middle">Wug</forename><surname>Ho Kei Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joon-Young</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1316" to="1326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Putting the object back into video object segmentation</title>
		<author>
			<persName><forename type="first">Seoung</forename><forename type="middle">Wug</forename><surname>Ho Kei Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joon-Young</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="3151" to="3161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Yangming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liulei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanyou</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.06558</idno>
		<title level="m">Segment and track anything</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Rethinking attention with performers</title>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valerii</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyou</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreea</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamas</forename><surname>Sarlos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Afroz</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.14794</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">ELECTRA: Pre-training text encoders as discriminators rather than generators</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="https://openreview.net/pdf?id=r1xMH1BtvB" />
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning the what and how of annotation in video object segmentation</title>
		<author>
			<persName><forename type="first">Thanos</forename><surname>Delatolas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicky</forename><surname>Kalogeiton</surname></persName>
		</author>
		<author>
			<persName><surname>Dim P Papadopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="6951" to="6961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Segment anything model (sam) for digital pathology</title>
		<author>
			<persName><forename type="first">Ruining</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><forename type="middle">W</forename><surname>Remedios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shunxing</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename><forename type="middle">E</forename><surname>Bennett A Landman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lori</forename><forename type="middle">A</forename><surname>Wheless</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><forename type="middle">T</forename><surname>Coburn</surname></persName>
		</author>
		<author>
			<persName><surname>Wilson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.04155</idno>
	</analytic>
	<monogr>
		<title level="m">Assess zero-shot segmentation on whole slide imaging</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mose: A new dataset for video object segmentation in complex scenes</title>
		<author>
			<persName><forename type="first">Henghui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuting</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xudong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Hs Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="20224" to="20234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Sam2long: Enhancing sam 2 for long video segmentation with a training-free memory tree</title>
		<author>
			<persName><forename type="first">Shuangrui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhang</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.16268</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sstvos: Sparse spatiotemporal transformers for video object segmentation</title>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdalla</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parham</forename><surname>Aarabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5912" to="5921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Video segmentation by non-local consensus voting</title>
		<author>
			<persName><forename type="first">Alon</forename><surname>Faktor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multiscale vision transformers</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Haoqi Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karttikeya</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghao</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6824" to="6835" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Video segmentation by tracing discontinuities in a trajectory embedding</title>
		<author>
			<persName><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1846" to="1853" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Editanything: Empowering unparalleled flexibility in image editing and generation</title>
		<author>
			<persName><forename type="first">Shanghua</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijie</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st ACM International Conference on Multimedia</title>
		<meeting>the 31st ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="9414" to="9416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Levit: a vision transformer in convnet&apos;s clothing for faster inference</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12259" to="12269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficient hierarchical graph-based video segmentation</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Grundmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irfan</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 ieee computer society conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2141" to="2148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Dongsheng</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaoning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maryam</forename><surname>Qamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuna</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungkyu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sung-Ho</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Choong</forename><surname>Seon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2305.00278</idno>
		<title level="m">Segment anything model (sam) meets glass: Mirror and transparent objects cannot be easily detected</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Interactive video object segmentation using global and local transfer modules</title>
		<author>
			<persName><forename type="first">Yuk</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeong</forename><surname>Jun Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang-Su</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">August 23-28, 2020. 2020</date>
			<biblScope unit="page" from="297" to="313" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XVII 16</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Videoclick: Video object segmentation with a single click</title>
		<author>
			<persName><forename type="first">Namdar</forename><surname>Homayounfar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Chiu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.06545</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Lvos: A benchmark for large-scale long-term video object segmentation</title>
		<author>
			<persName><forename type="first">Lingyi</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenchao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenzhi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pinxue</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinglun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuyong</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.19326</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Restore anything pipeline: Segment anything meets image restoration</title>
		<author>
			<persName><forename type="first">Jiaxi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Holz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.13093</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Transformers are rnns: Fast autoregressive transformers with linear attention</title>
		<author>
			<persName><forename type="first">A</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Segment anything</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Mintun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhila</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanzi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chloe</forename><surname>Rolland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Gustafson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Spencer</forename><surname>Whitehead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="4015" to="4026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donnie</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Key-segments for video object segmentation</title>
		<author>
			<persName><forename type="first">Jae</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaechul</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristen</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International conference on computer vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1995" to="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Video segmentation by tracking many figure-ground segments</title>
		<author>
			<persName><forename type="first">Fuxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taeyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Humayun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2192" to="2199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Nextvit: Next generation vision transformer for efficient deployment in realistic industrial scenarios</title>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huixia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuefeng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Pan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.05501</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Recurrent dynamic embedding for video object segmentation</title>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="page" from="1332" to="1341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Exploring plain vision transformer backbones for object detection</title>
		<author>
			<persName><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanzi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="280" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Mvitv2: Improved multiscale vision transformers for classification and detection</title>
		<author>
			<persName><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao-Yuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karttikeya</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4804" to="4814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Efficientformer: Vision transformers at mobilenet speed</title>
		<author>
			<persName><forename type="first">Yanyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ju</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Evangelidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="12934" to="12949" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Grounding dino: Marrying dino with grounded pre-training for open-set object detection</title>
		<author>
			<persName><forename type="first">Shilong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianhe</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.05499</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Efficientvit: Memory efficient vision transformer with cascaded group attention</title>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ningxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="14420" to="14430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.12306</idno>
	</analytic>
	<monogr>
		<title level="m">Segment anything in medical images</title>
		<imprint>
			<date type="published" when="2019-06">2019. Jun. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Mobilevit: Light-weight, general-purpose, and mobile-friendly vision transformer</title>
		<author>
			<persName><forename type="first">Sachin</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Video object segmentation using space-time memory networks</title>
		<author>
			<persName><forename type="first">Seoung</forename><surname>Wug Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joon-Young</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seon Joo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9226" to="9235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Fast object segmentation in unconstrained video</title>
		<author>
			<persName><forename type="first">Anestis</forename><surname>Papazoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1777" to="1784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Saliency filters: Contrast based filtering for salient region detection</title>
		<author>
			<persName><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yael</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="733" to="740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="724" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergi</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00675</idno>
		<title level="m">The 2017 davis challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Mobilenetv4-universal models for the mobile ecosystem</title>
		<author>
			<persName><forename type="first">Danfeng</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chas</forename><surname>Leichner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manolis</forename><surname>Delakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Fornoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colby</forename><surname>Banbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengxi</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Berkin</forename><surname>Akin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.10518</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Ded-sam: Adapting segment anything model 2 for dual encoder-decoder change detection</title>
		<author>
			<persName><forename type="first">Junlong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erzhu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">Frano</forename><surname>Rajič</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.01197</idno>
		<title level="m">Segment anything meets point tracking</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">Nikhila</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Gabeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan-Ting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaitanya</forename><surname>Ryali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haitham</forename><surname>Khedr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Rädle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chloe</forename><surname>Rolland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Gustafson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.00714</idno>
		<title level="m">Segment anything in images and videos</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Learning fast and robust target models for video object segmentation</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Jaremo Lawin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7406" to="7415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Hiera: A hierarchical vision transformer without the bells-andwhistles</title>
		<author>
			<persName><forename type="first">Chaitanya</forename><surname>Ryali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan-Ting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Bolya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Po-Yao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaibhav</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arkabandhu</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omid</forename><surname>Poursaeed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="29441" to="29454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Performance and non-adversarial robustness of the segment anything model 2 in surgical video segmentation</title>
		<author>
			<persName><forename type="first">Yiqing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyuan</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Unberath</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.04098</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Efficient attention: Attention with linear complexities</title>
		<author>
			<persName><forename type="first">Zhuoran</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01243</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Explain any concept: Segment anything meets concept-based explanation</title>
		<author>
			<persName><forename type="first">Ao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pingchuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanyuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.10289</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Segment any mesh: Zero-shot mesh part segmentation via lifting segment anything 2 to 3d</title>
		<author>
			<persName><forename type="first">George</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Logan</forename><surname>Ford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Benhaim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.13679</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Can sam segment anything? when sam meets camouflaged object detection</title>
		<author>
			<persName><forename type="first">Lv</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoke</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.04709</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<author>
			<persName><forename type="first">Shehbaz</forename><surname>Tariq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">Estadimas</forename><surname>Arfeto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaoning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyundong</forename><surname>Shin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.02094</idno>
		<title level="m">Segment anything meets semantic communication</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Causal video object segmentation from persistence of occlusions</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vasiliy</forename><surname>Karasev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4268" to="4276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS&apos;17</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems, NIPS&apos;17<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Look before you match: Instance understanding matters in video object segmentation</title>
		<author>
			<persName><forename type="first">Junke</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanxin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yucheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2268" to="2278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Linformer: Self-attention with linear complexity</title>
		<author>
			<persName><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Belinda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04768</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Saliency-aware geodesic video object segmentation</title>
		<author>
			<persName><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3395" to="3402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="568" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Tinyvit: Fast pretraining distillation for small vision transformers</title>
		<author>
			<persName><forename type="first">Kan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinnian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="68" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Scalable video object segmentation with simplified framework</title>
		<author>
			<persName><forename type="first">Qiangqiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="13879" to="13889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Sam2-unet: Segment anything 2 makes strong encoder for natural and medical image segmentation</title>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihuang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuangyi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenxue</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feilong</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.08870</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Nyströmformer: A nyström-based algorithm for approximating self-attention</title>
		<author>
			<persName><forename type="first">Yunyang</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanpeng</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rudrasis</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Glenn</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="14138" to="14148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Efficientsam: Leveraged masked image pretraining for efficient segment anything</title>
		<author>
			<persName><forename type="first">Yunyang</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bala</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lemeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fanyi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Forrest</forename><surname>Iandola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="16111" to="16121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Evaluation of super-voxel methods for early video processing</title>
		<author>
			<persName><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1202" to="1209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Youtube-vos: Sequence-to-sequence video object segmentation</title>
		<author>
			<persName><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingcheng</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="585" to="601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<author>
			<persName><forename type="first">Jinyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingqi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangjing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.11968</idno>
		<title level="m">Track anything: Segment anything meets videos</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Decoupling features in hierarchical propagation for video object segmentation</title>
		<author>
			<persName><forename type="first">Zongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="36324" to="36336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Scalable video object segmentation with identification mechanism</title>
		<author>
			<persName><forename type="first">Zongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Castling-vit: Compressing self-attention via switching towards linear-angular attention at vision transformer inference</title>
		<author>
			<persName><forename type="first">Haoran</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunyang</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingyan</forename><surname>Celine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="14431" to="14442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runseng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoyu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhibo</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.06790</idno>
		<title level="m">Inpaint anything: Segment anything meets image inpainting</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Big bird: Transformers for longer sequences</title>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guru</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avinava</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.14062</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Scaling vision transformers</title>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="12104" to="12113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<author>
			<persName><forename type="first">Chaoning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongshen</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung</forename><forename type="middle">Uk</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sung-Ho</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungkyu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Choong</forename><surname>Seon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2306.14289</idno>
		<title level="m">Faster segment anything: Towards lightweight sam for mobile applications</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Video object segmentation through spatially accurate and temporally dense extraction of primary object regions</title>
		<author>
			<persName><forename type="first">Dong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="628" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Joint modeling of feature, correspondence, and a compressed memory for video object segmentation</title>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutao</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gangshan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.13505</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Sam2-path: A better segment anything model for semantic segmentation in digital pathology</title>
		<author>
			<persName><forename type="first">Mingya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limei</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaohui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingshen</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianping</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.03651</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Deshadow-anything: When segment anything model meets zero-shot shadow removal</title>
		<author>
			<persName><forename type="first">Xiao Feng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><forename type="middle">Wei</forename><surname>Yao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.11715</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longjin</forename><surname>Ran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.20076</idno>
		<title level="m">Evf-sam: Early vision-language fusion for text-prompted segment anything model</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Xu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenchao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongqi</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinglong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinqiao</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.12156</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">Fast segment anything. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">When sam2 meets video camouflaged object segmentation: A comprehensive evaluation and adaptation</title>
		<author>
			<persName><forename type="first">Yuli</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guolei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yawei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Benini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ender</forename><surname>Konukoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.18653</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
