- **Key Model**: Segment Anything Model 2 (SAM 2) - foundational model for unified image and video object segmentation.
- **Core Components**:
  - **Multistage Image Encoder**: Extracts hierarchical frame features.
  - **Memory Mechanism**: Stores past frame contexts for current frame segmentation.
- **Efficiency Issues**: High computation complexity limits real-world applications, especially on mobile devices.
- **Proposed Solution**: EfficientTAMs - lightweight models for high-quality results with low latency and model size.
- **Image Encoder**: Utilizes plain, non-hierarchical Vision Transformer (ViT) (e.g., ViT-Tiny/-Small) to reduce complexity.
- **Memory Module**: Introduces efficient cross-attention leveraging memory spatial token structure to enhance performance.
- **Performance Metrics**:
  - EfficientTAM achieves comparable performance to SAM 2 with ∼2x speedup on A100 and ∼2.4x parameter reduction.
  - On image tasks, EfficientTAM shows ∼20x speedup and ∼20x parameter reduction compared to SAM.
  - EfficientTAM runs at ∼10 FPS on mobile devices (e.g., iPhone 15 Pro Max) for video object segmentation.
- **Evaluation Benchmarks**: Tested on MOSE, DAVIS, LVOS, SA-V for video segmentation, and SA-23 for image segmentation.
- **Results**:
  - EfficientTAM: 74.5% accuracy on SA-V vs. 74.7% for SAM 2.
  - EfficientTAM: 60.7% accuracy for zero-shot image segmentation vs. 59.1% for SAM and 61.9% for SAM 2.
- **Contributions**:
  - Revisiting plain ViT for video object segmentation.
  - Efficient memory cross-attention method.
  - Delivering EfficientTAMs with state-of-the-art quality-efficiency tradeoffs.
- **Memory Attention Module**: Comprises transformer blocks with self-attention, cross-attention, and MLP for temporal dependencies in video.
- **Related Works**: Discusses advancements in video object segmentation, interactive segmentation, and efficient attention mechanisms in Vision Transformers.