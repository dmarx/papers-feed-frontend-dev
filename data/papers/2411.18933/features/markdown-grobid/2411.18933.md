# Efficient Track Anything

## Abstract

## 

Segment Anything Model 2 (SAM 2) has emerged as a powerful tool for video object segmentation and tracking anything. Key components of SAM 2 that drive the impressive video object segmentation performance include a large multistage image encoder for frame feature extraction and a memory mechanism that stores memory contexts from past frames to help current frame segmentation. The high computation complexity of multistage image encoder and memory module has limited its applications in real-world tasks, e.g., video object segmentation on mobile devices. To address this limitation, we propose EfficientTAMs, lightweight track anything models that produce high-quality results with low latency and model size. Our idea is based on revisiting the plain, nonhierarchical Vision Transformer (ViT) as an image encoder for video object segmentation, and introducing an efficient memory module, which reduces the complexity for both frame feature extraction and memory computation for current frame segmentation. We take vanilla lightweight ViTs and efficient memory module to build EfficientTAMs, and train the models on SA-1B and SA-V datasets for video object segmentation and track anything tasks. We evaluate on multiple video segmentation benchmarks including semisupervised VOS and promptable video segmentation, and find that our proposed EfficientTAM with vanilla ViT perform comparably to SAM 2 model (HieraB+SAM 2) with ∼2x speedup on A100 and ∼2.4x parameter reduction. On segment anything image tasks, our EfficientTAMs also perform favorably over original SAM with ∼20x speedup on A100 and ∼20x parameter reduction. On mobile devices such as iPhone 15 Pro Max, our EfficientTAMs can run at ∼10 FPS for performing video object segmentation with reasonable quality, highlighting the capability of small models for on-device video object segmentation applications.

## Introduction

Segment Anything Model 2 (SAM 2) [(Ravi et al., 2024](#b57)) is a foundational model for unified image and video object segmentation, achieving state-of-the-art performance in various segmentation tasks such as zero-shot image segmentation [(Kirillov et al., 2023;](#b35)[Chen et al., 2023a;](#)[Deng et al., 2023;](#b18)[Chen et al., 2023b)](#), semi-supervised video object segmentation [(Pont-Tuset et al., 2017;](#b53)[Xu et al., 2018;](#b79)[Oh et al., 2019;](#b49)[Bhat et al., 2020;](#b1)[Robinson et al., 2020;](#b58)[Li et al., 2022b;](#)[Yang and Yang, 2022;](#b81)[Cheng and Schwing, 2022;](#b8)[Zhang et al., 2023b;](#)[Wang et al., 2023;](#b69)[Wu et al., 2023;](#b74)[Cheng et al., 2024;](#)[Yang et al., 2024)](#b82), interactive video segmentation [(Caelles et al., 2018;](#b3)[Heo et al., 2020;](#b30)[Cheng et al., 2021a;](#)[Homayounfar et al., 2021;](#b31)[Yang et al., 2023;](#b14)[Cheng et al., 2023b;](#)[Rajič et al., 2023;](#b56)[Cheng et al., 2024;](#)[Delatolas et al., 2024)](#b17), and other real-world applications [(Zhang et al., 2024b;](#)[Xiong et al., 2024a;](#)[Shen et al., 2024;](#b60)[Zhang et al., 2024a;](#)[Ding et al., 2024;](#b20)[Qiu et al., 2024;](#b55)[Tang et al., 2024;](#b63)[Zhou et al., 2024)](#b94). SAM 2 uses a multistage image encoder to extract hierarchical frame features and introduces a memory module to cross-attend to both current frame features and stored memories from observed frames for consistent object segmentation across frames and interactive tracking in videos. Despite these advantages, SAM 2 is not efficient for mobile deployment, particularly because the large image encoder (e.g., HieraB+) and memory module are expensive. The default image encoder of SAM 2, HieraB+ [(Ryali et al., 2023)](#b59), is parameter inefficient, e.g., ∼80M parameters. While SAM 2 provides a tiny version, it has a running time of 43.8 FPS comparable to 47.2 FPS of the default SAM 2 model, due to the hierarchical image encoder. Additionally, that the memory tokens (e.g., a concatenation of spatial memory tokens and object pointer tokens) are long, e.g., ∼30K, which hurts the efficiency of the memory module with cross-attention.

In this paper, we revisit plain, nonhierarchical image encoder for video object segmentation and tracking anything. We propose using a lightweight vanilla ViT image encoder (e.g., ViT-Tiny/-Small [(Touvron et al., 2021)](#b67)) as EfficientSAMs [(Xiong et al., 2024b)](#) did to reduce the complexity of SAM 2 while maintaining decent performance. Further, we propose an efficient cross-attention method for accelerating the memory module. This is achieved by leveraging the underlying structure of memory spatial tokens. We observed that the memory spatial tokens have strong locality and a coarser representation of memory spatial tokens can be a good proxy for performing cross-attention. We show that this yields a good alternative to the original memory module.

To evaluate our method, we conduct extensive experiments across video and image segmentation benchmarks, including MOSE, DAVIS, LVOS, and SA-V for video segmentation, and SA-23 for image segmentation. Our EfficientTAM outperforms strong semi-supervised video object segmentation methods such as Cutie-base, XMem, and DEVA while being more efficient. Compared with SAM 2, our EfficientTAM is comparable, e.g., 74.5% vs 74.7% on SA-V test dataset, with ∼ 2x reduced FPS. On image segmentation benchmark, SA-23, our EfficientTAM achieves 60.7% accuracy for zero-shot image segmentation compared to 59.1% accuracy for SAM and and 61.9% for SAM 2. We also benchmarked our EfficientTAM model on iPhone 15 Pro Max, which can run ∼ 10 frames per second with reasonable video segmentation performance.

Our main contributions can be summarized as follows:

• We revisit using plain, non-hierarchical image encoder, ViT-Tiny/-Small for video object segmentation and show that vanilla ViT can achieve competing performance comparing to SAM 2 with hierarchical image encoder. • We propose an efficient memory cross-attention by exploiting the underlying memory spatial token structure and demonstrate the favorable performance. • We deliver EfficientTAMs, lightweight video object segmentation and track anything models with state-ofthe-art quality-efficiency tradeoffs (figure [1](#fig_0)), which is complementary to SAM 2 for practical deployment.

## Related Work

Video Object Segmentation (VOS) is a fundamental task in computer vision, segments objects of interest from the background and tracks target objects in a video. In the unsupervised setting [(Grundmann et al., 2010;](#b28)[Brox and Malik, 2010;](#b2)[Lee et al., 2011;](#b37)[Xu and Corso, 2012;](#b78)[Fragkiadaki et al., 2012;](#b25)[Perazzi et al., 2012;](#b51)[Zhang et al., 2013;](#b88)[Li et al., 2013;](#b38)[Papazoglou and Ferrari, 2013;](#b50)[Faktor and Irani, 2014;](#b23)[Wang et al., 2015;](#b71)[Taylor et al., 2015;](#b66)[Perazzi et al., 2016)](#b52), VOS models segment salient objects without a reference mask. In the semi-supervised setting [(Pont-Tuset et al., 2017;](#b53)[Xu et al., 2018;](#b79)[Oh et al., 2019;](#b49)[Bhat et al., 2020;](#b1)[Robinson et al., 2020;](#b58)[Li et al., 2022b;](#)[Yang and Yang, 2022;](#b81)[Cheng and Schwing, 2022;](#b8)[Zhang et al., 2023b;](#)[Wang et al., 2023;](#b69)[Wu et al., 2023;](#b74)[Cheng et al., 2024;](#)[Yang et al., 2024)](#b82), VOS requires tracking and segmenting objects based on a first-frame mask of target objects. For interactive video object segmentation (iVOS) [(Caelles et al., 2018;](#b3)[Heo et al., 2020;](#b30)[Cheng et al., 2021a;](#)[Homayounfar et al., 2021;](#b31)[Yang et al., 2023;](#b14)[Cheng et al., 2023b;](#)[Rajič et al., 2023;](#b56)[Cheng et al., 2024;](#)[Delatolas et al., 2024)](#b17), iVOS models perform object segmentation in videos (masklets) with user guidance, e.g., clicks, bounding boxes, scribbles. In SAM 2 [(Ravi et al., 2024)](#b57). Semi-supervised VOS and iVOS have been extended to promptable visual segmentation (PVS), where the model can be interactively prompted with different types of inputs such as clicks, boxes, and masks on any frame in a video for segmenting and tracking a valid object.

Vision Transformers (ViTs) have achieved huge success on various vision tasks including image classification [(Dosovitskiy et al., 2020)](#b21), object detection [(Li et al., 2022c)](#), image segmentation [Cheng et al. (2022)](#b8); [Kirillov et al. (2023](#b35)[), video classification (Fan et al., 2021)](#), and video object segmentation [(Duke et al., 2021;](#b22)[Yang et al., 2023)](#b14). The original ViT family scales from the efficient ViT-Tiny up to ViT-Huge, with a plain, non-hierarchical architecture. There are also hierarchical vision transformers that combine transformers with hierarchical stage structure, such as Swin [(Liu et al., 2021)](#b46), [MViT (Fan et al., 2021;](#)[Li et al., 2022d)](#), PViT [(Wang et al., 2021)](#b72), and Hiera [(Ryali et al., 2023)](#b59). While being successful, hierarchical models are usually slower than their plain ViT counterparts for practical deployment [(Ryali et al., 2023)](#b59). Combining ViT with convolutions [(LeCun et al., 1989)](#b36) has been explored for fast hybrid models such as MobileViT [(Mehta and Rastegari, 2021)](#b48), LeViT [(Graham et al., 2021)](#b27), EfficientFormer [(Li et al., 2022e)](#), Next-ViT [(Li et al., 2022a)](#), Tiny-ViT [(Wu et al., 2022)](#b73), Castling-ViT [(You et al., 2023)](#b83), EfficientViT [(Liu et al., 2023b)](#), and MobileNetv4 [(Qin et al., 2024)](#b54). This line of progression towards building efficient ViTs is orthogonal to our EfficientTAM work towards building efficient video object segmentation. Following SAM [(Kirillov et al., 2023)](#b35) and EfficientSAMs [(Xiong et al., 2024b)](#), we are pursuing plain ViT backbones for efficient video object segmentation and track anything tasks.

Efficient Attention. The field has developed methods to reduce the quadratic cost of standard self-attention with respect to input sequence length [Vaswani et al. (2017)](#b68). Local windowed attention has been applied in [Beltagy et al. (2020)](#b0); [Zaheer et al. (2020)](#b85) for reducing the complexity of self-attention. In [Shen et al. (2018)](#b61); [Katharopoulos et al. (2020)](#b34), a linear dot product approximation is proposed to linearize the softmax matrix in self-attention by heuristically separating keys and queries. In [Choromanski et al. (2020)](#b15), the Performer model uses random features to approximate self-attention, achieving linear time and memory cost. Nyströmformer in [Xiong et al. (2021)](#b76) makes use of the Nyström method to approximate self-attention with a linear cost. Linformer [Wang et al. (2020)](#b70) shows that self-attention is low-rank, which can be approximated by learning linear projection matrices for the keys and values. The approach of [(Liu et al., 2023b;](#)[You et al., 2023)](#b83) leverages the associative property of matrix multiplication for efficient attentions in vision transformers. This direction has shown success and has achieved decent performance on vision tasks. However, in preliminary experiments we found that these methods underperformed in a memory cross-attention module when adapted for efficiency improvement.

Segment Anything Model. SAM [(Kirillov et al., 2023)](#b35) is a vision foundation model that can segment any object in an image using interactive prompts such as points and bounding boxes. SAM has demonstrated remarkable zero-shot transfer performance and high versatility for many vision tasks including a broad range of segmentation applications [(Chen et al., 2023a;](#)[Cen et al., 2023;](#b5)[Deng et al., 2023;](#b18)[Chen et al., 2023b)](#), in-painting [(Yu et al., 2023)](#b84), image restoration [(Jiang and Holz, 2023)](#b33), image editing [(Gao et al., 2023)](#b26), image shadow removal [(Zhang et al., 2023c)](#), medical image segmentation (Ma and [Wang, 2023)](#), camouflaged object detection [(Tang et al., 2023)](#b64), transparent object detection [(Han et al., 2023)](#b4), concept-based explanation [(Sun et al., 2023)](#b62), semantic communication [(Tariq et al., 2023)](#b65), and object tracking [(Cheng et al., 2023b;](#)[Yang et al., 2023)](#b14). The strong ability on image segmentation with flexible prompts motivates the extension of SAM for video object segmentation and track anything. Track Anything Model (TAM) [(Yang et al., 2023)](#b14) combines SAM and XMem [Cheng and Schwing (2022)](#b8) for interactive video object tracking and segmentation with SAM for frame segmentation and XMem for tracking. SAM-Track [(Cheng et al., 2023b)](#) perform object tracking and segmentation in videos by combining SAM [(Kirillov et al., 2023)](#b35), DeAOT [(Yang and Yang, 2022)](#b81), and Grounding-Dino [(Liu et al., 2023a)](#). The latest SAM 2 [(Ravi et al., 2024)](#b57) extended SAM for video segmentation through a hierarchical image encoder for frame embeddings and a memory module that conditions current frame embeddings on past frames. Motivated by mobile app use-cases and computationally-constrained applications, recent works have reduced the computational cost of SAM, such as MobileSAM [(Zhang et al., 2023a)](#), FastSAM [(Zhao et al., 2023)](#b93), and EfficientSAM [(Xiong et al., 2024b)](#). The present paper focuses on improving the efficiency challenges of SAM 2 for practical deployment of video object segmentation and track anything.

3 Approach

## Preliminaries

Segment Anything. SAM [(Kirillov et al., 2023)](#b35) contains a ViT image encoder and a prompt-guided mask decoder. The encoder takes an image and outputs image embeddings. Then the decoder takes the image embeddings and a prompt, which allows cutting out any object from the background in an image. SAM is trained on an image dataset of over 1B masks.

Segment Anything 2. The architecture of segment anything 2 (SAM 2) [(Ravi et al., 2024)](#b57) largely follows SAM, which consists of a hierarchical image encoder, a prompt-guided lightweight mask decoder, and a new memory mechanism. SAM 2 uses a hierarchical image encoder, Hiera [(Ryali et al., 2023)](#b59), to produce image embeddings for each frame. The stride 16 and 32 features from Stage 3 and 4 are used for the memory module.

The stride 4 and 8 features from Stage 1 and Stage 2 are not used in the memory module but are fed to upsampling layers in the mask decoder for generating segmentation masks. For stable object tracking, SAM 2 employs a memory mechanism consisting of a lightweight memory encoder, a lightweight memory bank, and a memory attention module. It stores information from past frames and uses the memory attention module to perform cross-attention between the stored memory in the memory bank and current frame features, thereby understanding temporal dependencies in video.

The memory attention module consists of a stack of transformer blocks. Each block contains self-attention, cross-attention, and MLP. The first transformer block takes the image embedding from the current frame as an input. The core component of each transformer block, cross-attention, integrates the current frame embedding and the memory stored in memory bank to produce an embedding with temporal correspondence information. For memory tokens, it includes two parts, the spatial embedding tokens from memory encoder and the object-level pointer tokens from mask decoder. Let us assume the number of spatial tokens is n, the number of object-level pointer tokens is P , and d m is the channel dimension, memory tokens can be formulated as

$M b = M s ∈ R n×dm M p ∈ R P ×dm .$Let L be the number of tokens and d q be the dimension of each token for input frame features after selfattention, X ∈ R L×dq . The input sequence X ∈ R L×dq is linearly projected to input queries Q ∈ R L×d , and the memory tokens, M b ∈ R (n+P )×dm are linearly projected to keys K ∈ R (n+P )×d , and values V ∈ R (n+P )×d respectively, where d is the embedding dimension of queries, keys, and values. The scaled dot-product cross attention mechanism applied on the queries Q, keys K, values V can be formally written as,

$C(Q, K, V ) = softmax QK T √ d V,(1)$where the softmax operation is applied row-wise. A single head cross attention is used in the memory module.

In later discussion, we also consider keys and values as memory tokens for simplification.

Efficiency Bottleneck. Despite the advantages of the hierarchical image encoder for multiscale frame feature extraction and cross-attention for integrating current frame features with stored memory, it poses the challenges for practical deployment of SAM 2. The inefficient SAM 2 (tiny) even shows comparable FPS to the base SAM 2, 47.2 FPS vs 43.8 FPS due to the hierarchical design of the image encoder and the use of hierarchical features, which also makes SAM 2 challenging to deploy on mobile devices. Moreover, the number of tokens in keys and values for performing cross-attention in the memory module are super long, e.g., 30K. It leads to a large computation and memory cost when performing cross-attention, which becomes the efficiency bottleneck of the memory module for real-world deployment.

Mask Decoder Memory Bank Pooling Object Pointer Spatial Embedding xL Prompt Encoder Prompt Memory encoder Vanilla ViT Encoder Efficient Memory Cross-Attention Video sequence  At right, we visualize the difference between original cross-attention of equation ( [1](#formula_1)) and efficient cross-attention of equation ( [5](#formula_7)); the relative error w.r.t original cross-attention is 0.03 under Frobenius norm.

## Efficient Video Object Segmentation and Track Anything

We now address the efficiency issue of SAM 2 for building efficient video object segmentation and track anything model, EfficientTAM. Motivated by the high quality segmentation performance of SAM and EfficientSAM, we revisit using plain, non-hierarchical lightweight image encoders such as ViT-Small/ViT-Tiny, for frame feature extraction. We found that the use of vanilla ViT for frame feature extraction makes EfficientTAM highly efficient and deployable on mobile devices. Further, we introduce an efficient memory module to reduce the computation and memory cost by proposing an efficient cross-attention operation. Based on these two designs, we build efficient video object segmentation and track anything model by largely following SAM2. figure [2](#fig_1) illusrates an overview of our proposed EfficientTAM.

Efficient Image Encoder. The image encoder's role is to produce feature embeddings for each high-resolution frame. We use a SAMI [(Xiong et al., 2024b)](#) pretrained vanilla ViT image encoder [(Dosovitskiy et al., 2020;](#b21)[Touvron et al., 2021)](#b67) to extract frame features. Differing from the image encoder of SAM 2, our image encoder provides a single-scale feature map and no other features in the mask decoder are added to the upsampling layers during decoding for segmentation mask generation. We adopt the lightweight image encoders ViT-Small and ViT-Tiny with a 16 × 16 patch size. Following [(Li et al., 2022c)](#), we use 14 × 14 non-overlapping windowed attention and 4 equally-spaced global attention blocks to efficiently extract features from high-resolution frames. Our image encoder outputs a single-scale feature embedding with a 16x reduced resolution, which takes high-resolution (e.g., 1024 × 1024) frames as input and transforms it into a dense embedding of downscaled size 64 × 64.

Efficient Memory Module. The memory module leverages information from previous frames to facilitate consistent object tracking. Cross-attention is a major efficiency bottleneck of the memory module in SAM 2 [(Ravi et al., 2024)](#b57) due to its long memory token sequence. We now discuss how exploiting the underlying structure of memory tokens -local smoothness (strong locality) within spatial memory tokens -can yield a more efficient cross-attention.

Consider two consecutive memory spatial tokens, k i and k i+1 , local smoothness implies that ||k i -k i+1 || 2 2 ≤ c K n 2 , for i = 1, . . . , n -1, where c K is a positive constant. This suggests that given a sufficient small local window, l w × l h , using a single token to represent other tokens in the homogeneous window may provide a coarser representation of the full set of memory spatial tokens K s as Ks . We can construct a good surrogate of K s with the same size, Ks , from Ks by repeating the single token in each window l w × l h times. Under the smoothness assumption, Ks will not be far from K s . Empirically, we observed that a coarser representation of spatial memory tokens is good surrogate of the full spatial memory tokens. figure [3](#fig_2) confirms the coarser representation of input keys and values are close to the original keys and values of cross-attention in the memory module.

Utilizing highly correlated neighboring tokens in cross-attention, we perform average pooling to efficiently compute a coarser representation for keys K and values V in our model. For input spatial tokens K s = [k 11 , . . . , k 1h ; . . . ; k w1 , . . . , k wh ] where w ×h is the resolution size, we divide the n = w ×h tokens into k = w × h rectangular pooling regions and compute the average token of each region. For simplicity, we assume w is divisible by w and h is divisible by h. Denote l w = w w , l h = h h . Ks and Ṽs can be computed by averaging each region as,

$kij = (i+1)×lw p=i×lw+1 (j+1)×l h q=j×l h +1 k pq l w × l h , ṽij = (i+1)×lw p=i×lw+1 (j+1)×l h q=j×l h +1 v pq l w × l h ,(2)$$where i = 1, • • • , w, j = 1, • • • , h.$This token-pooling scheme requires a single scan of the tokens leading to an efficient coarse token generation. We find that using averaging pooling with window size, 2 × 2, is sufficient to ensure a good approximation for spatial memory tokens.

Assume Ks is a coarser representation of memory spatial keys, K s , we can construct a good surrogate of K s ∈ R n×d with the same size, Ks ∈ R n×d from Ks ∈ R wh ×d by stacking each ki , i = 1, . . . , wh , l w × l h times, which can be written as, Ks = [ k1 ; . . . ; k1

lw×l h ; k2 ; . . . ; k2 lw×l h ; . . . ; k wh ; . . . ; k wh lw×l h ] Similarly, we stack each ṽi , i = 1, . . . , wh , l w × l h times to construct Vs ∈ R n×d as a good surrogate of values, V s ∈ R n×d , which can be written as, Vs = [ṽ1; . . . ; ṽ1 lw×l h ; ṽ2; . . . ; ṽ2 lw×l h ; . . . ; ṽ wh; . . . ; ṽ wh lw×l h

## ]

Then we concatenate this coarse spatial tokens with object pointer tokens, K = [ Ks ; K p ] ∈ R (n+P )×d and V = [ Vs ; K p ] ∈ R (n+P )×d , for a good surrogate of original memory tokens, K and V . For the coarse memory tokens, K and V , we have,

$softmax Q KT √ d V = softmax (A) Ṽ ,(3)$where A = [

$Q KT s √ d + ln (l w × l h ), QK T p √ d ] ∈ R L×( wh +P ) , Ṽ = [ Ṽs ; V p ] ∈ R ( wh +P )×d .$We provide a proof of equation ( [7](#)) in the appendix. Since K and V are good surrogate of K and V respectively, we obtain a good surrogate of the original cross-attention, softmax

$QK T √ d V in equation (1), C(Q, K, V ) = softmax Q KT √ d V .(4)$With equation ( [7](#)), we have an efficient version of cross-attention,

$C(Q, K, V ) = softmax(A) Ṽ . (5$$)$Link to efficient cross-attention variants. Interestingly, we can find some cross-attention variants based on our proposed efficient cross-attention in equation ( [5](#formula_7)). We notice there is a constant for balancing the attention score between coarse spatial tokens and object pointer tokens, avoiding reducing the attention to spatial tokens after pooling. If we remove this constant, it can lead to a linformer variant using averaging pooling to replace the learnable projection. Instead of removing the constant, we add it to keys for regularizing the attention between coarse spatial tokens and object pointer tokens in equation ( [6](#formula_9)), for obtaining another variant.

$C(Q, K, V ) = softmax Q KT √ d Ṽ ,(6)$where

$K = [ Ks + ln (l w × l h ), K p ] ∈ R ( wh +P )×d .$It is feasible to achieve a good surrogate of the original cross-attention because spatial memory embeddings have strong locality. Our efficient cross-attention is close to the original cross-attention, visualized in figure [3](#fig_2).

## Experiments

## Experimental Setting

Pretraining. The SA-1B dataset consists of 11M diverse, high resolution images with 1.1B high-quality segmentation masks. Similar to [(Ravi et al., 2024)](#b57), we pretrain our EfficientTAM without memory components on SA-1B dataset [(Kirillov et al., 2023)](#b35) for 90k steps. Our ViT image encoder is initialized from pre-trained ViTs [(Xiong et al., 2024b)](#) . We use the AdamW optimizer [(Loshchilov and Hutter, 2019)](#b47) with a momentum, (β 1 = 0.9, β 2 = 0.999), a global batch size of 256, and a initial learning rate of 4e -4. The learning rate is decayed by a reciprocal square root learning rate schedule [(Zhai et al., 2022)](#b86) with 1k iterations linear warmup and 5k iterations linear cooldown. We set weight decay to 0.1. We do not apply drop path for our image encoder. Layer-wise decay [(Clark et al., 2020)](#b16) is set to 0.8. We apply horizontal flip augmentation and resize the input image resolution to 1024 × 1024. We restrict our training to 64 masks per image. Our models are pre-trained on 256 A100 GPUs with 80GB GPU memory with a linear combination of focal and dice loss for mask prediction (e.g., a ratio of 20:1). Bfloat16 is used during the training.

Full Training Datasets. Following [(Ravi et al., 2024)](#b57), we train our EfficientTAM including memory components on SA-V dataset [(Ravi et al., 2024)](#b57) and a 10% subset of SA-1B [(Kirillov et al., 2023)](#b35). SA-V is a large-scale and diverse video segmentation dataset, including 51K videos captured across 47 countries and 600K mask annotations covering whole objects and parts. SA-V video resolution ranges from 240p to 4K and duration ranges from 4 seconds to 138 seconds. Unlike SAM 2, we do not use other open-source datasets or internal datasets during our training for a fair comparison with baselines.

Full Training Implementation Details. Similar to [(Ravi et al., 2024)](#b57), we train our EfficientTAM for 300k steps after pretraining. We use the AdamW optimizer [(Loshchilov and Hutter, 2019)](#b47) with a momentum, (β 1 = 0.9, β 2 = 0.999), a batch size of 256, and a initial learning rate of 6e -5 for image encoder and 3e -4 for other components of the model. The learning rate is decayed by a cosine schedule with 15k iterations linear warmup. We set weight decay to 0.1. We do not apply drop path for our image encoder. Layer-wise decay [(Clark et al., 2020)](#b16) is set to 0.8. We apply horizontal flip image augmentation and resize the input image resolution to 1024 × 1024. For video, we apply horizontal flip augmentation, affine transformation with degree 25 and shear 20, color jittering with brightness 0.1, contrast 0.03, saturation 0.03, gray scale augmentation with a probability of 0.05, We restrict our training to 64 masks per image and 3 masks per frame for video. Our models are trained on 256 A100-80G GPUs with a linear combination of focal and dice losses for mask prediction, mean-absolution-error loss for IoU prediction, and cross-entropy loss for object prediction. The ratio for the linear combination loss is 20:1:1:1. Bfloat16 is used for training. demonstrate the competing capabilities of EfficientTAM on image and video segmentation. For zero-shot image tasks, we evaluate EfficientTAM on 37 datasets including 23 datasets of SA-23 [(Kirillov et al., 2023)](#b35) and 14 video datasets introduced in [(Ravi et al., 2024)](#b57). For zero-shot video tasks, we evaluate our EfficientTAM on 9 densely annotated datasets for promptable video segmentation. We use 17 video datasets to evaluate zero-shot accuracy under interactive semi-supervised VOS setting using different prompts. For the standard semi-supervised VOS setting where a ground-truth mask on the first frame is provided, MOSE [(Ding et al., 2023)](#b19), DAVIS2017 [(Pont-Tuset et al., 2017)](#b53), LVOS [(Hong et al., 2024)](#b32), SA-V [(Ravi et al., 2024)](#b57), and YTVOS [(Xu et al., 2018)](#b79) are used to measure the VOS accuracy. We refer readers to [(Kirillov et al., 2023;](#b35)[Ravi et al., 2024)](#b57) for the details of these datasets. Models. We use our EfficientTAM for zero-shot image and video tasks.

Baselines and Evaluation Metrics. Baselines. For the standard semi-supervised VOS task, where the first-frame mask is provided, we compare the performance of our EfficientTAM with SAM 2 [(Ravi et al., 2024)](#b57), Cutiebase [(Cheng et al., 2024)](#), DEVA [(Cheng et al., 2023a)](#), XMem [(Cheng and Schwing, 2022)](#b8), etc. For the zero-shot promptable video segmentation task and the interactive semi-supervised video object segmentation task using different prompts, we compare our method with SAM2 [(Ravi et al., 2024)](#b57), SAM+XMem++ [(Ravi et al., 2024)](#b57), and SAM+Cutie [(Ravi et al., 2024)](#b57). For zero-shot image segmentation task, we compare with SAM [(Kirillov et al., 2023)](#b35) and SAM2 [(Ravi et al., 2024)](#b57). Note that we use the opensource version of SAM 2 (without training on MOSE/LVOS/YTVOS) for comparison. We also acknowledge the very recent release of SAM 2.1 trained with long memory contexts. Evaluation Metrics. We evaluate our method and all baselines using the accuracy metrics of the combined J (region similarity)&F(contour accuracy), for zero-shot video segmentation tasks; mIoU (mean intersection over union) for zero-shot image segmentation tasks. For efficiency metrics, we compare the number of model parameters or inference throughput on GPU (e.g, A100) and latency on mobile devices (e.g., iPhone 15 Pro Max). We follow SAM 2 [(Ravi et al., 2024)](#b57) to report metrics. When providing main results on MOSE, LVOS and YTVOS, we submit to their benchmarking servers to evaluate on, MOSE val, LVOS val, and YTVOS2019 val, for final performance. For ablation studies, we evaluate on a MOSE development set, MOSE dev with 200 randomly-sampled videos from the MOSE training split [(Ravi et al., 2024)](#b57).

## Main Results

Standard Semi-Supervised Video Object Segmentation. Semi-supervised video object segmentation is the process of object segmentation and tracking in a video based on a ground-truth mask on the first frame. We follow SAM 2 [(Ravi et al., 2024)](#b57) and report accuracy of our methods on this standard semi-supervised video object segmentation task. We also report latency on a single A100 GPU with a batch size of 1. We evaluate EfficientTAMs with different image encoders, ViT-Tiny and ViT-Small, and memory modules, original memory block and efficient memory block with a 2 × 2 window pooling for a trade-off between efficiency and accuracy. EfficientTAM-S denotes EfficientTAM using a ViT-Small image encoder and the original memory block, and EfficientTAM-S/2 denotes EfficientTAM with a ViT-Small image encoder and efficiency memory block with a 2 × 2 window pooling. table 1 compares our EfficientTAM with VOS baselines including SAM 2 [(Ravi et al., 2024)](#b57), Cutie-base [(Cheng et al., 2024)](#), and XMem [(Cheng and Schwing, 2022)](#b8). On SA-V test, our EfficientTAM-S achieves 74.5 J &F, outperforming Cutie-base, Cutie-base+, and XMem by 12.2, 12.9, and 14.4, respectively. On long-term video object segmentation benchmark, LVOS, we can also see that Our EfficientTAM-S outperform Cutie-base and XMem by a large margin. Notice that our EfficientTAM-S only underperforms SAM 2 by < 2 J &F or G across 5 video benchmarks with ∼2x speedup and ∼2.4x fewer parameters. Further, EfficientTAM with efficient memory attention performs slightly worse than the one with original memory attention, but with much speedup, especially on mobile devices, >2x reduced latency on iPhone 15. For example, EfficientSAM-S achieves 74.5 J &F on SA-V test with 1010.8 ms running time per frame on iPhone 15. EfficientSAM-S/2 with efficient cross-memory attention obtain 74.0 J &F with only 450 ms. These results show the extraordinary benefits of EfficientTAMs for semi-supervised video object segmentation and validate the advantages of our methods for practical deployment. Promptable Video Segmentation. Similar to SAM 2 [(Ravi et al., 2024)](#b57), we evaluate promptable video segmentation using two settings, offline evaluation and online evaluation. For offline evaluation, we make multiple passes through a video to annotate frames w.r.t. the largest model error. For online evaluation, we make a single pass through the video to annotate frames. 3 clicks per frame are used for the evaluations on 9 densely annotated video datasets including EndoVis, ESD, LVOSv2, LV-VIS, UVO, VOST, PUMaVOS, Virtual KITTI 2, and VIPSeg. Average J &F accuracy over 1, . . . , 8 interacted frames is reported. figure [4](#fig_3) shows the comparison between our method and strong baselines including SAM 2, SAM + XMem++, and SAM + Cutie. EfficientTAM outperforms SAM + XMem++ and SAM + Cutie for both evaluation settings. EfficientTAM also reduces the gap between SAM 2 for offline and online settings. Specifically, with 8 annotated frames with 3-click, EfficientTAM-S and EfficientTAM-S/2 achieve ∼ 82 J &F in average for offline evaluation setting and ∼ 81 J &F in average for online evaluation, outperforming SAM + XMem++, and SAM + Cutie by >3 J &F and reducing the gap of SAM 2. This set of experiments further validate the effectiveness of our EfficientTAM on promptable video segmentation.

Interactive Semi-Supervised Video Object Segmentation. We also evaluate our method on the interactive semi-supervised video object segmentation task with click, box, or mask prompts provided only on the first Table [3](#) Segment anything results on SA-23 benchmark [(Kirillov et al., 2023)](#b35) and 14 new video benchmark [(Ravi et al., 2024)](#b57). The average 1-click (5-click) mIoU is reported.

frame by following SAM 2. In table 2, we report the average J &F accuracy over 17 video datasets for each type of prompt. We observe that EfficientTAM outperforms SAM + XMem++, and SAM + Cutie with different input prompts. We also notice the reduced gap between EfficientTAM and SAM 2. With 1 click, our EfficientTAM-S obtain 63 J &F accuracy, with a 6 J &F gain over SAM + XMem++ and SAM + Cutie and a slight loss, 1.3 J &F comparing to SAM 2. In summary, EfficientTAM performs favorably on the interactive semi-supervised VOS task using different prompts.

Segment Anything on Images. We now evaluate our model for the segment anything task on images. In Table [table](#) 3, we report 1-click and 5-click mIoU accuracy on both SA-23 benchmark, plus the new benchmark introduced in SAM 2 [(Ravi et al., 2024)](#b57) with 14 video datasets from video domain. We compare our EfficientTAMs with SAM (ViT-H) and HQ-SAM (ViT-H). Our EfficientTAM-S obtains a 2.6 mIoU improvement over SAM (ViT-H) and 1.6 mIoU improvement over HQ-SAM (ViT-H) on 1-click accuracy. For 5-click, we observe consistent improvement over SAM (ViT-H) and HQ-SAM (ViT-H). We also notice a significant improvement on the video benchmarks of SA-23 and the one with 14 new videos. This indicates our EfficientTAMs are strong for both image and video segmentation.

Qualitative Evaluation. figure 5 shows two video examples. We compare EfficientTAM and SAM 2 with a mask in the first frame prompted. We find that our EfficientTAM can generate high-quality masklet for the target object as SAM 2. More video examples are in the appendix. These results suggest that our EfficientTAMs have similar abilities to SAM 2, while EfficientTAM is more efficient.

## Ablation Studies

Impact of the object pointer tokens. We study the effect of the object pointer tokens when performing cross-attention in the memory module. We ablate the cross-attention with or without the object pointer tokens. We find that object pointers significantly improve the performance on SA-V test dataset, 74.5 vs 72.1 J &F, consistent with SAM 2 [(Ravi et al., 2024)](#b57). This demonstrates that object pointer tokens need to be cross-attended with spatial tokens from the memory bank.

Structure of memory tokens. We ablate the impact of memory tokens for efficient cross-attention in the memory module. In our efficient cross-attention, we leverage the locality of memory spatial tokens for a coarser representation, and we concatenate the coarser embedding with object pointer tokens. We observe that naively pooling the entire memory tokens instead of only the spatial tokens yields a large performance Table [5](#) Ablation study on the effect of input resolution.

drop, 2.3 J &F on SA-V test.

Impact of window size. We perform an averaging pooling for a good surrogate in equation ( [5](#formula_7)). We experiment with window sizes 2 × 2 and 4 × 4. We find increasing the window from 2 × 2 to 4 × 4 for efficient cross-attention will lead to ∼ 1 J &F accuracy drop with marginal speed improvement. Therefore, we use window size 2 × 2 to achieve a trade-off between accuracy and efficiency.

Linear cross-attention. We explore adapting one representative efficient attention method such as linear attention [(Choromanski et al., 2020;](#b15)[Cai et al., 2023;](#b4)[You et al., 2023)](#b83) by leveraging the associative property of matrix multiplication. We find that linear attention using associative property of matrix multiplication leads to significant performance drop, > 10 J &F accuracy on SA-V test, comparing to our proposed efficient cross-attention. Therefore, leveraging the underlying token structure for efficient cross-attention is more effective.

Efficient cross-attention variants. We compare efficient cross-attention variants. We find that the Linformer variant underperforms the efficient cross-attention in equation ( [5](#formula_7)), 73.4 vs 74 J &F on SA-V test. However, we find that equation ( [6](#formula_9)), can achieve comparable performance, shown in table [4](#tab_4).

Impact of input resolution. We ablate the impact of input resolution for video object segmentation. By default, we used 1024 × 1024. We experiment with different input resolution, e.g., 512 × 512. table [5](#) shows that decreasing the input resolution leads to some performance drop. But it improves the efficiency, especially on mobile device, 12.5x speedup on iPhone 15. This gives flexibility for practical deployments with different latency and quality needs.

## Conclusions

We revisited using a 

$q i √ d K T p )V p ) = D Sii (l w × l h × (e( q i √ d kT 1 )ṽ 1 + • • • + e( q i √ d$kT 1 )ṽ wh ) + e(

$q i √ d K T p )V p ) = D Sii (l w × l h × e( q i √ d KT s ) Ṽ T s + e( q i √ d K T p )V p ) = D Sii (e(ln(l w × l h ) + q i √ d KT s ) Ṽs + e( q i √ d K T p )V p ) = softmax[ q i KT s √ d + ln (l w × l h ), q i KT p √ d ][ Ṽs ; V p ](8)$where D Sii is the i th diagonal element of the matrix D S . Note that the right side of equation ( [8](#formula_12)) is the i th row of softmax (A) Ṽ . It concludes the proof.

## B Ablation Studies

Impact of the object pointer tokens. We study the effect of the object pointer tokens when performing crossattention in the memory module. We ablate the cross-attention with or without the object pointer tokens. When performing cross-attention, we find that object pointers significantly improve the performance on SA-V test dataset, 74.5 vs 72.1 J &F, shown in table [6](#tab_5). The observations are consistent with SAM 2 [(Ravi et al., 2024)](#b57). This demonstrates that object pointer tokens need to be cross-attended with spatial tokens.

Structure of memory tokens. We ablate the impact of memory tokens for efficient cross-attention in the memory module. In our efficient cross-attention, we leverage the locality of memory spatial tokens for a coarser representation, and we concatenate the coarser embedding with object pointer tokens. In table 7, we observe that naively pooling the entire memory tokens instead of only the spatial tokens yields a large performance drop, 2.3 J &F on SA-V test.

Local windowed cross-attention. We adapt local windowed attention for efficient cross-attention by partitioning input tokens into 4 non-overlapping segments (windows), within which we conduct cross-attention. In table [8](#tab_7), we find that local windowed cross-attention underperforms our proposed efficient cross-attention using averaging pooling, 72.4 vs 74.0 J &F on SA-V test dataset. These results demonstrate the effectiveness of our efficient cross-attention by leveraging the strong locality of spatial memory tokens.

Efficient cross-attention variant. We observe that equation (6) in the main paper is close to original crossattention, visualized in figure [7](#fig_5). This suggests that equation ( [6](#formula_9)) can also serve as a surrogate of the original cross-attention.  

## C Qualitative Evaluation

We provide more qualitative results of EfficientTAMs for video and image instance segmentation. figure [6](#) shows two challenging video examples with occluded objects. We compare EfficientTAM and SAM 2 with a mask in the first frame prompted. We find that our EfficientTAM can generate high-quality masklet for the target occluded object as SAM 2. For image segmentation, we also observe that our EfficientTAM can generate quality image segmentation results as SAM and SAM 2, shown in figure [8](#). We report the predicted masks with two types of prompts, point and box, and also segment everything results. These results suggest that our EfficientTAMs have similar abilities to SAM 2, while EfficientTAM is more efficient.

![Figure 1 Comparative analysis. (Left) Speed comparison between EfficientTAM and SAM 2 on a single NVIDIA A100 GPU. While SAM 2 is challenging for on-device deployment, our EfficientTAM can run 261 ms per frame on iPhone 15 Pro Max. (Right) FPS/Parameter/Performance comparison of EfficientTAM, SAM 2, and other efficient models for zero-shot video object segmentation on SA-V test. We benchmark FPS (frames per second) of all models with 1024 × 1024 input resolution on a single NVIDIA A100.]()

![Figure 2 EfficientTAM architecture. Our proposed EfficientTAM takes a vanilla lightweight ViT image encoder for frame feature extraction. An efficient memory cross-attention is proposed to further improve the efficiency of EfficientTAM by leveraging the strong locality of memory spatial embeddings. EfficientTAM is fully trained on SA-1B (image) and SA-V (video) for unified image and video segmentation.]()

![Figure 3 An example to show strong locality of the Keys and Values in the cross-attention of the memory module. Keys and Values are a matrix of size 28700 × 256. Cross-attention is a matrix of size 4096 × 256. For simplicity of visualizing and comparison, we only draw the top matrix of size 320 × 256. We use a single averaged token to represent other tokens in the homogeneous window with a 2 × 2 size, for Keys and Values to obtain coarse Keys and Values. At right, we visualize the difference between original cross-attention of equation (1) and efficient cross-attention of equation (5); the relative error w.r.t original cross-attention is 0.03 under Frobenius norm.]()

![Figure4Promptable video segmentation results across 9 video segmentation datasets under interactive offline (left) and online (right) evaluation settings. The average J &F over 1, . . . , 8 interacted frames is reported.]()

![Figure5Visualization results on video segmentation and tracking with SAM 2, and our EfficientTAM model. We sampled a subset of frames for visualization. The segmented objects, e.g., the goose and the camel, are colored in red.]()

![Figure7Visualization of the difference between original cross-attention and efficient cross-attention of equation (6).]()

![Standard semi-supervised video object segmentation results across video object segmentation benchmarks.]()

![Interactive semi-supervised video object segmentation results with different prompts. We report averaged J &F zero-shot accuracy across 17 video datasets for each type of prompt.]()

![Efficient cross-attention variants.]()

![plain, non-hierachical image encoder for building efficient video object segmentation and track anything model, EfficientTAM. With a vanilla lightweight ViT image encoder, EfficientTAM demonstrated competing image and video segmentation capabilities as hierarchical image encoder while being more efficient and deployable on mobile devices. We also proposed an efficient memory module with faster cross-attention, leveraging the locality of spatial memory embeddings. The efficient memory module further improves EfficientTAM's accuracy-efficiency tradeoff on video segmentation and tracking anything. Extensive experiments on semi-supervised video object segmentation, promptable video segmentation, and the segment anything tasks consistently validate the advantages of our EfficientTAM. Our preliminary work suggests that EfficientTAM has many potential applications for on-device tracking anything.Ablation study on the design of memory cross-attention in EfficientTAM.]()

![Ablation study on taking care of the memory token structure for efficient cross-attention in EfficientTAM.sum up to 1, and e(•) denotes exp(•). For each row of the cross-attention matrix, we have,]()

![Cross-Attention MOSE dev DAVIS 2017 val SA-V testComparing with local windowed attention.]()

