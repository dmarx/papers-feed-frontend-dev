{
  "arxivId": "2405.07883",
  "title": "Zero-Shot Tokenizer Transfer",
  "authors": "Benjamin Minixhofer, Edoardo Maria Ponti, Ivan Vuli\u0107",
  "abstract": "Language models (LMs) are bound to their tokenizer, which maps raw text to a\nsequence of vocabulary items (tokens). This restricts their flexibility: for\nexample, LMs trained primarily on English may still perform well in other\nnatural and programming languages, but have vastly decreased efficiency due to\ntheir English-centric tokenizer. To mitigate this, we should be able to swap\nthe original LM tokenizer with an arbitrary one, on the fly, without degrading\nperformance. Hence, in this work we define a new problem: Zero-Shot Tokenizer\nTransfer (ZeTT). The challenge at the core of ZeTT is finding embeddings for\nthe tokens in the vocabulary of the new tokenizer. Since prior heuristics for\ninitializing embeddings often perform at chance level in a ZeTT setting, we\npropose a new solution: we train a hypernetwork taking a tokenizer as input and\npredicting the corresponding embeddings. We empirically demonstrate that the\nhypernetwork generalizes to new tokenizers both with encoder (e.g., XLM-R) and\ndecoder LLMs (e.g., Mistral-7B). Our method comes close to the original models'\nperformance in cross-lingual and coding tasks while markedly reducing the\nlength of the tokenized sequence. We also find that the remaining gap can be\nquickly closed by continued training on less than 1B tokens. Finally, we show\nthat a ZeTT hypernetwork trained for a base (L)LM can also be applied to\nfine-tuned variants without extra training. Overall, our results make\nsubstantial strides toward detaching LMs from their tokenizer.",
  "url": "https://arxiv.org/abs/2405.07883",
  "issue_number": 872,
  "issue_url": "https://github.com/dmarx/papers-feed/issues/872",
  "created_at": "2025-01-10T01:45:17.778727",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 15,
  "last_read": "2025-01-10T02:01:59.116135",
  "last_visited": "2025-01-10T02:01:14.129000+00:00",
  "main_tex_file": null,
  "published_date": "2024-05-13T16:17:10Z",
  "arxiv_tags": [
    "cs.CL"
  ]
}