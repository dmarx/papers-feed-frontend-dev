# Zero-Shot Tokenizer Transfer

## Abstract

## 

Language models (LMs) are bound to their tokenizer, which maps raw text to a sequence of vocabulary items (tokens). This restricts their flexibility: for example, LMs trained primarily on English may still perform well in other natural and programming languages, but have vastly decreased efficiency due to their Englishcentric tokenizer. To mitigate this, we should be able to swap the original LM tokenizer with an arbitrary one, on the fly, without degrading performance. Hence, in this work we define a new problem: Zero-Shot Tokenizer Transfer (ZeTT). The challenge at the core of ZeTT is finding embeddings for the tokens in the vocabulary of the new tokenizer. Since prior heuristics for initializing embeddings often perform at chance level in a ZeTT setting, we propose a new solution: we train a hypernetwork taking a tokenizer as input and predicting the corresponding embeddings. We empirically demonstrate that the hypernetwork generalizes to new tokenizers both with encoder (e.g., XLM-R) and decoder LLMs (e.g., Mistral-7B). Our method comes close to the original models' performance in cross-lingual and coding tasks while markedly reducing the length of the tokenized sequence. We also find that the remaining gap can be quickly closed by continued training on less than 1B tokens. Finally, we show that a ZeTT hypernetwork trained for a base (L)LM can also be applied to fine-tuned variants without extra training. Overall, our results make substantial strides toward detaching LMs from their tokenizer. 1 We adopt a broad definition of LMs that also includes models that do not define a probability distribution over finite-length sequences, such as text encoders.

Preprint. Under review.

## Introduction

Language Models 1 typically operate on discrete tokens, so they need a means to map text into a sequence of tokens, namely a tokenizer. The vast majority of contemporary LMs use subword tokenizers [(Devlin et al., 2019;](#b16)[Jiang et al., 2023;](#b27)[Touvron et al., 2023;](#b64)[Parmar et al., 2024, among others)](#), whereas others use byte-level [(Xue et al., 2022;](#b69)[Yu et al., 2023;](#b71)[Wang et al., 2024)](#b67) or character-level tokenizers [(Clark et al., 2022;](#b9)[Tay et al., 2022)](#b63). Regardless of the chosen tokenization 'granularity', these models share a fundamental limitation: once they are trained with a particular tokenizer, inference with a different tokenizer is impossible. In other terms, a pre-trained LM is "bound" to the tokenizer it was trained with. This has wide-ranging implications: since the focus during pretraining is typically primarily on the English language, the tokenizer often encodes languages besides English [(Rust et al., 2021)](#b56) or other domains, such as code, less efficiently. This leads to large disparities in the inference cost between English and non-English text [(Ahia et al., 2023;](#b0)[Petrov et al., 2023)](#b51). Tokenizers may also be sub-optimal for domains which they were not designed to be used with, e.g. fine-tunings of the Llama models performing subpar on coding tasks [(Dagan et al., 2024)](#b14). Efficiency and performance are only some of the reasons to transfer models across tokenizers: methods of interaction between models, such as ensembling [(Sagi & Rokach, 2018)](#b57) and model merging [(Wortsman et al., 2022;](#b68)[Ainsworth et al., 2023;](#b1)[Yadav et al., 2023)](#b70), typically assume the same unit of representation (i.e., equivalent tokenization) across models; if two models adopt different tokenizers, they become unsuitable for ensembling or merging. Problematic artifacts of tokenization such as 'Glitch tokens' [(Land & Bartolo, 2024)](#b34) may also be fixed via transfer to a new tokenizer.

To address these issues, past work developed methods to equip an LM with a new tokenizer by retraining the embedding parameters, and optionally continuing to train the entire model [(Artetxe et al., 2020;](#b2)[de Vries & Nissim, 2021)](#b15). This adaptation can be made faster by initializing the embedding parameters through heuristics [(Tran, 2020;](#b65)[Minixhofer et al., 2022;](#b46)[Gee et al., 2022;](#b18)[Dobler & de Melo, 2023;](#b17)[Liu et al., 2023)](#b38). In this work, we formulate a new problem: given an LM, can we create an embedding matrix on-the-fly for any arbitrary tokenizer, without ever observing data for it? While past work investigated n-shot tokenizer transfer, we refer to this new problem as zero-shot tokenizer transfer (ZeTT). If the performance of the model can be approximately preserved, ZeTT effectively "detaches" LMs from the tokenizer they were trained with. We first evaluate the efficacy of prior (heuristic-based) approaches for ZeTT, finding that, while heuristics can preserve performance to some extent, there is generally a large gap to the original LM performance.

To close this gap, we introduce a new paradigm: We train a hypernetwork on a diverse distribution of tokenizers to predict the embedding parameters for any given tokenizer. By investing into the one-time-cost of training the hypernetwork, we aim to subsequently enable effective ZeTT. This proves to be possible: ZeTT via the hypernetwork preserves performance to a few percent accuracy in many cases. Furthermore, the hypernetwork can learn to rapidly adapt to a given target tokenizer by continued training on a small amount (<1B) of extra tokens, whereas previous work typically needed hundreds of billions of tokens [(Dagan et al., 2024)](#b14). As such, our hypernetwork provides a state-of-the-art solution to n-shot tokenizer transfer, while also establishing a competitive baseline to our newly introduced zero-shot tokenizer transfer problem. This unlocks a range of new ways to combine language models with tokenizers. For example, in this work, we zero-shot substitute the Mistral-7B tokenizer [(Jiang et al., 2023)](#b27) with a tokenizer that encodes code using 10% less tokens on average, while preserving functional code generation correctness to approx. 3% (Section 4.2). We also evaluate zero-shot cross-lingual transfer of the multilingual XLM-R encoder model to a range of different languages by substituting the XLM-R tokenizer with a target-language specific tokenizer and reusing adapters trained for the original XLM-R. This leads to a >16% speedup and preserves performance on XNLI [(Conneau et al., 2018)](#b12) to 1% on average, although the language model has never been trained with the target-language tokenizers. Finally, we show that a hypernetwork trained for a base large LM (e.g. Mistral-7B) can also be applied to fine-tunings of the same model (e.g. Mistral-7B-Instruct-v0.1), preserving capabilities to a large extent (Section 4.3). Our code and models are publicly available at github.com/bminixhofer/zett.

## Background

Tokenizers and Embeddings. Tokenizers operate as a tokenization function T mapping a text to a sequence of elements in the vocabulary V. By the term tokenizer, we henceforth refer to the tuple comprising the two crucial components, (V, T ). Importantly, the vocabulary and the tokenization function are distinct components; given some vocabulary, there are many ways to encode text as a sequence of tokens in this vocabulary (e.g. [Hofmann et al., 2022;](#b23)[Uzan et al., 2024)](#b66). After tokenization, the model represents the sequence of tokens via a function E ϕ : V → R dmodel (the embeddings). The embeddings are typically parametrized by a matrix ϕ as a lookup table which assigns a distinct d model -dimensional vector (a row of the matrix) to every element in V. Embeddings are used twice in the language model: once at the input to map tokens to a fixed-size vector, and again at the output to compute a logit for every token, typically via a dot-product of E ϕ (t) with the final hidden state of the LM. Embedding parameters may or may not be shared between the input and the output;[foot_0](#foot_0) our method works with both. We denote the entire set of embedding parameters via ϕ, denoting input embeddings as ϕ in and output embeddings as ϕ out , if necessary.

Contemporary language models typically use subword tokenizers via BPE [(Sennrich et al., 2016)](#b60) or UnigramLM [(Kudo, 2018)](#b30). Subword tokenization is a common choice since it can represent arbitrary sequences of text ("open-vocabulary" language modeling) while largely retaining the efficiency of word-level models [(Mielke et al., 2021)](#b44). However, there are a number of problems with subword tokenization, e.g. models using subword tokenization struggle parsing sequences of numbers [(Golkar et al., 2023)](#b19) and text with spelling mistakes [(Xue et al., 2022)](#b69). A recent strand of work aims to get rid of subword tokenization via byte-level (so-called "token-free") models [(Xue et al., 2022;](#b69)[Yu et al., 2023)](#b71). However, these models still operate on tokens, using the set of 256 bytes as the vocabulary, and Unicode as the tokenization function [(Mielke et al., 2021)](#b44). In a similar vein, some models use character-level tokenization [(Tay et al., 2022;](#b63)[Clark et al., 2022)](#b9), optionally learning to pool characters into longer tokens [(Nawrot et al., 2023)](#b48). [3](#foot_1) So far, byte-or character-level approaches have been unable to supplant subword tokenization due to reduced compute efficiency (because of longer sequences), and not necessarily being more robust [(Libovický et al., 2022)](#b36). Thus, although our approach is in principle applicable to any tokenizer, we focus our experiments on subword tokenizers. Specifically, we use the UnigramLM parametrization of the tokenization function, and show that other tokenizers can be converted to this parametrization later in Section 5. UnigramLM sets

$T (x) := argmax C∈Cx t∈C log p(t)$where C x is the set of all possible tokenizations of x (i.e., all possible decompositions of x in V). UnigramLM provides a convenient way to represent tokens as a 2-tuple (t, p(t)) ∈ (V, R).

Embedding Initialization Heuristics. Prior work transfers LMs to a new tokenizer by initializing embedding parameters via a heuristic, then continuing to train the embeddings. We denote the original tokenizer as (V a , T a ) and the original embedding parameters as ϕ a . Analogously, the target tokenizer is (V b , T b ) with embedding parameters ϕ b . FVT [(Gee et al., 2022)](#b18) initializes embeddings for any new token t ∈ V b as the mean of the embeddings of T a (t) i.e. the mean of the sequence of embeddings the new token is decomposed into by the previous tokenizer T a . RAMEN [(Tran, 2020)](#b65), WECHSEL [(Minixhofer et al., 2022)](#b46) and OFA [(Liu et al., 2023)](#b38) require auxiliary embeddings

$E aux : V aux → R daux with |V aux ∩ V a | ̸ ≪ |V a | and |V aux ∩ V b | ̸ ≪ |V b |.$They use E aux to embed tokens in V a and V b in the same semantic space, then initialize embeddings in E ϕ b as a weighted average of embeddings in E ϕa with weights given by their similarity in E aux . FOCUS [(Dobler & de Melo, 2023)](#b17) initializes embeddings of tokens in V b \ V a as a weighted combination of the overlapping tokens V a ∩ V b , and copies the embeddings of the overlapping tokens. Weights are again computed using an auxiliary embedding matrix E aux , but the only requirement is

$|V aux ∩ V b | ̸ ≪ |V b |.$We use FOCUS as the main baseline since [Dobler & de Melo (2023)](#b17) show it obtains better performance without any training (i.e., zero-shot) than other heuristics, which we also confirm later in Section 4.2.

Heuristic-Free Tokenizer Transfer. While a significant amount of prior work has investigated heuristics to initialize the embedding layer, there is also research into changing the training procedure to facilitate n-shot tokenizer transfer. [Marchisio et al. (2023)](#b42) show that forward-and backwardpropagating through a subset of the model layers is sufficient for learning embeddings for a new tokenizer. [Chen et al. (2023)](#b24) find that regularly resetting the embedding parameters during pretraining boosts the speed at which they are relearnt upon transfer. These approaches can be seen as orthogonal to ours. They could be freely combined with our method; we leave this to future work.

Embedding Prediction Hypernetworks. Hypernetworks are networks that predict the parameters of another network [(Ha et al., 2017)](#b21). Prior work uses neural networks to predict embeddings for out-of-vocabulary [(Pinter et al., 2017)](#b52) or rare words [(Schick & Schütze, 2019)](#b58) of word embedding models [(Mikolov et al., 2013)](#b45). [Schick & Schütze (2020)](#b59) extend this approach to predict embeddings for rare words in BERT models [(Devlin et al., 2019)](#b16). These methods can also be viewed as embedding prediction hypernetworks. In contrast, the hypernetwork we propose (i) approaches the more general problem of transferring to an arbitrary tokenizer, instead of extending the original tokenizer and (ii) can be applied to encoder, decoder, and encoder-decoder LMs, that is, it is objective-agnostic.

## Algorithm 1 Hypernetwork training loop for Zero-Shot Tokenizer Transfer

Input: corpus D, tokenizer sample size n, batch size m, max. token length l, vocabulary size k, noise parameters (µ, σ), pretrained LM parameters ψ, initial hypernetwork parameters θ init .

Output: Hypernetwork parameters θ. 1: procedure TRAINHYPERNETWORK 2:

θ ← θ init 3: q ← queue(x 1 , .., x n ∼ D) ▷Create a pool of n texts (where n ≥ m).

## 4: 5:

for step in train_steps do 6:

x 1 , .., x m ∼ D 7:

q ← pop(q, m) ▷Remove the least-recently-added batch.

## 8:

q ← push(q, x 1 , .., x m ) ▷Add the current batch. 9: 10: t, f ← substrings(q, l) ▷Compute all substrings and their frequency in q.

11:

$f ← f / i f i ▷Normalize frequencies to sum to one. 12: z ∼ Lognormal(µ, σ 2 ) 13: for t, f ∈ (t, f ) do 14: p(t) ← f + N (0, z 2 )$▷Assign a score based on frequency + noise to the substrings. 15: Sort t by p(t) descending. 16: V b ← t[: k] ▷Assemble the top k substrings into the tokenizer. 17: 

$T b ← UnigramLM({(t, p(t)) | t ∈ t[: k]}) 18: 19: loss ← L θ (T b (x), H θ (V b , T b ), ψ)$
## Methodology

## Hypernetwork Training

We aim to find parameters θ of a hypernetwork H θ : (V b , T b ) → ϕ b for some pretrained LM. Let ϕ a and ψ be the embedding and inner (non-embedding) parameters of the language model, respectively. L is the loss of the language model as a function of the tokens, the embedding parameters, and the inner parameters, typically:

$L(t, ϕ a , ψ) = CrossEntropy(LM ψ (E ϕa (t)), label(t)),$where LM ψ is the language model and label maps the sequence of tokens to corresponding labels, e.g., shifting the sequence in case of standard (autoregressive, causal) language modeling, or masking the sequence in case of Masked Language Modeling [(Devlin et al., 2019)](#b16). Importantly, however, we do not make any specific assumptions on L.

Note that the loss of the language model under the original tokenizer T a on a text x is L(T a (x), ϕ a , ψ). We train our hypernetwork to minimize the loss

$L θ (T b (x), H θ (V b , T b ), ψ).$That is, we substitute the original embedding parameters for the hypernet predictions, and substitute the original tokenizer for a tokenizer (V b , T b ). Figure [1](#fig_0) illustrates the flow of information.

Defining Distributions over Texts and Tokenizers. We follow standard practice and sample texts uniformly from the training corpus. Tokenizer sampling is not as trivial: we would like a distribution over tokenizers (V b , T b ) with high variance to encourage generalization to unseen tokenizers. To this end, we introduce a procedure to sample a diverse set of UnigramLM tokenizers. We show later in Section 5 that arbitrary tokenizers can be well-approximated via UnigramLM, motivating this choice.

We initially fill a queue q with n texts sampled randomly from the training corpus and, at every step in the training loop, push the m texts in the current batch and remove the m least recently added texts. We then compute all substrings t up to length l and their frequency in q. 45 We add Gaussian noise to the frequencies to arrive at a final score p(t) for every token t. Finally, we assemble the tokenizer by taking the top k tokens with the highest p(t) as the vocabulary and UnigramLM parametrized by p(t) as the tokenization function. The training loop is summarized in Algorithm 1. The 'rolling' queue of texts q ensures high variance in the vocabulary, while the Gaussian noise added to the frequencies ensures high variance in the tokenization function.

Importantly, the texts and the tokenizer are sampled dependently: the batch of m texts used for training is a subset of the n texts used for sampling the tokenizer. If they were sampled independently, the probability for a token to occur would be p(token) ∝ p(token ∈ V b ) × p(token ∈ x). Since both these factors are small for rare tokens, p(token) would get vanishingly small in this case.

## MIMICK-Style

Warmup & Auxiliary Loss. In practice, directly minimizing L θ starting from randomly initialized θ is difficult. Thus, we include a warmup stage where we train the hypernetwork to mimic the embedding parameters of the original tokenizer, akin to MIMICK [(Pinter et al., 2017)](#b52).

$L warmup θ = ∥H θ (V a , T a ) -ϕ a )∥ 2$The warmup stage is substantially quicker than the main stage because there is no need to propagate through the main model. We found it prevents divergence in some cases. Afterwards, we add an auxiliary loss, which, for every token in the sampled vocabulary V b that also exists in the original vocabulary V a , penalizes the distance to the corresponding embedding in ϕ a .

$L aux θ = 1 |V a ∩ V b | t∈|Va∩V b | ∥H θ (V b , T b )[V b [t]] -ϕ a [V a [t]]∥ 2$This penalizes drift from the warmup stage. Combining it with the main loss yields the final loss.

$L final θ = L θ (T b (x), H θ (V b , T b ), ψ) + α • L aux θ$The hyperparameter α weighs the contribution of the auxiliary loss. Since H θ (V b , T b ) is also required for the main loss, it requires negligible extra computation. The auxiliary loss is necessary especially for models with separate input and output embedding matrices as shown in Appendix B.

## Hypernetwork Architecture

It remains to define the hypernetwork architecture, that is, how to map the tokenizer (V b , T b ) to the embedding parameters ϕ b . To this end, we represent the new tokens t b ∈ V b by decomposing them using the original tokenization function T a , and embedding them with the original embeddings E ϕa . [6](#foot_4) This sequence of embeddings is passed through multiple Transformer layers, plus a separate prediction head for the input embeddings and output embeddings ϕ in b and ϕ out b . The hypernetwork thus consists of another language model which is applied separately for every token. We refer to the hypernetwork's language model as HLM θ . HLM θ can be thought of as learning how to compose Table [1](#): Accuracy on XNLI when reusing adapters trained for the original XLM-R model with new zero-shot transferred language-specific tokenizers. Also shown are the absolute change in accuracy from applying our hypernetwork (∆accuracy) and the average decrease in token length of the language-specific tokenizers over the original tokenizer (∆length).

ar bg de el en es fr hi ru sw tr ur vi Avg. original 68.9 75.6 74.7 73.7 82.3 76.9 76.8 68.4 72.9 63.5 72.2 64.7 73.1 72.6 Lexical 58.7 63.1 65.3 61.7 72.8 68.4 66.7 61.8 62.3 51.8 58.5 60.0 72.0 63.3 FVT 63.9 70.3 70.9 67.4 79.0 73.9 71.9 65.7 67.8 57.1 66.3 61.7 72.9 68.4 OFA 57.3 64.2 67.3 62.8 73.6 68.6 68.4 61.8 63.1 54.8 59.7 59.3 72.3 64.1 FOCUS 64.8 71.0 71.6 67.7 79.6 74.4 72.6 64.5 68.1 55.7 67.3 61.9 72.6 68.6 ours 67.9 73.9 74.1 71.4 81.1 76.2 74.7 67.7 70.7 62.3 68.7 63.2 73.9 71.2 ∆accuracy -1% -2% -1% -2% -1% -1% -2% -1% -2% -1% -3% -2% +1% -1% ∆length -22% -14% -13% -23% -9% -11% -12% -13% -13% -19% -15% -9% -3% -14% the sequence of tokens T a (t)-which any given token is decomposed into-into one embedding, as illustrated in Figure [2](#fig_2). Importantly, we do not take the tokenization function into account. By sampling diverse tokenizers during the training process, we aim for the hypernetwork to learn to produce a single embedding suitable to a wide variety of different tokenization functions. We analyze the impact of this choice later in Section 5. We also experiment with hypernetworks which do take the tokenization function into account in Appendix C.

On Token Decomposition. The input to the hypernetwork consists of the sequence of tokens T a (t) that any given token is decomposed into. However, this decomposition is not always trivial: for example, T a could be character-level, while the token t could be in the vocabulary of a byte-level tokenizer T b . In this case, t could be any arbitrary sequence of bytes (not necessarily valid UTF-8).

To solve this issue, we introduce a procedure to convert tokenizers to the byte level by adding a small amount of extra tokens to the vocabulary (c.f. Section 5). This guarantees that T a can decompose arbitrary tokens. The embeddings of the extra vocabulary are initialized randomly and trainable alongside the hypernetwork parameters.

## Experiments

## Setup

Data. We use the English subset of the MADLAD-400 corpus [(Kudugunta et al., 2023)](#b32) and code from the StarCoder data [(Li et al., 2023)](#b74) for hypernetwork training. The sampling ratio of English to Code is 7:3 following [Zhang et al. (2024)](#b73). For the multilingual hypernetwork, we use a subset of 26 of the languages used in XGLM [(Lin et al., 2022)](#b37).[foot_5](#foot_5) with data from MADLAD-400. We sample  Evaluation. We use the standard benchmarks PiQA [(Bisk et al., 2020)](#b3), HellaSwag (HS [Zellers et al., 2019)](#b72), BoolQ [(Clark et al., 2019)](#b8), MMLU [(Hendrycks et al., 2021)](#) and the "easy" subset of ARC [(Clark et al., 2018)](#b10) for evaluation in English and the synthesis task of HumanEvalPack [(Muennighoff et al., 2023)](#b47) for coding evaluation. For multilingual evaluation, we use XNLI [(Conneau et al., 2018)](#b12), XCOPA [(Ponti et al., 2020)](#b53) and MMLU as machine-translated by [Lai et al. (2023)](#b33).

Models. To evaluate our method, we use Mistral-7B [(Jiang et al., 2023)](#b27) as the main decoder-style language model and XLM-R [(Conneau et al., 2020)](#b13) as a representative of encoder-style models. 9 We also experiment with the smaller TinyLlama-1.1B model [(Zhang et al., 2024)](#b73) in Appendix G.

Tokenizers. We transfer models to the GPT2 tokenizer [(Radford et al., 2019)](#b55) for evaluation on natural language benchmarks and to the StarCoder tokenizer [(Li et al., 2023)](#b74) for evaluation on code benchmarks. 10 For multilingual evaluation, we train language-specific monolingual tokenizers with a vocabulary size of 50k using SentencePiece [(Kudo & Richardson, 2018)](#b31) and evaluate transfer to these. We also verify that the hypernetwork is robust to the choice of vocabulary size in Appendix E.

Hypernetwork training. We train the hypernetwork for 200k gradient update steps (10k of which are MIMICK-style warmup) with a batch size of 128 tokens and a sequence length of 128 (we find it sufficient to use short sequence lengths for learning embedding parameters). For the multilingual decoder-style models, we start from the English + Code checkpoint and forgo MIMICK-style warmup, keeping other hyperparameters unchanged. We use a RoBERTa-style architecture i.e. bidirectional attention and Post-LayerNorm Transformer layers [(Liu et al., 2019)](#b39), but use a feedforward dimension of 2x the hidden dimension (instead of RoBERTa's 4x) for the hypernetwork. See Appendix D for a full list of hyperparameters.

Continued training details. To keep runtime comparable between training the model with hypernetwork and direct training (without hypernetwork), we run hypernetwork inference only for a subset of 8 We use Flan v2 because we observed a strong decrease in accuracy from continuing to train on the MADLAD-400 data (even with the original tokenizer). The training data for most LLMs (including Mistral-7B) is not public, but it is plausible that this decrease stems from higher-quality data mixed in especially towards the end of training as in e.g. [Groeneveld et al. (2024)](#b20).

9 Although (decoder-style) LLMs are the centerpiece of a large amount of current NLP research, encoder-style LMs have wide-ranging applications in e.g. retrieval [(Khattab & Zaharia, 2020)](#b29) and LLM distillation [(Hsieh et al., 2023)](#b24) due to their lower computational cost. 10 We chose these tokenizers due to their popularity and comparatively efficient encoding of the target domain.

Table [5](#): Single model rating results on MT-Bench of transferring Mistral-7B-Instruct-v0.1 to the GPT2 tokenizer using the hypernetwork trained for the base Mistral-7B model. We use gpt-3.5-turbo-1106 as a judge. orig. is the original fine-tuned model, base the model with the same tokenizer but embeddings substituted for the base models' embeddings. λ is the scaling factor for the weight differences in Task Arithmetic [(Ilharco et al., 2023)](#b26).

original 0-shot n-shot Embeddings orig. base FOCUS ours ours@800 λ ----0.0 0.3 0.5 0.7 Score (1 to 10) 7.33 7.48 5.03 6.56 6.59 6.75 6.82 6.77 k = 16384 tokens in the continued training case. The subset consists of all tokens occurring in the batch, plus a uniform sample of those that do not occur. The language modeling loss is then only computed over this subset of tokens. We found in preliminary experiments that this causes only minor performance degradation. Furthermore, we use the zero-shot predicted embeddings as the target for the auxiliary loss instead of using the original embeddings. This stabilizes training. We train for 50k steps with a batch size of 32 and sequence length of 512, resulting in 'seeing' 819.2M tokens.

## Zero-Shot and n-shot Results

Results for XLM-R are shown in Table [1](#). We take task adapters trained for the original XLM-R model on the English XNLI dataset via [Poth et al. (2023)](#b54) and substitute the tokenizer for our language-specific one. We compare our hypernetwork against a simple lexical baseline (copying the embeddings of overlapping tokens and initializing the rest randomly), FVT, OFA, and FOCUS (c.f. Section 2). We focus only on FOCUS in the following since it performs best among the baselines.[foot_6](#foot_6) Our hypernetwork consistently outperforms all baselines and preserves accuracy to 1% on average, losing 3% in the worst case and improving by 1% in the best case, while sequences are on average 14% shorter for the language-specific tokenizers; inference is thus more than 16% faster. 12 We show in Appendix E that these results are robust to the target vocabulary size.

Table [2](#tab_2) shows results on English and Code for Mistral-7B. We find that ZeTT is more challenging in the decoder case: FOCUS performs roughly random in the worst case (-23.2% on BoolQ) and is reduced to 0% pass@1 on HumanEval in Python. The hypernetwork goes a long way in closing this gap but still falls behind on some benchmarks. However, continuing to train the hypernetwork with the target tokenizer closes the gap almost completely. In fact, continued training on 800M tokens with the StarCoder tokenizer performs better than continued training for the same amount of tokens with the original tokenizer, potentially because the StarCoder tokenizer is more well suited towards code; it results in approx. 10% less tokens on average. Also, notably, continued training with the original tokenizer slightly degrades performance on average; this may be due to a higher-quality data mix used for pretraining Mistral-7B, whereas we use public data sources (c.f. Section 4.1).

Results of the multilingual hypernetwork for Mistral-7B are shown in Table [3](#tab_3) and [Table 4](#tab_4). On XCOPA, the hypernetwork on average improves performance over the original model, while also more than halving sequence length. XCOPA performance is close to random in some languages (e.g. Southern Quechua (qu) and Estonian (et)), so we also evaluate on multilingual MMLU. Here, although the hypernetwork clearly outperforms FOCUS (which performs close to random), there is still a substantial gap to the original model; this could presumably be fixed via continued training.

## Applying a Hypernetwork trained for a Base Model to Fine-Tuned Models

So far, we have shown that the hypernetwork can be successfully applied for transferring the tokenizer of the base model 13 it was trained on. However, a large amount of the models used by practitioners are fine-tuned versions of base models, e.g. via SFT or RLHF [(Ouyang et al., 2022)](#b49). We now attempt to answer the question: Given a hypernetwork trained for a base model, can we apply this hypernetwork to fine-tuned versions of the same model without any extra training? This would act as a multiplying factor for the hypernetwork's applicability. First, we observe that the embedding space of a fine-tuned model is compatible with that of the base model: the embeddings of the fine-tuned Mistral-7B-Instruct-v0.1 have an average cosine similarity of 98.6% to the corresponding embedding in the base model while the average cosine similarity of the mean embedding vector is 17.4%.[foot_8](#foot_8) Embedding compatibility also holds true for other models (Appendix G). The predictions of a hypernetwork trained for a base model can thus be used out-of-the-box with fine-tuned models. We verify that this is the case by evaluating Mistral-7B-Instruct-v0.1 transferred to the GPT2 tokenizer on the corrected[foot_9](#foot_9) version of MT-Bench [(Zheng et al., 2023)](#b74). For n-shot transfer, since we train the full model we also need a way to transfer the non-embedding parameters; we achieve this via Task Arithmetic [(Ilharco et al., 2023)](#b26). Results are shown in Table [5](#).

The transferred fine-tuned model performs well, coming within approx. 0.5 score of the original model. Also, curiously, the fine-tuned model with the original tokenizer performs better when using the embeddings of the (not fine-tuned) base model; this may be a prudent direction for future work.

## Discussion

Converting tokenizers to byte-level. As per Section 3.2, we need a procedure to convert tokenizers to the byte level to ensure that token decomposition is always possible. This is trivial in most cases; the bytes just need to be added to the vocabulary. BPE is an exception: here, we need to change the atomic units on which merges are defined from characters to bytes. This can be achieved by adding merges to assemble the characters used by the tokenizer from their constituent bytes to the beginning of the merge table. We measure the success of the conversion to byte level as the probability that, given some pretoken sampled from a corpus, this pretoken results in the same token sequence in the original and the converted tokenizer. Results are shown in Table [6](#tab_5).

Converting tokenizers to UnigramLM. We also introduce a procedure to convert arbitrary tokenizers to tokenizers using UnigramLM as the tokenization function. We refer to this process as unigramifying (details in Appendix A). An important assumption of the hypernetwork training is that by using the UnigramLM parametrization with scores distributed as Gaussians we can cover a sufficiently Table [8](#): Parameter count and FLOPs estimates for our hypernetwork (and the corresponding main model) in different setups. The relatively lower computational cost compared to parameter count is mainly due to forgoing de-embedding which contributes significantly to FLOPs [(Kaplan et al., 2020)](#b28).

Model Hypernet #params FLOPs / token #params FLOPs / token GPT2 124M 253M 21M (16%) 4.5M (1.8%) TinyLlama-1.1B 1.1B 2.1G 170M (15%) 33.1M (1.6%) Mistral-7B 7.2G 15.4G 678M (9%) 132.1M (0.9%)

diverse distribution of tokenizers to enable the hypernetwork to generalize to e.g. BPE tokenizers.

Unigramifying allows us to check if, in principle, this is possible. Luckily, we find that it is: unigramifying results in minimal performance degradation when substituting the original tokenizer with the corresponding UnigramLM tokenizer (Table [6](#tab_5)). Although this does not guarantee that our distribution of tokenizers is sufficiently diverse, our empirical results suggest it is (cf. Section 4.2).

We believe our conversion methods to UnigramLM and to byte-level will simplify further research into tokenizer transfer, showing that the wildly heterogeneous landscape of tokenizers can be well approximated via byte-level UnigramLM tokenizers.

What is the effect of amortizing over the tokenization function? As described earlier in Section 3, we 'amortize' over the tokenization function, that is, the tokenization function is not an input to our hypernetwork. We find that the predicted amortized embeddings are robust to the choice of tokenization function. For example, the set of embeddings predicted for the GPT2 vocabulary has low bits-per-character for both the original GPT2 tokenization function and a different UnigramLM tokenization function with scores based on token frequencies (Table [7](#tab_6)). This is not the case for the original GPT2 embeddings: while they (as expected) perform well with the original GPT2 tokenizer, there is significant performance degradation when switching to the frequency-based UnigramLM tokenization function. This calls into question prior work copying the embeddings of overlapping tokens for transfer across tokenizers [(Dobler & de Melo, 2023;](#b17)[Gee et al., 2022, among others)](#), indicating that even if there is an exactly overlapping token in the original tokenizer, it is not necessarily the optimal initialization of the corresponding token in the new tokenizer.

Although we amortize over most of the aspects of the tokenization function, in practice, tokenization functions rely on a considerable amount of engineering, so it is not possible to amortize over everything; we discuss remaining assumptions in Appendix H.

Analyzing computational overhead of the hypernetwork. We estimate the FLOPs per token of multiple hypernetworks in Table [8](#). [16](#foot_10) Given a batch size n and sequence length s for the main model, and using the hypernetwork to compose k token sequences of length t, the FLOPs per batch will be n × s × ( FLOPs token ) main + k × t × ( FLOPs token ) hypernet . Taking hypernet training for Mistral-7B as an example with n = s = 128, k = 32768 and t = 7 the FLOPs per batch will be 252T + 30T i.e. a 12% overhead from applying the hypernet. Notably, we observed in preliminary experiments that a hypernetwork size of three layers is sufficient, regardless of model size, so the relative overhead decreases with increased amounts of layers in the main model (as also evident in Table [8](#)).

## Conclusion

We have established Zero-Shot Tokenizer Transfer (ZeTT), the difficult problem of transferring language models to a new tokenizer without any training. We have found that prior heuristics for embedding initialization provide a first baseline for ZeTT, but fall short in many cases. To establish a much stronger baseline, we introduced a hypernetwork-based approach that closes the gap to a large extent, and can be further improved via continued training on a few (<1B) tokens. Due to preserving the embedding space of the original model, ZeTT can be applied to e.g. reusing adapters trained for the original model with a different tokenizer, and to transferring fine-tuned models to a new tokenizer using a hypernetwork trained for the base model. In aggregate, this work is a substantial step towards detaching language models from their tokenizer, increasing their flexibility and reusability.

## A Unigramifying: Approximating Arbitrary Tokenizers via UnigramLM

We introduce a procedure to convert arbitrary tokenizers to UnigramLM in an optimal (but lossy) way which we refer to as unigramifying. Given a text x and the sequence of tokens T (x), for the UnigramLM tokenizer T to be equivalent to T , it is necessary that T fulfills t∈T (x) log p T (t) > t∈C log p T (t) for all C in C x \ {T (x)}.[foot_11](#foot_11) Thus, given a corpus of texts X we can formulate a loss

$L T (X, T ) = x∈X C∈Cx\{T (x)} max   0, t∈T (x) log p T (t) - t∈C log p T (t)  $which is zero if and only if the condition above is satisfied for all texts in X. This objective is piecewise linear, so it can be converted to a standard Linear Programming (LP) form and solved via an LP solver. In practice, we use the CPLEX v22.1 (IBM ILOG, 2022) solver. Since applying the procedure to a corpus directly would be costly, we first pre-tokenize the training corpus, then count the pretokens, and choose the top n = 1000000 pretokens as the set X. 

## B Stabilization Effect of the Auxiliary Loss

We found in preliminary experiments that the auxiliary loss is necessary, especially for models that do not share embedding parameters between the input and the output (models with untied embeddings).

To validate this hypothesis, we conducted an experiment where we manually untied the embeddings of GPT2 i.e. used a separate hypernetwork prediction head for the input and the output embeddings. Although everything else is kept the same, the untied GPT2 model diverges without the auxiliary loss, whereas the original GPT2 trains as expected, even without an auxiliary loss (Figure [3](#fig_3)).

## C Non-Amortizing Hypernetworks

We experimented with hypernetworks taking the tokenization function into account by adding sparse inter-token attention blocks between the self-attention and the FFN in every hypernetwork layer. Sparse inter-token attention consists of two attention blocks. The first attention block attends from a fixed amount of learnable inter-token embeddings (e.g. 16, each a vector of size d model ) to the ith token representation of every token sequence passed to the hypernetwork. The second block attends from the ith token representation to the inter-token embeddings. This way, we factorize the attention to e.g. one 16 × k attention and one k × 16 attention, instead of regular the k × k self-attention which would be infeasibly slow for typical vocabulary sizes. We only add inter-token attention for the first token in every sequence. This improves performance on the sampled tokenizers, but does not improve performance on 'real-world' tokenizers (Table [9](#tab_8)); investigating this mismatch is a direction for future work.

## D Additional Hyperparameters

Hyperparameters for hypernetwork training are shown in Table [10](#tab_9). For continued training, we use the same optimizer, but sequence length of 512, batch size of 32, training for 50k steps and a constant learning rate chosen among the set {1e-6, 3e-6, 6e-6, 1e-5, 3e-5} to maximize performance. The chosen learning rate is 1e-6 for the runs keeping the original tokenizer (original@800M), 6e-6 for continued training starting from FOCUS (FOCUS@800M) and 3e-6 for continued training with the hypernetwork (ours@800M). 

## E Sensitivity to Tokenizer Size

Since the tokenizers we experiment with have similar vocabulary sizes (50k for the language-specific tokenizers and for GPT2, 49k for the StarCoder tokenizer) we conduct an additional experiment to quantify the sensitivity of the performance of our hypernetwork to the size of the target tokenizer. We find that although there is slight performance degradation when increasing the size of the new tokenizers' vocabulary, the hypernetwork is fairly robust to vocabulary size (Figure [4](#fig_4)).

## F Reliance on Vocabulary Overlap

Intuitively, transfer is easier the more the target has in common with the source. One way to measure commonality between the original (source) and the target tokenizer is the fraction of tokens of the  target vocabulary which also exist in the source vocabulary (vocabulary overlap). Performance correlates with vocabulary overlap, but it correlates more strongly with the probability for tokens to overlap: that is, when randomly sampling some token from a corpus tokenized with T b , the probability that this token also exists in the vocabulary of T a . We refer to this metric as p(overlap). p(overlap) has higher correlation with the performance of FOCUS, indicating that our hypernetwork depends less on overlap (Figure [5](#fig_5)).  

## G Additional LLM Results

Zero-shot and n-shot results for TinyLlama-1.1B are shown in Table [11](#tab_10) and MT-Bench results of transferring TinyLlama-1.1B-Chat-v1.0 in Table [12](#tab_11). We observe the same patterns as on Mistral-7B.

## H Assumptions on the Tokenization Function

In practice, besides the tokenization algorithm itself (e.g. BPE, UnigramLM) tokenization functions also contain other steps, in particular pretokenizing text into smaller chunks (usually words) on which to apply the tokenization function [(Mielke et al., 2021)](#b44). In our experiments, we assume fixed pretokenization given by a regular expression based on the regular expression used by GPT2 [(Radford et al., 2019)](#b55), adjusted to not over-segment text in languages using characters in the Unicode Mark category within words (e.g. Hindi and Tamil). We also add a prefix space (i.e., a whitespace at the start of the text to tokenize) if and only if the original tokenizer also uses a prefix space. Finally, we always add whitespace characters covering sequences of consecutive whitespaces up to 16 characters long similar to [Black et al. (2022)](#b4) to ensure code is tokenized efficiently. These light assumptions mostly preserve the generality of our method but could be further relaxed in future work.

![Figure 1: The hypernetwork predicts input and output embeddings based on the tokenizer.]()

![▷Compute the loss on the m texts in the current batch. 20: 21: update θ using ∇θ w.r.t. loss.]()

![Figure 2: The hypernetwork consists of a language model HLM θ learning to compose embeddings under the original tokenization into a new embedding and amortizes over the tokenization function.]()

![Figure 3: Language modeling loss of GPT2, and GPT2 with untied weight embeddings with and without the auxiliary loss across the first 50k training steps, excluding MIMICK-style warmup.]()

![Figure 4: Difference in accuracy to the original XLM-R model on XNLI of our method and FOCUS across vocabularies with size 30k, 50k, and 100k of the new tokenizer.]()

![Figure 5: Correlation of the difference in accuracy to the original XLM-R model with Unigram overlap probability p(overlap) (left) and vocabulary overlap (right).]()

![Performance of Mistral-7B-v0.1 after zero-shot and n-shot tokenizer transfer (training on 800M tokens). We evaluate transfer to the GPT2 tokenizer on natural language benchmarks and transfer to the StarCoder tokenizer on HumanEvalPack. Note that continued training with the original tokenizer (original@800M) does not consistently improve performance.]()

![Accuracy of Mistral-7B on XCOPA with language-specific tokenizers zero-shot transferred via FOCUS and our hypernetwork. The standard errors are between 2.1% and 2.3%.]()

![5-shot accuracy of Mistral-7B on multilingual MMLU with the original tokenizer and language-specific tokenizers zero-shot transferred via FOCUS and our hypernetwork. multinomial distribution as inConneau & Lample (2019) with α = 0.1. For the n-shot experiments, we also train on the StarCoder data, but substitute the English section of the MADLAD-400 corpus for Flan v2(Longpre et al., 2023) sampled as inSoldaini et al. (2024). 8]()

![Probability of pretokens sampled from the English MADLAD-400 data to be tokenized equivalently to the original tokenization when converting the tokenizer to byte-level (To Byte-Level) or to UnigramLM (Unigramify). Also shown is the LMs bits-per-character when applying the original vs. the corresponding UnigramLM tokenizer. Bits-per-character can not be measured for conversion to byte-level since extra tokens are added in this process (which there are no embeddings for).]()

![Bits-per-character of GPT2 with the original tokenizer and the tokenization function being original (left), unigramified (middle) and UnigramLM with scores set to the substring frequency of the tokens (right). We compare the original embeddings with embeddings predicted from our hypernetwork, with or without Gaussian noise in the sampling process.]()

![Performance of the hypernetwork in bits-per-byte with and without inter-token attention. Sampled Tokenizers are tokenizers as sampled during the training loop (c.f. Algorithm 1), en is an English UnigramLM tokenizer. The respective vocabulary sizes are shown in brackets.]()

![Hypernetwork hyperparameters.]()

![Performance of TinyLlama-1.1B after zero-shot and n-shot tokenizer transfer (training on 800M tokens), compare Table2.]()

![Single model rating results on MT-Bench of transferring TinyLlama-1.1B-Chat-v1.0 to the GPT2 tokenizer, compare Table12.]()

Some models share the input and the output embedding parameters (e.g.[Conneau et al., 2020)](#b13), this has been shown to be problematic[(Chung et al., 2021)](#b7) and many recent LLMs (e.g.[Jiang et al., 2023)](#b27) separate them.

See also[Mielke et al. (2021)](#b44) for a comprehensive overview of tokenizers.

In practice, implementing q as a queue allows efficiently caching the substrings and their probability p(t) at this step. They only need to be recomputed for the new m texts encountered in every batch.

To ensure substrings do not cross word boundaries we pretokenize the text before computing substrings.

In the multilingual case, we also append an element containing a learnable language-specific embedding.

We exclude languages without whitespace between words since they would require language-specific pretokenizers (e.g.[Sun, 2012)](#b62). Although our method is also applicable to this case, we leave this to future work.

We note, however, that FVT comes close to FOCUS' performance without requiring auxiliary embeddings so it may be a better choice for practical applications.

1/(1-14%)=16%, plus additional speedup due to attention scaling quadratically with sequence length.13  We refer to models purely pretrained on the Language Modeling task as base models.

Averaged across the input and the output embeddings.

Using the corrections from https://github.com/InflectionAI/Inflection-Benchmarks.

We estimate FLOPs on the basis of XLA-compiled instructions using Jax[(Bradbury et al., 2018)](#b5).

This is not sufficient for equivalence since order is ignored e.g. T (x) = {ab, a, b} and T (x) = {a, b, ab} fulfill the criterion but are not equivalent.

