- **Zero-Shot Tokenizer Transfer (ZeTT)**: A new problem defined to enable language models (LMs) to use arbitrary tokenizers without prior training data.
  
- **Hypernetwork Approach**: A hypernetwork is trained to predict embeddings for new tokenizers, allowing for effective ZeTT. This hypernetwork generalizes across different tokenizers.

- **Embedding Function**: The embedding function \( E_\phi: V \rightarrow \mathbb{R}^{d_{model}} \) maps tokens in vocabulary \( V \) to \( d_{model} \)-dimensional vectors.

- **Tokenization Function**: Tokenization is defined as \( T(x) := \text{argmax}_{C \in C_x} \sum_{t \in C} \log p(t) \), where \( C_x \) is the set of all possible tokenizations of \( x \).

- **Performance Metrics**: The hypernetwork maintains performance within a few percent accuracy compared to original models, with a notable reduction in tokenized sequence length.

- **Continued Training**: The gap in performance can be closed with continued training on less than 1 billion tokens, significantly less than previous methods requiring hundreds of billions.

- **Cross-Lingual Transfer**: Demonstrated effectiveness in cross-lingual tasks, achieving over 16% speedup while preserving performance on XNLI to within 1%.

- **Algorithm Overview**: The hypernetwork training loop involves sampling a corpus, computing substrings, normalizing frequencies, and assembling a new tokenizer based on the top substrings.

- **Embedding Initialization Heuristics**: Prior methods for embedding initialization include FVT, RAMEN, WECHSEL, and FOCUS, with FOCUS being the main baseline for comparison.

- **Applications**: The hypernetwork can be applied to both encoder and decoder models, and can transfer learned embeddings to fine-tuned variants without additional training.

- **Public Availability**: Code and models are accessible at [github.com/bminixhofer/zett](https://github.com/bminixhofer/zett).