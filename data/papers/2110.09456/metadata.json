{
  "arxivId": "2110.09456",
  "title": "NormFormer: Improved Transformer Pretraining with Extra Normalization",
  "authors": "Sam Shleifer, Jason Weston, Myle Ott",
  "abstract": "During pretraining, the Pre-LayerNorm transformer suffers from a gradient\nmagnitude mismatch: gradients at early layers are much larger than at later\nlayers. These issues can be alleviated by our proposed NormFormer architecture,\nwhich adds three normalization operations to each layer: a Layer Norm after\nself attention, head-wise scaling of self-attention outputs, and a Layer Norm\nafter the first fully connected layer. The extra operations incur negligible\ncompute cost (+0.4% parameter increase), but improve pretraining perplexity and\ndownstream task performance for both causal and masked language models ranging\nfrom 125 Million to 2.7 Billion parameters. For example, adding NormFormer on\ntop of our strongest 1.3B parameter baseline can reach equal perplexity 24%\nfaster, or converge 0.27 perplexity better in the same compute budget. This\nmodel reaches GPT3-Large (1.3B) zero shot performance 60% faster. For masked\nlanguage modeling, NormFormer improves fine-tuned GLUE performance by 1.9% on\naverage. Code to train NormFormer models is available in fairseq\nhttps://github.com/pytorch/fairseq/tree/main/examples/normformer .",
  "url": "https://arxiv.org/abs/2110.09456",
  "issue_number": 850,
  "issue_url": "https://github.com/dmarx/papers-feed/issues/850",
  "created_at": "2025-01-07T23:18:44.185821",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 36,
  "last_read": "2025-01-07T23:18:44.187174",
  "last_visited": "2025-01-07T23:17:21.742Z",
  "main_tex_file": null,
  "published_date": "2021-10-18T16:47:45Z",
  "arxiv_tags": [
    "cs.CL",
    "cs.AI"
  ]
}