# Probabilistic Programming Interfaces for Random Graphs: Markov Categories, Graphons, and Nominal Sets

## Abstract

## 

We study semantic models of probabilistic programming languages over graphs, and establish a connection to graphons from graph theory and combinatorics. We show that every well-behaved equational theory for our graph probabilistic programming language corresponds to a graphon, and conversely, every graphon arises in this way.

We provide three constructions for showing that every graphon arises from an equational theory. The first is an abstract construction, using Markov categories and monoidal indeterminates. The second and third are more concrete. The second is in terms of traditional measure theoretic probability, which covers 'black-and-white' graphons. The third is in terms of probability monads on the nominal sets of Gabbay and Pitts. Specifically, we use a variation of nominal sets induced by the theory of graphs, which covers ErdÅ‘s-RÃ©nyi graphons. In this way, we build new models of graph probabilistic programming from graphons.

CCS Concepts: â€¢ Theory of computation â†’ Semantics and reasoning; Probabilistic computation.

## INTRODUCTION

This paper is about the semantic structures underlying probabilistic programming with random graphs. Random graphs have applications in statistical modelling across biology, chemistry, epidemiology, and so on, as well as theoretical interest in graph theory and combinatorics (e.g. [[Bornholdt and Schuster 2002]](#)). Probabilistic programming, i.e. programming for statistical modelling [van de [Meent et al. 2018]](#b81), is useful for building the statistical models for the applications. Moreover, as we show (Theorem 23 and Corollary 26), the semantic aspects of programming languages for random graphs correspond to graphons [[LovÃ¡sz 2012](#b62)], a core structure in graph theory and combinatorics.

To set the scene more precisely, we recall the setting of probabilistic programming with realvalued distributions, and contrast it with the setting with graphs. Many probabilistic programming languages provide a type of real numbers (real) and distributions such as the normal distribution normal : real * real â†’ real

(1)

together with arithmetic operations such as (+) : real * real â†’ real.

(2)

Even if we encounter an unfamiliar distribution over (real) in a library, we have a rough idea of how to explain what it could be, in terms of probability densities and measures.

In this paper, we consider the setting of probabilistic programming with graphs, where the probabilistic programming language or library provides a type (vertex) and some distribution new : unit â†’ vertex

(3) together with a test edge : vertex * vertex â†’ bool.

(4)

Our goal is to analyze the interface (vertex, new, edge) for graphs semantically, and answer, for instance, what they could be and what they could do. We give one example analysis in Section 1.1 first, and the general one later in Theorem 23 and Corollary 26, which says that to give an implementation of (vertex, new, edge), satisfying the laws of probabilistic programming, is to give a graphon. In doing so, we connect the theory of probabilistic programming with graph theory and combinatorics.

Probabilistic programming is generally used for statistical inference, in which we describe a generative model by writing a program using primitives such as (1)-( [4](#)) above, and then infer a distribution on certain parameters, given particular observed data. This paper is focused on the generative model aspect, and not inference (although for simple examples, generic inference methods apply immediately, see Â§1.5).

## Example of an Implementation of a Random Graph: Geometric Random Graphs

To illustrate the interface (vertex, new, edge) of ( [3](#))-( [4](#)), we consider for illustration a random geometric graph (e.g. [[Bubeck et al. 2016;](#b14)[Penrose 2003]](#b70)) where the vertices are points on the surface of the unit sphere, chosen uniformly at random, and where there is an edge between two vertices if the angle between them is less than some fixed ğœƒ . This random graph might be used, for instance, to model the connections between people on the earth.

For example, a simple statistical inference problem might start from the observed connectivity in Figure [1](#fig_1)(a). We might ask for the distribution on ğœƒ given that this graph arose from the spherical random geometric graph. One sample from this posterior distribution on random geometric graphs with ğœƒ = ğœ‹/3 is shown in Figure [1(b)](#fig_1). Another, unconditioned sample from the random geometric graph with ğœƒ = ğœ‹/6 is shown in Figure [1(c)](#fig_1). We can regard this example as an implementation of the interface (vertex, new, edge) as follows: we implement (vertex) as the surface of the sphere (e.g. implemented as Euclidean coordinates).

â€¢ new() : vertex randomly picks a new vertex as a point on the sphere uniformly at random. Figure [1](#fig_1)(c) shows the progress after calling new() 15 times. â€¢ edge : vertex * vertex â†’ bool checks whether there is an edge between two vertices; this amounts to checking whether the angle between two points is less than ğœƒ . randomly returns true or false; the probability of true is the probability of a triangle. This implementation using the sphere is only one way to implement (vertex, new, edge). There are implementations using higher-dimensional spheres, or other geometric objects. We can also consider random equivalence relations as graphs, i.e. disjoint unions of complete graphs, or random bipartite graphs, which are triangle-free. We can consider the ErdÅ‘s-RÃ©nyi random graph, where the chance of an edge between two vertices is independent of the other edges, and has a fixed probability. These are all different implementations of the same abstract interface, (vertex, new, edge), and programs such as (5) make sense for all of them. The point of this paper is to characterize all these implementations, as graphons.

## Implementations Regarded as Equational Theories

The key method of this paper is to treat implementations of the interface (vertex, new, edge) extensionally, as equational theories. That is, rather than looking at specific implementation details, we look at the equations between programs that a user of the implementation would rely on. (This is analogous to the idea in model theory of studying first-order theories rather than specific models; similar ideas arise in the algebraic theory of computational effects [Plotkin and [Power 2002]](#).) For example, if an implementation always provides a bipartite random graph, we have the equation [Program (5)](#) â‰¡ false between programs, because a triangle is never generated. This equation does not hold in the example of Figure [1(b-c](#fig_1)), since triangles are possible. We focus on a class of equational theories that are well behaved, as follows. First, we suppose that they contain basic laws for probabilistic programming (eqns. ( [7](#)) -(11), Â§2.2). This basic structure already appears broadly in different guises, including in Moggi's monadic metalanguage [[Moggi 1989](#b67)], in linear logic [[Ehrhard and Tasson 2019]](#b26), and in synthetic probability theory [[Fritz 2020](#b32)]. Second, we suppose that the equational theories are equipped with a 'Bernoulli base', which means that although we do not specify an implementation for the type (vertex), each closed program of type (bool) is equated with some ordinary Bernoulli distribution, in such a way as to satisfy the classical laws of traditional finite probability theory ( Â§ 2.4). Finally, we suppose that the edge relation is symmetric (the graphs are undirected) and that it doesn't change when the same question is asked multiple times ('deterministic'), e.g. let ğ‘ = new() in let ğ‘ = new() in edge(ğ‘, ğ‘) & Â¬edge(ğ‘, ğ‘) â‰¡ false.

(6)

A graphon is a symmetric measurable function [0, 1] 2 â†’ [0, 1]. We show that every equational theory for the interface [(vertex, new, edge)](#) gives rise to a graphon (Theorem 23), and conversely that every graphon arises in this way [(Corollary 26)](#).

We emphasize that this abstract treatment of implementations, in terms of equational theories, is very open-ended, and permits a diverse range of implementation methods. Indeed, we show in Section 5 that any implementation using traditional measure-theoretic methods will only produce black-and-white graphons, so this abstract treatment is crucial.

## From Equational Theories to Graphons

In Section 3, we show how an equational theory over programs in the interface [(vertex, new, edge)](#) gives rise to a graphon. The key first step is that graphons (modulo equivalence) can be characterized in terms of sequences of finite random graphs that satisfy three conditions: exchangeability, consistency, and locality.

To define a graphon, we show how to define programs that describe finite random graphs, by using new and edge to build boolean-valued ğ‘› Ã— ğ‘› adjacency matrices, for all ğ‘› (shown in ( [18](#formula_30))). Assuming that the equational theory of programs is Bernoulli-based, these programs can be interpreted as probability distributions on the finite spaces of adjacency matrices which, we show, are finite random graphs.

It remains to show that the induced sequence of random graphs satisfies the three conditions for graphons (exchangeability, consistency, and locality). These can be formulated as equational properties, and so they can be verified by using the equational reasoning in the equational theory. This is Theorem 23. A key part of the proof is the observation that exchangeability for graphons connects to commutativity of let (9): we can permute the order in which vertices are instantiated without changing the distributions.

## From Graphons to Equational Theories

We also show the converse: every graphon arises from a good equational theory for the interface [(vertex, new, edge)](#). We look at this from three angles: first, we prove this in the general case using an abstract method, and then, we use concrete methods for two special cases.

Fixing a graphon, we build an equational theory by following a categorical viewpoint. A good equational theory for probabilistic programming amounts to a 'distributive Markov category', which is a monoidal category with coproducts that is well-suited to probability ( Â§2.2 and [[Fritz 2020]](#b32)). The idea that distributive categories are a good way to analyze abstract interfaces goes back at least to [[Walters 1989](#b83)], which used distributive categories to study interfaces for stacks and storage. We can thus use now-standard abstract methods for building monoidal and distributive categories to build an equational theory for the programming language.

We proceed in two steps. We first use methods such as [[Hermida and Tennent 2012;](#b39)[Hu and Tholen 1995]](#b41) to build an abstract distributive Markov category that supports the interface (vertex, new, edge) in a generic way. This equational theory is generic and not Bernoulli-based: although it satisfies the equational laws of probabilistic programming, there is no given connection to traditional probability. The second step is to show that (a) it is possible to quotient this generic category to get Bernoulli-based equational theories; (b) the choices of quotient are actually in bijective correspondence with graphons. Thus, we can build an equational theory from which any given graphon arises, via (18): this is Corollary 26. (The framework of Bernoulli-based Markov categories is new here, and the techniques of [[Hermida and Tennent 2012;](#b39)[Hu and Tholen 1995]](#b41) have not previously been applied in categorical probability, so a challenge for future work is to investigate these ideas in other aspects of categorical probability.)

Although this is a general method, it is an abstract method involving quotient constructions. The ideal form of denotational semantics is to explain what programs are by regarding them as functions between certain kinds of spaces. Although Corollary 26 demonstrates that every graphon arises from an equational theory, the type (vertex) is interpreted as an object of an abstract category, and programs are equivalence classes of abstract morphisms. In the remainder of the paper, we give two situations where we can interpret (vertex) as a genuine concrete space, and programs are functions or distributions on spaces. Such an interpretation immediately yields an equational theory, where two programs are equal if they have the same interpretation.

â€¢ Section 5: For 'black-and-white graphons', we present measure-theoretic models of the interface, based on a standard measure-theoretic interpretation of probabilistic programming (e.g. [[Kozen 1981]](#b60)). We interpret (vertex) as a measurable space, and (new) as a probability measure on it, and (edge) in terms of a measurable predicate. Then, the composition of programs is defined in terms of probability kernels and Lebesgue integration. This kind of model exactly captures the black-and-white graphons (Prop. 29). â€¢ Section 6: For 'ErdÅ‘s-RÃ©nyi' graphons, which are constantly gray, and not black-and-white, we present a model based on Rado-nominal sets ( Â§6.1). These are a variant of nominal sets ( [[Gabbay and Pitts 1999;](#b34)[Pitts 2013]](#b71)) where the atoms are vertices of the Rado graph (following [[BojaÅ„czyk et al. 2014]](#b12)). We consider a new notion of 'internal probability measure' in this setting, and use this to give a compositional semantics that gives rise to the ErdÅ‘s-RÃ©nyi graphons (Corollary 45).

Together, these more concrete sections then provide further intuition for the correspondence between equational theories and graphons.

## Connection to Practice

We conclude this introduction with remarks on the connection to practical modelling. In practice, the graph interface might form part of a generative model, on which we perform inference. The structure is clearest in a typed language, and one example is the LazyPPL library for Haskell [[Dash et al. 2023](#b24) We can use this as a building block for more complex models. For a simple example, we generated Figure [1](#fig_1)(b) by using the generic Metropolis-Hastings inference of the LazyPPL library to infer ğœƒ given a particular graph (Fig. [1](#fig_1)(a)). We have also implemented other random graphs; our implementation of the ErdÅ‘s-RÃ©nyi graph uses stochastic memoization [[Kaddar and Staton 2023;](#b52)[Roy et al. 2008](#b74)].

Summary and context. As we have discussed, our main result is that equational theories for the programming interface ( Â§1.1) give rise to graphons ( Â§1.3) and every graphon arises in this way ( Â§1.4). These results open up new ways to study random graphs, by using programming semantics. On the other hand, our results here put the abstractions of practical probabilistic programming on a solid theoretical foundation (see also Â§7).

## PROGRAMMING INTERFACES FOR RANDOM GRAPHS: EQUATIONAL THEORIES AND MARKOV CATEGORIES

In Section 1.1, we considered probabilistic programming over a graph interface. To make this formal, we now recall syntax, types, and equational reasoning for simple probabilistic programming languages. We begin with a general syntax ( Â§2.1), which can accommodate various interfaces in the form of type and term constants, including the interface for graphs (Ex. 1(3)).

We study different instantiations of the probabilistic programming language in terms of the equational theories that they satisfy. We consider two equivalent ways of understanding equational theories: as distributive Markov categories ( Â§2.2) and in terms of affine monads ( Â§2.3). Markov categories are a categorical formulation of probability theory (e.g. [[Fritz 2020]](#b32)), and affine monads arise in the categorical analysis of probability (e.g. [[Fritz et al. 2023;](#b33)[Jacobs 2018;](#b45)[Kock 2012]](#b59)) as well as in the semantics for probabilistic programming (e.g. [Azevedo de [Amorim 2023;](#b10)[Dahlqvist et al. 2018;](#b23)[Dash et al. 2023]](#b24)). We make a connection with traditional probability via the notion of Bernoulli base ( Â§2.4).

Much of this section will be unsurprising to experts: the main purpose is to collect definitions and results. The definition of distributive Markov category appears to be novel, and so we go over that definition and correspondence with monads (Propositions 8 and 13). In Section 2.5, we give a construction for quotienting a distributive Markov category, which we will need in Section 4. We include the result in the section because it may be of independent interest.

## Syntax for a Generic Probabilistic Programming Language

Our generic probabilistic programming language is, very roughly, an idealized, typed fragment of a typical language like Church [[Goodman et al. 2008](#b37)]. We start with a simple programming language (following [[Ehrhard and Tasson 2019;](#b26)[Staton 2017;](#)[Stein 2021]](#b78) but also [[Moggi 1989](#b67)]) with at least the following product and sum type constructors:

$Î“ âŠ¢ ğ‘¡ : 0 Î“ âŠ¢ case ğ‘¡ of {} : ğµ Î“ âŠ¢ ğ‘¡ : ğ´ 1 + ğ´ 2 Î“, ğ‘¥ ğ‘– : ğ´ ğ‘– âŠ¢ ğ‘¢ ğ‘– : ğµ ğ‘– âˆˆ {1,2} Î“ âŠ¢ case ğ‘¡ of {in 1 (ğ‘¥ 1 ) â‡’ ğ‘¢ 1 ; in 2 (ğ‘¥ 2 ) â‡’ ğ‘¢ 2 } : ğµ (Here, a context Î“ is a sequence of assignments of types ğ´ to variables ğ‘¥.)$In what follows, we use shorthands such as bool = unit + unit, and if-then-else instead of case. This language is intended to be a generic probabilistic programming language, but so far there is nothing specifically probabilistic about this syntax. Different probabilistic programming languages support distributions over different kinds of structures. Thus, our language is extended according to an 'interface' by specifying type constants and typed term constants ğ‘“ : ğ´ â†’ ğµ. For each term constant ğ‘“ : ğ´ â†’ ğµ, we include a new typing rule, Î“ âŠ¢ ğ‘¡ : ğ´ Î“ âŠ¢ ğ‘“ (ğ‘¡) : ğµ Example 1. We consider the following examples of interfaces.

(1) For probabilistic programming over finite domains, we may have term constants such as bernoulli 0.5 : unit â†’ bool, intuitively a fair coin toss.

(2) For probabilistic programming over real numbers, we may have a type constant real and term constants such as normal : real * real â†’ real, intuitively a parameterized normal distribution, and arithmetic operations such as (+) : real * real â†’ real.

(3) The main interface of this paper is for random graphs: this has a type constant vertex and term constants new : unit â†’ vertex and edge : vertex * vertex â†’ bool.

(We have kept this language as simple as possible, to focus on the interesting aspects. A practical probabilistic programming language will include other features, which are largely orthogonal, and indeed within our implementation in Haskell ( Â§1.5), programming features like higher order functions and recursion are present and useful. See also the discussion in Â§2.3.4.)

## Equational Theories and Markov Categories

Section 2.1 introduced a syntax for various probabilistic programming interfaces. The idea is that this is a generic language which applies to different interfaces with different distributions that are implemented in different ways. Rather than considering various ad hoc operational semantics, we study the instances of interfaces by the program equations that they support.

Regardless of the specifics of a particular implementation, we expect basic equational reasoning principles for probabilistic programming to hold, such as the following laws:

$(let ğ‘¦=(let ğ‘¥=ğ‘¡ in ğ‘¢) in ğ‘¡ â€² ) â‰¡ (let ğ‘¥=ğ‘¡ in let ğ‘¦=ğ‘¢ in ğ‘¡ â€² ) (where ğ‘¥ âˆ‰ fv(ğ‘¡ â€² )) (7) (ğ‘¡, ğ‘¢) â‰¡ (let ğ‘¥ = ğ‘¡ in let ğ‘¦ = ğ‘¢ in (ğ‘¥, ğ‘¦)) (8) (let ğ‘¥=ğ‘¡ in let ğ‘¥ â€² =ğ‘¡ â€² in ğ‘¢) â‰¡ (let ğ‘¥ â€² =ğ‘¡ â€² in let ğ‘¥=ğ‘¡ in ğ‘¢)$(where ğ‘¥ âˆ‰ fv(ğ‘¡ â€² ) and ğ‘¥ â€² âˆ‰ fv(ğ‘¡)) (9)

$(let ğ‘¥ = ğ‘¡ â€² in ğ‘¡) â‰¡ ğ‘¡ (where ğ‘¥ âˆ‰ fv(ğ‘¡))(10)$The following law does not always hold, but does hold when ğ‘£ is 'deterministic'.

$(let ğ‘¥ = ğ‘£ in ğ‘¡) â‰¡ ğ‘¡ [ğ‘£/ğ‘¥](11)$Equations ( [9](#)) and (10) say that parts of programs can be re-ordered and discarded, as long as the dataflow is respected. This is a feature of probabilistic programming. For example, coins do not remember the order nor how many times they have been tossed. But these equations would typically not hold in a language with state.

The cleanest way to study equational theories of programs is via a categorical semantics, and for Markov categories have arisen as a canonical setting for categorical probability. Informally, a category is a structure for composition, and this matches the composition structure of let in our language. We also have monoidal structure which allows for the type constructor ğ´ Ã— ğµ and for the compound contexts Î“, comonoid structure which allows duplication of variables, and distributive coproduct structure which allows for the sum types. Definition 2. A symmetric monoidal category (C, âŠ—, ğ¼ ) is a category C equipped with a functor âŠ— : C Ã— C â†’ C and an object ğ¼ together with associativity, unit and symmetry structure ( [[Mac Lane 1998, XI.1.]](#)). A Markov category ( [[Fritz 2020]](#b32)) is a symmetric monoidal category in which

â€¢ the monoidal unit ğ¼ is a terminal object (ğ¼ = 1), and â€¢ every object ğ‘‹ is equipped with a comonoid Î” ğ‘‹ : ğ‘‹ â†’ ğ‘‹ âŠ— ğ‘‹ , compatible with the tensor product (Î” ğ‘‹ âŠ—ğ‘Œ = (ğ‘‹ âŠ— swp âŠ— ğ‘Œ ) â€¢ (Î” ğ‘‹ âŠ— Î” ğ‘Œ ), where swp is the swap map of C).

A morphism ğ‘“ : ğ‘‹ â†’ ğ‘Œ in a Markov category is deterministic if it commutes with the comonoids:

$(ğ‘“ âŠ— ğ‘“ ) â€¢ Î” ğ‘‹ = Î” ğ‘Œ â€¢ ğ‘“ .$A distributive symmetric monoidal category (e.g. [[Jay 1993;](#b48)[Walters 1989]](#b83)) is a symmetric monoidal category equipped with chosen finite coproducts such that the canonical maps ğ‘‹ âŠ— ğ‘ + ğ‘Œ âŠ— ğ‘ â†’ (ğ‘‹ + ğ‘Œ ) âŠ— ğ‘ and 0 â†’ 0 âŠ— ğ‘ are isomorphisms. A distributive Markov category is a Markov category whose underlying monoidal category is also distributive and whose chosen coproduct injections ğ‘‹ â†’ ğ‘‹ + ğ‘Œ â† ğ‘Œ are deterministic. A distributive category [[Carboni et al. 1993;](#b18)[Cockett 1993](#b20)] is a distributive Markov category where all morphisms are deterministic.

A (strict) distributive Markov functor is a functor ğ¹ : C â†’ D between distributive Markov categories which strictly preserves the chosen symmetric monoidal, coproduct, and comonoid structures.

In this paper we mainly focus on functors between distributive Markov categories that strictly preserve the relevant structure, so we elide 'strict'. (Nonetheless, non-strict functors are important, e.g. [[Fritz 2020, Â§10](#).2] and Prop. 13.)

We interpret the language of Section 2.1 in a distributive Markov category C by interpreting types ğ´ and type contexts Î“ as objects ğ´ and Î“ , and typed terms Î“ âŠ¢ ğ‘¡ : ğ´ as morphisms

$Î“ Î” Î“ ---â†’ Î“ âŠ— Î“ Î“ âŠ— ğ‘¡ ------â†’ Î“ âŠ— ğ´ 1 + ğ´ 2 Î“, ğ‘¥ : ğ´ 1 + Î“, ğ‘¥ : ğ´ 2 âŸ¨ ğ‘¢ 1 , ğ‘¢ 2 âŸ© --------â†’ ğµ Î“ âŠ¢ ğ‘“ (ğ‘¡) : ğµ = Î“ ğ‘¡ --â†’ ğ´ ğ‘“ --â†’ ğµ$An interpretation in a Markov category induces an equational theory between programs: let Î“ âŠ¢ ğ‘¡ = ğ‘¢ : ğ´ if ğ‘¡ = ğ‘¢ . Proposition 3 (e.g. [[Stein 2021](#b78)], Â§7.1). The equational theory induced by the interpretation in a distributive Markov category, with given interpretations of type and term constants, always includes the equations (7)-( [10](#formula_2)), and also (11) whenever ğ‘£ is a deterministic morphism.

Example 4. The category (FinSet, Ã—, 1) of finite sets is a distributive Markov category. As in any category with products, each object has a unique comonoid structure, and all morphisms are deterministic. This is a good Markov category for interpreting the plain language with no type or term constants. For example, bool is a set with two elements.

Example 5. The category FinStoch has natural numbers as objects and the morphisms are stochastic matrices. In more detail, a morphism ğ‘š â†’ ğ‘› is a matrix in (R â‰¥0 ) ğ‘šÃ—ğ‘› such that each row sums to 1. Composition is by matrix multiplication. The monoidal structure is given on objects by multiplication of numbers, and on morphisms by Kronecker product of matrices. By choosing an enumeration of each finite set, we get a functor FinSet â†’ FinStoch that converts a function to the corresponding (0/1)-valued matrix. So every object of FinStoch can be regarded with the comonoid structure from FinSet. The deterministic morphisms in FinStoch are exactly the morphisms from FinSet [[Fritz 2020, 10.3]](#). This is a good Markov category for interpreting the language with Bernoulli distributions (Ex. 1(1)). We interpret the fair coin as the 1 Ã— 2 matrix (0.5, 0.5).

We can also give some interpretations for the graph interface (Ex. 1(3)) in FinStoch. For instance, consider random graphs made of two disjoint complete subgraphs, as is typical in a clustering model. We can interpret this by putting vertex = 2, edge = ( 1 0 0 1 0 1 1 0 ) âŠ¤ , and new = (0.5, 0.5).

We look at other examples of distributive Markov categories and interpretations of these interfaces in [Sections 2.3.2](#) and [2.3.3,](#) and [then in Sections 4-6.](#) 2.3 Equational Theories and Affine Monads 2.3.1 Distributive Markov Categories from Affine Monads. One way to generate equational theories via Markov categories is by considering certain kinds of monads, following Moggi [[Moggi 1989](#b67)]. Definition 6. A strong monad on a category A with finite products is given by

â€¢ for each object ğ‘‹ , an object ğ‘‡ (ğ‘‹ );

â€¢ for each object ğ‘‹ , a morphism ğœ‚ ğ‘‹ : ğ‘‹ â†’ ğ‘‡ (ğ‘‹ );

â€¢ for objects ğ‘, ğ‘‹, ğ‘Œ , a family of functions natural in ğ‘

$(>>=) : A(ğ‘,ğ‘‡ (ğ‘‹ )) Ã— A(ğ‘ Ã— ğ‘‹,ğ‘‡ (ğ‘Œ )) â†’ A(ğ‘,ğ‘‡ (ğ‘Œ ))$such that >>= is associative with unit ğœ‚.

(There are various different formulations of this structure. When A is cartesian closed, as in Defs. 9 and 41, then the bind (>>=) is represented by a morphism (>>=) : ğ‘‡ (ğ‘‹ ) Ã— (ğ‘‹ â‡’ ğ‘‡ (ğ‘Œ )) â†’ ğ‘‡ (ğ‘Œ ), by the Yoneda lemma.) Definition 7 ( [[Jacobs 1994;](#b43)[Kock 1970;](#b58)[Lindner 1979]](#b61)). Given a strong monad ğ‘‡ , we say that two morphisms ğ‘“ :

$ğ‘‹ 1 â†’ ğ‘‡ (ğ‘‹ 2 ), ğ‘” : ğ‘‹ 1 â†’ ğ‘‡ (ğ‘‹ 3 ) commute if ğ‘“ >>= ((ğ‘” â€¢ ğœ‹ 1 ) >>= (ğœ‚ â€¢ âŸ¨ğœ‹ 2 â€¢ ğœ‹ 1 , ğœ‹ 2 âŸ©)) = ğ‘” >>= ((ğ‘“ â€¢ ğœ‹ 1 ) >>= (ğœ‚ â€¢ âŸ¨ğœ‹ 2 , ğœ‹ 2 â€¢ ğœ‹ 1 âŸ©)) : ğ‘‹ 1 â†’ ğ‘‡ (ğ‘‹ 2 Ã— ğ‘‹ 3 ). A strong monad is commutative if all morphisms commute. It is affine if ğ‘‡ (1) â†’ 1 is an isomorphism.$The Kleisli category Kl(ğ‘‡ ) of a strong monad ğ‘‡ has the same objects as A, but the morphisms are different: Kl(ğ‘‡ ) (ğ´, ğµ) = A(ğ´,ğ‘‡ (ğµ)). There is a functor ğ½ : A â†’ Kl(ğ‘‡ ), given on morphisms by composing with ğœ‚ (e.g. [[Mac Lane 1998, Â§VI.5](#)], [[Moggi 1989]](#b67)). Proposition 8. Let ğ‘‡ be a strong monad on a category A. If ğ‘‡ is commutative and affine and A has finite products, then the Kleisli category Kl(ğ‘‡ ) has a canonical structure of a Markov category. Furthermore, if A is distributive, then Kl(ğ‘‡ ) can be regarded as a distributive Markov category.

Proof notes. The Markov structure follows [[Fritz 2020, Â§3]](#). Since ğ‘‡ is commutative, the product structure of A extends to a symmetric monoidal structure on Kl(ğ‘‡ ). Since ğ‘‡ (1) = 1, the monoidal unit (1) is terminal in Kl(ğ‘‡ ). Every object in A has a comonoid structure, and this is extended to Kl(ğ‘‡ ) via ğ½ . The morphisms in the image of ğ½ are deterministic, although this need not be a full characterization of determinism.

For the distributive structure, recall that ğ½ preserves coproducts and indeed it has a right adjoint. Hence, the coproduct injections will be deterministic. â–¡

We can thus interpret the language of Section 2.1 using any strong monad, interpreting the types ğ´ as objects ğ´ of A, and a term Î“ âŠ¢ ğ‘¡ : ğ´ as a morphism ğ‘¡ : Î“ â†’ ğ‘‡ ( ğ´ ). This interpretation matches Moggi's interpretation of the language of Section 2.1 in a strong monad.

## Example Affine Monad: Distribution Monad.

Definition 9 (e.g. [[Jacobs 2016](#b44)], Â§4.1). The distribution monad D on Set is defined as follows:

â€¢ On objects: each set ğ‘‹ is mapped to the set of all finitely-supported discrete probability measures on ğ‘‹ , that is, all functions ğ‘ : ğ‘‹ â†’ R that are non-zero for only finitely many elements and satisfy ğ‘¥ âˆˆğ‘‹ ğ‘ (ğ‘¥) = 1. â€¢ The unit ğœ‚ ğ‘‹ : ğ‘‹ â†’ D (ğ‘‹ ) maps ğ‘¥ âˆˆ ğ‘‹ to the indicator function ğœ†ğ‘¦. [ğ‘¦ = ğ‘¥], i.e. the Dirac distribution ğ›¿ ğ‘¥ . â€¢ The bind function (>>=) is defined as follows:

$(ğ‘“ >>= ğ‘”) (ğ‘§) (ğ‘¦) = ğ‘¥ âˆˆğ‘‹ ğ‘“ (ğ‘§) (ğ‘¥) â€¢ ğ‘”(ğ‘§, ğ‘¥) (ğ‘¦)$By the standard construction for strong monads, each morphism ğ‘“ : ğ‘‹ â†’ ğ‘Œ gets mapped to D ğ‘“ : Dğ‘‹ â†’ Dğ‘Œ , that is, the pushforward in this case:

$D ğ‘“ (ğ‘) (ğ‘¦) = ğ‘¥ âˆˆ ğ‘“ -1 (ğ‘¦) ğ‘ (ğ‘¥).$Consider the language with no type constants, and just the term constant bernoulli 0.5 (Ex. 1( [1](#))). This can be interpreted in the distribution monad. Since every type ğ´ is interpreted as a finite set ğ´ , and every context Î“ as a finite set Î“ , a term Î“ âŠ¢ ğ‘¡ : ğ´ is interpreted as a function Î“ â†’ D ğ´ . To give a Kleisli morphism between finite sets is to give a stochastic matrix, and so the induced equational theory is the same as the interpretation in FinStoch (Ex. 5).

## Example Affine

Monad: Giry Monad. We recall some rudiments of measure-theoretic probability.

Definition 10. A ğœ-algebra on a set is a non-empty collection of subsets that contains the empty set and is closed under countable unions and complements. A measurable space is a pair (ğ‘‹, Î£) of a set and a ğœ-algebra on it. A measurable function

$(ğ‘‹, Î£ ğ‘‹ ) â†’ (ğ‘Œ , Î£ ğ‘Œ ) is a function ğ‘“ : ğ‘‹ â†’ ğ‘Œ such that ğ‘“ -1 (ğ‘ˆ ) âˆˆ Î£ ğ‘‹ for all ğ‘ˆ âˆˆ Î£ ğ‘Œ .$A probability measure on a measurable space (ğ‘‹, Î£) is a function ğœ‡ : Î£ â†’ [0, 1] that has total mass 1 (ğœ‡ (ğ‘‹ ) = 1) and that is ğœ-additive: ğœ‡ ( âˆ ğ‘–=1 ğ‘ˆ ğ‘– ) = âˆ ğ‘–=1 ğœ‡ (ğ‘ˆ ğ‘– ) for any sequence of disjoint ğ‘ˆ ğ‘– . Examples of measurable spaces include: the finite sets ğ‘‹ equipped with their powerset ğœ-algebras; the unit interval [0, 1] equipped with its Borel ğœ-algebra, which is the least ğœ-algebra containing the open sets. Examples of probability measures include: discrete probability measures (Def. 9); the uniform measure on [0, 1]; the Dirac distribution

$ğ›¿ ğ‘¥ (ğ‘ˆ ) = [ğ‘¥ âˆˆ ğ‘ˆ ].$The product of two measurable spaces (ğ‘‹, Î£ ğ‘‹ ) Ã— (ğ‘Œ , Î£ ğ‘Œ ) = (ğ‘‹ Ã— ğ‘Œ , Î£ ğ‘‹ âŠ— Î£ ğ‘Œ ) comprises the product of sets with the least ğœ-algebra making the projections ğ‘‹ â† ğ‘‹ Ã— ğ‘Œ â†’ ğ‘Œ measurable. The category of measurable spaces and measurable functions is a distributive category.

A probability kernel between measurable spaces (ğ‘‹, Î£ ğ‘‹ ) and (ğ‘Œ , Î£ ğ‘Œ ) is a function ğ‘˜ : ğ‘‹ Ã— Î£ ğ‘Œ â†’ [0, 1] that is measurable in the first argument and that is ğœ-additive and has mass 1 in the second argument.

To compose probability kernels, we briefly recall Lebesgue integration. Consider a measurable space (ğ‘‹, Î£ ğ‘‹ ), a measure ğœ‡ : Î£ ğ‘‹ â†’ [0, 1], and a measurable function

$ğ‘“ : ğ‘‹ â†’ [0, 1]. If ğ‘“ is a simple function, i.e. ğ‘“ (ğ‘¥) = ğ‘š ğ‘–=1 ğ‘Ÿ ğ‘– â€¢ [ğ‘¥ âˆˆ ğ‘ˆ ğ‘– ] for some ğ‘š, ğ‘Ÿ ğ‘– âˆˆ [0, 1], and ğ‘ˆ ğ‘– âˆˆ Î£ ğ‘‹ , the Lebesgue integral âˆ« ğ‘“ dğœ‡ = âˆ« ğ‘“ (ğ‘¥) ğœ‡ (dğ‘¥) âˆˆ [0, 1] is defined to be ğ‘š ğ‘–=1 ğ‘Ÿ ğ‘– Ã— ğœ‡ (ğ‘ˆ ğ‘– ).$If ğ‘“ is not a simple function, there exists a sequence of increasing simple functions ğ‘“ 1 , ğ‘“ 2 , . . . : ğ‘‹ â†’ [0, 1] such that sup ğ‘˜ ğ‘“ ğ‘˜ (ğ‘¥) = ğ‘“ (ğ‘¥) (for example, by taking ğ‘“ ğ‘˜ (ğ‘¥) def = âŒŠ10 ğ‘˜ ğ‘“ (ğ‘¥)âŒ‹/10 ğ‘˜ ). In that case, the integral is defined to be the limit of the integrals of the ğ‘“ ğ‘˜ 's (which exists by monotone convergence).

Probability kernels can be equivalently formulated as morphisms ğ‘‹ â†’ G(ğ‘Œ ), where G is the Giry monad: Definition 11 ( [[Giry 1980]](#b36)). The Giry monad G is a strong monad on the category Meas of measurable spaces given by â€¢ G(ğ‘‹ ) is the set of probability measures on ğ‘‹ , with the least ğœ-algebra making âˆ« ğ‘“ d(-) : G(ğ‘‹ ) â†’ [0, 1] measurable for all measurable ğ‘“ : ğ‘‹ â†’ [0, 1];

â€¢ the unit ğœ‚ maps ğ‘¥ to the Dirac distribution ğ›¿ ğ‘¥ ;

â€¢ the bind is given by composing kernels:

$(ğ‘˜ >>= ğ‘™) (ğ‘§, ğ‘ˆ ) = âˆ« ğ‘™ ((ğ‘§, ğ‘¥), ğ‘ˆ ) ğ‘˜ (ğ‘§, dğ‘¥). (12$$)$Proposition 12. The monad G is commutative and affine.

Proof notes. Commutativity boils down to Fubini's theorem for reordering integrals and affineness is marginalization (since probability measures have mass 1). See also [[Jacobs 2018](#b45)]. â–¡ Consider the real-numbers language (Ex. 1(2)). Let real = R, with the Borel sets, and interpret normal as the normal probability measure on R. The basic arithmetic operations are all measurable.

Among the following three programs

$let ğ‘¥ = normal(0, 1) in ğ‘¥ + ğ‘¥ (13) let ğ‘¥ = normal(0, 1) in let ğ‘¦ = normal(0, 1) in ğ‘¥ + ğ‘¦ (14) normal(0, 1) + normal(0, 1)(15)$the programs ( [14](#)) and ( [15](#formula_15)) denote the same normal distribution with variance 2, whereas (13) denotes a distribution with variance 4. Notice that we cannot use ( [11](#formula_3)) to equate all the programs, because normal is not deterministic. We can also interpret the Bernoulli language (Ex. 1(1)) in the Giry monad; this interpretation gives the same equational theory as the interpretation in FinStoch and in the distribution monad in Section 2.3.2.

We can also give some interpretations for the graph interface (Ex. 1(3)) in the Giry monad. For an informal example, consider the geometric example from Section 1.1, let vertex = ğ‘† 2 (the sphere), and define new to be the uniform distribution on the sphere. (See also Section 5.2.) 2.3.4 Affine Monads from Distributive Markov Categories. The following result, a converse to Proposition 8, demonstrates that the new notion of distributive Markov category (Def. 2) is a canonical one, and emphasizes the close relationship between semantics with distributive Markov categories and semantics with commutative affine monads.

Proposition 13. Let C be a small distributive Markov category. Then, there is a distributive category A with a commutative affine monad ğ‘‡ on it and a full and faithful functor C â†’ Kl(ğ‘‡ ) that preserves symmetric monoidal structure, comonoids, and sums.

Proof notes. Our proof is essentially a recasting of [[Power 2006b, Â§7]](#) to this different situation, as follows.

Let C det be the wide subcategory of C comprising the deterministic morphisms, and write ğ½ : C det â†’ C for the identity-on-objects inclusion functor. Note that C det is a distributive category. We would like to exhibit C as the Kleisli category for a monad on C det , but this might not be possible: intuitively, C det might be too small for the monad to exist. Instead, we first embed C det in a larger category A and construct a monad on A.

The main construction in our proof is the idea that if X is a small distributive monoidal category, then the category FP(X op , Set) of finite-product-preserving functors is such that

â€¢ FP(X op , Set) is cocomplete and moreover total [([Street and Walters 1978]](#)) as a category;

â€¢ FP(X op , Set) admits a distributive monoidal structure;

â€¢ the Yoneda embedding X â†’ [X op , Set], which is full and faithful, factors through FP(X op , Set), and this embedding X â†’ FP(X op , Set) preserves finite sums and is strongly monoidal; â€¢ the Yoneda embedding exhibits FP(X op , Set) as a free colimit completion of X as a monoidal category that already has finite coproducts. So we let A = FP(C op det , Set) comprise the finite-product-preserving functors C op det â†’ Set. This is a distributive category. To get a monad on A, we note that since FP(C op , Set) has finite coproducts and C det â†’ C â†’ FP(C op , Set) preserves finite coproducts and is monoidal, the monoidal structure induces a canonical colimit-preserving monoidal functor ğ½ ! : FP(C op det , Set) â†’ FP(C op , Set). Any colimit-preserving functor ğ½ ! out of a total category has a right adjoint ğ½ * , and hence a monoidal monad (ğ½ * ğ½ ! ) is induced on A.

It remains for us to check that the embedding C â†’ FP(C op , Set) factors through the comparison functor Kl(ğ½ * ğ½ ! ) â†’ FP(C op , Set), which follows from the fact that ğ½ : C det â†’ C is identity on objects.

â–¡

As an aside, we note that, although our simple language in Section 2.1 did not include higherorder functions, the category A constructed in the proof of Proposition 13 is cartesian closed, and since the embedding is full and faithful, this shows that higher-order functions would be a conservative extension of our language. Indeed, this kind of conservativity result was part of the motivation of [[Power 2006b](#)]. For the same reason, inductive types (lists, and so on) would also be a conservative extension. We leave conservativity with other language features for future work. Recursion in probabilistic programming is still under investigation [[Ehrhard et al. 2018;](#b25)[Goubault-Larrecq et al. 2021;](#)[Jia et al. 2021;](#b49)[Matache et al. 2022;](#b66)[VÃ¡kÃ¡r et al. 2019]](#b80); there is also the question of conservativity with respect to combining Markov categories, e.g. combining real number distributions ((1)-( [2](#))) with graph programming ((3)-( [4](#))).

## Bernoulli Bases, Numerals and Observation

Although an interface may have different type constants, it will always have the 'numeral' types, sometimes called 'finite' types:

$0 unit bool = unit + unit unit + unit + unit . . .$For probabilistic programming languages, there is a clear expectation of what will happen when we run a program of type bool: it will randomly produce either true or false, each with some probability. Similarly for other numeral types. For type constants, we might not have evident notions of observation or expected outcomes. But for numeral types, it should be routine. We now make this precise via the notion of Bernoulli base.

On the semantic side, distributive Markov categories will always have 'numeral' objects

$0 1 2 def = 1 + 1 3 def = 1 + 1 + 1 . . .$For any type ğ´ formed without type constants, and any Markov category, we have that ğ´ ğ‘› for some numeral object. Any equational theory for the programming language induces in particular an equational theory for the sub-language without any type constants. Proposition 14. For any distributive Markov category C, let C N be the category whose objects are natural numbers, and where the morphisms are the morphisms in C between the corresponding numeral objects. This is again a distributive Markov category.

Example 15.

(1) FinSet N = Set N is equivalent to FinSet as a category.

(2) For the finite distributions and the Giry monad ( Â§2.

$3.2-2.3.3), Kl(D) N â‰ƒ Kl(G) N â‰ƒ FinStoch.$Recall that a functor is faithful if it is injective on hom-sets. Definition 16. A Bernoulli base for a distributive Markov category C is a faithful distributive Markov functor Î¨ : C N â†£ FinStoch.

Thus, for any distributive Markov category with a Bernoulli base, for any closed term âŠ¢ ğ‘¡ : ğ´ of numeral type ( ğ´ = ğ‘›), we can regard its interpretation ğ‘¡ : 1 â†’ ğ‘› as nothing but a probability distribution Î¨( ğ‘¡ ) on ğ‘› outcomes. This is the case even if ğ‘¡ uses term constants and has intermediate subterms using type constants.

Example 17. All the examples seen so far can be given Bernoulli bases. In fact, for FinStoch, Kl(D) and Kl(G), the functor Î¨ : C N â†£ FinStoch is an isomorphism of distributive Markov categories.

When Î¨ is an isomorphism of categories, that means that all the finite probabilities are present in C. This is slightly stronger than we need in general. For instance, when C = FinSet, there is a unique Bernoulli base Î¨ : FinSet N â†£ FinStoch, taking a function to a 0/1-valued matrix, but it is not full. We could also consider variations on FinStoch. For example, consider the subcategory FinQStoch of FinStoch where the matrices are rational-valued; this has a Bernoulli base that is not an isomorphism.

## Quotients of Distributive Markov Categories

We provide a new, general method for constructing a Bernoulli-based Markov category out of a distributive Markov category. Our construction is a categorical formulation of the notion of contextual equivalence.

Recall that, in general, contextual equivalence for a programming language starts with a notion of basic observation for closed programs at ground types. We then say that programs Î“ âŠ¢ ğ‘¡, ğ‘¢ : ğ´ at other types are contextually equivalent if for every context C with âŠ¢ C [ğ‘¡], C [ğ‘¢] : ğ‘›, for some ground type ğ‘›, we have that C [ğ‘¡] and C [ğ‘¢] satisfy the same observations. In the categorical setting, the notion of observation is given by a distributive Markov functor C N â†’ FinStoch, and the notion of context C is replaced by suitable morphisms (â„, ğ‘˜ below). We now introduce a quotient construction that will be key in showing that every graphon arises from a distributive Markov category (Corollary 26), via Theorem 23. We note that this is a general new method for building Markov categories.

Proposition 18. Let C be a distributive Markov category, and let Î¨ : C N â†’ FinStoch be a distributive Markov functor. Suppose that for every object ğ‘‹ âˆˆ C, either ğ‘‹ = 0 or there exists a morphism 1 â†’ ğ‘‹ . Then, there is a distributive Markov category C/ Î¨ with a Bernoulli base, equipped with a distributive Markov functor C â†’ C/ Î¨ and a factorization of distributive Markov functors

$Î¨ = C N â†’ (C/ Î¨ ) N â†£ FinStoch. Proof. Define an equivalence relation âˆ¼ on each hom-set C(ğ‘‹, ğ‘Œ ), by ğ‘“ âˆ¼ ğ‘” : ğ‘‹ â†’ ğ‘Œ if âˆ€ğ‘, ğ‘›. âˆ€â„ : 1 â†’ ğ‘‹ âŠ— ğ‘ . âˆ€ğ‘˜ : ğ‘Œ âŠ— ğ‘ â†’ ğ‘›. Î¨(ğ‘˜ â€¢ (ğ‘“ âŠ— ğ‘ ) â€¢ â„) = Î¨(ğ‘˜ â€¢ (ğ‘” âŠ— ğ‘ ) â€¢ â„) in FinStoch(1, ğ‘›).$Informally, our equivalence relation considers all ways of generating ğ‘‹ 's via precomposition (â„), all ways for testing ğ‘Œ 's via postcomposition (ğ‘˜), and all ways of combining with some ancillary data (ğ‘ ). It is essential that we consider all these kinds of composition in order for the quotient category to have the categorical structure.

It is immediate that composition of morphisms respects âˆ¼, and hence we have a category: the objects are the same as C, and the morphisms are âˆ¼-equivalence classes. This is our category C/ Î¨ .

It is also immediate that if ğ‘“ âˆ¼ ğ‘” and ğ‘“ â€² âˆ¼ ğ‘” â€² then (ğ‘“ âŠ— ğ‘“ â€² ) âˆ¼ (ğ‘” âŠ— ğ‘” â€² ). Thus, C/ Î¨ is a monoidal category.

For the coproduct structure, we must show that if ğ‘“ âˆ¼ ğ‘” : ğ‘‹ â†’ ğ‘Œ and ğ‘“ â€² âˆ¼ ğ‘” â€² : ğ‘‹ â€² â†’ ğ‘Œ â€² then (ğ‘“ + ğ‘“ â€² ) âˆ¼ (ğ‘” + ğ‘” â€² ) : ğ‘‹ + ğ‘‹ â€² â†’ ğ‘Œ + ğ‘Œ â€² . We proceed by noting that since we have morphisms ğ‘¥ : 1 â†’ ğ‘‹ and ğ‘¥ â€² : 1 â†’ ğ‘‹ â€² , as well as terminal morphisms ğ‘‹ â†’ 1 and ğ‘‹ â€² â†’ 1, we have that ğ‘‹ + ğ‘‹ â€² is a retract of ğ‘‹ âŠ— ğ‘‹ â€² âŠ— 2, with the section and retraction given by:

$ğ‘‹ + ğ‘‹ â€² ğ‘‹ âŠ—ğ‘¥ â€² +ğ‘¥ âŠ—ğ‘‹ ---------â†’ ğ‘‹ âŠ— ğ‘‹ â€² + ğ‘‹ âŠ— ğ‘‹ â€² ğ‘‹ âŠ— ğ‘‹ â€² âŠ— 2 ğ‘‹ âŠ— ğ‘‹ â€² âŠ— 2 ğ‘‹ âŠ— ğ‘‹ â€² + ğ‘‹ âŠ— ğ‘‹ â€² ğ‘‹ âŠ—!+!âŠ—ğ‘‹ -------â†’ ğ‘‹ + ğ‘‹ â€²$Thus, by composing with this retract, it suffices to check that (ğ‘“ âŠ— ğ‘“ â€² âŠ— 2) âˆ¼ (ğ‘” âŠ— ğ‘” â€² âŠ— 2), which we have already shown.

The functor Î¨ : C N â†’ FinStoch clearly factors through (C/ Î¨ ) N , but it remains to check that the functor (C/ Î¨ ) N â†’ FinStoch is now faithful (Bernoulli base). So suppose that Î¨(ğ‘“ ) = Î¨(ğ‘”). To show that ğ‘“ âˆ¼ ğ‘” : 1 â†’ ğ‘š, we consider â„ : 1 â†’ 1 âŠ— ğ‘ , and ğ‘˜ : ğ‘š âŠ— ğ‘ â†’ ğ‘›. We must show that

$Î¨(ğ‘˜ â€¢ (ğ‘“ âŠ— ğ‘ ) â€¢ â„) = Î¨(ğ‘˜ â€¢ (ğ‘” âŠ— ğ‘ ) â€¢ â„). Since â„ = 1 âŠ— â„ â€² , for some â„ â€² : 1 â†’ ğ‘ , we have Î¨(ğ‘˜ â€¢ (ğ‘“ âŠ— ğ‘ ) â€¢ â„) = Î¨(ğ‘˜ â€¢ (ğ‘š âŠ— â„ â€² ) â€¢ ğ‘“ ) = Î¨(ğ‘˜ â€¢ (ğ‘š âŠ— â„ â€² )) â€¢ Î¨(ğ‘“ ) = Î¨(ğ‘˜ â€¢ (ğ‘š âŠ— â„ â€² )) â€¢ Î¨(ğ‘”) = Î¨(ğ‘˜ â€¢ (ğ‘š âŠ— â„ â€² ) â€¢ ğ‘”) = Î¨(ğ‘˜ â€¢ (ğ‘” âŠ— ğ‘ ) â€¢ â„). â–¡ 3 FROM PROGRAM EQUATIONS TO GRAPHONS$The graph interface for the probabilistic programming language (Ex. 1(3)) does not have one fixed equational theory. Rather, we want to consider different equational theories for the language, corresponding to different implementations of the interface for the graph (see also Â§1.2). We now show how the different equational theories for the graph language each give rise to a graphon, by building adjacency matrices for finite graphs (shown in ( [18](#formula_30))). To do this, we set up the well-behaved equational theories ( Â§2.4), recall the connection between graphons and finite random graphs ( Â§3.1), and then show the main result ( Â§3.2, Theorem 23).

## Graphons as Consistent and Local Random Graph Models

For all ğ‘› â‰¥ 1, let [ğ‘›] be the set {1, . . . , ğ‘›}. (We sometimes omit the square brackets, when it is clear.) A simple undirected graph ğ‘” with ğ‘› nodes can be represented by its adjacency matrix ğ´ ğ‘” âˆˆ 2 [ğ‘›] 2 such that ğ´ ğ‘” (ğ‘–, ğ‘–) = 0 and ğ´ ğ‘” (ğ‘–, ğ‘—) = ğ´ ğ‘” ( ğ‘—, ğ‘–). Henceforth, we will assume that finite graphs are simple and undirected, unless otherwise stated. A random finite graph, then, has a probability distribution in D 2 [ğ‘›] 2 that only assigns non-zero probability to adjacency matrices.

Definition 19 (e.g. [[LovÃ¡sz 2012, Â§11.2.1]](#)). A random graph model is a sequence of distributions of random finite graphs of the form:

$ğ‘ 1 âˆˆ D 2 [1] 2 , ğ‘ 2 âˆˆ D 2 [2] 2 , . . . , ğ‘ ğ‘› âˆˆ D 2 [ğ‘›] 2 , . . .$We say such a sequence is

â€¢ exchangeable if each of its elements is invariant under permuting nodes: for every ğ‘› and bijection ğœ : ğ‘›] 2 is the function that permutes the rows and columns according to ğœ; we are regarding D as a covariant functor, Def. 9, and 2 (-) as a contravariant functor); â€¢ consistent if the sequence is related by marginals: for every ğ‘› and for the inclusion function

$[ğ‘›] â†’ [ğ‘›], we have D 2 (ğœ 2 ) (ğ‘ ğ‘› ) = ğ‘ ğ‘› (where 2 (ğœ 2 ) : 2 [ğ‘›] 2 â†’ 2 [$$ğœ„ : [ğ‘›] â†©â†’ [ğ‘› + 1], D 2 (ğœ„ 2 ) (ğ‘ ğ‘›+1 ) = ğ‘ ğ‘› ($where 2 (ğœ„ 2 ) : 2 ( [ğ‘›+1] 2 ) â†’ 2 [ğ‘›] 2 is the evident projection); â€¢ local if the subgraphs are independent: if ğ´ âŠ† [ğ‘›] and ğµ âŠ† [ğ‘›] are disjoint, then we have an injective function ğš¥ :

$ğ´ 2 + ğµ 2 â†©â†’ [ğ‘›] 2 , and D 2 ğš¥ (ğ‘ ğ‘› ) âˆˆ D 2 (ğ´ 2 ) Ã— 2 (ğµ 2 ) is a product measure ğ‘ ğ´ âŠ— ğ‘ ğµ (where 2 ğš¥ : 2 [ğ‘›] 2 â†’ 2 (ğ´ 2 ) Ã— 2 (ğµ 2 )$is the evident pairing of projections).

Definition 20 (e.g. [[LovÃ¡sz 2012])](#b62). A graphon ğ‘Š is a symmetric measurable function ğ‘Š :

$[0, 1] 2 â†’ [0, 1].$Given a graphon ğ‘Š , we can generate a finite simple undirected graph ğ‘” with vertex set [ğ‘›] by sampling ğ‘› points ğ‘¥ 1 , . . . , ğ‘¥ ğ‘› uniformly from [0, 1] and, then, including the edge (ğ‘–, ğ‘—) with probability ğ‘Š (ğ‘¥ ğ‘– , ğ‘¥ ğ‘— ) for all 1 â‰¤ ğ‘–, ğ‘— â‰¤ ğ‘›. This sampling procedure defines a distribution over finite graphs: the probability ğ‘ ğ‘Š ,ğ‘› (ğ´ ğ‘” ) of the graph ğ‘” = ( [ğ‘›], ğ¸) is:

$âˆ« [0,1] ğ‘› (ğ‘–,ğ‘— ) âˆˆğ¸ ğ‘Š (ğ‘¥ ğ‘– , ğ‘¥ ğ‘— ) (ğ‘–,ğ‘— )âˆ‰ğ¸ 1 -ğ‘Š (ğ‘¥ ğ‘– , ğ‘¥ ğ‘— ) d(ğ‘¥ 1 . . . ğ‘¥ ğ‘› )(16)$Proposition 21 ( [[LovÃ¡sz and Szegedy 2006]](#), [[LovÃ¡sz 2012, Â§11.2]](#)). Every graphon generates an exchangeable, consistent, and local random graph model, by the sampling procedure of (16). Conversely, every exchangeable, consistent, and local random graph model is of the form ğ‘ ğ‘Š ,ğ‘› for some graphon ğ‘Š .

Note. There are various methods for constructing ğ‘Š from an exchangeable, consistent and local random graph model, however all are highly non-trivial. A general idea is that ğ‘Š is a kind of limit object. For examples see e.g. [[LovÃ¡sz and Szegedy 2006, Â§11.3]](#) or [[Tao 2013](#b79)]. Fortunately though, we will not need explicit constructions in this paper. â–¡

## Theories of Program Equivalence Induce Graphons

In this section we consider the instance of the generic language with the graph interface (Ex. 1(3)):

$vertex new : unit â†’ vertex edge : vertex * vertex â†’ bool$We consider a theory of program equivalence, i.e. a distributive Markov category with a distinguished object vertex and morphisms new : 1 â†’ vertex and edge : vertex âŠ— vertex â†’ 1 + 1. We make two assumptions about the theory:

â€¢ The graphs are simple and undirected:

ğ‘¥ : vertex âŠ¢ edge(ğ‘¥, ğ‘¥) â‰¡ false ğ‘¥, ğ‘¦ : vertex âŠ¢ edge(ğ‘¥, ğ‘¦) â‰¡ edge(ğ‘¦, ğ‘¥)

and edge is deterministic. â€¢ The theory is Bernoulli based ( Â§2.4).

For each ğ‘› âˆˆ N, we can build a random graph with ğ‘› vertices as follows. We consider the following program ğ‘¡ ğ‘› :

$âŠ¢ let ğ‘¥ 1 = new() in . . . let ğ‘¥ ğ‘› = new() in edge(ğ‘¥ 1 , ğ‘¥ 1 ) . . . edge(ğ‘¥ 1 , ğ‘¥ ğ‘› ) . . . . . . edge(ğ‘¥ ğ‘› , ğ‘¥ 1 ) . . . edge(ğ‘¥ ğ‘› , ğ‘¥ ğ‘› ) : bool (ğ‘› 2 )(18)$(Here we use syntactic sugar, writing a matrix instead of iteratively using pairs.) Because the equational theory is Bernoulli-based, the interpretation ğ‘¡ ğ‘› induces a probability distribution Î¨ ğ‘¡ ğ‘› on 2 (ğ‘› 2 ) . For clarity, we elide Î¨ in what follows, since it is faithful. Proposition 22. Each random matrix in (18) is a random adjacency matrix, i.e. a random graph.

Proof note. This follows from (17). â–¡ Theorem 23. For any Bernoulli-based equational theory, the random graph model ( ğ‘¡ ğ‘› ) ğ‘› in (18) is exchangeable, consistent, and local. Thus, the equational theory induces a graphon.

Proof. We denote the matrix in ( [18](#formula_30)) by (edge(ğ‘¥ ğ‘– , ğ‘¥ ğ‘— )) ğ‘–,ğ‘— âˆˆ [ğ‘›] .

Exchangeability. We show that the distribution ğ‘¡ ğ‘› is invariant under relabeling the nodes. By commutativity of the let construct (9), the program

$ğ‘¡ ğœ ğ‘› def = let ğ‘¥ ğœ -1 (1) = new() in . . . let ğ‘¥ ğœ -1 (ğ‘›) = new() in (edge(ğ‘¥ ğ‘– , ğ‘¥ ğ‘— )) ğ‘–,ğ‘— âˆˆ [ğ‘›]$satisfies ğ‘¡ ğœ ğ‘› = ğ‘¡ ğ‘› . Hence, D (2 ğœ 2 ) ( ğ‘¡ ğ‘› ) = ğ‘¡ ğœ ğ‘› = ğ‘¡ ğ‘› , for every ğ‘› and bijection ğœ : [ğ‘›] â†’ [ğ‘›]. Consistency. We define a macro subm ğ¼ in the graph programming language to extract a submatrix at the index set ğ¼ âŠ† [ğ‘›]: we have the (definitional) equality

$subm ğ¼ ((ğ‘ ğ‘–,ğ‘— ) ğ‘–,ğ‘— âˆˆ [ğ‘›] ) def = (ğ‘ ğ‘–,ğ‘— ) ğ‘–,ğ‘— âˆˆğ¼ for ğ¼ âŠ† [ğ‘›].$We need to show that, if we delete the last node from a graph sampled from ğ‘¡ ğ‘›+1 , the resulting graph has distribution ğ‘¡ ğ‘› . This amounts to the affineness property (10), as follows. Let ğ‘” âˆ¼ ğ‘¡ ğ‘›+1 be a random graph, and let ğ‘” â€² def = ğ‘” | [ğ‘›] be the graph obtained by deleting the last node from ğ‘”. Then clearly, the adjacency matrix of ğ‘” â€² is the adjacency matrix of ğ‘” where the last row and column have been removed, i.e. ğ‘” â€² is sampled from the interpretation of the program:

$ğ‘¡ â€² def = let ğ‘¥ 1 = new() in . . . let ğ‘¥ ğ‘› = new() in let ğ‘¥ ğ‘›+1 = new() in subm [ğ‘›] (edge(ğ‘¥ ğ‘– , ğ‘¥ ğ‘— )) ğ‘–,ğ‘— âˆˆ [ğ‘›+1] â‰¡ let ğ‘¥ 1 = new() in . . . let ğ‘¥ ğ‘› = new() in let ğ‘¥ ğ‘›+1 = new() in (edge(ğ‘¥ ğ‘– , ğ‘¥ ğ‘— )) ğ‘–,ğ‘— âˆˆ [ğ‘›] â‰¡ let ğ‘¥ 1 = new() in . . . let ğ‘¥ ğ‘› = new() in (edge(ğ‘¥ ğ‘– , ğ‘¥ ğ‘— )) ğ‘–,ğ‘— âˆˆ [ğ‘›]$(by ( [10](#formula_2))) â‰¡ ğ‘¡ ğ‘› .

Locality. Without loss of generality (by exchangeability and consistency), we need to show that for every random graph ğ‘” âˆ¼ ğ‘¡ ğ‘› and 1 < ğ‘˜ < ğ‘›, the subgraphs ğ‘” ğ´ ğ‘˜ , ğ‘” ğµ ğ‘˜ respectively induced by the sets ğ´ ğ‘˜ def = [ğ‘˜] and ğµ ğ‘˜ def = {ğ‘˜ + 1, . . . , ğ‘›} are independent as random variables. Let ğš¥ be the injection ğš¥ : ğ´ 2 ğ‘˜ + ğµ 2 ğ‘˜ â†©â†’ ğ‘› 2 , and

$ğ‘” â€² âˆ¼ D (2 ğš¥ ) ( ğ‘¡ ğ‘› ) âˆˆ D (2 (ğ´ 2 ğ‘˜ ) Ã— 2 (ğµ 2 ğ‘˜ )$). We want to show that ğ‘” â€² and (ğ‘” ğ´ ğ‘˜ , ğ‘” ğµ ğ‘˜ ) âˆ¼ ğ‘¡ ğ‘˜ âŠ— ğ‘¡ ğ‘›-ğ‘˜ (by consistency) are equal in distribution. Modulo ğ›¼-renaming, (ğ‘” ğ´ ğ‘˜ , ğ‘” ğµ ğ‘˜ ) is sampled from the interpretation of the program:

$ğ‘¡ â€² def = let ğ‘¥ 1 = new() in . . . let ğ‘¥ ğ‘˜ = new() in (edge(ğ‘¥ ğ‘– , ğ‘¥ ğ‘— )) ğ‘–,ğ‘— âˆˆ [ğ‘˜ ] , let ğ‘¥ ğ‘˜+1 = new() in . . . let ğ‘¥ ğ‘› = new() in (edge(ğ‘¥ ğ‘– , ğ‘¥ ğ‘— )) ğ‘˜+1â‰¤ğ‘–,ğ‘— â‰¤ğ‘› â‰¡ let ğ‘¢ 1 = (let ğ‘¥ 1 = new() in . . . let ğ‘¥ ğ‘˜ = new() in (edge(ğ‘¥ ğ‘– , ğ‘¥ ğ‘— )) ğ‘–,ğ‘— âˆˆğ´ ğ‘˜ ) in let ğ‘¢ 2 = (let ğ‘¥ ğ‘˜+1 = new() in . . . let ğ‘¥ ğ‘› = new() in (edge(ğ‘¥ ğ‘– , ğ‘¥ ğ‘— )) ğ‘–,ğ‘— âˆˆğµ ğ‘˜ ) in (ğ‘¢ 1 , ğ‘¢ 2 ) (by (8)) â‰¡ let ğ‘¥ 1 = new() in . . . let ğ‘¥ ğ‘˜ = new() in let ğ‘¥ ğ‘˜+1 = new() in . . . let ğ‘¥ ğ‘› = new() in ((7),(9)) let ğ‘¢ 1 = subm ğ´ ğ‘˜ (edge(ğ‘¥ ğ‘– , ğ‘¥ ğ‘— )) ğ‘–,ğ‘— âˆˆ [ğ‘›] in let ğ‘¢ 2 = subm ğµ ğ‘˜ (edge(ğ‘¥ ğ‘– , ğ‘¥ ğ‘— )) ğ‘–,ğ‘— âˆˆ [ğ‘›] in (ğ‘¢ 1 , ğ‘¢ 2 ) â‰¡ let ğ‘¥ 1 = new() in . . . let ğ‘¥ ğ‘› = new() in let ğ‘¡ = (edge(ğ‘¥ ğ‘– , ğ‘¥ ğ‘— )) ğ‘–,ğ‘— âˆˆ [ğ‘›] in let ğ‘¢ 1 = subm ğ´ ğ‘˜ (ğ‘¡) in let ğ‘¢ 2 = subm ğµ ğ‘˜ (ğ‘¡) in (ğ‘¢ 1 , ğ‘¢ 2 ) (by (11)) â‰¡ let ğ‘¥ 1 = new() in . . . let ğ‘¥ ğ‘› = new() in let ğ‘¡ = (edge(ğ‘¥ ğ‘– , ğ‘¥ ğ‘— )) ğ‘–,ğ‘— âˆˆ [ğ‘›] in subm ğ´ ğ‘˜ (ğ‘¡), subm ğµ ğ‘˜ (ğ‘¡) (by (8)) â‰¡ let ğ‘¡ = let ğ‘¥ 1 = new() in . . . let ğ‘¥ ğ‘› = new() in (edge(ğ‘¥ ğ‘– , ğ‘¥ ğ‘— )) ğ‘–,ğ‘— âˆˆ [ğ‘›] in subm ğ´ ğ‘˜ (ğ‘¡), subm ğµ ğ‘˜ (ğ‘¡))$(by ( [7](#)))

and ğ‘” â€² âˆ¼ D (2 ğš¥ ) ( ğ‘¡ ğ‘› ) is indeed sampled from the interpretation of the latter program, which yields the result. â–¡

## FROM GRAPHONS TO PROGRAM EQUATIONS

In Section 3, we showed how a distributive Markov category modelling the graph interface (Ex. 1(3)) gives rise to a graphon. In this section, we establish a converse: every graphon arises in this way (Corollary 26). Theorem 25 will establish slightly more: there is a 'generic' distributive Markov category ( Â§4.1) modelling the graph interface whose Bernoulli-based quotients are in precise correspondence with graphons ( Â§4.2). This approach also suggests an operational way of implementing the graph interface for any graphon ( Â§4.3).

## A Generic Distributive Markov Category for the Graph Interface

We construct this generic category in two steps. We first create a distributive Markov category, actually a distributive category, Fam(G op ), that supports (vertex, edge). We then add new using the monoidal indeterminates method of [[Hermida and Tennent 2012]](#b39).

## 4.1.1

Step 1: A Distributive Category with edge. We first define a distributive category that supports (vertex, edge). Let G be the category of finite graphs and functions that preserve and reflect the edge relation. That is, a morphism ğ‘“ : ğ‘” â†’ ğ‘” â€² is a function ğ‘“ : ğ‘‰ ğ‘” â†’ ğ‘‰ â€² ğ‘” such that for all ğ‘£, ğ‘¤ âˆˆ ğ‘‰ ğ‘” we have ğ¸ ğ‘” (ğ‘£, ğ‘¤) if and only if ğ¸ ğ‘” (ğ‘“ (ğ‘£), ğ‘“ (ğ‘¤)).

Recall (e.g. [[Hu and Tholen 1995]](#b41)) that the free finite coproduct completion of a category C, Fam(C) is given as follows. The objects of Fam(C) are sequences (ğ‘‹ 1 . . . ğ‘‹ ğ‘› ) of objects of C, and the morphisms (ğ‘‹ 1 . . . ğ‘‹ ğ‘š ) â†’ (ğ‘Œ 1 . . . ğ‘Œ ğ‘› ) are pairs (ğ‘“ , {ğ‘“ ğ‘– } ğ‘š ğ‘–=1 ) of a function ğ‘“ : ğ‘š â†’ ğ‘› and a sequence of morphisms ğ‘“ 1 : ğ‘‹ 1 â†’ ğ‘Œ ğ‘“ (1) , . . . , ğ‘“ ğ‘š : ğ‘‹ ğ‘š â†’ ğ‘Œ ğ‘“ (ğ‘š) in C.

We consider the category Fam(G op ). Let vertex = (1), the singleton sequence comprising the one-vertex graph.

Proposition 24.

(1) The free coproduct completion Fam(G op ) is a distributive category, with the product vertex ğ‘› being the sequence of all graphs with ğ‘› vertices. In particular, vertex 2 is a sequence with two components, the complete graph and the edgeless graph with two vertices.

(2) Let edge : vertex Ã— vertex â†’ 1 + 1 be the morphism (id, {!, !}), intuitively returning true for the edge, and false for the edgeless graph. Here the terminal object 1 of Fam(G op ) is the singleton tuple of the empty graph. This interpretation satisfies (17).

Proof notes. Item (1) follows from [[Hu and Tholen 1995]](#b41), which shows that limits in Fam(G op ) amount to "multi-colimits" in G. For example, the family of all graphs with ğ‘› vertices is a multicoproduct of the one-vertex graph in G, hence forms a product in Fam(G op ). Item ( [2](#)) is then a quick calculation. All morphisms in Fam(G op ) are deterministic. â–¡

## 4.1.2

Step 2: Adjoining new. In Section 4.1.1, we introduced a distributive category that interprets the interface (vertex, edge). But it does not support new, and indeed there are no morphisms 1 â†’ vertex . To additionally interpret (new), we freely adjoin it. We essentially use the 'monoidal indeterminates' method of Hermida and Tennent [[Hermida and Tennent 2012]](#b39) to do this. Their work was motivated by semantics of dynamic memory allocation, but has also been related to quantum phenomena [[AndrÃ©s-MartÃ­nez et al. 2022;](#b8)[Huot and Staton 2018]](#b42) and to categorical gradient/probabilistic methods [[Cruttwell et al. 2021;](#b22)[Fong et al. 2021;](#b29)[Shiebler 2021]](#b75), where it is known as the 'para construction'. It is connected to earlier methods for the action calculus [[PavloviÄ‡ 1997](#b69)].

Let FinSetInj be the category of finite sets and injections. It is a monoidal category with the disjoint union monoidal structure (e.g. [[Fiore 2005;](#b28)[Power 2006a]](#)). Consider the functor ğ½ : FinSetInj op â†’ Fam(G op ), with ğ½ (ğ‘›) = vertex ğ‘› , and where the functorial action is by exchange and projection. This is a strong monoidal functor. (Indeed, it is the unique monoidal functor with ğ½ (1) = vertex .) For any monoidal functor, Hermida and Tennent [[Hermida and Tennent 2012]](#b39) provide monoidal indeterminates by introducing a 'polynomial category', by analogy with a polynomial ring. Unfortunately, a general version for distributive monoidal categories is not yet known, so we focus on the specific case of ğ½ : FinSetInj op â†’ Fam(G op ). We build a new category Fam(G op ) [ğœˆ : ğ½ FinSetInj op ], which we abbreviate Fam(G op ) [ğœˆ]. It has the same objects as Fam(G op ), but the morphisms Ã¬ ğ‘‹ â†’ Ã¬ ğ‘Œ are equivalence classes of morphisms [ğ‘˜, ğ‘“ ] : vertex ğ‘˜ Ã— Ã¬ ğ‘‹ â†’ Ã¬ ğ‘Œ in Fam(G op ), modulo reindexing. The reindexing equivalence relation is generated by putting [ğ‘˜, ğ‘“ ] âˆ¼ [ğ‘™, ğ‘”] when there exist injections ğœ„ 1 . . . ğœ„ ğ‘š : ğ‘˜ â†’ ğ‘™ such that

$ğ‘” = vertex ğ‘™ Ã— Ã¬ ğ‘‹ ğ‘š ğ‘—=1 vertex ğ‘™ Ã— ğ‘‹ ğ‘— vertex (ğœ„ ğ‘— ) Ã—ğ‘‹ ğ‘— ------------â†’ ğ‘š ğ‘—=1 vertex ğ‘˜ Ã— ğ‘‹ ğ‘— vertex ğ‘˜ Ã— Ã¬ ğ‘‹ ğ‘“ - â†’ Ã¬ ğ‘Œ$where Ã¬ ğ‘‹ = (ğ‘‹ 1 , . . . , ğ‘‹ ğ‘š ). In particular, when ğ‘š = 1, i.e. Ã¬ ğ‘‹ = ğ‘‹ is a singleton sequence, we have

$Fam(G op ) [ğœˆ] (ğ‘‹, Ã¬ ğ‘Œ ) colim ğ‘˜ âˆˆFinSetInj Fam(G op ) ( vertex ğ‘˜ Ã— ğ‘‹, Ã¬ ğ‘Œ ). (19$$)$Composition and monoidal structure accumulate in vertex ğ‘˜ , as usual in the monoidal indeterminates ('para') construction, e.g. â€¢ Fam(G op ) [ğœˆ] is a distributive Markov category.

$Ã¬ ğ‘‹ [ğ‘˜,ğ‘“ ] ----â†’ Ã¬ ğ‘Œ [ğ‘™,ğ‘”] ---â†’ Ã¬ ğ‘ = Ã¬ ğ‘‹ [ğ‘™+ğ‘˜,ğ‘”â€¢( vertex ğ‘™ Ã—ğ‘“ ) ] -----------------â†’ Ã¬ ğ‘$â€¢ Fam(G op ) [ğœˆ] supports the graph interface, via the interpretation of (vertex, edge) in Fam(G op ), but also with the interpretation new = ğœˆ : 1 â†’ vertex .

## Bernoulli Bases for Random Graph Models

The following gives a precise characterization of graphons in terms of the numerals of Fam(G op ) [ğœˆ].

Theorem 25. To give a distributive Markov functor Fam(G op ) [ğœˆ] N â†’ FinStoch is to give a graphon.

Proof outline. We begin by showing a related characterization: that graphons correspond to certain natural transformations. Observe that any distributive Markov category C gives rise to a symmetric monoidal functor C(1, -) : FinSet N â†’ Set, regarding the numerals of FinSet N as objects of C ( Â§2.4). Let ğº ğ‘˜ = 2 ğ‘˜ (ğ‘˜ -1)/2 be the set of ğ‘˜-vertex graphs. We can characterize the natural

$transformations ğ›¼ : Fam(G op ) [ğœˆ] (1, -) â†’ FinStoch(1, -) as follows. Nat(Fam(G op ) [ğœˆ] (1, -) , FinStoch(1, -)) Nat colim ğ‘˜ âˆˆFinSetInj FinSet(ğº ğ‘˜ , -) , D (-)$(Ex. 15(2), Prop. 24( [1](#)) and ( [19](#formula_37))) lim ğ‘˜ âˆˆFinSetInj op Nat(FinSet(ğº ğ‘˜ , -) , D (-))

(universal property of colimits) lim ğ‘˜ âˆˆFinSetInj op D (ğº ğ‘˜ ) (Yoneda lemma) An element of this limit of sets is by definition a sequence of distributions ğ‘ ğ‘˜ on ğº ğ‘˜ that is invariant under reindexing by FinSetInj op . Since injections are generated by inclusions and permutations, this is then a sequence that is consistent and exchangeable (Def. 19), respectively. Such a natural transformation ğ›¼ is monoidal if and only if the sequence is also local. Hence a monoidal natural transformation is the same thing as a random graph model.

In fact, every monoidal natural transformation ğ›¼ : Fam(G op ) [ğœˆ] (1, -) â†’ FinStoch(1, -) arises uniquely by restricting a distributive Markov functor ğ¹ : Fam(G op ) [ğœˆ] N â†’ FinStoch. We now show this, to conclude our proof. Given ğ›¼, let ğ¹ ğ‘š,ğ‘› : Fam(G op ) [ğœˆ] N (ğ‘š, ğ‘›) â†’ FinStoch(ğ‘š, ğ‘›) be:

$Fam(G op ) [ğœˆ] N (ğ‘š, ğ‘›) Fam(G op ) [ğœˆ] N (1, ğ‘›) ğ‘š ğ›¼ ğ‘š ğ‘› --â†’ FinStoch(1, ğ‘›) ğ‘š FinStoch(ğ‘š, ğ‘›).$It is immediate that this ğ¹ preserves the symmetric monoidal structure and coproduct structure, but not that ğ¹ is a functor. However, the naturality of ğ›¼ in FinSet N gives us that ğ¹ preserves postcomposition by morphisms of FinSet N . All of this implies that general categorical composition is preserved as well, since, in any distributive Markov category of the form C N , for ğ‘“ : ğ‘™ â†’ ğ‘š and ğ‘” : ğ‘š â†’ ğ‘›, the composite ğ‘” â€¢ ğ‘“ : ğ‘™ â†’ ğ‘› is equal to

$ğ‘™ = ğ‘™ âŠ— 1 âŠ—ğ‘š ğ‘“ âŠ—ğ‘” 1 âŠ—...âŠ—ğ‘” ğ‘š ----------â†’ ğ‘š âŠ— ğ‘› âŠ—ğ‘š eval ---â†’ ğ‘›$where ğ‘” ğ‘– = ğ‘” â€¢ ğœ„ ğ‘– for ğ‘– = 1, . . . , ğ‘š and eval is just the evaluation map ğ‘š Ã— ğ‘› ğ‘š â†’ ğ‘› in FinSet. â–¡ Corollary 26. Every graphon arises from a distributive Markov category via the random graph model in (18). Proof summary. Given a graphon, we consider the distributive Markov functor that corresponds to it, Î¨ : Fam(G op ) [ğœˆ] N â†’ FinStoch, by Theorem 25. Using the quotient construction of Proposition 18, we get a distributive Markov category with a Bernoulli base. It is straightforward to verify that the random graph model induced by ( [18](#formula_30)) is the original graphon. â–¡

## Remark on Operational Semantics

The interpretation in this section suggests a general purpose operational semantics for closed programs at ground type, âŠ¢ ğ‘¡ : ğ‘›, along the following lines:

(1) Calculate the interpretation ğ‘¡ : 1 â†’ ğ‘› in Fam(G op ) [[ğœˆ]](#). There are no probabilistic choices in this step, it is a symbolic manipulation, because the morphisms of the Markov category Fam(G op ) [ğœˆ] are built from tuples of finite graph homomorphisms. In effect, this interpretation pulls all the new's to the front of the term. (2) Apply the Markov functor Î¨( ğ‘¡ ) to obtain a probability distribution on ğ‘›, and sample from this distribution to return a result.

## INTERPRETATION: BLACK-AND-WHITE GRAPHONS VIA MEASURE-THEORETIC PROBABILITY

In Section 4, we gave a general syntactic construction for building an equational theory from a graphon. Since that definition is based on free constructions and quotients, a priori, it does not 'explain' what the type vertex stands for. Like contextual equivalence of programs, a priori, it does not give useful compositional reasoning methods. To prove two programs are equal, according to the construction of Prop. 18, one needs to quantify over all ğ‘ , â„, and ğ‘˜, in general.

In this section, we show that one class of graphons, black-and-white graphons (Def. 27), admits a straightforward measure-theoretic semantics, and we can thus use the equational theory induced by this semantics, rather than the method of Section 4. This measure-theoretic semantics is close to previous measure-theoretic work on probabilistic programming languages (e.g. [[Kozen 1981;](#b60)[Staton 2017]](#)).

After recapping measure-theoretic probability ( Â§2.3.3), in Section 5.1, we show that every blackand-white graphon arises from a measure-theoretic interpretation (Prop. 28). In Section 5.2, by defining 'measure-theoretic interpretation' more generally, we show that, conversely, this measuretheoretic approach can only cater for black-and-white graphons (Prop. 29).

## Black-and-White Graphons from Equational Theories

Definition 27. [e.g. [[Janson 2013](#b47)

$]] A graphon ğ‘Š : [0, 1] 2 â†’ [0, 1] is black-and-white if there exists ğ¸ : [0, 1] 2 â†’ {0, 1} such that ğ‘Š (ğ‘¥, ğ‘¦) = ğ¸ (ğ‘¥, ğ‘¦) for almost all ğ‘¥, ğ‘¦.$Recall that the Giry monad (Def. 11) gives rise to a Bernoulli-based distributive Markov category ( Â§2.3.3, Ex. 15). For any black-and-white graphon ğ‘Š , we define an interpretation of the graph interface for the probabilistic programming language using G, as follows.

â€¢ vertex ğ‘Š = [0, 1]; bool ğ‘Š = 2, the discrete two element space;

â€¢ new() ğ‘Š = Uniform(0, 1), the uniform distribution on [0, 1];

â€¢ edge ğ‘Š (ğ‘¥, ğ‘¦) = ğœ‚ (ğ¸ (ğ‘¥, ğ‘¦)).

Proposition 28. Let ğ‘Š be a black-and-white graphon. The equational theory induced by -ğ‘Š induces the graphon ğ‘Š according to the construction in Section 3.2.

Proof. Suppose thatğ‘Š corresponds to the sequence of random graphs ğ‘ 1 , ğ‘ 2 , . . . as in Section 3.1. Consider the term ğ‘¡ ğ‘› in (18), and directly calculate its interpretation. Then, we get ğ‘¡ ğ‘› ğ‘Š = ğ‘ ğ‘› , via ( [16](#formula_27)), as required.

The choice of ğ¸ does not matter in the interpretation of these terms, because ğ‘Š = ğ¸ almost everywhere. â–¡

## All Measure-Theoretic Interpretations are Black-and-White

Although the model in Section 5.1 is fairly canonical, there are sometimes other enlightening interpretations using the Giry monad. These also correspond to black-and-white graphons. For example, consider the geometric-graph example from Figure [1](#fig_1). We interpret this using the Giry monad, putting

â€¢ vertex = ğ‘† 2 , the sphere; bool = 2;

â€¢ new() = Uniform(ğ‘† 2 ), the uniform distribution on the sphere;

â€¢ edge (ğ‘¥, ğ‘¦) = ğœ‚ (ğ‘‘ (ğ‘¥, ğ‘¦) < ğœƒ ), i.e. an edge if their distance is less than ğœƒ .

This will again induce a graphon, via (18). We briefly look at theories that arise in this more flexible way:

Proposition 29. Consider any interpretation of the graph interface in the Giry monad: a measurable space vertex , a measurable set edge âŠ† vertex 2 , and a probability measure new() on vertex . The induced graphon is black-and-white.

Proof notes. If vertex is standard Borel, the randomization lemma [[Kallenberg 2010, Lem. 3.22]](#) gives a function ğ‘“ : [0, 1] â†’ vertex that pushes the uniform distribution on [0, 1] onto the probability measure new() . We define a black-and-white graphon ğ‘Š by ğ‘Š (ğ‘¥, ğ‘¦) = 1 if (ğ‘“ (ğ‘¥), ğ‘“ (ğ‘¦)) âˆˆ edge , and ğ‘Š (ğ‘¥, ğ‘¦) = 0 otherwise. This graphon interpretation -ğ‘Š gives the same sequence of graphs in (18), just by reparameterizing the integrals.

If vertex is not standard Borel, we note that there is an equivalent interpretation where it is, because there exists a measure-preserving map vertex â†’ Î© to a standard Borel space Î© and a measurable set ğ¸ âŠ† Î© 2 that pulls back to edge , giving rise to the same graphon (e.g. [[Janson 2013, Lemma 7.3]](#)). â–¡

Discussion. Proposition 29 demonstrates that this measure-theoretic interpretation has limitations.

Definition 30. For ğ›¼ âˆˆ (0, 1), the ErdÅ‘s-RÃ©nyi graphon

$ğ‘Š ğ›¼ : [0, 1] 2 â†’ [0, 1] is given by ğ‘Š ğ›¼ (ğ‘¥, ğ‘¦) = ğ›¼.$The ErdÅ‘s-RÃ©nyi graphons cannot arise from measure-theoretic interpretations of the graph interface, because they are not black-and-white. In Section 6, we give an alternative interpretation for the ErdÅ‘s-RÃ©nyi graphons.

The reader might be tempted to interpret an ErdÅ‘s-RÃ©nyi graphon by defining edge ğ‘Š ğ›¼ (ğ‘¥, ğ‘¦) = bernoulli(ğ›¼).

However, this interpretation does not provide a model for the basic equations of the language, because this edge is not deterministic, and derivable equations such as (6) will fail. Intuitively, once an edge has been sampled between two given nodes, its presence (or absence) remains unchanged in the rest of the program, i.e. the edge is not resampled again, it is memoized (see also [[Kaddar and Staton 2023;](#b52)[Roy et al. 2008]](#b74)).

Although not all graphons are black-and-white, these are still a widely studied and useful class. They are often called 'random-free'. For example, an alternative characterization is that the random graph model of Prop. 21 has subquadratic entropy function [[Janson 2013, Â§10.6](#)].

## INTERPRETATION: ERDÅS-RÃ‰NYI GRAPHONS VIA RADO-NOMINAL SETS

In Section 4, we gave a general construction to show that every graphon arises from a Bernoullibased equational theory. In Section 5, we gave a more concrete interpretation, based on measuretheory, for black-and-white graphons. We now consider the ErdÅ‘s-RÃ©nyi graphons (Def. 30), which are not black-and-white.

Our interpretation is based on Rado-nominal sets. These are also studied elsewhere, but for different purposes (e.g. [[BojaÅ„czyk et al. 2014;](#b12)[BojaÅ„czyk and Place 2012;](#b13)[Klin et al. 2016](#b57)], [Pitts 2013, Â§1.9]).

Rado-nominal sets ( Â§6.1) are sets that are equipped with an action of the automorphisms of the Rado graph, which is an infinite graph that contains every finite graph. There is a particular Rado-nominal set V of the vertices of the Rado graph. The type vertex will be interpreted as V; edge is interpreted using the edge relation ğ¸ on V. The equational theory induced by this interpretation gives rise to the ErdÅ‘s-RÃ©nyi graphons (Def. 30).

Since Rado-nominal sets form a model of ZFA set theory (Prop. 36), we revisit probability theory internal to this setting. We consider internal probability measures on Rado-nominal sets ( Â§6.3), and we show that there are internal probability measures on V that give rise to ErdÅ‘s-RÃ©nyi graphons ( Â§6.3). The key starting point here is that, internal to Rado-nominal sets, the only functions V â†’ 2 are the sets of vertices that are definable in the language of graphs ( Â§6.2).

We organize the probability measures (Def. 37) into a probability monad on Rado-nominal sets ( Â§6.4), analogous to the Giry monad. Fubini does not routinely hold in this setting ( Â§6.4.4), but we use a standard technique to cut down to a commutative affine monad ( Â§6.4.5). This gives rise to a Bernoulli-based equational theory, and in fact, this theory corresponds to ErdÅ‘s-RÃ©nyi graphons (via (18): Corollary 45).

## Definition and First Examples

The Rado graph (V, ğ¸) ( [[Ackermann 1937;](#b6)[Rado 1964]](#b73), also known as the 'random graph' [[ErdÅ‘s and RÃ©nyi 1959]](#b27)) is the unique graph, up to isomorphism, with a countably infinite set of vertices that has the extension property: if ğ´, ğµ are disjoint finite subsets of V, then there is a vertex ğ‘ âˆˆ V \ (ğ´ âˆª ğµ) with an edge to all the vertices in ğ´ but none of the vertices in ğµ.

The Rado graph embeds every finite graph, which can be shown by using the extension property inductively.

An automorphism of the Rado graph is a graph isomorphism V â†’ V. The automorphisms of the Rado graph relate to isomorphisms between finite graphs, as follows. First, if ğ´ is a finite graph regarded as a subset of V, then any automorphism ğœ induces an isomorphism of finite graphs ğ´ ğœ [[ğ´]](#). Conversely, if ğ‘“ : ğ´ ğµ is an isomorphism of finite graphs, and we regard ğ´ and ğµ as disjoint subsets of V, then there exists an automorphism ğœ of V that restricts to ğ‘“ (i.e. ğ‘“ = ğœ | ğ´ ).

We write Aut [(Rado)](#) for the group of automorphisms of (V, ğ¸). (This has been extensively studied in model theory and descriptive set theory, e.g. [[Angel et al. 2014;](#b9)[Kechris et al. 2005](#b55) An element ğ‘¥ âˆˆ ğ‘‹ is defined to have finite support if there is a finite set ğ´ âŠ† V such that for all automorphisms ğœ, if ğœ fixes ğ´ (i.e. ğœ | ğ´ = id ğ´ ), it also fixes ğ‘¥ (i.e. ğœ â€¢ ğ‘¥ = ğ‘¥).

Equivariant functions between Rado-nominal sets are functions that preserve the group action (i.e. ğ‘“ (ğœ â€¢ ğ‘¥) = ğœ â€¢ (ğ‘“ (ğ‘¥))).

Proposition 32 [([Pitts 2013]](#b71)). If finite sets ğ´, ğµ âŠ† V both support ğ‘¥, so does ğ´ âˆ© ğµ. Hence every element has a least support.

Example 33.

(1) The set V of vertices is a Rado-nominal set, with ğœ â€¢ ğ‘ = ğœ (ğ‘). The support of vertex ğ‘ is {ğ‘}.

(2) The set V Ã— V of pairs of vertices is a Rado-nominal set, with ğœ â€¢ (ğ‘, ğ‘) = (ğœ (ğ‘), ğœ (ğ‘)).

The support of (ğ‘, ğ‘) is {ğ‘, ğ‘}. More generally, a finite product of Rado-nominal sets has a coordinate-wise group action.

(3) The edge relation ğ¸ âŠ† V Ã— V is a Rado-nominal subset (which is formally defined in Â§6.2) because automorphisms preserve the edge relation. (4) Any set ğ‘‹ can be regarded with the discrete action, ğœ â€¢ ğ‘¥ = ğ‘¥, and then every element has empty support. We regard these sets with the discrete action: 1 = {â˜…}; 2 = {0, 1}; N; and the unit interval [0, 1].

## Powersets and Definable Sets

For any subset ğ‘† âŠ† ğ‘‹ of a Rado-nominal set, we can define

$ğœ â€¢ ğ‘† = ğœ [ğ‘†] = {ğœ â€¢ ğ‘¥ | ğ‘¥ âˆˆ ğ‘† }. We let 2 ğ‘‹ = {ğ‘† âŠ† ğ‘‹ | ğ‘† has finite support}. (20$$)$This is a Rado-nominal set.

Example 34. We give some concrete examples of subsets.

(1) For vertices ğ‘ and ğ‘ in V with no edge between them, the set {ğ‘ âˆˆ V | ğ¸ (ğ‘, ğ‘) âˆ§ ğ¸ (ğ‘, ğ‘)} is the set of ways of forming a horn. It has support {ğ‘, ğ‘}.

(2) {(ğ‘, ğ‘) âˆˆ V 2 | ğ¸ (ğ‘, ğ‘) âˆ§ ğ¸ (ğ‘, ğ‘) âˆ§ Â¬ğ¸ (ğ‘, ğ‘)} is the set of horns with apex ğ‘; it has support {ğ‘}.

(3) {(ğ‘, ğ‘, ğ‘) âˆˆ V 3 | ğ¸ (ğ‘, ğ‘) âˆ§ ğ¸ (ğ‘, ğ‘) âˆ§ Â¬ğ¸ (ğ‘, ğ‘)} is the set of all oriented horns; it has empty support. (4) (Non-example) There is a countable totally disconnected subgraph of V; it does not have finite support as a subset of V.

In fact, the finitely supported subsets correspond exactly to the definable sets in first-order logic over the theory of graphs. The following results may be folklore.

Proposition 35. Let ğ‘† âŠ† V ğ‘› , and ğ´ âŠ† V be finite. The following are equivalent:

â€¢ ğ‘† = {(ğ‘  1 , . . . ğ‘  ğ‘› ) | ğœ™ (ğ‘  1 . . . ğ‘  ğ‘› )}, for a first-order formula ğœ™ over the theory of graphs, with parameters in ğ´; â€¢ ğ‘† has support ğ´.

Proof. (â‡’) For all isomorphisms ğ‘“ : V â†’ V that fix ğ´, and for all elements ğ‘ 1 . . . ğ‘ ğ‘˜ âˆˆ ğ´ and subsets ğ‘† = {(ğ‘  1 , . . . , ğ‘  ğ‘› ) | ğœ™ (ğ‘  1 . . . ğ‘  ğ‘› , ğ‘ 1 . . . ğ‘ ğ‘˜ )}, we have ğœ™ (ğ‘“ (ğ‘  1 ) . . . ğ‘“ (ğ‘  ğ‘› ), ğ‘ 1 . . . ğ‘ ğ‘˜ ) = ğœ™ (ğ‘“ (ğ‘  1 ) . . . ğ‘“ (ğ‘  ğ‘› ), ğ‘“ (ğ‘ 1 ) . . . ğ‘“ (ğ‘ ğ‘˜ )).

Furthermore, ğœ™ is invariant with respect to ğ‘“ . Thus, the image ğ‘“ (ğ‘†) âŠ† ğ‘†. By a similar argument, we have ğ‘“ -1 (ğ‘†) âŠ† ğ‘†, so that ğ‘† âŠ† ğ‘“ (ğ‘†). Thus, ğ‘“ (ğ‘†) = ğ‘† ( [[Marker 2002, Prop. 1.3.5]](#)).

(â‡) This is a consequence of the Ryll-Nardzewski theorem for the theory of the Rado graph (which can be shown to be ğœ”-categorical by a back-and-forth argument, using the extension property of the Rado graph). But we give here a more direct proof, assuming ğ‘› = 1 for simplicity. Suppose ğ´ âŠ† V is a finite support for ğ‘†. Then, for any ğ‘£, ğ‘£ â€² âˆˆ V\ğ´, if ğ‘£ and ğ‘£ â€² have the same connectivity to ğ´, then they are either both in or not in ğ‘† since, by the extension property, we can find an automorphism fixing ğ´ and sending ğ‘£ to ğ‘£ â€² . The set of vertices with the same connectivity to ğ´ as ğ‘£ is definable, and there are only 2 |ğ´| such sets. Hence, ğ‘†\ğ´ is a union of finitely many definable sets, and as ğ‘† âˆ© ğ´ is definable (being finite), so is ğ‘† = (ğ‘†\ğ´) âˆª (ğ‘† âˆ© ğ´). â–¡

We note that 2 ğ‘‹ in ( [20](#formula_45)) is a canonical notion of internal powerset, from a categorical perspective.

Proposition 36. RadoNom is a Boolean Grothendieck topos, with powerobject 2 ğ‘‹ in (20).

Proof notes. RadoNom can be regarded as continuous actions of Aut [(Rado)](#), regarded as a topological group with the product topology, and then we invoke standard methods [[Johnstone 2002, Ex. A2.1.6]](#). It is also equivalent to the category of sheaves over finite graphs and embeddings with the atomic topology. See [[Caramello 2013](#b16)[[Caramello , 2014] ]](#b17) for general discussion. â–¡

## Probability Measures on Rado-Nominal Sets

The finitely supported sets ğ‘† âŠ† V can be regarded as 'events' to which we would assign a probability. For example, if we already have vertices ğ‘ and ğ‘, we may want to know the chance of picking a vertex that forms a horn, and this would be the probability of the set in Ex. 34(a).

Definition 37. A sequence ğ‘† 1 , ğ‘† 2 â€¢ â€¢ â€¢ âŠ† ğ‘‹ is said to be support-bounded if there is one finite set ğ´ âŠ† V that supports all the sets ğ‘† ğ‘– .

A function ğœ‡ : 2 ğ‘‹ â†’ [0, 1] is (internally) countably additive if for any support-bounded sequence

$ğ‘† 1 , ğ‘† 2 â€¢ â€¢ â€¢ âŠ† ğ‘‹ of disjoint sets, ğœ‡ ( âˆ ğ‘–=1 ğ‘† ğ‘– ) = âˆ ğ‘–=1 ğœ‡ (ğ‘† ğ‘– ).$A probability measure on a Rado-nominal set ğ‘‹ is an equivariant function ğœ‡ : 2 ğ‘‹ â†’ [0, 1] that is internally countably additive, such that ğœ‡ (ğ‘‹ ) = 1.

We remark that there are two subtleties here. First, we restrict to support-bounded sequences. These are the correctly internalized notion of sequence in Rado-nominal sets, since they correspond precisely to finitely-supported functions N â†’ 2 ğ‘‹ . Second, we consider a Rado-nominal set to be equipped with its internal powerset 2 ğ‘‹ , rather than considering sub-ğœ-algebras.

Measures on the space of vertices. We define an internal probability measure (Def. 37) on the space V of vertices, which, we will show, corresponds to the ErdÅ‘s-RÃ©nyi graphon. Fix ğ›¼ âˆˆ [0, 1], the chance of an edge.

We define the measure ğœˆ ğ›¼ of a definable set ğ‘† âˆˆ 2 V as follows. Suppose that ğ‘† has support {ğ‘ 1 , . . . , ğ‘ ğ‘› }. We choose an enumeration of vertices (ğ‘£ 1 , . . . , ğ‘£ 2 ğ‘› ) in V (disjoint from {ğ‘ 1 , . . . , ğ‘ ğ‘› }) that covers all the 2 ğ‘› possible edge relationships that a vertex could have with the ğ‘ ğ‘– 's. (For example, ğ‘£ 1 has no edges to any ğ‘ ğ‘– , and ğ‘£ 2 ğ‘› has an edge to every ğ‘ ğ‘– , and the other ğ‘£ ğ‘— 's have the other possible edge relationships.) Let:

$ğœˆ ğ›¼ (ğ‘†) = 2 ğ‘› âˆ‘ï¸ ğ‘—=1 [ğ‘£ ğ‘— âˆˆ ğ‘†] ğ‘› ğ‘–=1 ğ›¼ğ¸ (ğ‘£ ğ‘— , ğ‘ ğ‘– ) + (1 -ğ›¼) (1 -ğ¸ (ğ‘£ ğ‘— , ğ‘ ğ‘– )) . (21$$)$Proposition 38. The assignment given in ( [21](#formula_48)) is an internal probability measure (Def. 37) on V.

Proof. The function ğœˆ ğ›¼ is well-defined: it does not depend on the choice of ğ‘£ ğ‘— 's (by Prop. 35), nor on the choice of support (by direct calculation). It is equivariant, since for ğœ â€¢ ğ‘†, a valid enumeration of vertices is given by ğœ

$â€¢ ğ‘£ 1 , . . . ğœ â€¢ ğ‘£ 2 ğ‘› . Also, ğœˆ (V) = 1, since V has empty support. Internal countable additivity follows from the identity ğ‘£ ğ‘— âˆˆ âˆ ğ‘–=1 ğ‘† ğ‘– = âˆ ğ‘–=1 [ğ‘£ ğ‘— âˆˆ ğ‘† ğ‘– ]. â–¡ Remark.$The definitions and results of this section appear to be novel. However, the general idea of considering measures on formulas which are invariant to substitutions that permute the variables goes back to work of Gaifman [[Gaifman 1964](#b35)]. The paper [[Ackerman et al. 2016a](#)] characterizes those countably infinite graphs that can arise with probability 1 in that framework; see [[Ackerman et al. 2017b](#)] for a discussion of how Gaifman's work connects to Prop. 21.

## Nominal Probability Monads

Since RadoNom is a Boolean topos with natural numbers object (Prop. 36), we can interpret measure-theoretic notions in the internal language of the topos, as long as they do not require the axiom of choice. We now spell out the resulting development, without assuming familiarity with topos theory. By doing this, we build new probability monads on RadoNom.

6.4.1 Finitely Supported Functions and Measures. Let ğ‘‹ and ğ‘Œ be Rado-nominal sets. The set of all functions ğ‘‹ â†’ ğ‘Œ has an action of Aut [(Rado)](#), given by (ğœ â€¢ ğ‘“ ) (ğ‘¥) = ğœ -1 â€¢ (ğ‘“ (ğœ â€¢ ğ‘¥)). The function space [ğ‘‹ â‡’ ğ‘Œ ] comprises those functions that have finite support under this action. Categorically, this structure is uniquely determined by the 'currying' bijection, natural in ğ‘ :

RadoNom(ğ‘ Ã— ğ‘‹, ğ‘Œ ) RadoNom(ğ‘, ğ‘‹ â‡’ ğ‘Œ ).

(For example, the powerobject 2 ğ‘‹ ( Â§6.2) can be regarded as [ğ‘‹ â‡’ 2], if we regard a set as its characteristic function.)

In Def. 37, we focused on equivariant probability measures. We generalize this to finitely supported measures. For example, pick a vertex ğ‘ âˆˆ V. Then, the Dirac measure on V (i.e. ğ›¿ ğ‘ (ğ‘†) = 1 if ğ‘ âˆˆ ğ‘†, and ğ›¿ ğ‘ (ğ‘†) = 0 if ğ‘ âˆ‰ ğ‘†) has support {ğ‘}.

Definition 39. For a Rado-nominal set ğ‘‹ , let P (ğ‘‹ ) comprise the finitely supported functions ğœ‡ : 2 ğ‘‹ â†’ [0, 1] that are internally countably additive, and satisfy ğœ‡ (ğ‘‹ ) = 1. This is a Rado-nominal set, as a subset of [2 ğ‘‹ â‡’ [0, 1]]. Functions in P (ğ‘‹ ) are called finitely supported probability measures. 6.4.2 Internal Integration. We revisit some basic integration theory in this nominal setting. In traditional measure theory, one can define the Lebesgue integral of a measurable function ğ‘“ : ğ‘‹ â†’ [0, 1] by âˆ« ğ‘“ (ğ‘¥)ğœ‡ (dğ‘¥) = sup ğ‘› ğ‘–=1 ğ‘Ÿ ğ‘– ğœ‡ (ğ‘ˆ ğ‘– ) where the supremum ranges over simple functions ğ‘– ğ‘Ÿ ğ‘– [-âˆˆ ğ‘ˆ ğ‘– ] with ğ‘ˆ ğ‘– measurable in ğ‘‹ and bounded above by ğ‘“ ( Â§2.3.3). The same construction works in the internal logic of RadoNom.

Note that the following does not mention ğ‘“ being measurable: since ğ‘‹ is considered to have its internal powerset ğœ-algebra, finite-supportedness implies 'measurability' here.

Proposition 40. Let ğœ‡ âˆˆ P (ğ‘‹ ) be a finitely supported probability measure on ğ‘‹ . For any finitely supported function ğ‘“ : ğ‘‹ â†’ [0, 1], the internally-constructed Lebesgue integral âˆ« ğ‘“ (ğ‘¥) ğœ‡ (dğ‘¥) âˆˆ [0, 1] exists. Moreover, integration is an equivariant map

$âˆ« : P (ğ‘‹ ) Ã— [ğ‘‹ â‡’ [0, 1]] â†’ [0, 1]$which preserves suprema of internally countable monotone sequences in its second argument.

Proof. If ğ‘ˆ 1 , . . . , ğ‘ˆ ğ‘› âŠ† ğ‘‹ are finitely supported, ğ‘Ÿ 1 , . . . , ğ‘Ÿ ğ‘› âˆˆ [0, 1], and ğ‘– ğ‘Ÿ ğ‘– [-âˆˆ ğ‘ˆ ğ‘– ] â‰¤ ğ‘“ , then by ordinary additivity of ğœ‡, we have ğ‘Ÿ ğ‘– ğœ‡ (ğ‘ˆ ğ‘– ) âˆˆ [0, 1]. By ordinary real analysis, the supremum of all such values exists and is in [0, 1]. For equivariance, recall that [0, 1] is equipped with the trivial action of Aut [(Rado)](#). Use the fact that

$ğ‘– ğ‘Ÿ ğ‘– [-âˆˆ ğ‘ˆ ğ‘– ] â‰¤ ğ‘“ if and only if ğ‘– ğ‘Ÿ ğ‘– [-âˆˆ ğœ â€¢ ğ‘ˆ ğ‘– ] â‰¤ ğœ â€¢ ğ‘“ .$The last claim is the monotone convergence theorem internalized to RadoNom. â–¡ 6.4.3 Kernels and a Monad. We can regard a 'probability kernel' as a finitely supported function ğ‘˜ : ğ‘‹ â†’ P (ğ‘Œ ). Equivalently, ğ‘˜ is a finitely supported function ğ‘˜ : ğ‘‹ Ã— 2 ğ‘Œ â†’ [0, 1] that is countably additive and has mass 1 in its second argument. (In traditional measure theory, one would explicitly ask that ğ‘˜ is measurable in its first argument, but as we observed, finite-supportedness already implies it.)

As usual, probability kernels compose, and this allows us to regard them as Kleisli morphisms for a monad (Def. 6), defined as follows.

Definition 41. We define the strong monad P on RadoNom as follows.

â€¢ For a Rado-nominal set ğ‘‹ , let P (ğ‘‹ ) comprise the finitely supported probability measures (Def. 39). â€¢ The unit of the monad ğœ‚ ğ‘‹ : ğ‘‹ â†’ P (ğ‘‹ ) is the Dirac measure, ğœ‚ ğ‘‹ (ğ‘¥) (ğ‘†) = [ğ‘¥ âˆˆ ğ‘†].

â€¢ The bind (>>=) : P (ğ‘‹ ) Ã— (ğ‘‹ â‡’ P (ğ‘Œ )) â†’ P (ğ‘Œ ) is given by

$(ğœ‡ >>= ğ‘˜) (ğ‘†) = âˆ« ğ‘‹ ğ‘˜ (ğ‘¥, ğ‘†) ğœ‡ (dğ‘¥).$We note that this is similar to the 'expectations monad' [[Jacobs and Mandemaker 2012, Thm. 4](#)].

## Commuting Integrals (Fubini).

For measures ğœ‡ 1 âˆˆ P (ğ‘‹ ) and ğœ‡ 2 âˆˆ P (ğ‘Œ ), the monad structure allows us to define a product measure

$ğœ‡ 1 âŠ— ğœ‡ 2 = ğœ‡ 1 >>= (ğœ†ğ‘¥ . ğœ‡ 2 >>= ğœ†ğ‘¦. ğœ‚ (ğ‘¥, ğ‘¦)) âˆ« ğ‘“ (ğ‘¥, ğ‘¦) (ğœ‡ 1 âŠ— ğœ‡ 2 ) (d(ğ‘¥, ğ‘¦)) = âˆ« âˆ« ğ‘“ (ğ‘¥, ğ‘¦) ğœ‡ 2 (dğ‘¦) ğœ‡ 1 (dğ‘¥).(22)$Although this iterated integration is reminiscent of the traditional approach, in general we cannot reorder integrals ('Fubini does not hold'). For example, given two measures ğœˆ ğ›¼ and ğœˆ ğ›½ for ğ›¼ â‰  ğ›½ and ğ‘“ being the characteristic function of the set {(ğ‘¥, ğ‘¦) :

$ğ¸ (ğ‘¥, ğ‘¦)} âŠ† V 2 , we have âˆ« âˆ« [ğ¸ (ğ‘¥, ğ‘¦)] ğœˆ ğ›¼ (dğ‘¦) ğœˆ ğ›½ (dğ‘¥) = âˆ« ğ›¼ ğœˆ ğ›½ (dğ‘¥) = ğ›¼ â‰  ğ›½ = âˆ« ğ›½ ğœˆ ğ›¼ (dğ‘¦) = âˆ« âˆ« [ğ¸ (ğ‘¥, ğ‘¦)] ğœˆ ğ›½ (dğ‘¥) ğœˆ ğ›¼ (dğ‘¦).(23)$However, it does hold when we consider only copies of the same measure.

Proposition 42. For ğœˆ ğ›¼ âˆˆ P (V) as in ( [21](#formula_48)), ğœˆ ğ›¼ commutes with ğœˆ ğ›¼ . That is, for any finitely supported

$ğ‘“ : V Ã— V â†’ [0, 1], âˆ« âˆ« ğ‘“ (ğ‘¥, ğ‘¦) ğœˆ ğ›¼ (dğ‘¦) ğœˆ ğ›¼ (dğ‘¥) = âˆ« âˆ« ğ‘“ (ğ‘¥, ğ‘¦) ğœˆ ğ›¼ (dğ‘¥) ğœˆ ğ›¼ (dğ‘¦).$Proof notes. By Prop. 35 and 40, it suffices to check on the indicator functions of definable subsets of V 2 . The indicators of sets {(ğ‘¥, ğ‘¦) | Î¦(ğ‘¥, ğ‘¦)} where Î¦(ğ‘¥, ğ‘¦) is a disjunction of ğ‘¥ = ğ‘¦, ğ‘¥ = ğ‘, or ğ‘¦ = ğ‘ for some ğ‘ âˆˆ V are seen to have integral 0 on both sides. The remaining possibilities can be reduced to the case where Î¦ ğ´,ğœ™,ğœ“,ğœ– (ğ‘¥, ğ‘¦) is (ğ‘¥, ğ‘¦ âˆ‰ ğ´) âˆ§ (ğ‘¥ â‰  ğ‘¦) âˆ§ (ğ¸ (ğ‘¥, ğ‘¦) â†” ğœ–) âˆ§ ğ‘âˆˆğ´ (ğ¸ (ğ‘, ğ‘¥) â†” ğœ™ ğ‘ ) âˆ§ (ğ¸ (ğ‘, ğ‘¦) â†” ğœ“ ğ‘ ) where ğ´ âŠ† V is a finite set, ğœ– âˆˆ {âŠ¥, âŠ¤}, and ğœ™,ğœ“ âˆˆ {âŠ¥, âŠ¤} ğ´ . This formula corresponds to choosing a two-vertex extension of the finite graph spanned by ğ´ âŠ† V. Intuitively, the two double integrals correspond to the two alternative two-step computations of the conditional probability of extending the graph ğ´ to this extension according to which of the two vertices is sampled first, and indeed both evaluate to ğ›¼ ğ‘˜ (1 -ğ›¼) 2|ğ´|+1-ğ‘˜ where ğ‘˜ = [ğœ–] + ğ‘âˆˆğ´ ( [ğœ™ ğ‘ ] + [ğœ“ ğ‘ ]). â–¡ Remark. In traditional measure theory, iterated integrals are defined using product ğœ-algebras.

Here we have not constructed product ğœ-algebras, but rather always take the internal powerset as the ğœ-algebra. This allows us to view all the definable sets as measurable on V ğ‘› (Prop. 35), which is very useful. We remark that alternative product spaces also arise in non-standard approaches to graphons (see [[Tao 2013, Â§6]](#) for an overview), and also in quasi-Borel spaces [[Heunen et al. 2017]](#b40) for different reasons.

6.4.5 A Commutative Monad. We now use Prop. 42 to build a commutative affine submonad P ğ›¼ of the monad P, which we will use to model the graph interface for the probabilistic programming language. With Prop. 36, we use the following general result. Proposition 43. Let T be a strong monad on a Grothendieck topos. Consider a family of morphisms {ğ‘“ ğ‘– : ğ‘‹ ğ‘– â†’ T (ğ‘Œ ğ‘– )} ğ‘– âˆˆğ¼ .

â€¢ There is a least strong submonad T ğ‘“ âŠ† T through which all ğ‘“ ğ‘– factor.

â€¢ If the morphisms ğ‘“ ğ‘– all commute with each other, then T ğ‘“ is a commutative monad (Def. 7).

Proof notes. Our argument is close to [[Kammar and McDermott 2018, Â§2.3]](#) and also [[Kammar 2014, Thms. 7.5 & 12.8]](#).

We let T ğ‘“ be the least subfunctor of T that contains the images of the ğ‘“ ğ‘– 's and ğœ‚, and is closed under the image of monadic bind (>>=). To show that this exists, we proceed as follows. First, fix a regular cardinal ğœ† > ğ¼ such that ğ‘Œ ğ‘– 's are all ğœ†-presentable, such that the topos is locally ğœ†-presentable (e.g. [[AdÃ¡mek and RosickÃ½ 1994]](#b7)). Consider the poset Sub ğœ† (T ) of ğœ†-accessible subfunctors of T . The cardinality bound ğœ† ensures it is small. Ordered by pointwise inclusion, this is a complete lattice: the non-empty meets are immediate, and the empty meet requires us to consider the ğœ†-accessible coreflection of T . We defined T ğ‘“ by a monotone property which we can regard as a monotone operator on this complete lattice Sub ğœ† (T ), and so the least ğœ†-accessible subfunctor exists. This is T ğ‘“ . Concretely, it is a least upper bound of an ordinal indexed chain. The chain starts with the functor ğ¹ 0 (ğ‘ ) = ğ‘– âˆˆğ¼,ğ‘”:ğ‘Œ ğ‘– â†’ğ‘ image(T (ğ‘”) â€¢ ğ‘“ ğ‘– ) âŠ† T (ğ‘ ) which is ğœ†-accessible because the ğ‘Œ ğ‘– 's are ğœ†-presentable. The chain iteratively closes under the image of monadic bind, until we reach a subfunctor that is a submonad of T .

To see that T ğ‘“ is commutative, we appeal to (transfinite) induction. Say that a subfunctor ğ¹ of T is commutative if all morphisms that factor through ğ¹ commute (Def. 7), and then note that the property of being commutative is preserved along the ordinal indexed chain. â–¡

With this in mind, fixing a measure ğœˆ ğ›¼ as in ( [21](#formula_48)), we form the least submonad P ğ›¼ of P induced by the morphisms ğœˆ ğ›¼ : 1 â†’ P (V) bernoulli : [0, 1] â†’ P (2) (24) where bernoulli(ğ‘Ÿ ) = ğ‘Ÿ â€¢ ğœ‚ (0) + (1 -ğ‘Ÿ ) â€¢ ğœ‚ (1).

Corollary 44. The least submonad P ğ›¼ of the probability monad P induced by the morphisms in (24) is a commutative affine monad (Def. 7).

Proof notes. It is easy to show that bernoulli commutes with every morphism ğ‘‹ â†’ P (ğ‘Œ ). Moreover, ğœˆ ğ›¼ commutes with itself (Prop. 42). Finally, P ğ›¼ is affine since P is. â–¡

## Summary and Interpretation

Fix ğ›¼ âˆˆ [0, 1]. We induce an internal measure ğœˆ ğ›¼ on the vertices of the Rado graph as explained in (21); and build a commutative submonad P ğ›¼ of P. We can then interpret the graph probabilistic programming language. We interpret types as Rado-nominal sets:

$bool = 2 vertex = V unit = 1 ğ´ 1 * ğ´ 2 = ğ´ 1 Ã— ğ´ 2 . (25$$)$We interpret typed programs Î“ âŠ¢ ğ‘¡ : ğ´ as Kleisli morphisms Î“ â†’ P ğ›¼ ( ğ´ )

i.e. internal probability kernels Î“ Ã— 2 ğ´ â†’ [0, 1]. Sequencing (let) is interpreted using the monad structure, with new : 1 â†’ P ğ›¼ (V) and edge : V Ã— V â†’ P ğ›¼ (2) as new() = ğœˆ ğ›¼ edge (ğ‘£, ğ‘¤) = ğœ‚ (ğ¸ (ğ‘£, ğ‘¤))

Corollary 45. Consider the interpretation in Rado-nominal sets (( [25](#formula_57))-( [26](#formula_59))). If we form the sequence of random graphs in (18), then these correspond to the ErdÅ‘s-RÃ©nyi graphon.

Proof notes. The semantics interprets ground types as finite sets with discrete Aut [(Rado)](#) action -in which case internal probability kernels correspond to stochastic matrices, agreeing with FinStoch. Thus, the theory is Bernoulli-based. To see that the graphon arises, consider for instance when ğ‘› = 2, we have: for ğ‘¡ 2 as in (18), and therefore ğ‘¡ 2 = ğ›¿ 0 , bernoulli(ğ›¼) bernoulli(ğ›¼), ğ›¿ 0 : P (2 4 )

For general ğ‘›, this corresponds to the random graph model ğ‘ ğ‘Š ğ›¼ ,ğ‘› for the ErdÅ‘s-RÃ©nyi graphon ğ‘Š ğ›¼ . â–¡

## CONCLUSION

Summary. We have shown that equational theories for the graph interface to the probabilistic programming language (Ex. 1) give rise to graphons (Theorem 23). Conversely, every graphon arises in this way. We showed this generally using an abstract construction based on Markov categories (Corollary 26) and methods from category theory [[Hermida and Tennent 2012;](#b39)[Hu and Tholen 1995]](#b41). Since this is an abstract method, we also considered two concrete styles of semantic interpretation that give rise to classes of graphons: traditional measure-theoretic interpretations give rise to black-and-white graphons (Prop. 28), and an interpretation using the internal probability theory of Rado-nominal sets gives rise to ErdÅ‘s-RÃ©nyi graphons (Corollary 45).

Further context, and future work. The idea of studying exchangeable structures through program equations is perhaps first discussed in the abstract [[Staton et al. 2017](#b77)], whose Â§3.2 ends with an open question about semantics of languages with graphs that the present paper addresses. Subsequent work addressed the simpler setting of exchangeable sequences and beta-bernoulli conjugacy through program equations [[Staton et al. 2018]](#b76), and stochastic memoization [[Kaddar and Staton 2023]](#b52); the latter uses a category similar to RadoNom, although the monad is different. Beyond sequences [[Staton et al. 2018](#b76)] and graphs (this paper), a natural question is how to generalize to arbitrary exchangeable interfaces (see e.g. [[Orbanz and Roy 2015]](#b68)). For example, we could consider exchangeable random boolean arrays via the interface new-row : unit â†’ row, new-column() : unit â†’ column, entry : row * column â†’ bool and random hypergraphs with the interface new : unit â†’ vertex, hyperedge ğ‘› : vertex ğ‘› â†’ bool.

We could also consider interfaces for hierarchical structures, such as arrays where every entry contains a graph. Diverse exchangeable random structures have been considered from the modeltheoretic viewpoint [[Ackerman 2015;](#b0)[Crane and Towsner 2018]](#b21) and from the perspective of probability theory (e.g. [[Campbell et al. 2023;](#b15)[Jung et al. 2021;](#b51)[Kallenberg 2010]](#b53)), but it remains to be seen whether the programming perspective here can provide a unifying view. Another point is that graphons correspond to dense graphs, and so a question is how to accommodate sparse graphs from a programming perspective (e.g. [[Caron and Fox 2017;](#b19)[Veitch and Roy 2019]](#b82)).

This paper has focused on a very simple programming language ( Â§2.1). As mentioned in Section 1.5, several implementations of probabilistic programming languages do support various Bayesian nonparametric primitives based on exchangeable sequences, partitions, and relations (e.g. [[Dash et al. 2023;](#b24)[Goodman et al. 2008;](#b37)[Kiselyov and Shan 2010;](#b56)[Mansinghka et al. 2014;](#b64)[Roy et al. 2008;](#b74)[Wood et al. 2014]](#b84)). In particular, the 'exchangeable random primitive' (XRP) interface [[Ackerman et al. 2016b;](#)[Wu 2013](#b85)] provides a built-in abstract data type for representing exchangeable sequences. This aids model design by its abstraction, but also aids inference performance by clarifying the independence relationships.

Aside from practical inference performance, we can ask whether representation and inference are computable. For the simpler setting of exchangeable sequences, this is dealt with positively by [[Freer and](#)[Roy 2010, 2012]](#). The question of computability for graphons and exchangeable graphs is considerably subtler, and some standard representations are noncomputable [[Ackerman et al. 2019](#b4)] (see also [[Ackerman et al. 2017a]](#)). This suggests several natural questions about whether certain natural classes of computable exchangeable graphs can be identified by program analyses in the present context.

![For a simple example, we can write a program over the interface to calculate the probability of three random vertices forming a triangle: the program let ğ‘ = new() in let ğ‘ = new() in let ğ‘ = new() in edge(ğ‘, ğ‘) & edge(ğ‘, ğ‘) & edge(ğ‘, ğ‘) : bool (5)]()

![Fig. 1. (a) A graph; (b) an inferred geometric realization of it (ğœƒ â‰ˆ ğœ‹/3); (c) a generated sample for ğœƒ = ğœ‹/6.]()

![]. (Similar examples are implemented in[Goodman and Tenenbaum 2023, Ch. 12], albeit untyped.) Then our interface is captured by a Haskell type class: 1 class RandomGraph p vertex | p â†’ vertex where new :: p â†’ Prob vertex edge :: p â†’ vertex â†’ vertex â†’ Bool Here p is a parameter type, and we write Prob for a probability monad. A spherical implementation of the interface (following Â§1.1) is parameterized by the dimension ğ‘‘ and the distance ğœƒ , as follows: data SphGrph = SG Int Double --parameters for a sphere graph data SphVertex = SV [ Double ] --vertices are Euclidean coordinates instance RandomGraph SphGrph SphVertex where new :: SphGrph â†’ Prob SphVertex new ( SG d theta ) = ... --sample a random unit d -vector uniformly edge :: SphGrph â†’ SphVertex â†’ SphVertex â†’ Bool edge ( SG d theta ) v w = ... --check whether arccos (v.w) < theta]()

![and although our equivalence relation is slightly coarser, it still respects the symmetric monoidal category structure, and there is a monoidal functor Fam(G op ) â†’ Fam(G op ) [ğœˆ], regarding each morphism ğ‘“ : Ã¬ ğ‘‹ â†’ Ã¬ ğ‘Œ in Fam(G op ) as a morphism [0, ğ‘“ ] in Fam(G op ) [ğœˆ]. But there is also now an adjoined morphism ğœˆ = [1, id] : 1 â†’ vertex . This monoidal category Fam(G op ) [ğœˆ] moreover inherits the distributive coproduct structure from Fam(G op ), and the functor Fam(G op ) â†’ Fam(G op ) [ğœˆ] is a distributive Markov functor. To define copairing of [ğ‘˜, ğ‘“ ] : Ã¬ ğ‘‹ â†’ Ã¬ ğ‘ and [ğ‘™, ğ‘”] : Ã¬ ğ‘Œ â†’ Ã¬ ğ‘ we use the reindexing equivalence relation to assume ğ‘˜ = ğ‘™ and then define the copairing as âŸ¨[ğ‘˜, ğ‘“ ], [ğ‘˜, ğ‘”]âŸ© = [ğ‘˜, âŸ¨ğ‘“ , ğ‘”âŸ©] : Ã¬ ğ‘‹ + Ã¬ ğ‘Œ â†’ Ã¬ ğ‘ . In summary:]()

![].) Definition 31. A Rado-nominal set is a set ğ‘‹ equipped with an action â€¢ : Aut(Rado) Ã— ğ‘‹ â†’ ğ‘‹ (i.e. id â€¢ ğ‘¥ = ğ‘¥; (ğœ 2 â€¢ ğœ 1 ) â€¢ ğ‘¥ = ğœ 2 â€¢ ğœ 1 â€¢ ğ‘¥) such that every element has finite support.]()

![ğ‘¥ 1 , ğ‘¥ 1 )], [ğ¸ (ğ‘¥ 1 , ğ‘¥ 2 )] [ğ¸ (ğ‘¥ 2 , ğ‘¥ 1 )], [ğ¸ (ğ‘¥ 2 , ğ‘¥ 2 )] (ğœˆ ğ›¼ âŠ— ğœˆ ğ›¼ ) (d(ğ‘¥ 1 , ğ‘¥ 2 ))]()

See https://lazyppl-team.github.io/GraphDemo.html for full details in literate Haskell.

ğ´, ğ´ 1 , ğ´ 2 , ğµ ::= unit | 0 | ğ´ 1 * ğ´ 2 | ğ´ 1 + ğ´ 2 | . . . and terms, including the typical constructors and destructors but also explicit sequencing (let in)ğ‘¡, ğ‘¡ 1 , ğ‘¡ 2 , ğ‘¢ ::= ğ‘¥ | () | (ğ‘¡ 1 , ğ‘¡ 2 ) | ğœ‹ 1 ğ‘¡ | ğœ‹ 2 ğ‘¡ | in 1 ğ‘¡ | in 2 ğ‘¡ | let ğ‘¥ = ğ‘¡ 1 in ğ‘¡ 2 | case ğ‘¡ of {} | case ğ‘¡ of {in 1 (ğ‘¥ 1 ) â‡’ ğ‘¢ 1 ; in 2 (ğ‘¥ 2 ) â‡’ ğ‘¢ 2 } | . . .We consider the standard typing rules (where ğ‘– âˆˆ {1, 2}):Î“, ğ‘¥ : ğ´, Î“ â€² âŠ¢ ğ‘¥ : ğ´ Î“ âŠ¢ () : unit Î“ âŠ¢ ğ‘¡ 1 : ğ´ 1 Î“ âŠ¢ ğ‘¡ 2 : ğ´ 2 Î“ âŠ¢ (ğ‘¡ 1 , ğ‘¡ 2 ) : ğ´ 1 * ğ´ 2 Î“ âŠ¢ ğ‘¡ : ğ´ 1 * ğ´ 2 Î“ âŠ¢ ğœ‹ ğ‘– ğ‘¡ : ğ´ ğ‘– Î“ âŠ¢ ğ‘¡ : ğ´ ğ‘– Î“ âŠ¢ in ğ‘– ğ‘¡ : ğ´ 1 + ğ´ 2 Î“ âŠ¢ ğ‘¡ : ğ´ Î“, ğ‘¥ : ğ´ âŠ¢ ğ‘¢ : ğµ Î“ âŠ¢ let ğ‘¥ = ğ‘¡ in ğ‘¢ : ğµ

Î“ â†’ ğ´ . (See e.g.[[Pitts 2001](#b71)] for a general discussion of terms as morphisms.)In more detail, to give such an interpretation, type constants must first be given chosen interpretations as objects of C. We can then interpret types and contexts using the monoidal and coproduct structure of C. Following this, term constants ğ‘“ : ğ´ â†’ ğµ must be given chosen interpretations as morphisms ğ‘“ : ğ´ â†’ ğµ in C. The interpretation of other terms is made by induction on the structure of typing derivations in a standard manner, using the structure of the distributive Markov category (e.g.[[Benton et al. 1992](#b11)],[Stein 2021,  Â§7.2]). For example,Î“, ğ‘¥ : ğ´, Î“ â€² âŠ¢ ğ‘¥ : ğ´ = Î“, ğ‘¥ : ğ´, Î“ â€² Î“ âŠ— ğ´ âŠ— Î“ â€² !âŠ— ğ´ âŠ—! ------â†’ 1 âŠ— ğ´ âŠ— 1 ğ´ Î“ âŠ¢ let ğ‘¥ = ğ‘¡ in ğ‘¢ : ğµ = Î“ Î” Î“ ---â†’ Î“ âŠ— Î“ Î“ âŠ— ğ‘¡ ------â†’ Î“ âŠ— ğ´ = Î“, ğ‘¥ : ğ´ ğ‘¢ --â†’ ğµÎ“ âŠ¢ case ğ‘¡ of {in 1 (ğ‘¥ 1 ) â‡’ ğ‘¢ 1 ; in 2 (ğ‘¥ 2 ) â‡’ ğ‘¢ 2 } : ğµ =

