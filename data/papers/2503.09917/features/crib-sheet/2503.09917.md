- **System Overview**
  - MareNostrum5: Pre-exascale supercomputer at BSC, peak performance of **314 PFlop/s**.
  - Hybrid architecture: Intel Sapphire Rapids CPUs, NVIDIA Hopper GPUs, DDR5, and HBM.
  - Organized into four partitions: GPP, ACC, General Purpose - Next Generation, Accelerated - Next Generation.

- **General Purpose Partition (GPP)**
  - Composed of dual-socket Intel Sapphire Rapids 8480+ CPUs (56 cores/socket, 2 GHz).
  - Configurations:
    - **DDR Compute Nodes**: 6,192 nodes, 256 GB DDR5 (16 DIMMs of 16 GB).
    - **DDR-HM Compute Nodes**: 216 nodes, 1 TB DDR5 (16 DIMMs of 64 GB).
    - **HBM Compute Nodes**: 72 nodes, 128 GB HBM2 + 32 GB DDR5 (2 DIMMs of 16 GB).
  - Power consumption under HPL workloads: 
    - DDR: 11.4 kW, DDR-HM: 12.0 kW, HBM: 10.4 kW.

- **Accelerated Partition (ACC)**
  - Composed of dual Intel Sapphire Rapids 8460Y+ CPUs (40 cores, 2.3 GHz) and four NVIDIA Hopper GPUs (H100).
  - Each node: 512 GB DDR5, 64 GB HBM2e per GPU.
  - Power consumption under HPL workloads: **3.5 kW**.

- **Benchmarking**
  - Micro-benchmarks: Floating-point performance, memory bandwidth, interconnect throughput.
  - HPC benchmarks: HPL and HPCG results for GPP and ACC.
  - Application studies: Alya (fluid dynamics), OpenFOAM (computational fluid mechanics), IFS (weather modeling).

- **Energy Efficiency**
  - EAR (Energy Aware Runtime) framework for power consumption monitoring.
  - Impact of direct liquid cooling on energy efficiency.

- **Memory Technologies**
  - Comparison of HBM and DDR5 configurations for performance trade-offs.

- **Storage Infrastructure**
  - **HPC Storage**: Elastic Storage Server (ESS-3500) configurations for performance and capacity.
    - ESS-3500-P: 368 TB raw storage (performance).
    - ESS-3500-C: 1836 TB raw storage (capacity).
  - **Archive Storage**: IBM TS4500 tape system, 44 PB disk cache, 400 PB tape capacity.
  - **Parallel Filesystem**: GPFS, features include:
    - POSIX-compliant access, data striping, distributed metadata, fault tolerance, scalability (1.2 TB/s read, 1.6 TB/s write).

- **Network Architecture**
  - Infiniband NDR200 fabric, three-layer fat-tree topology with 324 switches.
  - Layer 1: Compute and storage nodes; Layer 2: Switch connections; Layer 3: Core switches.

- **System Software**
  - OS: Red Hat Enterprise Linux 9.2, Linux kernel 5.14.
  - Job scheduling: SLURM, supports interactive and batch jobs.
  - Software modules managed via Lmod, including compilers and libraries.