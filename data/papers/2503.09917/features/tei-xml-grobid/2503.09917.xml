<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Introducing MareNostrum5: A European pre-exascale energy-efficient system designed to serve a broad spectrum of scientific workloads</title>
				<funder ref="#_bzcNkeg">
					<orgName type="full">Connecting Europe Facility</orgName>
				</funder>
				<funder ref="#_5dN6emX">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder>
					<orgName type="full">European Union</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Fabio</forename><surname>Banchelli</surname></persName>
							<email>fabio.banchelli@bsc.es</email>
							<affiliation key="aff0">
								<orgName type="institution">Barcelona Supercomputing Center</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marta</forename><surname>Garcia-Gasulla</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Barcelona Supercomputing Center</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Filippo</forename><surname>Mantovani</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Barcelona Supercomputing Center</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Joan</forename><surname>Vinyals</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Barcelona Supercomputing Center</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Josep</forename><surname>Pocurull</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Barcelona Supercomputing Center</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Vicente</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Barcelona Supercomputing Center</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Beatriz</forename><surname>Eguzkitza</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Barcelona Supercomputing Center</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Flavio</forename><forename type="middle">C C</forename><surname>Galeazzo</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">HLRS -University of Stuttgart</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mario</forename><forename type="middle">C</forename><surname>Acosta</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Barcelona Supercomputing Center</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sergi</forename><surname>Girona</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Barcelona Supercomputing Center</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Introducing MareNostrum5: A European pre-exascale energy-efficient system designed to serve a broad spectrum of scientific workloads</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">59588E0F213736496F5198E797148A44</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-03-15T12:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>supercomputing</term>
					<term>exascale</term>
					<term>benchmarks</term>
					<term>performance</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>MareNostrum5 is a pre-exascale supercomputer at the Barcelona Supercomputing Center (BSC), part of the EuroHPC Joint Undertaking. With a peak performance of 314 petaflops, MareNostrum5 features a hybrid architecture comprising Intel Sapphire Rapids CPUs, NVIDIA Hopper GPUs, and DDR5 and high-bandwidth memory (HBM), organized into four partitions optimized for diverse workloads. This document evaluates MareNostrum5 through micro-benchmarks (floating-point performance, memory bandwidth, interconnect throughput), HPC benchmarks (HPL and HPCG), and application studies using Alya, OpenFOAM, and IFS. It highlights MareNostrum5 's scalability, efficiency, and energy performance, utilizing the EAR (Energy Aware Runtime) framework to assess power consumption and the effects of direct liquid cooling. Additionally, HBM and DDR5 configurations are compared to examine memory performance trade-offs. Designed to complement standard technical documentation, this study provides insights to guide both new and experienced users in optimizing their workloads and maximizing MareNostrum5 's computational capabilities.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>MareNostrum5, hosted at the Barcelona Supercomputing Center (BSC), is a pre-exascale supercomputer within the EuroHPC Joint Undertaking. It is designed to support high-performance computing (HPC) workloads, offering a peak computational capacity of 314 petaflops (PFlop/s). MareNostrum5 features a hybrid architecture that integrates different technologies provided by multiple vendors and assembled by Atos. The system is organized into four primary partitions optimized for general-purpose and accelerated workloads: General Purpose Partition (GPP), based on Intel Sapphire Rapids CPUs, with a small fraction of nodes featuring HBM memory technology; Accelerated Partition (ACC), based on Intel Sapphire Rapids CPUs coupled with NVIDIA Hopper GPUs; General Purpose -Next Generation Partition, based on the NVIDIA Grace Superchip; Accelerated -Next Generation Partition, which is still under definition. This document provides a report on HPC benchmarks (HPL and HPCG) for both the GPP and ACC partitions, along with a detailed evaluation of the GPP partition using architectural micro-benchmarks and applications.</p><p>The General Purpose Partition employs Intel Sapphire Rapids CPUs and provides configurations with DDR5 memory and highbandwidth memory (HBM), targeting applications that require substantial memory throughput. GPP earned the spot #22 in the Top500 list of June 2024 <ref type="bibr" target="#b0">[1]</ref>.</p><p>The Accelerated Partition combines Sapphire Rapids CPUs with NVIDIA Hopper GPUs, interconnected via NVLink and PCIe Gen5. Two additional partitions, featuring next-generation CPU and GPU technologies, are planned for deployment in the near future to further extend MareNostrum5 's capabilities. ACC earned the spot #8 in the Top500 list of June 2024 <ref type="bibr" target="#b1">[2]</ref> and the spot #15 in the Green500.</p><p>The benchmarking detailed in this document is structured arXiv:2503.09917v1 [cs.DC] 13 Mar 2025</p><p>into three levels of increasing complexity: i) Results of microbenchmarking focus on metrics such as floating-point performance, memory bandwidth, and interconnect throughput. ii) Summaries of HPC benchmarks, including HPL and HPCG, are presented. iii) Scalability and efficiency studies are conducted using real-world scientific applications such as Alya for fluid dynamics, OpenFOAM for computational fluid mechanics, and IFS for weather and climate modeling. MareNostrum5 integrates advanced energy monitoring and management systems through the EAR (Energy Aware Runtime) framework, enabling precise power consumption monitoring at both the component and job levels. This study leverages EAR to evaluate the impact of direct liquid cooling technology on energy efficiency, particularly for the densely packed compute nodes of MareNostrum5.</p><p>Additionally, MareNostrum5 serves as a testbed for emerging memory technologies, with a detailed comparison of HBM and DDR5 configurations provided.</p><p>This evaluation aims to establish a reference for scientists using MareNostrum5 to produce scientific results with complex computational codes. This document complements the technical documentation typically provided with new HPC clusters 1 . It is intended to assist both novice users, offering insights into the system's behavior, and expert users, helping them anticipate the challenges and benefits of running their scientific codes on MareNostrum5.</p><p>The document is organized as follows: Section 2 provides an overview of the system, describing the different partitions of Ma-reNostrum5 and the underlying hardware components and system software; Section 3 is dedicated to low-level micro-architectural benchmarks; Section 4 summarizes the performance and energy results measured when running the Top500 benchmarks, HPL and HPCG; Section 5 reports about the HPC applications performances and scalability with runs up to thousand of compute nodes; Section 6 studies the impact on performance and power consumption of two different memory technologies, DDR5 and HBM, when used for running scientific applications; Section 7 recollect comments and conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">System Overview</head><p>MareNostrum 5 is a pre-exascale EuroHPC supercomputer hosted at the Barcelona Supercomputing Center (BSC). The system has a total peak computational power of 314 PFlop/s and is supplied by Bull SAS, combining Bull Sequana XH3000 and Lenovo ThinkSystem architectures. The system is organized into four partitions with different technical characteristics, collectively meeting the needs of any HPC user.</p><p>In the following sections, we introduce the overall infrastructure of the MareNostrum 5 supercomputer, with a focus on the first two partitions, GPP and ACC, which were deployed and made available to users in Q2 2024.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">General Purpose Partition (GPP)</head><p>The General Purpose Partition (GPP) of the MareNostrum 5 supercomputer is composed of dual-socket compute nodes powered by Intel Sapphire Rapids 8480+ CPUs with 56 cores per socket at 2 GHz. These compute nodes are organized into three configurations:</p><p>1 <ref type="url" target="https://bsc.es/supportkc/docs/MareNostrum5/intro">https://bsc.es/supportkc/docs/MareNostrum5/intro</ref> </p><p>• DDR Compute Nodes: 6,192 nodes, each with 16 DIMMs of 16GB DDR5 memory, totaling 256 GB per node.</p><p>• DDR-HM Compute Nodes: 216 nodes, each with 16 DIMMs of 64GB DDR5 memory, providing 1 TB per node.</p><p>• HBM Compute Nodes: 72 nodes, equipped with 2 DIMMs of 16GB DDR5 and an additional 128 GB of HBM2 memory for high-bandwidth applications <ref type="foot" target="#foot_0">2</ref> .</p><p>Each pair of compute nodes is installed on a ThinkSystem SD650v3 dual-node tray within a 19-inch chassis. These nodes share a single Infiniband NDR200 link, achieving up to 100 Gb/s bandwidth per node, alongside a dedicated 25 GbE link for service networking. Figure <ref type="figure" target="#fig_0">1</ref> shows the block diagram of the main components of a MareNostrum5 GPP node <ref type="foot" target="#foot_1">3</ref> . The nodes are arranged in groups of 12 per chassis, which fit in a 6U space in each rack. Cooling is handled with two circuits. One circuit, at 32-42°C, manages the direct liquid cooling of the compute nodes and power supplies (2 × 7.2 kW), while the second circuit, at 17-27°C, cools the cold doors for air-cooled components. Power consumption varies by node type under HPL workloads (i.e., 85% of efficiency): 11.4 kW for DDR nodes, 12.0 kW for DDR-HM nodes, and 10.4 kW for HBM nodes.</p><p>Each rack contains, from bottom to top: three Lenovo chassis corresponding to 36 compute nodes, one Infiniband switch (model QM9790), another three Lenovo chassis corresponding to 36 nodes, and a top-of-rack Infiniband switch (model SN3700C).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Accelerated Partition (ACC)</head><p>The compute nodes of the accelerated partition of MareNostrum5 are composed of a host CPU board (blue in Figure <ref type="figure" target="#fig_1">2</ref>) powered by two Intel Sapphire Rapids processors 8460Y+ (40 cores, 2.3 GHz, 300 W) and four NVIDIA Hopper GPU modules. The CPU host board is equipped with 16 DIMMs of 32 GB running at 4.8 GHz DDR5, summing up 512 GB of RAM per ACC compute node. Each Hopper GPU (H100) offers 64 GB of HBM2e and they are fully connected with NVLINK (green lines in Figure <ref type="figure" target="#fig_1">2</ref>). GPU pairs are connected with a bi-directional bandwidth of 300 GB/s. Each of the four GPU modules provides a x16 PCIe Gen5 link for the upstream connectivity to the host CPU (blue lines in Figure <ref type="figure" target="#fig_1">2</ref>). Each node houses 480 GB NVMe local storage. The compute node has been designed by Atos R&amp;D labs in France primarily for the MareNostrum5 project. All components of the ACC node are direct liquid cooled with the same hot-water circuit described for the General Purpose Partition. Each GPU has a direct 200 Gb/s connection to the Infiniband network (ConnectX-7). ACC compute nodes are interconnected with 2× QSFP connectors, each one routing an Infiniband NDR200 link. Power consumption under HPL workloads (i.e., 77% of efficiency) is 3.5 kW.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Storage</head><p>Each GPP node in MareNostrum5 houses one NVME drive as local storage (omitted in Figure <ref type="figure" target="#fig_0">1</ref> for simplicity). This local storage can be configured upon request to provide users with a fast IO setup. Beyond the node, the storage infrastructure of MareNostrum5 is divided into: HPC storage, archive storage and the parallel filesystem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HPC storage</head><p>The building block of the HPC storage is the 5th generation of the Elastic Storage Server (model ESS-3500). Each ESS-3500 is powered by 1x48 core AMD EPYC Rome running at 2.2 GHz, 512 GB of DDR4 memory, 2× NDR 200, 2× 100 Gbps ethernet, over PCI Gen 4 links. Depending on the kind of storage task to be performed, the ESS-3500 server can be configured as i) ESS-3500-P (as performance), i.e., coupled with 24× NVMe PCIe attached flash drives delivering 368 TB raw storage in 2U (net capacity 2.81 PB, 8 + 2P); ii) ESS-3500-C (as capacity), i.e., coupled with 102× HDD of 18 TB each HDD, delivering a total of 1836 TB raw storage in 4U (net capacity 248 PB, 8 + 3P). A storage unit (∼ half rack) includes 2× ESS-3500-P coupled with 4× ESS-3500-C for a total of 18U and ∼ 8 PB of raw storage. Combining these storage units into the 25 cabinet installed in MareNostrum5 it allows a total raw capacity of 4.79 PB of level 1 (or fast) storage (delivered by 13× ESS-3500-P), combined with 367 PB of level 2 (or slow) storage (delivered by 50× ESS-3500-C) and 4.79 PB. Level 1 storage is logically dedicated to store data of highly accessed partitions, such as /home or /apps while level 2 storage is used for mid-term storage locations such as /projects or /scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Archive storage</head><p>The long term storage is handled with a tape system that includes 2× IBM TS4500. The archive of MareNos-trum5 offers a total of 44 PB disk cache combined with 400 PB total tape capacity. The underling resources are 64 TS1160 fibre channel drives coupled with 20100× JE Enterprise Gen6 tapes of 20 TB each, orchestrated by 8× Spectrum Archive servers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parallel filesystem</head><p>The parallel filesystem adopted in MareNos-trum5 is GPFS. It is a high-performance, distributed filesystem optimized for scalability and fault tolerance in large computing clusters. Key features include:</p><p>• POSIX-Compliant Access -Supports standard system calls, allowing concurrent access to files by multiple processes across nodes.</p><p>• Data Striping -Distributes file blocks across multiple disks in a round-robin fashion, improving aggregate bandwidth and load balancing.</p><p>• Distributed Metadata -Spreads metadata across nodes to avoid bottlenecks and enhance performance.</p><p>• Fault Tolerance -Eliminates single points of failure and ensures data integrity despite node or disk failures.</p><p>• Scalability -The parallel filesystem is able to sustain 1.2 TB/s and 1.6 TB/s of read and write bandwidth, respectively.</p><p>GPFS is ideal for high-performance computing and big data environments, providing efficient, concurrent data access with robust fault tolerance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Network</head><p>MareNostrum5 has a high-speed network that uses Infiniband NDR200 fabric. The network topology is a three-layer fat-tree with a total of 324 switches (model QM9790). Figure <ref type="figure" target="#fig_2">3</ref> represents a schematic view of the MareNostrum5 network. It has an implementation of 3 GPP islands (blue), 1 storage island (red) and 7 ACC islands (green). The layer levels are as follows: Layer 1 includes compute and storage nodes. Layer 2 connects the switches of the Layer 1 to the core switch. Layer 3 is the top-level layer including core switches (yellow). For clarity, Figure <ref type="figure" target="#fig_2">3</ref> does not depict all the connections between switches and nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">System software Operating System</head><p>The OS in all compute nodes of MareNos-trum5 is an instance of Red Hat Enterprise Linux version 9.2 with a Linux kernel version 5.14. Lmod Software tools and libraries are provided to users via environment modules with Lmod <ref type="bibr" target="#b2">[3]</ref>. Modules are classified into different types such as compilers, libraries, and applications. Each module describes the dependencies and conflicts with other modules to ensure that users are able to configure a functioning software environment. The list of available software changes over time, but it is always updated on the BSC website <ref type="foot" target="#foot_2">4</ref> .</p><p>SLURM Access to compute resources is granted via the SLURM scheduler <ref type="bibr" target="#b3">[4]</ref>. The SLURM configuration also includes job statistics to evaluate the current status of the cluster and perform statistical analysis of its usage. MareNostrum5 users may run interactive and batch jobs targeting one of the multiple partitions of the cluster. Users may also toggle certain features such as access to hardware counters (enabling perfparanoid), configuring a specific CPU frequency governor, or requesting exclusive access to the allocated nodes.</p><p>Compilers In this work we use a subset of the available compilers in MareNostrum5. These are the Intel compiler version 2023.2 with a new backend which is based on LLVM (i.e., icx), and the GNU compiler 13.2.0. MPI Similarly, in this work we use only two MPI flavors of all the ones available in MareNostrum5. These are Intel MPI and OpenMPI. Throughout the evaluation, we ensure that the correct compiler backend is chosen by setting the appropriate environment variables. For example, OMPI_CC to specify the C compiler in OpenMPI.</p><p>PAPI PAPI <ref type="bibr" target="#b4">[5]</ref> is a performance analysis tool that allows user code to read hardware counters. It abstracts the implementation details of each CPU and provides a unified interface that is portable across architectures. In this work we use PAPI version 7.0 to read the cycles and instructions counters of the Sapphire Rapids CPU. The values reported by these counters are the base on which we conduct performance analysis in Section 3 and Section 5.</p><p>TALP TALP <ref type="bibr" target="#b5">[6]</ref> is the profiling tool that we use to gather the efficiency metrics presented in Section 5.1 (detailed definition can be found in Appendix A). The current version of TALP supports MPI, OpenMP and hardware PAPI counters. TALP can be used straightforward without any code modification nor recompilation of the code, it will automatically hook into the MPI and OpenMP calls and will use PAPI underneath to obtain cycles and instructions accounting.</p><p>TALP also provides an API to manually annotate the code, allowing to obtain the efficiency metrics for different code regions annotated by the user. One of the codes studied in the evaluation leverages this feature allowing a more detailed analysis of its performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Power monitoring</head><p>EAR Energy Aware Runtime (EAR) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> is a tool that monitors power consumption and CPU usage in MareNostrum5. It can also apply CPU frequency scaling depending on its measurements although we do not include this feature in our work.</p><p>The measurement methodology of EAR varies depending on the monitoring infrastructure provided by the hardware vendor (etc., RAPL counters, out-of-band power monitors, etc.). In the case of the general purpose nodes in MareNostrum5, EAR collects energy measurements of three components which are color-coded in Figure <ref type="figure" target="#fig_0">1</ref>: light-red each Intel CPU (a.k.a., PCK or package), light-orange each memory block (a.k.a., DRAM), and light-blue the whole node (a.k.a., DC). Measurements of PCK and DRAM are performed via RAPL counters while full-node power consumption is measured through an IPMI interface that queries a board management control unit. A daemon process (eard) communicates with the respective hardware component that performs the measurements and stores the data in a database. Even if the monitoring infrastructure in MareNostrum5 provides energy measurements, eard averages power samples over a certain win-dow of time and stores it as a timestamped series in a database. Moreover, PCK and DRAM measurements are performed on each socket independently, but the EAR database stores the aggregated measurements of both sockets.</p><p>There are three agents that request measurements to eard, each one with a different sampling rate and serving a specific purpose. The rest of this section is dedicated to explaining each one of these agents and a brief experiment to approximate the idle power of one CPU. 1. Periodic metrics -The first agent is implemented as a process that performs periodic measurements every minute and is always enabled. The set of data collected by this agent is labeled periodic metrics and it is used by system administrators to observe the health of a node throughout its lifetime. By default, this data is not available to users and it has no notion of SLURM jobs: the reader can see this as a coarse-grained free-running polling.</p><p>2. Job accounting -The second agent is triggered by a SLURM plugin that enables it at the beginning and disables it the end of a SLURM job step. These data are also measured periodically, once every minute and stored into de EAR database. Users can query the power and energy measurements of their jobs after they have ended. Data is presented either by a time series that includes each sample, or as a summary of all measurements which reports the total energy and average power consumption. We use this method to measure power consumption of scientific applications (Section 5).</p><p>3. EAR Library -The third agent is disabled by default and can be enabled by users with the use of a SLURM flag. The flag enables instrumentation by the EAR Library (EARL), which spawns a thread that performs measurements every ten seconds (6 samples/min). The data collected by this agent can be either stored in the MareNostrum5 database or into a local database managed by the user. We use this method to measure power consumption of micro-benchmarks in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Takeaways:</head><p>• PCK power includes CPU and HBM memory, if present • DRAM power includes consumption of the DDR • PCK and DRAM power aggregate both sockets</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Low-level Benchmarks</head><p>In this section we evaluate three major components of the GPP nodes in an isolated method. We start by empirically measuring the idle CPU power consumption in GPP and then measure performance using micro-benchmarks. In particular, we present the measurement of the performance of the CPU by measuring the frequency and the performance in floating-point operations per cycle achieved, we also show the bandwidth and latency of the memory subsystem and the bandwidth and latency of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Idle power</head><p>We conduct an experiment to measure the power consumption while the node is idle. Our test consists on allocating a node of MareNostrum5 and running a SLURM job with one process executing the sleep command for 60s. This puts one of the cores into sleep mode, while the other cores are considered to be idling. We repeat this experiment on multiple nodes, always enabling EARL. Since the power monitoring infrastructure exposed to users does not allow to read each CPU separately, we assume that the idle power consumption of one CPU is half of the PCK measurement reported by EAR. Similarly, we assume that the idle power consumption of the memory in one socket is half of the DRAM measurement.</p><p>EAR reports a PCK idle power of 287.39 W, DRAM of 5.39 W, and that the total node idle power is 379.40 W. The standard deviation of PCK and node power is under 6% while the DRAM is 18.59%. Takeaways:</p><p>• Idle power for one CPU is 143.70 W • Idle power for memory of one socket is 9.30 W</p><p>• Idle power for one node is 379.40 W</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Floating-point unit</head><p>Theoretical peak For any given floating point precision, let: V be the vector length of a SIMD unit, and N be the number of SIMD pipelines. Assuming a constant throughput of one Fused-Multiply-Add (FMA) instruction per cycle on each pipeline, we compute the theoretical peak performance P, in Flop/cycle as:</p><formula xml:id="formula_0">P = V × N × 2</formula><p>Table 1 lists the theoretical peak of FMA instructions in the Sapphire Rapids CPU for three ISAs and both single and double precision elements. The naming convention of the first column is: isa.datatype, where isa can be x86, avx2, or avx512; and datatype can be sp, or dp. The following experiments analyze if the CPU can sustain these theoretical performance numbers.</p><p>FPU_µKernel The FPU_µKernel is a synthetic benchmark developed at the Barcelona Supercomputing Center (BSC) to measure the sustained floating-point performance of the CPU. The source code is available upon request to the authors of this work.</p><p>The FPU_µKernel is written in C and parallelized with OpenMP. The benchmark loops over a sequence of assembly instructions and measures their performance. The type and number of floating point operations is known at compile time. Runtime parameters allow to customize the amount of iterations to perform.</p><p>Performance is calculated based on three measurements: i) cycles via PAPI (PAPI_TOT_CYC), ii) instructions via PAPI (PAPI_TOT_INS), and iii) micro-seconds via gettimeofday. On top of the raw measurements, the benchmark reports performance (Flop/cycle and Flop/s), frequency (MHz), and Instructions Per Cycle (IPC).</p><p>For parallel runs, every thread performs the same amount of work (i.e., weak scaling) and threads synchronize at the end of each iteration. Another runtime parameter allows to choose between reporting metrics per thread or summarized. We combine the performance reported by the FPU_µKernel with the measurements performed by EAR to calculate the power efficiency of each type of instruction.</p><p>Single-core performance This experiment tries to measure the single-core performance of the Sapphire Rapids CPU and compare the measurements with the theoretical peak listed in Table <ref type="table" target="#tab_1">1</ref>. In this experiment, we run the FPU_µKernel with different types of instructions and EARL is enabled. We report the PCK power consumption measured by EAR subtracting the idle power of one CPU as explained in Section 2.6. With the data gathered with EARL we can also calculate the power efficiency of each type of instruction. Table <ref type="table" target="#tab_2">2</ref> lists our results for each kernel.</p><p>Our measurements show that the CPU frequency changes depending on the type of instruction being executed. In the case of floating-point scalar and AVX2 code, frequency is consistently at 2.98 GHz. AVX512 instructions run at different frequencies depending on the precision: 2.20 GHz for avx512.sp, and 2.47 GHz for avx512.dp. Automatic frequency adjustments depending on the type of instruction are a staple feature in modern Intel CPUs [9], but the specific frequency values and which instructions trigger them depend on the micro-architecture.</p><p>We observe that the sustained Flop/Cycle of each instruction type is within 1% of the peak, meaning that the sustained performance matches the theoretical specs. If the CPU could execute AVX512 instructions at 3.00 GHz, it could theoretically achieve 94.00 GFlop/s (1.19× the real sustained performance).</p><p>Looking at the power consumption, there is no real difference between any of the instruction types. We observe an increase between 9 W and 16 W compared to the empirical idle power reported in Section 3. All of our tests report a power consumption between 152.35 W and 159.46 W, which is a gap of less than 5%. It is interesting to note that avx512 instructions, even when running at a lower frequency, consume the same amount of power compared to other instruction types.</p><p>Regarding power efficiency, our results show that avx512.sp instructions are the most efficient (0.92 GFlop/(s×W)) being 50% more efficient compared to the avx2.sp instructions. On the other hand, the avx512.dp achieve slightly lower power efficiency compared to the avx2.dp instructions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Takeaways:</head><p>• Single-core performance matches theoretical peak • There are three frequency levels that change depending on the type of instruction being executed • avx512 is more power efficient than avx2</p><p>Single-node performance This experiment analyzes the aggregated core performance and power efficiency of one node in MareNostrum5. We repeat the same executions as in the previous experiment with a full node, pinning one thread to each core, enabling EARL, and reporting the aggregated performance of all threads. For this experiment, we take the total node power reported by EAR to calculate the power efficiency in a more representative metric of how much one node consumes. Figure <ref type="figure" target="#fig_3">4</ref> shows our results. The left y-axis (solid red bars) represents performance in TFlop/s, while right y-axis (dashed gray bars) represents power efficiency. We observe that the aggregated performance is within 5% the theoretical peak even when all threads are running. For avx512.dp the sustained performance is 8.64 TFlop/s.</p><p>Looking at the power efficiency, we observe that using wider SIMD instructions provides better power efficiency. Comparing avx2.dp and avx512.dp, we see that the performance increases 1.23× at the cost of lowering the frequency 10% and consuming 14% less power.</p><p>During this experiment, we noticed that avx512 runs had at least one thread running slower than the rest. We explore this effect in the following test.</p><p>Takeaways:</p><p>• Aggregated performance of one node is 8.64 GFlop/s • Node power efficiency is 11.79 GFlop/(s×W)</p><p>Performance balance and thread pinning This experiment tries to analyze the impact of Operating System (OS) noise by trying different thread pinning options. We run the FPU_µKernel (avx512.dp) with increasing number of OpenMP threads and with three values of OMP_PROC_BIND: undefined, true, and close. All runs are done with EARL disabled. Results are shown in Figure <ref type="figure" target="#fig_4">5</ref>. Measurements with OMP_PROC_BIND=true are very similar to close and have been omitted for clarity.</p><p>The x-axis represents the number of threads. The y-axis of the top plot represents average performance per thread in GFlop/s as reported by the benchmark, with the error bars representing the standard deviation. The y-axis of the bottom plot represents the performance balance across all threads computed as the minimum performance across all threads divided by the average. This metric has values between 0 and 1, and quantifies how badly the slowest thread performs compared to the average (higher is better).</p><p>We observe that the average performance per thread consistently stays around 77 GFlop/s when using close. Moreover, the standard deviation is small, always below 5% of the average. However, the run with one thread performs remarkably low, achieving 70.44 GFlop/s. This is probably due to a clash with other processes running on the same core, since the thread has been pinned to core 0. This issue disappears when OMP_PROC_BIND is not defined, since the thread can be mapped to a different core and avoid sharing hardware resources with  other processes. When OMP_PROC_BIND is not defined, the average performance per thread drops for runs above 56 threads (one socket). Our hypothesis is that there are thread migrations across sockets, but we have not conducted any further analysis to confirm that this is the case.</p><p>Looking at the performance balance, all runs with more than one thread have a balance of 0.9 or below (i.e., the slowest thread runs 10% slower compared to the average). For runs with small standard deviations, this means that all of the variability comes from one thread. The single-thread run and OMP_PROC_BIND not defined does not suffer any performance loss. However, the performance unbalance is not present when running the benchmark with other instruction types. Our conclusion is that the OS impact comes from frequency scaling transitions since avx512 are the only instructions to have this variation. We have also confirmed that this behavior is consistent across multiple nodes.</p><p>Takeaways:</p><p>• Pinning reduces performance variability across threads</p><p>• Some system daemons are pinned to core zero</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Memory hierarchy</head><p>In this section, we analyze the memory hierarchy of the DDR5based nodes. For a comparison between DDR5 and HBM, please refer to Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theoretical peak</head><p>The micro-architecture of the Sapphire Rapids core includes three load pipelines, each one being able to serve one cache line per cycle. Similarly, there are two store pipelines. Given these hardware resources, the theoretical L1 peak throughput that the core can achieve is three and two instructions per cycle for read and write operations, respectively. The bandwidth depends on the type of instruction, if the data being accessed is cache-aligned, etc. For example, an avx2 load instruction loads 32 bytes. With all three load pipelines, the peak bandwidth of avx2 load instructions would be 96 Bytes/cycle. Assuming a constant CPU frequency of 3.00 GHz, it would be equivalent to 288 GB/s.</p><p>On the opposite end, the peak memory bandwidth that the DDR5 can provide running at 4800 MHz is 307.20 GB/s per socket, or 614.40 GB/s per node.</p><p>MEM_µKernel The MEM_µKernel is a synthetic benchmark developed at BSC to measure the sustained memory bandwidth of the CPU. It is based on the FPU_µKernel and shares with it the same code structure and measurement methodology. Source code is available upon request.</p><p>The MEM_µKernel has different kernels that perform a given amount of read and write instructions with different ISAs to contiguous memory positions that may or may not be cache aligned. In this case, the amount of bytes read and written is known at compile time. Bandwidth is reported as Byte/cycle and GB/s. We use MEM_µKernel to measure the bandwidth of the L1 cache in the Sapphire Rapids CPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L1 cache bandwidth</head><p>In this experiment we aim to measure the L1 cache bandwidth using the MEM_µKernel. We use kernels that perform only read or only write operations to the L1 cache and report the sustained bandwidth. Table <ref type="table" target="#tab_3">3</ref> lists our results.</p><p>Firstly, we note that frequency stays at 3.00 GHz regardless of the type of instruction being executed. Memory operations do not utilize floating-point functional units, so it is reasonable to conclude that they are not subject to the same power consumption limits as arithmetic operations.</p><p>Secondly, we observe that both x86 and avx2 instructions achieve an IPC of 3. This is somewhat expected, since the Sapphire Rapids CPU includes three load pipelines and control instructions such as loop conditions and jumps have a negligible impact thanks to aggressive loop unrolling. Our measurements show that such pipelines can handle from one to 32 double-precision elements per cycle.</p><p>On the other hand, the avx512 achieve an IPC of 2. This means that avx512 instructions cannot fully leverage the three Nonetheless, the sustained bandwidth of the avx512 is 1.35× better than the one of avx2.</p><p>In the case of store instructions, the scalar x86 instructions are able to leverage the two store pipelines, but avx2 and avx512 instructions do not. This is again due to the limitation of writing only one L1 cache line per cycle (64 Bytes/cycle).</p><p>Takeaways:</p><p>• Single-core sustained read bandwidth is 359.54 GB/s • Single-core sustained write bandwidth is 190.65 GB/s Single-node memory bandwidth In this experiment we use the MEM_µKernel to measure the aggregated memory bandwidth across all threads when copying data. Each thread allocates two arrays of 64 MiB, which is large enough to exceed the capacity of any cache in the hierarchy. The kernel under study is written in C and we let the compiler auto-vectorize the code. We use the optimization flag -qopt-zmm-usage=high to encourage the compiler to introduce avx512 instructions. We also run the benchmark trying two values of OMP_PROC_BIND: close and spread. Figure <ref type="figure" target="#fig_5">6</ref> shows the sustained bandwidth on the y-axis, and the number of threads on the x-axis. We observe that the close binding policy results on a curve with two steps. This is expected, since executions up to 56 threads have all threads compete for memory resources on the same socket. It is interesting to note that the sustained bandwidth within one socket flattens between 16 and 32 threads. Adding the other socket brings more memory bandwidth to the table, but it again flattens starting at 84 cores, which corresponds with 75% of the cores in one node.</p><p>On the other hand, the spread binding policy allows threads to distribute evenly across the whole node and leverage all of the available bandwidth. The curve does not have the same rough steps as with close and rises smoothly as the number of threads increases. Up until 56 threads, the spread policy achieves better bandwidth since threads can leverage both sockets. There is a window between 64 threads and the full node in which the close policy achieves higher bandwidth. We repeated the test multiple times and observed the same results. Furthermore, a close inspection of the bandwidth per thread reveals that, in the spread policy, there are some threads sustaining 25% less memory bandwidth, which translates to a lower aggregate bandwidth. Once the node is full, there is no difference between the two policies. The sustained bandwidth of a single DDR node is 386.25 GB/s, which corresponds to 62.87% of the theoretical peak.</p><p>Takeaways:</p><p>• Single-core sustained memory bandwidth is 31.50 GB/s • Single-node sustained memory bandwidth is 62.87 GB/s (62.87% of the peak)</p><p>• 84 threads saturate the memory bandwidth of one node (75% of all the cores)</p><p>Power efficiency We take the aggregated memory bandwidth measured in the previous experiment using the close policy and combine it with the power measurements taken with EARL.</p><p>Figure <ref type="figure" target="#fig_6">7</ref> shows the average DRAM and PCK power consumption when increasing the number of OpenMP threads. The left y-axis represents the power in Watts, the right y-axis represents the power efficiency in GB/(s×W), and while the labels on top of the bars indicate the percentage that the DRAM represents with respect to the whole node.</p><p>Our measurements show that, as we add more threads that perform memory operations, the impact of the DRAM on the overall power consumption of the node increases. It starts with one thread at 1.23% and goes up to 2.28% with a full-node. This is a very small portion of the total power consumption, but it highlights that the consumption of DRAM becomes more noticeable as more threads access it. The reader should note that currently, we do not have a way to measure power consumption of caches in isolation. This is included within the PCK measurement, which also increases with the number of threads.</p><p>Looking at the power efficiency, we see that it reaches a peak of 481.29 GB/(s×W) when running with 84 threads. After that The rules of the benchmark state that the size of the arrays must be at least three times the size of the last level of cache, so for MareNostrum5 we set a problem size of 83886080 elements (640 MiB per array). This version of STREAM is parallelized with OpenMP, using worksharing clauses to implement strong scaling on each kernel. We compile the code with the Intel compiler 2023.2.0 and use the following optimization flags: -O3, -ffast-math, and -march=sapphirerapids. Figure <ref type="figure" target="#fig_7">8</ref> shows the memory bandwidth reported by STREAM in two relevant HPC kernels, Copy and Triad, when running in DDR nodes. We also tried two mapping policies, with threads pinned to cores. Compared to the results of the MEM_µKernel, STREAM reports lower bandwidth with small pools of cores (i.e., up to 32 threads), but achieves higher bandwidth with high core counts. We do not have an explanation for this discrepancies but we highlight two differences found by analyzing the binaries of each benchmark: i) the compiler introduces software prefetching instructions in STREAM, but not in the MEM_µKernel; ii) STREAM opens and closes OpenMP parallel regions at each kernel execution, while MEM_µKernel does it only once.</p><p>Looking at the shape of the bandwidth curves, we observe that STREAM shows the same trend as the MEM_µKernel: the spread mapping policy achieves better memory bandwidth when the node is not full. This is expected since threads are evenly distributed across the two sockets of the node, balancing the memory requests to different memory controllers.</p><p>At best, our tests achieve 66.66% of the theoretical peak with DDR nodes. This result is comparable to measurements in older Intel-based CPUs such as Skylake <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b10">12]</ref>. The case of the Sapphire Rapids CPU is also discussed by McCalpin, the author of the STREAM benchmark, in <ref type="bibr" target="#b11">[13]</ref>.</p><p>memlat memlat is a synthetic benchmark developed at BSC to measure the single-core memory latency at different levels of the cache hierarchy. This code performs a pointer chase through an array of a given size, N. Each element of the array points to another element which is separated by a stride, S. The benchmark performs a number of warm-up traversals w and then runs for another t traversals, which are timed. N, S, w, and t are runtime parameters. The array allocation is aligned to a 4 KiB page. The code reports the measured average memory latency per memory access in micro-seconds and we convert this measurement to cycles assuming that the CPU frequency is 3.00 GHz.</p><p>Latency across the hierarchy Figure <ref type="figure" target="#fig_8">9</ref> shows the measured latency with buffer of increasing size. Each line corresponds to a different stride S (64-bit elements). The y-axis corresponds to latency in cycles. Both axes are in logarithmic scale. The plot is divided into four regions which correspond to each cache level and the main memory. The image also includes the range of cycles measured for each region.</p><p>Accesses to L1 cache take 5 cycles consistently. Accesses to L2 range from 5 to 8 cycles; and in L3 range from 11 to 158. In the case of L2 and L3 accesses, the latency depends on the stride size. The reader should consider that the effective latency for any given buffer size is within the range of the measured values for different values of S. For small strides, the cache pre-fetcher is able to recognize the access pattern and reduce the measured latency. On the contrary, bigger strides approximate better the actual memory latency. Measurements in the L3 and beyond have higher variability and a wider range of latency because they are memory resources outside the core itself and shared across cores. Takeaways:</p><p>• Latency to the L1 cache is within 5 cycles</p><p>• Hardware prefetching makes it difficult to accurately measure latency to higher cache levels</p><p>• Depending on the amount of prefetching, accessing the L3 cache is between 2× and 32× slower than L1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Network bandwidth and latency</head><p>Network topology and theoretical peak In this section, we classify pairs of nodes in four categories:</p><p>1. Same tray -Messages reach the network controller, but do not leave the tray, so they are not limited by Infiniband.</p><p>2. Within the same rack -Messages go through level-one network switches, which are shared between 72 nodes.</p><p>3. Same island -Messages go through level-two network switches, which are shared between 30 racks (2160 nodes). 4. Different islands -Messages go through level-three network switches, which there are nine in total and connect the whole general purpose partition of MareNostrum5. Please refer to Section 3.3 for a detailed description of the network connectivity in MareNostrum5. The theoretical peak bandwidth of the Infiniband NDR200 network fabric is 25 GB/s and the theoretical latency is under 5 us. Pairs of nodes that share a tray may achieve better network speeds, but we could not find information on which is the theoretical peak.</p><p>OSU benchmarks We use the OSU benchmarks 7.4 [14] to evaluate the network capabilities of MareNostrum5. This is a benchmark suite that includes codes for different parallel programming paradigms and languages. In this work, we focus on two of the simplest benchmarks, which are osu_bw and osu_latency. Both codes are written in C and parallelized with MPI. They perform multiple point-to-point messages between two processes and report the sustained bandwidth and latency, respectively.</p><p>Network bandwidth In this experiment, we run osu_bw with OpenMPI 4.1.5 having pairs of MPI ranks mapped to different locations of the general purpose partition. Processes are always in different nodes, but the distance between nodes varies. Figure <ref type="figure" target="#fig_9">10</ref> shows the bandwidth measured with osu_bw with increasing message sizes. The bandwidth reported by the benchmark increases with the message size. We observe a breaking point around messages of 4 KiB, which corresponds to a transition in the communication protocol (rendezvous to eager). For messages of 64 KiB and bigger, pairs of processes are able to match the theoretical peak of the Infiniband NDR200 (25 GB/s) and there is no meaningful difference between all but one of the lines. In the case of processes within the same tray, our measurements show a greater bandwidth that reaches up to 29.83 GB/s. As stated previously, this is due to the fact that these type of pairs do not require the Infiniband fabric to communicate.</p><p>Weak links In this experiment, we try to identify pairs of nodes which cannot sustain 25 GB/s. We call these pairs "weak links". To do so, we repeat the osu_bw experiment with multiple pairs of nodes in MareNostrum5. Figure <ref type="figure" target="#fig_10">11</ref> shows a heatmap in which columns and rows represent nodes and each cell represents the network bandwidth achieved by a given pair of nodes. Cells are color-coded and contain the value in GB/s reported by osu_bw.</p><p>The reader should note that nodes in Figure <ref type="figure" target="#fig_10">11</ref> are not consecutive. Given the amount of nodes in MareNostrum5, it is unfeasible to check every possible pair, so we test only on a subset of 15 nodes, all of which are from the same island. In line with the results from our previous experiment, pairs of nodes within the same tray reach up to 30 GB/s, while other pairs achieve around 25 GB/s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network latency</head><p>In this experiment, we run osu_latency with the same methodology as with osu_bw. This code measures the time that MPI messages take to go from one process to another, not the actual latency of the communication fabric. Figure <ref type="figure" target="#fig_11">12</ref> shows the latency reported by osu_latency when increasing the message size. Processes are mapped to nodes in the same rack, but different trays.</p><p>We observe a minimum message latency of 1.50 us when the message size is under 16 B. With such a small message, we can assume that this latency corresponds to the physical transport layer plus the overhead introduced by the MPI implementation. As the message size increases, the latency reported by osu_latency increases, since it requires exchanging more packets of data through the network. In this experiment, we do not include messages bigger than 4 KiB because at that point, the communication will be bound by the bandwidth of the network, which we have already discussed in the previous experiments. • Network latency for small messages is 1.50 us</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">HPC Benchmarks</head><p>In this section we present the sustained performance of two major HPC benchmarks that are used to rank supercomputers in the Top500 list, HPL and HPCG. Furthermore, we present the power consumption and efficiency of HPL. We include single and multiple node results of both GPP and ACC partitions. The reader should note that multi-node runs may not use all the available nodes in MareNostrum5 because we present the results of runs with a parameter configuration (e.g., N, P, Q) that yields the highest performance. Moreover, we report the performance of widely used AI benchmarks (HPL-MxP, Graph500, and DL-BENCH) when running in the ACC partition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">HPL</head><p>The following paragraphs describe how we run HPL in Mare-Nostrum5 and Table <ref type="table" target="#tab_4">4</ref> summarizes the performance and power efficiency results. GPP In the case of the GPP partition, the HPL binary was provided by Intel and includes MKL optimizations. The execution maps 2 MPI processes per node (i.e., one process per socket) with 56 OpenMP threads per process. The execution with the full partition earned the spot #22 in the Top500 list of June 2024 <ref type="bibr" target="#b0">[1]</ref> reaching 40.10 TFlop/s which accounts for 92.19% of the peak. ACC In the case of the ACC partition, the HPL binary was provided by Nvidia through Nvidia NGC as a Singularity image. The execution maps 4 MPI processes per node (i.e., one process per GPU) limiting the number of threads per GPU to 20. The execution with the full partition earned the spot #8 in the Top500 list of June 2024 <ref type="bibr" target="#b1">[2]</ref> and the spot #15 in the Green500. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">HPCG</head><p>The following paragraphs describe how we run HPCG in Mare-Nostrum5 and Table <ref type="table" target="#tab_5">5</ref> summarizes the performance results.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">HPL-MxP</head><p>HPL-MxP is an HPC benchmark that is based on the traditional HPL benchmark but it includes mixed precision algorithms. In recent years, this HPL-MxP has become an important benchmark to evaluate performance of HPC systems that target AI workloads.</p><p>We run a binary HPL-MxP provided by Nvidia in the ACC partition. The execution maps 4 MPI processes per node and expands across 1080 nodes, achieving 1.836 EFlop/s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Graph500</head><p>Graph500 is a benchmark that aims to complement traditional HPC benchmarks such as HPL and HPCG by introducing data intensive kernels that are used in graph-related codes.</p><p>A BFS run with 1024 ACC nodes, mapping 4 tasks per node and setting the scale to 35 scores a performance of 15737.43 GTEPS. Similar to the Top500, there is also a Graph500 list. The ACC partition of MareNostrum5 has the earned the spot #9 on the November 2024 BFS list <ref type="bibr" target="#b12">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">DL-BENCH</head><p>DL-BENCH consists of one single PyTorch code which is called pytorch_imagenet_resnet50.py, extracted directly from the horovod repository 5 . It runs a distributed training across the tested system, using the popular resnet50 neural network to train on the Imagenet dataset, for 90 Epochs (-epochs 90). Performance is reported as the elapsed time to complete the training.</p><p>We use this benchmark to evaluate the performance at a smaller scale, with runs in the ACC partition using 16 and 78 nodes. Table <ref type="table" target="#tab_6">6</ref> shows the results of DL-BENCH. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">HPC Applications Scalability</head><p>After measuring the capabilities of the hardware resources of the cluster in isolation, and the sustained performance achieved by standard HPC benchmarks, in this section, we will evaluate the scalability of three scientific applications. The scientific applications chosen cover computational fluid dynamics (CFD) and numerical modeling or weather and climate prediction. These two scientific domains represent a significant part of the computational resources of datacenters worldwide and have a recognized research impact <ref type="bibr" target="#b13">[16]</ref><ref type="bibr" target="#b14">[17]</ref><ref type="bibr" target="#b15">[18]</ref>. All three applications are studied following the same methodology, which is described in Section 5.1. For a full breakdown of the software environment and compiler configuration used for each application, contact the corresponding author of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Tools and methodology</head><p>The methodology used to evaluate each one of the applications will be the same one and consists of three steps:</p><p>1. Scalability: Measure elapsed time and throughput (note that for each application, a different throughput metric is defined, it should be meaningful for users and developers of the code) when scaling to an increasing number of nodes.</p><p>2. Efficiency metrics: Obtain efficiency metrics for the scaling runs. The efficiency metrics used are formally described in Appendix A and are based on the POP methodology <ref type="bibr" target="#b16">[19]</ref>. 5 <ref type="url" target="https://github.com/horovod/horovod/blob/v0.18.2/examples/">https://github.com/horovod/horovod/blob/v0.18.2/ examples/</ref> 3. Energy consumption: Gather the energy consumption and the Energy-Delay-Product for the scaling runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Focus of Analysis</head><p>The region under study of each application is called focus of analysis. All of the codes analyzed in this section have a similar execution structure with an initialization phase, an iterative execution phase, and a finalization phase. Generally, the focus of analysis excludes the initialization and finalization phase. It may also exclude some of the iterations of the execution phase.</p><p>The focus of analysis of each application studied in this work is explained at the beginning of its corresponding section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Efficiency metrics model</head><p>The Efficiency metrics model defines a collection of metrics arranged in a hierarchy. Figure <ref type="figure" target="#fig_13">13</ref> shows a schematic breakdown of the metrics displayed as a tree. For all metrics, higher values are better. Moreover, all metrics with the exception of the ones labeled Scalability take values between zero and one. Energy consumption Let W be the power consumption of a whole node, in Watts, averaged across all nodes N and throughout the whole execution. Let T be the elapsed time of the region under study of each application in seconds. We compute the total energy consumption E, in kW h, as E = W × N × T /3600. And the Energy-Delay-Product EDP, in kW h 2 , as EDP = E × T .</p><p>This definition of E and EDP combines measurements in two overlapping, but not equal, windows of time. The average power W takes into account the whole duration of a job, while the time T takes into account only our focus of analysis. For jobs in which there is a lot of time outside the focus of analysis in which the power consumption behaves completely different than inside, our definition may not accurately represent the energy consumption of the application. Nonetheless, we try to make the focus of analysis as big as possible, so that the effect of other node activity in the averaged power consumption is negligible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Alya</head><p>Introduction Alya <ref type="bibr" target="#b17">[20]</ref> is a high performance computational mechanics code to solve complex coupled multi-physics prob-lems, discretized with Finite Element methods. This software has been developed by the CASE Department of the Barcelona Supercomputing Center. It is a modular scientific code written in Fortran and parallelized with MPI that is divided into multiple modules. The particular software modules that are active during execution vary depending on the physics involved in the simulation. In this work, we use the simulation of incompressible flows (nastin) and particle transport (partis).</p><p>Input The presented problem solves the simulation of airflow through the respiratory system extending up to approximately 17 bronchial generations and the transport of particles using a Lagrangian one-way coupling strategy. The simulated steady airflow corresponds to a constant flow rate of 30 L/min passing through the inlet boundary. The initial mesh consists of approximately 45M cells.</p><p>To enhance simulation accuracy and increase the problem size, the mesh can be refined. Each refinement step subdivides the mesh by introducing intermediate nodes along the edges, resulting in an eightfold increase in the number of elements. A single subdivision produces a mesh with roughly 362M elements, while two subdivisions yield a mesh with approximately 2.90B elements. This mechanism is applied during the initialization phase of Alya and allows us to obtain a problem size that can scale to a higher number of nodes.</p><p>For the particle transport simulation, 550K particles, each with a diameter of 120 nm (representing the size of the SARS-CoV-2 virus), are injected at the start of the simulation (t = 0 s) and transported until they reach the walls, where they are considered deposited. The number of particles remains constant regardless of mesh refinement. In this work, we present results for both one and two levels of subdivision, demonstrating the performance of Alya at these two distinct scales.</p><p>Methodology We compiled two builds of Alya: i) performance, to measure timing and energy metrics; and ii) instrumented, to gather hardware counters and other runtime information for building the efficiency metrics model.</p><p>Regardless of the build type, the simulation runs for 100 timesteps and we repeat each run three times. The focus of analysis encompasses all the timesteps. All the results presented in this section show the measurements from the best run for a given node count.</p><p>Scalability Figure <ref type="figure" target="#fig_14">14</ref> shows the scalability of Alya with the 362M input when increasing the number of MareNostrum5 nodes. The plot at the top represents elapsed time and the plot at the bottom represents throughput measured in thousand cells per second (kCells/s) and computed as the number of simulated cells (362M) divided by the elapsed time divided by 1000. Both plots include a dotted line which represents the ideal scalability with respect to the baseline run (8 nodes). The reader should note that axes are in logarithmic scale.</p><p>We observe a scalability that closely follows the ideal line, with some points slightly above, when running between 10 and 100 nodes. This is because the work is distributed across more nodes, giving more memory resources and improving cache locality of each MPI rank. The positive effects of this partitioning are present when running with higher node counts, but have diminishing returns as throughput deviates from the ideal starting at 96 nodes. The throughput curve flattens completely from 300 nodes onward.</p><p>Figure <ref type="figure" target="#fig_15">15</ref> shows the same scalability plots but running with the input with 2.90B cells. In this case, the baseline run is with 200 nodes, since it is the minimum number of nodes required to run due to memory size constraints. The cache locality effects also contribute to the scalability of the 2.90B input, although there are other limiting factors that we will now discuss. <ref type="bibr">Figure 18</ref> shows the efficiency metrics for both problem sizes. The leftmost plot shows the metrics at the top level of the model; the plot on the center shows the metrics underneath Parallel Efficiency; and the plot on the right shows the metrics underneath Computational Efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Efficiency metrics</head><p>The Parallel Efficiency is always under 0.75 and decreases as the node count increases. This is due to the decrease of both Communication Efficiency and Load Balance. Of the two metrics, the Communication Efficiency is the one that drops de most for the 362M input; starting at 0.96 with eight nodes and decreasing to 0.58 with 120 nodes.</p><p>The Load Balance is the main limiting factor since it is always the metric with the lowest value. The cause of the imbalance is not discussed in this work and could be for a variety of reasons which include: i) uneven workload distribution, in which some ranks have to do more work; ii) imbalance of hardware resource usage, in which all ranks do similar amount of work, but some ranks perform it faster due to higher IPC.</p><p>We also observe a Computational Scalability which is always above 1. This is mainly due to a previously discussed fact: higher node counts yield better memory locality, achieving better IPC. The Frequency Scalability also stays high with values in the range of 0.9 to 1.0. On the other hand, the Instruction Scalability drops under 0.9 starting at 80 nodes for the 362M input. <ref type="bibr">Figures 16 and 17</ref> show the total energy consumption and energy-delay-product of Alya when running with each of the problem sizes. The x-axis represents the number of nodes and some runs are omitted for clarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Energy consumption</head><p>In the case of the input with 362M cells, we observe a linear increase in energy consumption as we increase the number of nodes. Also, we observe that the EDP follows a U-shape. By comparing these plots with the ones in Figures <ref type="figure" target="#fig_14">14</ref> and <ref type="figure" target="#fig_15">15</ref>, we make two observations. First, the energy consumption stays flat when the application scales close to the ideal (2.90B cells in Figure <ref type="figure" target="#fig_17">17</ref>), but it increases close to linearly with the number of nodes when the application does not scale (362M cells with more than 100 nodes in Figure <ref type="figure" target="#fig_16">16</ref>).</p><p>Secondly, the EDP follows a U-shape: When the application scales close to ideal, the EDP decreases as we increase nodes. The absolute minimum of the EDP curve corresponds to the point in which the performance increase gained by adding more hardware resources does not compensate for the energy consumption cost. This breaking point can be seen in Figure <ref type="figure" target="#fig_16">16</ref> and happens around 160 nodes, which is when the throughput curve in Figure <ref type="figure" target="#fig_14">14</ref> deviates from the ideal and the Global Efficiency is below 0.35. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">OpenFOAM</head><p>Introduction OpenFOAM <ref type="bibr" target="#b18">[21]</ref> is a widely used free and opensource Computational Fluid Dynamics (CFD) software that provides an extensive range of features for solving complex fluid flow problems. The version used in this analysis, OpenFOAM v2312, includes a new coherent file format for parallel I/O of unstructured polyhedral meshes <ref type="bibr" target="#b19">[22]</ref>, which optimizes data access and reduces pre-and post-processing times. This approach improves scalability and performance on large HPC systems, making OpenFOAM more efficient for handling massive datasets and multi-node executions. This development is part of the work driven by the exaFOAM project <ref type="bibr" target="#b20">[23]</ref>, aimed at enhancing OpenFOAM's scalability on modern HPC architectures, with these improvements being validated using small-scale microbenchmarks, followed by benchmarks, and large-scale HPC Grand Challenges, demonstrating its impact across a range of scales and applications.</p><p>Input The OpenFOAM case selected is the exaFOAM Grand Challenge test case of the High Lift Common Research Model (CRM-HL) <ref type="bibr" target="#b21">[24]</ref>, a full aircraft configuration with deployed highlift devices using wall-modelled LES (WMLES). This case simulates the airflow around an aircraft in a landing configuration.</p><p>The geometry includes inboard and outboard leading-edge slats, single-slotted flaps, and simplified flow-through engine nacelles, adding geometric complexity and meshing challenges. The mesh used for this case consists of 1.79 billion cells.</p><p>Methodology The simulation is executed for 200 timesteps, with the first timestep excluded from the measurements as it involves initialization processes. Each run is repeated five times to ensure reproducibility, and results from the best run on a given node count are presented.</p><p>Scalability Figure <ref type="figure" target="#fig_19">19</ref> shows the scalability of OpenFOAM as the number of MareNostrum5 nodes increases. The top plot represents the elapsed time, while the bottom plot shows the throughput, measured in Finite Volumes solved Per Second (FVOPS) <ref type="bibr" target="#b22">[25]</ref>. FVOPS is calculated based on the number of cells in the grid processed during the time taken for a single timestep (in seconds), offering a measure of the system's efficiency for each individual computational step. Both plots include a dotted line representing the ideal scalability with respect to the baseline run (64 nodes).</p><p>As the number of nodes increases, the execution time decreases, showing significant performance improvements, especially between 64 and 540 nodes. However, beyond 540 nodes, the  reduction in execution time becomes less pronounced. This trend is clearer in the throughput plot, where the measured throughput in FVOPS closely follows the ideal scalability up to 540 nodes. However, at 1080 nodes, the actual throughput begins to diverge from the ideal, and at 2160 nodes the actual FVOPS is approximately half of the ideal value. <ref type="bibr">Figure 20</ref> presents the efficiency metrics for OpenFOAM for the Grand Challenge case. The leftmost plot shows the top-level metrics, the center plot focuses on the metrics under Parallel Efficiency, and the rightmost plot presents the metrics under Computation Efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Efficiency metrics</head><p>The Parallel Efficiency of OpenFOAM decreases significantly as the number of nodes increases, starting at 0.74 with 64 nodes and dropping to 0.17 with 2160 nodes. This decline is primarily due to the reduction in Communication Efficiency, which falls from 0.95 at 64 nodes to 0.35 at 2160 nodes. As more nodes are added, the ratio of inner cells to boundary faces decreases, meaning that processors spend more time exchanging data rather than performing computation. This increased communication overhead between processes leads to a decrease in Communication Efficiency, causing an imbalance where the time spent managing communications outweighs the time spent computing.</p><p>Load Balance experiences an initial drop as the number of nodes increases, starting at 0.78 with 64 nodes and falling to 0.60 with 540 and 1080 nodes. It then drops further to 0.48 with 2160 nodes. This decline in Load Balance can be attributed to the uneven distribution of computational work among the processing nodes. Adding more nodes, compute-intensive regions of the simulation become sparsely distributed and separated from regions that require less computational effort and converge more quickly, resulting in some processors handling significantly more workload than others.</p><p>On the other hand, Computation Scalability increases with the number of nodes, peaking at 1.83 with 1080 nodes. This high efficiency is attributed to the good IPC Scalability of the case. IPC increases with more nodes, indicating that with fewer cells per core, memory bandwidth becomes less of a bottleneck, resulting in better memory locality. This observation aligns with findings from prior works <ref type="bibr" target="#b23">[26]</ref>, which identified that the region under study in OpenFOAM is memory bandwidth bound.</p><p>The superlinear Computation Scalability compensates the low Communication Efficiency up to 1000 nodes. Beyond this point, the limited Instruction Scalability outweighs the increase in IPC, making it so the Global Efficiency drops.</p><p>Energy consumption Figure <ref type="figure" target="#fig_21">21</ref> presents the total energy consumption and energy-delay-product of OpenFOAM for all executions. We observe that, up to 1080 nodes, energy usage remains stable, fluctuating slightly but staying within a narrow range. At 2160 nodes, there is a significant increase in energy consumption. The EDP curve decreases as the number of nodes increases, reaching its lowest point at 1080 nodes. Beyond this point, the EDP rises again at 2160 nodes.</p><p>OpenFOAM shows a constant energy consumption and a decreasing EDP up to 1080 nodes, where it scales efficiently. However, at 2160 nodes, both energy consumption and EDP increase, matching the flattening of the curve observed in Figure <ref type="figure" target="#fig_19">19</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">IFS Introduction</head><p>The Integrated Forecasting System (IFS) <ref type="bibr" target="#b24">[27]</ref> atmospheric model is an operational global meteorological forecasting model developed by the European Centre for Medium-Range Weather Forecasts (ECMWF). The dynamical core of IFS is hydrostatic, two-time-level, semi-implicit, semi-Lagrangian and applies spectral transformations between grid-point and spectral space. This model is used for daily weather predictions by ECMWF, and its results serve as the foundation for forecasts issued by major meteorological agencies across Europe.</p><p>IFS can also be employed for climate studies when combined with simulations of other components, such as the ocean. These studies require high-resolution global simulations, which involve significant computational costs and would be impossible without supercomputers.</p><p>The model is adapted for HPC environments, written in Fortran, and optimized for parallel processing through a hybrid combination of MPI and OpenMP. The version used in this work (CY48R1) is also currently used in a production environment.</p><p>Input The configuration set for the tests is used for climate prediction, starting from the year 2020 onward. This configuration is based on real observational forcing data, including variables such as wind and CO2 emissions from 1950, to achieve a stable and realistic setup for predictions from 2020 onward.</p><p>For the results presented in this paper, a 10-day simulation was performed with a horizontal resolution of 4 km and 137 vertical levels. The NPROMA (block processing size) used for the evaluation runs was set to 16.</p><p>Methodology We compile IFS with the ifs bundle from ECMWF. The model runs for 10 simulated days and we repeat each run three times. The time (T IFS , in seconds) of each run is measured by aggregating the execution time of each timestep as reported by IFS (removing the first 3 timesteps, which contain initialization). To compute the throughput (SDPD, Simulated Days Per Day), we divide the simulated time in days (9.992188 days, after removing 3 timesteps) by the execution time in days.  In this Figure we can observe that when running in 50, 100, 200 and 400 nodes the best combination of MPI-OpenMP is to use 7 or 14 OpenMP threads for each MPI process. However, when running in 800 nodes, using 28 OpenMP threads per MPI ranks is the best option. We can conclude that having less OpenMP threads is better for lower amounts of nodes. However, we see that for the higher amounts of nodes trading processes for threads increases the throughput. We observe that the MPI scalability stalls at around 10k MPI ranks, and only then its useful to increase the amount of OpenMP threads.</p><p>Efficiency metrics We have measured the IFS efficiency metrics only for one MPI-OpenMP combination (8 MPI x 14 OpenMP) <ref type="foot" target="#foot_3">6</ref> . Figure <ref type="figure" target="#fig_24">23</ref> shows the efficiency metrics, and for this application we include the OpenMP efficiencies. From top to bottom, Figure <ref type="figure" target="#fig_24">23</ref> is read as follows: em i) parent metrics of all the other plots, with the addition of the Hybrid Parallel Efficiency and Global Efficiency; ii) breakdown of the Computation Scalability; iii) breakdown of the MPI Parallel Efficency; and iv) breakdown of the OpenMP Parallel Efficiency.</p><p>Looking at the top chart we can see that IFS have a good Computation Scalability and MPI efficiency, being both metrics above 0.80 for all the cases. On the other hand, the global efficiency and the hybrid parallel efficiency are very low, with values always below 0.80 and reaching 0.30 when running in 800 nodes. These low values are due to the lo OpenMP parallel efficiency which is the metric dragging the other ones.</p><p>Looking at the Computational Scalability metrics (red lines) we can see that the IPC and frequency scalability show very good scaling values always above 0.90. Only the Instruction Scalability is the worse one but still showing very good values always above 0.80, this metric measures the amount of replicated code and is not surprising to see it decrease with this large number of core counts, usually meaning that the amount of work assigned to each MPI rank is getting smaller and the computational of boundaries or replicated code is becoming relevant. For the MPI metrics (blue lines) we can observe that MPI Serialization Efficiency and MPI Load Balance Efficiency both have nearly the same impact on their parent metric. However, as mentioned in Appendix A, the inefficiencies introduced by MPI fall into OpenMP metrics in the Hybrid model (i.e.,OpenMP Serial Efficiency). Knowing the nature of the earth models load imbalance (the load distribution imbalance is not constant across timesteps due to changes in solar radiation and atmospheric phenomena such as precipitation) <ref type="bibr" target="#b25">[28]</ref>, these kind of irregular imbalances are accounted under MPI Serialization Efficiency. So we can conclude that the main issue when scaling MPI is the load imbalance, either from a global point of view or because of micro-imbalances. When we analyze the OpenMP metrics (green lines) we can see that the OpenMP scheduling efficiency and OpenMP Load Balance are almost perfect, and the low OpenMP efficiency is due to OpenMP Serialization. OpenMP serialization is a metric that indicates the amount of code parallelized with OpenMP, in this case we can assume that the OpenMP parallelization of the code is not enough, this is also backed up by the scaling data that we have seen before, were it was better to use less OpenMP threads and more MPI ranks.</p><p>Moreover, another factor that impacts the serialization metric is MPI blocking calls that are not overlapped with parallel OpenMP work, this is also considered OpenMP Serialization, for this reason we see a decrease of the OpenMP Serialization metric when increasing the number of nodes, even though the number of OpenMP threads per process is constant.</p><p>Combining both the results from this metrics and the scalability we can tell that, overall, IFS performance is driven by time spent in MPI calls and low OpenMP parallelization. <ref type="bibr">Figure 24</ref> show the total energy consumption of IFS for the different MPI-OpenMP combinations.  Figure <ref type="figure" target="#fig_26">25</ref> shows the energy-delay-product of IFS for the different MPI-OpenMP combinations. The 7 OpenMP threads executions draw a U-shape, with its minimum at 400 nodes. This plot also shows that when looking for the best energy-throughput tradeoff it makes more sense to run with lower number of OpenMP threads, for example at 400 nodes 7 OpenMP threads shows better EDP compared to 14 OpenMP threads, even through the 14 OpenMP threads run was faster. Its with 800 nodes that it starts to make sense energetically to use more OpenMP threads. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Energy consumption</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Memory Technology Evaluation</head><p>In this section we evaluate the HBM nodes of the general purpose partition in MareNostrum5. Although they share the same core micro-architecture and core count with the DDR nodes, these nodes include 8 dies of 16 GB of HBM2 memory, adding up to 64 GB. HBM nodes have a peak memory bandwidth of 3.28 TB/s, which is 5.33× higher than the peak of a DDR node.</p><p>In addition to the sustained bandwidth, we try to measure the power efficiency of the HBM nodes. Unfortunately, the power monitoring infrastructure does not provide a mechanism to measure the consumption of HBM in isolation. Due to this limitation, we only measure the power consumption of the whole node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Low-level Benchmarks MEM_µKernel</head><p>We repeat the same experiment as in Section 3.2 in which we run the MEM_µKernel with a code in which each OpenMP thread copies and array of 64 MiB from one memory location to another. We also run with two binding policies, close and spread. Figure <ref type="figure" target="#fig_27">26</ref> shows the sustained bandwidth measured with MEM_µKernel comparing the DDR and HBM nodes, and the two binding policies. Our measurements show that the close policy in the HBM nodes behaves similarly compared to the DDR nodes. Both curves (blue and green) have two steps which correspond to the bandwdith saturation of the first and second socket, respectively. On the other hand, the spread policy is somewhat different in the HBM nodes. It is similar to the one in DDR nodes in that it has a single smooth step. However, the sustained bandwidth is always higher than the close policy. This was not the case when running in the DDR nodes between 64 and 112 OpenMP threads.</p><p>Looking at the maximum aggregated bandwidth, we observe that the HBM nodes achieve 1.35 TB/s, which is 3.5× more memory bandwidth than in the DDR nodes. Nonetheless, this bandwidth is very far from the theoretical peak, achieving only 39.40%. Our experiment shows that the use of HBM memory is beneficial for a memory bound code in raw bandwidth numbers, the Sapphire Rapids CPU cannot fully leverage all the bandwidth that the HBM can provide.</p><p>Power Efficiency Figure <ref type="figure" target="#fig_28">27</ref> shows the node power consumption and efficiency of DDR and HBM nodes when running the MEM_µKernel with a full node (112 OpenMP threads). The x-axis represents the number of threads, the left y-axis represents the node power consumption, and the right y-axis represents the power efficiency. The reader should note that the x-axis is neither linear nor logarithmic scale. Our measurements show that the HBM nodes consume around the same power compared to the DDR nodes. Runs with small core counts (a.k.a., up to 32 threads) consume more in the HBM nodes, probably because the idle power consumption is higher. Considering a similar power consumption but a 3.5× increase in memory bandwidth, the HBM nodes achieve a much higher power efficiency than the DDR counterparts.</p><p>STREAM We repeat the executions of STREAM in the HBM nodes to compare their performance with the DDR nodes (see Section 3.2). Figure <ref type="figure" target="#fig_29">28</ref> compares the memory bandwidth achieved by the Triad kernel when using the DDR and HBM nodes. In line with our results with the MEM_µKernel, the sustained bandwidth when using HBM memory is much higher compared to DDR. At best, our measurements show an 3.63× improvement of HBM over DDR5. However, it is also important to note that the bandwidth achieved in the HBM configuration is very far from its theoretical peak. It only reaches up to 44.49% of the peak. This is due to the limitation on the CPU to leverage the huge bandwidth that the HBM memory offers and has also been discussed by McCalpin <ref type="bibr" target="#b11">[13]</ref>.</p><p>Takeaways:</p><p>• HBM nodes provide up to 3.63× more memory bandwidth than DDR nodes</p><p>• HBM nodes have a similar power consumption than DDR nodes, which means that they provied higher power efficiency</p><p>• The Sapphire Rapids can only leverage up to 44.49% of the peak HBM2 memory bandwidth</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Alya</head><p>In this section, we compare the execution of Alya in one node of MareNostrum5 with and without HBM memory. Due to memory constraints, we run a different input than presented in Section 5.2.</p><p>For the DDR and HBM comparison, the input simulates a synthetic sphere mesh of 16M cells. We run the simulation for 750 timesteps and measure the total execution time as well as the energy and EDP using the same methodology explained in Section 5.1.</p><p>Figure <ref type="figure" target="#fig_30">29</ref> shows the measurements of the HBM node normalized to the ones in DDR. Values under one mean that the respective metric is lower in the DDR nodes compared to HBM. For speedup, higher is better; while for energy and EDP, lower is better. Moreover, each point in the curve of EDP is mathematically equivalent to the product of the points in the speedup and energy curves. The results show that for up to 32 cores, the HBM node runs slightly slower, with speedup values below one. Beyond 32 cores, the speedup increases, peaking at 1.31× with a full node.</p><p>At core counts lower than 32, the HBM node consumes less energy compared to the DDR node. However, the difference between the two node types decreases as the core count increases. With 112 cores, energy consumption in the HBM configuration is 1.52× more than the DDR configuration.</p><p>The EDP follows a similar pattern: it is lowest at lower core counts in the HBM node, indicating better energy delay performance. However, as the number of cores scales, the EDP increases significantly, reaching a peak of 1.99× at 112 cores, double the one of DDR nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">OpenFOAM</head><p>This section compares the performance of OpenFOAM on a single node of MareNostrum5 when using DDR and HBM memory.</p><p>For this experiment, we use a microbenchmark representative of the use case used in Section 5.3, referred as MB9 in the work by Galeazzo et.al <ref type="bibr" target="#b26">[29]</ref>. This case, referred to as the High-Lift Configuration, is a compressible Large Eddy Simulation (LES) benchmark simulating the aerodynamic behavior of a wing with complex flow patterns. The mesh for this case consists of 19.5 million cells.</p><p>The simulation is executed for different number of cores inside a node, running 20 timesteps for each configuration and discarding the first timestep to exclude initialization processes.</p><p>We report the speedup in execution time, energy consumption, and EDP for the HBM node, normalized to the DDR configuration. Figure <ref type="figure" target="#fig_31">30</ref> shows these metrics, where values below one indicate that the respective metric is lower on the DDR nodes compared to the HBM nodes.</p><p>The results show that for up to 16 cores, the HBM node runs slightly slower, with speedup values below one. Beyond 16 cores, the speedup increases significantly, peaking at 1.95× at 112 cores, nearly doubling the performance.</p><p>At lower core counts, the HBM node consumes more energy. However, the difference between the two node types decreases as the core count increases. With 112 cores, energy consumption in the HBM configuration is approximately 2.15× more than the DDR configuration. The EDP follows a similar pattern: it is lowest at lower core counts in the HBM node, indicating better energy delay performance. However, as the number of cores scales, the EDP increases significantly, reaching a peak of 4.19× at 112 cores.</p><p>These results indicate that, although HBM gives better speedup for OpenFOAM at higher levels of parallelism, it also comes with increased energy consumption and EDP than DDR nodes.</p><p>Takeaways:</p><p>• HBM nodes achieve 1.31× and 1.95× speedup for Alya and OpenFOAM, respectively, with respect to the same core counts in DDR nodes</p><p>• The improved speedup in HBM nodes does not compensate for the increase in energy consumption, yielding an increase of EDP of up to 4.19× compared to DDR</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>This document provides a detailed evaluation of MareNostrum5, a pre-exascale supercomputer hosted at the Barcelona Supercomputing Center. The analysis encompasses the system's architectural features, benchmarking at multiple levels, and the performance of key scientific applications. The document aims to complement traditional technical documentation by offering insights into Ma-reNostrum5 's behavior and capabilities, thus assisting users in optimizing their workflows. The micro-benchmarking results reveal that MareNostrum5 achieves performance near theoretical peaks for many low-level metrics. Floating-point performance tests confirm that Intel Sapphire Rapids CPUs in the General Purpose Partition (GPP) sustain at least 99.7% of their maximum performance. Memory bandwidth evaluations show that DDR5 configurations achieve up to 62.87% of their theoretical peak, a result consistent with similar architectures. The interconnect tests demonstrate a bandwidth of 25 GB/s for packages greater than 64 KiB and latency of 1.5 us.</p><p>Application benchmarks highlight the practical capabilities and challenges of using MareNostrum5 for scientific workloads. Alya scales efficiently up to moderate node counts (around 100), with diminishing returns observed as communication overheads and load imbalance emerge at higher node counts. OpenFOAM demonstrates strong scalability up to approximately 1000 nodes but experiences performance degradation due to increased communication overheads and reduced computational efficiency be-yond this point. IFS exhibits notable inefficiencies at large scales (more than 200 nodes), particularly due to load imbalance and serialization in MPI and OpenMP hybrid parallel configurations. These findings emphasize the importance of optimizing both code and workload distribution to fully leverage MareNostrum5 's resources.</p><p>Energy efficiency, a key focus of MareNostrum5 's design, is supported by the EAR (Energy Aware Runtime) framework. Direct liquid cooling effectively manages thermal loads, ensuring consistent performance and reducing energy consumption. Benchmarking results show that energy consumption scales nearly linearly with increased node usage for most applications, while the Energy-Delay Product (EDP) indicates that optimal configurations balance computational efficiency with energy use. Notably, for applications like Alya, there is a clear U-shaped EDP curve, with the lowest values observed at the scalability sweet spot.</p><p>The comparison between DDR5 and HBM configurations reveals significant differences in performance and energy consumption. HBM nodes achieve speedups of up to 3.6× for memorybound workloads, particularly in applications requiring high memory throughput. However, these gains come with increased energy costs at higher core counts, where HBM configurations consume approximately 2.1× the energy of their DDR5 counterparts. This trade-off highlights the importance of selecting memory configurations based on the workload's computational characteristics. For example, HBM is advantageous for highly memory-intensive applications, while DDR5 remains a more energy-efficient choice for less memory-demanding scenarios.</p><p>Overall, the evaluation underscores MareNostrum5 's strengths and provides a comprehensive resource for understanding its performance characteristics. While the system achieves impressive results across various benchmarks, further work is needed to optimize application scalability and energy efficiency at larger scales. The findings herein will guide users in tailoring their workloads to maximize scientific output and make effective use of MareNostrum5 's computational power.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Efficiency Metrics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Base model</head><p>This performance efficiency model is structured as a tree of metrics which is shown in Figure <ref type="figure" target="#fig_32">31</ref>. All metrics with the exception of the ones labeled Scalability take values between zero and one. The Scalability metrics are computed with respect to a reference run (e.g., single-core execution) and can take values above one.</p><p>Metrics shown in blue boxes have been previously defined in <ref type="bibr" target="#b16">[19]</ref>. Metrics shown in green boxes are part of a new hybrid efficiency model that is currently in development and we formally formulate them later in this section.</p><p>In the base model, all metrics can be computed by the product of their children metrics. This is noted with × in Figure <ref type="figure" target="#fig_32">31</ref>. The hybrid model renames the Parallel Efficiency metric to Hybrid Parallel Efficiency, which accounts for both MPI and OpenMP, and defines an efficiency metric for each one of the programming models (MPI E f f and OMP E f f ). This is the only instance of the hybrid model in which children metrics cannot be multiplied to compute the parent metric.</p><p>Lastly, some metrics require accessing hardware counters (cycles and instructions) to be computed. Computational Scalability represents the change of useful computation across different executions. Ideally, the aggregated time of useful computation across all cores should remain constant for strong scaling applications. The children metrics Instruction, IPC, and Frequency Scalability represent the change of their respective elements across different executions. These four metrics are the only ones that require reading hardware counters, and are highlighted with a dotted-line box in Figure <ref type="figure" target="#fig_32">31</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Extended hybrid model</head><p>This extended model includes metrics for the OpenMP parallel programming model. This work is the first one to formally formulate such metrics. For simplicity, we assume that all MPI processes run with the same number of OpenMP threads, and that all OpenMP threads execute the same parallel regions. This assumtion holds true for IFS, which is the only hybrid application under study in this work.</p><p>Let N be the number of MPI processes; M be the number of OpenMP threads per process; and P the number of parallel regions of each thread.</p><p>Classification of execution time Let T i, j be the total time spent in the j-th OpenMP thread of the i-th MPI process. We classify time spent by threads in three categories: i) U, doing useful computation; ii) C, MPI communication; and iii) I, idle OpenMP threads 7 . Let T k i, j , ∀k ∈ {U,C, I} be the time spent in the k-th category for the given thread and process. Therefore, T i, j = T U i, j + T C i, j + T I i, j</p><p>Let T k i be the average thread time per process such that:</p><formula xml:id="formula_1">T k i = ∑ M j T k i, j M , ∀k ∈ {U,C, I}</formula><p>We further subdivide the time spent by idle threads (T I ) into three subcategories: i) I serial , outside parallel regions; ii) I lb , waiting for the slowest thread of each parallel region; and iii) I sch , all other time spent idling inside parallel regions.</p><p>Let T I i, j,p be the idle time inside the p-th OpenMP parallel region of the j-th thread of the i-th process.</p><p>Let T I serial i be the average thread time spent by process i outside OpenMP,  T i -T M i )/T Please note that, when M = 1 (executions with one OpenMP thread or without OpenMP all together), Hyb E f f = MPI E f f . In other words, when M = 1, Hyb E f f and all MPI metrics are mathematically equivalent to the ones defined in the base model <ref type="bibr" target="#b16">[19]</ref>.</p><p>Hybrid parallel efficiency Let Hyb E f f (Hybrid Parallel Efficiency) be the efficiency considering both time in MPI calls and time in idle OpenMP threads as lost:</p><formula xml:id="formula_2">Hyb E f f = T U /T</formula><p>Interaction between MPI and OpenMP metrics There are certain situations in which threads are idling waiting for an MPI communication to finish. On the OpenMP side, communication and computation can be overlapped to mitigate this inefficiency. On the MPI side, reducing the time of the communication would also reduce the time that OpenMP threads are waiting. Despite both programming models might be at fault, the current formulation of the hybrid model classifies this situation under the OpenMP metrics. There is an ongoing work to incorporate interactions between MPI and OpenMP to the hybrid efficiencies model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. High level components connectivity of a MareNostrum5 GPP tray which houses two compute nodes 3 .</figDesc><graphic coords="2,305.43,277.95,255.83,287.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. High level components connectivity of a MareNostrum5 ACC compute node.</figDesc><graphic coords="3,34.02,225.63,255.82,237.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Network diagram of MareNostrum5.</figDesc><graphic coords="4,34.02,56.69,527.27,257.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Single-node performance and power efficiency</figDesc><graphic coords="6,305.43,80.52,255.82,168.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Average performance per thread, standard deviation, and performance balance.</figDesc><graphic coords="7,34.02,187.93,255.83,267.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Aggregated copy bandwidth reported by MEM_µKernel using two binding policies.</figDesc><graphic coords="8,34.02,523.24,255.83,163.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Power consumption and efficiency running MEM_µKernel in MareNostrum5 (close policy)</figDesc><graphic coords="9,34.02,56.69,255.83,168.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Sustained memory bandwidth running STREAM in MareNostrum5 DDR nodes and using two binding policies. Dotted lines represent the theoretical peak.</figDesc><graphic coords="9,305.43,56.69,255.83,259.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Measured latency of each level of the memory hierarchy in MareNostrum5. Dotted lines represent the boundaries of each cache level. Different series correspond to different stride S values.</figDesc><graphic coords="10,34.02,66.61,255.83,129.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Network bandwidth between pairs of processes running osu_bw in MareNostrum5. The dotted line represents the theoretical peak of the Infiniband network.</figDesc><graphic coords="10,305.43,92.92,255.82,133.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Network bandwidth between pairs of nodes in MareNostrum5.</figDesc><graphic coords="10,305.43,546.96,255.84,164.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. Network latency between pairs of processes within the same rack.</figDesc><graphic coords="11,34.02,276.15,255.83,129.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>GPP</head><figDesc>In the case of the GPP partition, the HPCG binary was provided by Intel and includes MKL optimizations. The execution maps 8 MPI processes per node (i.e., four processes per socket) with 11 OpenMP threads per process. The execution with the full partition earned the spot #26 in the HPCG500 list of June 2024. ACC In the case of the ACC partition, the HPL binary was provided by Nvidia through Nvidia NGC as a Singularity image. The MPI processes and threads per GPU are configured in the same way as with HPL: 4 MPI processes per node (i.e., one process per GPU) limiting the number of threads per GPU to 20. The execution with the full partition earned the spot #12 in the HPCG500 list of June 2024.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 13 .</head><label>13</label><figDesc>Figure 13. Efficiency metrics model.</figDesc><graphic coords="12,305.43,268.28,255.81,164.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 14 .</head><label>14</label><figDesc>Figure 14. Elapsed time and throughput of Alya (362M cells) running in MareNostrum5.</figDesc><graphic coords="13,305.43,56.69,255.83,264.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 15 .</head><label>15</label><figDesc>Figure 15. Elapsed time and throughput of Alya (2.90B cells) running in MareNostrum5.</figDesc><graphic coords="13,305.43,417.03,255.83,264.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 16 .</head><label>16</label><figDesc>Figure 16. Total energy consumption and EDP of Alya (362M cells) running in MareNostrum5.</figDesc><graphic coords="14,34.02,448.05,255.83,141.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 17 .</head><label>17</label><figDesc>Figure 17. Total energy consumption and EDP of Alya (2.90B cells) running in MareNostrum5.</figDesc><graphic coords="14,305.43,77.12,255.83,141.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 18 .</head><label>18</label><figDesc>Figure 18. Efficiency metrics of Alya running in MareNostrum5.</figDesc><graphic coords="15,34.02,65.61,527.26,251.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 19 .</head><label>19</label><figDesc>Figure 19. Elapsed time and throughput of OpenFOAM running in MareNostrum5.</figDesc><graphic coords="15,34.02,422.95,255.83,264.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 20 .</head><label>20</label><figDesc>Figure 20. Efficiency metrics of OpenFOAM running in MareNostrum5.</figDesc><graphic coords="16,34.02,211.74,255.83,391.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 21 .</head><label>21</label><figDesc>Figure 21. Total energy consumption and EDP of OpenFOAM running in MareNostrum5.</figDesc><graphic coords="16,305.43,243.25,255.83,139.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Scalability</head><figDesc>Figure 22 shows the scalability of IFS when increasing the number of MareNostrum5 nodes in the x-axis. The different series plotted represent different combinations of MPI processes and OpenMP threads. All the runs of Figure 22 use the whole node, therefore, the amount of MPI processes per node is 112 divided by the amount of OpenMP threads. The plot at the top shows elapsed time (in logarithmic scale for both axes) and the plot at the bottom displays throughput. Both plots include a dotted line which illustrates the ideal scalability with respect to the best MPI-OpenMP configuration with 50 nodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 22 .</head><label>22</label><figDesc>Figure 22. Elapsed time and throughput of IFS running in MareNostrum5.</figDesc><graphic coords="17,34.02,389.60,255.83,234.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 23 .</head><label>23</label><figDesc>Figure 23. Efficiency metrics of IFS running in MareNostrum5.</figDesc><graphic coords="18,34.02,102.51,255.82,621.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 24 .</head><label>24</label><figDesc>Figure 24. Total energy consumption of IFS running in MareNostrum5.</figDesc><graphic coords="18,305.43,93.65,255.83,144.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>Figure 25 .</head><label>25</label><figDesc>Figure 25. EDP of IFS running in MareNostrum5.</figDesc><graphic coords="18,305.43,398.27,255.83,143.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>Figure 26 .</head><label>26</label><figDesc>Figure 26. Aggregated copy bandwidth in DDR and HBM nodes.</figDesc><graphic coords="19,34.02,130.61,255.83,166.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>Figure 27 .</head><label>27</label><figDesc>Figure 27. Node power consumption in DDR and HBM nodes.</figDesc><graphic coords="19,34.02,637.52,255.83,132.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head>Figure 28 .</head><label>28</label><figDesc>Figure 28. Sustained memory bandwidth running STREAM in MareNostrum5 nodes with and without HBM. Dotted lines represent the theoretical peak.</figDesc><graphic coords="19,305.43,208.50,255.83,132.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head>Figure 29 .</head><label>29</label><figDesc>Figure 29. Speedup, energy and EDP of HBM nodes with respect to DDR nodes in MareNostrum5 when running Alya (close).</figDesc><graphic coords="20,34.02,128.78,255.82,128.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_31"><head>Figure 30 .</head><label>30</label><figDesc>Figure 30. Speedup, energy and EDP of HBM nodes with respect to DDR nodes in MareNostrum5 when running OpenFOAM (spread).</figDesc><graphic coords="20,305.43,56.69,255.82,128.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_32"><head>=Figure 31 .</head><label>31</label><figDesc>Figure 31. Efficiency metrics model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_33"><figDesc>Let T k be the average process execution time such that:T k = ∑ N i T k i N , ∀k ∈ {U,C, I serial , I lb , I sch }OpenMP metrics Let OMP Serial (OpenMP Serial Efficiency) represent the time lost because OpenMP was not running a parallel region.OMP Serial = (T -T I serial )/TThis metric is used to account for regions not parallelized with OpenMP.Let OMP LB (OpenMP Load Balance Efficiency) represent the time lost in idle OpenMP threads within a parallel region due to an uneven distribution among its threads.OMP LB = (T -T I serial -T I lb )/(T -T I serial )Let OMP Sched (OpenMP Scheduling) represent the time lost in idle OpenMP threads within a parallel region not caused by load imbalance.OMP Sched = (T -T I serial -T I lb -T I sch )/(T -T I serial -T I lb )Let OMP E f f (OpenMP Parallel Efficiency) be the efficiency considering only time when OpenMP threads are idle as lost,OMP E f f = (T -T I )/T = OMP Serial × OMP LB × OMP SchedRedefinition of MPI metrics In the hybrid model, the MPI metrics are redefined as: MPI E f f = (T -T M )/T MPI LB = (T -T M )/(max i∈[1,N] T i -T M i ) MPI Comm = (max i∈[1,N] i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Theoretical</figDesc><table><row><cell></cell><cell cols="3">single-core performance of Sapphire Rapids</cell></row><row><cell>ISA</cell><cell cols="3">Vector Length Pipelines Performance</cell></row><row><cell></cell><cell>V</cell><cell>N</cell><cell>P</cell></row><row><cell>x86.sp</cell><cell>1</cell><cell>2</cell><cell>4</cell></row><row><cell>x86.dp</cell><cell>1</cell><cell>2</cell><cell>4</cell></row><row><cell>avx2.sp</cell><cell>8</cell><cell>2</cell><cell>32</cell></row><row><cell>avx2.dp</cell><cell>4</cell><cell>2</cell><cell>16</cell></row><row><cell>avx512.sp</cell><cell>16</cell><cell>2</cell><cell>64</cell></row><row><cell>avx512.dp</cell><cell>8</cell><cell>2</cell><cell>32</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>FPU_µKernel: single-core performance ISA Instr. Operations Duration Cycles Freq.</figDesc><table><row><cell>Performance</cell><cell>IPC Power</cell><cell>Efficiency</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>MEM_µKernel: single-core performance</figDesc><table><row><cell>ISA</cell><cell>Instr.</cell><cell cols="5">Read Written Duration Cycles Freq.</cell><cell cols="2">Bandwidth</cell><cell>IPC</cell></row><row><cell></cell><cell>10 9</cell><cell>GB</cell><cell>GB</cell><cell>s</cell><cell>10 9</cell><cell cols="2">GHz Byte/Cycle</cell><cell>GB/s</cell></row><row><cell>x86.rd</cell><cell>53.63</cell><cell>1280</cell><cell>0</cell><cell cols="2">17.99 160.31</cell><cell>2.98</cell><cell>23.87</cell><cell cols="2">71.14 2.99</cell></row><row><cell>avx2.rd</cell><cell>54.04</cell><cell>5120</cell><cell>0</cell><cell cols="2">18.13 160.31</cell><cell>2.98</cell><cell cols="3">94.75 282.38 2.97</cell></row><row><cell>avx512.rd</cell><cell cols="2">84.87 10240</cell><cell>0</cell><cell cols="2">28.48 160.31</cell><cell>2.98</cell><cell cols="3">120.65 359.54 1.89</cell></row><row><cell>x86.wr</cell><cell>160.31</cell><cell>0</cell><cell>1280</cell><cell>26.86</cell><cell>80.08</cell><cell>2.98</cell><cell>15.98</cell><cell cols="2">47.65 2.00</cell></row><row><cell>avx2.wr</cell><cell>160.31</cell><cell>0</cell><cell>5120</cell><cell>26.86</cell><cell>80.08</cell><cell>2.98</cell><cell cols="3">63.94 190.59 2.00</cell></row><row><cell cols="2">avx512.wr 160.31</cell><cell>0</cell><cell>10240</cell><cell cols="2">53.71 160.31</cell><cell>2.98</cell><cell cols="3">63.96 190.65 1.00</cell></row><row><cell cols="5">load pipelines. The reason behind this limitation comes from how</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">many cache lines can be served from the L1 at any given cycle,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">which our measurements suggest that it is two lines per cycle.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>HPL single and multi node results in MareNostrum5</figDesc><table><row><cell cols="3">Partition Nodes RPeak</cell><cell cols="3">RMax %Rpeak Power</cell><cell>Efficiency</cell></row><row><cell></cell><cell></cell><cell cols="2">TFlop/s TFlop/s</cell><cell></cell><cell>kW</cell><cell>GFlop/(s×W)</cell></row><row><cell>GPP</cell><cell>1</cell><cell>7.17</cell><cell>6.61</cell><cell>92.19</cell><cell>0.87</cell><cell>7.59</cell></row><row><cell>ACC</cell><cell>1</cell><cell>232.00</cell><cell>179.70</cell><cell>77.46</cell><cell>3.50</cell><cell>66.29</cell></row><row><cell cols="3">Partition Nodes RPeak</cell><cell cols="3">RMax %Rpeak Power</cell><cell>Efficiency</cell></row><row><cell></cell><cell></cell><cell cols="2">PFlop/s PFlop/s</cell><cell></cell><cell>MW</cell><cell>GFlop/(s×W)</cell></row><row><cell>GPP</cell><cell>6264</cell><cell>44.90</cell><cell>40.10</cell><cell>89.31</cell><cell>5.75</cell><cell>6.97</cell></row><row><cell>ACC</cell><cell>1080</cell><cell>250.56</cell><cell>175.30</cell><cell>69.96</cell><cell>4.16</cell><cell>42.15</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>HPCG single and multi node results in MareNostrum5</figDesc><table><row><cell cols="3">Partition Nodes RPeak</cell><cell cols="2">RMax %Rpeak</cell></row><row><cell></cell><cell></cell><cell cols="2">TFlop/s TFlop/s</cell><cell></cell></row><row><cell>GPP</cell><cell>1</cell><cell>7.17</cell><cell>0.08</cell><cell>1.16</cell></row><row><cell>ACC</cell><cell>1</cell><cell>232.00</cell><cell>1.06</cell><cell>0.46</cell></row><row><cell cols="2">Partition Nodes</cell><cell>RPeak</cell><cell cols="2">RMax %Rpeak</cell></row><row><cell></cell><cell></cell><cell>TFlop/s</cell><cell>TFlop/s</cell><cell></cell></row><row><cell>GPP</cell><cell>6270</cell><cell>44943.36</cell><cell>484.00</cell><cell>1.08</cell></row><row><cell>ACC</cell><cell cols="3">1024 237568.00 1145.98</cell><cell>0.48</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>DL-BENCH results in the ACC partition</figDesc><table><row><cell cols="2">Partition Nodes</cell><cell>Time</cell><cell cols="2">Energy Power</cell></row><row><cell></cell><cell></cell><cell>s</cell><cell>kWh</cell><cell>kW</cell></row><row><cell>ACC</cell><cell cols="2">16 2473.04</cell><cell>32.76</cell><cell>47.68</cell></row><row><cell>ACC</cell><cell cols="2">78 644.254</cell><cell cols="2">33.36 186.41</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>The CPU model in HBM nodes is Intel Xeon CPU Max 9480. Visit Intel Ark for more details.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>HBM is depicted in gray because it is present only on the 216 nodes of the DDR-HM configuration.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>https://www.bsc.es/marenostrum/marenostrum/ available-software</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3"><p>we have picked this combination, because we thought it would be the combination that would give more insight for both MPI and OpenMP efficiencies</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>Supported by the pre-doctoral program AGAUR-FI ajuts (<rs type="grantNumber">2024 FI-200424</rs>) <rs type="person">Joan Oró</rs> offered by <rs type="institution">Secretaria d'Universitats i Recerca del Departament de Recerca i Universitats de la Generalitat de Catalunya</rs>.</p><p>The acquisition and operation of the EuroHPC supercomputer is funded jointly by the <rs type="institution">EuroHPC Joint Undertaking</rs>, through the <rs type="funder">European Union</rs>'s <rs type="funder">Connecting Europe Facility</rs> and the <rs type="programName">Horizon 2020 research and innovation programme</rs>, as well as the Participating States Spain, Portugal, and Türkiye.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_5dN6emX">
					<idno type="grant-number">2024 FI-200424</idno>
				</org>
				<org type="funding" xml:id="_bzcNkeg">
					<orgName type="program" subtype="full">Horizon 2020 research and innovation programme</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="https://www.top500.org/system/180237/" />
		<title level="m">Top500 Marenostrum5 General Purpose Partition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<ptr target="https://www.top500.org/system/180238/" />
		<title level="m">Top500 Marenostrum5 Accelerated Partition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Modern scientific software management using easybuild and lmod</title>
		<author>
			<persName><forename type="first">Markus</forename><surname>Geimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Hoste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Mclay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 First International Workshop on HPC User Support Tools</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="41" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Slurm: Simple linux utility for resource management</title>
		<author>
			<persName><forename type="first">Andy</forename><forename type="middle">B</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morris</forename><forename type="middle">A</forename><surname>Jette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Grondona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on job scheduling strategies for parallel processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="44" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Papi: A portable interface to hardware performance counters</title>
		<author>
			<persName><forename type="first">J</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirley</forename><surname>Mucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christine</forename><surname>Browne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Deane</surname></persName>
		</author>
		<author>
			<persName><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the department of defense HPCMP users group conference</title>
		<meeting>the department of defense HPCMP users group conference</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">710</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Talp: A lightweight tool to unveil parallel efficiency of large-scale executions</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Ramirez Miranda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marta</forename><surname>Garcia-Gasulla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 on Performance EngineeRing, Modelling, Analysis, and Visual-izatiOn STrategy, PERMAVOST &apos;21</title>
		<meeting>the 2021 on Performance EngineeRing, Modelling, Analysis, and Visual-izatiOn STrategy, PERMAVOST &apos;21<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Energy optimization and analysis with ear</title>
		<author>
			<persName><forename type="first">Julita</forename><surname>Corbalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lluis</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordi</forename><surname>Aneas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luigi</forename><surname>Brochard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CLUSTER</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="464" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Explicit uncore frequency scaling for energy optimisation policies with ear in intel architectures</title>
		<author>
			<persName><forename type="first">Julita</forename><surname>Corbalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lluis</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordi</forename><surname>Aneas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Cluster Computing (CLUSTER)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="572" to="581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Memory bandwidth and machine balance in current high performance computers. IEEE computer society technical committee on computer architecture (TCCA) newsletter</title>
		<author>
			<persName><surname>John D Mccalpin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995. 1995</date>
			<biblScope unit="page" from="19" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cluster of emerging technology: evaluation of a production hpc system based on a64fx</title>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Banchelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><surname>Peiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Ramirez-Gargallo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marta</forename><surname>Garcia-Gasulla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filippo</forename><surname>Mantovani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Cluster Computing (CLUSTER)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="741" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Performance and energy consumption of hpc workloads on a cluster based on arm thunderx2 cpu</title>
		<author>
			<persName><forename type="first">Filippo</forename><surname>Mantovani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marta</forename><surname>Garcia-Gasulla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José</forename><surname>Gracia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esteban</forename><surname>Stafford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Banchelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Josep-Fabrego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Criado-Ledesma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Nachtmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Generation Computer Systems</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page" from="800" to="818" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bandwidth limits in the intel xeon max (sapphire rapids with hbm) processors</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Mccalpin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High Performance Computing</title>
		<editor>
			<persName><forename type="first">Amanda</forename><surname>Bienz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michèle</forename><surname>Weiland</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Marc</forename><surname>Baboulin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Carola</forename><surname>Kruse</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Nature Switzerland</publisher>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Graph500 BFS list</title>
		<ptr target="https://" />
		<imprint>
			<date type="published" when="2024-11">November 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neko: A modern, portable, and scalable framework for high-fidelity computational fluid dynamics</title>
		<author>
			<persName><forename type="first">Niclas</forename><surname>Jansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Karp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Artur</forename><surname>Podobas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Markidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Schlatter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Fluids</title>
		<imprint>
			<biblScope unit="volume">275</biblScope>
			<biblScope unit="page">106243</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Multi-year simulations at kilometre scale with the integrated forecasting system coupled</title>
		<author>
			<persName><forename type="first">T</forename><surname>Rackow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">2024</biblScope>
			<biblScope unit="page" from="1" to="59" />
		</imprint>
	</monogr>
	<note>to fesom2.5/nemov3.4. EGUsphere</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Benchmarking of state-of-the-art HPC Clusters with a Production CFD Code</title>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Banchelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marta</forename><surname>Garcia-Gasulla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Houzeaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filippo</forename><surname>Mantovani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Platform for Advanced Scientific Computing Conference</title>
		<meeting>the Platform for Advanced Scientific Computing Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A generic performance analysis technique applied to different cfd methods for hpc</title>
		<author>
			<persName><forename type="first">Marta</forename><surname>Garcia-Gasulla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Banchelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><surname>Peiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Ramirez-Gargallo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Houzeaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ismaïl</forename><surname>Ben Hassan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Saïdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Tenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filippo</forename><surname>Spisso</surname></persName>
		</author>
		<author>
			<persName><surname>Mantovani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computational Fluid Dynamics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">7-8</biblScope>
			<biblScope unit="page" from="508" to="528" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Alya: Multiphysics engineering simulation toward exascale</title>
		<author>
			<persName><forename type="first">Mariano</forename><surname>Vázquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Houzeaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seid</forename><surname>Koric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoni</forename><surname>Artigues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jazmin</forename><surname>Aguado-Sierra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruth</forename><surname>Arís</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Mira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hadrien</forename><surname>Calmet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Cucchietti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herbert</forename><surname>Owen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Taha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><forename type="middle">Dering</forename><surname>Burness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José</forename><surname>María Cela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateo</forename><surname>Valero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Route to Exascale: Novel Mathematical Methods, Scalable Algorithms and Computational Science Skills</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="15" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A Tensorial Approach to Computational Continuum Mechanics Using Object Orientated Techniques</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Weller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tabor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jasak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fureby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers in Physics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="620" to="631" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Coherent mesh representation for parallel I/O of unstructured polyhedral meshes</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Gregor</forename><surname>Weiß</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Lesnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Flavio</forename><surname>Galeazzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Ruopp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrik</forename><surname>Rusche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Supercomputing</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page">2024</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<ptr target="https://exafoam.eu/" />
		<title level="m">exaFOAM Project Website</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Charles</forename><surname>Mockett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Kramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julius</forename><surname>Bergmann</surname></persName>
		</author>
		<ptr target="https://develop.openfoam.com/committees/hpc/-/tree/develop/compressible/rhoPimpleFoam/LES/highLiftCommonResearchModelONERA_LRM-LDG-HV" />
		<title level="m">High-Lift Common Research Model</title>
		<imprint>
			<biblScope unit="page" from="2022" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Understanding superlinear speedup in current HPC architectures</title>
		<author>
			<persName><forename type="first">Flavio</forename><surname>Galeazzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Gregor</forename><surname>Weiß</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Lesnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrik</forename><surname>Rusche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Ruopp</surname></persName>
		</author>
		<idno>1312:012009</idno>
	</analytic>
	<monogr>
		<title level="m">IOP Conference Series: Materials Science and Engineering</title>
		<imprint>
			<biblScope unit="page" from="8" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ismaïl Ben Hassan Saïdi, Christian Tenaud, Ivan Spisso, and Filippo Mantovani. A generic performance analysis technique applied to different cfd methods for hpc</title>
		<author>
			<persName><forename type="first">Marta</forename><surname>Garcia-Gasulla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Banchelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><surname>Peiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Ramirez-Gargallo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Houzeaux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computational Fluid Dynamics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">7-8</biblScope>
			<biblScope unit="page" from="508" to="528" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A baseline for global weather and climate simulations at 1 km resolution</title>
		<author>
			<persName><forename type="first">Inna</forename><surname>Nils P Wedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Polichtchouk</surname></persName>
		</author>
		<author>
			<persName><surname>Dueben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Valentine G Anantharaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Souhail</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Boussetta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Willem</forename><surname>Browne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><surname>Deconinck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioan</forename><surname>Gaudin</surname></persName>
		</author>
		<author>
			<persName><surname>Hadade</surname></persName>
		</author>
		<idno>MS002192</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Advances in Modeling Earth Systems</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Balancing ecearth3 improving the performance of ec-earth cmip6 configurations by minimizing the coupling cost</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Palomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tourigny</surname></persName>
		</author>
		<idno>EA002912 2023EA002912</idno>
	</analytic>
	<monogr>
		<title level="j">Earth and Space Science</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2023">2023. 2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Performance comparison of cfd microbenchmarks on diverse hpc architectures</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Flavio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marta</forename><surname>Galeazzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elisabetta</forename><surname>Garcia-Gasulla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josep</forename><surname>Boella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Pocurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrik</forename><surname>Lesnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Rusche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Bnà</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Cerminara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filippo</forename><surname>Brogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniele</forename><surname>Marchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Gregor</forename><surname>Gregori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Weiß</surname></persName>
		</author>
		<author>
			<persName><surname>Ruopp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
