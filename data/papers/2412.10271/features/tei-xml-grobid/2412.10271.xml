<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Benchmarking Linguistic Diversity of Large Language Models</title>
				<funder ref="#_DXmGpVX">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-12-13">13 Dec 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yanzhu</forename><surname>Guo</surname></persName>
							<email>yanzhu.guo@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">ALMAnaCH Inria Paris Guokan Shang France Lab MBZUAI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Clavel</forename><surname>Chloé</surname></persName>
							<email>chloe.clavel@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">ALMAnaCH Inria Paris Guokan Shang France Lab MBZUAI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Inria</forename><surname>Almanach</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ALMAnaCH Inria Paris Guokan Shang France Lab MBZUAI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><surname>Paris</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ALMAnaCH Inria Paris Guokan Shang France Lab MBZUAI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Benchmarking Linguistic Diversity of Large Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-12-13">13 Dec 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">1138907074FB711B33AAED08A4181281</idno>
					<idno type="arXiv">arXiv:2412.10271v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The development and evaluation of Large Language Models (LLMs) has primarily focused on their task-solving capabilities, with recent models even surpassing human performance in some areas. However, this focus often neglects whether machine-generated language matches the human level of diversity, in terms of vocabulary choice, syntactic construction, and expression of meaning, raising questions about whether the fundamentals of language generation have been fully addressed. This paper emphasizes the importance of examining the preservation of human linguistic richness by language models, given the concerning surge in online content produced or aided by LLMs. We propose a comprehensive framework for evaluating LLMs from various linguistic diversity perspectives including lexical, syntactic, and semantic dimensions. Using this framework, we benchmark several state-of-the-art LLMs across all diversity dimensions, and conduct an indepth case study for syntactic diversity. Finally, we analyze how different development and deployment choices impact the linguistic diversity of LLM outputs.</p><p>* This work was partially done during the author's affiliation with École Polytechnique.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent Large Language Models (LLMs) have exhibited outstanding capabilities in generating both natural and formal language <ref type="bibr" target="#b4">(Brown et al., 2020;</ref><ref type="bibr" target="#b56">Touvron et al., 2023)</ref>, while also achieving human-level performance in language understanding, commonsense reasoning, and various other tasks <ref type="bibr" target="#b24">(Hendrycks et al., 2020)</ref>. This has led to evaluations that predominantly focus on these specific abilities <ref type="bibr" target="#b58">(Wang et al., 2024)</ref>. Meanwhile, other evaluation studies address well-recognized issues in LLMs, such as factuality <ref type="bibr" target="#b37">(Maynez et al., 2020)</ref>, safety <ref type="bibr" target="#b64">(Zhang et al., 2024)</ref>, and fairness <ref type="bibr" target="#b15">(Gallegos et al., 2024)</ref>, which remain focal points of ongoing research. However, there is a notable lack of attention paid to linguistic perspectives, particularly in diversity <ref type="bibr">(Guo et al., 2024b)</ref>, despite the fundamental objective of natural language generation being to produce outputs that are not only accurate but also diverse <ref type="bibr" target="#b55">(Tevet and Berant, 2021)</ref>.</p><p>Recent studies have highlighted concerns regarding the linguistic diversity of LLM outputs. By comparing human and model-generated content, researchers have shown that models frequently struggle to reflect the nuances and variations characteristic of human expression <ref type="bibr">(Shaib et al., 2024a;</ref><ref type="bibr" target="#b16">Giulianelli et al., 2023)</ref>. Additionally, these concerns are reinforced by findings that training language models on synthetic text can lead to a further decline in linguistic diversity <ref type="bibr">(Guo et al., 2024b)</ref>.</p><p>In fact, LLMs tend to be inherently conservative in producing diverse content. During training, models undergo homogenization to the most frequent patterns in the training data, where creative outlier narratives, views, styles, and knowledge are often underrepresented <ref type="bibr" target="#b29">(Kandpal et al., 2023)</ref>. Unlike models, human language production involves a complex interplay of factors that go beyond merely optimizing probabilities <ref type="bibr" target="#b25">(Holtzman et al., 2020)</ref>. It is therefore crucial to emphasize evaluating output diversity in language models and systematically consider these metrics to guide future model development and deployment decisions.</p><p>Currently, a principled and comprehensive evaluation framework for linguistic diversity is lacking in the literature <ref type="bibr">(Shaib et al., 2024a)</ref>. While some studies on Natural Language Generation (NLG) report diversity metrics, they typically focus on a single diversity aspect (e.g., lexical diver-sity <ref type="bibr" target="#b6">(Chakrabarty et al., 2022)</ref>), often experimenting within a single domain and task (e.g., news summarization <ref type="bibr">(Shaib et al., 2024a)</ref>). This narrow focus is problematic since diversity varies across aspects and depends on the domain <ref type="bibr">(Guo et al., 2024b)</ref>. Although some efforts have been made to assess the influence of reinforcement learning from human feedback (RLHF) on diversity <ref type="bibr" target="#b30">(Kirk et al., 2024)</ref>, the impact of other key design and development stages-such as model scale, quantization, and decoding strategy-remains unexplored. Additionally, there is a limited understanding of how LLMs develop the capability to generate diverse language through successive pretraining checkpoints. Ultimately, no study has benchmarked the diversity performance of state-of-theart LLMs across different aspects and domains.</p><p>In this work, we first establish a framework for evaluating linguistic diversity of LLM outputs on a corpus level. We then benchmark six prominent LLMs on five NLG tasks, and compare the diversity of their outputs across three different aspects: lexical, syntactic, and semantic. We further explore syntactic diversity, and conduct a case study comparing the distribution of dependency trees generated by LLMs and humans. Finally, we investigate how LLM output diversity changes across different development stages, and with varying decisions of deployment. The main research questions we address are as follows:</p><p>1. What are the key aspects of LLM output diversity, and how can they be evaluated? (See § 3) 2. How do state-of-the-art LLMs perform in terms of diversity across different tasks? (See § 5) 3. How does diversity change during each LLM development stage (e.g., pretraining, supervised fine-tuning (SFT), preference tuning)? (See § 6.1) 4. How do different design (e.g., model scale, training data) and deployment (e.g., decoding strategy, quantization) choices affect diversity? (See § 6.2 and § 6.3)</p><p>It is worth noting that we study linguistic diversity in a monolingual context, focusing on the English language. However, the evaluation methodology is language agnostic and could easily be extended to other languages, given that employed NLP toolkits (e.g., dependency parsers, sentence embeddings) exist for the language. Furthermore, our approach to analyzing the influence of various factors on LLM outputs is adaptable to other dimensions, such as linguistic naturalness <ref type="bibr">(Guo et al., 2024a)</ref>. The code is publicly available 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we review methods for evaluating and analyzing linguistic diversity. We define linguistic diversity as the natural variation in human language across core linguistic properties, including vocabulary usage, grammatical structures, and semantic nuances. In contrast, a separate line of research focuses on socio-linguistic diversity <ref type="bibr">(Hayati et al., 2023;</ref><ref type="bibr" target="#b31">Lahoti et al., 2023)</ref>, which falls beyond the scope of our study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Evaluation of Human Language</head><p>Early metrics for linguistic diversity, proposed by linguists, were developed for studies of language acquisition and language disorder detection. For example, <ref type="bibr" target="#b14">Fergadiotis et al. (2013)</ref> employed lexical diversity metrics to identify symptoms of aphasia, while <ref type="bibr" target="#b38">McNamara et al. (2010)</ref> showed that both syntactic complexity and lexical diversity can predict essay quality. Another study by Clercq and Housen (2017) manually annotated a small corpus of texts produced by second language learners for syntactic features such as syntactic length and clause types, considering their variation as a diversity index. However, these metrics are limited to evaluating human-written texts and either focus exclusively on lexical diversity or lack scalability due to the need for manual annotation. The evaluation of linguistic diversity in modelgenerated language has emerged as a relatively recent focus of research. This development is driven, in part, by growing concerns over the increasing prevalence of model-generated or modelinfluenced content online, prompting questions about whether LLMs can maintain the linguistic richness characteristic of human language <ref type="bibr">(Guo et al., 2024b)</ref>. Moreover, advances in language generation quality have brought model outputs closer than ever to human-level coherence and plausibility. Assessing linguistic diversity becomes meaningful only when the generated text meets these standards. For example, a randomly initialized model might produce token sequences with high lexical diversity, but such outputs hold no practical value <ref type="bibr" target="#b57">(Uchendu et al., 2023)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Evaluation of Generated Language</head><p>To the best of our knowledge, <ref type="bibr" target="#b55">Tevet and Berant (2021)</ref> were the first authors to systematically evaluate diversity in NLG. They proposed to create diversity metrics from any two-sentence similarity measure, defining diversity as the inverse of the mean similarity score across all unordered pairs. N-gram-based metrics were used to assess form diversity, while model-based metrics like Sentence-BERT similarity measured content diversity. They concluded that a notable disparity exists between automatic metrics and human judgment, and that human evaluation of diversity becomes challenging in sets with more than ten responses. Since then, additional metrics have been proposed to capture linguistic diversity, including semantic diversity metrics based on natural language inference <ref type="bibr" target="#b52">(Stasaski and Hearst, 2022)</ref> or semantic entropy <ref type="bibr" target="#b20">(Han et al., 2022)</ref>, and syntactic diversity metrics derived from n-grams of Partof-Speech (POS) tags <ref type="bibr" target="#b16">(Giulianelli et al., 2023)</ref> or graph similarity kernels of syntax trees <ref type="bibr">(Guo et al., 2024b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Impact of LLMs on Linguistic Diversity</head><p>Diverging from the above research focused on developing methods to evaluate linguistic diversity, another line of work explores the the impact of LLMs on both human and model generated text, often demonstrating a decline in diversity. <ref type="bibr">Guo et al. (2024b)</ref> showed that iteratively training LLMs on synthetic data generated by earlier models, leads to a consistent decline in lexical, syntactic, and semantic diversity, especially for tasks requiring high creativity. Similarly, Padmakumar and He (2024) reported a statistically significant reduction in linguistic diversity when humans write with InstructGPT. This reduction in linguistic diversity is also observed in other contexts: <ref type="bibr" target="#b34">Liang et al. (2024)</ref> identified a significant frequency shift toward LLM-preferred words in academic writing, and <ref type="bibr" target="#b36">Luo et al. (2024)</ref> reported reduced morphosyntactic diversity in machine translations compared to human translations.</p><p>Closely related to our work, <ref type="bibr" target="#b30">Kirk et al. (2024)</ref> examined how SFT and preference tuning affect LLM generalization and diversity. They found that preference tuning substantially reduces lexical and semantic diversity compared to SFT. Our research also explores the factors that influence diversity while broadening the analysis to include a wider range of diversity aspects, models, tasks and factors. Moreover, our findings on the impact of preference tuning differ from those of <ref type="bibr" target="#b30">Kirk et al. (2024)</ref>, likely due to differences in task domain, accentuating the importance of contextualizing conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Metrics for Linguistic Diversity</head><p>In this section, we present the three types of diversity central to our study: lexical, syntactic, and semantic diversity.</p><p>According to <ref type="bibr" target="#b55">Tevet and Berant (2021)</ref>, diversity can be viewed as having a hierarchical structure with various sub-aspects. Lexical diversity and syntactic diversity are considered sub-aspects of form diversity, while semantic diversity reflects content diversity. Although there are potentially other sub-aspects of linguistic diversity, such as style (register) diversity within form diversity, these elements are often more ambiguous, harder to quantify, and tend to overlap with the existing sub-aspects. For example, style diversity is partially captured through lexical and syntactic diversity, as they reflect preferences in vocabulary and grammatical structures. Consequently, we focus on the three aspects of diversity that are more clearly defined, easier to quantify, and have relatively low correlation with each other (further discussed in Section 5.1).</p><p>In terms of evaluation protocol, <ref type="bibr" target="#b30">Kirk et al. (2024)</ref> distinguish between across-input diversity and per-input diversity. Across-input diversity refers to the diversity of outputs across different inputs, with only one output generated per input. In contrast, per-input diversity evaluates the capability of the model to produce diverse outputs for a single input.</p><p>In our study, we choose to measure across-input diversity, as we focus on linguistic patterns across a broad range of generations. Formally, given a set of generated outputs S = {s 1 , s 2 , . . . , s n }, we compute Div(S) differently depending on the aspect of diversity: for lexical diversity, S is treated as a set of n-grams, while for syntactic and semantic diversity, S is considered as a set of sentences.</p><p>In the following sections, we explain each diversity aspect and the specific metrics used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Lexical Diversity</head><p>Lexical diversity is a measure of the variety of vocabulary used within a text or set of texts. In essence, it assesses the richness or variability of word choices. High lexical diversity indicates a broad range of unique words, while low lexical diversity suggests repetitive or limited vocabulary.</p><p>We employ Unique-n <ref type="bibr" target="#b28">(Johnson, 1944;</ref><ref type="bibr" target="#b54">Templin, 1957)</ref>, established for evaluating lexical diversity. It is calculated as the ratio of unique n-grams to the total number of n-grams. When n = 1, it is equivalent to Type-Token Ratio <ref type="bibr" target="#b28">(Johnson, 1944;</ref><ref type="bibr" target="#b54">Templin, 1957)</ref>. We report the average Uniquen across unigrams, bigrams, and trigrams. Originally used in child language research, Unique-n is useful for assessing language development, where a lower value might indicate limited lexical variety <ref type="bibr" target="#b40">(Miller, 1981)</ref>. We use the global Unique-n measure rather than the moving average Uniquen because we are interested in the overall diversity capabilities of LLMs across different inputs rather than their performance on individual inputs. Moving average methods might miss global lexical repetitions due to their localized nature (Bestgen, 2023). To mitigate the influence of output length on Unique-n, we always randomly choose 40K samples to constitute the set of n-grams for each n.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Syntactic Diversity</head><p>Syntactic diversity refers to the range and variety of sentence structures used in a text or set of texts. It assesses how flexibly and creatively different grammatical structures, such as phrases, clauses, and sentence types, are employed. High syntactic diversity suggests varied sentence forms, while low syntactic diversity indicates repetitive or simplistic sentence structures. Syntactic diversity is a crucial but often neglected aspect of language. Exposure to a variety of syntactic structures helps language learners and models develop a richer understanding of language <ref type="bibr" target="#b0">(Aggarwal et al., 2022)</ref>. Diverse syntactic forms enhance expressiveness and subtlety in text, impacting its style and tone <ref type="bibr" target="#b12">(Edwards and Bastiaanse, 1998)</ref>. While research on syntactic diversity exists, it typically relies on manual annotation, which can be both costly and error-prone <ref type="bibr" target="#b8">(Clercq and Housen, 2017)</ref>.</p><p>To address this limitation, we employ a graphbased metric for quantifying syntactic diversity <ref type="bibr">(Guo et al., 2024b)</ref>. This metric relies on a neural parser <ref type="bibr" target="#b42">(Qi et al., 2020)</ref> to generate dependency trees from sentences, following the universal dependencies framework. These trees are converted into graph representations, where nodes represent words and edges denote dependency relationships. The nodes are labeled by the PoS tag of each word. The Weisfeiler-Lehman (WL) graph kernel <ref type="bibr" target="#b49">(Shervashidze et al., 2011;</ref><ref type="bibr" target="#b50">Siglidis et al., 2020)</ref> is then applied to map these graphs into a vector space. This kernel, based on the WL isomorphism test, positions structurally similar graphs closer together in the vector space. Syntactic diversity is then measured using the average pairwise distance between graphs, formalized as: Div syn (S) = 1</p><p>( n 2 ) 1≤i&lt;j≤n W L(s i , s j ). Alternatively, the pairwise distances between the dependency trees could also be measured by the tree editing distance <ref type="bibr" target="#b62">(Zhang and Shasha, 1989)</ref>. However, the algorithm to compute the tree editing distance has much higher computational complexity and is thus not scalable to a large set of texts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Semantic Diversity</head><p>Semantic diversity refers to the range and variety of meanings or ideas conveyed within a text or set of texts. It evaluates how broadly and uniquely different concepts, topics, or ideas are expressed, reflecting the depth and scope of the content. High semantic diversity suggests a text covers diverse ideas or meanings, while low semantic diversity implies repetition or a narrow focus on specific concepts. Recent studies <ref type="bibr" target="#b55">(Tevet and Berant, 2021;</ref><ref type="bibr" target="#b52">Stasaski and Hearst, 2022)</ref> have pointed out that traditional lexical metrics may not fully capture semantic diversity. Similar words can convey different meanings, and different words can convey similar meanings <ref type="bibr" target="#b60">(Yarats and Lewis, 2018)</ref>.</p><p>To address this, we first convert sentences into semantically meaningful embeddings using Sentence-BERT <ref type="bibr" target="#b45">(Reimers and Gurevych, 2019)</ref>. Semantic diversity is then quantified as the dispersion of these embeddings in the semantic space, measured by the average pairwise cosine distance (scaled to the range [0, 1]) between all embedding vectors: Div sem (S) =</p><formula xml:id="formula_0">1 ( n 2 ) 1≤i&lt;j≤n 1+dcos 2 (e(s i ), e(s j ))</formula><p>, where e represents Sentence-BERT embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Settings for Diversity Benchmarking</head><p>We outline the tasks, datasets, and models used to establish our linguistic diversity benchmark. Generation tasks. To effectively compare the linguistic diversity of LLM outputs across various scenarios, we selected five tasks with progressively increasing levels of "creativity": lan-   <ref type="table" target="#tab_0">1</ref> outlines the inputs, outputs, and datasets associated with each task. Note that the input is combined with a task-specific instruction (e.g., "Continue the following story:" for story generation) to construct the final prompt.</p><p>For each task, we randomly select 10K samples from the original dataset. To obtain the benchmark results, we decode the outputs for all models and tasks using a combination of nucleus sampling (t=0.6) and top-k sampling (k=0.9). We further analyze in Section 6.2 the impact of different decoding parameters on output diversity.</p><p>Large language models. We evaluate the following families of models: Llama <ref type="bibr" target="#b10">(Dubey et al., 2024)</ref>, <ref type="bibr">Mistral (Jiang et al., 2023)</ref>, Olmo (Groeneveld et al., 2024), Gemma <ref type="bibr">(Team et al., 2024)</ref>, Qwen <ref type="bibr" target="#b59">(Yang et al., 2024)</ref>, Falcon <ref type="bibr" target="#b1">(Almazrouei et al., 2023)</ref>. To ensure comparability, we select the latest version of each model family that is closest in scale to 7 billion parameters. The scale selected for each model is specified in the legend of Table <ref type="table" target="#tab_0">1</ref>. We purposefully include models devel-oped by organizations from different countries to be culturally inclusive. For language modeling, we use base models. For all other tasks, we employ instruction-tuned versions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results for Diversity Benchmarking</head><p>Figure <ref type="figure">1</ref> presents the benchmarking results of linguistic diversity across various tasks. Round dots represent the diversity of model outputs, while solid lines represent human reference outputs. Dashed lines depict the diversity of task-specific inputs (as detailed in Table <ref type="table" target="#tab_0">1</ref>), reflecting the conditions under which the outputs were generated. Tasks are organized in ascending order of human reference diversity for each aspect of diversity. For the machine translation task, the inputs are in French; hence, semantic diversity is measured using a multilingual SentenceBERT <ref type="bibr" target="#b46">(Reimers and Gurevych, 2020)</ref>, and syntactic diversity is evaluated with a French-specific dependency parser. As a result, these scores may not be directly comparable to those for English. We analyze the results in Figure <ref type="figure">1</ref> in Sections 5.2 and 5.3.</p><p>In this section, we first analyze metric correla-  tions in Section 5.1, then compare diversity scores across tasks and models in Section 5.2. Finally, we perform a case study on syntactic diversity in story generation, comparing human and model outputs in Section 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Correlation Study</head><p>Correlation between diversity and quality. As noted in Section 2.1, diversity matters only if the text is of plausible quality. Figure <ref type="figure" target="#fig_1">2</ref> illustrates the correlation between diversity and quality in model outputs, using task-specific automatic metrics as quality indicators. For the language modeling task, perplexity is used to evaluate the model's performance on reference text continuations. For machine translation, we use COMET <ref type="bibr" target="#b44">(Rei et al., 2020)</ref>, which takes into account both the source text and reference translation. For the remaining three tasks, BERTScore <ref type="bibr" target="#b63">(Zhang et al., 2020)</ref> is used to measure the relevance between inputs and outputs. However, for these last three tasks with a subjective nature, this relevance score serves only as a proxy for quality, as automatic metrics for such tasks generally exhibit low correlation with human judgments <ref type="bibr" target="#b7">(Chhun et al., 2024;</ref><ref type="bibr" target="#b35">Liu et al., 2023)</ref>.</p><p>Our results show a positive correlation between quality and lexical as well as semantic diversity in model outputs. In contrast, syntactic diversity often exhibits negative correlations, where higher syntactic diversity is associated with lower quality scores. This may be attributed to the tested domains inherently exhibiting low ground-truth syntactic diversity (e.g., in language modeling) or to the limitations of quality metrics in recognizing the value of syntactic variation (e.g., in summarization, automatic story generation, and next utterance generation). These findings highlight the need to report diversity metrics alongside quality metrics for comprehensive evaluation.</p><p>Correlation between diversity aspects. The correlations between different diversity aspects are shown in Figure <ref type="figure" target="#fig_2">3</ref>, revealing a moderate positive correlation between syntactic and semantic diversity (0.55). However, lexical diversity shows a weak positive relationship with syntactic diversity (0.13) and a slight negative correlation with semantic diversity (-0.14), indicating that the richness of vocabulary is independent from the variety of grammatical structures and meaning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparison Across Tasks and Models</head><p>We now examine the results in Figure <ref type="figure">1</ref> to assess human diversity results across tasks, compare model diversity against human diversity, and finally evaluate the diversity performance across different models. Human output diversity. Human-level diversity varies across tasks, with no clear correlation observed among different aspects. Notably, utterances in human dialogue exhibit the lowest lexical diversity and the highest syntactic diversity, unlike the written text present in the remaining four categories. The low lexical diversity may be attributed to the conversations being specifically scripted for English learners to practice daily-life dialog. These dialogs focus on generic topics, leading to a limited range of vocabulary. In contrast, the high syntactic diversity can be explained by the inherent spontaneity of conversational language, where different speakers tend to vary significantly in their use of syntactic structures <ref type="bibr" target="#b23">(Healey et al., 2014;</ref><ref type="bibr" target="#b11">Dubuisson Duplessis et al., 2017)</ref>. Model output diversity. LLMs generally lack diversity compared to humans, especially remarkable for tasks demanding high levels of creativity,</p><p>Human Language Models POS tag n-gram Example POS tag n-gram Example n=3 (ADV, ADV, ADP) right along with (PRON, NOUN, ADJ) her voice soft n=4 (VERB, ADP, DET, NOUN) picking up the pieces (NOUN, CCONJ, NOUN, PRON) carvings and symbols that n=5 (DET, NOUN, ADP, DET, NOUN) the cackling of the fire (PRON, NOUN, VERB, ADP, NOUN) its feathers stained with blood n=6 DET, ADJ, NOUN, ADP, DET, NOUN) the old woman down the street (ADJ, NOUN, ADP, NOUN, CCONJ, NOUN) particular focus on time and space Table 3: Comparison of dependency tree distributions between humans and models.</p><p>such as story generation. Overall, the scores of different LLMs across tasks and diversity aspects tend to resemble each other due to the use of similar development procedures, architectures, and datasets. Compared to human outputs, LLMs generally exhibit less diversity by a considerable margin, with only a few exceptions. This is especially evident in the task of story generation, which demands the highest levels of creativity and freedom of expression, LLMs consistently lag behind humans in all three diversity aspects. In contrast, for tasks like next utterance LLMs surpass human references in both lexical and semantic diversity. This discrepancy arises because the Dai-lyDialog dataset focus on generic, everyday topics designed for English language learning, while LLMs, unconstrained by this context, frequently steer conversations toward more complex topics.</p><p>LLM comparisons. While the overall performance of the models appears to be similar, indepth comparisons showcase notable differences. Models pretrained on fewer tokens, such as Falcon and OLMo, consistently generate outputs with lower lexical diversity. Specifically, Falcon and OLMo are pretrained on 1.5T and 2.7T tokens, respectively, compared to Llama-3.1, which is trained on 15T tokens. However, this effect is not observed for syntactic or semantic diversity. Models with less strict data filtration exhibit greater diversity in creative tasks, such as story generation. For example, Qwen2.5, which filters data exclusively for quality, exhibits significantly higher diversity in story generation across all aspects compared to Llama-3.1, Gemma-2, and OLMo, whose data is extensively filtered for quality, privacy, and safety.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparing Syntactic Diversity Between Humans and Models</head><p>To further compare humans and models, we conduct a case study on syntactic diversity using dependency tree distribution. Syntactic diversity is chosen as it is less explored than lexical and semantic diversity. Moreover, syntactic patterns reflected by POS tag n-grams are more generalizable than lexicon n-grams and more interpretable than semantic embeddings. We adopt the Precision-Recall framework proposed by Le <ref type="bibr" target="#b32">Bronnec et al. (2024)</ref>. In our study, we substitute the original GPT-2 embeddings with the distribution of dependency trees, allowing for a fully interpretable analysis of syntactic structural differences between human and model outputs. Precision thus quantifies the percentage of modelgenerated dependency trees that fall within the neighborhood of human-written ones, while recall measures the percentage of human-written dependency trees within the neighborhood of modelgenerated ones. The method for computing the distance matrix between dependency trees is detailed in Section 3.1.1. All other hyper-parameters remain consistent with the original work (Le <ref type="bibr" target="#b32">Bronnec et al., 2024)</ref>.</p><p>Table <ref type="table">3</ref> presents the precision and recall scores for all evaluated models on the story generation task. The results reveal that all models exhibit near-perfect precision, indicating that almost all generated sentences are syntactically plausible. However, the recall scores are significantly lower across all models, highlighting their inability to cover the full diversity of human syntax. This points to a notable gap between models and humans in syntactic diversity for the story generation task requiring high creativity.</p><p>To further illustrate these findings, Table <ref type="table" target="#tab_2">2</ref>  examples of syntactic patterns (POS tag n-grams) that are frequently found in human dependency trees but are missing from the model-generated ones. Conversely, we also identify syntactic patterns that models over-generate but are less common in human outputs. Recent studies <ref type="bibr">(Shaib et al., 2024b)</ref> indicate that models often memorize syntactic templates encountered during pretraining, which are rarely overwritten during finetuning. This suggests that the observed gap in syntactic patterns may stem from a mismatch between pretraining and downstream task domains.</p><formula xml:id="formula_1">lists 0 B 2 B 4 B 6 B 8 B 1 0 B 1 2 B 2 0 B 3 1 B 6 2 B 1 2 9 B 2 5 7 B 5 3 0 B 1 0 2 7 B 2 0 0 0 B S F</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Factors Influencing LLM Diversity</head><p>In this section, we explore key factors that may influence the diversity of LLM outputs. The factors under consideration include decoding parameters, pretraining token counts, instruction tuning, model scale, and quantization. For decoding parameters and instruction tuning, we conduct experiments across all models. We employ OLMo for assessing the impact of pretraining token counts, which provides full access to its pretraining datasets, training code, and model weights at various checkpoints throughout its development. Since OLMo models are available in only two sizes, we additionally leverage Qwen2.5 models <ref type="bibr" target="#b59">(Yang et al., 2024)</ref> to investigate the effects of model scale and quantization.</p><p>All experiments in this section are conducted for story generation, selected for its minimal constraints and high emphasis on creativity, making it an ideal benchmark for linguistic diversity. Addi-tionally, as illustrated in Figure <ref type="figure">1</ref>, we observe that all models substantially underperform relative to human scores in terms of diversity metrics on the story generation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Impact of Training Stages</head><p>OLMo was pretrained on the Dolma corpus <ref type="bibr" target="#b51">(Soldaini et al., 2024)</ref> before going through supervised fine-tuning (SFT) on Tulu v2 <ref type="bibr" target="#b26">(Ivison et al., 2023)</ref> and direct preference optimization (DPO) <ref type="bibr" target="#b43">(Rafailov et al., 2023)</ref> on Ultrafeedback <ref type="bibr" target="#b9">(Cui et al., 2024)</ref>. DPO is performed on top of the model that underwent SFT. We take different checkpoints throughout the pretraining stage as well as the models after SFT and DPO. We use them to perform the story generation task and analyse the linguistic diversity of outputs. The results are presented in Figure <ref type="figure" target="#fig_3">4</ref>. Initially, lexical diversity is exceptionally high, as expected for an untrained model that generates random tokens. This metric drops sharply after the first checkpoint (2B tokens) but then gradually increases throughout the pretraining process, without reaching saturation. In contrast, syntactic diversity also experiences a sharp decline early on; however, it saturates much more quickly, fluctuating within a narrow range afterward. Semantic diversity shows a steady increase from the beginning but also saturates relatively quickly. These observations suggest that while increasing training data generally improves lexical diversity, alternative strategies are needed to enhance syntactic and semantic diversity.</p><p>In the later stages, beyond pretraining, SFT has minimal impact on any diversity metric, while DPO leads to a decrease in syntactic diversity and an increase in lexical diversity. We now explore the impact of instruction tuning across all models in greater detail.</p><p>Impact of instruction tuning. To complement the previous discussion, we compare the output diversity between the base versions and instructiontuned versions of all models in the context of story generation. The results, presented in Figure <ref type="figure" target="#fig_5">6</ref>, reveal a consistent pattern across models: instruction-tuned versions show higher lexical diversity compared to their base counterparts but exhibit reductions in syntactic and semantic diversity. Notably, the decline in syntactic diversity is more pronounced than that in semantic diversity.</p><p>These findings indicate that while additional training-regardless of the stage-enhances vocabulary richness, aligning models with human preferences tends to constrain them to a narrower range of grammatical structures and meanings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Impact of Decoding Parameters</head><p>Achieving a balance between quality and diversity in LLM outputs is a known challenge, as there is often a trade-off between these two aspects <ref type="bibr">(Cac-cia et al., 2020;</ref><ref type="bibr" target="#b61">Zhang et al., 2021)</ref>. While decoding strategies significantly affect the trade-off between n-gram metrics and perplexity, their influence on other facets of text generation, such as semantic and syntactic diversity, remains underexplored. Here, we investigate how varying the decoding temperature affects the outputs in the story generation task, with results visualized in Figure <ref type="figure" target="#fig_4">5</ref>.</p><p>Output quality is estimated based on their relevance to the inputs, using BERTScore as a metric.</p><p>The results show that increasing the temperature-making decoding less restrictiveleads to greater lexical diversity, with only a minor reduction in relevance to the prompts. It might be due to the creative nature of the stroy generation task that the quality-diversity trade-off is so subtle. For syntactic diversity, while most models show fluctuating performance within a certain range, some exhibit a clear downward trend, specially OLMo and Falcon, which are trained on significantly fewer tokens compared to the other models. However, no consistent trends are observed for semantic diversity metric as decoding parameters change. This aligns with the observations of <ref type="bibr" target="#b55">Tevet and Berant (2021)</ref>, which indicate that adjusting "decoding parameters" tends to affect the form of the text rather than its meaning. Furthermore, we note that, across most models, the relative ranking of diversity scores remains stable as the temperature varies. This suggests that conducting experiments with a fixed temperature is sufficient for consistent evaluation. Based on our findings, we set the temperature to 0.6 for all other experiments. Figure <ref type="figure" target="#fig_4">5</ref> shows that at a temperature of 0.6, the relevance to prompts remains relatively high while diversity scores significantly improve compared to lower temperatures. In fact, the creators of Llama-3.1 <ref type="bibr" target="#b10">(Dubey et al., 2024</ref>) also identified a temperature of 0.6 as achieving the optimal balance between creativity and coherence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Impact of Model Scale and Quantization</head><p>The Qwen2.5 model has been released in various sizes, ranging from 0.5B to 72B parameters. Due to computational resource constraints, we limit our exploration of linguistic diversity to models up to 32B parameters. The results are presented in Figure <ref type="figure" target="#fig_7">7</ref>. We observe that lexical diversity consistently increases with model size, while semantic diversity remains stable throughout. In contrast, syntactic diversity remains relatively stable overall but exhibits an initial increase followed by a decline, peaking at 7B parameters, indicating that scaling up is not always the solution to higher diversity.</p><p>We further investigate the impact of posttraining quantization on linguistic diversity. We quantize the Qwen2.5 models of various scales to 4-bit precision with the bitsandbytes library 2 , whereas the original models were run with bf16. As shown in Figure <ref type="figure" target="#fig_7">7</ref>, quantization does not affect semantic diversity but reduces both syntactic and  lexical diversity. The reduction in lexical diversity is more pronounced in smaller models, while the effect on syntactic diversity becomes more evident in larger models. This finding suggests that quantization has greater impact on the diversity of form rather than content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>Our study offers crucial insights into the linguistic diversity of current LLMs. By leveraging a comprehensive evaluation framework focused on lexical, syntactic, and semantic diversity, we provide a fresh perspective beyond traditional quality metrics. Our analysis reveals that, despite the impressive capabilities of LLMs in generating coherent and contextually appropriate text, there is a significant gap when it comes to replicating the linguistic richness characteristic of human language, especially for more creative tasks. Specifically, we observe that factors such as model scale, training data volume, and fine-tuning techniques critically influence diversity metrics.</p><p>These findings raise an important concern: as LLMs become more prevalent in content creation, their outputs may trend towards homogenization, risking a loss of linguistic richness. Notably, while instruction tuning improves lexical diversity, it constrains syntactic and semantic diversity, indicating a narrowing of expressive flexibility. Our research highlights the necessity of a more holistic and forward-looking approach in developing language models, one that prioritizes the preservation of linguistic diversity alongside optimizing performance metrics. We emphasize the need for novel strategies to balance the trade-offs between diversity and quality, ensuring that future models are capable of not only mimicking human language fluency but also maintaining its inherent diversity.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Pearson correlation matrix between diversity metrics and quality metrics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Pearson correlation matrix between different diversity metrics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Linguistic diversity metrics after different LLM training stages. The pretraining stage is broken into various steps with increasing token counts, which are presented on a log scale for visualization. Experiments are conducted with the OLMo model on the story generation task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Impact of decoding parameters. Experiments are conducted on the story generation task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Impact of instruction tuning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><figDesc>2 https://huggingface.co/docs/bitsandbytes/index</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Impact of model scale and quantization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Figure1: Lingustic diversity benchmarking results for NLG tasks detailed in Table1. ASG) WritingPrompts<ref type="bibr" target="#b13">(Fan et al., 2018)</ref> Story prompt shared by Reddit users Story continuation based on the prompt Summary of datasets, inputs, and outputs for benchmarked NLG tasks.</figDesc><table><row><cell>0.48 Semantic Diversity</cell></row><row><cell>0.47</cell></row><row><cell>0.46</cell></row><row><cell>0.45</cell></row><row><cell>0.44</cell></row><row><cell>0.43</cell></row><row><cell>0.42</cell></row><row><cell>0.41</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Examples of syntactic patterns favored by either humans or models are illustrated using n-grams of POS tags. Human patterns are derived from human dependency trees that are not within the model dependency tree neighborhoods, while model patterns have high frequency in model dependency trees and low frequency in human dependency trees.</figDesc><table><row><cell></cell><cell cols="5">Llama Mistral Qwen Gemma Falcon OLMo</cell></row><row><cell cols="2">Precision 99.20</cell><cell>99.20</cell><cell>99.47 99.07</cell><cell>99.63</cell><cell>99.73</cell></row><row><cell>Recall</cell><cell>35.20</cell><cell>65.87</cell><cell>75.27 37.97</cell><cell>75.00</cell><cell>39.40</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We thank <rs type="person">Professor Michalis Vazirgiannis</rs> for providing the computational resources that supported this project. This research was partially funded by the <rs type="grantNumber">ANR-23-CE23-0033-01</rs> <rs type="projectName">SINNet</rs> project and the <rs type="funder">ANR-TSIA</rs> HELAS chair.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_DXmGpVX">
					<idno type="grant-number">ANR-23-CE23-0033-01</idno>
					<orgName type="project" subtype="full">SINNet</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards robust NLG bias evaluation with syntactically-diverse prompts</title>
		<author>
			<persName><forename type="first">Arshiya</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.findings-emnlp.445</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2022</title>
		<meeting><address><addrLine>Abu Dhabi, United Arab Emirates</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="6022" to="6032" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Ebtesam</forename><surname>Almazrouei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamza</forename><surname>Alobeidli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdulaziz</forename><surname>Alshamsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Cappelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruxandra</forename><surname>Cojocaru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mérouane</forename><surname>Debbah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Étienne</forename><surname>Goffinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Hesslow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Launay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Malartic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.16867</idno>
		<title level="m">The falcon series of open language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Measuring lexical diversity in texts: The twofold length problem</title>
		<author>
			<persName><forename type="first">Yves</forename><surname>Bestgen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>Language Learning</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Findings of the 2014 workshop on statistical machine translation</title>
		<author>
			<persName><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Leveling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Pecina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herve</forename><surname>Saint-Amand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleš</forename><surname>Tamchyna</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/W14-3302</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Workshop on Statistical Machine Translation</title>
		<meeting>the Ninth Workshop on Statistical Machine Translation<address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="12" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Language gans falling short</title>
		<author>
			<persName><forename type="first">Massimo</forename><surname>Caccia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Caccia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Help me write a poem -instruction tuning as a vehicle for collaborative poetry writing</title>
		<author>
			<persName><forename type="first">Tuhin</forename><surname>Chakrabarty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishakh</forename><surname>Padmakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">He</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.emnlp-main.460</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Abu Dhabi</addrLine></address></meeting>
		<imprint>
			<publisher>United Arab Emirates. Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="6848" to="6863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Do Language Models Enjoy Their Own Stories? Prompting Large Language Models for Automatic Story Evaluation</title>
		<author>
			<persName><forename type="first">Cyril</forename><surname>Chhun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chloé</forename><surname>Clavel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1122" to="1142" />
		</imprint>
	</monogr>
	<note>Transactions of the</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A cross-linguistic perspective on syntactic complexity in l2 development: Syntactic elaboration and diversity</title>
		<author>
			<persName><forename type="first">Bastien</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clercq</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Housen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Modern Language Journal</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="315" to="334" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ultrafeedback: Boosting language models with scaled ai feedback</title>
		<author>
			<persName><forename type="first">Ganqu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingxiang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guotong</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Forty-first International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Abhimanyu</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Jauhri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kadian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Al-Dahle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aiesha</forename><surname>Letman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akhil</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Schelten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.21783</idno>
		<title level="m">The llama 3 herd of models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatic measures to characterise verbal alignment in human-agent interaction</title>
		<author>
			<persName><forename type="first">Chloé</forename><surname>Guillaume Dubuisson Duplessis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frédéric</forename><surname>Clavel</surname></persName>
		</author>
		<author>
			<persName><surname>Landragin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-5510</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue</title>
		<meeting>the 18th Annual SIGdial Meeting on Discourse and Dialogue<address><addrLine>Saarbrücken, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="71" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Diversity in the lexical and syntactic abilities of fluent aphasic speakers</title>
		<author>
			<persName><forename type="first">Susan</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roelien</forename><surname>Bastiaanse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Aphasiology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="117" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hierarchical neural story generation</title>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1082</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Mel-bourne</addrLine></address></meeting>
		<imprint>
			<publisher>Australia. Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="889" to="898" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Measuring lexical diversity in narrative discourse of people with aphasia</title>
		<author>
			<persName><surname>Fergadiotis</surname></persName>
		</author>
		<author>
			<persName><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><surname>West</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Speech-language Pathology</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="397" to="408" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bias and fairness in large language models: A survey</title>
		<author>
			<persName><forename type="first">Isabel</forename><forename type="middle">O</forename><surname>Gallegos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Barrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Md</forename><surname>Mehrab Tanjim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungchul</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nesreen</forename><forename type="middle">K</forename><surname>Ahmed</surname></persName>
		</author>
		<idno type="DOI">10.1162/coli_a_00524</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1097" to="1179" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">What comes next? evaluating uncertainty in neural text generators against human production variability</title>
		<author>
			<persName><forename type="first">Mario</forename><surname>Giulianelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joris</forename><surname>Baan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilker</forename><surname>Aziz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.emnlp-main.887</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2023 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="14349" to="14371" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Groeneveld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pete</forename><surname>Walsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshita</forename><surname>Bhagia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodney</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananya</forename><surname>Harsh Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamish</forename><surname>Ivison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Magnusson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><surname>Authur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghavi</forename><surname>Khyathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Chandu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanai</forename><surname>Dumas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuling</forename><surname>Elazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Merrill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Morrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Crystal</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentina</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhilasha</forename><surname>Pyatkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Ravichander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emma</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nishant</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Subramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><surname>Soldaini</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Noah A. Smith, and Hannaneh Hajishirzi. 2024. Olmo: Accelerating the science of language models</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">2024a. Do large language models have an english accent? evaluating and improving the naturalness of multilingual llms</title>
		<author>
			<persName><forename type="first">Yanzhu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Conia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zelin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saloni</forename><surname>Potdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.15956</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Michalis Vazirgiannis, and Chloé Clavel. 2024b. The curious decline of linguistic diversity: Training language models on synthetic text</title>
		<author>
			<persName><forename type="first">Yanzhu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guokan</forename><surname>Shang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2024.findings-naacl.228</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: NAACL 2024</title>
		<meeting><address><addrLine>Mexico City, Mexico</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="3589" to="3604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Measuring and improving semantic diversity of dialogue generation</title>
		<author>
			<persName><forename type="first">Seungju</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beomsu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Buru</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.findings-emnlp.66</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2022</title>
		<meeting><address><addrLine>Abu Dhabi, United Arab Emirates</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="934" to="950" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">XL-sum: Large-scale multilingual abstractive summarization for 44 languages</title>
		<author>
			<persName><forename type="first">Tahmid</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhik</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Md</forename><forename type="middle">Saiful</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazi</forename><surname>Mubasshir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan-Fang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong-Bin</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sohel Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rifat</forename><surname>Shahriyar</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.413</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4693" to="4703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Minhwa</forename><surname>Shirley Anugrah Hayati</surname></persName>
		</author>
		<author>
			<persName><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.09799</idno>
		<title level="m">Dheeraj Rajagopal, and Dongyeop Kang. 2023. How far can we extract diverse perspectives from large language models? criteria-based diversity prompting! arXiv preprint</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">T</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Healey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christine</forename><surname>Purver</surname></persName>
		</author>
		<author>
			<persName><surname>Howes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Divergence in dialogue</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">98598</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Measuring massive multitask language understanding</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Collin</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The curious case of neural text degeneration</title>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Camels in a changing climate: Enhancing lm adaptation with tulu 2</title>
		<author>
			<persName><forename type="first">Hamish</forename><surname>Ivison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentina</forename><surname>Pyatkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Albert Q Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Bamford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gianna</forename><surname>Bressand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lengyel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucile</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><surname>Saulnier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.06825</idno>
		<title level="m">Devendra Singh Chaplot, Diego de las Casas</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Mistral 7b</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Studies in language behavior: A program of research</title>
		<author>
			<persName><forename type="first">Wendell</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Monographs</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="1944">1944</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Large language models struggle to learn long-tail knowledge</title>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Kandpal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haikang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="15696" to="15707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Understanding the effects of RLHF on LLM generalisation and diversity</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Kirk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishita</forename><surname>Mediratta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoforos</forename><surname>Nalmpantis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jelena</forename><surname>Luketina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberta</forename><surname>Raileanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Improving diversity of demographic representation in large language models via collectivecritiques and self-voting</title>
		<author>
			<persName><forename type="first">Preethi</forename><surname>Lahoti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Blumm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghavendra</forename><surname>Kotikalapudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sahitya</forename><surname>Potluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qijun</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hansa</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Beirami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jilin</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.emnlp-main.643</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2023 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="10383" to="10405" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Exploring precision and recall to assess the quality and diversity of LLMs</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Le Bronnec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Verine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Negrevergne</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2024.acl-long.616</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 62nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Bangkok, Thailand</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="11418" to="11441" />
		</imprint>
	</monogr>
	<note>Yann Chevaleyre, and Alexandre Allauzen Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">DailyDialog: A manually labelled multi-turn dialogue dataset</title>
		<author>
			<persName><forename type="first">Yanran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqiang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuzi</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Joint Conference on Natural Language Processing</title>
		<meeting>the Eighth International Joint Conference on Natural Language Processing<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="986" to="995" />
		</imprint>
	</monogr>
	<note>Long Papers) Asian Federation of Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mapping the increasing use of LLMs in scientific papers</title>
		<author>
			<persName><forename type="first">Weixin</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaohui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haley</forename><surname>Lepp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenlong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuandong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hancheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First Conference on Language Modeling</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Geval: NLG evaluation using gpt-4 with better human alignment</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Iter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruochen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.emnlp-main.153</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2023 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2511" to="2522" />
		</imprint>
	</monogr>
	<note>Singapore. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">To diverge or not to diverge: A morphosyntactic perspective on machine translation vs human translation</title>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00645</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="355" to="371" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">On faithfulness and factuality in abstractive summarization</title>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Maynez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.173</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1906" to="1919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Linguistic features of writing quality</title>
		<author>
			<persName><forename type="first">Danielle</forename><forename type="middle">S</forename><surname>Mcnamara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">A</forename><surname>Crossley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">M</forename><surname>Mccarthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Written communication</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="86" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pointer sentinel mixture models</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Assessing Language Production in Children: Experimental Procedures. Assessing communicative behavior</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981">1981</date>
			<publisher>University Park Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Does writing with language models reduce content diversity?</title>
		<author>
			<persName><forename type="first">Vishakh</forename><surname>Padmakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">He</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Stanza: A python natural language processing toolkit for many human languages</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-demos.14</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="101" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Direct preference optimization: Your language model is secretly a reward model</title>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Rafailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Archit</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-seventh Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">COMET: A neural framework for MT evaluation</title>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Rei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ana</forename><forename type="middle">C</forename><surname>Farinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.213</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2685" to="2702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Sentence-BERT: Sentence embeddings using Siamese BERT-networks</title>
		<author>
			<persName><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1410</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3982" to="3992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Making monolingual sentence embeddings multilingual using knowledge distillation</title>
		<author>
			<persName><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.365</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4512" to="4525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">2024a. Standardizing the measurement of text diversity: A tool and a comparative analysis of scores</title>
		<author>
			<persName><forename type="first">Chantal</forename><surname>Shaib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Barrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiuding</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexa</forename><forename type="middle">F</forename><surname>Siu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byron</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Detection and measurement of syntactic templates in generated text</title>
		<author>
			<persName><forename type="first">Chantal</forename><surname>Shaib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanai</forename><surname>Elazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyi</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jessy</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byron</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2024.emnlp-main.368</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2024 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Miami, Florida, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="6416" to="6431" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Weisfeilerlehman graph kernels</title>
		<author>
			<persName><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Jan Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2539" to="2561" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Konstantinos Skianis, and Michalis Vazirgiannis</title>
		<author>
			<persName><forename type="first">Giannis</forename><surname>Siglidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giannis</forename><surname>Nikolentzos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stratis</forename><surname>Limnios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Giatsidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">54</biblScope>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Grakel: A graph kernel library in python</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Dolma: an open corpus of three trillion tokens for language model pretraining research</title>
		<author>
			<persName><forename type="first">Luca</forename><surname>Soldaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodney</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshita</forename><surname>Bhagia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><surname>Authur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Bogin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khyathi</forename><surname>Chandu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Dumas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanai</forename><surname>Elazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananya</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sachin</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Lucy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinxi</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Magnusson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Morrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Crystal</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhilasha</forename><surname>Ravichander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zejiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nishant</forename><surname>Subramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Walsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Groeneveld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2024.acl-long.840</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 62nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Bangkok, Thailand</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="15725" to="15788" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Semantic diversity in dialogue with natural language inference</title>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Stasaski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marti</forename><surname>Hearst</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.naacl-main.6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter</title>
		<meeting>the 2022 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Seattle, United States. Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="85" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">Gemma</forename><surname>Team</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgane</forename><surname>Riviere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shreya</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pier</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Sessa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cassidy</forename><surname>Hardin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Bhupatiraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léonard</forename><surname>Hussenot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Mesnard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bobak</forename><surname>Shahriari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.00118</idno>
		<title level="m">Alexandre Ramé, et al. 2024. Gemma 2: Improving open language models at a practical size</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Certain Language Skills in Children: Their Development and Interrelationships</title>
		<author>
			<persName><forename type="first">Mildred</forename><forename type="middle">C</forename><surname>Templin</surname></persName>
		</author>
		<idno type="DOI">10.5749/j.ctttv2st</idno>
		<imprint>
			<date type="published" when="1957">1957</date>
			<publisher>University of Minnesota Press</publisher>
			<biblScope unit="volume">26</biblScope>
		</imprint>
	</monogr>
	<note>ned -new edition edition</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Evaluating the evaluation of diversity in natural language generation</title>
		<author>
			<persName><forename type="first">Guy</forename><surname>Tevet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.eacl-main.25</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter</title>
		<meeting>the 16th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="326" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasmine</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prajjwal</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.09288</idno>
	</analytic>
	<monogr>
		<title level="m">Open foundation and fine-tuned chat models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Does human collaboration enhance the accuracy of identifying llm-generated deepfake texts?</title>
		<author>
			<persName><forename type="first">Adaku</forename><surname>Uchendu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jooyoung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thai</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongwon</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Human Computation and Crowdsourcing</title>
		<meeting>the AAAI Conference on Human Computation and Crowdsourcing</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="163" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">Yubo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueguang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ge</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuansheng</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhranil</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiguang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiming</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaran</forename><surname>Arulraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyan</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.01574</idno>
		<title level="m">Mmlu-pro: A more robust and challenging multi-task language understanding benchmark</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">An</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baosong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binyuan</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dayiheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.10671</idno>
		<title level="m">Qwen2 technical report</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Hierarchical text generation and planning for strategic dialogue</title>
		<author>
			<persName><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="5591" to="5599" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Trading off diversity and quality in natural language generation</title>
		<author>
			<persName><forename type="first">Hugh</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Duckworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Ippolito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Human Evaluation of NLP Systems (HumEval)</title>
		<meeting>the Workshop on Human Evaluation of NLP Systems (HumEval)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="25" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Simple fast algorithms for the editing distance between trees and related problems</title>
		<author>
			<persName><forename type="first">Kaizhong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dennis</forename><surname>Shasha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM journal on computing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1245" to="1262" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Bertscore: Evaluating text generation with bert</title>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varsha</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">SafetyBench: Evaluating the safety of large language models</title>
		<author>
			<persName><forename type="first">Zhexin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leqi</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lindong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongkang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanyu</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2024.acl-long.830</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 62nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Bangkok, Thailand</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="15537" to="15553" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
