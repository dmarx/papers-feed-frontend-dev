# Benchmarking Linguistic Diversity of Large Language Models

## Abstract

## 

The development and evaluation of Large Language Models (LLMs) has primarily focused on their task-solving capabilities, with recent models even surpassing human performance in some areas. However, this focus often neglects whether machine-generated language matches the human level of diversity, in terms of vocabulary choice, syntactic construction, and expression of meaning, raising questions about whether the fundamentals of language generation have been fully addressed. This paper emphasizes the importance of examining the preservation of human linguistic richness by language models, given the concerning surge in online content produced or aided by LLMs. We propose a comprehensive framework for evaluating LLMs from various linguistic diversity perspectives including lexical, syntactic, and semantic dimensions. Using this framework, we benchmark several state-of-the-art LLMs across all diversity dimensions, and conduct an indepth case study for syntactic diversity. Finally, we analyze how different development and deployment choices impact the linguistic diversity of LLM outputs.

* This work was partially done during the author's affiliation with École Polytechnique.

## Introduction

Recent Large Language Models (LLMs) have exhibited outstanding capabilities in generating both natural and formal language [(Brown et al., 2020;](#b4)[Touvron et al., 2023)](#b56), while also achieving human-level performance in language understanding, commonsense reasoning, and various other tasks [(Hendrycks et al., 2020)](#b24). This has led to evaluations that predominantly focus on these specific abilities [(Wang et al., 2024)](#b58). Meanwhile, other evaluation studies address well-recognized issues in LLMs, such as factuality [(Maynez et al., 2020)](#b37), safety [(Zhang et al., 2024)](#b64), and fairness [(Gallegos et al., 2024)](#b15), which remain focal points of ongoing research. However, there is a notable lack of attention paid to linguistic perspectives, particularly in diversity [(Guo et al., 2024b)](#), despite the fundamental objective of natural language generation being to produce outputs that are not only accurate but also diverse [(Tevet and Berant, 2021)](#b55).

Recent studies have highlighted concerns regarding the linguistic diversity of LLM outputs. By comparing human and model-generated content, researchers have shown that models frequently struggle to reflect the nuances and variations characteristic of human expression [(Shaib et al., 2024a;](#)[Giulianelli et al., 2023)](#b16). Additionally, these concerns are reinforced by findings that training language models on synthetic text can lead to a further decline in linguistic diversity [(Guo et al., 2024b)](#).

In fact, LLMs tend to be inherently conservative in producing diverse content. During training, models undergo homogenization to the most frequent patterns in the training data, where creative outlier narratives, views, styles, and knowledge are often underrepresented [(Kandpal et al., 2023)](#b29). Unlike models, human language production involves a complex interplay of factors that go beyond merely optimizing probabilities [(Holtzman et al., 2020)](#b25). It is therefore crucial to emphasize evaluating output diversity in language models and systematically consider these metrics to guide future model development and deployment decisions.

Currently, a principled and comprehensive evaluation framework for linguistic diversity is lacking in the literature [(Shaib et al., 2024a)](#). While some studies on Natural Language Generation (NLG) report diversity metrics, they typically focus on a single diversity aspect (e.g., lexical diver-sity [(Chakrabarty et al., 2022)](#b6)), often experimenting within a single domain and task (e.g., news summarization [(Shaib et al., 2024a)](#)). This narrow focus is problematic since diversity varies across aspects and depends on the domain [(Guo et al., 2024b)](#). Although some efforts have been made to assess the influence of reinforcement learning from human feedback (RLHF) on diversity [(Kirk et al., 2024)](#b30), the impact of other key design and development stages-such as model scale, quantization, and decoding strategy-remains unexplored. Additionally, there is a limited understanding of how LLMs develop the capability to generate diverse language through successive pretraining checkpoints. Ultimately, no study has benchmarked the diversity performance of state-of-theart LLMs across different aspects and domains.

In this work, we first establish a framework for evaluating linguistic diversity of LLM outputs on a corpus level. We then benchmark six prominent LLMs on five NLG tasks, and compare the diversity of their outputs across three different aspects: lexical, syntactic, and semantic. We further explore syntactic diversity, and conduct a case study comparing the distribution of dependency trees generated by LLMs and humans. Finally, we investigate how LLM output diversity changes across different development stages, and with varying decisions of deployment. The main research questions we address are as follows:

1. What are the key aspects of LLM output diversity, and how can they be evaluated? (See § 3) 2. How do state-of-the-art LLMs perform in terms of diversity across different tasks? (See § 5) 3. How does diversity change during each LLM development stage (e.g., pretraining, supervised fine-tuning (SFT), preference tuning)? (See § 6.1) 4. How do different design (e.g., model scale, training data) and deployment (e.g., decoding strategy, quantization) choices affect diversity? (See § 6.2 and § 6.3)

It is worth noting that we study linguistic diversity in a monolingual context, focusing on the English language. However, the evaluation methodology is language agnostic and could easily be extended to other languages, given that employed NLP toolkits (e.g., dependency parsers, sentence embeddings) exist for the language. Furthermore, our approach to analyzing the influence of various factors on LLM outputs is adaptable to other dimensions, such as linguistic naturalness [(Guo et al., 2024a)](#). The code is publicly available 1 .

## Related Work

In this section, we review methods for evaluating and analyzing linguistic diversity. We define linguistic diversity as the natural variation in human language across core linguistic properties, including vocabulary usage, grammatical structures, and semantic nuances. In contrast, a separate line of research focuses on socio-linguistic diversity [(Hayati et al., 2023;](#)[Lahoti et al., 2023)](#b31), which falls beyond the scope of our study.

## Evaluation of Human Language

Early metrics for linguistic diversity, proposed by linguists, were developed for studies of language acquisition and language disorder detection. For example, [Fergadiotis et al. (2013)](#b14) employed lexical diversity metrics to identify symptoms of aphasia, while [McNamara et al. (2010)](#b38) showed that both syntactic complexity and lexical diversity can predict essay quality. Another study by Clercq and Housen (2017) manually annotated a small corpus of texts produced by second language learners for syntactic features such as syntactic length and clause types, considering their variation as a diversity index. However, these metrics are limited to evaluating human-written texts and either focus exclusively on lexical diversity or lack scalability due to the need for manual annotation. The evaluation of linguistic diversity in modelgenerated language has emerged as a relatively recent focus of research. This development is driven, in part, by growing concerns over the increasing prevalence of model-generated or modelinfluenced content online, prompting questions about whether LLMs can maintain the linguistic richness characteristic of human language [(Guo et al., 2024b)](#). Moreover, advances in language generation quality have brought model outputs closer than ever to human-level coherence and plausibility. Assessing linguistic diversity becomes meaningful only when the generated text meets these standards. For example, a randomly initialized model might produce token sequences with high lexical diversity, but such outputs hold no practical value [(Uchendu et al., 2023)](#b57).

## Evaluation of Generated Language

To the best of our knowledge, [Tevet and Berant (2021)](#b55) were the first authors to systematically evaluate diversity in NLG. They proposed to create diversity metrics from any two-sentence similarity measure, defining diversity as the inverse of the mean similarity score across all unordered pairs. N-gram-based metrics were used to assess form diversity, while model-based metrics like Sentence-BERT similarity measured content diversity. They concluded that a notable disparity exists between automatic metrics and human judgment, and that human evaluation of diversity becomes challenging in sets with more than ten responses. Since then, additional metrics have been proposed to capture linguistic diversity, including semantic diversity metrics based on natural language inference [(Stasaski and Hearst, 2022)](#b52) or semantic entropy [(Han et al., 2022)](#b20), and syntactic diversity metrics derived from n-grams of Partof-Speech (POS) tags [(Giulianelli et al., 2023)](#b16) or graph similarity kernels of syntax trees [(Guo et al., 2024b)](#).

## Impact of LLMs on Linguistic Diversity

Diverging from the above research focused on developing methods to evaluate linguistic diversity, another line of work explores the the impact of LLMs on both human and model generated text, often demonstrating a decline in diversity. [Guo et al. (2024b)](#) showed that iteratively training LLMs on synthetic data generated by earlier models, leads to a consistent decline in lexical, syntactic, and semantic diversity, especially for tasks requiring high creativity. Similarly, Padmakumar and He (2024) reported a statistically significant reduction in linguistic diversity when humans write with InstructGPT. This reduction in linguistic diversity is also observed in other contexts: [Liang et al. (2024)](#b34) identified a significant frequency shift toward LLM-preferred words in academic writing, and [Luo et al. (2024)](#b36) reported reduced morphosyntactic diversity in machine translations compared to human translations.

Closely related to our work, [Kirk et al. (2024)](#b30) examined how SFT and preference tuning affect LLM generalization and diversity. They found that preference tuning substantially reduces lexical and semantic diversity compared to SFT. Our research also explores the factors that influence diversity while broadening the analysis to include a wider range of diversity aspects, models, tasks and factors. Moreover, our findings on the impact of preference tuning differ from those of [Kirk et al. (2024)](#b30), likely due to differences in task domain, accentuating the importance of contextualizing conclusions.

## Metrics for Linguistic Diversity

In this section, we present the three types of diversity central to our study: lexical, syntactic, and semantic diversity.

According to [Tevet and Berant (2021)](#b55), diversity can be viewed as having a hierarchical structure with various sub-aspects. Lexical diversity and syntactic diversity are considered sub-aspects of form diversity, while semantic diversity reflects content diversity. Although there are potentially other sub-aspects of linguistic diversity, such as style (register) diversity within form diversity, these elements are often more ambiguous, harder to quantify, and tend to overlap with the existing sub-aspects. For example, style diversity is partially captured through lexical and syntactic diversity, as they reflect preferences in vocabulary and grammatical structures. Consequently, we focus on the three aspects of diversity that are more clearly defined, easier to quantify, and have relatively low correlation with each other (further discussed in Section 5.1).

In terms of evaluation protocol, [Kirk et al. (2024)](#b30) distinguish between across-input diversity and per-input diversity. Across-input diversity refers to the diversity of outputs across different inputs, with only one output generated per input. In contrast, per-input diversity evaluates the capability of the model to produce diverse outputs for a single input.

In our study, we choose to measure across-input diversity, as we focus on linguistic patterns across a broad range of generations. Formally, given a set of generated outputs S = {s 1 , s 2 , . . . , s n }, we compute Div(S) differently depending on the aspect of diversity: for lexical diversity, S is treated as a set of n-grams, while for syntactic and semantic diversity, S is considered as a set of sentences.

In the following sections, we explain each diversity aspect and the specific metrics used.

## Lexical Diversity

Lexical diversity is a measure of the variety of vocabulary used within a text or set of texts. In essence, it assesses the richness or variability of word choices. High lexical diversity indicates a broad range of unique words, while low lexical diversity suggests repetitive or limited vocabulary.

We employ Unique-n [(Johnson, 1944;](#b28)[Templin, 1957)](#b54), established for evaluating lexical diversity. It is calculated as the ratio of unique n-grams to the total number of n-grams. When n = 1, it is equivalent to Type-Token Ratio [(Johnson, 1944;](#b28)[Templin, 1957)](#b54). We report the average Uniquen across unigrams, bigrams, and trigrams. Originally used in child language research, Unique-n is useful for assessing language development, where a lower value might indicate limited lexical variety [(Miller, 1981)](#b40). We use the global Unique-n measure rather than the moving average Uniquen because we are interested in the overall diversity capabilities of LLMs across different inputs rather than their performance on individual inputs. Moving average methods might miss global lexical repetitions due to their localized nature (Bestgen, 2023). To mitigate the influence of output length on Unique-n, we always randomly choose 40K samples to constitute the set of n-grams for each n.

## Syntactic Diversity

Syntactic diversity refers to the range and variety of sentence structures used in a text or set of texts. It assesses how flexibly and creatively different grammatical structures, such as phrases, clauses, and sentence types, are employed. High syntactic diversity suggests varied sentence forms, while low syntactic diversity indicates repetitive or simplistic sentence structures. Syntactic diversity is a crucial but often neglected aspect of language. Exposure to a variety of syntactic structures helps language learners and models develop a richer understanding of language [(Aggarwal et al., 2022)](#b0). Diverse syntactic forms enhance expressiveness and subtlety in text, impacting its style and tone [(Edwards and Bastiaanse, 1998)](#b12). While research on syntactic diversity exists, it typically relies on manual annotation, which can be both costly and error-prone [(Clercq and Housen, 2017)](#b8).

To address this limitation, we employ a graphbased metric for quantifying syntactic diversity [(Guo et al., 2024b)](#). This metric relies on a neural parser [(Qi et al., 2020)](#b42) to generate dependency trees from sentences, following the universal dependencies framework. These trees are converted into graph representations, where nodes represent words and edges denote dependency relationships. The nodes are labeled by the PoS tag of each word. The Weisfeiler-Lehman (WL) graph kernel [(Shervashidze et al., 2011;](#b49)[Siglidis et al., 2020)](#b50) is then applied to map these graphs into a vector space. This kernel, based on the WL isomorphism test, positions structurally similar graphs closer together in the vector space. Syntactic diversity is then measured using the average pairwise distance between graphs, formalized as: Div syn (S) = 1

( n 2 ) 1≤i<j≤n W L(s i , s j ). Alternatively, the pairwise distances between the dependency trees could also be measured by the tree editing distance [(Zhang and Shasha, 1989)](#b62). However, the algorithm to compute the tree editing distance has much higher computational complexity and is thus not scalable to a large set of texts.

## Semantic Diversity

Semantic diversity refers to the range and variety of meanings or ideas conveyed within a text or set of texts. It evaluates how broadly and uniquely different concepts, topics, or ideas are expressed, reflecting the depth and scope of the content. High semantic diversity suggests a text covers diverse ideas or meanings, while low semantic diversity implies repetition or a narrow focus on specific concepts. Recent studies [(Tevet and Berant, 2021;](#b55)[Stasaski and Hearst, 2022)](#b52) have pointed out that traditional lexical metrics may not fully capture semantic diversity. Similar words can convey different meanings, and different words can convey similar meanings [(Yarats and Lewis, 2018)](#b60).

To address this, we first convert sentences into semantically meaningful embeddings using Sentence-BERT [(Reimers and Gurevych, 2019)](#b45). Semantic diversity is then quantified as the dispersion of these embeddings in the semantic space, measured by the average pairwise cosine distance (scaled to the range [0, 1]) between all embedding vectors: Div sem (S) =

$1 ( n 2 ) 1≤i<j≤n 1+dcos 2 (e(s i ), e(s j ))$, where e represents Sentence-BERT embeddings.

## Settings for Diversity Benchmarking

We outline the tasks, datasets, and models used to establish our linguistic diversity benchmark. Generation tasks. To effectively compare the linguistic diversity of LLM outputs across various scenarios, we selected five tasks with progressively increasing levels of "creativity": lan-   [1](#tab_0) outlines the inputs, outputs, and datasets associated with each task. Note that the input is combined with a task-specific instruction (e.g., "Continue the following story:" for story generation) to construct the final prompt.

For each task, we randomly select 10K samples from the original dataset. To obtain the benchmark results, we decode the outputs for all models and tasks using a combination of nucleus sampling (t=0.6) and top-k sampling (k=0.9). We further analyze in Section 6.2 the impact of different decoding parameters on output diversity.

Large language models. We evaluate the following families of models: Llama [(Dubey et al., 2024)](#b10), [Mistral (Jiang et al., 2023)](#), Olmo (Groeneveld et al., 2024), Gemma [(Team et al., 2024)](#), Qwen [(Yang et al., 2024)](#b59), Falcon [(Almazrouei et al., 2023)](#b1). To ensure comparability, we select the latest version of each model family that is closest in scale to 7 billion parameters. The scale selected for each model is specified in the legend of Table [1](#tab_0). We purposefully include models devel-oped by organizations from different countries to be culturally inclusive. For language modeling, we use base models. For all other tasks, we employ instruction-tuned versions.

## Results for Diversity Benchmarking

Figure [1](#) presents the benchmarking results of linguistic diversity across various tasks. Round dots represent the diversity of model outputs, while solid lines represent human reference outputs. Dashed lines depict the diversity of task-specific inputs (as detailed in Table [1](#tab_0)), reflecting the conditions under which the outputs were generated. Tasks are organized in ascending order of human reference diversity for each aspect of diversity. For the machine translation task, the inputs are in French; hence, semantic diversity is measured using a multilingual SentenceBERT [(Reimers and Gurevych, 2020)](#b46), and syntactic diversity is evaluated with a French-specific dependency parser. As a result, these scores may not be directly comparable to those for English. We analyze the results in Figure [1](#) in Sections 5.2 and 5.3.

In this section, we first analyze metric correla-  tions in Section 5.1, then compare diversity scores across tasks and models in Section 5.2. Finally, we perform a case study on syntactic diversity in story generation, comparing human and model outputs in Section 5.3.

## Correlation Study

Correlation between diversity and quality. As noted in Section 2.1, diversity matters only if the text is of plausible quality. Figure [2](#fig_1) illustrates the correlation between diversity and quality in model outputs, using task-specific automatic metrics as quality indicators. For the language modeling task, perplexity is used to evaluate the model's performance on reference text continuations. For machine translation, we use COMET [(Rei et al., 2020)](#b44), which takes into account both the source text and reference translation. For the remaining three tasks, BERTScore [(Zhang et al., 2020)](#b63) is used to measure the relevance between inputs and outputs. However, for these last three tasks with a subjective nature, this relevance score serves only as a proxy for quality, as automatic metrics for such tasks generally exhibit low correlation with human judgments [(Chhun et al., 2024;](#b7)[Liu et al., 2023)](#b35).

Our results show a positive correlation between quality and lexical as well as semantic diversity in model outputs. In contrast, syntactic diversity often exhibits negative correlations, where higher syntactic diversity is associated with lower quality scores. This may be attributed to the tested domains inherently exhibiting low ground-truth syntactic diversity (e.g., in language modeling) or to the limitations of quality metrics in recognizing the value of syntactic variation (e.g., in summarization, automatic story generation, and next utterance generation). These findings highlight the need to report diversity metrics alongside quality metrics for comprehensive evaluation.

Correlation between diversity aspects. The correlations between different diversity aspects are shown in Figure [3](#fig_2), revealing a moderate positive correlation between syntactic and semantic diversity (0.55). However, lexical diversity shows a weak positive relationship with syntactic diversity (0.13) and a slight negative correlation with semantic diversity (-0.14), indicating that the richness of vocabulary is independent from the variety of grammatical structures and meaning.

## Comparison Across Tasks and Models

We now examine the results in Figure [1](#) to assess human diversity results across tasks, compare model diversity against human diversity, and finally evaluate the diversity performance across different models. Human output diversity. Human-level diversity varies across tasks, with no clear correlation observed among different aspects. Notably, utterances in human dialogue exhibit the lowest lexical diversity and the highest syntactic diversity, unlike the written text present in the remaining four categories. The low lexical diversity may be attributed to the conversations being specifically scripted for English learners to practice daily-life dialog. These dialogs focus on generic topics, leading to a limited range of vocabulary. In contrast, the high syntactic diversity can be explained by the inherent spontaneity of conversational language, where different speakers tend to vary significantly in their use of syntactic structures [(Healey et al., 2014;](#b23)[Dubuisson Duplessis et al., 2017)](#b11). Model output diversity. LLMs generally lack diversity compared to humans, especially remarkable for tasks demanding high levels of creativity,

Human Language Models POS tag n-gram Example POS tag n-gram Example n=3 (ADV, ADV, ADP) right along with (PRON, NOUN, ADJ) her voice soft n=4 (VERB, ADP, DET, NOUN) picking up the pieces (NOUN, CCONJ, NOUN, PRON) carvings and symbols that n=5 (DET, NOUN, ADP, DET, NOUN) the cackling of the fire (PRON, NOUN, VERB, ADP, NOUN) its feathers stained with blood n=6 DET, ADJ, NOUN, ADP, DET, NOUN) the old woman down the street (ADJ, NOUN, ADP, NOUN, CCONJ, NOUN) particular focus on time and space Table 3: Comparison of dependency tree distributions between humans and models.

such as story generation. Overall, the scores of different LLMs across tasks and diversity aspects tend to resemble each other due to the use of similar development procedures, architectures, and datasets. Compared to human outputs, LLMs generally exhibit less diversity by a considerable margin, with only a few exceptions. This is especially evident in the task of story generation, which demands the highest levels of creativity and freedom of expression, LLMs consistently lag behind humans in all three diversity aspects. In contrast, for tasks like next utterance LLMs surpass human references in both lexical and semantic diversity. This discrepancy arises because the Dai-lyDialog dataset focus on generic, everyday topics designed for English language learning, while LLMs, unconstrained by this context, frequently steer conversations toward more complex topics.

LLM comparisons. While the overall performance of the models appears to be similar, indepth comparisons showcase notable differences. Models pretrained on fewer tokens, such as Falcon and OLMo, consistently generate outputs with lower lexical diversity. Specifically, Falcon and OLMo are pretrained on 1.5T and 2.7T tokens, respectively, compared to Llama-3.1, which is trained on 15T tokens. However, this effect is not observed for syntactic or semantic diversity. Models with less strict data filtration exhibit greater diversity in creative tasks, such as story generation. For example, Qwen2.5, which filters data exclusively for quality, exhibits significantly higher diversity in story generation across all aspects compared to Llama-3.1, Gemma-2, and OLMo, whose data is extensively filtered for quality, privacy, and safety.

## Comparing Syntactic Diversity Between Humans and Models

To further compare humans and models, we conduct a case study on syntactic diversity using dependency tree distribution. Syntactic diversity is chosen as it is less explored than lexical and semantic diversity. Moreover, syntactic patterns reflected by POS tag n-grams are more generalizable than lexicon n-grams and more interpretable than semantic embeddings. We adopt the Precision-Recall framework proposed by Le [Bronnec et al. (2024)](#b32). In our study, we substitute the original GPT-2 embeddings with the distribution of dependency trees, allowing for a fully interpretable analysis of syntactic structural differences between human and model outputs. Precision thus quantifies the percentage of modelgenerated dependency trees that fall within the neighborhood of human-written ones, while recall measures the percentage of human-written dependency trees within the neighborhood of modelgenerated ones. The method for computing the distance matrix between dependency trees is detailed in Section 3.1.1. All other hyper-parameters remain consistent with the original work (Le [Bronnec et al., 2024)](#b32).

Table [3](#) presents the precision and recall scores for all evaluated models on the story generation task. The results reveal that all models exhibit near-perfect precision, indicating that almost all generated sentences are syntactically plausible. However, the recall scores are significantly lower across all models, highlighting their inability to cover the full diversity of human syntax. This points to a notable gap between models and humans in syntactic diversity for the story generation task requiring high creativity.

To further illustrate these findings, Table [2](#tab_2)  examples of syntactic patterns (POS tag n-grams) that are frequently found in human dependency trees but are missing from the model-generated ones. Conversely, we also identify syntactic patterns that models over-generate but are less common in human outputs. Recent studies [(Shaib et al., 2024b)](#) indicate that models often memorize syntactic templates encountered during pretraining, which are rarely overwritten during finetuning. This suggests that the observed gap in syntactic patterns may stem from a mismatch between pretraining and downstream task domains.

$lists 0 B 2 B 4 B 6 B 8 B 1 0 B 1 2 B 2 0 B 3 1 B 6 2 B 1 2 9 B 2 5 7 B 5 3 0 B 1 0 2 7 B 2 0 0 0 B S F$
## Factors Influencing LLM Diversity

In this section, we explore key factors that may influence the diversity of LLM outputs. The factors under consideration include decoding parameters, pretraining token counts, instruction tuning, model scale, and quantization. For decoding parameters and instruction tuning, we conduct experiments across all models. We employ OLMo for assessing the impact of pretraining token counts, which provides full access to its pretraining datasets, training code, and model weights at various checkpoints throughout its development. Since OLMo models are available in only two sizes, we additionally leverage Qwen2.5 models [(Yang et al., 2024)](#b59) to investigate the effects of model scale and quantization.

All experiments in this section are conducted for story generation, selected for its minimal constraints and high emphasis on creativity, making it an ideal benchmark for linguistic diversity. Addi-tionally, as illustrated in Figure [1](#), we observe that all models substantially underperform relative to human scores in terms of diversity metrics on the story generation task.

## Impact of Training Stages

OLMo was pretrained on the Dolma corpus [(Soldaini et al., 2024)](#b51) before going through supervised fine-tuning (SFT) on Tulu v2 [(Ivison et al., 2023)](#b26) and direct preference optimization (DPO) [(Rafailov et al., 2023)](#b43) on Ultrafeedback [(Cui et al., 2024)](#b9). DPO is performed on top of the model that underwent SFT. We take different checkpoints throughout the pretraining stage as well as the models after SFT and DPO. We use them to perform the story generation task and analyse the linguistic diversity of outputs. The results are presented in Figure [4](#fig_3). Initially, lexical diversity is exceptionally high, as expected for an untrained model that generates random tokens. This metric drops sharply after the first checkpoint (2B tokens) but then gradually increases throughout the pretraining process, without reaching saturation. In contrast, syntactic diversity also experiences a sharp decline early on; however, it saturates much more quickly, fluctuating within a narrow range afterward. Semantic diversity shows a steady increase from the beginning but also saturates relatively quickly. These observations suggest that while increasing training data generally improves lexical diversity, alternative strategies are needed to enhance syntactic and semantic diversity.

In the later stages, beyond pretraining, SFT has minimal impact on any diversity metric, while DPO leads to a decrease in syntactic diversity and an increase in lexical diversity. We now explore the impact of instruction tuning across all models in greater detail.

Impact of instruction tuning. To complement the previous discussion, we compare the output diversity between the base versions and instructiontuned versions of all models in the context of story generation. The results, presented in Figure [6](#fig_5), reveal a consistent pattern across models: instruction-tuned versions show higher lexical diversity compared to their base counterparts but exhibit reductions in syntactic and semantic diversity. Notably, the decline in syntactic diversity is more pronounced than that in semantic diversity.

These findings indicate that while additional training-regardless of the stage-enhances vocabulary richness, aligning models with human preferences tends to constrain them to a narrower range of grammatical structures and meanings.

## Impact of Decoding Parameters

Achieving a balance between quality and diversity in LLM outputs is a known challenge, as there is often a trade-off between these two aspects [(Cac-cia et al., 2020;](#)[Zhang et al., 2021)](#b61). While decoding strategies significantly affect the trade-off between n-gram metrics and perplexity, their influence on other facets of text generation, such as semantic and syntactic diversity, remains underexplored. Here, we investigate how varying the decoding temperature affects the outputs in the story generation task, with results visualized in Figure [5](#fig_4).

Output quality is estimated based on their relevance to the inputs, using BERTScore as a metric.

The results show that increasing the temperature-making decoding less restrictiveleads to greater lexical diversity, with only a minor reduction in relevance to the prompts. It might be due to the creative nature of the stroy generation task that the quality-diversity trade-off is so subtle. For syntactic diversity, while most models show fluctuating performance within a certain range, some exhibit a clear downward trend, specially OLMo and Falcon, which are trained on significantly fewer tokens compared to the other models. However, no consistent trends are observed for semantic diversity metric as decoding parameters change. This aligns with the observations of [Tevet and Berant (2021)](#b55), which indicate that adjusting "decoding parameters" tends to affect the form of the text rather than its meaning. Furthermore, we note that, across most models, the relative ranking of diversity scores remains stable as the temperature varies. This suggests that conducting experiments with a fixed temperature is sufficient for consistent evaluation. Based on our findings, we set the temperature to 0.6 for all other experiments. Figure [5](#fig_4) shows that at a temperature of 0.6, the relevance to prompts remains relatively high while diversity scores significantly improve compared to lower temperatures. In fact, the creators of Llama-3.1 [(Dubey et al., 2024](#b10)) also identified a temperature of 0.6 as achieving the optimal balance between creativity and coherence.

## Impact of Model Scale and Quantization

The Qwen2.5 model has been released in various sizes, ranging from 0.5B to 72B parameters. Due to computational resource constraints, we limit our exploration of linguistic diversity to models up to 32B parameters. The results are presented in Figure [7](#fig_7). We observe that lexical diversity consistently increases with model size, while semantic diversity remains stable throughout. In contrast, syntactic diversity remains relatively stable overall but exhibits an initial increase followed by a decline, peaking at 7B parameters, indicating that scaling up is not always the solution to higher diversity.

We further investigate the impact of posttraining quantization on linguistic diversity. We quantize the Qwen2.5 models of various scales to 4-bit precision with the bitsandbytes library 2 , whereas the original models were run with bf16. As shown in Figure [7](#fig_7), quantization does not affect semantic diversity but reduces both syntactic and  lexical diversity. The reduction in lexical diversity is more pronounced in smaller models, while the effect on syntactic diversity becomes more evident in larger models. This finding suggests that quantization has greater impact on the diversity of form rather than content.

## Conclusion

Our study offers crucial insights into the linguistic diversity of current LLMs. By leveraging a comprehensive evaluation framework focused on lexical, syntactic, and semantic diversity, we provide a fresh perspective beyond traditional quality metrics. Our analysis reveals that, despite the impressive capabilities of LLMs in generating coherent and contextually appropriate text, there is a significant gap when it comes to replicating the linguistic richness characteristic of human language, especially for more creative tasks. Specifically, we observe that factors such as model scale, training data volume, and fine-tuning techniques critically influence diversity metrics.

These findings raise an important concern: as LLMs become more prevalent in content creation, their outputs may trend towards homogenization, risking a loss of linguistic richness. Notably, while instruction tuning improves lexical diversity, it constrains syntactic and semantic diversity, indicating a narrowing of expressive flexibility. Our research highlights the necessity of a more holistic and forward-looking approach in developing language models, one that prioritizes the preservation of linguistic diversity alongside optimizing performance metrics. We emphasize the need for novel strategies to balance the trade-offs between diversity and quality, ensuring that future models are capable of not only mimicking human language fluency but also maintaining its inherent diversity.

![Figure 2: Pearson correlation matrix between diversity metrics and quality metrics.]()

![Figure 3: Pearson correlation matrix between different diversity metrics.]()

![Figure 4: Linguistic diversity metrics after different LLM training stages. The pretraining stage is broken into various steps with increasing token counts, which are presented on a log scale for visualization. Experiments are conducted with the OLMo model on the story generation task.]()

![Figure 5: Impact of decoding parameters. Experiments are conducted on the story generation task.]()

![Figure 6: Impact of instruction tuning.]()

![2 https://huggingface.co/docs/bitsandbytes/index]()

![Figure 7: Impact of model scale and quantization.]()

![Figure1: Lingustic diversity benchmarking results for NLG tasks detailed in Table1. ASG) WritingPrompts(Fan et al., 2018) Story prompt shared by Reddit users Story continuation based on the prompt Summary of datasets, inputs, and outputs for benchmarked NLG tasks.]()

![Examples of syntactic patterns favored by either humans or models are illustrated using n-grams of POS tags. Human patterns are derived from human dependency trees that are not within the model dependency tree neighborhoods, while model patterns have high frequency in model dependency trees and low frequency in human dependency trees.]()

