# Evolving Deeper LLM Thinking

## Abstract

## 

We explore an evolutionary search strategy for scaling inference time compute in Large Language Models. The proposed approach, Mind Evolution, uses a language model to generate, recombine and refine candidate responses. The proposed approach avoids the need to formalize the underlying inference problem whenever a solution evaluator is available. Controlling for inference cost, we find that Mind Evolution significantly outperforms other inference strategies such as Best-of-N and Sequential Revision in natural language planning tasks. In the TravelPlanner and Natural Plan benchmarks, Mind Evolution solves more than 98% of the problem instances using Gemini 1.5 Pro without the use of a formal solver.

## Introduction

How can a large language model (LLM) be guided to think deeper about a complex problem and leverage inference time compute to improve its problem solving ability? Prior research has investigated various strategies for leveraging inference time compute, such as chain-of-thought [[41,](#b40)[21]](#b20), self-consistency [[39]](#b38), sequential revision based on feedback [[36,](#b35)[30,](#b29)[8,](#b7)[19,](#b18)[1]](#b0), and search guided by auxiliary verifiers or evaluators [[43]](#b42). When a solution evaluator is available, search strategies have an advantage of being able to reliably improve problem solving ability with increased compute. For example, methods such as Bestof-N [[4,](#b3)[24,](#b23)[25]](#b24) and tree search [[37]](#b36) naturally exploit additional compute to explore a larger set of solution candidates, thereby increasing the probability of finding a successful solution.

To better exploit inference time compute, we propose an evolutionary search strategy for LLMs that combines free-flowing stochastic exploration with large-scale iterative refinement. We refer to this approach as Mind Evolution. As illustrated in Figure [1](#fig_5), Mind Evolution is a genetic search strategy that evolves a diverse population of candidate solutions, leveraging an LLM to generate, recombine and refine solution candidates based on feedback from an evaluator. The overall process is analogous to combining divergent thinking (free-flowing parallel idea exploration) with convergent thinking (idea evaluation and selection), considered as hallmarks of intelligent problem solving behavior [[14]](#b13).

Unlike Best-of-N, which searches broadly by generating independent candidates for evaluation, Mind Evolution searches both broadly and deeply, exploring a diverse set of candidates and refining the most promising alternatives. Unlike sequential reasoning approaches, such as self-refinement or tree search [[37,](#b36)[25]](#b24), which require evaluation of individual reasoning steps, Mind Evolution performs global refinement of complete solutions, and therefore only requires a global solution evaluator rather than a stepwise process reward. Also, typical of evolutionary methods, Mind Evolution can be easily parallelized.

There has been prior work on combining evolutionary search with LLMs, primarily in the literature on evolutionary program generation [[34,](#b33)[17,](#b16)[29,](#b28)[23,](#b22)[6]](#b5). However, this prior work focuses on searching through formal program spaces, using guidance from execution feedback or code explanation. By contrast, Mind Evolution is not restricted to searching in a formal space. This allows Mind Evolution to be applied to problems that are not formalized, or remain difficult to formalize, as long as a programmatic solution evaluator is available. In particular, we focus on natural language planning tasks where candidate solutions can still be automatically parsed, evaluated and critiqued using an implementable oracle evaluator. This approach exploits the observation that it is often easier to evaluate the quality of a candidate solution than it is to generate good solutions for a given problem [[11]](#b10).

In the domain of natural language planning, we consider the TravelPlanner [[42]](#b41) and Natural Plan [[47]](#b46) benchmarks, where constraint satisfaction problems are expressed in natural language without any explicit formalization of the underlying objectives, constraints or variables. These problems require a set of interconnected decisions that satisfy a set of global and local constraints. For example, in TravelPlanner, a travel plan should be produced that respects various accommodation and dinning constraints, while also considering budget limitations and other preferences, all expressed solely in natural language. To date, LLMs have yet to achieve good performance on these tasks Feedbacks from Evaluator: 。The cost exceeds budget limit 。Dining preference is not met ...

## Terminate at

(a) A valid solution (b) Gen N (max compute budget)

Figure [1](#fig_5) | Mind Evolution is a genetic-based evolutionary search strategy that operates in natural language space. The figure illustrates how Mind Evolution evolves a population of solution candidates toward higher quality candidates for a travel planning task. The candidate population is improved through an iterative process, where an LLM is used to recombine and refine candidates in each iteration.

without the aid of formal solvers [[16]](#b15). For example, Gemini 1.5 Flash and o1-preview only achieve a success rate of 5.6% and 11.7% on TravelPlanner respectively, while for the Meeting Planning domain in Natural Plan, they respectively only achieve 20.8% and 44.2%. Even exploiting Best-of-N over 800 independently generated responses, Gemini 1.5 Flash still only achieves 55.6% success on TravelPlanner and 69.4% on Meeting Planning. In this paper, we show that exploration and refinement with evolutionary search can notably improve problem solving ability. In particular, when controlling for inference time compute, Mind Evolution allows Gemini 1.5 Flash to achieve a 95.6% success rate on TravelPlanner and 85.0% on Meeting Planning. We further experiment with a two-stage approach, where any unsolved problem instances are subsequently tackled by Mind Evolution with Gemini 1.5 Pro, which leads to 100% success on TravelPlanner and 98.4% on Meeting Planning. All of the experiments in this paper only use off-the-shelf LLMs without any finetuning.

To our knowledge, the only prior work that achieves comparable performance on the TravelPlanner benchmark is [[16]](#b15), which leverages an auxiliary formal solver and requires the LLM to first translate a given problem instance into an equivalent formalization. In general, it takes significant effort and expertise to correctly formalize a problem expressed in natural language; prompting an LLM to correctly perform such a translation requires at least as much domain expertise. Mind Evolution removes this constraint by directly optimizing solutions in the space of natural language.

Finally, we introduce a new benchmark problem, StegPoet, that involves encoding a hidden message in a generated essay, story or poem. This form of stenography [[33]](#b32) is difficult to formalize and solve, yet a hidden message detector can still be implemented to programmatically guide the search. Our motivation is to demonstrate the applicability of search beyond natural language domains that can be easily formalized. We find that Mind Evolution allows Gemini 1.5 Pro to achieve a success rate of 87% in this task.

## Related Work

Pairing LLMs with Evolutionary Search In addition to the program generation studies discussed in Section 1, several recent works have explored combining LLMs and evolution for numerical optimization [[26,](#b25)[3]](#b2) and combinatorial optimization [[28,](#b27)[44]](#b43). The problem spaces we tackle in this work, such as natural language planning, can also be viewed as combinatorial optimization problems -optimizing plans subject to constraints specified in natural language. In contrast to these previous studies, we focus on evolving solutions in natural language spaces instead of formal spaces. This removes the requirement of task formalization, which requires significant effort and expert knowledge for each task instance.

Other works have also applied evolutionary search to prompt optimization, with the goal of improving performance on target tasks [[45,](#b44)[10,](#b9)[15]](#b14). Among these, EvoAgent [[45]](#b44) also evaluated their approach on the TravelPlanner benchmark. In contrast to our work, which performs evolutionary search directly on plans, EvoAgent evolves new LLM agents to form a multi-agent system for problem solving. Their best success rate on the TravelPlanner validation set was 7.2% with GPT-4, while our approach achieved over 95% with Gemini 1.5 Flash.

## Pairing LLMs with Evaluators

In this work, we evaluate solutions with program-based evaluators during the evolutionary search. The idea of integrating execution-based evaluators in the inference loop has been widely adopted in the literature of code generation, where the execution environment provides feedback for the LLM to fix bugs in the generated code [[7,](#b6)[22,](#b21)[27,](#b26)[46,](#b45)[8,](#b7)[17,](#b16)[29,](#b28)[23,](#b22)[6,](#b5)[36]](#b35).

Other prior work has also considered using learned verifiers, reward models, or self-evaluation for response refinement [[20,](#b19)[30]](#b29), search [[37,](#b36)[4,](#b3)[9,](#b8)[43,](#b42)[35]](#b34), and improving model learning [[40,](#b39)[25,](#b24)[32,](#b31)[1]](#b0). These approaches can often be applied to wider domains and free-form solutions, but learned feedback models or self-evaluators can be noisy and are not perfectly reliable. We leave consideration of such approximate feedback mechanisms for future work.

## Method

Mind Evolution employs a genetic search strategy, combined with an LLM and a tailored set of prompts, to orchestrate an efficient search for solutions to natural language planning tasks. Before describing Mind Evolution in detail, we first provide a brief overview of language-based genetic algorithms.

## Language-based Genetic Algorithm Overview

Genetic algorithms [[18,](#b17)[12,](#b11)[31]](#b30) are a meta-heuristic inspired by natural selection. In a genetic algorithm, a population of candidate solutions is evolved toward populations that contain a greater proportion of higher quality individuals with respect to a target optimization objective. Such an objective is also often referred to as the "fitness" function. Each individual candidate has a genetic representation that can be mutated and recombined with others.

Evolutionary search usually begins with a population of independently generated candidate solutions. In each generation, the fitness of every individual is evaluated with respect to the target objective. Candidates are then stochastically selected for reproduction based on their fitness ("selection"). In reproduction, the genetic representations of selected parents are combined ("crossover") and potentially altered ("mutation") to produce new child solutions. Such a process creates the next generation of children, which then enter the population. Population fitness generally increases over successive generations, as parents with greater fitness are more likely to be selected for recombination.

## Island Model

To sustain diversity in an evolving population it is also helpful to introduce an island model [[38,](#b37)[5]](#b4), where distinct sub-populations ("islands") are created and evolved independently between "migration" and "island reset" events that occur at specified frequencies. For a migration operation, the solutions on one island are stochastically chosen based on fitness to migrate to an adjacent island. For an Island Reset operation, the populations on islands with low overall fitness are replaced by strong solutions from the global population, which also has a selection effect. The island model has been adopted in recent successful efforts, such as FunSearch [[34]](#b33).

## Language-based Genetic Representation

The individual candidates in a language-based genetic algorithm are represented by natural language. This allows the strong language understanding and generation capabilities of an LLM to be leveraged to implement powerful recombination (crossover and mutation) and island reset operations through prompting.

## Mind Evolution

Figure [1](#fig_5) illustrates the design of Mind Evolution, with its hyperparameters listed in Table [1](#tab_0). The core components of Mind Evolution are:

1. the specific choices for the selection and migration operations; 2. the set of prompts that implement the initialization, recombination (crossover and mutation), and island reset operations with an LLM; 3. the fitness function that evaluates the quality of a given solution and optionally provides feedback on issues detected.

The overall evolution process is repeated until a valid solution is found, or until 𝑁 gens generations have been completed, after which the best scoring candidate is returned. Note that for many classical search problems (e.g., NP-complete problems), verifying solutions can be much easier than solving the problem [[11]](#b10). Similarly, we observe that it is possible to write an evaluation function for the natural language planning tasks we consider. The ability to check the correctness of a candidate solution does not obviously lead to the ability to generate a valid solution in the tasks we consider. That is, implementing an evaluation function is not equivalent to solving the task.

## Fitness Evaluation

Population Initialization Given a target problem, we independently sample 𝑁 convs initial solutions by prompting an LLM with a description of the problem, any information needed for solving the problem, and relevant instructions. If 𝑁 seq > 1, each of these initial solutions is then evaluated and refined sequentially through 𝑁 seq -1 additional turns of the "Refinement through Critical Conversation" process explained below. In total, this initialization procedure generates 𝑁 convs × 𝑁 seq candidate solutions, which forms the initial population on the first island for the first generation.  

## Refinement through Critical Conversation (RCC)

Given a candidate solution (or a set of candidate solutions for the process of recombination) we leverage an LLM to generate an improved solution by organizing a critical conversation between a "critic" character and an "author" character, as illustrated in Figure [2](#fig_2). Separating these two roles is intended to improve the critical thinking ability of an LLM. Each conversational turn is structured as a prompt-driven process, where solutions are refined based on critical feedback, similar to Reflexion [[36]](#b35). In particular, the critic first analyzes the candidate solution(s) provided as input, interprets the textual evaluation feedback, and suggest ways to correct any issues presented in the feedback. The author then proposes a single refined solution based on the input candidate(s), the subsequent evaluation(s), and the critic's analyses. The specific prompts used to drive these conversations are given in Appendix A.1. An ablation study in Section 4.4 shows that the critic's analysis step provides substantial performance improvements.

Selection To produce the next generation of an island, we follow Boltzmann tournament selection [[13]](#b12) where 0 to 𝑁 parent parents are stochastically sampled from the population according a probability distribution that is derived from a softmax transformation of their fitness scores. In this way, higher-performing solutions are more likely to be selected for reproduction, while other candidates can still be occasionally selected for diversity.

## Crossover and Mutation

We implement the crossover and mutation operations as a single recombination step, where an LLM is instructed to improve a given set of parents using the RCC process described above (Figure [2](#fig_2)). In particular, for recombination we sample 1 to 𝑁 parent parents and alter Step (b) in Figure [2](#fig_2) to first incorporate the evaluation results of the parents, then apply the critic to all parents and propose the revised solution as an "initial solution" for the next generation. Then, if 𝑁 seq > 1, we continue to follow Steps (c)(d)(e) to sequentially generate 𝑁 seq -1 child solutions by refining each previous child using the RCC process.

For each generation on each island, 𝑁 convs × 𝑁 seq child solutions are added to the island population, with duplicate solutions removed. For selection, we follow a Boltzmann tournament instead of explicitly retiring candidate solutions, except when performing an Island Reset below.

## Migration between Islands

Between migration events, each island population is evolved independently. During a migration, the top 𝑁 emigrate solutions are cloned from the current Island 𝑖 to the next Island 𝑖 + 1 after completing the generation on the current island (we update the populations on the islands sequentially from 1 to 𝑁 island ). Migration is performed cyclically between the islands, so emigrants from Island 𝑁 island arrive at Island 1. We have found that this form of cyclic migration accelerates the overall evolution process.

Island Reset Island reset happens every 𝑁 reset interval generations. During an Island Reset event, the top performers are first selected from the global population, the populations on 𝑁 reset islands with the lowest average scores are retired, and the selected top performers are cloned onto the reset islands. To select top performers, we explore two approaches: (1) directly select the top 𝑁 top candidates according to fitness; and (2) first select the top 𝑁 candidate candidates according to fitness, then prompt the LLM to select 𝑁 top good candidates from this pool that are substantially different from each other. The ablation study in [Section 4.4](#) show that the latter strategy, using an LLM for Island Reset, achieves better performance.

## Experiments

Tasks We evaluate Mind Evolution on three benchmark natural language planning domains: two tasks from Natural Plan [[47]](#b46), including Trip Planning (Section 4.2) and Meeting Planning (Section 4.3), and the TravelPlanner [[42]](#b41) benchmark (Section 4.1). (We omit the Calendar Scheduling task from Natural Plan, since these problems can be solved by enumeration.) Implementation details for each task is provided in Appendix A, including the prompts (Appendix A.1) and evaluation functions used (Appendix A.2).

## Models

We use Gemini 1.5 Flash (gemini-1.5-flash-001) as the default LLM in our experiments below. The hyperparameters used when applying Mind Evolution to Flash are specified in Table [1](#tab_0). In addition to evaluating Mind Evolution with the Flash model, we also investigate a two-stage approach, where Gemini 1.5 Pro model (gemini-1.5-pro-exp-0827) is used to tackle problems that are not solved within the 𝑁 gens generation limit. Such a two-stage approach provides better cost-efficiency than using the Pro model on every problem instance. When applying Mind Evolution to the Pro model we alter the hyperparameters from those specified in Table [1](#tab_0) to: 𝑁 convs = 8, 𝑁 seq = 3, 𝑁 parent = 10, 𝑃𝑟 no parents = 1/5.

Baselines For each task, we compare Mind Evolution to three baseline search strategies that use the same solution evaluator and task-specific prompts:

1. 1-Pass, where a solution is proposed using a single forward pass of the LLM. 2. Best-of-N [[4]](#b3), where up to 800 candidate solutions are independently generated until a successful solution is found (the same upper bound as Mind Evolution). 3. Sequential-Revision+, where 10 candidate solutions are proposed independently, then revised separately for 80 turns using the RCC process (Figure [2](#fig_2)). Note that 10 independent threads of 80-turn refinements are used instead of a single 800-turn refinement, because we rarely observe improvements after 80 turns. This baseline is similar to running 10 trials of multi-turn Reflexion [[36]](#b35).

Additionally, for reference, we also include an additional 1-Pass baseline that uses OpenAI o1-preview.

## Metrics

We measure Success Rate as the percentage of problem instances that are solved completely within a benchmark domain, separating the validation and test sets. (Note that the Success rate is referred to as Solve Rate in Natural Plan [[47]](#b46) and Final Pass Rate in TravelPlanner [[42]](#b41).)

To assess the cost of inference compute we report the number of LLM calls, the number of input and output tokens, and the total API cost of calling the LLM. (These costs are given in US Dollars, using prices from October 2024 when the experiments were conducted. The base rates are listed in Appendix D.) Note that assessing computational cost is particularly important when evaluating search strategies like Mind Evolution, since search is more expensive than generating a single solution. These statistics can help researchers and developers understand the cost-benefit trade-offs when using search to enhance LLM problem solving ability.

## TravelPlanner

TravelPlanner [[42]](#b41) is a natural language planning benchmark that simulates the problem of organizing a trip plan for a user who expresses preferences and constraints. We focus on the sole-planning mode (see [[42]](#b41) for details), where each problem instance consists of a list of options regarding accommodation, restaurants, attractions and transportation, plus additional constraints that specify user preferences for budget, cuisine, etc. A plan is evaluated based on whether it satisfies the user preferences and commonsense constraints.

Table [2](#tab_2) gives detailed results that compare the overall Success Rate and computational cost of Mind Evolution versus the baseline strategies. In terms of Success Rate, Mind Evolution clearly outperforms the baseline strategies, achieving over 95%. By comparison, Sequential-Revision+ provides a reasonable baseline, achieving almost 83%, while Best-of-N struggles, achieving only 55.6%. Overall, these results demonstrate a clear advantage of an evolutionary strategy that combines a broad search, through stochastic exploration, with a deep search that leverages an LLM for solution refinement.

Considering the two-stage approach, where Mind Evolution uses Gemini 1.5 Pro for any unsolved problems, we find that nearly the entire dataset can be solved, achieving a 100% success rate on validation and 99.9% on test problems respectively. The only work we are aware of that comes close to this success rate is [[16]](#b15), which uses GPT-4 for auto-formalization then leverages a formal solver to achieve 98.9% and 97.0% on validation and test respectively. Mind Evo- lution achieves comparable results without requiring a formal solver.

Finally, we note that the TravelPlanner dataset is organized into three levels of difficulty (Easy, Medium, Hard) and three trip durations (3 days, 5 days, 7 days), rendering 9 different problem classes. Figure [3](#fig_3) presents a breakdown of the success rates achieved across these different categories, showing that the success rates of 1-Pass and Best-of-N decline when planning for more travel days, but the trend is less clear for Mind Evolution and Sequential-Revision+, both of which iteratively refine proposed solutions.

## Natural Plan -Trip Planning

The Trip Planning task [[47]](#b46) involves finding an itinerary that consists of a sequence of cities to visit and number of days in each that satisfies flight connectivity and scheduling constraints -see Table [3](#tab_3) for a problem instance. We split the benchmark into 320 validation and 1,280 test instances (described in more detail in Appendix B).

The results in Table [2](#tab_2) again show that Mind Evolution strongly outperforms the baselines on this task, achieving 96.2% on the validation and 94.1% on the test instances. Table 2 also shows a qualitative comparison between the results produced by Mind Evolution and the baseline strategies. Note that Best-of-N performs better in this scenario (77.2%), even beating Sequential-Revision+ (74.4%). We find that for the two-stage approach, Mind Evolution achieves 100% on the validation set and 99.6% on the test set. These findings again highlight the benefit of evolutionary search versus simple sampling and sequential refinement.

Finally, we note that the difficulty of this task varies with the number of cities to visit, ranging from 3 to 10. Figure [4](#fig_4) shows a breakdown of the Success Rate in terms of number of cities, where the relative advantage of Mind Evolution appears to increase as the number of cities grows.

Set Success Rate LLM Calls Input Tokens Output Tokens API Cost (Oct 2024) TravelPlanner [42] 1-Pass val 10/180 = 5.6% 1 0.009M 0.001M US$0.001 (o1-preview 1-Pass) val 21/180 = 11.7% 1 0.008M 0.008M US$0.601 Best-of-N val 100/180 = 55.6% 472 4.44M 0.47M US$0.47 Sequential-Revision+ val 149/180 = 82.8% 280 35.53M 0.29M US$2.75 Mind Evolution val 172/180 = 95.6% 174 3.10M 0.18M US$0.29 (+pro) val 180/180 = 100% (257) (3.25M) (0.19M) (US$0.54) Mind Evolution test 952/1000 = 95.2% 167 3.02M 0.18M US$0.28 (+pro) test 999/1000 = 99.9% (67) (3.05M) (0.18M) (US$0.33) Natural Plan [47] Trip Planning 1-Pass val 66/320 = 20.6% 1 0.002M 0.001M <US$0.001 (o1-preview 1-Pass) val 116/320 = 36.2% 1 0.002M 0.008M US$0.53 Best-of-N val 247/320 = 77.2% 274 0.61M 0.18M US$0.10 Sequential-Revision+ val 238/320 = 74.4% 391 41.57M 0.38M US$3.23 Mind Evolution val 308/320 = 96.2% 168 1.48M 0.19M US$0.17 (+pro) val 320/320 = 100% (111) (1.51M) (0.19M) (US$0.22) Mind Evolution test 1204/1280 = 94.1% 196 1.78M 0.22M US$0.20 (+pro) test 1275/1280 = 99.6% (211) (1.86M) (0.24M) (US$0.37) Natural Plan [47] Meeting Planning 1-Pass val 104/500 = 20.8% 1 0.007M 0.001M US$0.001 (o1-preview 1-Pass) val 221/500 = 44.2% 1 0.006M 0.006M US$0.47 Best-of-N val 347/500 = 69.4% 444 3.99M 0.31M US$0.39 Sequential-Revision+ val 310/500 = 62.0% 484 32.16M 0.40M US$2.53 Mind Evolution val 425/500 = 85.0% 406 5.35M 0.41M US$0.52 (+pro) val 492/500 = 98.4% (890) (13.36M) (0.91M) (US$2.55) Mind Evolution test 419/500 = 83.8% 394 5.24M 0.40M US$0.51 (+pro) test 491/500 = 98.2% (828) (12.25M) (0.83M) (US$2.34)  

## Natural Plan -Meeting Planning

For the Meeting Planning task a sequence of meetings should be scheduled to maximize the number of meetings between individuals subject to availability, location and travel time constraints [[47]](#b46). This task differs from TravelPlanner and Trip Planning in that not every meeting can be scheduled for every problem instance, implying that it is not possible to know whether an optimal solution has been reached. There-fore, to obtain the results shown in Table [2](#tab_2), we allow the searches to proceed until the upper bounds on iteration counts have been reached. For this task, we split the set of instances into 500 validation and 500 test instances (see Appendix B for details).

The results shown in Table [2](#tab_2) continue to demonstrate a significant performance for Mind Evolution over baseline strategies, achieving an 85.0% Success Rate on the validation set and 83.8% on the test set. Notably, the two-stage approach using Gemini 1.5 Pro achieves success rates to 98.4% and 98.2% on validation and test respectively. Finally, Figure [5](#) shows the breakdown of success rates by the number of people to schedule meetings with. In this case, we find that Mind Evolution sustains a significant advantage in success rate as the number of people increases.

Q: You plan to visit 5 European cities for 16 days in total. You only take direct flights to commute between cities. You want to spend 5 days in Madrid. From day 3 to day 7, there is a annual show you want to attend in Madrid. You plan to stay in Zurich for 3 days. You would like to visit Frankfurt for 3 days. You would like to visit Santorini for 6 days. You are going to attend a wedding in Santorini between day 7 and day 12. You want to spend 3 days in Riga.

Here are the cities that have direct flights: Zurich and Riga, Frankfurt and Riga, Santorini and Zurich, Madrid and Zurich, Frankfurt and Zurich, Madrid and Santorini, Frankfurt and Madrid.

Find a trip plan of visiting the cities for 16 days by taking direct flights to commute between them.  

## Method

## Analysis and Ablation Studies

To understand how Mind Evolution's performance scales, and how the different components affect its behavior, we provide additional measurements and ablations to gain additional insight.

Scaling Regarding scaling, Figure [6](#) reports the Success Rate achieved by Mind Evolution across the planning tasks as a function of the number of generations. These results clearly show steady improvement for Mind Evolution as the number of generations is increased.

To compare the scaling of Mind Evolution to that of the baseline search methods, we also plot the Success Rate and average task evaluation scores as a function of the number of candidate solutions generated by the each strategy (Figures [7](#)[8](#)[9](#)). The task evaluation scores are calculated by penalizing unsatisfied constraints and suboptimality of the objective value, hence the maximum score that can be achieved in any problem instance is zero (see Appendix A.2 for details). In Appendix D, we provide another perspective on the cost-benefit trade-offs in terms of the specific API costs incurred.

Figures [7](#)[8](#)[9](#)show the results for the TravelPlanner, Trip Planning and Meeting Planning tasks respectively. In each case, we see that the overall success rates and average task evaluation scores improve monotonically with an increasing number of proposed solutions across all search methods. These plots also show that Mind Evolution is consistently more effective than the baseline strategies with respect to the number of candidate solutions needed to achieve a specified level of success rate (or average task performance).

We note that Best-of-N appears to be significantly underperforming on TravelPlanner. We hypothesize that this occurs because this task involves implicit commonsense constraints (e.g., a trip plan should return to the origin city, a restaurant cannot be visited twice, etc.), which are not given in the problem instance but instead learned from evaluation feedback, which Best-of-N does not leverage. Ablations We also conducted a set of ablations to study the contribution of the different components of Mind Evolution. Table [4](#) shows that using the critic step in the RCC process (Figure [2](#fig_2) in Section 3.2) and textual feedback from the evaluation functions are the most critical to performance, although the other components also make meaningful contributions to performance.

To assess hyperparameter sensitivity, we investigated the Trip Planning task in greater detail, choosing the harder setting with 10 cities to better reveal differences in performance. (Similar results are also

0 200 400 600 800 # Candidate Solutions 8 7 6 5 4 3 2 1 Averaged Evaluation Scores 0 200 400 600 800 # Candidate Solutions 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Success Rate Mind Evolution Seq. Revisions+ Best-of-N Figure 9 | Meeting Planning success rates and evaluation scores as the number of candidate solutions is increased. Critic ✓ ✓ ✓ ✓ S/Q Prompts ✓ ✓ ✓ Textual Feedback ✓ ✓ Reset with LLM ✓ Success Rate 46.1% 71.1% 76.1% 91.1% 95.6%

Table 4 | An ablation study of Mind Evolution components on the TravelPlanner validation set. Each column in the table shows an experiment where ✓ indicates whether a component is used. If "Critic" is disabled, we skip the critic step in Figure [2](#fig_2) and go straight to the author step. "S/Q Prompts" stands for Strategy/Question prompts, which are additional taskspecific instructions in the critical thinking prompts (see Appendix A.1 for details). If "Textual Feedback" is disabled, we do not include evaluation feedback in the prompts. If "Reset with LLM" is disabled, we directly select global elites by their evaluation scores in island reset events, rather than use an LLM to choose, as described in Section 3.

2. Succ. Rate w/ island model (𝑁 island = 4, 𝑁 convs = 5) 87.5% w/o island model (𝑁 island = 1, 𝑁 convs = 20) 77.4% 𝑁 convs = 10, 𝑁 gens = 5 82.5% 𝑁 convs = 5, 𝑁 gens = 10 (default) 87.5% 𝑁 convs = 4, 𝑁 gens = 13 85.0%  improves the performance of Mind Evolution. The bottom three rows compare the effect of increasing the number of candidate solutions per generation versus having more generations while controlling for a similar number of candidates considered overall. In this case, it appears that deeper evolutionary search indeed has benefits, although it is also important to continue exploring broadly in each generation.

## A Challenging New Task: StegPoet

We introduce a challenging new task, StegPoet, where a hidden message should be stenographically encoded [[33]](#b32) into a piece of creative writing. Even though the problem is difficult to formalize, it remains amenable to programmatic verification, which makes it addressable by the methods considered in this paper. In this task, a hidden message (𝑀) expressed by a sequence of numbers should be encoded in a piece of creative text about a particular topic, expressed in the form of an essay, story or poem. The goal is to both provide a number-to-word substitution cipher and a generated text that uses the cipher to encode the message. Figure [10](#fig_5) gives an example. We impose an additional constraint that there must be, on average, 𝐵 words between successive cipher words in the generated text, which ensures that simply listing the cipher words as the text portion does not qualify as solution when 𝐵 > 0.

The difficulty of this problem varies along four axes:

1. Difficulty increases with the length of the hidden message, 𝑀. We set 10 ≤ |𝑀| ≤ 30. 2. The repetition of the numbers in 𝑀. The more repetition, the more stringent the constraints. 3. The "closeness" of the repeated numbers to each other. Each form of writing dictates how much repetition of the same word and proximity of occurrence is acceptable. The LLM must balance adherence to the form with the need to correctly encode the message. 4. Empirically, as 𝐵 (the mean distance between cipher words) grows, the problem becomes more difficult.

In our tests, 3 ≤ 𝐵 ≤ 7.

We divide the problem instances into a validation split of 101 instances and a test split of 245 instances. See Appendix F for additional details about the StegPoet evaluation.

Detailed performance results for Mind Evolution and the baseline strategies are given in Table [6](#tab_8), while Figure [11](#fig_5) shows performance per difficulty level. Here the two-stage Mind Evolution (+pro) achieves 87.1% on validation and 79.2% on test. Best-of-N only manages to solve 1% of the validation tasks.

## Conclusion

We have presented Mind Evolution, an evolutionary search approach for solving challenging natural language planning problems, by scaling inference-time compute for stochastic exploration and iterative refinement. An evaluation on the TravelPlanner and Natural Plan natural language planning benchmarks, as well as a new benchmark StegPoet introduced in this paper, demonstrates that Mind Evolution significantly outperforms Best-of-N and sequential revision. To our knowledge, this is the first approach that is able to achieve such a level of success on these tasks without explicitly leveraging a formal solver.

## Limitations

The main limitation of the current work is the focus on natural language planning problems where proposed solutions can be programmatically evaluated and critiqued. In future work, we aim to extend beyond this limitation by developing LLM-based evaluators that would enable broader applications.

## A. Implementation Details

Here we describe the implementation details of Mind Evolution. The code will be made available.

## A.1. Prompt Design

We first use Meeting Planning as an example to illustrate the structure of the prompts used. The prompts, as well as the model responses when parent solutions are given, are shown in Figures [12](#fig_5)[13](#fig_5)[14](#fig_5)[15](#fig_5)[16](#fig_5). The prompts begin with general instructions and a general problem definition, few-shot examples, then a task description. The few-shot examples help the LLM understand the problem and generate solutions closer to the desired formats. For TravelPlanner, we take two 3-day example plans from the training set and use them across all tasks (3-7 days). For Trip Planning, we take two example plans from the few-shot examples provided by the benchmark and use them across all tasks. For Meeting Planning, we use the 5-shot examples provided by the benchmark for each task.

After the task description, we include parent solutions with corresponding evaluation feedback, followed by critical thinking instructions (in Figures [14](#fig_5)[15](#fig_5)). These instructions lead the LLM to improve the parent solutions, following the Refinement through Critical Conversation (RCC) process described in Section 3.2. The critical thinking instructions include problem-specific Strategy/Question prompts based on findings in each validation set (ablated in Section 4.4). In the model responses, one can see that the LLM follows the critical thinking instructions in playing the critic role to analyze the parent solutions, and playing the author role to propose a new solution. We also give an example of the prompt and a model response for TravelPlanner, which has the same structure, in Figures [17](#fig_5)[18](#fig_5)[19](#fig_5)[20](#fig_2)[21](#fig_5)[22](#fig_2).

## A.2. Evaluation Functions

In this work, solutions are evaluated programmatically with a function. As described in Section 3.2, an evaluation function has three main roles: (1) scoring solutions by measuring the optimization objective, if any; (2) verifying whether the solution satisfies given constraints; and (3) providing corresponding textual feedback. Specifically, we score natural language plans by penalizing the constraints that are not satisfied, the objectives that are not maximized, and for not following the required solution format. Thus the maximum score for all tasks is zero. We also provide textual feedback that describes how the constraints are not satisfied and how the objectives are not maximized.

TravelPlanner Our evaluation function for Trav-elPlanner is modified from the TravelPlanner evaluation code [[42]](#b41). The evaluation code expects travel plans in JSON format. We modify the original evaluation code to make it output a cumulative score that reflects all the constraints that are not satisfied, instead of simply answering whether or not a plan satisfies all the constraints. We also make it provide textual feedback for the violated constraints.

In the TravelPlanner validation set, the constraints are provided in both user query text and a structured JSON format. However, in the test set, the constraints are only described in user query text. To make it easier for the evaluation function to consider the constraints, we extract them from user query into JSON using Gemini 1.5 Flash. For example, to extract the requested cuisines, we prompt Gemini with "Look at the following text and tell me if there are any cuisine requirements on the upcoming trip..." multiple times, and formulate the final answer via majority voting. To verify the reliability of this approach, we tested on the validation set and found complete agreement between the JSON extracted from user query and the provided JSON. In addition, we upload our test solutions to the TravelPlanner evaluation server, and found that the results agree with the official evaluation.

Trip Planning Similar to TravelPlanner, the Trip Planning evaluation function expects plans in JSON format. Since Trip Planning user queries are programmatically generated, we can parse the constraints specified in user queries. These constraints include number of days to stay in a city, specific days to be in a city (e.g., for events), and whether there are flights between cities. Our evaluation function scores a plan by the constraints that are not satisfied and whether it conforms with the desired JSON format, while also providing corresponding textual feedback.

## Meeting Planning

The Meeting Planning evaluation function also expects plans in JSON. Constraints are also provided in structured JSON format. Unlike Trav-elPlanner and Trip Planning, Meeting Planning has an optimization objective -the number of friends to meet with. We modify the original evaluation evaluation function to score a proposed plan by how many people that are not going to be met with; whether it conflicts with the schedules of other people; whether it includes meetings with the same person more than once; whether any part of the plan conflict with other parts; whether it follows the desired format as instructed. In Figures [23](#fig_3)[24](#fig_4)we present the evaluation function that implements the simple logic described above as an example. import d a t e t i m e from t y p i n g import Any , Sequence d e f m e e t i n g _ p l a n _ e v a l ( plan : l i s t [ s t r ] , s t a r t _ l o c a t i o n : s t r , i n i t i a l _ t i m e : s t r , f r i e n d _ s c h e d u l e s : d i c t [ s t r , Any ] , d i s t a n c e _ m a t r i x : d i c t [ s t r , Any ] ) : " " " E v a l u a t e meeting plan . Args : plan : a l i s t o f planned s t e p s , such as [ ' You s t a r t a t R u s s i a n H i l l a t 9:00AM. ' , ' You t r a v e l t o Marina D i s t r i c t i n 7 minutes and a r r i v e a t 9:07AM. ' , ' You w a i t u n t i l 3:45 PM. ' , ' You meet James f o r 75 minutes from 3:45PM t o 5:00PM . ' ] s t a r t _ l o c a t i o n : Your i n i t i a l l o c a t i o n i n i t i a l _ t i m e : t h e i n i t i a l time , such as 10:30AM f r i e n d _ s c h e d u l e s : f r i e n d ' s l o c a t i o n , a v a i l a b l e time , t h e amount o f time f o r t h e meeting , such as { ' Stephanie ' : { ' l o c a t i o n ' : ' M i s s i o n D i s t r i c t ' , ' s t a r t _ t i m e ' : ' 1 0 : 3 0AM' , ' end_time ' : ' 1 : 3 0PM' , ' meeting_time ' : 120}} d i s t a n c e _ m a t r i x : D i s t a n c e s between l o c a t i o n s , such as { ' Marina D i s t r i c t ' : { ' M i s s i o n D i s t r i c t ' : 20} , ' M i s s i o n D i s t r i c t ' : { ' Marina D i s t r i c t ' : 19}} " " " met_with = {} s c o r e = 0 . 0 f e e d b a c k = [ ] c u r _ l o c a t i o n = s t a r t _ l o c a t i o n c u r _ t i m e = d a t e t i m e . d a t e t i m e . s t r p t i m e ( i n i t i a l _ t i m e , "% a s s e r t i s i n s t a n c e ( plan , l i s t ) f o r s t e p i n plan : t r y : i f s t e p . s t a r t s w i t h ( " You s t a r t " ) : c o n t i n u e e l i f s t e p . s t a r t s w i t h ( " You t r a v e l " ) : d e s t i n a t i o n = s t e p . s p l i t ( " t r a v e l t o " ) [ 1 ] . s p l i t ( " i n " ) [ 0 ] . s t r i p ( ) c u r _ t i m e = c u r _ t i m e + d a t e t i m e . t i m e d e l t a ( minutes=d i s t a n c e _ m a t r i x [ c u r _ l o c a t i o n ] [ d e s t i n a t i o n ] ) c u r _ l o c a t i o n = d e s t i n a t i o n e l i f s t e p . s t a r t s w i t h ( " You w a i t " ) : raw_end_time = s t e p . s p l i t ( " w a i t u n t i l " ) [ 1 ] . s p l i t ( " . " ) [ 0 ] . s t r i p ( ) end_time = None t r y : end_time = d a t e t i m e . d a t e t i m e . s t r p t i m e ( raw_end_time , "% e x c e p t V a l u e E r r o r : s c o r e -= 2 f e e d b a c k . append ( f " \ " { s t e p } \ " i s i n v a l i d because t h e time format doesn ' t f o l l o w t h e examples . " ) i f end_time <= c u r _ t i m e : e n d _ t i m e _ s t r = end_time . s t r f t i m e ("% s c o r e -= 2 f e e d b a c k . append ( f " \ " { s t e p } \ " i s i n v a l i d because but t h e p r e v i o u s s t e p a l r e a d y ends a t { e n d _ t i m e _ s t r } and you cannot go backwards i n time . " ) c u r _ t i m e = end_time e l i f s t e p . s t a r t s w i t h ( " You meet " ) : 

## B. Data Splits

TravelPlanner TravelPlanner has 45 training tasks, 180 validation tasks, and 1,000 test tasks in the original benchmark. Natural Plan -Trip Planning The Trip Planning benchmakr has 1,600 example tasks. There are eight different difficulty levels, ranging from 3 to 10 cities. Each difficulty level has 200 examples. We split the dataset into validation and test sets by putting the first 40 examples from each difficulty level into validation, and the last 160 examples into test, giving 320 examples in validation (which we used for prompt development) and 1,280 for test. In Figure 4, we show the performance at each difficulty level. Natural Plan -Meeting Planning The Meeting Planning benchmark has 1,000 example tasks. There are ten different difficulty levels, ranging from meeting one to ten different friends. Each difficulty level has 100 examples. We split the dataset into validation and test sets by putting the first 50 examples from each difficulty level into validation, and the last 50 examples into test, giving 500 examples in validation (which we used for prompt development) and 500 for test. In Figure 5, we show the performance at difficulty level.

## C. GPT Results

Table [7](#) presents the results of Mind Evolution using GPT-4o-mini with the same sets of prompts. Specifically, with 1-pass inference, GPT-4o-mini also struggles at planning tasks, achieving 0%

on TravelPlanner, 9.1% success rate on Trip Planning, and 20.2% success rate on Meeting Planning. Again, Mind Evolution significantly improves the performance by over 100% relatively across different benchmarks. Success Rate TravelPlanner [42] 79.4% Natural Plan [47] Trip Planning 48.1% Natural Plan [47] Meeting Planning 86.4%

Table 7 | Mind Evolution with GPT-4o-Mini results on validation sets.

## D. Model Pricing and API Cost Curves

Table [8](#) shows the API pricing of different models used in our evaluation (Tables 2), at the time of writing (October 2024).

Figure [25](#fig_2) gives insight into the scaling properties of the various strategies in terms of their API cost, which Model Input Token Output Token Gemini 1.5 Flash $0.075/M $0.30/M Gemini 1.5 Pro $1.25/M $5.00/M GPT-4o-Mini $0.15 $0.60 OpenAI o1-preview $15.00/M $60.00/M Table [8](#) | Pricing at the time of writing (October 2024). These differences serve as a proxy for real computational cost differences among models.

is also a linear combination of the input token counts and the output token counts, weighted by base rate (Table [8](#)).

## E. Additional Examples

In addition to

Table 3, we present qualitative examples of TravelPlanner and Meeting Planning in Table 9 and Table 10, respectively.

## F. Additional Details for StegPoet

The prompt design used for StegPoet is given in Figure [26](#fig_2).

## StegPoet Evaluation

Each proposed solution should contain a cipher and text component. The first step is to calculate what is encoded in the text by finding all the cipher strings; this is done via simple capitalization-agnostic character-matches. We refer to the actual encoded string as 𝑀 . If 𝑀 = 𝑀 ′ the problem is solved correctly. The numeric evaluation of a proposed solution is computed as follows: 1. Invalid if the text or cipher component cannot be parsed or violates constraints. 2. What is the first position, 𝑖, in which 𝑀 𝑖 ≠ 𝑀 ′ 𝑖 ? This is the integer part of the score. 3. Compute the Levenshtein distance between 𝑀 and 𝑀 ′ . Levenshtein distance is often used in information theory and linguistics to measure the difference between two sequences [[2]](#b1). This is scaled between (0,1) and added to the integer component above.

Additionally, textual feedback, without numeric penalties, is also provided in the revision request made to the LLM.

1. A clearly marked list of what 𝑀 ′ was found. 2. A list of number mappings missing from the cipher, or unnecessary numbers specified in the cipher.

3. If a word appears an incorrect number of times (too few or too many) in the text, it is indicated, along with the error. 4. An annotated copy of the text is returned. The annotations indicate where the cipher-keywords were found (they are shown asterisked), and the first error is indicated. 5. If the text encodes the cipher correctly, but also encodes extra words, that is indicated. 6. If everything in 𝑀 ′ is correct, but |𝑀 ′ | < |𝑀|, it is indicated as such.

For this task, we experimented with many different genre forms (poetry, short-story fiction, essay, monologue, etc.), as well as inspirations from contemporary to classic writers.

![plan a 5-day trip from Seattle to LA and SD with a budget of $800. We want to have Japanese for at least one dinner and prefer private hotel rooms. Seattle to LA 。Attractions: -。Transport: Flight 9587 。Hotel: Grand Hotel, private rm 。Dinner: Din Tai Fung Day 2: 。Places: LA 。Attraction: Dodgers Stadium 。Transport: -。Hotel: Grand Hotel, private rm 。Dinner: STK … Evaluate Refine (Selection, Crossover, Mutation) [Gen 1] A valid or the best solution : "Improve previous solutions" LLM [Gen 2, 3, …]]()

![refines Q: You are planning to visit 7 Asian cities. You want to spend 5 days in Tokyo. You want to attend a wedding on day 11 ... Plan: [{'city': 'Tokyo', 'days': '3'}, {'city': 'Taipei', days': '2'}, …] Feedback: ['You planned 3 days for Tokyo, but you are required to stay 5 days in Tokyo ', …] The number of days to stay in Tokyo should be 5 instead of 3 ... Plan: [{'city': 'Tokyo', 'days': '5'}, {'city': 'Taipei', days': '2'}, …]]()

![Figure2| Illustrating the Refinement through Critical Conversation (RCC) process, where an initial solution is first proposed, then evaluated and subjected to feedback from a critic, after which an author proposed a refined solution and the process iterates.]()

![Figure 3 | Success rate on the validation set of the TravelPlanner benchmark, organized by problem instance difficulty and the number of travel days.]()

![Figure 4 | Success rate on the validation set of the Trip Planning benchmark per number of cities to visit.]()

![Pass Madrid (Day 1-7) ∠ Santorini (Day 7-12) ∠ Zurich (Day 12-14) ∠ Frankfurt (Day 14-16) ∠ Riga (Day 16-19) 7 days for Madrid instead of 5; 4 days for Riga instead of 3; 19 days in total instead of 16. Best-of-N Madrid (Day 1-7) ∠ Santorini (Day 7-12) ∠ Zurich (Day 12-14) ∠ Frankfurt (Day 14-16) ∠ Riga (Day 16-16) 7 days for Madrid instead of 5; 1 day for Riga instead of 3. Sequential Revisions+ Zurich (Day 1-3) ∠ Frankfurt (Day 3-5) ∠ Riga (Day 5-7) ∠ Santorini (Day 7-12) ∠ Madrid (Day 12-16) omitted the show in Madrid (Day 3-7); no direct flight from Riga to Santorini. Mind Evolution (ours) Frankfurt (Day 1-3) ∠ Madrid (Day 3-7) ∠ Santorini (Day 7-12) ∠ Zurich (Day 12-14) ∠ Riga (Day 14-16)]()

![Figure6| Success rate on the validation set for each natural language planning benchmark at each generation of Mind Evolution.]()

![Figure 12 | Example Meeting Planning prompt and model response with parent solutions given (Part 1)]()

![Figure 23 | The Meeting Planning evaluation function (part 1).]()

![Words in the cipher cannot be subsets of each other (e.g., origin and original). (b) Words in the cipher cannot be repeated. (c) Words in the cipher should be at least 4 characters long. (d) Words in the cipher should contain only alphabetic characters.]()

![Definition of hyperparameters in Mind Evolution. Unless otherwise specified, the experiments in work use the default values. The product of the first four hyperparameters gives the maximum number of candidate solutions generated (800 in the default setting).]()

![Experimental results on benchmark natural language planning tasks. "(+pro)" denotes the two-stage results, where we use Gemini 1.5 Pro to solve the problems that were not solved in experiments using Gemini 1.5 Flash. Number of LLM calls, token counts, and API cost are averaged across the validation or test problem set, and they are calculated only on the remaining problems for the "(+pro)" experiments. Here, we also show OpenAI o1-preview results as a reference.]()

![An example problem instance from the Trip Planning task in Natural Plan, with the predicted plans from Mind Evolution and the baselines. 1-Pass and Best-of-N both make mistakes on number of days to stay, but satisfy the requirements of being in Madrid and Santorini on specific days. The Sequential-Revision+ plan omits the annual show in Madrid and plans a non-existent flight, but is correct in the number of days. In contrast, the Mind Evolution plan satisfies all specified requirements.]()

![Hyperparameter studies on the Trip Planning problem instances with 10 cities. The first two rows show the difference between enabling and disabling the island model. The bottom three rows illustrate a trade-off between the number of candidates per generation versus the number of generations. (Note that the bottom row (𝑁 convs = 4, 𝑁 gens = 13) produces slightly more than 800 solutions (832).Example of the encoding of a StegPoet problem instance (left) and a correct solution (right) that includes the number-to-word cipher and a poem in the style of a children's poetry author. Note that |𝑀| = 12 in this instance. We added capitalization to the code words to highlight them.]()

![Experimental results on StegPoet. Price and token counts are averages per problem. All results use Gemini 1.5 Flash, except (+pro), which solves the problems that were not solved in the Flash runs, using Gemini 1.5 Pro.]()

