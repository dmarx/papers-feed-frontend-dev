<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">StyleCrafter: Taming Stylized Video Diffusion with Reference-Augmented Adapter Learning</title>
				<funder ref="#_ehbbcj7">
					<orgName type="full">Shenzhen Science and Technology Program</orgName>
				</funder>
				<funder ref="#_G5GDxkU">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-09-12">12 Sep 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Gongye</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tsinghua</forename><surname>University</surname></persName>
						</author>
						<author>
							<persName><forename type="first">China</forename><forename type="middle">Menghan</forename><surname>Xia</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yong</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><roleName>Tencent</roleName><forename type="first">Haoxin</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Lab</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Menghan</forename><surname>Xia</surname></persName>
							<email>menghanxyz@gmail.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">JINBO XING</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">YIBO WANG</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">XINTAO WANG</orgName>
								<orgName type="institution" key="instit2">YING SHAN</orgName>
								<address>
									<region>Tencent</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">YUJIU YANG *</orgName>
								<orgName type="institution">AI Lab</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">Menghan Xia; Yong Zhang; Haoxin Chen</orgName>
								<address>
									<region>Tencent</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="institution">AI Lab</orgName>
								<address>
									<addrLine>Jinbo Xing</addrLine>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<addrLine>Yibo Wang</addrLine>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff9">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<addrLine>Ying Shan</addrLine>
									<settlement>Shenzhen Xintao Wang</settlement>
									<region>Tencent</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff10">
								<orgName type="institution">AI Lab</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff11">
								<orgName type="institution" key="instit1">Yujiu Yang</orgName>
								<orgName type="institution" key="instit2">Tsinghua University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff12">
								<orgName type="institution">Jinbo Xing</orgName>
								<address>
									<addrLine>Yibo Wang</addrLine>
									<settlement>Xintao Wang Ying Shan</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">StyleCrafter: Taming Stylized Video Diffusion with Reference-Augmented Adapter Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-09-12">12 Sep 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">7CF021B27BCD738DC9948E01EA247398</idno>
					<idno type="DOI">10.1145/3687975</idno>
					<idno type="arXiv">arXiv:2312.00330v2[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Diffusion Model</term>
					<term>Stylized Generation</term>
					<term>Image/Video Synthesis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 1. Stylized Generation Results Produced by StyleCrafter</head><p>Text-to-video (T2V) models have shown remarkable capabilities in generating diverse videos. However, they struggle to produce user-desired artistic videos due to (i) text's inherent clumsiness in expressing specific styles and (ii) the generally degraded style fidelity. To address these challenges, we introduce StyleCrafter, a generic method that enhances pretrained T2V models with a style control adapter, allowing video generation in any style by feeding a reference image. Considering the scarcity of artistic video data, we propose to first train a style control adapter using</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The popularity of powerful diffusion models has led to remarkable progress in the field of content generation. For instance, text-toimage (T2I) models are capable of generating diverse and vivid images from text prompts, encompassing various visual concepts. This great success can be attributed not only to the advancement of models but also to the availability of various image data over the Internet. Constrastingly, text-to-video (T2V) models fall short of the data categories especially in styles, since existing videos predominantly feature photorealism. While these strategies, like initializing weights from well-trained T2I models or joint training with image and video datasets, can help mitigate this issue, the generated stylized videos generally suffer from degraded style fidelity. Although significant success has been achieved in style transfer/preservation in T2I generation, the field of stylized video generation remains largely unexplored, and effective solutions are yet to be discovered.</p><p>In this paper, we propose StyleCrafter, a generic method that enhances pre-trained T2V models with a style control adapter, enabling text-to-video generation in any desired style by providing a reference image. Anyhow, it is non-trivial to achieve this goal. (i) as a classic problem of style transfer/preservation, the style control adapter requires to extract accurate style concepts from the reference image in a content-style decoupled manner. (ii) the scarcity of open-source stylized videos challenges the adaptation training of the T2V models.</p><p>Considering the scarcity of stylized videos, we propose to first train a style adapter to extract desired style concepts from images over image datasets, and then transfer the learned stylization ability to a T2V model with shared spatial weights through a tailor-made finetuning paradigm. The advantages are twofold: on the one hand, the adapter trained over stylized images can effectively extract the style concept from input images, eliminating the necessity for scarcely available stylized videos. On the other, a finetuning paradigm enables text-to-video models with better adaptation to the style concepts extracted from the previously trained style adapter, while avoiding degradation of temporal quality in video generation.</p><p>To effectively capture the style features and promote content-style disentanglement, we adopt the widely used query transformer to extract style concepts from a single image. Particularly, we design a scale-adaptive fusion module to balance the influences of text-based content features and image-based style features, which helps generalization across various text and style combinations. During the training process, we employ carefully designed data augmentation strategies to enhance decoupled learning.</p><p>StyleCrafter efficiently generates high-quality stylized videos that align with the content of the texts and resemble the style of the reference images. Comprehensive experiments are conducted to assess our proposed approach, demonstrating that it significantly outperforms existing competitors in both stylized image generation and stylized video generation. Furthermore, ablation studies offer a thorough analysis of the technical decisions made in developing the complete method, which provides valuable insights for the community. Our contributions are summarized as follows:</p><p>‚Ä¢ We propose the concept of improving stylized generation for pre-trained T2V models by adding a style adapter.</p><p>‚Ä¢ We explore an efficient network for stylized generation, which facilitates the content-style disentangled generation from text and image inputs. Our method attains notable advantages over existing baselines. ‚Ä¢ We propose a training paradigm for generic T2V style adapter without requiring any stylized videos for supervision.</p><p>2 Related Works 2.1 Text to Video Synthesis</p><p>Text-to-video synthesis (T2V) is a highly challenging task with significant application value, aiming to generate corresponding videos from text descriptions. Various approaches have been proposed, including autoregressive transformer <ref type="bibr" target="#b63">[Vaswani et al. 2017</ref>] models and diffusion models <ref type="bibr" target="#b23">[Ho et al. 2020;</ref><ref type="bibr" target="#b40">Nichol and Dhariwal 2021;</ref><ref type="bibr">Song et al. 2021a,b]</ref>. Video Diffusion Model <ref type="bibr">[Ho et al. 2022b</ref>] employs a space-time factorized U-Net to execute the diffusion process in pixel space. Imagen Video <ref type="bibr">[Ho et al. 2022a]</ref> proposes a cascade diffusion model and v-parameterization to enhance VDM. Another branch of techniques makes good use of pre-trained T2I models and further introduces some temporal blocks for video generation extension.</p><p>CogVideo <ref type="bibr" target="#b25">[Hong et al. 2022</ref>] builds upon CogView2 <ref type="bibr" target="#b12">[Ding et al. 2022</ref>] and employs multi-frame-rate hierarchical training strategy to transition from T2I to T2V. Similarly, Make-a-video <ref type="bibr" target="#b55">[Singer et al. 2022]</ref>, MagicVideo <ref type="bibr" target="#b79">[Zhou et al. 2022]</ref> and LVDM <ref type="bibr" target="#b19">[He et al. 2022]</ref> inherit pretrained T2I diffusion models and extend them to T2V generation by incorporating temporal attention modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Stylized Image Generation</head><p>Stylized image generation aims to create images that exhibit a specific style. Decoupling style and content is a classic challenge <ref type="bibr" target="#b60">[Tenenbaum and Freeman 2000]</ref>. Early research primarily concentrated on image style transfer, a technique that involves the transfer of one image's style onto the content of another, requiring a source image to provide content. Traditional style transfer methods <ref type="bibr" target="#b21">[Hertzmann et al. 2001;</ref><ref type="bibr" target="#b64">Wang et al. 2004;</ref><ref type="bibr" target="#b76">Zhang et al. 2013</ref>] employ low-level, hand-crafted features to align patches between content images and style images. Since Gatys et al. <ref type="bibr" target="#b15">[Gatys et al. 2016</ref>] discovered that the feature maps in CNNs capture style patterns effectively, a number of studies <ref type="bibr" target="#b1">[An et al. 2021;</ref><ref type="bibr" target="#b11">Deng et al. 2022;</ref><ref type="bibr" target="#b29">Huang and Belongie 2017;</ref><ref type="bibr" target="#b35">Li et al. 2017;</ref><ref type="bibr" target="#b37">Liu et al. 2021;</ref><ref type="bibr">Texler et al. 2020a;</ref><ref type="bibr" target="#b78">Zhang et al. 2022]</ref> have been denoted to utilize neural networks to achieve arbitrary style transfer. A common practice involves utilizing a pretrained VGG network <ref type="bibr" target="#b54">[Simonyan and Zisserman 2014]</ref> to extract style information or compute Gram matrix loss <ref type="bibr" target="#b15">[Gatys et al. 2016]</ref> to enable self-supervised learning of visual styles.</p><p>As the field of generation models progressed, researchers began exploring stylized image generation for T2I models. Although T2I models can generate various artistic images from corresponding text prompts, words are often limited to accurately convey the stylistic elements in artistic works. Consequently, recent works have shifted towards example-guided artistic image generation. Several studies <ref type="bibr" target="#b26">[Hu et al. 2022;</ref><ref type="bibr" target="#b33">Kumari et al. 2023;</ref><ref type="bibr" target="#b51">Ruiz et al. 2023;</ref><ref type="bibr" target="#b53">Shi et al. 2023</ref>] developed various optimization techniques on a small collection of input images that share a common style concept. Inspired by Textural Inversion (TI) <ref type="bibr" target="#b13">[Gal et al. 2022]</ref>, some methods <ref type="bibr" target="#b0">[Ahn et al. 2023;</ref><ref type="bibr" target="#b56">Sohn et al. 2023;</ref><ref type="bibr" target="#b77">Zhang et al. 2023]</ref> propose to optimize a specific textual embedding to represent a certain style. Similarly to our work, IP-Adapter <ref type="bibr" target="#b75">[Ye et al. 2023</ref>] trains an image adapter based on pretrained Stable Diffusion to adapt T2I models to image conditions. Although IP-Adapter can produce similar image variants, it fails to decouple style concepts from input images or generate images with other content through text conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Stylized Video Generation</head><p>Building upon the foundation of stylized image generation, researchers have extended the concept to video style transfer and stylized video generation. Due to the scarcity of large-scale stylized video data, a common approach for video stylization involves applying image stylization techniques on a frame-by-frame basis. Before the advent of ML, researchers have explored methods for rendering specific artistic styles such as video watercolorization <ref type="bibr" target="#b4">[Bousseau et al. 2007</ref>]. Early deep learning methods of video style transfer <ref type="bibr" target="#b6">[Chen et al. 2017;</ref><ref type="bibr" target="#b10">Deng et al. 2021;</ref><ref type="bibr" target="#b14">Gao et al. 2020;</ref><ref type="bibr" target="#b31">Jamri≈°ka et al. 2019;</ref><ref type="bibr" target="#b50">Ruder et al. 2016;</ref><ref type="bibr">Texler et al. 2020b</ref>] apply style transfer in video sequences, generating stable stylized video sequences through the use of optical flow constraints. Additionally, Some video editing methods <ref type="bibr" target="#b16">[Geyer et al. 2024;</ref><ref type="bibr">Huang et al. 2023b;</ref><ref type="bibr" target="#b32">Khachatryan et al. 2023;</ref><ref type="bibr" target="#b46">Qi et al. 2023;</ref><ref type="bibr" target="#b70">Wu et al. 2023;</ref><ref type="bibr" target="#b73">Yang et al. 2023</ref><ref type="bibr" target="#b74">Yang et al. , 2024] ]</ref> based on pretrained T2I models also support text-guided video style transfer. Although these methods effectively improve temporal consistency, they often fail to handle frames with a large action span. Reliance on a source video also undermines flexibility. Similarly, certain image-to-video(I2V) methods <ref type="bibr" target="#b3">[Blattmann et al. 2023;</ref><ref type="bibr" target="#b71">Xing et al. 2024</ref><ref type="bibr" target="#b72">Xing et al. , 2023] ]</ref> demonstrate capabilities in stylized video generation, particularly in the anime domain. However, I2V models still face challenges when tased with interpreting and animating highly artistic images, producing frames that veer towards realism, since real-world videos dominated its training data.</p><p>VideoComposer <ref type="bibr" target="#b66">[Wang et al. 2024]</ref> focuses on controllable video generation, allowing multiple conditional input to govern the video generation, including structure, motion, style, etc. Although Video-Composer enables multiple controls including style, they fail to decouple style concepts, leading to limited visual quality and motion naturalness. AnimateDiff <ref type="bibr" target="#b17">[Guo et al. 2024</ref>] employs a T2I model as a base generator and adds a motion module to learn motion dynamics, which enables extending the success of personalized T2I models(e.g., LoRA <ref type="bibr" target="#b26">[Hu et al. 2022]</ref>, Dreambooth <ref type="bibr" target="#b51">[Ruiz et al. 2023]</ref>) to video animation. However, the dependence on a personalized model restricts its ability to generate videos with arbitrary styles. Another associated research is Text2Cinemagraph <ref type="bibr" target="#b39">[Mahapatra et al. 2023]</ref>, which utilizes pretrained text-to-image models to pioneer text-guided artistic cinemagraph creation. This approach surpasses some existing text-to-video models like VideoCrafter <ref type="bibr">[Chen et al. 2023a</ref>] in generating plausible motion in artistic scenes. Nevertheless, its main limitation lies in its confined applicability, primarily to landscapes, and its tendency to generate scanty motion patterns solely for fluid elements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>We propose a method to equip pre-trained Text-to-Video (T2V) models with a style adapter, allowing for the generation of stylized videos based on both a text prompt and a style reference image. The overview is illustrated in Figure <ref type="figure" target="#fig_0">2</ref>. In this framework, the textual description dictates the video content, while the style image governs the visual style, ensuring a disentangled control over the video generation process. Given the limited availability of stylized videos, we employ a two-stage training strategy. Initially, we utilize an image dataset abundant in artistic styles to learn reference-based style modulation. Subsequently, adaptation finetuning on a mixed dataset of style images and realistic videos is conducted to improve the temporal quality of the generated videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Reference-Based Style Modulation</head><p>Our style adapter serves to extract style features from the input reference image and infuse them into the backbone features of the denoising U-Net. As mainstream T2V models <ref type="bibr">[Chen et al. 2023a</ref><ref type="bibr" target="#b8">[Chen et al. , 2024;;</ref><ref type="bibr">Wang et al. 2023c,a]</ref> are generally initialized from open-source T2I Models and trained with image and video datasets in a joint strategy, they support not only text-to-video generation but also retain the capacity for text-to-image generation. To overcome the scarcity of stylized videos, we propose to train the style adapter based on a pre-trained T2V model (i.e. VideoCrafter <ref type="bibr">[Chen et al. 2023a]</ref>) for stylized image generation under the supervision of stylistic images.</p><p>Content-Style Decoupled Data Augmentation. We use the stylistic images from two publicly available datasets, i.e. WikiArt <ref type="bibr" target="#b43">[Phillips and Mackintosh 2011]</ref> and a subset of Laion-Aesthetics <ref type="bibr" target="#b52">[Schuhmann et al. 2022</ref>] (aesthetics score above 6.5). In the original image-caption pairs, we observe that the captions generally contain both content and style descriptions, and some of them do not match the image content well. To promote the content-style decoupling, we use BLIP-2 <ref type="bibr">[Li et al. 2023a]</ref> to regenerate captions for the images and remove certain forms of style description (e.g., a painting of ) with regular expressions. In addition, as an image contains both style and content information, it is necessary to construct a decoupling supervision strategy to guarantee the extracted style feature free of content features. Although a stylistic image may contain different Based on this insight, we process each stylistic image to obtain the target image and style image through different strategies: for target image, we scale the shorter side of the image to 512 and then crop the target content from the central area; for style image, we scale the shorter side of the image to 800 and randomly crop a local patch with 512 √ó 512. This approach reduces the overlap between the style reference and generation target, while still preserving the global style semantics complete and consistent.</p><p>Style Embedding Extraction. CLIP <ref type="bibr" target="#b47">[Radford et al. 2021</ref>] has demonstrated remarkable capability in extracting visual features from open-domain images. To capitalize on this advantage, we employ a pre-trained CLIP image encoder as a feature extractor. Specifically, we utilize both the global semantic token and the full 256 local tokens (i.e., from the final layer of the Transformer) since our desired style embedding should not only serve as an accurate style trigger for the T2V model, but also provide auxiliary feature references. As image tokens encompass both style and content information, we further employ a trainable Query Transformer (Q-Former) <ref type="bibr">[Li et al. 2023a</ref>] to extract style embedding F ùë† . We create ùëÅ learnable style query embeddings as input for the Q-Former, which interact with image features through self-attention layers. Note that this is a commonly adopted architecture for visual condition extraction <ref type="bibr">[Li et al. 2023a;</ref><ref type="bibr" target="#b53">Shi et al. 2023;</ref><ref type="bibr" target="#b72">Xing et al. 2023;</ref><ref type="bibr" target="#b75">Ye et al. 2023</ref>]. But it is the style-content fusion mechanism that makes our proposed design novel and insightful for style modulation, as detailed below.</p><p>Adaptive Style-Content Fusion. With the extracted style embedding, there are two ways to combine the style and text conditions, including (i) attach-to-text <ref type="bibr">[Huang et al. 2023a;</ref><ref type="bibr">Li et al. 2023b;</ref><ref type="bibr" target="#b48">Ramesh et al. 2022]</ref>: attach the style embedding to the text embedding and then interact with the backbone feature via the originally text-based cross-attention as a whole; (ii) dual cross-attention <ref type="bibr" target="#b69">[Wei et al. 2023;</ref><ref type="bibr" target="#b75">Ye et al. 2023</ref>]: adding a new cross-attention module for the style embedding and then fuse the text-conditioned feature and styleconditioned feature. According to our experiment (see Sec. 4.4), solution (ii) surpasses solution (i) in disentangling the roles of text and style conditions, therefore we have adopted it as our final solution. The formula can be written as:</p><formula xml:id="formula_0">F ùëñ ùëúùë¢ùë° = TCA(F ùëñ ùëñùëõ , F ùë° ) + ùë† ùëñ * LN(SCA(F ùëñ ùëñùëõ , F ùë† )),<label>(1)</label></formula><p>where F ùëñ ùëñùëõ denotes the backbone feature of layer ùëñ, LN denotes layer normalization, and TCA and SCA denote text-based cross attention and style-based cross attention respectively. ùë† ùëñ is a scale factor learned by a context-aware scale factor prediction network, to balance the magnitudes of text-based feature and style-based feature. The motivation is that different stylistic genres may have different emphasis on content expression. For example, the abstract styles tend to diminish the concreteness of the content, while realism styles tend to highlight the accuracy and specificity of the content. So, we propose a context-aware scale factor prediction network to predict fusion scale factors according to the input contexts. Specifically, we create a learnable factor query, it interacts with textual features F ùë° and style features F ùë† to generate scale features via a Q-Former and then project it into layer-wise scale factors s ‚àà R 16 . Figure <ref type="figure" target="#fig_1">3</ref> illustrates the learned scale factors across multiple contexts. It shows that the adaptive scale factors have a strong correlation with style genres while also depending on the text prompts. Style references with rich style-semantics(i.e., ukiyo-e style) typically yield higher scale factors to emphasize style; while complex prompts tend to produce lower scale factors to enhance content control. This is consistent with our hypothesis to motivate our design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Temporal Adaptation to Stylized Features</head><p>Given a pre-trained T2V model, the style adapter trained on image dataset works well for stylized image generation. However, it still struggles to generate satisfactory stylized videos, which is vulnerable to temporal jittering and visual artifacts. The possible causes are that the cross-frame operations, i.e. temporal self-attention, do not involve in the process of stylized image generation, and thus induce incompatible issues. So, it is necessary to finetune the temporal self-attention with the style adapter incorporated. Following the practice of T2V image and video joint training, the finetuning is performed on the mixed datasets of stylistic images and photorealistic videos. This is an adaptation training of temporal blocks while the other modules remain frozen, and the model converges efficiently.</p><p>Classifier-Free Guidance for Multiple Conditions. Unlike T2I models, video models exhibit a higher sensitivity to style guidance due to their limited stylized generation capabilities. Using a unified ùúÜ for both style and context guidance may lead to undesirable generation results. Regarding this, we adopt a more flexible mechanism for multiple conditions classifier-free guidance. Building upon the vanilla text-guided classifier-free guidance, which controls context alignment by contrasting textual-conditioned distribution ùúñ (ùëß ùë° , ùëê ùë° ) with unconditional distribution ùúñ (ùëß ùë° , ‚àÖ), we introduce the style guidance with ùúÜ ùë† by emphasizing the difference between the text-style-guided distribution ùúñ (ùëß ùë° , ùëê ùë° , ùëê ùë† ) and the text-guided distribution ùúñ (ùëß ùë° , ùëê ùë° ). The complete formulation is as below:</p><formula xml:id="formula_1">Œµ (ùëß ùë° , ùëê ùë° , ùëê ùë† ) = ùúñ (ùëß ùë° , ‚àÖ) + ùúÜ ùë† (ùúñ (ùëß ùë° , ùëê ùë° , ùëê ùë† ) -ùúñ (ùëß ùë° , ùëê ùë° )) + ùúÜ ùë° (ùúñ (ùëß ùë° , ùëê ùë° ) -ùúñ (ùëß ùë° , ‚àÖ)),<label>(2)</label></formula><p>where ùëê ùë° and ùëê ùë† denote textual and style condition respectively. ‚àÖ denotes using no text or style conditions. In our experiment, we follow the recommended configuration of text guidance in VideoCrafter <ref type="bibr">[Chen et al. 2023a</ref>], setting ùúÜ ùë° = 15.0, while the style guidance is configured with ùúÜ ùë† = 7.5 empirically. Similarly, we set ùúÜ ùë° = 7.5 and ùúÜ ùë† = 5.0 for style-guided image generation.</p><p>4 Experimental Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental settings</head><p>Implementation Details. We adopt the VideoCrafter <ref type="bibr">[Chen et al. 2023a]</ref> as our base T2V model, which shares the same spatial weights with Stable Diffusion 2.1. We first train the style modulation on image dataset, i.e. WikiArt <ref type="bibr" target="#b43">[Phillips and Mackintosh 2011]</ref> and Laion-Aesthetics-6.5+ <ref type="bibr" target="#b52">[Schuhmann et al. 2022</ref>] for 40k steps with a batch size of 32 per GPU. In the second stage, we froze the style modulation part and only train temporal blocks of VideoCrafter, we jointly train image datasets and video datasets(subset of WebVid-10M <ref type="bibr" target="#b2">[Bain et al. 2021]</ref>) for 20k steps with a batch size of 1 on video data and 16 on image data, sampling image batches with a ratio of 20%. The training process is performed on 8 A100 GPUs and can be completed within 3 days. Furthermore, to ensure a fair comparison with some SDXL-based models <ref type="bibr" target="#b20">[Hertz et al. 2023;</ref><ref type="bibr" target="#b75">Ye et al. 2023]</ref> on stylized image generation, we also trained the first stage of StyleCrafter on SDXL <ref type="bibr">[Podell et al. 2023a</ref>].</p><p>Testing Datasets. To evaluate the effectiveness and generalizability of our method, we construct testsets comprising content prompts and style references. For content prompts, we use <ref type="bibr">GPT-4 [OpenAI 2023</ref>] to generate recognizable textual descriptions from four metacategories (human, animal, object, and landscape). We manually filter out low-quality prompts, retaining 20 image prompts and 12 video prompts. For style references, we collect 20 stylized images and 8 sets of style images with multi-reference (each contains 5 to 7 images in similar styles) from the Internet. In total, the test set contains 400 pairs for stylized image generation, and 300 pairs for stylized video generation (240 single-reference pairs and 60 multi-reference pairs). Details are available in the supplementary materials.</p><p>Evaluation Metrics. Following previous practice <ref type="bibr" target="#b56">[Sohn et al. 2023;</ref><ref type="bibr">Wang et al. 2023b;</ref><ref type="bibr" target="#b77">Zhang et al. 2023]</ref>, we employ CLIP-based <ref type="bibr" target="#b47">[Radford et al. 2021</ref>] scores and DINO-based <ref type="bibr" target="#b5">[Caron et al. 2021</ref>] scores to measure the text alignment and style conformity. Following Eval-Crafter <ref type="bibr" target="#b38">[Liu et al. 2023]</ref>, we measure the temporal consistency of video generation by (i) calculating clip scores between contiguous frames and (ii) calculating the warping error on every two frames with estimated optical flow. Note that these metrics are not perfect. For example, one can easily achieve a close-to-1 style score by entirely replicating the style reference. Similarly, stylized results may yield inferior text scores compared to realistic results, even though both accurately represent the content descriptions. We recommend a comprehensive consideration of both CLIP-based text scores and style scores, rather than relying solely on a single metric.</p><p>User Preference Study. In addition to quantitative analysis, we conducted a user study to make comparisons among our method, VideoCrafter, Gen-2, and AnimateDiff in the context of singlereference and multi-reference stylized video generation. Users are instructed to select their preferred option based on style conformity, temporal quality, and all options fulfill text alignment for each comparison pair. We randomly chose 15 single-reference pairs and 10 multi-reference pairs, collecting 1125 votes from 15 users. Further details can be found in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Style-Guided Text-to-Image Generation</head><p>As mentioned in Sec. 3.1 and Sec. 4.1, our proposed method also supports to generate stylized images (using model before temporal finetuning). We are interested to evaluate our method against state-of-the-art style-guided T2I synthesis methods, which are better-established than video counterparts. The competitors include optimization-based methods like DreamBooth <ref type="bibr" target="#b51">[Ruiz et al. 2023]</ref>, inversion-based methods such as InST <ref type="bibr" target="#b77">[Zhang et al. 2023</ref>] and Style-Aligned <ref type="bibr" target="#b20">[Hertz et al. 2023]</ref>, and adapter-based methods like IP-Adapter-Plus <ref type="bibr" target="#b75">[Ye et al. 2023</ref>]. Besides, we consider two unique competitors: SD* <ref type="bibr" target="#b49">[Rombach et al. 2022</ref>] and SDXL* <ref type="bibr">[Podell et al. 2023b</ref>] (text-to-image models equipped with GPT-4V [OpenAI 2023], where GPT-4V generates textual descriptions about the reference's style and merges them with content prompts as input for models). This comparison aims to validate the advantages of employing image conditions to enhance stylized generation instead of relying solely on text conditions. Implementation details of competitors are available in supplementary materials.</p><p>The quantitative comparison is tabulated in Table <ref type="table" target="#tab_0">1</ref>. Results reveal that Dreambooth <ref type="bibr" target="#b51">[Ruiz et al. 2023]</ref> and InST <ref type="bibr" target="#b77">[Zhang et al. 2023]</ref> struggle to accurately capture the style from various style references and exhibit low style conformity. SD* <ref type="bibr" target="#b49">[Rombach et al. 2022]</ref> and SDXL <ref type="bibr">[Podell et al. 2023b</ref>] demonstrate good stylistic ability but still fail to reproduce the style of the reference image, possibly because of the text's inherent clumsiness in expressing specific styles despite utilizing the powerful GPT4V for visual style understanding. IP-Adapter <ref type="bibr" target="#b75">[Ye et al. 2023</ref>] and style-aligned <ref type="bibr" target="#b20">[Hertz et al. 2023</ref>] generate aesthetically pleasing images, while their style-content decoupled learning is not perfect and exhibits limited control over content. In contrast, our method efficiently generates high-quality stylized images that align with the content of the texts and resemble the style of the reference image. Our method demonstrates stable stylized generation capabilities when dealing with various types of prompts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Style-Guided Text-to-Video Generation</head><p>Existing approaches for style-guided video generation can be divided into two categories: one is the single-reference based methods that are usually tuning-free, e.g. VideoComposer <ref type="bibr" target="#b66">[Wang et al. 2024]</ref>; the other is the multi-reference based methods that generally requires multiple references for fine-tuning, e.g. AnimateDiff <ref type="bibr" target="#b17">[Guo et al. 2024</ref>]. We make comparisons with these methods, respectively.</p><p>Single-Reference based Guidance. VideoComposer <ref type="bibr" target="#b66">[Wang et al. 2024</ref>] is a controllable video generation model that allows multiple conditional inputs including style reference image. It is a natural competitor of our method. Besides, we construct two additional comparative methods, i.e. VideoCrafter* and Gen2*, which extend VideoCrafter <ref type="bibr">[Chen et al. 2023a</ref>] and Gen2 [Gen-2 2023], the stateof-the-art T2V models in open-source and close-source channels respectively, to make use of style reference images by utilizing GPT-4V [OpenAI 2023] to generate style prompts from them. The quantitative comparison is tabulated in Table <ref type="table">2</ref>. Several typical visual examples are illustrated in Figure <ref type="figure" target="#fig_3">5</ref>.   -3 ) ‚Üì VideoComposer 0.0468 0.7306 0.9853 9.903 VideoCrafter* 0.2209 0.3124 0.9757 61.41 Ours 0.2726 0.4531 0.9892 18.73 AnimateDiff 0.2867 0.3528 0.8903 37.17 Ours(S-R) 0.2661 0.4803 0.9851 14.13 Ours(M-R) 0.2634 0.4887 0.9852 9.396</p><p>We can observe that: (i) VideoComposer tends to copy content from style references and struggles to generate text-aligned content, which is possibly because of the invalid decoupling learning. Consequently, its results exhibit abnormally high style conformity and "A wooden sailboat docked in a harbor"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AnimateDiff Ours(S-R)</head><p>"A student walking to school with backpack" very low text alignment. In addition, VideoComposer often generates static videos, thus having the lowest warping errors, but this does not mean that their results perform best in temporal quality.</p><formula xml:id="formula_2">Ours(M-R) AnimateDiff Ours(S-R) Ours(M-R)</formula><p>(ii) VideoCrafter* exhibits limited stylized generation capabilities, producing videos with diminished style and disjointed movements. Gen-2* demonstrates superior stylized generation capabilities. However, Gen-2 is still limited by the inadequate representation of style in textual descriptions, and is more prone to sudden changes in color and luminance. (iii) In comparison, our method captures styles more effectively and reproduces them in the generated results.</p><p>Multi-Reference based Guidance. AnimateDiff <ref type="bibr" target="#b17">[Guo et al. 2024</ref>] denotes a paradigm to turn personalized SD (i.e., SD fine-tuned on specific-domain images via LoRA <ref type="bibr" target="#b26">[Hu et al. 2022]</ref> or Dreambooth <ref type="bibr" target="#b51">[Ruiz et al. 2023]</ref>) for video generation, namely combined with pre-trained temporal blocks of T2V models. It can generate very impressive results if the personalized SD is carefully prepared, however, we find it struggle to achieve as satisfactory results if only a handful of style reference images are available for training. We conduct an evaluation on 60 text-style pairs with multi-references, as presented in Sec. 4.1. We train Dreambooth <ref type="bibr" target="#b51">[Ruiz et al. 2023</ref>] models for each style and incorporate them into AnimateDiff based on their released codebase. Thanks to the flexibility of Q-Former, our method also supports multiple reference images in a tuning-free fashion, i.e. computing the image embeddings of each reference image and concatenating all embeddings as input to the Q-Former.</p><p>Results are compared in Table <ref type="table" target="#tab_2">3</ref> and Figure <ref type="figure" target="#fig_4">6</ref> respectively. According to the results, AnimateDiff struggles to achieve highfidelity stylistic appearance while tends to generate close-to-realism results despite the style references are typical artistic styles. In addition, it is vulnerable to temporal artifacts. As the trained personalized-SD can generate decent stylistic images (provided in the supplementary materials), we conjecture that the performance degradation is caused by the incompatibility from the pre-trained temporal blocks and independently trained personalized-SD models, which not only interrupts temporal consistency but also weakens the stylistic effect. In contrast, our method can generate temporal consistent videos with high style conformity to the reference images and accurate content alignment with the text prompts. Furthermore, using multiple references can further promote the performance, which offers additional advantages in practical applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>Data Augmentation. We first study the effectiveness of contentstyle decoupled data augmentation. As depicted in Table <ref type="table" target="#tab_3">4</ref>, training with the original image-caption pairs restricts the model's ability to extract style representations, leading to lower style conformity. For example, as shown in Figure <ref type="figure">7</ref>, method without data augmentation fails to capture the "3D render" style from the reference.</p><p>Dual Cross-Attention. As discussed in Sec. 3.1, we make a comparison between attach-to-text and dual cross-attention to study their effects. Results are presented in Table <ref type="table" target="#tab_3">4</ref> and Figure <ref type="figure">7</ref>, revealing that attach-to-text tends to directly fuse the content from the reference image and the text prompts rather than combining the text-based content and image-based style. This indicates the effectiveness of dual cross-attention in facilitating content-style decoupling.</p><p>Adaptive Style-Content Fusion. As previously discussed in Figure <ref type="figure" target="#fig_1">3</ref>, our proposed adaptive style-content fusion module demonstrates effectiveness in adaptively processing various conditional contexts. It benefits the generalization ability of model to deal with diverse combinations of content prompt and style image. Figure <ref type="figure" target="#fig_5">8</ref> reveals that although the baseline can handle easy prompt inputs like "A little girl", it struggles to accurately generate all objects described in longer prompts. In contrast, the adaptive fusion module can achieve decent text alignment for long text descriptions thanks to its flexibility to adaptive balance between content and style. Two-Stage Training Scheme. Our proposed training scheme consists of two stages, i.e., style adapter training and temporal adaption. To show its necessity, we build two baselines: (i) w/o Temporal Adaption: that we train a style adapter on image data and apply it directly to stylized video generation without finetuning; (ii) joint training: that we conduct style adapter training and temporal blocks finetuning on image-video dataset simultaneously. As depicted in Table <ref type="table" target="#tab_4">5</ref>, baseline (i) exhibits inferior temporal consistency when applied directly to video, and undermines the content alignment and style conformity. As for baseline (ii), the learning of style embedding extraction seems to be interfered by the joint finetuning of temporal blocks, which impedes it to generate desirable stylized videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Limitations</head><p>We have presented StyleCrafter, a generic method enabling pretrained T2V model for video generation in any style by providing a reference image. To achieve this, we made exploration in three aspects, including the architecture of style adapter, the content and style feature fusion mechanism, and some tailor-made strategies for data augmentation and training stylized video generation without stylistic video data. All of these components allow our method to generate high quality stylized videos that align with text prompts and conform to style references. Extensive experiments have evidenced the effectiveness of our proposed designs and comparisons with existing competitors demonstrate the superiority of our method in visual quality and efficiency. Anyway, our method also has certain limitations, e.g., unable to generate desirable results when the reference image can not represent the target style sufficiently or the presented style is extremely unseen. Further explorations are demanded to address those issues.</p><p>Our Supplementary Material consists of 7 sections:</p><p>‚Ä¢ Section A provides a detailed statement of our experiments, including the implementation details of comparison methods, and details of our test set.</p><p>‚Ä¢ Section B provides a detailed statement of our evaluation, including the details of evaluation metrics, and details of the user study.</p><p>‚Ä¢ Section C adds more comparison experiments, including the comparison with StyleDrop, comparison in multi-reference stylized image generation, and comparison with style transfer methods. ‚Ä¢ Section D adds additional ablation study on different adapter architecture. ‚Ä¢ Section E explores the extended application of StyleCrafter, including the collaboration with depth control. ‚Ä¢ Section F demonstrates more results of our methods. ‚Ä¢ Section G discusses the limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Implementation Details A.1 Comparison methods</head><p>For all comparison methods, we follow the instructions from the official papers and open-source implementations. Since some methods including Dreambooth and InST require additional finetuning, we provide all implementation details as follows:</p><p>Dreambooth. Dreambooth <ref type="bibr" target="#b51">[Ruiz et al. 2023</ref>] aims to generate images of a specific concept (e.g., style) by finetuning the entire textto-image model on one or serveral images. We train Dreambooth based on Stable Diffusion 1.5. The training prompts are obtained from BLIP-2 <ref type="bibr">[Li et al. 2023a</ref>], and we manually add a style postfix using the rare token "sks". For example, "two slices of watermelon on a red surface in sks style" is used for the first style reference in Table <ref type="table" target="#tab_2">S3</ref>. We train the model for 500 steps for single-reference styles and 1500 steps for multi-reference styles, with learning rates of 5 √ó 10 -6 and a batch size of 1. The training steps are carefully selected to achieve the balance between text alignment and style conformity.</p><p>InST. InST <ref type="bibr" target="#b77">[Zhang et al. 2023</ref>] propose a inversion-based method to achieve style-guided text-to-image generation through learning a textual description from style reference. We train InST for 1000 steps with learning rates of 1 √ó 10 -4 and a batch size of 1.</p><p>StableDiffusion 2.1 and SDXL. We extend Stable Diffusion to styleguided text-to-video gerneration by utilizing GPT-4v to generate style descriptions from style reference. Details about style descriptions can be found in Table <ref type="table" target="#tab_2">S3</ref> IP-Adapter. IP-Adapter <ref type="bibr" target="#b75">[Ye et al. 2023]</ref> propose to train an imageconditioned adapter to generate images from image prompts. We use the official checkpoint of IP-Adapter-Plus(SDXL) for evaluation. Note that IP-Adapter is primarily designed for image variants and other editing tasks. When conducted with its default scale value ùë† = 1, IP-Adapter tends to simply reconstruct style references, which actually underestimates the ability of IP Adapters in stylized generation. During the evaluation, we adjust the scale value to 0.5 to ensure a more balanced comparison.</p><p>Style Aligned. Style Aligned <ref type="bibr" target="#b20">[Hertz et al. 2023</ref>] design a selfattention sharing mechanism to ensure constant style among different samples, supporting both stylized generation and style transfer tasks. We conduct the official implementation on SDXL during the evaluation.</p><p>VideoCrafter and Gen-2. Similar to SD*, We use VideoCrafter <ref type="bibr">[Chen et al. 2023a</ref>] 320 √ó 512 Text2Video Model and Gen-2 [Gen-2 2023] equipped with GPT-4v to generate stylized videos from style references and text prompts.</p><p>AnimateDiff. AnimateDiff <ref type="bibr" target="#b17">[Guo et al. 2024</ref>] aims to extend personalized T2I model(i.e., Dreambooth or LoRA <ref type="bibr" target="#b26">[Hu et al. 2022]</ref>) for video generation. To compare with AnimateDiff, we first train personalized dreambooth models for each group of multi-reference style images, then we incorporate them into AnimateDiff based on their released codebase. We did not use LoRA because we observed that AnimateDiff fails to turn LoRA-SD for video generation in most cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Testing Datasets</head><p>We provide a detailed description of the testing datasets.</p><p>Content Prompts. We utilize GPT4 to generate prompts across four meta-categories: human, animal, object, and landscape. Initially, 15/10 prompts (for images/videos) were generated in each category. Recognized as low-quality prompts, semantically repeated prompts, containing style descriptions, and other less informative ones were manually filtered, leading to 5/3 prompts per category. For video prompts, we specifically encouraged the generation of scenarios involving motion. The final prompts in testset are provided in Table <ref type="table" target="#tab_0">S1</ref> and Table <ref type="table">S2</ref>.</p><p>Style References. We collect 20 diverse single-reference stylized images and 8 sets of style images with multi-reference(each contains 5 to 7 images in similar styles) from the Internet<ref type="foot" target="#foot_3">foot_3</ref> . Besides, for the comparison with the Text-to-Image model including Stable Diffusion and the Text-to-Video model including VideoCrafter and Gen-2, we extend them to stylized generation by equipped them with GPT-4v to generate textual style descriptions from style reference. We provide style references and corresponding style descriptions in Table <ref type="table" target="#tab_2">S3</ref> and Figure <ref type="figure">S1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Evaluation Details B.1 Evaluation Metrics</head><p>We employ CLIP-based similarity scores to evaluate text alignment and style conformity, as is commonly done by existing methods. Additionally, we include two metrics to measure the temporal consistency of generated videos, i.e., CLIP-Temp and Warping Error. A detailed calculation process for each metric is presented below. CLIP-Text. We utilize the pretrained CLIP-ViT-H-14-laion2B-s32B-b79K<ref type="foot" target="#foot_4">foot_4</ref> as a feature extractor(also for CLIP-Style and CLIP-Temp), which is trained on LAION-2B and demonstrates enhanced performance across various datasets. We extract the frame-wise image A rowboat docked on a peaceful lake. Object A winding path through a tranquil garden. Landscape A lighthouse standing tall on a rocky coast. Object An ancient temple surrounded by lush vegetation. Landscape A rustic windmill in a field. Object A serene mountain landscape with a river flowing through it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Landscape</head><p>Table <ref type="table">S2</ref>.</p><p>Text prompts used in the testset for video generation Prompt Meta Category Prompt Meta Category A street performer playing the guitar. Human A bear catching fish in a river. Animal A chef preparing meals in kitchen. Human A knight riding a horse through a field. Animal A student walking to school with backpack. Human A wolf walking stealthily through the forest. Animal A campfire surrounded by tents. Object A river flowing gently under a bridge. Landscape A hot air balloon floating in the sky. Object A field of sunflowers on a sunny day. Landscape A rocketship heading towards the moon. Object A wooden sailboat docked in a harbor. Landscape Fig. S2. User Preference Study Interface Table S3. Style references in the testset and corresponding style descriptions generated from GPT-4v[OpenAI 2023]. Style Reference Style Descriptions Style Reference Style Descriptions 3D Digital Art, {prompt}, whimsical and modern, smooth and polished surfaces, bold and contrasting colors, soft shading and lighting, surreal representation.</p><p>Digital Painting, {prompt}, detailed rendering, vibrant color palette, smooth gradients, realistic light and reflection, immersive natural landscape scene.</p><p>Manga-inspired digital art, {prompt}, dynamic composition, exaggerated proportions, sharp lines, cel-shading, high-contrast colors with a focus on sepia tones and blues.</p><p>Childlike watercolor, {prompt}, simple brush strokes, primary and secondary colors, bold outlines, flat washes, playful, spontaneous, and expressive.</p><p>Comic book illustration, {prompt}, digital medium, clean inking, cell shading, saturated colors with a natural palette, and a detailed, textured background.</p><p>Pixel art illustration, {prompt}, digital medium, detailed sprite work, vibrant color palette, smooth shading, and a nostalgic, retro video game aesthetic.</p><p>Ink and watercolor on paper, {prompt}, urban sketching style, detailed line work, washed colors, realistic shading, and a vintage feel.</p><p>Flat Vector Illustration, {prompt}, simplified shapes, uniform color fills, minimal shading, absence of texture, clean and modern aesthetic.</p><p>Watercolor and ink illustration, {prompt}, traditional comic style, muted earthy color palette, detailed with a sense of movement, soft shading, and a historic ambiance.</p><p>Low Poly Digital Art, {prompt}, geometric shapes, vibrant colors, flat texture, sharp edges, gradient shading, modern graphic style.</p><p>Chinese ink wash painting, {prompt}, minimalistic color use, calligraphic brushwork, emphasis on flow and balance, with poetic inscription.</p><p>Chinese Ink Wash Painting, {prompt}, monochromatic palette, dynamic brushstrokes, calligraphic lines, with a focus on negative space and movement.</p><p>Manga Style, {prompt}, black and white digital inking, high contrast, detailed line work, crosshatching for shadows, clean, no color.</p><p>Line Drawing, {prompt}, simple and clean lines, monochrome palette, smooth texture, minimalist and cartoonish representation .</p><p>Van Gogh's "Starry Night" style, {prompt}, with expressive, swirling brushstrokes, rich blue and yellow palette, and bold, impasto texture.</p><p>Watercolor Painting, {prompt}, fluid brushstrokes, transparent washes, color blending, visible paper texture, impressionistic style.</p><p>Van Gogh-inspired pen sketch, {prompt}, dynamic and swirling line work, monochromatic sepia tones, textured with a sense of movement and energy.</p><p>Ukiyo-e Woodblock Print, {prompt}, gradation, limited color palette, flat areas of color, expressive line work, stylized wave forms, traditional Japanese art.</p><p>Watercolor Painting, {prompt}, fluid washes of color, wet-on-wet technique, vibrant hues, soft texture, impressionistic portrayal.</p><p>Victorian watercolor, {prompt}, fine detail, soft pastel hues, gentle lighting, clear texture, with a quaint, realistic portrayal of everyday life.</p><p>embeddings from the generated results and text embeddings from the input content prompts, then compute their average cosine similarity. The overall CLIP-Text is calculated as:</p><formula xml:id="formula_3">ùëÜ ùë°ùëíùë•ùë° = 1 ùëÄ ùëÄ ‚àëÔ∏Å ùëñ=1 (<label>1</label></formula><formula xml:id="formula_4">ùëá ùëá ‚àëÔ∏Å ùë° =1 ùëíùëöùëè (ùë• ùëñ ùë° ) ‚Ä¢ ùëíùëöùëè (ùëù ùëñ ) ‚à•ùëíùëöùëè (ùë• ùëñ ùë° )‚à• ‚Ä¢ ‚à•ùëíùëöùëè (ùëù ùëñ )‚à• )<label>(3)</label></formula><p>where ùëÄ represents the total number of testing videos and ùëá represents the total number of frames in each video(ùëá = 1 for image generation), ùëíùëöùëè (ùë• ùëñ ùë° ) and ùëíùëöùëè (ùëù ùëñ ) indicate the CLIP embedding of the ùë°-th frame of the ùëñ-th video ùë• ùëñ ùë° and the corresponding prompt ùëù ùëñ , respectively. CLIP-Style. Similarly, we extract the frame-wise image embeddings ùëíùëöùëè (ùë• ùëñ ùë° ) from the generated results and image embeddings ùë† ùëñ from the input style reference. The overall CLIP-Text is calculated as:</p><formula xml:id="formula_5">ùëÜ ùë†ùë° ùë¶ùëôùëí = 1 ùëÄ ùëÄ ‚àëÔ∏Å ùëñ=1 (<label>1</label></formula><formula xml:id="formula_6">ùëá ùëá ‚àëÔ∏Å ùë° =1 ùëíùëöùëè (ùë• ùëñ ùë° ) ‚Ä¢ ùëíùëöùëè (ùë† ùëñ ) ‚à•ùëíùëöùëè (ùë• ùëñ ùë° )‚à• ‚Ä¢ ‚à•ùëíùëöùëè (ùë† ùëñ )‚à• )<label>(4)</label></formula><p>CLIP-Temp. Considering the semantic consistency between every two frames, we extract the frame-wise CLIP image embeddings and compute the cosine similarity between each of the two frames, as follows:</p><formula xml:id="formula_7">ùëÜ ùë°ùëíùëöùëù = 1 ùëÄ ùëÄ ‚àëÔ∏Å ùëñ=1 ( 1 ùëá -1 ùëá -1 ‚àëÔ∏Å ùë° =1 ùëíùëöùëè (ùë• ùëñ ùë° ) ‚Ä¢ ùëíùëöùëè (ùë• ùëñ ùë° +1 ) ‚à•ùëíùëöùëè (ùë• ùëñ ùë° )‚à• ‚Ä¢ ‚à•ùëíùëöùëè (ùë• ùëñ ùë° +1 )‚à• )<label>(5)</label></formula><p>Warping Error. For the warping error, we first obtain the optical flow between each two frames using RAFT-small <ref type="bibr" target="#b59">[Teed and Deng 2020]</ref>, a pre-trained optical flow estimation network. Subsequently, we compute the pixel-wise differences between the warped image and the predicted image, as follows:</p><formula xml:id="formula_8">ùëä ùëíùëüùëüùëúùëü = 1 ùëÄ ùëÄ ‚àëÔ∏Å ùëñ=1 ( 1 ùëá -1 ùëá -1 ‚àëÔ∏Å ùë° =1 ‚à•ùë• ùëñ ùë° -ùë§ùëéùëüùëù (ùë• ùëñ ùë° +1 , ùëì ùëôùëúùë§ (ùë• ùëñ ùë° +1 , ùë• ùëñ ùë° ))‚à•) (6)</formula><p>where ùë§ùëéùëüùëù (ùë• ùëñ ùë° +1 , ùëì ùëôùëúùë§ (ùë• ùëñ ùë° +1 , ùë• ùëñ ùë° )) represents the warped frame of ùë• ùëñ ùë° +1 using the optical flow between frame ùë• ùëñ ùë° +1 and frame ùë• ùëñ ùë° .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 User Study</head><p>In this subsection, we provide a detailed introduction about our user study. We randomly selected 15 single-reference style-text pairs to compare the generated results among VideoCrafter <ref type="bibr">[Chen et al. 2023a</ref>], Gen-2 [Gen-2 2023], and our proposed method. Given that videocomposer <ref type="bibr" target="#b66">[Wang et al. 2024</ref>] directly replicates the style reference and is minimally influenced by the prompt in most cases, we excluded it from the comparison in the user study. Additionally, we randomly chose 10 multi-reference style-text pairs for the comparison between AnimateDiff <ref type="bibr" target="#b17">[Guo et al. 2024</ref>] (multiple style-specific models) and our method (a generic model). To ensure a blind comparison, we randomized the order of options for each question and masked the possible model watermark in the lower right corner. The designed user preference interface is illustrated in Figure <ref type="figure" target="#fig_0">S2</ref>. We invited 15 users of normal eyesight to evaluate the generated results in three aspects: text alignment, style conformity, and temporal quality. The instructions and questions are provided as below. Consequently, a total of 1125 votes are collected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instructions.</head><p>‚Ä¢ Task: Watch the following video results generated from the style reference and text description, with 3 sub-questions for each set of comparisons (please separately review the generated results from the following three perspectives:</p><p>-Text Alignment (multiple choice, means that the content of the generated video is aligned with the text description(prompt), and the content of the text description should appear in the generated result); -Style Conformity (single choice, means that the style of the generated video is consistent with the style of the reference image, where the style includes both the color tone, texture, brush strokes, etc., as well as the painting style, emotion, and mood); -Temporal Quality (single choice, consists of two aspects:</p><p>First, the generated video should include certain action or camera movement, and should be in line with the picture context; Second, the content of the picture should be coherent, without abrupt changes or flickering); ‚Ä¢ Please ignore the watermark effect and the missing area in the bottom right corner of the result. Questions.</p><p>‚Ä¢ Which one is aligned with text description? [Multiple choice] ‚Ä¢ Which performs best in Style Conformity? [Single choice] ‚Ä¢ Which performs best in Temporal Quality? [Single choice] C Extended Comparison C.1 Multi-reference Stylized Image Generation</p><p>We conduct comparisons of multi-reference stylized image generation with Dreambooth <ref type="bibr" target="#b51">[Ruiz et al. 2023]</ref> and CustomDiffusion <ref type="bibr" target="#b33">[Kumari et al. 2023]</ref>, both of which support generating images in specific styles by finetuning on the reference images. Figure <ref type="figure">S1</ref> and Table <ref type="table" target="#tab_3">S4</ref> present the visual and quantitative results respectively, demonstrating that our method surpasses all competitors in terms of style conformity for multi-reference stylized generation. Although Dreambooth and CustomDiffusion exhibit competitive performance in certain cases, their stylized generation abilities tend to vary with different prompts, i.e., struggling to maintain consistent visual styles across arbitrary prompts. It is possibly because several images are insufficient to allow the model to disentangle the contents and styles, thus harming the generalization performance. Besides, the requirement for finetuning during the testing process also undermines their flexibility. In contrast, our method efficiently generates high-quality stylized images that align with the prompts and conform the style of reference images without additional finetuning costs.</p><p>Table S4. Quantitative comparison on Multi-reference style-guided T2I generation. Bold: Best. Methods Dreambooth CustomDiffsion Ours Text ‚Üë 0.2868 0.2986 0.2924 Style ‚Üë 0.4270 0.4441 0.5333 Style Reference (a) DreamBooth (b) CustomDiffusion (c) Ours </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Comparison with StyleDrop</head><p>Here we present a supplementary comparison with StyleDrop <ref type="bibr" target="#b56">[Sohn et al. 2023]</ref>. StyleDrop proposes a versatile method for synthesizing images that faithfully follow a specific style using a text-to-image model. Owing to the absence of an official StyleDrop implementation, we have excluded the comparison with StyleDrop from the main text. Instead, we include a comparison with an unofficial Style-Drop implementation 3 here. We train StyleDrop based on StableDiffusion 2.1 for 1000 steps with a batch size of 8 and a learning rate of 3 √ó 10 -4 . The quantitative and qualitative results are presented in Table <ref type="table" target="#tab_4">S5</ref> and Figure <ref type="figure" target="#fig_3">S5</ref> respectively. Results show that compared to StyleDrop, our proposed method more effectively captures the visual characteristics of a user-provided style and combines them with various prompts in a flexible manner.</p><p>Table S5. Quantitative comparison with StyleDrop. Methods StyleDrop Ours(SD 2.1) Text ‚Üë 0.2389 0.3028 Style ‚Üë 0.3962 0.4836 3 <ref type="url" target="https://github.com/aim-uofa/StyleDrop-PyTorch">https://github.com/aim-uofa/StyleDrop-PyTorch</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Comparison with Style Transfer Methods</head><p>In this section, we perform comparisons with a particular set of competitors, i.e., style transfer methods. As style transfer methods require extra source images or videos for content provision, we utilize text-to-image models(Stable Diffusion 2.1) and text-to-video models(VideoCrafter) to initially generate visual content. Subsequently, we apply existing style transfer methods based on the reference to produce the stylized output. For stylized image generation, we opt for T2I + CAST <ref type="bibr" target="#b78">[Zhang et al. 2022]</ref>, while for stylized video generation, we choose T2V + MCCNet <ref type="bibr" target="#b10">[Deng et al. 2021]</ref>. The results of this comparison are illustrated in the Table <ref type="table">S6</ref> and Table <ref type="table">S7</ref>. Both two competitors underperform our approach. The reasons are in two aspects: (i) Style transfer methods mainly work well on transferring tones and local textures while fall short in transferring semantic style features; (ii) Style transfer method cannot change the structure of the content image, which hinders the generation of styles that associated with special geometry features, e.g. layouts of logo style and 3D render style.</p><p>Table S8. Ablation studies on style feature extractor architecture. The performance is evaluated on the style-guided T2I generation. Alternatives MLP Transformer Q-Former (Ours) Text ‚Üë 0.3415 0.3221 0.3028 Style ‚Üë 0.2843 0.4149 0.4836  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Application Extension</head><p>In this section, we further explore the compatibility with additional controllable conditions, e.t., depth. Following the approach of structure control in Animate-A-Story <ref type="bibr" target="#b18">[He et al. 2023]</ref>, we introduce video structure control by integrating a well-trained depth adapter into the base T2V model. Note that StyleCrafter and depth-adapter are trained independently, the only operation we take is to combine the both during the inference stage. Instead of employing DDIM Inversion to ensure consistency, we generate the videos from random noise. The visual comparison with VideoComposer <ref type="bibr" target="#b66">[Wang et al. 2024</ref>] is present in Figure <ref type="figure">S4</ref>. VideoComposer struggles to produce results faithful to text descriptions when faced with artistic styles, such as the "boat" mentioned in the prompt. In contrast, our method not only supports collaboration with depth guidance, but also generates videos with controllable content, style, and structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F More Results</head><p>In this section, we provide more visual results and comparisons of our method. Specifically, we provide: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Limitations</head><p>While our proposed method effectively handles most common styles, it does have certain limitations. Firstly, since StyleCrafter is developed based on existing T2I models and T2V models, such as SDXL and VideoCrafter, it unavoidably inherits part of the base model's shortcomings, such as fixed resolution and video length, less satisfactory temporal consistency. For example, our method fails to generate high-definition faces in certain cases, as shown in Figure <ref type="figure" target="#fig_12">S7</ref>. Despite the fact that our approach successfully enhances the stylistic generation capacity of T2I/T2V models, leveraging a powerful base model will always amplify its performance, as showcased in the superior style conformity of StyleCrafter(SDXL) over StyleCrafter(SD2).</p><p>Besides, artistic style is a very comprehensive perceptual feeling and visual styles are considerably more complex than what we explore in our paper. Our model may produce just passable results when confronted with reference images possessing highly stylized semantics. For example, as depicted in Figure <ref type="figure" target="#fig_12">S7</ref>, although our model successfully reproduces ink strokes, there are still discrepancies with reference images in the aesthetic level, such as the lack of "blankleaving" in the generation results. Additionally, considering the absence of stylized video data, our stylized video generation results are somewhat less satisfactory than stylized image generation in visual style expression. A possible solution is to collect sufficient stylized video data for training, which we leave for further work.  Style Reference (i) (ii) (iii) (iv) (v) </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Overview of our proposed style adapter. It consists of three components, i.e. style feature extractor, dual cross-attention module, and contextaware scale factor predictor.</figDesc><graphic coords="3,361.23,159.44,79.05,63.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Illustration of content-style fusion scale factors across multiple input pairs. Four short prompts(less than 5 words) with prompt id ‚àà [1, 4] and four long prompts(more than 8 words) with prompt id ‚àà [5, 8] are randomly selected. Results indicate that shorter prompts and images with richer stylesemantics tend to have relatively higher scale factors.</figDesc><graphic coords="4,185.57,88.56,113.47,85.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><figDesc>Fig. 4. Visual comparison on style-guided T2I generation. Blue: methods based on SD 2.1. Green: based on SDXL. Prompt: A rabbit nibbling on a carrot.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Visual comparison of single-reference guided T2V generation. Vid.Comp.: VideoComposer, Vid.Craf.: VideoCrafter Table 2. Quantitative comparison of style-guided T2V generation. Top 3 rows: single-reference based gudiance. Bottom 3 rows: multi-reference based guidance. S-R: Single-Refernce, M-R: Multi-Reference, W.E.: Warping Errors. Methods CLIP-Text ‚Üë CLIP-Style ‚Üë Temporal Consistency CLIP-Temp ‚Üë W.E.(√ó10 -3 ) ‚Üì</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Qualitative comparison of multi-reference style-guided T2V generation. S-R: Single-Reference, M-R: Multi-Reference</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Effect of adaptive content-style fusion. It shows superiority in generalization to extreme cases, e.g. long text description. Two text prompts are used: (i) A little girl; (ii) A little girl reading a book in the park, with a telescope nearby pointed at the sky.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><figDesc>Fig. S1. Multiple references in the testset Table S1. Text prompts used in the testset for image generation Prompt Meta Category Prompt Meta Category A man playing the guitar on a city street. Human A flock of birds flying gracefully in the sky. Animal A woman reading a book in a park. Human A colorful butterfly resting on a flower. Animal A couple dancing gracefully together. Human A bear fishing in a river. Animal A person sitting on a bench, feeding birds. Human A dog running in front of a house. Animal A person jogging along a scenic trail. Human A rabbit nibbling on a carrot. Animal A bouquet of flowers in a vase. Object A cobblestone street lined with shops and cafes. Landscape A telescope pointed at the stars. Object A modern cityscape with towering skyscrapers. Landscape</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. S3 .</head><label>S3</label><figDesc>Fig. S3. Visual comparison on mulit-reference stylized T2I generation. Testing prompts: (i) A rustic windmill in a field.; (ii) A person jogging along a scenic trail.; (iii) A flock of birds flying gracefully in the sky.; (iv) A rowboat docked on a peaceful lake.; (v) An ancient temple surrounded by lush vegetation.</figDesc><graphic coords="16,173.39,342.63,357.36,57.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. S6 .</head><label>S6</label><figDesc>Fig. S6. Visual comparison between different adapter architectures. Prompt: A couple dancing gracefully together.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><figDesc>(i) style-guided text-to-image generation results on StyleCrafter(SD2) and StyleCrafter(SDXL) in Figure S8 and Figure S9; (ii) comparison of single-reference stylized video generation and multi-reference stylized video generation, as illustrated in Figure S10 and Figure S11, respectively. (iii) additional stylized video results in Figure S12 and Figure S13.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><figDesc>"A street performer playing the guitar." "A student walking to school with backpack." (a) Artifacts in human faces (b) Unsatisfatory styles for complex style refernces</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. S7 .</head><label>S7</label><figDesc>Fig. S7. Failure cases of our methods. (a). Our method inherits limitations from the pretrained T2V model, tends to generate human faces with obvious artifacts. (b). Our method produces less satisifactory styles for complex style references such as Chinese Ink Painting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. S9 .Fig</head><label>S9</label><figDesc>Fig. S9. More Results of StyleCrafter(SDXL) on Style-Guided Text-to-Image Generation. Prompts: (i) A couple dancing gracefully together. (ii) A dog is running in front of a house. (iii) A bouquet of flowers in a vase. (iv) A rowboat docked on a peaceful lake. (v) A lighthouse standing tall on a rocky coast.</figDesc><graphic coords="20,166.67,476.81,72.64,72.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative comparison on single-reference style-guided T2I generation. We conduct evaluation on a test set of 400 pairs. Bold: Best.</figDesc><table><row><cell>Method</cell><cell cols="4">Stable Diffusion 2.1 based</cell><cell></cell><cell>SDXL based</cell></row><row><cell></cell><cell cols="2">Dreambooth InST</cell><cell>SD*</cell><cell>Ours</cell><cell cols="3">IP-Adapter-Plus Style-Aligned SDXL* Ours(SDXL)</cell></row><row><cell>CLIP-Text ‚Üë</cell><cell>0.3047</cell><cell cols="3">0.3004 0.2766 0.3028</cell><cell>0.2768</cell><cell>0.2254</cell><cell>0.2835</cell><cell>0.2918</cell></row><row><cell>CLIP-Style ‚Üë</cell><cell>0.3459</cell><cell cols="3">0.3708 0.4183 0.4836</cell><cell>0.5182</cell><cell>0.5515</cell><cell>0.4348</cell><cell>0.5615</cell></row><row><cell>DINO-Style ‚Üë</cell><cell>0.2278</cell><cell cols="3">0.2587 0.2890 0.3652</cell><cell>0.4367</cell><cell>0.4395</cell><cell>0.2912</cell><cell>0.4514</cell></row></table><note><p>Style Reference</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>User study statistics of the selection rate for text alignment(Text), and preference rate for style conformity(Style) and temporal quality(Temporal). Top 3 rows: single-reference based guidance. Bottom 2 rows: multi-reference based guidance.</figDesc><table><row><cell>Methods</cell><cell cols="3">Text ‚Üë Style ‚Üë Temporal ‚Üë</cell></row><row><cell>VideoCrafter*</cell><cell>0.391</cell><cell>8.0%</cell><cell>4.4%</cell></row><row><cell>Gen-2*</cell><cell>0.747</cell><cell>23.1%</cell><cell>51.1%</cell></row><row><cell>Ours</cell><cell>0.844</cell><cell>68.9%</cell><cell>44.4%</cell></row><row><cell>AnimateDiff</cell><cell>0.647</cell><cell>10.0%</cell><cell>19.3%</cell></row><row><cell>Ours(M-R)</cell><cell>0.907</cell><cell>90.0%</cell><cell>80.7%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Ablation studies on style modulation designs. The performance is evaluated based on the style-guided T2I generation.</figDesc><table><row><cell>Methods</cell><cell cols="2">CLIP-Text ‚Üë CLIP-Style ‚Üë</cell></row><row><cell>Ours</cell><cell>0.3028</cell><cell>0.4836</cell></row><row><cell>w/o Data Augmentation</cell><cell>0.3173</cell><cell>0.4005</cell></row><row><cell>w/o Dual Cross Attention</cell><cell>0.0983</cell><cell>0.7332</cell></row><row><cell>w/o Adaptive Fusion</cell><cell>0.2807</cell><cell>0.4925</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Ablation study on our two-stage training scheme.</figDesc><table><row><cell>Methods</cell><cell cols="2">CLIP-Text ‚Üë CLIP-Style ‚Üë</cell><cell cols="2">Temporal Consistency CLIP-Temp ‚Üë W.E.(√ó10 -3 ) ‚Üì</cell></row><row><cell>w/o Temporal Adaption</cell><cell>0.2691</cell><cell>0.3923</cell><cell>0.9612</cell><cell>47.88</cell></row><row><cell>Joint Training</cell><cell>0.3138</cell><cell>0.2226</cell><cell>0.9741</cell><cell>24.74</cell></row><row><cell>Two-Stage(ours)</cell><cell>0.2726</cell><cell>0.4531</cell><cell>0.9892</cell><cell>18.73</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>ACM Trans. Graph., Vol. 43, No. 6, Article . Publication date: December 2024.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>ACM Trans. Graph., Vol. 43, No. 6, Article . Publication date: December 2024. StyleCrafter: Taming Stylized Video Diffusion with Reference-Augmented Adapter Learning ‚Ä¢ 5</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>ACM Trans. Graph., Vol. 43, No. 6, Article . Publication date: December 2024. StyleCrafter: Taming Stylized Video Diffusion with Reference-Augmented Adapter Learning ‚Ä¢ 9</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_3"><p>The style references are collected from https://unsplash.com/, https://unsplash.com/, https://en.m.wikipedia.org/wiki/, https://civitai.com/, https://clipdrop.co/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_4"><p>https://huggingface.co/laion/CLIP-ViT-H-14-laion2B-s32B-b79K</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_5"><p>ACM Trans. Graph., Vol. 43, No. 6, Article . Publication date: December 2024. StyleCrafter: Taming Stylized Video Diffusion with Reference-Augmented Adapter Learning ‚Ä¢ 7</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This work was partly supported by the <rs type="funder">National Natural Science Foundation of China</rs> (Grant No. <rs type="grantNumber">61991451</rs>) and the <rs type="funder">Shenzhen Science and Technology Program</rs> (<rs type="grantNumber">JSGG20220831093004008</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_G5GDxkU">
					<idno type="grant-number">61991451</idno>
				</org>
				<org type="funding" xml:id="_ehbbcj7">
					<idno type="grant-number">JSGG20220831093004008</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Extended Ablation Study</head><p>Since we have made ablation studies on some key design in the main text(i.e., Dual Cross-Attention, Data Augmentation, and Adaptive  <ref type="figure">S6</ref> and Table <ref type="table">S8</ref> show that Query Transformer excels over the other alternatives.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">DreamStyler: Paint by Style Inversion with Text-to-Image Diffusion Models</title>
		<author>
			<persName><forename type="first">Junsoo</forename><surname>Namhyuk Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunggi</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunhee</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daesik</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seung-Hun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kibeom</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Artflow: Unbiased image style transfer via reversible neural flows</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yibing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dejing</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="862" to="871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Frozen in time: A joint video and image encoder for end-to-end retrieval</title>
		<author>
			<persName><forename type="first">Max</forename><surname>Bain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G√ºl</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1728" to="1738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Stable video diffusion: Scaling latent video diffusion models to large datasets</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dockhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumith</forename><surname>Kulal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Mendelevitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maciej</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yam</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zion</forename><surname>English</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikram</forename><surname>Voleti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Letts</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.15127</idno>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Video watercolorization using bidirectional texture advection</title>
		<author>
			<persName><forename type="first">Adrien</forename><surname>Bousseau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrice</forename><surname>Neyret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jo√´lle</forename><surname>Thollot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Salesin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">104</biblScope>
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herv√©</forename><surname>J√©gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9650" to="9660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Coherent online video style transfer</title>
		<author>
			<persName><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1105" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Haoxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menghan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingqing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoshu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinbo</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaofang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.19512</idno>
		<title level="m">2023a. VideoCrafter1: Open Diffusion Models for High-Quality Video Generation</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Videocrafter2: Overcoming data limitations for high-quality video diffusion models</title>
		<author>
			<persName><forename type="first">Haoxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menghan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.09047</idno>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">TSSAT: Two-Stage Statistics-Aware Transformation for Artistic Style Transfer</title>
		<author>
			<persName><forename type="first">Haibo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st ACM International Conference on Multimedia</title>
		<meeting>the 31st ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="6878" to="6887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Arbitrary video style transfer via multi-channel correlation</title>
		<author>
			<persName><forename type="first">Yingying</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiming</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongyang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1210" to="1217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Stytr2: Image style transfer with transformers</title>
		<author>
			<persName><forename type="first">Yingying</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiming</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongyang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingjia</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="11326" to="11336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cogview2: Faster and better text-to-image generation via hierarchical transformers</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wendi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyi</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="16890" to="16902" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">An image is worth one word: Personalizing text-toimage generation using textual inversion</title>
		<author>
			<persName><forename type="first">Rinon</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Alaluf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Atzmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Or</forename><surname>Patashnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gal</forename><surname>Bermano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName><surname>Cohen-Or</surname></persName>
		</author>
		<idno>arXiv preprint:2208.01618</idno>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast video multi-style transfer</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yihang</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF winter conference on applications of computer vision</title>
		<meeting>the IEEE/CVF winter conference on applications of computer vision</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3222" to="3230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName><forename type="first">Leon</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2414" to="2423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Tokenflow: Consistent diffusion features for consistent video editing</title>
		<author>
			<persName><forename type="first">Michal</forename><surname>Geyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Bar-Tal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shai</forename><surname>Bagon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tali</forename><surname>Dekel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Animatediff: Animate your personalized text-to-image diffusion models without specific tuning</title>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anyi</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaohui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Animate-a-story: Storytelling with retrieval-augmented video generation</title>
		<author>
			<persName><forename type="first">Yingqing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menghan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinbo</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.06940</idno>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Latent video diffusion models for high-fidelity video generation with arbitrary lengths</title>
		<author>
			<persName><forename type="first">Yingqing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<idno>arXiv preprint:2211.13221</idno>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Style aligned image generation via shared attention</title>
		<author>
			<persName><forename type="first">Amir</forename><surname>Hertz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Voynov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shlomi</forename><surname>Fruchter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.02133</idno>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Image analogies</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">E</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nuria</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">H</forename><surname>Salesin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Annual Conference on Computer Graphics and Interactive Techniques</title>
		<meeting>the 28th Annual Conference on Computer Graphics and Interactive Techniques</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="327" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">2022a. Imagen video: High definition video generation with diffusion models</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<idno>arXiv preprint:2210.02303</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Alexey Gritsenko</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Gritsenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.03458</idno>
		<title level="m">2022b. Video diffusion models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Cogvideo: Large-scale pretraining for text-to-video generation via transformers</title>
		<author>
			<persName><forename type="first">Wenyi</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wendi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinghan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno>arXiv preprint:2205.15868</idno>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">LoRA: Low-Rank Adaptation of Large Language Models</title>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shean</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Lianghua</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<title level="m">Composer: Creative and controllable image synthesis with composable conditions</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
			<biblScope unit="page">9778</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Nisha</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiming</forename><surname>Dong</surname></persName>
		</author>
		<idno>arXiv preprint:2305.05464</idno>
		<title level="m">Style-A-Video: Agile Diffusion for Arbitrary Text-based Video Style Transfer</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1501" to="1510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Manifold alignment for semantically aligned style transfer</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Kun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinghuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="14861" to="14869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Stylizing video by example</title>
		<author>
			<persName><forename type="first">Ond≈ôej</forename><surname>Jamri≈°ka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">≈†√°rka</forename><surname>Sochorov√°</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ond≈ôej</forename><surname>Texler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Luk√°ƒç</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Fi≈°er</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingwan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel S'</forename><surname>Ykora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Text2videozero: Text-to-image diffusion models are zero-shot video generators</title>
		<author>
			<persName><forename type="first">Levon</forename><surname>Khachatryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andranik</forename><surname>Movsisyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vahram</forename><surname>Tadevosyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shant</forename><surname>Navasardyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Humphrey</forename><surname>Shi</surname></persName>
		</author>
		<idno>arXiv preprint:2303.13439</idno>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multi-concept customization of text-to-image diffusion</title>
		<author>
			<persName><forename type="first">Nupur</forename><surname>Kumari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1931" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">2023a. BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</title>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Universal style transfer via feature transforms</title>
		<author>
			<persName><forename type="first">Yijun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Gligen: Open-set grounded text-to-image generation</title>
		<author>
			<persName><forename type="first">Yuheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangzhou</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="22511" to="22521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Adaattn: Revisit attention mechanism in arbitrary neural style transfer</title>
		<author>
			<persName><forename type="first">Songhua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongliang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meiling</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6649" to="6658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Yaofang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuebo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tieyong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.11440</idno>
		<title level="m">Evalcrafter: Benchmarking and evaluating large video generation models</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Text-guided synthesis of eulerian cinemagraphs</title>
		<author>
			<persName><forename type="first">Aniruddha</forename><surname>Mahapatra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aliaksandr</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsin-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Improved denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nichol</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8162" to="8171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">GPT-4V(ision) System Card</title>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer with style-attentional networks</title>
		<author>
			<persName><forename type="first">Young</forename><surname>Dae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwang</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename><surname>Hee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5880" to="5888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Wiki Art Gallery, Inc.: A case for critical thinking</title>
		<author>
			<persName><forename type="first">Fred</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandy</forename><surname>Mackintosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Issues in Accounting Education</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="593" to="608" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">2023a. Sdxl: Improving latent diffusion models for high-resolution image synthesis</title>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Podell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zion</forename><surname>English</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lacey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dockhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Penna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.01952</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Joe Penna, and Robin Rombach. 2023b. Sdxl: Improving latent diffusion models for high-resolution image synthesis</title>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Podell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zion</forename><surname>English</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lacey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dockhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>M√ºller</surname></persName>
		</author>
		<idno>arXiv preprint:2307.01952</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Fatezero: Fusing attentions for zero-shot text-based video editing</title>
		<author>
			<persName><forename type="first">Chenyang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyang</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Hierarchical text-conditional image generation with clip latents</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Casey</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.06125</idno>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bj√∂rn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Artistic style transfer for videos</title>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition: 38th German Conference</title>
		<meeting><address><addrLine>Hannover, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016-09-12">2016. 2016. September 12-15, 2016</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="26" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Dreambooth: Fine tuning text-to-image diffusion models for subjectdriven generation</title>
		<author>
			<persName><forename type="first">Nataniel</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yael</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kfir</forename><surname>Aberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="22500" to="22510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Laion-5b: An open large-scale dataset for training next generation image-text models</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Schuhmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romain</forename><surname>Beaumont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Vencu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cade</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Cherti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theo</forename><surname>Coombes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aarush</forename><surname>Katta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clayton</forename><surname>Mullis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="25278" to="25294" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Instantbooth: Personalized textto-image generation without test-time finetuning</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyun</forename><surname>Joon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2304.03411</idno>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Make-a-video: Text-to-video generation without text-video data</title>
		<author>
			<persName><forename type="first">Uriel</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiyuan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harry</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oron</forename><surname>Ashual</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oran</forename><surname>Gafni</surname></persName>
		</author>
		<idno>arXiv preprint:2209.14792</idno>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">StyleDrop: Textto-Image Generation in Any Style</title>
		<author>
			<persName><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nataniel</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kimin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Castro Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irina</forename><surname>Blok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huiwen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jarred</forename><surname>Barber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Glenn</forename><surname>Entis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhen</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">2021a. Denoising Diffusion Implicit Models</title>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">2021b. Score-Based Generative Modeling through Stochastic Differential Equations</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Raft: Recurrent all-pairs field transforms for optical flow</title>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Teed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020-08-23">2020. August 23-28, 2020</date>
			<biblScope unit="page" from="402" to="419" />
		</imprint>
	</monogr>
	<note>Proceedings, Part II 16</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Separating style and content with bilinear models</title>
		<author>
			<persName><forename type="first">B</forename><surname>Joshua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">T</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1247" to="1283" />
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">2020a. Arbitrary style transfer using neurally-guided patch-based synthesis</title>
		<author>
			<persName><forename type="first">Ond≈ôej</forename><surname>Texler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Futschik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Fi≈°er</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Luk√°ƒç</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingwan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>S·ª≥kora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="62" to="71" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Interactive video stylization using few-shot patch-based training</title>
		<author>
			<persName><forename type="first">Ond≈ôej</forename><surname>Texler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Futschik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Kuƒçera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ond≈ôej</forename><surname>Jamri≈°ka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">≈†√°rka</forename><surname>Sochorov√°</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menclei</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>S·ª≥kora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="73" to="74" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">≈Åukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Efficient examplebased painting and synthesis of 2d directional texture</title>
		<author>
			<persName><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaiping</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaguang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="266" to="277" />
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<author>
			<persName><forename type="first">Jiuniu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hangjie</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dayou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingya</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.06571</idno>
		<title level="m">Xiang Wang, and Shiwei Zhang. 2023c. Modelscope text-to-video technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Videocomposer: Compositional video synthesis with motion controllability</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hangjie</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dayou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiuniu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">2023a. LAVIE: High-Quality Video Generation with Cascaded Latent Diffusion Models</title>
		<author>
			<persName><forename type="first">Yaohui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shangchen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashuo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peiqing</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.15103</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<author>
			<persName><forename type="first">Zhouxia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangbin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<idno>arXiv preprint:2309.01770</idno>
		<title level="m">StyleAdapter: A Single-Pass LoRA-Free Model for Stylized Image Generation</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Elite: Encoding visual concepts into textual embeddings for customized textto-image generation</title>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yabo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="15943" to="15953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Tune-a-video: Oneshot tuning of image diffusion models for text-to-video generation</title>
		<author>
			<persName><forename type="first">Jay Zhangjie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><forename type="middle">Weixian</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wynne</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohu</forename><surname>Qie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><forename type="middle">Zheng</forename><surname>Shou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="7623" to="7633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<author>
			<persName><forename type="first">Jinbo</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menghan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tien-Tsin</forename><surname>Wong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.17933</idno>
		<title level="m">ToonCrafter: Generative Cartoon Interpolation</title>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<author>
			<persName><forename type="first">Jinbo</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menghan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tien-Tsin</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.12190</idno>
		<title level="m">Dynamicrafter: Animating open-domain images with video diffusion priors</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Rerender A Video: Zero-Shot Text-Guided Video-to-Video Translation</title>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH Asia 2023 Conference Proceedings</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">FRESCO: Spatial-Temporal Correspondence for Zero-Shot Video Translation</title>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="8703" to="8712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<author>
			<persName><forename type="first">Jun</forename><surname>Hu Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sibo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.06721</idno>
		<title level="m">IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Style transfer via image component analysis</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianzhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on multimedia</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1594" to="1601" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Inversion-based style transfer with diffusion models</title>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nisha</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongyang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiming</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="10146" to="10156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Domain enhanced arbitrary image style transfer via contrastive learning</title>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiming</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongyang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong-Yee</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2022 Conference Proceedings</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Magicvideo: Efficient video generation with latent diffusion models</title>
		<author>
			<persName><forename type="first">Daquan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weimin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanshu</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhe</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<idno>arXiv preprint:2211.11018</idno>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
