# Mean Field Residual Networks: On the Edge of Chaos

## Abstract

## 

We study randomly initialized residual networks using mean field theory and the theory of difference equations. Classical feedforward neural networks, such as those with tanh activations, exhibit exponential behavior on the average when propagating inputs forward or gradients backward. The exponential forward dynamics causes rapid collapsing of the input space geometry, while the exponential backward dynamics causes drastic vanishing or exploding gradients. We show, in contrast, that by adding skip connections, the network will, depending on the nonlinearity, adopt subexponential forward and backward dynamics, and in many cases in fact polynomial. The exponents of these polynomials are obtained through analytic methods and proved and verified empirically to be correct. In terms of the "edge of chaos" hypothesis, these subexponential and polynomial laws allow residual networks to "hover over the boundary between stability and chaos," thus preserving the geometry of the input space and the gradient information flow. In our experiments, for each activation function we study here, we initialize residual networks with different hyperparameters and train them on MNIST. Remarkably, our initialization time theory can accurately predict test time performance of these networks, by tracking either the expected amount of gradient explosion or the expected squared distance between the images of two input vectors. Importantly, we show, theoretically as well as empirically, that common initializations such as the Xavier or the He schemes are not optimal for residual networks, because the optimal initialization variances depend on the depth. Finally, we have made mathematical contributions by deriving several new identities for the kernels of powers of ReLU functions by relating them to the zeroth Bessel function of the second kind.

## Introduction

Previous works [[9,](#b8)[3,](#b2)[11]](#b10) have shown that randomly initialized neural networks exhibit a spectrum of behavior with depth, from stable to chaotic, which depends on the variance of the initializations: the cosine distance of two input vectors converges exponentially fast with depth to a fixed point in [0, 1]; if this fixed point is 1, then the behavior is stable; if this fixed point is 0, then the behavior is chaotic. It has been argued in many prior works [[1,](#b0)[9]](#b8) that effective computation can only be supported by a dynamical behavior that is on the edge of chaos. Too much stability prevents the neural network from telling apart two different inputs. While some chaotic behavior can increase the expressivity of a network, too much chaos makes the neural network think two similar inputs are very different. At the same time, the same initialization variances also control how far gradient information can be propagated through the network; the networks with chaotic forward dynamics will tend to suffer from exploding gradients, while networks with stable forward dynamics will tend to suffer from vanishing gradients.

These works have focused on vanilla (fully connected) feedforward networks. Here we consider residual networks [[6,](#b5)[7]](#b6) (with fully-connected layers and without batchnorm), which are a family of recently proposed neural network architectures that has achieved state-of-the-art performance on image recognition tasks, beating all other approaches by a large margin. The main innovation of this family of architectures is the addition of a passthrough (identity) connection from the previous layer to the next, such that the usual nonlinearity computes the "residual" between the next-layer activation and the previous-layer activation.

In this work, we seek to characterize randomly initialized residual networks. One of our main results is that random residual networks for many nonlinearities such as tanh live on the edge of chaos, in that the cosine distance of two input vectors will converge to a fixed point at a polynomial rate, rather than an exponential rate, as with vanilla tanh networks. Thus a typical residual network will slowly cross the stable-chaotic boundary with depth, hovering around this boundary for many layers. In addition, for most of the nonlinearities considered here, the mean field estimate of the gradient grows subexponentially with depth. In fact, for α-ReLU, the αth-power of ReLU, for α < 1, the gradient grows only polynomially. These theoretical results provide some theoretical justification for why residual networks work so well in practice. In our experiments, we are also able to predict surprisingly well the relative performances of trained residual networks based only on their initialization hyperparameters, in a variety of settings. In particular, we find that the quality of initialization for tanh resnets is determined by trainability (how much gradient explosion on average) while that for (α-)ReLU resnets is determined by expressivity (how far can two different input vectors be pulled apart) (see [Section 6)](#). To the best of our knowledge, this is the first time that a quantity other than gradient explosion/vanishing has been found to control the quality of initialization. We establish theoretically and empirically that the best initialization variances for residual networks depend on the depth of the network (contrary to the feedforward case [[11]](#b10)), so that common initialization schemes like Xavier [[4]](#b3) or He [[5]](#b4) cannot be optimal. In fact, even the rationale of He initialization is incorrect for ReLU residual networks because it tries to control gradient dynamics rather than expressivity. However we want to emphasize that we study a simplified model of residual networks in this work, with no batchnorm or convolutional layers, so that these results are not necessarily indicative of the MSRA residual network used in practice [[6]](#b5).

In the body of this paper, we give account of general intuition and/or proof strategy when appropriate for our theoretical results, but we relegate all formal statements and proofs to the appendix.

## Background

Consider a vanilla feedforward neural network of L layers, with each layer l having N (l) neurons; here layer 0 is the input layer. For the ease of presentation we assume all hidden layer widths are the same N (l) = N for all l > 0. Let x (0) = (x (0) 1 , . . . , x (0) N (0) ) be the input vector to the network, and let x (l) for l > 0 be the activation of layer l. Then a neural network is given by the equations

$x (l) i = φ(h (l) i ), h (l) i = N j=1 w (l) ij x (l-1) j + b (l) i$where (i) h (l) is the pre-activation at layer l, (ii) w (l) is the weight matrix, (iii) b (l) is the bias vector, and (iv) φ is a nonlinearity, for example tanh or ReLU, which is applied coordinatewise to its input.

To lighten up notation, we suppress the explicit layer numbers l and write

$x i = φ(h i ), h i = j w ij x j + b i$where • implicitly denotes • (l) , and • denotes • (l-1) (and analogously, • denotes • (l+1) ).

A series of papers [[9,](#b8)[10,](#b9)[11](#b10)] investigated the "average behavior" of random neural networks sampled via w

$(l) ij ∼ N (0, σ 2 w /N ), b(l)$i ∼ N (0, σ 2 b ), for fixed parameters σ w and σ b , independent of l. Consider the expectation of 1 N N i=1 x 2 i , the normalized squared length of x, over the sampling of w and b. Poole et al. [[9]](#b8) showed that this quantity converges to a fixed point exponentially fast for sigmoid nonlinearities. Now suppose we propagate two different vectors x (0) and (x (0) ) through the network. Poole et al. [[9]](#b8) also showed that the expectation of the normalized dot product 1 N N i=1 x i x i converges exponentially fast to a fixed point. The ratio between the normalized squared length and the normalized dot product is the cosine distance between x and x . Thus these two exponential convergence results show that the cosine distance converges exponentially fast to a fixed point as well. Intuitively, this means that a vanilla feedforward network "forgets" the geometry of the input space "very quickly," after only a few layers.

In addition, Schoenholz et al. [[11]](#b10), under certain independence assumptions, showed that the expected normalized squared norm of the gradient also vanishes or explodes in an exponential fashion with depth, with the "half-life" controlled by σ w and σ b . They verified that this theoretical "half-life" correlates in practice with the maximal number of layers that are admissible to good performance.

At the same time, Daniely et al. [[3]](#b2) published work of similar nature, but phrased in the language of reproducing kernel Hilbert spaces, and provided high probability estimates that are meaningful for the case when the width N is finite and the depth is logarithmic in N . However, they essentially fixed the variance parameters σ • , and furthermore, their framework (for example the notion of a "skeleton") does not immediately generalize to the residual network case.

In this work, we show that residual networks have very different dynamics from vanilla feedforward networks. In most cases, the cosine distance convergence rate and the gradient growth rate are subexponential in a residual network, and in most cases, these rates may be polynomial.

## Preliminaries

Residual networks were first introduced by [[6]](#b5) and later refined by [[7]](#b6), and they are now commonplace among deployed neural systems. The key innovation there is the addition of a shortcut connection from the previous layer to the next. We define the following idealized architectures for ease of analysis. Note that we only consider fully-connected affine layers instead of convolutional layers. A reduced residual network (RRN) has the recurrence

$x i = φ(h i ) + x, h i = j w ij x j + b i .$A (full) residual network (FRN) in addition has an affine connection given by weights v and biases a from the nonlinearity φ(h) to the next layer:

$x i = j v ij φ(h j ) + x i + a i , h i = j w ij x j + b i$We are interested in the "average behavior" of these network when the weights and biases, w

$(l) ij , b(l) i , v (l) ij , and a (l)$i are sampled i.i.d. from Gaussian distributions resp. with standard deviations σ w , σ b , σ v , and σ a , independent from l. Here we take the variance of w (l) ij to be σ 2 w /N so that the variance of each h i is σ 2 w , assuming each x j is fixed (similarity for v (l) ij ). Such an initialization scheme is standard in practice.

We make several key "physical assumptions" to make theoretical computations tractable: Axiom 3.1 (Symmetry of activations and gradients). (a) We assume (h 2 for any i, j, l. (b) We also assume that the gradient ∂E/∂x (l) i with respect to the loss function E satisfies (∂E/∂x (l) i ) 2 = (∂E/∂x (l) j ) 2 for any i, j, l.

$(l) i ) 2 = (h (l) j ) 2 and (x (0) i ) 2 = (x (0) j )$One can see that Axiom 3.1(a) is satisfied if the input x (0) ∈ {±1} N and Axiom 3.1(b) is satisfied if Axiom 3.2 below is true and the gradient at the last layer ∂E/∂xL ∈ {±1} N . But in general it is justified both empirically and theoretically as an approximation, because (h 2 grow rather quickly at the same pace with l (as will be seen later in calculations), so that their additive difference becomes negligible; similarly for (x

$(l) i ) 2 -(h (l) j ) 2 stays about constant with l, but (h (l) i ) 2 and (h (l) j )$$(l) i ) 2 and (∂E/∂h (l) i ) 2 .$
## Axiom 3.2 (Gradient independence). (a)

We assume the we use a different set of weights for backpropagation than those used to compute the network outputs, but sampled i.i.d. from the same distributions. (b) For any loss function E, we assume that the gradient at layer l, ∂E/∂x (l) i , is independent from all activations h (l) j and x (l-1) j from the previous layer. Axiom 3.2(a) was first made in [[11]](#b10) for computing the mean field theory of gradients for feedforward tanh networks. This is similar to the practice of feedback alignment [[8]](#b7). Even though we are the first to explicitly formulate Axiom 3.2(b), in fact it was already applied implicitly in the gradient calculations of [[11]](#b10). Note that a priori Axiom 3.2(b) is not true, as ∂E/∂x (l) i depends on φ(h (l+1) k ) for every k, which depend on h (l) j for each j, and which depends on x (l-1) k for every k. Nevertheless, in practice both subassumptions hold very well. Now we define the central quantities studied in this paper. Inevitably, our paper involves a large amount of notation that may be confusing for the first-time reader. We have included a glossary of symbols (Table [A](#).1) to ameliorate notation confusion. Definition 3.3. Fix an input x (0) . Define the length quantities q (l) := (h (l) 1 ) 2 and p (l) := (x (l) 1 ) 2 for l > 0 and p (0) = x (0) 2 /N . Here the expectations • are taken over all random initialization of weights and biases for all layers l, as N → ∞ (large width limit).

Note that in our definition, the index 1 does not matter by Axiom 3.1. Definition 3.4. Fix two inputs x (0) and x (0) . We write • to denote a quantity • with respect to the input x (0) . Then define the correlation quantities γ (l) := h

$(l) 1 h (l) 1 and λ (l) := x (l) 1 x (l) 1$for l > 0 and γ (0) = x (0) • x (0) /N , where the expectations • are taken over all random initialization of weights and biases for all layers l, as N → ∞ (large width limit). Again, here the index 1 does not matter by Axiom 3.1. By metric expressivity, we mean s (l) := l) . Additionally, define the cosine distance quantities e (l) := γ (l) / p (l) p (l) and c (l) := λ (l) / q (l) q (l) , and we will also call e (l) angular expressivity.

$1 2N x (l) -x (l) 2 = 1 2N ( x (l) 2 + x (l) 2 -2 x (l) • x (l) ) = 1 2 (p (l) + p (l) ) -γ ($In this paper, for the ease of presentation, we assume p (0) = p (0) . Then, as we will see, p (l) = p (l) , q (l) = q (l) for all l, and as a result, e (l) = γ (l) /p (l) and s (l) = p (l) -γ (l) = (1 -e (l) )p (l) . Definition 3.5. Fix an input x (0) and a gradient vector (∂E/∂x (L) i ) i of some loss function E with respect to the last layer x (L) . Then define the gradient quantities χ (l) := (∂E/∂x

$(l) 1 ) 2 , χ (l) • := (∂E/∂• (l) 1 ) 2 for • = a, b, and χ (l) • := (∂E/∂• (l) 11 ) 2 for • = w, v.$Here the expectations are taken with Axiom 3.2 in mind, over both random initialization of forward and backward weights and biases, as N → ∞ (large width limit). Again, the index 1 or 11 does not matter by Axiom 3.1.

## Asymptotic notations.

The expressions f = O(g) ⇐⇒ g = Ω(f ) have their typical meanings, and

$f = Θ(g) iff f = O(g), g = O(f ). We take f (x) = Õ(g(x)) ⇐⇒ g(x) = Ω(f (x)) to mean f (x) = O(g log k x) for some k ∈ Z (this is slightly different from the standard usage of Õ), and f = Θ(g) ⇐⇒ f = Õ(g) & g = Õ(f ). We introduce a new notation: f = Θ(g) if f (x) = O(g(x) • x ) and f (x) = Ω(g(x) • x -), as x → ∞,$for any > 0. All asymptotic notations are sign-less, i.e. can indicate either positive or negative quantities, unless stated otherwise.

## Overview

The primary reason we may say anything about the average behavior of any of the above quantities is the central limit theorem: every time the activations of the previous layer pass through an affine layer whose weights are sampled i.i.d., the output is a sum of a large number of random variables, and thus follows approximately Gaussian distributions. The mean and variance of these distributions can be computed by keeping track of the mean and variances of the activations in the previous layer.

In what follows, we use this technique to derive recurrence equations governing p, q, γ, λ, χ for different architectures and different activation functions. We use these equations to investigate the dynamics of e and s, the key quantities in the forward pass, and the dynamics of χ, the key quantity in the backward pass.

The cosine distance e in some sense measures the angular geometry of two vectors. If e = 1, then the vectors are parallel; if e = 0, then they are orthogonal. Just as in [[9]](#b8) and [[11]](#b10), we will show that in all of the architectures and activations we consider in this paper, e (l) converges to a fixed point e * as l → ∞ 1 . Thus, on the average, as vectors propagate through network, the geometry of the original input space, for example, linear separability, is "forgotten" by residual networks as well as by vanilla networks. But we will prove and verify experimentally that, while Poole et al. [[9]](#b8) and [[11]](#b10) showed that the convergence rate to e * is exponential in a vanilla network, the convergence rate is rather only polynomial in residual networks, for tanh and α-ReLU (Defn 5.2) nonlinearities; see Thm B.5, Thm B.11, Thm B.17, and Thm B.18. This slow convergence preserves geometric information in the input space, and allows a typical residual network to "hover over the edge of chaos": Even when the cosine distance e (l) converges to 0, corresponding to "chaos", (resp. 1, corresponding to "stability"), for the number of layers usually seen in practice, e (l) will reside well away from 0 (resp. 1).

Similarly, the quantity s measures the metric geometry of two vectors. The evolution of s (l) with l tells us the ability of the average network to separate two input points in terms of Euclidean distance. Again, for tanh and α-ReLU (α < 1) nonlinearities, s varies only polynomially with l.

On the other hand, χ (l) measures the size of gradient at layer l, and through it we track the dynamics of gradient backpropagation, be it explosion or vanishing. In contrast to vanilla tanh networks, which can experience both of these two phenomenon depending on the initialization variances, typical residual networks cannot have vanishing gradient, in the sense of vanishing χ (l) as l → 1; see Thm B.5 and Thm B.12. Furthermore, while vanilla tanh networks exhibit exponentially vanishing or exploding gradients, all of the activation/architecture pairings considered here, except the full residual network with ReLU, have subexponential gradient dynamics. While tanh residual networks (reduced or full) has χ (0) ≈ exp(Θ( √ l))χ (l) (Thm B.13), α-ReLU residual networks for α < 1 have χ (0) ≈ poly(l)χ (l) (Thm B.20). Instead of ∂E/∂x i , we may also consider the size of gradients of actual trainable parameters. For tanh and α-ReLU with α < 1, they are still subexponential and polynomial (Thm B.21). On the other hand, while χ (0) = exp(Θ(l))χ (l) for a ReLU resnet, its weight gradients have size independent of layer, within O(1) (Thm B.21)! This is the only instance in this paper of gradient norm being completely preserved across layers.

The above overviews the theoretical portion of this paper. Through experiments, we discover that we can very accurately predict whether one random initialization leads to better performance than another on the test set, after training, by leveraging this theory we build. Residual networks of different nonlinearities have different controlling quantities: for resnets with tanh, the optimal initialization is obtained by controlling the gradient explosion χ (0) /χ (L) ; whereas for ReLU and α-ReLU, the optimal initialization is obtained by maximizing s without running into numerical issues (with floating point computation). See Section 6 for details.

Over the course of our investigation of α-ReLU, we derived several new identities involving the associated kernel functions, first defined in [[2]](#b1), which relate them to the zeroth Bessel functions (Lemmas C.31 to C.34).

## Theoretical Results

In what follows in the main text, we assume σ • > 0 for all • = w, v, b, a; in the appendix, the formal statement of each main theorem will contain results for other cases. We are interested in the two major categories of nonlinearities used today: tanh-like and rectified units. We make the following formal definitions as a foundation for further consideration. Definition 5.1. We say a function φ is tanh-like if φ is antisymmetric (φ(-x) = -φ(x)), |φ(x)| ≤ 1 for all x, φ(x) ≥ 0, ∀x ≥ 0, and φ(x) monotonically increases to 1 as x → ∞. Definition 5.2. Define the α-ReLU ψ α (x) = x α if x > 0 and 0 otherwise. 2   By applying the central limit theorem as described in the last section, we derive a set of recurrences for different activation/architecture pairs, shown in Table [1](#tab_1) (see appendix for proofs). They leverage certain integral transforms 3 as in the following  

$= σ 2 w p + σ 2 b p = Vφ(q) + p λ = σ 2 w γ + σ 2 b γ = Wφ(q, λ) + γ χ = (σ 2 w V φ(q) + 1)χ q = σ 2 w p + σ 2 b p = σ 2 v Vφ(q) + σ 2 a + p λ = σ 2 w γ + σ 2 b γ = σ 2 v Wφ(q, λ) + σ 2 a + γ χ = (σ 2 v σ 2 w V φ(q) + 1)$$χ (l) exp(Θ( √ l)), B.6 exp(Θ( √ l)), B.12 exp(Θ(l)), B.20 Θ(l α2$(1-α)(2α-1) ), B.20 Definition 5.3. Define the transforms V and W by Vφ(q) := E[φ(z) 2 : z ∼ N (0, q)] and

$Wφ(ρ, ν) := E[φ(z)φ(z ) : (z, z ) ∼ N (0, ρ ν ν ρ )].$These recurrences are able to track the corresponding quantities in practice very well. For example, Fig. [1](#fig_1) compares theory vs experiments for the tanh/FRN pair. The agreement is very good for tanh/RRN (not shown, but similar to the case of tanh/FRN with σ v = 1 and σ a = 0) and α-ReLU/FRN as well (see Fig. As mentioned in previous sections, we seek to characterize the long term/high depth behavior of all of the quantities defined in Section 2. To do so, we solve for the asymptotics of the recurrences in Table [1](#tab_1), where φ is instantiated with tanh or α-ReLU. Our main dynamics results are summarized in Table [2](#tab_2).

## Tanh

Forward dynamics. When φ = tanh, p (l) and q (l) increase as Θ(l) in either RRN or FRN (Thm B.2), as one might expect by observing that V tanh(q) → 1 as q → ∞ so that, for example in the RRN case, the recurrence p = V tanh(q) + p becomes p = 1 + p. This is confirmed graphically by the black lines of the leftmost chart of Fig. [1](#fig_1). We carefully verify that this intuition is correct in its proof in the appendix, and find that in fact p (l) ∼ l in the RRN case and p (l) ∼ (σ 2 v + σ 2 a )l in the FRN case.

What about γ (l) ? The middle chart of Fig. [1](#fig_1) shows that over time, e (l) = γ (l) /p (l) contracts toward the center of the interval [0, 1], but from the looks of it, it is not clear whether there is a stable fixed point e * of e or not. We prove that, in fact, all trajectories of e not starting at 1 do converge to a single fixed point, but only at a polynomial rate, in both the RRN and FRN cases (Thm B.2 and Thm B.10); we can even explicitly compute the fixed point and the rate of convergence: For FRN, there is a unique stable fixed point e * < 1 determined by the equation

$e * = 1 σ 2 v + σ 2 a [σ 2 v 2 π arcsin (e * ) + σ 2 a ],$and |e * -e (l) | decreases like l -δ * , where   In log-log scale: the dashed line is l -δ * -1 , and the colored lines are e (l) -e (l-1) for different initial conditions e (0) . That they become parallel at about l = 400 on verifies that e (l) = Θ(l -δ * ). 4 (c) In log-log scale: The dashed line is A √ l (A given in Thm B.13), and the colored lines are log(• (1) /• (l) ) for • = χ, χ b , χ w . That they all converge together starting around l = 1000 indicates that the approximation in Thm B.13 is very good for large l.

$δ * := 1 - 2 π 1 1 -(e * ) 2 σ 2 v σ 2 v + σ 2 a .$Since e * < 1, s = (1 -e)p = Θ(p) = Θ(l). The case of RRN can be viewed as a special case of the above, setting σ 2 v = 1 and σ 2 a = 0, which yields e * = 0 and δ * = 1 -2 π . We observe that both e * and δ * only depend on the ratio ρ := σ a /σ v , so in Fig. [2](#fig_3) we graph these two quantities as a function of ρ. e * and δ * both increase with ρ and asymptotically approach 1 and 1 /2 respectively from below. When ρ = σ a = 0, e * = 0 and δ * = 1 -2 π . Thus the rate of convergence at its slowest for tanh/FRN is δ * = 1 -2 π ≈ 0.36338, where asymptotically the network tends toward a chaotic regime e * = 0, corresponding to a large weight variance and a small bias variance; it at its fastest is δ * = 1 /2, where asymptotically the network tends toward a stable regime e * = 1, corresponding to a large bias variance and small weight variance. We verify δ * by comparing e (l) -e (l-1) to l -δ * -1 in log-log scale. If e (l) = Θ(l -δ * ), then e (l) -e (l-1) = Θ(l -δ * -1 ) and should obtain the same slope as l -δ * -1 as l → ∞. The middle figure of Fig. [2](#fig_3) ascertains that this is indeed the case, starting around layer number 400.

Backward dynamics. Finally, we show that the gradient is approximated by

$χ (m) = exp(A( √ l - √ m) + O(log l -log m))χ (l) ( )$where A = 4 ). The rightmost plot of Fig. [2](#fig_3) verifies that indeed, for large l ≥ 1000, this is a very good approximation. This demonstrates that the mean field assumption of independent backpropagation weights is very practical and convenient even for residual networks.

Note that in the FRN case, the constant A can be decomposed into

$A = 4 3 2 π • σ v • σ w • (1 + σ 2 a /σ 2 v ) -1/2$. Consider the ratio ρ := σ a /σ v . If ρ 1, then e * ≈ 1 (Fig. [C](#).17), meaning that the typical network essentially computes a constant function, and thus unexpressive; at the same time, large ρ makes A small, and thus ameliorating the gradient explosion problem, making the network more trainable. On the other hand, if ρ 1, then e * ≈ 0 (Fig. [C](#).17), the typical network can tease out the finest differences between any two input vectors, and a final linear layer on top of such a network should be able to express a wide variety of functions [[9]](#b8); at the same time, small ρ increases A, worsening the gradient explosion problem, making the network less trainable. This is the same expressivity-trainability tradeoff discussed in [[11]](#b10).

## α-ReLU

Forward dynamics. As with the tanh case, to deduce the asymptotic behavior of random α-ReLU resnets, we need to understand the transforms Vψ α and Wψ α . Fortunately, Vψ α has a closed form, and Wψ α has been studied before [[2]](#b1). In particular, if α > - 1  2 , then Vψ α (q) = c α q α , where c α is a constant with a closed form given by Lemma B.15. In addition, by [[2]](#b1), we know that Wψ α (q, cq) = Vψ α (q)J α (c) for J α given in Appendix C.7.1. Fig. [C](#).17 shows a comparison of J α for different αs along with the identity function.

Substituting in c α q α for Vψ α , we get a difference equation p

$-p = σ 2 v c α (σ 2 w p + σ 2 b ) α + σ 2 a$governing the evolution of p. This should be reminiscent of the differential equation Ṗ (l) = CP (l) α , which has solution ∝ l 1/(1-α) for α < 1, and ∝ exp(Cl) when α = 1. And indeed, the solutions p (l) to these difference equations behave asymptotically exactly like so (Thm B.16). Thus ReLU behaves very explosively compared to α-ReLU with α < 1. In fact, in simulations, for σ 2 w = 1.69 and σ 2 v = 1.5, the ReLU resnets overflows into infs after around 100 layers, while there's no problem from any other kind of networks we consider.

Regardless, α-ReLU for all α massages e (l) toward a fixed point e * that depends on α. When φ = ψ 1 , the standard ReLU, e (l) converges to 1 asymptotically as Cl -2 for an explicit constant C depending on σ v and σ w only (Thm B.17), so that s = (1 -e)p = Θ(l -2 exp(Θ(l))) = exp(Θ(l)). When φ = ψ α for α < 1, then e (l) converges to the nonunit fixed point e * of J α at a rate of Θ(l -µ ), where µ = (1 -Jα (e * ))/(1 -α) is independent of the variances (Thm B.18), so that s = Θ(p). Backward dynamics. Finally, we have also characterized the rate of gradient growth for any α ∈ ( 3 4 , 1]. 5 In the case of α = 1, the dynamics of χ is exponential, the same as that of p,

## These rates are verified in

$χ (l-m) = χ (l) B m where B = 1 2 σ 2 v σ 2 w + 1. For α ∈ ( 3 4 , 1)$, the dynamics is polynomial, but with different exponent in general from that of the forward pass:

$χ (l-m) = Θ(1)χ (l) (l/(l -m)) R for R = α 2 (1-α)(2α-1)$, where the constants in Θ(1) do not depend on l or m. Looking only at χ and the gradients against the biases, it seems that ReLU suffers from a dramatic case of exploding gradients. But in fact, because χ gains a factor of B moving backwards while p loses a factor of B, the gradient norm χ (l-m) w (and similarly for χ

$This exponent R is minimized on α ∈ [ 3 4 , 1) at α = 3 /4, where R = 9 /2 (but on α ∈ ( 1 2 , 1) it is minimized at α = 2 /$$(l-m) v$) is independent of how far, m, the gradient has been propagated (Thm B.21) -this is certainly the best gradient preservation among all of the models considered in this paper. Thus strangely, random ReLU FRN exhibits both the best (constant for v and w) and the worse (exponential for a and b) gradient dynamics. This begs the question, then, is this a better deal than other α-ReLU for which for any learnable parameter we have at most a polynomial blowup with depth in its gradient? Our experiments (discussed below) show that α-ReLU is useful to the extent that smaller α avoids numerical issues with exponentiating forward and backward dynamics, but the best performance is given by the largest α that avoids them (Fig. [3(c,](#fig_44)[d](#))); in fact, the metric expressivity s, determines performance, not gradient explosion (see α-ReLU experiments).  

$(L) /s (0) ≈ log p (L) /p (0) ≈ log χ (0) /χ (L) = L log(1 + σ 2 v σ 2 w /2). (e)$Red heatmap shows the test accuracies of a grid of α-ReLU FRN with varying α and L as shown, but with all σ•s fixed. The white dashed curve gives a typical contour line of L R = const, where R = α 2

(1-α)(2α-1) . The yellow-to-blue curves form a set of level curves for s (l) = p (l) -γ (l) = const, with yellow curves corresponding to higher levels.

## Experimental Results

Our experiments show a dichotomy of what matters in initialization: for tanh resnets, quality of an initialization is determined by how much gradient explosion there is (measured by χ (0) /χ (L) ); for (α-)ReLU resnets, it is determined by how expressive the random network is (measured by the metric expressivity s (L) ). We hypothesize this is because in tanh resnets, the gradient dynamics is much more explosive than the expressivity dynamics (exp(Θ( √ l)) vs Θ(l)), whereas for ReLU it's somewhat the opposite (χ w , χ v = Θ(1) vs s = exp(Θ(l))).

Tanh, vary σ w . We train a grid of reduced and full tanh resnets on MNIST, varying the variance σ 2 w and the number of layers (for FRN we fix σ v = 1). The results are indicated in Fig. [3(a,](#fig_44)[b](#)). We see that in either model, deeper resnets favor much smaller σ w than shallower ones. The white dotted lines in Fig. [3(a,](#fig_44)[b](#)) confirm our theory: according to Eq. ( ), for the same gradient ratio R = χ (0) /χ (L) , we want log R ≈ σ w √ L. Indeed, the white dotted lines in Fig. [3(a,](#fig_44)[b](#)) trace out such a level curve and it remarkably pinpoints the largest σ w that gives the optimal test set accuracy for each depth L. Why isn't the best initialization given by R = 1 ⇐⇒ σ w = 0? We believe that when L and/or σ w is small, gradient dynamics no longer dominates the initialization quality because it has "less room to explode," and expressivity issues start to dampen the test time performance.

$Tanh, vary σ 2 a /σ 2 v .$As suggested in the analysis of Eq. ( ), the ratio ρ 2 = σ 2 a /σ 2 v determines the fixed point e * and its convergence rate by itself while also contributes to the rate of gradient explosion in tanh FRN. We seek to isolate its effect on forward dynamics by varying σ v with ρ such that σ v / 1 + ρ 2 is kept constant, so that the leading term of the log gradient ratio is kept approximately equal for each L and ρ. Fig. [3(c](#fig_44)) shows the test accuracies of a grid of tanh FRN initialized with such an ensemble of σ • s. What stands out the most is that performance is maximized essentially around a fixed value of L regardless of ρ, which shows that indeed gradient dynamics determines the initialization quality in tanh resnets. There is also a minor increase in performance with increasing ρ regardless of L; this is counterintuitive as increasing ρ means "decreasing expressivity." It is currently not clear what accounts for this effect.

ReLU, vary σ w We train a grid of ReLU FRN on MNIST, varying σ 2 w ∈ [0, 1.5] while fixing

$σ 2 v = 1, σ 2 a = σ 2 b = 1 2 .$The resulting test set accuracies are shown in Fig. [3(d](#fig_44)). The dark upper region signifies failure of training caused by numerical issues with exploding activation and gradient norms: This corresponds to the region where p (L) , which is a measure of the mean magnitude of an neuronal activation in layer L, becomes too big. We see that the best test accuracies are given by depths just below where these numerical issues occur. However, if we were to predict that the optimal init is the one minimizing χ (0) /χ (L) ≥ 1, then we would be wrong -in fact it is exactly the opposite. In this case, the dynamics of s (l) , p (l) , and χ (0) /χ (l) are approximately the same (all exp(Θ(l)) with the same hidden constants), and optimal performance corresponds to the highest s (L) , p (L) , and χ (0) /χ (L) without running into infs.

α-ReLU, vary α. We similarly trained a grid of α-ReLU FRN on MNIST, varying only α and the depth, fixing all σ • . Fig. [3](#fig_6)(e) shows their test accuracies. We see similar behavior to ReLU, where when the net is too deep, numerical issues doom the training (black upper right corner), but the best performance is given by L just below where this problem occurs. In this case, if we were to predict optimality based on minimizing gradient explosion, we would be again wrong, and furthermore, the contour plot of χ (0) /χ (L) (white dashed line) now gives no information at all on the test set accuracy. In contrast, the contours for s (l) succeeds remarkably well at this prediction (yellow/green lines). 6  By interpolation, this suggests that indeed in the ReLU case, it is expressivity, not trainability, which determines performance at test time.

In all of our experiments, we did not find e dynamics to be predictive of neural network performance.

## Conclusion

In this paper, we have extended the mean field formalism developed by [[9,](#b8)[10,](#b9)[11]](#b10) to residual networks, a class of models closer to practice than classical feedforward neural networks as were investigated earlier. We proved and verified that in both the forward and backward passes, most of the residual networks discussed here do not collapse their input space geometry or the gradient information exponentially. We found our theory incredibly predictive of test time performance despite saying nothing about the dynamics of training. In addition, we overwhelmingly find, through theory and experiments, that an optimal initialization scheme must take into account the depth of the residual network. The reason that Xavier [[4]](#b3) or He [[5]](#b4) scheme are not the best for residual networks is in fact not that their statistical assumptions are fragile -theirs are similar to our mean field theoretic assumptions, and they hold up in experiments for large width -but rather that their structural assumptions on the network break very badly on residual nets.

Open Problems. Our work thus have shown that optimality of initialization schemes can be very unstable with respect to architecture. We hope this work will form a foundation toward a mathematically grounded initialization scheme for state-of-the-art architectures like the original He et al. residual network. To do so, there are still two major components left to study out of the following three: 1. Residual/skip connection 2. Batchnorm 3. Convolutional layers. Recurrent architectures and attention mechanisms are also still mostly unexplored in terms of mean field theory. Furthermore, many theoretical questions still yet to be resolved; the most important with regard to mean field theory is: why can we make Axioms 3.1 and 3.2 and still be able to make accurate predictions? We hope to make progress on these problems in the future and encourage readers to take part in this effort.

## Notes

1 Under simplified conditions, Daniely et al. [[3]](#b2) showed that there exists a fixed point for any "well-behaved" activation function in a feedforward net. However, this result does not apply to architectures with residual connections.

## Appendices A Additional Figures

In figures appearing in the appendix, means χ (due to legacy reasons). We do so because the norm of the activation vector in a typical ReLU resnet blows up into NaN at around layer 90, while this is not a problem for α < 1. Our theoretical predictions track the average of empirical values closely for forward quantities p (l) , γ (l) , and e (l) for all α, but variance is extremely large for e (l) at α = 1; it also predicts the average gradient norm accurately for α = 1 to α = .7 (despite the fact that we should not expect so for α ≤ .75 due to exploding variance (Thm B.19)), although variance is large for α = 1 at earlier layers (i.e. later layers w.r.t backpropagation). However it consistently and significantly overestimates the average gradient norm for α = .6 to α = .5, where the variance is so large that one standard deviation below the mean results in negative values. All plots are made with parameters     [1](#formula_52))  [1](#formula_52))  [1](#formula_52))  [1](#formula_52)) . We draw |e (l) -e (l-1) | for each of these dynamics in colored solid lines. We predict that each dynamic is Θ(l -µ ), where µ = (1 -Jα(e * ))/(1 -α), and the dashed line gives l -µ-1 (Thm B.18), shifted vertically to better compare the slope in log scale (i.e. the exponent of the polynomial dynamics). (See footnote 4 for why we plot the dynamics this way). We see that the our asymptotic prediction is very accurate for the sequence of e (l) that starts with e (0) = 0.99, the closest to e * for each α, while other lines only slowly converge to the same exponent (which is the slope in the log-log plot). This is to be expected based on the proof of Thm B.18. For α = .9, the e (0) = .99 line upticks at around 10 3 and then turn into NaNs due to numerical instability. (b) Colored lines are • (0) /• (l) for • = χ, χ b , χ w (we are not taking logs in addition to plotting in log-log scale like in Fig. [C](#).15). The dashed lines are our asymptotic predictions for the dynamics with corresponding colors, based on Thm B.21, again shifted appropriately to easily compare slope visually. We see that for every alpha our asymptotic predictions are highly accurate. For both (a) and (b), we did not show α = 1 case as ReLU FRN runs into numerical issues quickly (i.e. with even for 100 layers) because of exponential explosions in p (l) and χ (l) as predicted by Thms B.16 and B.20, so we cannot expect to empirically verify the precise predicted asymptotics. All plots are made with parameters

$σ 2 v = 1.5, σ 2 a = .$$Â(0) b =Â(l) b Â(0) w =Â(l) w l R l R + 1 l R ¡ ®=(1 ¡ ®) α = .8$$Â(0) b =Â(l) b Â(0) w =Â(l) w l R l R + 1 l R ¡ ®=(1 ¡ ®) α = .7$$Â(0) b =Â(l) b Â(0) w =Â(l) w l R l R + 1 l R ¡ ®=(1 ¡ ®) α = .6$$Â(0) b =Â(l) b Â(0) w =Â(l) w l R l R + 1 l R ¡ ®=(1 ¡ ®) α = .55$$Â(0) b =Â(l) b Â(0) w =Â(l) w l R l R + 1 l R ¡ ®=(1 ¡ ®) α = .51$$Â(0) b =Â(l) b Â(0) w =Â(l) w l R l R + 1 l R ¡ ®=(1 ¡ ®)$$σ 2 v = 1.5, σ 2 a = .5, σ 2 w = 1.69, σ 2 b = .49; only α is varied.$Table A.1: Glossary of Symbols. "Mean normalized" is abbreviated "m.n." Symbol Meaning Ref σ • standard deviation of trainable parameter • x (l)

activation vector/input vector h (l)  hidden vector N width (same across all layers) p (l)  m.n. squared length of activation vector x (l) 3.3 q (l) m.n. squared length of hidden vector h (l) 3.3

$γ (l) m.n. dot product x (l) • x (l) 3.4 λ (l) m.n. dot product h (l) • h (l) 3.4 s (l) m.n. squared distance x (l) -x (l) 2 3.4 e (l)$cosine distance γ (l) / p (l) p (l) 3.4 e * limit value of e (l) as l → ∞ c (l)  cosine distance λ (l) / q (l) q (l) 3.4 χ (l)  m.n. gradient squared norm w.r.t. x (l) 3.5 Theorem B.2. Suppose φ is tanh-like. Assume RRN architecture.

$χ (l) • m.n. gradient squared norm w.r.t. trainable parameter • 3.5 φ variable nonlinearity R → R ψ α α-ReLU 5.2 V variance integral transform 5.3 W covariance integral transform 5.3 δ * e (l) converges like Θ(l -δ * ) in tanh FRN B.11 A leading coeff of log χ (0) /χ (L) in tanh FRN B.13 R log χ (0) /χ (L) ∼ R log L for (α < 1)-ReLU B.20 J α kernel function of α-ReLU C.30$• If σ w = 0, then p (l) = lVφ(σ 2 b ) + p (0) and q (l) = σ 2 b .

• If σ w > 0, lim l→∞ p (l) /l = 1 and lim l→∞ q (l) /(σ 2 w l) = 1. If φ = tanh, then we can obtain more terms of the asymptotic expansions:

$p (l) = l -2Cσ -1 w l 1/2 -C 2 σ -2 w log l + O(1) q (l) = σ 2 w l -2Cσ w l 1/2 -C 2 log l + O(1) as l → ∞, where C = 2/π.$Theorem B.3. Suppose φ is antisymmetric. Then in an RRN, λ and γ satisfy the recurrence

$λ = σ 2 w γ + σ 2 b γ = Wφ(q, λ) + γ.$Theorem B.4. Suppose φ is a tanh-like nonlinearity in an RRN. Assume e (0) < 1.

$• If σ w = 0, then γ (l) = lWφ(σ 2 b , σ 2 b ) + γ (0) = lVφ(σ 2 b ) + γ (0)$and λ (l) = σ 2 b , so that e (l) → 1 and 1 -e (l) = Θ(l -1 ). As a result, s (l) = p (l) (1 -e (l) ) = Θ(1).

• If σ w > 0, then γ (l) = Θ(l 2 π ), and e (l) → 0 like Θ(l 2 π -1 ). Thus s (l) = Θ(p (l) ) = Θ(l).

Theorem B.5. For any nonlinearity φ in an RRN, under assumptions Axiom 3.1 and Axiom 3.2, whenever φ2 (ζ) has finite variance for Gaussian variable ζ, χ = (σ 2 w V φ(q) + 1)χ, χ b = χV φ(q), χ w = χV φ(q)p.

Theorem B.6. For φ = tanh in an RRN,

• If σ w = 0, χ (m) = χ (l) for all l, m.

$• If σ w > 0, log(χ (m) /χ (l) ) = A( √ l - √ m) + B(log l -log m) + O(1)$where A = 4 Theorem B.7. Suppose φ = tanh. Then in an RRN

$• If σ w = 0, χ (l) b = χ (L) V φ(σ 2 b ) and χ (l) w = χ (L) V φ(σ 2 b )((l -1)Vφ(σ 2 b ) + p (0)$), where L is the last layer.

$• If σ w > 0, log(χ (m) b /χ (l) b ) = A( √ l - √ m) + B b (log l -log m) + O(1) log(χ (m) w /χ (l) w ) = A( √ l - √ m) + B w (log l -log m) + O(1)$where A = 4 

$q = σ 2 w p + σ 2 b p = σ 2 v Vφ(q) + σ 2 a + p$Theorem B.9. Suppose φ is tanh-like. Assume the FRN architecture.

$• If σ w = 0, then p (l) = (σ 2 v Vφ(σ 2 b ) + σ 2 a )l + p (0)$, and q (l) = σ 2 b .

$• If σ w > 0, then p (l) = b 0 l + b 1 l 1/2 + b 2 log l + O(1)$, where

$b 0 = σ 2 v + σ 2 a b 1 = -2Cσ 2 v σ -1 w σ 2 v + σ 2 a b 2 = -C 2 σ 4 v σ -2 w (σ 2 v + σ 2 a ) 2 and C = 2 π . Additionally, q (l) = σ 2 w b 0 l + σ 2 w b 1 l 1/2 + σ 2 w b 2 log l + O(1).$Theorem B.10. For any nonlinearity φ, in an FRN

$λ = σ 2 w γ + σ 2 b γ = σ 2 v Wφ(q, λ) + σ 2 a + γ$Theorem B.11. Assume φ = tanh in an FRN. Suppose e (0) < 1.

$• If σ w = 0, then λ (l) = σ 2 b and γ (l) = l(σ 2 v Wφ(σ 2 b , σ 2 b ) + σ 2 a ) + γ (0) = l(σ 2 v Vφ(σ 2 b ) + σ 2$a )+γ (0) . Thus e (l) → 1 and 1-e (l) = Θ(l -1 ). As a result, s (l) = p (l) (1-e (l) ) = Θ(1). • If σ w > 0, then e (l) converges to the unique fixed point e * = 1 determined by the equation

$e * = 1 σ 2 v + σ 2 a [σ 2 v 2 π arcsin (e * ) + σ 2 a ].$Furthermore, e (l) converges to e * polynomially:

$|e (l) -e * | is Θ(l -δ * ),$where

$δ * := 1 - 2 π 1 1 -(e * ) 2 σ 2 v σ 2 v + σ 2 a ∈ [ 2 π -1,1 2$)

$Since e * < 1, s (l) = Θ(p (l) ) = Θ(l).$Theorem B.12. For any nonlinearity φ in an FRN, under assumptions Axiom 3.1 and Axiom 3.2, whenever φ(ζ) 2 has finite variance for Gaussian variable ζ,

$χ = (σ 2 v σ 2 w V φ(q) + 1)χ, χ b = σ 2 v χV φ(q), χ w = σ 2 v χV φ(q)p, χ v = χVφ(q), χ a = χ$Theorem B.13. Assume φ = tanh in an FRN.

• If σ w = 0, χ (m) = χ (l) for all l, m.

$• If σ w > 0, then for l ≥ m ≥ 0, log(χ (m) /χ (l) ) = A( √ l - √ m) + B(log l -log m) + O(1)$where Theorem B.14. Suppose φ = tanh in an FRN.

$A = 4 3 2 π σ 2 v σ w σ 2 v + σ 2 a B = 4 9π σ 4 v σ 2 v + σ 2 a 3 σ 2 v + σ 2 a -σ 2 w$$• If σ w = 0, then χ (l) b = σ 2 v χ (L) V φ(σ 2 b ) χ (l) w = σ 2 v χ (L) V φ(σ 2 b )((σ 2 v Vφ(σ 2 b ) + σ 2 a )(l -1) + p (0) ) χ (l) v = χ (L) Vφ(σ 2 b ) χ (l) a = χ (L) .$• If σ w > 0, then for l ≥ m ≥ 0, log(χ

$(m) b /χ (l) b ) = A( √ l - √ m) + B b (log l -log m) + O(1) log(χ (m) w /χ (l) w ) = A( √ l - √ m) + B w (log l -log m) + O(1) log(χ (m) a /χ (l) a ) = A( √ l - √ m) + B(log l -log m) + O(1) log(χ (m) v /χ (l) v ) = A( √ l - √ m) + B(log l -log m) + O(1)$where

$A = 4 3 2 π σ 2 v σw √ σ 2 v +σ 2 a and B = 4 9π σ 4 v σ 2 v +σ 2 a 3 σ 2 v +σ 2 a$-σ 2 w are as in Thm B.13 and

$B b = B + 1 2 and B w = B -1 2 . B.2 α-ReLU Lemma B.15. If α > -1 2 , then Vψ α (q) = c α q α , where c α = 1 √ π 2 α-1 Γ α + 1 2 .$Note that if α ≤ -1 2 , then Vψ α (q) is not defined (its defining integral does not converge).

## B.2.1 Full Residual Network

By Thm B.8 and Lemma B.15, we have the length recurrences

$q = σ 2 w p + σ 2 b p = σ 2 v c α q α + σ 2 a + p$Theorem B.16. Suppose we have the nonlinearity φ = ψ α . The in an FRN:

$If α = 1, then p (l) = Θ((1 + σ 2 v σ 2 w /2) l ),$with the hidden constant depending on the initial condition. If 0 < α < 1, then p (l) = Θ(l Similarly, by Thm B.10, if q = q , then Theorem B.17. Suppose φ = ψ 1 . Then in an FRN, e (l) → 1 and

$1 1-α ). More precisely, lim l→∞ p/l 1 1-α = [σ 2 v σ 2α w c α (1 -α)] 1 1-α .$$λ = σ 2 w γ + σ 2 b γ = σ 2 v q α Wψ α (1, c) + σ 2 a + γ$$1 -e (l) ∼ [ 1 4 σ 2 v σ 2 w B -1 U l] -2 for B = 1+σ 2 v σ 2 w /2 and U = 2 √ 2$3π . As a result, s (l) = (1-e (l) )p (l) = Θ(l -2 exp(Θ(l))) = exp(Θ(l)). Theorem B.18. Suppose φ = ψ α for 0 < α < 1 in an FRN. Then e converges to the unique nonunit fixed point e * of J α , and |e * -e (l) | is Θ(l -µ ), where µ = (1 -Jα (e * ))/(1 -α). Additionally, s (l) = Θ(p (l) ) = Θ(l 1/(1-α) ). 2 (1 -α), but we have no proof for it. Based on this conjecture, we see there is a "discontinuity" of µ at α = 1: µ → 0 as α → 1, but for α = 1, the actual convergence dynamics has exponent -2 by Thm B.17.

Because of the following theorem, we cannot expect the equations of Thm B.12 to hold for α ≤ 3  4 . Theorem B.19. Suppose we have the nonlinearity ψ α in an FRN. Var( ψα (ζ) 2 ) diverges for any Gaussian variable ζ with mean 0 if α ≤ 3  4 but is finite if α > 3 4 .

Theorem B.20. Suppose we have the nonlinearity

$ψ α in an FRN. If α = 1, then χ (l-m) = χ (l) 1 2 σ 2 v σ 2 w + 1 m . If α ∈ ( 3 4 , 1), then χ (l-m) = Θ(1)χ (l) (l/(l -m)) R for R = α 2$(1-α)(2α-1) , where the constants in Θ(1) do not depend on l or m. 

## This exponent

$χ (l-m) v = Θ(1)χ (l) B l , χ (l-m) a = Θ(1)χ (l) B m .$where graphs the exponent α 1-α -R in terms of α. We see that on [0.5, 1], the maximum of this exponent is at α = 1.

$B = 1 + σ 2 v σ 2 w /2. If φ = ψ α in an FRN, for α < 1, then for l ≥ m ≥ 0, χ (l-m) b = Θ(1)χ (l) l R (l -m) -R-1 , χ (l-m) w = Θ(1)χ (l) l R (l -m) α 1-α -R , χ (l-m) v = Θ(1)χ (l) l R (l -m) α 1-α -R , χ (l-m) a = Θ(1)χ (l) (l/(l -m)) R .$
## C Proofs

A brief note about notation: We use ∼ to denote both how a random variable is sampled (ex: x ∼ N (0, 1) for a Gaussian x) and how a function behaves asymptotically, i.e. f (x) ∼ g(x) as x → a iff lim x→a f (x)/g(x) = 1. Context should be enough to differentiate between these two cases. We in addition use to denote asymptotic expansion. For example, if {α i } i≥0 is a sequence of strictly decreasing reals and {β i } i≥0 is a sequence of nonzero reals, then

$f (x) i≥0 β i (x -ξ) αi means that as x → ξ, f (x) - N i=0 β i (x -ξ) αi = Θ((x -ξ) α N +1 ).$
## C.1 Preliminary Lemmas

Lemma C.1. We have

$σ 2 w γ + σ 2 b σ 2 w p + σ 2 b = e(1 + O(γ -1 )).$regardless of whether e (l) = γ (l) /p (l) converges.

But suppose e (l) = γ (l) /p (l) → e * . If e * < 1, then

$σ 2 w γ + σ 2 b σ 2 w p + σ 2 b = e(1 + Θ(γ -1 ))$.

$If e * = 1, then σ 2 w γ + σ 2 b σ 2 w p + σ 2 b = e(1 + Θ( p -1 )),$where = 1 -e.

Proof.

$Write M = σ 2 b /σ 2 w . σ 2 w γ + σ 2 b σ 2 w p + σ 2 b = e(1 + 1 + M γ -1 1 + M p -1 ) = e(1 + M (γ -1 -p -1 ) + O(p -1 (γ -1 -p -1 ))).$In any situation, γ -1 -p -1 = O(γ -1 ) because γ ≤ p, so this gives the first statement. If e * exists and e * < 1, then γ -1 -p -1 = Θ(γ -1 ), which yields the second statement. If e * exists and e * = 1,

$then γ -1 -p -1 = p -1 ((1 -) -1 -1) = p -1 ( + O( 2 )) = Θ( p -1 ).$For any function f that is (k + 1)-times differentiable in a neighborhood of 0, we have the asymptotic expansion

$f (z) = k n=0 d n f dz n (0) z n n! + O(z k+1 ), as z → 0.$Since

$d n d(1/q) n q 1/2 Vφ(q) q→∞ = (-1) n 2 n √ 2π ∞ -∞ φ 2 (z)z 2n dz$whenever the RHS is integrable, we have Lemma C.2. Suppose φ 2 (z)z 2n is integrable over z ∈ R for all 0 ≤ n ≤ N + 1. Then Vφ(q) = q -1/2 ( N n=0 C n q -n + O(q -N -1 )) as q → ∞, where

$C n := (-1) n 2 n n! √ 2π ∞ -∞ φ 2 (z)z 2n dz.$Note that sech d (z) = Θ(e -d|z| ) for z → ∞ as long as d > 0, so that C n from the above result converges when φ = sech d . Therefore

Lemma C.3. Let d > 0. We have V sech d (q) q -1/2 n≥0 C n q -n , where

$C n := (-1) n 2 n n! √ 2π ∞ -∞ sech 2d (z)z 2n dz.$As corollaries, we obtain the following asymptotics.

Lemma C.4. V ṫanh(q) = 2 3 2 π q -1/2 + Θ(q -3/2 ) as q → ∞.

Proof. Use Lemma C.3 along with the fact that ṫanh(z) = sech 2 (z) and sech 4 z dz = 2 3 tanh z + 1 2 sech 2 z tanh z.

Lemma C.5. 1 -V tanh(q) = 2 π q -1/2 + Θ(q -3/2 ) as q → ∞.

Proof. Use Lemma C.3 along with the fact that 1 -tanh 2 (z) = sech 2 (z) and sech 2 z dz = tanh z.

Lemma C.6. sech 2 (t) ≥ exp(-t 2 ) for all t, with equality iff t = 0.

Proof. The lower bound is equivalent to

$2 ≥ e t-t 2 /2 + e -t-t 2 /2$The RHS has derivative (1 -t)e t-t 2 /2 -(1 + t)e -t-t 2 /2 . This is 0 iff

$1 -t 1 + t = e -2t$which has a solution 0 and in general can only have solution t ∈ (-1, 1) (by considering the sign of the LHS). Since each side is analytic in t ∈ (-1, 1), we expand

$log 1 -t 1 + t = log e -2t log(1 -t) -log(1 + t) = -2t (-t -t 2 -• • • ) -(t -t 2 + • • • ) = -2t -2t -2t 3 -• • • = -2t$which shows that the only solution is t = 0. A simple plot shows that t = 0 is a maximum, where the bound in question achieves equality.

Lemma C.7. Suppose φ = tanh. Then V φ(q) ≥ 1 √ 4q+1 .

As a sanity check, Lemma C.4 shows that V φ(q) ∼ C 0 q 1/2 where C 0 ≈ .5319, which is above the .5 in this lemma.

Proof. By Lemma C.6,  

$V φ(q) = dµ(z) φ2 ( √ qz) ≥ 1 √ 2π dz exp(-z 2 /2 -2qz 2 ) = 1 √ 2π dz exp(-(4q + 1)z 2 /2) = 1 √ 4q + 1 .$$Σ(M, N, d) =              Θ(1) if d < -1 log N + O(1) if d = -1 N d+1 d+1 + O(1) if -1 < d < 0 N -M + 1 if d = 0 1 d+1 N d+1 + 1 2 N d + O(N max(0,d-1) ) if d > 0$$a+1 a z d -a d dz = 1 d + 1 ((a + 1) d+1 -a d ) = (a d + d 2 a d-1 + • • • ) -a d = d 2 a d-1 + Θ(a d-2 ).$where the hidden constants in Θ depend only on d (and in fact this term vanishes if d = 1). Thus

$Σ(M, N, d) = N +1 M z d dz - N a=M [ d 2 a d-1 + Θ(a d-2 )] = 1 d + 1 ((N + 1) d+1 -M d+1 ) - d 2 Σ(M, N, d -1) + Θ(Σ(M, N, d -2)) If -1 < d < 0, then Σ(M, N, d -1) = Θ(1), so that Σ(M, N, d) = (N +1) d+1 d+1 + O(1) = N d+1 d+1 + O(1). If d > 0 and d = 1, then Σ(M, N, d -1) = N d d , so that Σ(M, N, d) = 1 d + 1 N d+1 + N d + Θ(N max(0,d-1) ) - 1 2 N d + Θ(Σ(M, N, d -2)) = 1 d + 1 N d+1 + 1 2 N d + O(N max(0,d-1) ).$We can obtain more terms in the expansion for higher d via the Euler-Maclaurin formula, but this suffices for our purposes.

## C.2 Dynamics Zoo

This section deduces the asymptotic behaviors of some sequences governed by recurrence equations.

For the most part, the leading term of their asymptotic expansions is as one would expect from the corresponding differential equation. However, in some cases we need subleading terms for later results. They require slightly more nuanced reasoning. First we present a technical lemma. Lemma C.9. Let F : R × N → R be a function such that for a subset U ⊆ R, and for all z, z ∈ U, z ≥ z =⇒ F (z, n) ≥ F (z , n) for every n. Suppose sequences a (l) , b (l) , c (l) satisfy

• a (l+1) = F (a (l) , l) for all l;

• b (l+1) ≤ F (b (l) , l) for all l above a constant K b .

• c (l+1) ≥ F (c (l) , l) for all l above a constant K c . and furthermore, a (l) , b (l) , c (l) all fall into U for l above a constant K U .

If for some m

$≥ max(K b , K U ), b (m) ≤ a (m) , then b (l) ≤ a (l) , ∀l ≥ m. Similarly, if for some n ≥ max(K c , K U ), c (n) ≥ a (n) , then c (l) ≥ a (l) , ∀l ≥ n. Proof. For the first claim: b (m) ≤ a (m) =⇒ b (m+1) ≤ F (b (m) , m) ≤ F (a (m) , m) = a (m+1) .$Here the last inequality used the monotonicity of F . Induction gives the desired result.

It's similar for the second claim, where the inductive step is

$c (m) ≥ a (m) =⇒ c (m+1) ≥ F (c (m) , m) ≥ F (a (m) , m) = a (m+1) .$Lemma C.10. Suppose (l) satisfies the recurrence

$(l) = (l-1) (1 + δ l β ). for some nonzero constant δ ∈ R independent of l. • If β > 1, then (l) = Θ(1). • If β = 1, then (l) = Θ(l δ ). • If 0 < β < 1, then (l) = exp( δ 1-β l 1-β + Θ(l ψ1(1-2β) ))$, where ψ 1 (x) = max(0, x) is the ReLU function.

Proof. We have log (l) = log (l-1) + log(1 + δ/l β ) = log (l-1) + δ/l β + Θ(δ 2 /l 2β ) for large l. If β > 1, then l l -β converges, and

$log (l) = log (0) -Θ(1) (l) = Θ(1). If β = 1, then log (l) = log (0) + δ log l + Θ(1) (l) = Θ(l δ ). If β < 1, then log (l) = log (0) + δ 1 -β l 1-β + Θ(l 1-2β ) (l) = exp( δ 1 -β l 1-β + Θ(l ψ1(1-2β) )).$Lemma C.11. Suppose (l) = Cl -α + (l-1) (1 + δ/l β ) for α ∈ R, C = 0, and δ = 0. Then

$• If β > 1, then -(l) = Θ(l 1-α ) if α ∈ (0, 1); -(l) = Θ(log l) if α = 1; -(l) = Θ(1) if α > 1.$Proof. Consider the differential equation

$ẋµ = -µx β+1 µ /t$for constant µ has solution x µ = [β(µ log t + C)] -1/β for some constant C determined by initial condition. Note that

$-µx µ (t) β+1 /t ≤ x µ (t + 1) -x µ (t) ≤ -µx µ (t + 1) β+1 /(t + 1) = -(1 -o(t -1 ))µx µ (t) β+1 /t.$For any small enough α > 0, we apply Lemma C.9 with F ( , l) = -µ β+1 /l (which is monotonic in for small enough ), c (l) = x µ (l), and b (l) = x µ-α (l) to obtain

$x µ-α (l) ≤ (l) ≤ x µ (l)$for large enough l and appropriately chosen initial conditions. This shows that (l) = Θ(log l -1/β ) Taking α → 0, we also obtain the leading coefficient (l) ∼ [βµ log l] -1/β .

Lemma C.13. Suppose a sequence u (l) is governed by the equation

$u (l) -u (l-1) = A(u (l-1) + B) α ,$where α ∈ [0, 1) and A > 0. Then

$u (l) = K 1 l 1 1-α -K 2 l α 1-α log l + o(l α 1-α log l), where K 1 = [A(1 -α)] 1 1-α and K 2 = 1 2 A 1 1-α (1 -α) α 1-α -1 α.$Proof. Leading term. The differential equation

$ẋA,B = A(x A,B + B) α has solution x A,B (l) = [A(1 -α)(l + S)]1$1-α -B for some constant S. Since ẋA,B is monotonic, we have (writing x = x A,B for brevity)

$A(x A,B (l) + B) α = ẋA,B (l) ≤ x A,B (l + 1) -x A,B (l) ≤ ẋA,B (l + 1) ≤ (A + o(1))(x A,B (l) + B) α$for large enough l. We apply Lemma C.9 with F (x, l) = x + A(x + B) α (which is monotonic in x for large x), c (l) = x A,B (l), and b (l) = x A-,B (l) to obtain

$x A-,B (l) ≤ u (l) ≤ x A,B (l)$for large enough l and appropriate initial conditions. Therefore lim u (l) /l

$1 1-α ∈ [[(A -)(1 - α)] 1 1-α , [A(1 -α)] 1 1-α ].$Taking → 0 gives the leading term.

$Subleading term. Now let v (l) := u (l) -ℵl 1 1-a , where ℵ = [A(1 -α)] 1 1-α . Then we have the recurrence v (l+1) + ℵ(l + 1) 1 1-α -v (l) -ℵl 1 1-α = A(v (l) + ℵl 1 1-α + B) α v (l+1) -v (l) + ℵ( 1 1 -α l α 1-α + 1 2 ( 1 1 -α )( α 1 -α )l α 1-α -1 + Θ(l α 1-α -2 )) = A[ℵ α l α 1-α + α(v (l) + B)ℵ α-1 l -1 + Θ((v (l) + B)l -1-1 1-α )] v (l+1) -v (l) = α 1 -α v (l) l -1 - 1 2 ℵ( 1 1 -α )( α 1 -α )l α 1-α -1 + g(l)$for some g(l) = O(l α 1-α -2 + l -1 ) and where, to get the last equation, we have used Aα α = 1 1-α ℵ to cancel the l α 1-α term and simplified αAℵ α-1 = α 1-α . For any J > 0, the differential equation vJ

$(l) = α 1-α v J (l)l -1 -Jl α 1-α -1 has solution v J (l) = C[l(1 -α)] α 1-α -Jl α 1-α log l. Note that the functions F J (z, n) = z + α 1-α zn -1 -Jn α 1-α -1 and G J (z, n) = F J (z, n) + g(n) is monotonic in z (for positive n).$For large l, we also have vJ (l) and F J (v J (l), l) = v J (l) + vJ (l) decreasing in l. Thus for any > 0 and l large enough 

$G J+ (v J (l), l) ≤ F J+ /2 (v J (l), l) ≤ v J (l)+ vJ (l+1) ≤ v J (l+1) ≤ F J (v J (l), l) ≤ G J-(v J (l), l).$
## Now define v

$(l) = u (l) -[A(1 -α)l] 1 1-α ,$and similar to the proof of Lemma C.13, we find

$v (l+1) -v (l) = α 1 -α v (l) l -1 -Kl α 1-α -1 + C + g(l)$where

$K = 1 2 A 1 1-α (1 -α) α 1-α -1 α and g(l) = O(l α 1-α -2 + l -1 ). If α 1-α > 1 ⇐⇒ α > 1 2 , then C + g(l) = o(l α 1-α -1$) and we can proceed as in the proof of Lemma C.13 to find v (l) ∼ Kl [1](#formula_52)), then by using the differential equation vJ (l) = α 1-α v J (l)l -1 + J to approximate the difference equation solution and applying Lemma C.9 as in the proof of Lemma C.13, we obtain v (l) (l) ∼ C(1-α)

$α 1-α log l. If α 1-α = 1 ⇐⇒ α = 1 and K = C, then v (l+1) -v (l) = α 1-α v (l) l -1 -(K -C)l α 1-α -1 + g(l), so that the technique used in Lemma C.13 would obtain v (l) ∼ (K -C)l α 1-α log l = (K -C)l log l. If α 1-α < 1 ⇐⇒ α < 1 2 , then v (l+1) -v (l) = α 1-α v (l) l -1 + C + o($1-2α l.

## C.3 Forward Dynamical Equations

Here we derive the recurrences governing the forward length and correlation quantities p, q, λ, γ.

We start with reduced residual networks.

Lemma B.1. Suppose φ is antisymmetric. Then in an RRN, p and q satisfy the recurrence q = σ 2 w p + σ 2 b p = Vφ(q) + p.

Proof. We have

$q = h 2 j = i (w ji x i + b j ) 2 = b 2 j + i w 2 ji x 2 i + 2 i w ji x i b j + 2 j =l w ji w li x 2 i$But w ji , w li , x, and b j form an independency, so the last two sums are 0, and the terms in the first sum split multiplicatively. Therefore

$q = σ 2 b + i w 2 ji x 2 i = σ 2 b + N • σ 2 w N p = σ 2 b + σ 2 w p.$For the recurrence of p, we have

$p = x 2 i = (φ(h i ) + x i ) 2 = φ(h i ) 2 + x 2 i + 2 φ(h i )x i$As N → ∞, the coefficient w ii of x i in h i has vanishing covariance, so h i and x i become independent. Therefore φ(h i )x i = φ(h i ) x i . Because h i is the sum of a large number of independent random variables, by CLT, h i is a Gaussian with mean i w ji x i + b j = 0 since w ji = b j = 0. Our antisymmetry assumption on φ then implies φ(h i ) = 0. Therefore,

$p = φ(h i ) 2 + x 2 i = Vφ(q) + p as desired.$Theorem B.3. Suppose φ is antisymmetric. Then in an RRN, λ and γ satisfy the recurrence

$λ = σ 2 w γ + σ 2 b γ = Wφ(q, λ) + γ.$Proof. Similar to Lemma B.1. Now, for the full residual networks, the proofs are similar, but we no longer need to assume that φ is antisymmetric because of the randomization via the extra sets of weights. Theorem B.8. For any nonlinearity φ in an FRN,

$q = σ 2 w p + σ 2 b p = σ 2 v Vφ(q) + σ 2 a + p Proof. q = h 2 j = (w i j x i + b j ) 2 = (w i j x i ) 2 + b 2 j = σ 2 w x 2 i + σ 2 b = σ 2 w p + σ 2 b p = x 2 i = (v j i φ(h j ) + x i + a i ) 2 = σ 2 v φ(h i ) 2 + x 2 i + σ 2 a = σ 2 v Vφ(q) + σ 2 a + p$where in the third equality for p, we are now using the independence of v j i from all other variables to cancel out the terms, whereas before we had to rely on φ being antisymmetric.

Theorem B.10. For any nonlinearity φ, in an FRN

$λ = σ 2 w γ + σ 2 b γ = σ 2 v Wφ(q, λ) + σ 2 a + γ$Proof. Similar to Thm B.8.

## C.4 Backward Dynamical Equations

Here we derive the recurrences governing the gradient quantities χ and χ • for different •, all under the gradient independence assumption. Write β 

$χ = (σ 2 w V φ(q) + 1)χ, χ b = χV φ(q), χ w = χV φ(q)p.$Proof. For a reduced residual network, we have the following derivative computation:

$∂x i ∂x j = δ ji + φ(h i ) ∂h i ∂x j , ∂x i ∂h j = δ ji φ(h j ), ∂h i ∂x j = w ij , ∂h i ∂w ij = x j , ∂h i ∂b j = δ ij .$Then

$β j = β j + i β i φ(h i ) ∂h i ∂x j = β j + i β i φ(h i )w ij β 2 j = [β j + i β i φ(h i )w ij ] 2 = β 2 j + i β 2 i φ2 (h i )(w ij ) 2 + 2 i<k β i β k φ(h i )w ij φ(h k )w kj + 2 i β j β i φ(h i )w ij$The last two terms of the above vanish as w ij is independent from w kj , h i , h k and β i , β j , β k by Axiom 3.2, and w ij = 0.

Therefore, applying Axiom 3.1,

$β 2 j = σ 2 w β 2 j φ2 (h i ) + β 2 j = (σ 2 w V φ(q) + 1) β 2 j$We similarly have

$∂E ∂b j = i ∂E ∂x i ∂x i ∂h j = β j φ(h j ), since ∂x i ∂h j = δ ji φ(h j ) ∂E ∂b j 2 = β 2 j φ(h j ) 2 = β 2 j V φ(q), by Axiom 3.2(b); ∂E ∂w ji = i ∂E ∂x i ∂x i ∂h j ∂h j ∂w ji = β j φ(h j )x i , since ∂x i ∂h j = δ ji φ(h j ) ∂E ∂w ji 2 = β 2 j φ2 (h j )x 2 i = β 2 j V φ(q)p, by Axiom 3.2(b)$In the last equation we have also used the fact that as N → ∞, h j and x i become independent (they are jointly Gaussian and their correlation w 2 ji goes to 0 with N ).

Theorem B.12. For any nonlinearity φ in an FRN, under assumptions Axiom 3.1 and Axiom 3.2, whenever φ(ζ) 2 has finite variance for Gaussian variable ζ,

$χ = (σ 2 v σ 2 w V φ(q) + 1)χ, χ b = σ 2 v χV φ(q), χ w = σ 2 v χV φ(q)p, χ v = χVφ(q), χ a = χ$Proof. For the full residual network, we have the following derivative computations:

$∂x i ∂x j = δ ji + k v ik φ(h k ) ∂h k ∂x j , ∂x i ∂h j = v ij φ(h j ), ∂h i ∂x j = w ij , ∂h i ∂w ij = x j , ∂h i ∂b i = 1, ∂x i ∂v ik = φ(h k ), ∂x i ∂a i = 1.$Again let β j = ∂E ∂xj . Then

$β j = i β i (δ ji + k v ik φ(h k ) ∂h k ∂x j ) = i β i (δ ji + k v ik φ(h k )w kj )$Thus,

$β 2 j = [ i β i (δ ji + k v ik φ(h k )w kj )] 2 = β 2 j + i,k v 2 ik w 2 kj V φ(q) β 2 i = β 2 j (1 + σ 2 v σ 2 w V φ(q))$where in the second equality we applied the independence argument as in the proof of Thm B.5, leveraging Axiom 3.2, and in the third equality we used Axiom 3.1 to get β 2 i = β 2 j . The other computations are similar to the proof of Thm B.12.

C.5 Tanh: Reduced Residual Network C.5.1 Forward Dynamics Theorem B.2. Suppose φ is tanh-like. Assume RRN architecture.

$• If σ w = 0, then p (l) = lVφ(σ 2 b ) + p (0) and q (l) = σ 2 b .$• If σ w > 0, lim l→∞ p (l) /l = 1 and lim l→∞ q (l) /(σ 2 w l) = 1. If φ = tanh, then we can obtain more terms of the asymptotic expansions:

$p (l) = l -2Cσ -1 w l 1/2 -C 2 σ -2 w log l + O(1) q (l) = σ 2 w l -2Cσ w l 1/2 -C 2 log l + O(1)$as l → ∞, where C = 2/π.

Proof. The case with σ w = 0 is trivial. We assume σ w > 0 from here on.

p and q are asymptotically linear with l. We first show that, for any ω < 1,

$l + p (0) ≥ p (l) ≥ ωl and σ 2 w (l + p (0) ) + σ 2 b ≥ q (l) ≥ σ 2 w ω(l -1) + σ 2 b , Lemma C.16. Let φ is antisymmetric. Then for τ ∈ [0, π/2], Wφ(q, q cos τ ) = lim t→τ 1 π sin t w ≥|w| dw dw Υ(w, w ; τ )φ( √ q √ 2 (w + w ))φ( √ q √ 2 (w -w)) = 1 π ∞ 0 r dre -r 2 /2 π 0 dθΣ( √ qr, θ; τ ) = 1 π ∞ 0 s dsq -1 e -s 2 q -1 /2 π 0 dθΣ(s, θ; τ ) = 1 π π 0 dθ ∞ 0 dse -s 2 q -1 /2 ∂ ∂s Σ(s, θ; τ )$where Υ(w, w ; τ

$) := e -1 2 ( w 2 1-c + (w ) 2 1+c ) -e -1 2 ( (w ) 2 1-c + w 2 1+c$) with c = cos τ , and Σ(s, θ; τ ) := φ(s sin θ)φ(s sin(θ -τ )).

Of course, in the above lemma, the limit in the first equation is only necessary when τ = 0 or τ = π/2.

Proof. Let c := cos τ and

$Γ := Wφ(q, cq) = 1 2πq √ 1 -c 2 dz exp(-z T Σ -1 z/2)φ(z)φ(z ),$where Σ = q cq cq q .

Our proof will have two portions: Symmetrization of the Γ integral and trigonometric change of variables for evaluation.

$Symmetrization. Σ is diagonalized by Ω = 1 √ 2q -1 1 1 1 , Σ = Ω T Diag(1 -c, 1 + c)Ω.$By a change of variable w = Ωz, so that dw = q -1 dz, we have

$Γ = 1 2π √ 1 -c 2 dw exp(-w T Diag(1 -c, 1 + c) -1 w/2)φ( √ q √ 2 (w -w))φ( √ q √ 2 (w + w )) = 1 2π √ 1 -c 2 dw dw e -1 2 ( w 2 1-c + (w ) 2 1+c ) φ( √ q √ 2 (w -w))φ( √ q √ 2 (w + w ))$By a change of variable swapping w with w , we get

$Γ = - 1 2π √ 1 -c 2 dw dw e -1 2 ( (w ) 2 1-c + w 2 1+c ) φ( √ q √ 2 (w + w ))φ( √ q √ 2 (w -w)) Thus 2Γ = 1 2π √ 1 -c 2 dw dw Υ(w, w ; τ )φ( √ q √ 2 (w + w ))φ( √ q √ 2 (w -w))$where

$Υ(w, w ; τ ) = e -1 2 ( w 2 1-c + (w ) 2 1+c ) -e -1 2 ( (w ) 2 1-c + w 2 1+c ) .$Note that, by the antisymmetry of φ, the integrand K := Υ(w, w ; τ )φ(. . .)φ(. . .) above has the symmetries K(w, w ) = K(w , w) = K(w, -w ), and is everywhere nonnegative.  This gives the first equation in the lemma.

$Polar Coordinates. Let w √ 1-c = r cos θ, w √ 1+c = r sin θ, so that w = r cos θ √ 1 -c = √ 2r cos θ sin τ 2 w = r sin θ √ 1 + c = √ 2r sin θ cos τ 2 dw dw = 1 -c 2 r dr dθ = (sin 2 τ )r dr dθ. Then A := w ≥|w| e -( w 2 1-c + (w ) 2 1+c )/2 φ( q/2(w + w ))φ( q/2(w -w)) dw dw = sin 2 τ ∞ 0 r dre -r 2 /2 π-τ /2 τ /2 dθφ( √ qr sin(θ + τ /2))φ( √ qr sin(θ -τ /2)).$Similarly, let w

$√ 1+c = r cos θ, w √ 1-c = r sin θ, so that w = r cos θ √ 1 + c = √ 2r cos θ cos τ 2 w = r sin θ √ 1 -c = √ 2r sin θ sin τ 2 dw dw = 1 -c 2 r dr dθ = (sin 2 τ )r dr dθ, and B = w ≥|w| e -( w 2 1+c + (w ) 2 1-c )/2 φ( q/2(w + w ))φ( q/2(w -w)) dw dw = -sin 2 τ ∞ 0 r dre -r 2 /2 π/2+τ /2 π/2-τ /2 dθφ( √ qr cos(θ + τ /2))φ( √ qr cos(θ -τ /2)) = -sin 2 τ ∞ 0 r dre -r 2 /2 τ /2 -τ /2$dθφ( √ qr sin(θ + τ /2))φ( qr sin(θ -τ /2)).

$Thus Γ = 1 π √ 1 -c 2 (A -B) = 1 π ∞ 0 r dre -r 2 /2 π-τ /2 -τ /2 dθφ( √ qr sin(θ + τ /2))φ( √ qr sin(θ -τ /2)) = 1 π ∞ 0 r dre -r 2 /2 π 0 dθφ( √ qr sin(θ))φ( √ qr sin(θ -τ )).$This gives the second equation in the lemma, and a change of variables s = √ qr gives the third.

For the fourth equality, we start from the third equality, and apply integration by parts:

$1 π ∞ 0 s dsq -1 e -s 2 q -1 /2 π 0 dθΣ(s, θ; τ ) = 1 π π 0 dθ ∞ 0 dssq -1 e -s 2 q -1 /2 Σ(s, θ; τ ) = 1 π π 0 dθ -e -s 2 q -1 /2 Σ(s, θ; τ ) ∞ s=0 + ∞ 0 dse -s 2 q -1 /2 ∂ ∂s Σ(s, θ; τ ) = 1 π π 0 dθ ∞ 0 dse -s 2 q -1 /2 ∂ ∂s Σ(s, θ; τ ).$where the last equality follows because Σ(0, θ; τ ) = 0 and e -s 2 q -1 /2 → 0 as s → ∞.

In the following lemmas, the "2" is not important, and can be any arbitrary finite or infinite value.

Lemma C.17. Suppose a function f

$: (0, 2) → R is C k on (0, 2). If lim x↓0 f (i) (x)$exists and is finite for every i ∈ [0, k], then f can be extended to [0, 2) such that one sided ith derivatives exist at 0 for all i ∈ [0, k].

$Proof. Consider f (i) (0) := f (i) (1) - 1 0 f (i+1) (x) dx for i ∈ [0, k -1], which naturally is also equal to f (i) ( ) -0 f (i+1) (x) dx for any > 0. Certainly f (i) (x) → f (i) (0)$as x → 0 if this limit exists -and by assumption it does, for 0 ≤ i ≤ k -1. Therefore, we can define the extension of f (i) to x = 0 to be f (i) (0) := f (i) (0). But we need to check that for i

$∈ [0, k -1]. lim →0 1 (f (i) ( ) -f (i) (0)) = f (i+1) (0)$so that all one sided ith derivatives exist. But

$1 (f (i) ( ) -f (i) (0)) = 1 0 f (i+1) (x) dx = f (i+1) (0) + 1 0 (f (i+1) (x) -f (i) (0))I(x ∈ [0, ]) dx Since lim x↓0 f (i+1) (x) = f (i+1) (0), f (i+1) (x)-f (i+1) (0)$is bounded for small x, and by dominated convergence,

$1 0 (f (i+1) (x) -f (i) (0))I(x ∈ [0, ]) dx → 1 0 0 dx = 0 as → 0. Thus lim →0 1 (f (i) ( ) -f (i) (0)) = f (i+1) (0) as desired. Lemma C.18. If f : [0, 2) → R is C k on (0,$2) and has one sided derivatives at 0 up to order k, then

$f ( ) = f (0) + f (1) (0) + • • • + i-1 (i -1)! f (i-1) (0) + O( i )$for any i ≤ k.

Proof. We have

$f ( ) = f (0) + 0 f (1) (x) dx = f (0) + f (1) (0) + 0 f (1) (x) -f (1) (0) dx = f (0) + f (1) (0) + 0 x0 0 f (2) (x 2 ) dx 2 dx 1 = f (0) + f (1) (0) + 2 2 f (2) (0) + 0 x1 0 f (2) (x 2 ) -f (2) (0) dx 2 dx 1 . . . f ( ) = f (0) + f (1) (0) + • • • + i-1 (i -1)! f (i-1) (0) + 0 dx 1 x1 0 dx 2 • • • xi-1 0 dx i f (i) (x i )$for any i ≤ k. It suffices then to bound the size of the integral. Since

$f (i) (x) → f (i) (0) as x ↓ 0 by assumption, |f (i) (x i )| is bounded by some constant C on the integration region A := {(x 1 , . . . , x i ) : ≥ x 1 ≥ • • • ≥ x i } for small enough . Therefore, 0 dx 1 x1 0 dx 2 • • • xi-1 0 dx i f (i) (x i ) = f (i) (x i )I( x ∈ A) d x ≤ C|A| = Θ( i ).$As a corollary, Lemma C.19. If f : (0, 2) → R is smooth on (0, 2) and lim x→0 f (i) (x) exists and is finite for all i, then f can be extended to [0, 2) and be one-sided smooth at 0, and

$f ( ) = f (0) + f (1) (0) + • • • + i-1 (i -1)! f (i-1) (0) + O( i )$for any i. Lemma C.20. Let φ = tanh. For any fixed c, Wφ(q, cq) is smooth (infinitely differentiable) on q ∈ (0, ∞). As a function of Q := q -1 , it can be extended smoothly to the point Q = 0, so that

$Wφ(q, cq) = lim q →∞ Wφ(q , cq ) + q -1 lim q →∞ ∂Wφ(q , cq )/∂(q ) -1 + • • • + q -i+1 (i -1)! lim q →∞ ∂ i-1 Wφ(q , cq )/∂(q ) -i+1 + O(q -i )$for any i ≥ 0. Furthermore, for c bounded away from 1, the constants hidden O can be taken independent of c.

Proof. Smoothness on (0, ∞). By the third equation of Lemma C.16, for Q ∈ (0, ∞) ⇐⇒ q ∈ (0, ∞),

$1 π ∞ 0 s ds ∂ n ∂Q n Qe -s 2 Q/2 π 0 dθ|φ(s sin θ)φ(s sin(θ -τ ))| ≤ ∞ 0 s ds ∂ n ∂Q n Qe -s 2 Q/2 < ∞,$so by Leibniz's integral rule and a simple induction, all derivatives of Wφ(q, cq) against Q exists for any Q ∈ (0, ∞).

Extension to Q = 0. By Lemma C.19, it suffices to show that the limit of ∂ k Wφ(q,cq)

∂Q k exists and is finite as Q → 0, for all k. Let τ = arccos c. By the fourth equation of Lemma C.16, we have explicitly

$∂ k Wφ(q, cq) ∂Q k = 1 π π 0 dθ ∞ 0 ds( -s 2 /2) k e -s 2 Q/2 ∂ ∂s Σ(s, θ; τ ) = (-2) -k π π 0 dθ ∞ 0 ds s 2k e -s 2 Q/2 ∂ ∂s Σ(s, θ; τ )$for any Q ∈ (0, ∞). Note that for φ = tanh, φ = sech 2 , ∂ ∂s Σ(s, θ; τ ) = sin θ φ(s sin θ)φ(s sin(θ -τ )) + sin(θ -τ )φ(s sin θ) φ(s sin(θ -τ )).

We split the integral of ∂ k Wφ ∂Q k as follows:

$∂ k Wφ(q, cq) ∂Q k = (-2) -k π π 0 dθ ∞ 0 ds s 2k e -s 2 Q/2 sin θ φ(s sin θ)φ(s sin(θ -τ )) + (-2) -k π π 0 dθ ∞ 0 ds s 2k e -s 2 Q/2 sin(θ -τ )φ(s sin θ) φ(s sin(θ -τ ))$We show that for each piece, the limit as Q → 0 exists and is finite, for any k. This will prove the smooth extendability of Wφ to Q = 0. We will do this for the first piece; the second is similar.

For Q > 0, the integrand is absolutely integrable, so we may switch the integrals. We now try to bound the inner integral by an exponentially decreasing term e -sµ for some µ; clearly, by monotone convergence on the outer integral as Q → 0, this would show the limit of the integral exists and is finite.

Because φ is odd and φ is even, the inner integrand is negative on θ ∈ [0, τ ) and positive on θ ∈ (τ, π].

We will break up the inner integral as follows, for some fixed > 0 satisfying τ -> 0 independent of s (recall τ ∈ (0, π/2]). For the other part: is finite as Q → 0, by monotone convergence.

Independence of constant hidden in O((q ) -i ). The constant hidden is a function of the chosen above, which depend on τ , but only to the extent that it must satisfy τ -> 0. As long as we are interested in a set C of c that is bounded away from 1, the corresponding set of τ is bounded away from 0, so can be taken to be some number smaller than all of the corresponding τ .

Lemma C.21. Suppose φ is tanh-like. Then for c ∈ [0, 1],

Wφ(q, cq) ≤ 2 π arcsin(c),

and weakly increases to this upper bound as q → ∞. Furthermore,

• If c = 0 or 1, then equality holds regardless of q.

• If c ∈ (0, 1) is held constant, 2 π arcsin(c) -Wφ(q, cq) = Θ(q -1 ), where the hidden constants in Θ depend on c. But the constants can be made independent of c if c ∈ [ , 1 -] for some > 0.

Proof. The cases of c = 0 or 1 are obvious by the definition of W. So from here on we assume c ∈ (0, 1). Wtanh(0, q, q, q cos 1) 2 π arcsin(cos 1)

## Figure C.11:

We verify empirically that the subleading term in W tanh(q, cq) is linear in q -1 , for constant c. Indeed, observe that the curve of of W tanh intersects the y-axis at an angle.

Let τ := arccos c. By the first equation of Lemma C.16 and the assumption that φ is tanh-like, it is immediate that Wφ(q, cq) is nondecreasing in q. By dominated convergence, using the second equation of Lemma C.16, we get

$lim q→∞ Wφ(q, cq) = 1 π ∞ 0 r dre -r 2 /2 (π -2τ ) = π -2τ π = 2 π arcsin c.$Then the convergence rate is O(q -1 ) by Lemma C.20 and Taylor's theorem. Thus to show the convergence rate is Θ(q -1 ), it suffices to show that D := ∂Wφ(q,cq) ∂Q < 0. But this is apparent from the first equation of Lemma C.16: For τ ∈ (0, π/2),

$D = 1 π sin τ w ≥|w| dw dw Υ(w, w ; τ )(- 1 2 √ 2 Q -3/2 )$× [ φ( q/2(w + w ))φ( q/2(w -w))

+ φ( q/2(w + w )) φ( q/2(w -w))] < 0 since Υ is positive on the integration domain, and φ and φ are both positive for positive arguments, by the assumption of φ being tanh-like.

Independence of the constants in Θ(q -1 ) from c when c ∈ [ , 1 -]. By Lemma C.20, the upper constant can be made independent from c. Since D is monotonically decreasing in c (or monotonically increasing in τ ) and |D| is monotonically increasing in c (or monotonically decreasing in τ ), we have

$|D| > |D| c=$, which can be taken to be the lower constant in Θ(q -1 ).

## Fig. C

.11 verifies empirically that the subleading term in W tanh(q, cq) is linear in q -1 , for constant c.

Theorem B.4. Suppose φ is a tanh-like nonlinearity in an RRN. Assume e (0) < 1.

$• If σ w = 0, then γ (l) = lWφ(σ 2 b , σ 2 b ) + γ (0) = lVφ(σ 2 b ) + γ (0)$and λ (l) = σ 2 b , so that e (l) → 1 and 1 -e (l) = Θ(l -1 ). As a result, s (l) = p (l) (1 -e (l) ) = Θ(1).

• If σ w > 0, then γ (l) = Θ(l 2 π ), and e (l) → 0 like Θ(l 2 π -1 ). Thus s (l) = Θ(p (l) ) = Θ(l).

Proof. We have by Lemma C.21, γ = 2 π arcsin(λ/q) -Θ(q -1 ) + γ. -Θ(q -1 ) + γ.

We claim that γ (l) → ∞ as l → ∞. Otherwise, there is some C such that γ (l) ≤ C for all l. For large enough l, p (l) ≥ ωl for any ω < 1 and arcsin C σ 2 w p (l-1) +σ 2 b = Θ(1/l) by linearization of arcsin. Thus γ (l) = Θ(log l), but this contradicts our assumption that γ is bounded. This proves our claim.

Therefore, for large enough l,

$σ 2 w γ + σ 2 b σ 2 w p + σ 2 b = γ/p + Θ(l -1$).

## Fig. C

.12 shows 2 π arcsin x vs x. One sees that 1 is an unstable fixed point; if e < 1 -, then 2 π arcsin e < 1 --δ for some δ. Thus c drops monotonically until some threshold under which the linearization of arcsin, arcsin x = x + Θ(x 3 ), is applicable. So for large enough l,

$γ -γ = 2 π arcsin(γ/p + Θ(l -1 )) -Θ(l -1 ) = 2 π γ/p + O(l -1 )$As p (l) ∼ l by Thm B.2, this difference equation has solution γ = Ω(l • If σ w = 0, χ (m) = χ (l) for all l, m.

$• If σ w > 0, log(χ (m) /χ (l) ) = A( √ l - √ m) + B(log l -log m) + O(1)$where A = 4 Proof. The σ w = 0 case is obvious. We will assume σ w > 0 from here on.

Let 

$p (l) = b 0 l + b 1 l 1/2 + b 2 log l + O(1). Then for D = 2$$q -1/2 = σ -1 w b -1/2 0 l -1/2 (1 -b 1 b -1 0 2 -1 l -1/2 -b 2 b -1 0 2 -1 l -1 log l + O(l -1 )) V φ(q) = Dq -1/2 + Θ(q -3/2 ) = Dσ -1 w b -1/2 0 l -1/2 (1 -b 1 b -1 0 2 -1 l -1/2 -b 2 b -1 0 2 -1 l -1 log l + O(l -1 )) log(BV φ(q) + 1) = BDσ -1 w b -1/2 0 l -1/2 -(BDσ -1 w b -3/2 0 b 1 2 -1 + B 2 D 2 σ -2 w b -1 0 2 -1 )l -1 + Θ(l -3/2 log l) l r=1 log(BV φ(q (r) ) + 1) = 2BDσ -1 w b -1/2 0 l 1/2 -(BDσ -1 w b -3/2 0 b 1 2 -1 + B 2 D 2 σ -2 w b -1 0 2 -1 ) log l + O(1)$In our case, we have b

$0 = 1, b 1 = -2Cσ -1 w , b 2 = C 2 σ -2 w , B = σ 2 w , C = 2 π , which gives l r=1 log(BV φ(q (r) ) + 1) = 4 3 2 π σ w l 1/2 + ( 4 3π -σ 2 w 4 9π ) log l + O(1).$so that

$χ (m) /χ (l) = exp 4 3 2 π σ w ( √ l - √ m) + ( 4 3π -σ 2 w 4 9π )(log l -log m) + O(1)$Theorem B.7. Suppose φ = tanh. Then in an RRN

$• If σ w = 0, χ (l) b = χ (L) V φ(σ 2 b ) and χ (l) w = χ (L) V φ(σ 2 b )((l -1)Vφ(σ 2 b ) + p (0)$), where L is the last layer.

$• If σ w > 0, log(χ (m) b /χ (l) b ) = A( √ l - √ m) + B b (log l -log m) + O(1) log(χ (m) w /χ (l) w ) = A( √ l - √ m) + B w (log l -log m) + O(1)$where A = 4 Proof. The σ w = 0 case is obvious. We will assume σ w > 0 from here on.

As in the proof of Thm B.6,

$V φ(q) = Dσ -1 w b -1/2 0 l -1/2 + Θ(l -1 )$where D = 2 

$log(χ (m) w /χ (l) w ) = 4 3 2 π σ w ( √ l - √ m) + ( 4 3π + 1 2 -σ 2 w 4 9π )(log l -log m) + O(1)$C.6 Tanh: Full Residual Network C.6.1 Forward Dynamics Theorem B.9. Suppose φ is tanh-like. Assume the FRN architecture.

$• If σ w = 0, then p (l) = (σ 2 v Vφ(σ 2 b ) + σ 2 a )l + p (0) , and q (l) = σ 2 b . • If σ w > 0, then p (l) = b 0 l + b 1 l 1/2 + b 2 log l + O(1), where b 0 = σ 2 v + σ 2 a b 1 = -2Cσ 2 v σ -1 w σ 2 v + σ 2 a b 2 = -C 2 σ 4 v σ -2 w (σ 2 v + σ 2 a ) 2 and C = 2 π . Additionally, q (l) = σ 2 w b 0 l + σ 2 w b 1 l 1/2 + σ 2 w b 2 log l + O(1).$Proof. The σ w = 0 case is obvious. We will assume σ w > 0 from here on.

As in Thm B.2, p will have expansion p

$= b 0 l + b 1 l 1/2 + b 2 log l + O(1). Then, for C = 2 π , q -1/2 = σ -1 w b -1/2 0 l -1/2 (1 -b 1 b -1 0 2 -1 l -1/2 -b 2 b -1 0 2 -1 l -1 log l + O(l -1 )) l r=1 Vφ(q (r) ) = l r=1 1 -C(q (r) ) -1/2 + Θ((q (r) ) -3/2 ) = l -2Cσ -1 w b -1/2 0 l 1/2 + Cσ -1 w b 1 b -3/2 0 2 -1 log l + O(1) p (l) = σ 2 v l r=1 +σ 2 a l = (σ 2 v + σ 2 a )l -2Cσ 2 v σ -1 w b -1/2 0 l 1/2 + Cσ 2 v σ -1 w b 1 b -3/2 0 2 -1 log l + O(1)$which yields

$b 0 = σ 2 v + σ 2 a b 1 = -2Cσ 2 v σ -1 w b -1/2 0 = -2Cσ 2 v σ -1 w σ 2 v + σ 2 a b 2 = -C 2 σ 4 v σ -2 w (σ 2 v + σ 2 a ) 2 Lemma C.22. Suppose φ is tanh-like. Then γ ≤ σ 2 v 2 π arcsin (λ/q) + σ 2 a + γ,and$$σ 2 v 2 π arcsin (λ/q) + σ 2 a + γ -γ = Θ(q -1 ).$Proof. Similar to the proof of Lemma C.21.

Lemma C.23. Let u * ∈ [0, 1). Let f t : [0, 1) → [0, 1] be a continuous function for each t ∈ N, to each of which we associate two numbers 0 ≤ a t ≤ u * ≤ b t ≤ 1. Suppose for each t, f t (u) > u for all u ∈ [0, a t ) and f t (u) < u for all u ∈ (b t , 1). Assume that for each u, f t (u) -u → 0 as t → ∞ uniformly over u. If a t u * and b t u * , then for any u 0 ∈ [0, 1), the dynamics u t = f t (u t-1 ) has a limiting point. Furthermore, either u t → u * or u t eventually converges monotonically (decreasing or increasing) to a limit point.

Proof. Fix a u 0 ∈ [0, 1). If u t → u * then we are done. Otherwise, suppose there is a neighborhood [u * -, u * + ] such that for an infinite sequence t 1 , t 2 , . . ., u ti ∈ [u * -, u * + ]. WLOG assume u ti < u *for all i and (t i ) i is the sequence of all ts that satisfy this inequality. If (t i ) i contains {s : s ≥ N } for some N , then for some M > N , for every t > M , a t > u * -> u t . By assumption, u t is monotonic for all t > M but is bounded above. Thus u t has a fixed point û ≤ u *as desired. Now assume there are infinite is such that t i -1 = t i-1 (i.e. t i -1 is not part of the sequence (t i ) i ). We will show that this case is contradictory. Take T large enough such that a t > u * -/2 and |f t (u) -u| < /4 for all u and for all t ≥ T (T exists by premise). Let j be the smallest index such that t j > T and t j -1 = t j-1 . By the definition of j, u tj

$-1 ≥ u * -. If u tj -1 ≥ u * -/2, then by definition of T , u * -> u tj = f tj (u tj -1 ) > u tj -1 -/4 > u * -3 /4 > u * -, a contradiction. If u * -≤ u tj -1 ≤ u * -/2, then by the definition of T , u tj -1 ≤ a tj -1 so that u tj = f tj (u tj -1 ) > u tj -1 ≥ u * -, a contradiction.$The "furthermore" claim is clear from our proof above.

Theorem B.11. Assume φ = tanh in an FRN. Suppose e (0) < 1.

$• If σ w = 0, then λ (l) = σ 2 b and γ (l) = l(σ 2 v Wφ(σ 2 b , σ 2 b ) + σ 2 a ) + γ (0) = l(σ 2 v Vφ(σ 2 b ) + σ2$a )+γ (0) . Thus e (l) → 1 and 1-e (l) = Θ(l -1 ). As a result, s (l) = p (l) (1-e (l) ) = Θ(1).

• If σ w > 0, then e (l) converges to the unique fixed point e * = 1 determined by the equation

$e * = 1 σ 2 v + σ 2 a [σ 2 v 2 π arcsin (e * ) + σ 2 a ].$Furthermore, e (l) converges to e * polynomially: |e (l) -e * | is Θ(l -δ * ), where

$δ * := 1 - 2 π 1 1 -(e * ) 2 σ 2 v σ 2 v + σ 2 a ∈ [ 2 π -1,1 2 )$Since e * < 1, s (l) = Θ(p (l) ) = Θ(l).

Proof. The σ w = 0 case is obvious. We will assume σ w > 0 from here on.

If σ a = 0, then e * as defined above is 0, and e = γ p decreases as Θ(l 2 π -1 ) to 0, by the same reason as before.

So from now on suppose σ a > 0. We apply Lemma C.23 first to show that e converges. We have

$σ 2 v Wφ(q, cq) + σ 2 a = ep -ep = ep -ep + ep -ep = (e -e)p + e(p -p) = (p -p)[(e -e) p p -p + e] σ 2 v Wφ(q, cq) + σ 2 a σ 2 v Vφ(q) + σ 2 σ 2 v +σ 2 a -u + o(1) > 0 (resp.$< 0) on larger and larger intervals [0, a l ] ∩ J k (resp. [b l , 1) ∩ J k ). This proves all the preconditions for Lemma C.23, which yields that I k converges to a limit point. As this argument is independent of k, we have that for all e (0) ∈ [0, 1), e (l) converges. Now we solve for the limit point.

Suppose e has limit point e † (possibly different from e * described in the theorem); if we express γ (l) = (e † + (l) )p (l) , then As l → ∞, c ∼ e → e † , and Wφ(q, e † q) → 2 π arcsin(e † ), and Vφ(q) → 1. Additionally, p/(pp) = Θ(l) and = o(1) so that -= o(l -1 ). Then we have, taking limits l → ∞,

$σ 2 v Wφ(q, cq) + σ 2 a = γ -γ = (e † + )p -(e † + )p = e † (p -p) + p -p σ 2 v Wφ(q, cq) + σ 2 a σ 2 v Vφ(q) + σ 2 a = e † + + ( -) p p -p$$σ 2 v 2 π arcsin(e † ) + σ 2 a σ 2 v + σ 2 a = e † .$Since f l (as defined above) repels points away from 1, the only solution for e † when e (0) < 1 is e † = e * as specified in the theorem statement.

We defer the proof of the convergence rate to e * to Thm C.25.

Lemma C.24. Let e * be the stable fixed point determined by σ a and σ v . Then as long as

$σ v > 0, 2 π 1 1 -(e * ) 2 σ 2 v σ 2 v + σ 2 a ∈ ( 1 2 ,2 π ]$Proof. Write ρ := 1 -e * Substituting ρ into the expression in question, it follows that we want to show

$2 π (1 -e * 2 ) -1/2 (1 + ρ) -1 = 2 π (1 -e * 2 ) -1/2 1 -2 π arcsin e * 1 -e * -1 ∈ ( 1 2 , 2 π ] for e * ∈ [0, 1) (the endpoint at 1 is not included since σ v > 0. But this is 2 π (1 -e * ) 1/2 (1 + e * ) -1/2 (1 - 2 π arcsin e * ) -1 .$Set g(e * ) to be this expression. We could proceed by finding critical points, but a simple plot Fig. [C](#).13 shows that g is decreasing on [0, 1), with extremal values at the end points:

$g(e * ) ∈ [ lim e * →1$g(e * ), g(0)), for e * ∈ [0, 1).

Obviously g(0) = 2 π . For the limit, we note that arcsin e * has an asymptotic expansion π

$2 - √ 2(1 - e) 1/2 + Θ((1 -e) 3/2 ) at 1, so that (1 -e * ) 1/2 (1 -2 π arcsin e * ) -1 → π 2 √ 2$, and g(e * ) → 1 2 as e * → 1.

Theorem C.25. If e (0) < 1, then |e (l) -e * | is Ω(l -δ * -ε ) and O(l -δ * +ε ) for any ε > 0, where

$δ * := 1 - 2 π 1 1 -(e * ) 2 σ 2 v σ 2 v + σ 2 a ∈ [1 - 2 π ,1 2 )$,

where the bounds on the right follow from Lemma C.24. Proof. Define ω(q, c) = 2 π arcsin(c) -W tanh(q, cq). By Lemma C.21, for large enough l, c is close to e * bounded away from 0 or 1, so that ω(q, c) = Θ(q -1 ) with the constant hidden in Θ independent of c. Additionally, by Lemma C.5, 1 -V tanh(q) = Θ(q -1/2 ). Therefore,

$(e * + )p = σ 2 v ( 2 π arcsin(e * + ) -ω(q, c)) + σ 2 a + γ = σ 2 v 2 π [arcsin(e * ) + 1 -(e * ) 2 + Θ( 2 )] -Θ(l -1 ) + σ 2 a + γ = e * (σ 2 v + σ 2 a ) + (e * + )p + σ 2 v 2 π 1 -(e * ) 2 + Θ( 2 ) -Θ(l -1 ) e * (p -p -σ 2 v -σ 2 a ) = p -p + σ 2 v 2 π 1 -(e * ) 2 + Θ( 2 ) -Θ(l -1 ) e * σ 2 v (Vφ(q) -1) = p -p + σ 2 v 2 π 1 -(e * ) 2 + Θ( 2 ) -Θ(l -1 ) = 1 p (e * σ 2 v (1 -Vφ(q)) + Θ( 2 ) -Θ(l -1 ) + (p + σ 2 v 2 π 1 1 -(e * ) 2 )) = Θ(l -3/2 ) + (1 -δ (l) /l)$where

$δ (l) = l p (σ 2 v Vφ(q) + σ 2 a -σ 2 v 2 π 1 1 -(e * ) 2 ) + Θ( /l) = (1 + Θ(l -1/2 ))(σ 2 v (1 -Θ(l -1/2 )) + σ 2 a -σ 2 v 2 π 1 1 -(e * ) 2 )/(σ 2 v + σ 2 a ) + Θ( /l) = δ * + O(l -1/2 ),$where δ * := 1-

$2 π 1 √ 1-(e * ) 2 σ 2 v σ 2 v +σ 2 a$, which is positive by Lemma C.24. By taking the δ of Lemma C.11 to be δ * + ε or δ * -ε respectively for lower and upper bounding the dynamics of (l) , the solution (l) is Ω(l -δ * -ε ) and O(l -δ * +ε ) for any ε > 0 since 1 2 > δ * .

## C.6.2 Backward Dynamics

Theorem B.13. Assume φ = tanh in an FRN.

• If σ w = 0, χ (m) = χ (l) for all l, m. Proof. The σ w = 0 case is obvious. We will assume σ w > 0 from here on.

As in the proof of Thm B.6, log(χ (m) /χ (l) ) = 2BDσ Proof. Similar to Thm B.7.

## C.7 α-ReLU: Full Residual Network

The following can be checked readily Lemma B.15. If α > -1 2 , then Vψ α (q) = c α q α , where c α = 1 √ π 2 α-1 Γ α + 1 2 .

Since ψα = αψ α-1 , we have as a corollary, Lemma C.26. If α > 1 2 , then V ψα (q) = α 2 c α-1 q α-1 .

As a special case, when α = 1, c α = 1 2 . The following is a trivial computation, but useful for many simplifications. By the standard method of characteristic equation, we get that p (l) = A + CB l where A = -

$σ 2 a +σ 2 b σ 2 v σ 2 v σ 2 w , B = 1 + σ 2 v σ 2 w$2 , and C is a coefficient determined by initial conditions. Theorem C.29. Suppose α < 1. We have the following asymptotic expansion

$p (l) = K 1 l 1 1-α + R(l)$where the remainder term

$R(l) ∼      -K 2 l α 1-α log l if α > 1 2 (C -K 2 )l log l if α = 1 2 and K 2 = C C(1-α) 1-2α l if α < 1 2$where  Note that the integrand is symmetric in u and v. Thus, if V = {(u, v) : u, v ≥ 0 & v ≥ u}, then 2πc α J α (θ) = 2 csc θ V du dve -(u 2 +v 2 -2uv cos θ)/2 sin 2 θ u α v α . Now make the change of variables from V to {(p, q) : q ≥ 2 √ p}: p = uv dp = v du + u dv q = u + v dq = du + dv dp dq = (v -u) du dv du dv = (q 2 -4p) -1/2 dp dq so that we have

$K 1 = [σ 2 v σ 2α w c α (1 -α)] 1 1-α , K 2 = 1 2 [σ 2 v c α σ 2α w ] 1 1-α (1 -α)$$2πc α J α (θ) = 2 csc θ ∞ 0 dpe p(1+cos θ) csc 2 θ p α ∞ 2 √ p dqe -q 2 csc 2 θ (q 2 -4p) -1/2 .$The inner integral in q can be expressed in terms of K 0 by a change of variable x = q 2 /2 √ p: Proof. We will prove this claim for θ < 1, and by continuity this also proves the case θ = 1. As remarked above, K 0 (z) = K0 (z) + z -1 K0 (z). Thus -dx[cos θe x cos θ x α + αe x cos θ x α-1 ] K0

-dx[cos θe x cos θ x α-1 + (α -1)e x cos θ x α-2 ]K 0

Asymptotically, K 0 (z) ∼ π 2z e -z as z → ∞ and K 0 (z) ∼ -ln(z) as z 0, and K0 (z) ∼ -π 2z e -z as z → ∞ and K0 (z) ∼ -z -1 as z 0. Thus, as α > 1, K0 e x cos θ x α | ∞ 0 = -lim x→∞ π/2e -x(1-cos θ) x α-1 + lim

x 0 e x cos θ x α-1 = 0 K 0 e x cos θ x α-1 | ∞ 0 = -lim x→∞ π/2e -x(1-cos θ) x α-2 + lim

x 0 e x cos θ x α-1 ln x = 0 So L α (θ) = -cos θL α-1 (θ) -(α -1)L α-2 (θ) -dx[cos θe x cos θ x α + αe x cos θ x α-1 ] K0

Via another integration by parts, the integral on the right is cos θe x cos θ x α K 0 | ∞ 0 + αe x cos θ x α-1 K 0 | ∞ 0 -dx[cos 2 θe x cos θ x α + 2α cos θe x cos θ x α-1 + α(α -1)e x cos θ x α-2 ]K 0 = -[cos 2 θL α (θ) + 2α cos θL α-1 (θ) + α(α -1)L α-2 (θ)]

Now, Vφ(q)γ -1 p -1 = Θ(l -1 1-α -1 ). By using the dynamics of Lemma C.11 to upper and lower bound our dynamics, we have (l) = Ω(l -µ-), O(l -µ+ ) for any > 0, where µ = min((1υ)/(1 -α), 1/(1 -α)) = (1 -υ)/(1 -α).

## C.7.2 Backward Dynamics

Lemma C.37. Suppose random variable X ∼ N (0, σ 2 ), and Y = ψ -β (X) for some β > 0, where ψ α is α-ReLU. Then for ξ > 0, Y has density

$Pr[Y ∈ [ξ, ξ + dξ]] = 1 β √ 2πσ 2 ξ -1 β -1 e -ξ -2/β /2σ 2 .$At ξ = 0, Y has density given by a Dirac delta of mass 1 2 . Furthermore, Y has finite second moment iff β < 1 2 .

Proof. We have

$Pr[Y ∈ [ξ, ∞)] = Pr[X ∈ [0, ξ -1/β ]] = 1 √ 2πσ2$ξ -1/β 0 e -x 2 /2σ 2 dx.

Differentiating the RHS against ξ using Leibniz's rule, we get

$d Pr[Y ∈ [ξ, ∞)]/dξ = 1 √ 2πσ 2 e -ξ -2/β /2σ 2 d dξ ξ -1/β = -1 β √ 2πσ 2 ξ -1 β -1 e -ξ -2/β /2σ 2 .$Negating both sides gives the density f Y of Y for ξ > 0. For ξ = 0, observe that lim ξ→0 f Y (ξ) = 0 because, while ξ -1 β -1 blows up polynomially, e -ξ -2/β /2σ 2 blows up exponentially. Thus the contribution of Y 's mass at Y = 0 from X > 0 is 0. On the other hand, all X < 0 gets mapped to Y = 0, so f Y (0) = 1 2 δ 0 , where δ 0 is the Dirac delta. For the second assertion, observe that

$f Y (ξ) ∼ 1 β √ 2πσ 2 ξ -1 β -1 as ξ → ∞. Thus, ξ 2 f Y (ξ) is integrable iff 2 -1 β -1 < -1 ⇐⇒ β < 1 2 .$Theorem B.19. Suppose we have the nonlinearity ψ α in an FRN. Var( ψα (ζ) 2 ) diverges for any Gaussian variable ζ with mean 0 if α ≤ 3 4 but is finite if α > 3 4 .

Proof. Note that ψα ∝ ψ α-1 , so it suffices to show that Var(ψ α-1 (ζ) (1-α)(2α-1) , where the constants in Θ(1) do not depend on l or m.

Proof. If α = 1, then

$χ = χ(1 + 1 2 σ 2 v σ 2 w ).$So χ (l-m) /χ (l) = Θ(1)B m for B = 1 + 1 2 σ 2 v σ 2 w .

If 1 2 < α < 1, then χ/χ -1 is Proof. The proof is similar to that of Thm B.7.

$σ 2 v σ 2 w V φ(q) = σ 2 v σ 2 w α 2 c α-1 q α-1 = σ 2 v σ 2 w α 2 c α-1 (σ 2 w p) α-1 + Θ(p α-2 ) = σ 2 v σ 2α w α 2 c α-1 (K 1 l 1 1-α -K 2 l$![A.1).]()

![Figure 1: Our equations predict the relevant quantities very well in practice. These plots make the comparison between prediction and measurements for the full resnet with tanh activation, with σ 2 v = 1.5, σ 2 a = .5, σ 2 w = 1.69, σ 2 b = .49. Left-to-right: (a) p (l) and γ (l) against layer l for 200 layers. (b) e (l) = γ (l) /p (l) against l for 200 layers. Both (a) and (b) trace out curves for different initial conditions. (c) Different gradient quantities against l for 50 layers. From left to right the layer number l decreases, following the direction of backpropagation. Notice that the gradient increases in norm as l → 1. All three figures exhibit smooth curves, which are theoretical estimates, and irregular curves with shades around them, which indicate empirical means and standard deviations (both of which taken in regular scale, not log scale). (a) and (b) are made with 20 runs of resnets of width 1000. (c) is made with 25 runs of resnets of width 250.]()

![Figure 2: Left-to-right: (a) Plots of e * and δ * against σa/σv. (b)In log-log scale: the dashed line is l -δ * -1 , and the colored lines are e (l) -e (l-1) for different initial conditions e (0) . That they become parallel at about l = 400 on verifies that e (l) = Θ(l -δ * ).4 (c) In log-log scale: The dashed line is A √ l (A given in Thm B.13), and the colored lines are log(•(1) /• (l) ) for • = χ, χ b , χ w . That they all converge together starting around l = 1000 indicates that the approximation in Thm B.13 is very good for large l.]()

![σ w in the RRN case and A = in the FRN case (Thm B.6 and Thm B.13]()

![Fig. A.2.]()

![where R = 4); see Fig. B.8. These exponents are verified empirically in Fig. A.2.]()

![Figure 3: From left to right, top to bottom: (a) and (b): σ 2 w , L, and test set accuracy of a grid of tanh reduced (left) and full (right) resnets trained on MNIST. Color indicates performance, with ligher colors indicating higher accuracy on test set. Other than the values on the axes, we have fixed σ 2 b = σ 2 a = 1 2 and σ 2 v = 1. The white dotted lines are given by σ 2 w L = C, where C = 170 on the left and C = 145 on the right. We see that both dotted lines accurately predict the largest optimal σw for each depth L. (c) Varying the ratio σ 2 a /σ 2 v while fixing σv/ 1 + σ 2 a /σ 2 v , and thus fixing A, the leading constant of log χ (0) /χ (L) . (d) in log-log scale: Heatmap gives the test accuracies of ReLU FRN for varying σ 2 w and L. Curves give level sets for the log ratios log s (L) /s (0) ≈ log p (L) /p (0) ≈ log χ (0) /χ (L) = L log(1 + σ 2 v σ 2 w /2). (e) Red heatmap shows the test accuracies of a grid of α-ReLU FRN with varying α and L as shown, but with all σ•s fixed. The white dashed curve gives a typical contour line of L R = const, where R =]()

![Figure A.1: Empirical vs theoretical dynamics for p(l) , e(l) , and different gradient quantities for α-ReLU, with format similar to Fig.1. We refer to each figure on each row from left to right as (a), (b), and (c). Note that in the α = 1 case, figure (a) (p (l) and γ(l) for different initial values) has log scale y-axis and (a) and (b) have x-axis ranging from 1 to 50, while for other α, (a) has normal y-axis and (a) and (b) have x-axis ranging from 1 to 200. We do so because the norm of the activation vector in a typical ReLU resnet blows up into NaN at around layer 90, while this is not a problem for α < 1. Our theoretical predictions track the average of empirical values closely for forward quantities p (l) , γ(l) , and e (l) for all α, but variance is extremely large for e (l) at α = 1; it also predicts the average gradient norm accurately for α = 1 to α = .7 (despite the fact that we should not expect so for α ≤ .75 due to exploding variance (Thm B.19)), although variance is large for α = 1 at earlier layers (i.e. later layers w.r.t backpropagation). However it consistently and significantly overestimates the average gradient norm for α = .6 to α = .5, where the variance is so large that one standard deviation below the mean results in negative values. All plots are made with parameters σ 2 v = 1.5, σ 2 a = .5, σ 2 w = 1.69, σ 2 b = .49; only α is varied. All figures exhibit smooth curves, which are theoretical estimates, and irregular curves with shades around them, which indicate empirical means and standard deviations (both of which taken in regular scale, not log scale). For each α, figures (a) and (b) are made with 20 runs of resnets of width 1000. (c) is made with 25 runs of resnets of width 250.]()

![Figure A.1: Empirical vs theoretical dynamics for p(l) , e(l) , and different gradient quantities for α-ReLU, with format similar to Fig.1. We refer to each figure on each row from left to right as (a), (b), and (c). Note that in the α = 1 case, figure (a) (p (l) and γ(l) for different initial values) has log scale y-axis and (a) and (b) have x-axis ranging from 1 to 50, while for other α, (a) has normal y-axis and (a) and (b) have x-axis ranging from 1 to 200. We do so because the norm of the activation vector in a typical ReLU resnet blows up into NaN at around layer 90, while this is not a problem for α < 1. Our theoretical predictions track the average of empirical values closely for forward quantities p (l) , γ(l) , and e (l) for all α, but variance is extremely large for e (l) at α = 1; it also predicts the average gradient norm accurately for α = 1 to α = .7 (despite the fact that we should not expect so for α ≤ .75 due to exploding variance (Thm B.19)), although variance is large for α = 1 at earlier layers (i.e. later layers w.r.t backpropagation). However it consistently and significantly overestimates the average gradient norm for α = .6 to α = .5, where the variance is so large that one standard deviation below the mean results in negative values. All plots are made with parameters σ 2 v = 1.5, σ 2 a = .5, σ 2 w = 1.69, σ 2 b = .49; only α is varied. All figures exhibit smooth curves, which are theoretical estimates, and irregular curves with shades around them, which indicate empirical means and standard deviations (both of which taken in regular scale, not log scale). For each α, figures (a) and (b) are made with 20 runs of resnets of width 1000. (c) is made with 25 runs of resnets of width 250.]()

![= 0: 01 e(0) = 0: 21 e(0) = 0: 40 e(0) = 0: 60 e(0) = 0: 79 e(0) = 0: 99]()

![Figure A.2: We verify the exponents of the forward and backward dynamics for α-ReLU FRN. For each row, the figures are labeled (a) and (b) from left to right. The format is the same as in Fig. C.17. All figures are in log-log scale. (a) We exhibit our theoretical dynamics of the cosine distance e (l) based on the recurrences Thm B.8 and Thm B.10 for different initial conditions e(0) . We draw |e (l) -e (l-1) | for each of these dynamics in colored solid lines. We predict that each dynamic is Θ(l -µ ), where µ = (1 -Jα(e * ))/(1 -α), and the dashed line gives l -µ-1 (Thm B.18), shifted vertically to better compare the slope in log scale (i.e. the exponent of the polynomial dynamics). (See footnote 4 for why we plot the dynamics this way). We see that the our asymptotic prediction is very accurate for the sequence of e (l) that starts with e (0) = 0.99, the closest to e * for each α, while other lines only slowly converge to the same exponent (which is the slope in the log-log plot). This is to be expected based on the proof of Thm B.18. For α = .9, the e (0) = .99 line upticks at around 10 3 and then turn into NaNs due to numerical instability. (b) Colored lines are • (0) /• (l) for • = χ, χ b , χ w (we are not taking logs in addition to plotting in log-log scale like in Fig.C.15). The dashed lines are our asymptotic predictions for the dynamics with corresponding colors, based on Thm B.21, again shifted appropriately to easily compare slope visually. We see that for every alpha our asymptotic predictions are highly accurate. For both (a) and (b), we did not show α = 1 case as ReLU FRN runs into numerical issues quickly (i.e. with even for 100 layers) because of exponential explosions in p (l) and χ (l) as predicted by Thms B.16 and B.20, so we cannot expect to empirically verify the precise predicted asymptotics. All plots are made with parameters σ 2 v = 1.5, σ 2 a = .5, σ 2 w = 1.69, σ 2 b = .49; only α is varied.]()

![Suppose φ is antisymmetric. Then in an RRN, p and q satisfy the recurrence q = σ 2 w p + σ 2 b p = Vφ(q) + p.]()

![]()

![σ w (same as A in Thm B.6) and B b = B + 1 2 , B w = B -1 2 , with B = (same as B in Thm B.6). B.1.2 Full Residual Network Theorem B.8. For any nonlinearity φ in an FRN,]()

![Figure B.3: Empirical verification of Thm B.9.]()

![Fig. B.4 shows empirical verification of the asymptotic expansion of χ for various values of σ • s.]()

![Figure B.4: Empirical verification of the asymptotic expansion of χ for various values of σ•s. Note that we have chosen all small values for σ•s. For larger values, the constant term in Thm B.13 begins to dominate (primarily because of the expansion log(1 + x) = x + Θ(x 2 ) has large Θ term when x is large), and χ behaves more like exp(Θ(l)) up to depth 1000.]()

![Fig. B.5 empirically verifies the asymptotics for α = 1 for various σ v and σ w .]()

![Figure B.5: Verification of the exponential asymptotics of p (l) when α = 1. The lines of each color correspond to different (σw, σv) pairs, which are given in the legend. The solid lines are given by the recurrences Thm B.8, and the dashed lines are given by our asymptotics (1 + σ 2 v σ 2 w /2) l (Thm B.16). Note that the y-axis is in log-scale.]()

![Fig. B.6 verifies empirically that e * is indeed the fixed point of e (l) . Fig. A.2 verifies empirically the convergence rate l -µ . Fig. B.7 plots Jα (e * ) and µ versus α. It certainly looks like µ = 12 (1 -α), but we have no proof for it. Based on this conjecture, we see there is a "discontinuity" of µ at α = 1: µ → 0 as α → 1, but for α = 1, the actual convergence dynamics has exponent -2 by Thm B.17.]()

![Figure B.7: (a) A plot of Jα(e * ) versus α. (b) A plot of the exponent µ of the dynamics of |e * -e (l) | (see Thm B.18)]()

![Fig. A.2 verifies the backward asymptotic dynamics empirically for different α < 1. Fig. B.8(b) graphs the exponent α1-α -R in terms of α. We see that on [0.5, 1], the maximum of this exponent is at α = 1.]()

![Fig. C.9 demonstrates Lemma C.7. Lemma C.8. Let d ∈ R and 1 < M < N with N -M ∈ Z ≥0 . Set Σ(M, N, d) := N a=M a d . If we fix M and let N → ∞,]()

![Figure C.9: Illustration of Lemma C.7: V φ(q) vs 1 √ 4q+1 for φ = tanh. This bound is very tight, and for most purposes, 1 √ 4q+1 can be taken as a good approximation of V φ(q).]()

![α , which gives the result.]()

![For any nonlinearity φ in an RRN, under assumptions Axiom 3.1 and Axiom 3.2, whenever φ2 (ζ) has finite variance for Gaussian variable ζ,]()

![Fig. C.10 displays a contour plot of K for typical values of q and c. So Γ = 1 π √ 1 -c 2 w ≥|w| dw dw K(w, w ).]()

![Figure C.10: The integrand of Γ after symmetrization. Here c = .2 and q = 100 and φ = tanh.]()

![e -s 2 Q/2 sin θ φ(s sin θ)φ(s sin(θ -τ )) θ φ(s sin θ)φ(s sin(θ -τ ))]()

![sin θ φ(s sin θ)φ(s sin(θ -τ ))sin θ φ(s sin θ)φ(s sin(θ -τ )) + πdθ sin θ φ(s sin θ)φ(s sin(θ -τ )) Now because φ(z) = sech 2 (z) ≤ 2e -z , and sin θ ≥ sin on θ ∈ [ , π -], πdθ sin θ φ(s sin θ)φ(s sin(θ -τ ))π -2 ) exp(-s sin ).]()

![dθ sin θ φ(s sin θ)φ(s sin(θ -τ )) = 0 sin(π -θ) φ(s sin π -θ)φ(s sin(π -θ -τ )) d(π -θ) = 0 dθ sin θ φ(s sin θ)φ(s sin θ + τ ) sin θ φ(s sin θ)φ(s sin(θ -τ )) = 0 dθ sin θ φ(s sin θ)[φ(s sin(τ + θ)) -φ(s sin(τ -θ))] But by intermediate value theorem, φ(s sin(τ +θ))-φ(s sin(τ -θ)) = 2θ∂φ(s sin(τ +θ))/∂θ| θ=ψ = 2θ φ(s sin(τ + ψ))s cos(τ + ψ) for some ψ ∈ [-θ, θ]. By the assumption on , φ(s sin(τ + θ))φ(s sin(τ -θ)) ≤ 2 φ(s sin(τ -))s cos(τ -). Then 0 dθ sin θ φ(s sin θ)[φ(s sin(τ + θ)) -φ(s sin(τ -θ))] ≤ 0 dθ sin θ φ(s sin θ)2 φ(s sin(τ -))s cos(τ -) ≤ 2 φ(s sin(τ -))s cos(τ -)O(1) Because τ -> 0 by assumption on , and because φ(z) = exp(-Θ + (z)), this quantity is exp(-Θ + (z)), as desired (here Θ + denotes a positive quantity). Thus π 0 dθ sin θ φ(s sin θ)φ(s sin(θ -τ )) sin θ φ(s sin θ)φ(s sin(θ -τ )) + πdθ sin θ φ(s sin θ)φ(s sin(θ -τ )) = exp(-Θ + (s)) and similarly for the other piece of ∂ k Wφ ∂Q k , so that ∞ 0 dss 2k e -s 2 Q/2 π 0 dθ sin θ φ(s sin θ)φ(s sin(θ -τ )) = ∞ Θ+(z) → ∞ 0 dss 2k e -Θ+(z)]()

![by Thm B.2, and λ = σ 2 w γ + σ 2 b by Thm B.3, γ]()

![+ ) for any by using the dynamics of Lemma C.11 to upper and lower bound this difference equation.C.5.2 Backward DynamicsTheorem B.6. For φ = tanh in an RRN,]()

![σ w and B = 4 3π -]()

![, we have (implicitly applying Lemma C.4 and Lemma C.8),]()

![σ w (same as A in Thm B.6) and B b = B + 1 2 , B w = B -1 2 , with B = (same as B in Thm B.6).]()

![. Thus by Thm B.5, log(χ (m) /χ (l) l -log m) + O(1) l -log m) + O(1) Similarly, since p = l + Θ( √ l) by Thm B.2, we have]()

![Figure C.12: Graph of y(e) = 1 σ 2 v +σ 2 a]()

![By definition of e * , we get e * = (1 -ρ) 2 π arcsin e * + ρ ρ = = e * -2 π arcsin e *]()

![Figure C.13: Plot of g(e * ) in the proof of Lemma C.24]()

![If σ w > 0, then for l ≥ m ≥ 0, log(χ (m) /χ (l) ) = A( √ l -√ m) + B(log l -log m) + O(1)]()

![B 2 D 2 σ -2 w b -1 0 2 -1 )(log l -log m) + O(1)with C = 2 π . This simplifies to the desired form.Theorem B.14. Suppose φ = tanh in an FRN.• If σ w = 0, then χ (l) b = σ 2 v χ (L) V φ(σ 2 b ) χ (l) w = σ 2 v χ (L) V φ(σ 2 b )((σ 2 v Vφ(σ 2 b ) + σ 2 a )(l -1) + p (0) ) χ (l) v = χ (L) Vφ(σ 2 b ) χ (l) a = χ (L) . • If σ w > 0, then for l ≥ m ≥ 0, B b (log l -log m) + O(1) log(χ (m) w /χ (l) w ) = A( √ l -√ m) + B w (log l -log m) + O(1) log(χ (m) a /χ (l) a ) = A( √ l -√ m) + B(log l -log m) + O(1) log(χ (m) v /χ (l) v ) = A( √ l -√ m) + B(log l -log m) + O(1)are as in Thm B.13 and B b = B + 1 2 and B w = B -1 2 .]()

![Figure C.14: Verification of leading term of Thm C.28 for α = 0.55.]()

![Lemma C.27. c α+1 /c α = 2α + 1.C.7.1 Forward DynamicsTheorem C.28. Suppose we have the nonlinearity φ = ψ 1 . Then p (l) = Θ((1 + σ 2 v σ 2 w /2) l ), with the hidden constant depending on the initial condition.]()

![α -1 α and C = σ 2 a .]()

![Fig. C.14 verifies the leading coefficient and the exponent of the leading term. Proof. The difference equation governing the evolution of p is pp = A(p + B) α + C where A = σ 2 v c α σ 2α w , B = σ 2 b /σ 2 w , and C = σ 2 a . Then Lemma C.15 yields the result. Thm C.29 combined with Thm C.28 gives the following result.]()

![-p csc 2 θ K 0 (p csc 2 θ) = csc θ ∞ 0 dpK 0 (p csc 2 θ)e p cos θ csc 2 θ p α = sin 2α+1 θ ∞ 0 dxK 0 (x)e x cos θ x α Define L α (θ) = 2πc α J α (θ) csc 2α+1 θ = ∞ 0 dxK 0 (x)e x cos θ x α . Lemma C.32. If α > 1, then L α (θ) = csc 2 θ[(2α -1) cos θL α-1 (θ) + (α -1) 2 L α-2 (θ)].]()

![(x) + x -1 K0 (x))e x cos θ x α = K0 e x cos θ x α | ∞ 0 + K 0 e x cos θ x α-1 | ∞ 0]()

![2 ) = Var(ψ 2α-2 (ζ)) is infinite for ζ ∼ N (0, σ 2 ). By Lemma C.37 with β = 2 -2α, ψ 2α-2 (ζ) has finite variance iff β < Theorem B.20. Suppose we have the nonlinearity ψ α in an FRN. If α = 1, then χ (l-m) = χ (l) 1 2 . If α ∈ ( 3 4 , 1), then χ (l-m) = Θ(1)χ (l) (l/(l -m)) R for R = α 2]()

![α log l + o(l α 1-α log l)) α-1 + Θ(l α--α ) by Thm C.29 = σ 2 v σ 2α w α 2 c α-1 [K α-1 -1 + Θ(l -2 log l)] + O(l --1 + Θ(l -2 log l) = Rl -1 + Θ(l -2 log l) where R = σ 2 v σ 2α w α 2 c α-1 K α-1 -α)(2α-1) and K 1 = [σ 2 v σ 2α w c α (1 -α)] -α . So χ = χ exp(Rl -1 + Θ(l -2 log l)) χ (l-m) = Θ(1)χ (l) l l -m R as desired. Theorem B.21. If φ = ψ 1 in an FRN, then for l ≥ m ≥ 0, χ (l-m) b = Θ(1)χ (l) B m , χ (l-m) w = Θ(1)χ (l) B l , χ (l-m) v = Θ(1)χ (l) B l , χ (l-m) a = Θ(1)χ (l) B m .whereB = 1 + If φ = ψ α in an FRN, for α < 1, then for l ≥ m ≥ 0, χ (l-m) b = Θ(1)χ (l) l R (l -m) -R-1 , χ (l-m) w = Θ(1)χ (l) l R (l -m) α 1-α -R , χ (l-m) v = Θ(1)χ (l) l R (l -m)α 1-α -R , χ (l-m) a = Θ(1)χ (l) (l/(l -m)) R .]()

![Main Recurrences]()

![Summary of Main Dynamics Results. Note that while χ (l) is exponential for ReLU/FRN, the gradients with respect to weight parameters have norms (χ w and χ v ) constant in l (Thm B.21).]()

Note that in practice, to avoid the diverging gradient ψα(x) → ∞ as x → 0, we can use a tempered version Ψα(x) of α-ReLU, defined by Ψα(x) = (x + ) αα on x > 0 and 0 otherwise, for some small > 0. The conclusions of this paper on ψα should hold similarly for Ψα as well.

Daniely et al.[[3]](#b2) called the version of Wφ with fixed ρ = 1 the "dual function" of φ.

A more natural visualization is to graph e (l) -e * versus l -δ * , but because of floating point precision, e (l) -e * doesn't converge to 0, but a small number close to 0, so that the log-log plot wouldn't look like what is expected.

Our derivations actually apply to all α ∈ ( 1 2 , 1], where at α = 1 2 , the expected norm of the gradient diverges within our mean field formalism. However, at α ≤[3](#b2) 4 , the variance of the gradient already diverges (Thm B.19), so we cannot expect the empirical values to agree with our theoretical predictions. But in fact, empirically our theoretical predictions seem to form an upper bound on the gradient norms (see Fig. A.1).

the contour for p (l) is similar, but its slopes are slightly off from the heatmap contours.

