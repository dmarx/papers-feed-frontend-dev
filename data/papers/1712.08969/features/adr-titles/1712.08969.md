- Decision on the choice of activation functions (e.g., tanh, ReLU, Î±-ReLU)
- Decision to focus on residual networks instead of vanilla feedforward networks
- Decision to use mean field theory for analysis
- Decision to analyze the dynamics of randomly initialized networks
- Decision to exclude batch normalization and convolutional layers from the study
- Decision on the initialization schemes (e.g., Xavier, He) and their applicability to residual networks
- Decision to track gradient explosion and expressivity as metrics for initialization quality
- Decision to use polynomial rates for convergence analysis
- Decision to simplify the model for theoretical tractability
- Decision to assume symmetry of activations and gradients
- Decision to assume gradient independence for backpropagation
- Decision to focus on the average behavior of the networks
- Decision to derive new identities for kernels of powers of ReLU functions
- Decision to conduct experiments on the MNIST dataset
- Decision to use specific hyperparameters for network initialization
- Decision to validate theoretical predictions with empirical results
- Decision to structure the paper with formal statements and proofs in the appendix
- Decision to define central quantities for analysis (e.g., length and correlation quantities)
- Decision to use large width limit for expectations in definitions
- Decision to emphasize the edge of chaos hypothesis in the context of residual networks