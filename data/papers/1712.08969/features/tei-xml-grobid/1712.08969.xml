<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mean Field Residual Networks: On the Edge of Chaos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2017-12-24">24 Dec 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Greg</forename><surname>Yang</surname></persName>
							<email>gregyang@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">31st Conference on Neural Information Processing Systems (NIPS 2017)</orgName>
								<orgName type="institution" key="instit1">Microsoft Research AI</orgName>
								<orgName type="institution" key="instit2">Harvard University</orgName>
								<address>
									<settlement>Long Beach</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">31st Conference on Neural Information Processing Systems (NIPS 2017)</orgName>
								<orgName type="institution" key="instit1">Microsoft Research AI</orgName>
								<orgName type="institution" key="instit2">Harvard University</orgName>
								<address>
									<settlement>Long Beach</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Mean Field Residual Networks: On the Edge of Chaos</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-12-24">24 Dec 2017</date>
						</imprint>
					</monogr>
					<idno type="MD5">8C2A7E50A6F836023026019589A0DC95</idno>
					<idno type="arXiv">arXiv:1712.08969v1[cs.NE]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study randomly initialized residual networks using mean field theory and the theory of difference equations. Classical feedforward neural networks, such as those with tanh activations, exhibit exponential behavior on the average when propagating inputs forward or gradients backward. The exponential forward dynamics causes rapid collapsing of the input space geometry, while the exponential backward dynamics causes drastic vanishing or exploding gradients. We show, in contrast, that by adding skip connections, the network will, depending on the nonlinearity, adopt subexponential forward and backward dynamics, and in many cases in fact polynomial. The exponents of these polynomials are obtained through analytic methods and proved and verified empirically to be correct. In terms of the "edge of chaos" hypothesis, these subexponential and polynomial laws allow residual networks to "hover over the boundary between stability and chaos," thus preserving the geometry of the input space and the gradient information flow. In our experiments, for each activation function we study here, we initialize residual networks with different hyperparameters and train them on MNIST. Remarkably, our initialization time theory can accurately predict test time performance of these networks, by tracking either the expected amount of gradient explosion or the expected squared distance between the images of two input vectors. Importantly, we show, theoretically as well as empirically, that common initializations such as the Xavier or the He schemes are not optimal for residual networks, because the optimal initialization variances depend on the depth. Finally, we have made mathematical contributions by deriving several new identities for the kernels of powers of ReLU functions by relating them to the zeroth Bessel function of the second kind.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Previous works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b10">11]</ref> have shown that randomly initialized neural networks exhibit a spectrum of behavior with depth, from stable to chaotic, which depends on the variance of the initializations: the cosine distance of two input vectors converges exponentially fast with depth to a fixed point in [0, 1]; if this fixed point is 1, then the behavior is stable; if this fixed point is 0, then the behavior is chaotic. It has been argued in many prior works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9]</ref> that effective computation can only be supported by a dynamical behavior that is on the edge of chaos. Too much stability prevents the neural network from telling apart two different inputs. While some chaotic behavior can increase the expressivity of a network, too much chaos makes the neural network think two similar inputs are very different. At the same time, the same initialization variances also control how far gradient information can be propagated through the network; the networks with chaotic forward dynamics will tend to suffer from exploding gradients, while networks with stable forward dynamics will tend to suffer from vanishing gradients.</p><p>These works have focused on vanilla (fully connected) feedforward networks. Here we consider residual networks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> (with fully-connected layers and without batchnorm), which are a family of recently proposed neural network architectures that has achieved state-of-the-art performance on image recognition tasks, beating all other approaches by a large margin. The main innovation of this family of architectures is the addition of a passthrough (identity) connection from the previous layer to the next, such that the usual nonlinearity computes the "residual" between the next-layer activation and the previous-layer activation.</p><p>In this work, we seek to characterize randomly initialized residual networks. One of our main results is that random residual networks for many nonlinearities such as tanh live on the edge of chaos, in that the cosine distance of two input vectors will converge to a fixed point at a polynomial rate, rather than an exponential rate, as with vanilla tanh networks. Thus a typical residual network will slowly cross the stable-chaotic boundary with depth, hovering around this boundary for many layers. In addition, for most of the nonlinearities considered here, the mean field estimate of the gradient grows subexponentially with depth. In fact, for α-ReLU, the αth-power of ReLU, for α &lt; 1, the gradient grows only polynomially. These theoretical results provide some theoretical justification for why residual networks work so well in practice. In our experiments, we are also able to predict surprisingly well the relative performances of trained residual networks based only on their initialization hyperparameters, in a variety of settings. In particular, we find that the quality of initialization for tanh resnets is determined by trainability (how much gradient explosion on average) while that for (α-)ReLU resnets is determined by expressivity (how far can two different input vectors be pulled apart) (see <ref type="bibr">Section 6)</ref>. To the best of our knowledge, this is the first time that a quantity other than gradient explosion/vanishing has been found to control the quality of initialization. We establish theoretically and empirically that the best initialization variances for residual networks depend on the depth of the network (contrary to the feedforward case <ref type="bibr" target="#b10">[11]</ref>), so that common initialization schemes like Xavier <ref type="bibr" target="#b3">[4]</ref> or He <ref type="bibr" target="#b4">[5]</ref> cannot be optimal. In fact, even the rationale of He initialization is incorrect for ReLU residual networks because it tries to control gradient dynamics rather than expressivity. However we want to emphasize that we study a simplified model of residual networks in this work, with no batchnorm or convolutional layers, so that these results are not necessarily indicative of the MSRA residual network used in practice <ref type="bibr" target="#b5">[6]</ref>.</p><p>In the body of this paper, we give account of general intuition and/or proof strategy when appropriate for our theoretical results, but we relegate all formal statements and proofs to the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Consider a vanilla feedforward neural network of L layers, with each layer l having N (l) neurons; here layer 0 is the input layer. For the ease of presentation we assume all hidden layer widths are the same N (l) = N for all l &gt; 0. Let x (0) = (x (0) 1 , . . . , x (0) N (0) ) be the input vector to the network, and let x (l) for l &gt; 0 be the activation of layer l. Then a neural network is given by the equations</p><formula xml:id="formula_0">x (l) i = φ(h (l) i ), h (l) i = N j=1 w (l) ij x (l-1) j + b (l) i</formula><p>where (i) h (l) is the pre-activation at layer l, (ii) w (l) is the weight matrix, (iii) b (l) is the bias vector, and (iv) φ is a nonlinearity, for example tanh or ReLU, which is applied coordinatewise to its input.</p><p>To lighten up notation, we suppress the explicit layer numbers l and write</p><formula xml:id="formula_1">x i = φ(h i ), h i = j w ij x j + b i</formula><p>where • implicitly denotes • (l) , and • denotes • (l-1) (and analogously, • denotes • (l+1) ).</p><p>A series of papers <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11</ref>] investigated the "average behavior" of random neural networks sampled via w</p><formula xml:id="formula_2">(l) ij ∼ N (0, σ 2 w /N ), b<label>(l)</label></formula><p>i ∼ N (0, σ 2 b ), for fixed parameters σ w and σ b , independent of l. Consider the expectation of 1 N N i=1 x 2 i , the normalized squared length of x, over the sampling of w and b. Poole et al. <ref type="bibr" target="#b8">[9]</ref> showed that this quantity converges to a fixed point exponentially fast for sigmoid nonlinearities. Now suppose we propagate two different vectors x (0) and (x (0) ) through the network. Poole et al. <ref type="bibr" target="#b8">[9]</ref> also showed that the expectation of the normalized dot product 1 N N i=1 x i x i converges exponentially fast to a fixed point. The ratio between the normalized squared length and the normalized dot product is the cosine distance between x and x . Thus these two exponential convergence results show that the cosine distance converges exponentially fast to a fixed point as well. Intuitively, this means that a vanilla feedforward network "forgets" the geometry of the input space "very quickly," after only a few layers.</p><p>In addition, Schoenholz et al. <ref type="bibr" target="#b10">[11]</ref>, under certain independence assumptions, showed that the expected normalized squared norm of the gradient also vanishes or explodes in an exponential fashion with depth, with the "half-life" controlled by σ w and σ b . They verified that this theoretical "half-life" correlates in practice with the maximal number of layers that are admissible to good performance.</p><p>At the same time, Daniely et al. <ref type="bibr" target="#b2">[3]</ref> published work of similar nature, but phrased in the language of reproducing kernel Hilbert spaces, and provided high probability estimates that are meaningful for the case when the width N is finite and the depth is logarithmic in N . However, they essentially fixed the variance parameters σ • , and furthermore, their framework (for example the notion of a "skeleton") does not immediately generalize to the residual network case.</p><p>In this work, we show that residual networks have very different dynamics from vanilla feedforward networks. In most cases, the cosine distance convergence rate and the gradient growth rate are subexponential in a residual network, and in most cases, these rates may be polynomial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries</head><p>Residual networks were first introduced by <ref type="bibr" target="#b5">[6]</ref> and later refined by <ref type="bibr" target="#b6">[7]</ref>, and they are now commonplace among deployed neural systems. The key innovation there is the addition of a shortcut connection from the previous layer to the next. We define the following idealized architectures for ease of analysis. Note that we only consider fully-connected affine layers instead of convolutional layers. A reduced residual network (RRN) has the recurrence</p><formula xml:id="formula_3">x i = φ(h i ) + x, h i = j w ij x j + b i .</formula><p>A (full) residual network (FRN) in addition has an affine connection given by weights v and biases a from the nonlinearity φ(h) to the next layer:</p><formula xml:id="formula_4">x i = j v ij φ(h j ) + x i + a i , h i = j w ij x j + b i</formula><p>We are interested in the "average behavior" of these network when the weights and biases, w</p><formula xml:id="formula_5">(l) ij , b<label>(l) i , v (l) ij , and a (l)</label></formula><p>i are sampled i.i.d. from Gaussian distributions resp. with standard deviations σ w , σ b , σ v , and σ a , independent from l. Here we take the variance of w (l) ij to be σ 2 w /N so that the variance of each h i is σ 2 w , assuming each x j is fixed (similarity for v (l) ij ). Such an initialization scheme is standard in practice.</p><p>We make several key "physical assumptions" to make theoretical computations tractable: Axiom 3.1 (Symmetry of activations and gradients). (a) We assume (h 2 for any i, j, l. (b) We also assume that the gradient ∂E/∂x (l) i with respect to the loss function E satisfies (∂E/∂x (l) i ) 2 = (∂E/∂x (l) j ) 2 for any i, j, l.</p><formula xml:id="formula_6">(l) i ) 2 = (h (l) j ) 2 and (x (0) i ) 2 = (x (0) j )</formula><p>One can see that Axiom 3.1(a) is satisfied if the input x (0) ∈ {±1} N and Axiom 3.1(b) is satisfied if Axiom 3.2 below is true and the gradient at the last layer ∂E/∂xL ∈ {±1} N . But in general it is justified both empirically and theoretically as an approximation, because (h 2 grow rather quickly at the same pace with l (as will be seen later in calculations), so that their additive difference becomes negligible; similarly for (x</p><formula xml:id="formula_7">(l) i ) 2 -(h (l) j ) 2 stays about constant with l, but (h (l) i ) 2 and (h (l) j )</formula><formula xml:id="formula_8">(l) i ) 2 and (∂E/∂h (l) i ) 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Axiom 3.2 (Gradient independence). (a)</head><p>We assume the we use a different set of weights for backpropagation than those used to compute the network outputs, but sampled i.i.d. from the same distributions. (b) For any loss function E, we assume that the gradient at layer l, ∂E/∂x (l) i , is independent from all activations h (l) j and x (l-1) j from the previous layer. Axiom 3.2(a) was first made in <ref type="bibr" target="#b10">[11]</ref> for computing the mean field theory of gradients for feedforward tanh networks. This is similar to the practice of feedback alignment <ref type="bibr" target="#b7">[8]</ref>. Even though we are the first to explicitly formulate Axiom 3.2(b), in fact it was already applied implicitly in the gradient calculations of <ref type="bibr" target="#b10">[11]</ref>. Note that a priori Axiom 3.2(b) is not true, as ∂E/∂x (l) i depends on φ(h (l+1) k ) for every k, which depend on h (l) j for each j, and which depends on x (l-1) k for every k. Nevertheless, in practice both subassumptions hold very well. Now we define the central quantities studied in this paper. Inevitably, our paper involves a large amount of notation that may be confusing for the first-time reader. We have included a glossary of symbols (Table <ref type="table">A</ref>.1) to ameliorate notation confusion. Definition 3.3. Fix an input x (0) . Define the length quantities q (l) := (h (l) 1 ) 2 and p (l) := (x (l) 1 ) 2 for l &gt; 0 and p (0) = x (0) 2 /N . Here the expectations • are taken over all random initialization of weights and biases for all layers l, as N → ∞ (large width limit).</p><p>Note that in our definition, the index 1 does not matter by Axiom 3.1. Definition 3.4. Fix two inputs x (0) and x (0) . We write • to denote a quantity • with respect to the input x (0) . Then define the correlation quantities γ (l) := h</p><formula xml:id="formula_9">(l) 1 h (l) 1 and λ (l) := x (l) 1 x (l) 1</formula><p>for l &gt; 0 and γ (0) = x (0) • x (0) /N , where the expectations • are taken over all random initialization of weights and biases for all layers l, as N → ∞ (large width limit). Again, here the index 1 does not matter by Axiom 3.1. By metric expressivity, we mean s (l) := l) . Additionally, define the cosine distance quantities e (l) := γ (l) / p (l) p (l) and c (l) := λ (l) / q (l) q (l) , and we will also call e (l) angular expressivity.</p><formula xml:id="formula_10">1 2N x (l) -x (l) 2 = 1 2N ( x (l) 2 + x (l) 2 -2 x (l) • x (l) ) = 1 2 (p (l) + p (l) ) -γ (</formula><p>In this paper, for the ease of presentation, we assume p (0) = p (0) . Then, as we will see, p (l) = p (l) , q (l) = q (l) for all l, and as a result, e (l) = γ (l) /p (l) and s (l) = p (l) -γ (l) = (1 -e (l) )p (l) . Definition 3.5. Fix an input x (0) and a gradient vector (∂E/∂x (L) i ) i of some loss function E with respect to the last layer x (L) . Then define the gradient quantities χ (l) := (∂E/∂x</p><formula xml:id="formula_11">(l) 1 ) 2 , χ (l) • := (∂E/∂• (l) 1 ) 2 for • = a, b, and χ (l) • := (∂E/∂• (l) 11 ) 2 for • = w, v.</formula><p>Here the expectations are taken with Axiom 3.2 in mind, over both random initialization of forward and backward weights and biases, as N → ∞ (large width limit). Again, the index 1 or 11 does not matter by Axiom 3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Asymptotic notations.</head><p>The expressions f = O(g) ⇐⇒ g = Ω(f ) have their typical meanings, and</p><formula xml:id="formula_12">f = Θ(g) iff f = O(g), g = O(f ). We take f (x) = Õ(g(x)) ⇐⇒ g(x) = Ω(f (x)) to mean f (x) = O(g log k x) for some k ∈ Z (this is slightly different from the standard usage of Õ), and f = Θ(g) ⇐⇒ f = Õ(g) &amp; g = Õ(f ). We introduce a new notation: f = Θ(g) if f (x) = O(g(x) • x ) and f (x) = Ω(g(x) • x -), as x → ∞,</formula><p>for any &gt; 0. All asymptotic notations are sign-less, i.e. can indicate either positive or negative quantities, unless stated otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Overview</head><p>The primary reason we may say anything about the average behavior of any of the above quantities is the central limit theorem: every time the activations of the previous layer pass through an affine layer whose weights are sampled i.i.d., the output is a sum of a large number of random variables, and thus follows approximately Gaussian distributions. The mean and variance of these distributions can be computed by keeping track of the mean and variances of the activations in the previous layer.</p><p>In what follows, we use this technique to derive recurrence equations governing p, q, γ, λ, χ for different architectures and different activation functions. We use these equations to investigate the dynamics of e and s, the key quantities in the forward pass, and the dynamics of χ, the key quantity in the backward pass.</p><p>The cosine distance e in some sense measures the angular geometry of two vectors. If e = 1, then the vectors are parallel; if e = 0, then they are orthogonal. Just as in <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b10">[11]</ref>, we will show that in all of the architectures and activations we consider in this paper, e (l) converges to a fixed point e * as l → ∞ 1 . Thus, on the average, as vectors propagate through network, the geometry of the original input space, for example, linear separability, is "forgotten" by residual networks as well as by vanilla networks. But we will prove and verify experimentally that, while Poole et al. <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b10">[11]</ref> showed that the convergence rate to e * is exponential in a vanilla network, the convergence rate is rather only polynomial in residual networks, for tanh and α-ReLU (Defn 5.2) nonlinearities; see Thm B.5, Thm B.11, Thm B.17, and Thm B.18. This slow convergence preserves geometric information in the input space, and allows a typical residual network to "hover over the edge of chaos": Even when the cosine distance e (l) converges to 0, corresponding to "chaos", (resp. 1, corresponding to "stability"), for the number of layers usually seen in practice, e (l) will reside well away from 0 (resp. 1).</p><p>Similarly, the quantity s measures the metric geometry of two vectors. The evolution of s (l) with l tells us the ability of the average network to separate two input points in terms of Euclidean distance. Again, for tanh and α-ReLU (α &lt; 1) nonlinearities, s varies only polynomially with l.</p><p>On the other hand, χ (l) measures the size of gradient at layer l, and through it we track the dynamics of gradient backpropagation, be it explosion or vanishing. In contrast to vanilla tanh networks, which can experience both of these two phenomenon depending on the initialization variances, typical residual networks cannot have vanishing gradient, in the sense of vanishing χ (l) as l → 1; see Thm B.5 and Thm B.12. Furthermore, while vanilla tanh networks exhibit exponentially vanishing or exploding gradients, all of the activation/architecture pairings considered here, except the full residual network with ReLU, have subexponential gradient dynamics. While tanh residual networks (reduced or full) has χ (0) ≈ exp(Θ( √ l))χ (l) (Thm B.13), α-ReLU residual networks for α &lt; 1 have χ (0) ≈ poly(l)χ (l) (Thm B.20). Instead of ∂E/∂x i , we may also consider the size of gradients of actual trainable parameters. For tanh and α-ReLU with α &lt; 1, they are still subexponential and polynomial (Thm B.21). On the other hand, while χ (0) = exp(Θ(l))χ (l) for a ReLU resnet, its weight gradients have size independent of layer, within O(1) (Thm B.21)! This is the only instance in this paper of gradient norm being completely preserved across layers.</p><p>The above overviews the theoretical portion of this paper. Through experiments, we discover that we can very accurately predict whether one random initialization leads to better performance than another on the test set, after training, by leveraging this theory we build. Residual networks of different nonlinearities have different controlling quantities: for resnets with tanh, the optimal initialization is obtained by controlling the gradient explosion χ (0) /χ (L) ; whereas for ReLU and α-ReLU, the optimal initialization is obtained by maximizing s without running into numerical issues (with floating point computation). See Section 6 for details.</p><p>Over the course of our investigation of α-ReLU, we derived several new identities involving the associated kernel functions, first defined in <ref type="bibr" target="#b1">[2]</ref>, which relate them to the zeroth Bessel functions (Lemmas C.31 to C.34).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Theoretical Results</head><p>In what follows in the main text, we assume σ • &gt; 0 for all • = w, v, b, a; in the appendix, the formal statement of each main theorem will contain results for other cases. We are interested in the two major categories of nonlinearities used today: tanh-like and rectified units. We make the following formal definitions as a foundation for further consideration. Definition 5.1. We say a function φ is tanh-like if φ is antisymmetric (φ(-x) = -φ(x)), |φ(x)| ≤ 1 for all x, φ(x) ≥ 0, ∀x ≥ 0, and φ(x) monotonically increases to 1 as x → ∞. Definition 5.2. Define the α-ReLU ψ α (x) = x α if x &gt; 0 and 0 otherwise. 2   By applying the central limit theorem as described in the last section, we derive a set of recurrences for different activation/architecture pairs, shown in Table <ref type="table" target="#tab_1">1</ref> (see appendix for proofs). They leverage certain integral transforms 3 as in the following  </p><formula xml:id="formula_13">= σ 2 w p + σ 2 b p = Vφ(q) + p λ = σ 2 w γ + σ 2 b γ = Wφ(q, λ) + γ χ = (σ 2 w V φ(q) + 1)χ q = σ 2 w p + σ 2 b p = σ 2 v Vφ(q) + σ 2 a + p λ = σ 2 w γ + σ 2 b γ = σ 2 v Wφ(q, λ) + σ 2 a + γ χ = (σ 2 v σ 2 w V φ(q) + 1)</formula><formula xml:id="formula_14">χ (l) exp(Θ( √ l)), B.6 exp(Θ( √ l)), B.12 exp(Θ(l)), B.20 Θ(l α<label>2</label></formula><p>(1-α)(2α-1) ), B.20 Definition 5.3. Define the transforms V and W by Vφ(q) := E[φ(z) 2 : z ∼ N (0, q)] and</p><formula xml:id="formula_15">Wφ(ρ, ν) := E[φ(z)φ(z ) : (z, z ) ∼ N (0, ρ ν ν ρ )].</formula><p>These recurrences are able to track the corresponding quantities in practice very well. For example, Fig. <ref type="figure" target="#fig_1">1</ref> compares theory vs experiments for the tanh/FRN pair. The agreement is very good for tanh/RRN (not shown, but similar to the case of tanh/FRN with σ v = 1 and σ a = 0) and α-ReLU/FRN as well (see Fig. As mentioned in previous sections, we seek to characterize the long term/high depth behavior of all of the quantities defined in Section 2. To do so, we solve for the asymptotics of the recurrences in Table <ref type="table" target="#tab_1">1</ref>, where φ is instantiated with tanh or α-ReLU. Our main dynamics results are summarized in Table <ref type="table" target="#tab_2">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Tanh</head><p>Forward dynamics. When φ = tanh, p (l) and q (l) increase as Θ(l) in either RRN or FRN (Thm B.2), as one might expect by observing that V tanh(q) → 1 as q → ∞ so that, for example in the RRN case, the recurrence p = V tanh(q) + p becomes p = 1 + p. This is confirmed graphically by the black lines of the leftmost chart of Fig. <ref type="figure" target="#fig_1">1</ref>. We carefully verify that this intuition is correct in its proof in the appendix, and find that in fact p (l) ∼ l in the RRN case and p (l) ∼ (σ 2 v + σ 2 a )l in the FRN case.</p><p>What about γ (l) ? The middle chart of Fig. <ref type="figure" target="#fig_1">1</ref> shows that over time, e (l) = γ (l) /p (l) contracts toward the center of the interval [0, 1], but from the looks of it, it is not clear whether there is a stable fixed point e * of e or not. We prove that, in fact, all trajectories of e not starting at 1 do converge to a single fixed point, but only at a polynomial rate, in both the RRN and FRN cases (Thm B.2 and Thm B.10); we can even explicitly compute the fixed point and the rate of convergence: For FRN, there is a unique stable fixed point e * &lt; 1 determined by the equation</p><formula xml:id="formula_16">e * = 1 σ 2 v + σ 2 a [σ 2 v 2 π arcsin (e * ) + σ 2 a ],</formula><p>and |e * -e (l) | decreases like l -δ * , where   In log-log scale: the dashed line is l -δ * -1 , and the colored lines are e (l) -e (l-1) for different initial conditions e (0) . That they become parallel at about l = 400 on verifies that e (l) = Θ(l -δ * ). 4 (c) In log-log scale: The dashed line is A √ l (A given in Thm B.13), and the colored lines are log(• (1) /• (l) ) for • = χ, χ b , χ w . That they all converge together starting around l = 1000 indicates that the approximation in Thm B.13 is very good for large l.</p><formula xml:id="formula_17">δ * := 1 - 2 π 1 1 -(e * ) 2 σ 2 v σ 2 v + σ 2 a .</formula><p>Since e * &lt; 1, s = (1 -e)p = Θ(p) = Θ(l). The case of RRN can be viewed as a special case of the above, setting σ 2 v = 1 and σ 2 a = 0, which yields e * = 0 and δ * = 1 -2 π . We observe that both e * and δ * only depend on the ratio ρ := σ a /σ v , so in Fig. <ref type="figure" target="#fig_3">2</ref> we graph these two quantities as a function of ρ. e * and δ * both increase with ρ and asymptotically approach 1 and 1 /2 respectively from below. When ρ = σ a = 0, e * = 0 and δ * = 1 -2 π . Thus the rate of convergence at its slowest for tanh/FRN is δ * = 1 -2 π ≈ 0.36338, where asymptotically the network tends toward a chaotic regime e * = 0, corresponding to a large weight variance and a small bias variance; it at its fastest is δ * = 1 /2, where asymptotically the network tends toward a stable regime e * = 1, corresponding to a large bias variance and small weight variance. We verify δ * by comparing e (l) -e (l-1) to l -δ * -1 in log-log scale. If e (l) = Θ(l -δ * ), then e (l) -e (l-1) = Θ(l -δ * -1 ) and should obtain the same slope as l -δ * -1 as l → ∞. The middle figure of Fig. <ref type="figure" target="#fig_3">2</ref> ascertains that this is indeed the case, starting around layer number 400.</p><p>Backward dynamics. Finally, we show that the gradient is approximated by</p><formula xml:id="formula_18">χ (m) = exp(A( √ l - √ m) + O(log l -log m))χ (l) ( )</formula><p>where A = 4 ). The rightmost plot of Fig. <ref type="figure" target="#fig_3">2</ref> verifies that indeed, for large l ≥ 1000, this is a very good approximation. This demonstrates that the mean field assumption of independent backpropagation weights is very practical and convenient even for residual networks.</p><p>Note that in the FRN case, the constant A can be decomposed into</p><formula xml:id="formula_19">A = 4 3 2 π • σ v • σ w • (1 + σ 2 a /σ 2 v ) -1/2</formula><p>. Consider the ratio ρ := σ a /σ v . If ρ 1, then e * ≈ 1 (Fig. <ref type="figure">C</ref>.17), meaning that the typical network essentially computes a constant function, and thus unexpressive; at the same time, large ρ makes A small, and thus ameliorating the gradient explosion problem, making the network more trainable. On the other hand, if ρ 1, then e * ≈ 0 (Fig. <ref type="figure">C</ref>.17), the typical network can tease out the finest differences between any two input vectors, and a final linear layer on top of such a network should be able to express a wide variety of functions <ref type="bibr" target="#b8">[9]</ref>; at the same time, small ρ increases A, worsening the gradient explosion problem, making the network less trainable. This is the same expressivity-trainability tradeoff discussed in <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">α-ReLU</head><p>Forward dynamics. As with the tanh case, to deduce the asymptotic behavior of random α-ReLU resnets, we need to understand the transforms Vψ α and Wψ α . Fortunately, Vψ α has a closed form, and Wψ α has been studied before <ref type="bibr" target="#b1">[2]</ref>. In particular, if α &gt; - 1  2 , then Vψ α (q) = c α q α , where c α is a constant with a closed form given by Lemma B.15. In addition, by <ref type="bibr" target="#b1">[2]</ref>, we know that Wψ α (q, cq) = Vψ α (q)J α (c) for J α given in Appendix C.7.1. Fig. <ref type="figure">C</ref>.17 shows a comparison of J α for different αs along with the identity function.</p><p>Substituting in c α q α for Vψ α , we get a difference equation p</p><formula xml:id="formula_20">-p = σ 2 v c α (σ 2 w p + σ 2 b ) α + σ 2 a</formula><p>governing the evolution of p. This should be reminiscent of the differential equation Ṗ (l) = CP (l) α , which has solution ∝ l 1/(1-α) for α &lt; 1, and ∝ exp(Cl) when α = 1. And indeed, the solutions p (l) to these difference equations behave asymptotically exactly like so (Thm B.16). Thus ReLU behaves very explosively compared to α-ReLU with α &lt; 1. In fact, in simulations, for σ 2 w = 1.69 and σ 2 v = 1.5, the ReLU resnets overflows into infs after around 100 layers, while there's no problem from any other kind of networks we consider.</p><p>Regardless, α-ReLU for all α massages e (l) toward a fixed point e * that depends on α. When φ = ψ 1 , the standard ReLU, e (l) converges to 1 asymptotically as Cl -2 for an explicit constant C depending on σ v and σ w only (Thm B.17), so that s = (1 -e)p = Θ(l -2 exp(Θ(l))) = exp(Θ(l)). When φ = ψ α for α &lt; 1, then e (l) converges to the nonunit fixed point e * of J α at a rate of Θ(l -µ ), where µ = (1 -Jα (e * ))/(1 -α) is independent of the variances (Thm B.18), so that s = Θ(p). Backward dynamics. Finally, we have also characterized the rate of gradient growth for any α ∈ ( 3 4 , 1]. 5 In the case of α = 1, the dynamics of χ is exponential, the same as that of p,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>These rates are verified in</head><formula xml:id="formula_21">χ (l-m) = χ (l) B m where B = 1 2 σ 2 v σ 2 w + 1. For α ∈ ( 3 4 , 1)</formula><p>, the dynamics is polynomial, but with different exponent in general from that of the forward pass:</p><formula xml:id="formula_22">χ (l-m) = Θ(1)χ (l) (l/(l -m)) R for R = α 2 (1-α)(2α-1)</formula><p>, where the constants in Θ(1) do not depend on l or m. Looking only at χ and the gradients against the biases, it seems that ReLU suffers from a dramatic case of exploding gradients. But in fact, because χ gains a factor of B moving backwards while p loses a factor of B, the gradient norm χ (l-m) w (and similarly for χ</p><formula xml:id="formula_23">This exponent R is minimized on α ∈ [ 3 4 , 1) at α = 3 /4, where R = 9 /2 (but on α ∈ ( 1 2 , 1) it is minimized at α = 2 /</formula><formula xml:id="formula_24">(l-m) v</formula><p>) is independent of how far, m, the gradient has been propagated (Thm B.21) -this is certainly the best gradient preservation among all of the models considered in this paper. Thus strangely, random ReLU FRN exhibits both the best (constant for v and w) and the worse (exponential for a and b) gradient dynamics. This begs the question, then, is this a better deal than other α-ReLU for which for any learnable parameter we have at most a polynomial blowup with depth in its gradient? Our experiments (discussed below) show that α-ReLU is useful to the extent that smaller α avoids numerical issues with exponentiating forward and backward dynamics, but the best performance is given by the largest α that avoids them (Fig. <ref type="figure" target="#fig_44">3(c,</ref> <ref type="figure">d</ref>)); in fact, the metric expressivity s, determines performance, not gradient explosion (see α-ReLU experiments).  </p><formula xml:id="formula_25">(L) /s (0) ≈ log p (L) /p (0) ≈ log χ (0) /χ (L) = L log(1 + σ 2 v σ 2 w /2). (e)</formula><p>Red heatmap shows the test accuracies of a grid of α-ReLU FRN with varying α and L as shown, but with all σ•s fixed. The white dashed curve gives a typical contour line of L R = const, where R = α 2</p><p>(1-α)(2α-1) . The yellow-to-blue curves form a set of level curves for s (l) = p (l) -γ (l) = const, with yellow curves corresponding to higher levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Results</head><p>Our experiments show a dichotomy of what matters in initialization: for tanh resnets, quality of an initialization is determined by how much gradient explosion there is (measured by χ (0) /χ (L) ); for (α-)ReLU resnets, it is determined by how expressive the random network is (measured by the metric expressivity s (L) ). We hypothesize this is because in tanh resnets, the gradient dynamics is much more explosive than the expressivity dynamics (exp(Θ( √ l)) vs Θ(l)), whereas for ReLU it's somewhat the opposite (χ w , χ v = Θ(1) vs s = exp(Θ(l))).</p><p>Tanh, vary σ w . We train a grid of reduced and full tanh resnets on MNIST, varying the variance σ 2 w and the number of layers (for FRN we fix σ v = 1). The results are indicated in Fig. <ref type="figure" target="#fig_44">3(a,</ref> <ref type="figure">b</ref>). We see that in either model, deeper resnets favor much smaller σ w than shallower ones. The white dotted lines in Fig. <ref type="figure" target="#fig_44">3(a,</ref> <ref type="figure">b</ref>) confirm our theory: according to Eq. ( ), for the same gradient ratio R = χ (0) /χ (L) , we want log R ≈ σ w √ L. Indeed, the white dotted lines in Fig. <ref type="figure" target="#fig_44">3(a,</ref> <ref type="figure">b</ref>) trace out such a level curve and it remarkably pinpoints the largest σ w that gives the optimal test set accuracy for each depth L. Why isn't the best initialization given by R = 1 ⇐⇒ σ w = 0? We believe that when L and/or σ w is small, gradient dynamics no longer dominates the initialization quality because it has "less room to explode," and expressivity issues start to dampen the test time performance.</p><formula xml:id="formula_26">Tanh, vary σ 2 a /σ 2 v .</formula><p>As suggested in the analysis of Eq. ( ), the ratio ρ 2 = σ 2 a /σ 2 v determines the fixed point e * and its convergence rate by itself while also contributes to the rate of gradient explosion in tanh FRN. We seek to isolate its effect on forward dynamics by varying σ v with ρ such that σ v / 1 + ρ 2 is kept constant, so that the leading term of the log gradient ratio is kept approximately equal for each L and ρ. Fig. <ref type="figure" target="#fig_44">3(c</ref>) shows the test accuracies of a grid of tanh FRN initialized with such an ensemble of σ • s. What stands out the most is that performance is maximized essentially around a fixed value of L regardless of ρ, which shows that indeed gradient dynamics determines the initialization quality in tanh resnets. There is also a minor increase in performance with increasing ρ regardless of L; this is counterintuitive as increasing ρ means "decreasing expressivity." It is currently not clear what accounts for this effect.</p><p>ReLU, vary σ w We train a grid of ReLU FRN on MNIST, varying σ 2 w ∈ [0, 1.5] while fixing</p><formula xml:id="formula_27">σ 2 v = 1, σ 2 a = σ 2 b = 1 2 .</formula><p>The resulting test set accuracies are shown in Fig. <ref type="figure" target="#fig_44">3(d</ref>). The dark upper region signifies failure of training caused by numerical issues with exploding activation and gradient norms: This corresponds to the region where p (L) , which is a measure of the mean magnitude of an neuronal activation in layer L, becomes too big. We see that the best test accuracies are given by depths just below where these numerical issues occur. However, if we were to predict that the optimal init is the one minimizing χ (0) /χ (L) ≥ 1, then we would be wrong -in fact it is exactly the opposite. In this case, the dynamics of s (l) , p (l) , and χ (0) /χ (l) are approximately the same (all exp(Θ(l)) with the same hidden constants), and optimal performance corresponds to the highest s (L) , p (L) , and χ (0) /χ (L) without running into infs.</p><p>α-ReLU, vary α. We similarly trained a grid of α-ReLU FRN on MNIST, varying only α and the depth, fixing all σ • . Fig. <ref type="figure" target="#fig_6">3</ref>(e) shows their test accuracies. We see similar behavior to ReLU, where when the net is too deep, numerical issues doom the training (black upper right corner), but the best performance is given by L just below where this problem occurs. In this case, if we were to predict optimality based on minimizing gradient explosion, we would be again wrong, and furthermore, the contour plot of χ (0) /χ (L) (white dashed line) now gives no information at all on the test set accuracy. In contrast, the contours for s (l) succeeds remarkably well at this prediction (yellow/green lines). 6  By interpolation, this suggests that indeed in the ReLU case, it is expressivity, not trainability, which determines performance at test time.</p><p>In all of our experiments, we did not find e dynamics to be predictive of neural network performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we have extended the mean field formalism developed by <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref> to residual networks, a class of models closer to practice than classical feedforward neural networks as were investigated earlier. We proved and verified that in both the forward and backward passes, most of the residual networks discussed here do not collapse their input space geometry or the gradient information exponentially. We found our theory incredibly predictive of test time performance despite saying nothing about the dynamics of training. In addition, we overwhelmingly find, through theory and experiments, that an optimal initialization scheme must take into account the depth of the residual network. The reason that Xavier <ref type="bibr" target="#b3">[4]</ref> or He <ref type="bibr" target="#b4">[5]</ref> scheme are not the best for residual networks is in fact not that their statistical assumptions are fragile -theirs are similar to our mean field theoretic assumptions, and they hold up in experiments for large width -but rather that their structural assumptions on the network break very badly on residual nets.</p><p>Open Problems. Our work thus have shown that optimality of initialization schemes can be very unstable with respect to architecture. We hope this work will form a foundation toward a mathematically grounded initialization scheme for state-of-the-art architectures like the original He et al. residual network. To do so, there are still two major components left to study out of the following three: 1. Residual/skip connection 2. Batchnorm 3. Convolutional layers. Recurrent architectures and attention mechanisms are also still mostly unexplored in terms of mean field theory. Furthermore, many theoretical questions still yet to be resolved; the most important with regard to mean field theory is: why can we make Axioms 3.1 and 3.2 and still be able to make accurate predictions? We hope to make progress on these problems in the future and encourage readers to take part in this effort.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Notes</head><p>1 Under simplified conditions, Daniely et al. <ref type="bibr" target="#b2">[3]</ref> showed that there exists a fixed point for any "well-behaved" activation function in a feedforward net. However, this result does not apply to architectures with residual connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices A Additional Figures</head><p>In figures appearing in the appendix, means χ (due to legacy reasons). We do so because the norm of the activation vector in a typical ReLU resnet blows up into NaN at around layer 90, while this is not a problem for α &lt; 1. Our theoretical predictions track the average of empirical values closely for forward quantities p (l) , γ (l) , and e (l) for all α, but variance is extremely large for e (l) at α = 1; it also predicts the average gradient norm accurately for α = 1 to α = .7 (despite the fact that we should not expect so for α ≤ .75 due to exploding variance (Thm B.19)), although variance is large for α = 1 at earlier layers (i.e. later layers w.r.t backpropagation). However it consistently and significantly overestimates the average gradient norm for α = .6 to α = .5, where the variance is so large that one standard deviation below the mean results in negative values. All plots are made with parameters     <ref type="formula" target="#formula_52">1</ref>)  <ref type="formula" target="#formula_52">1</ref>)  <ref type="formula" target="#formula_52">1</ref>)  <ref type="formula" target="#formula_52">1</ref>) . We draw |e (l) -e (l-1) | for each of these dynamics in colored solid lines. We predict that each dynamic is Θ(l -µ ), where µ = (1 -Jα(e * ))/(1 -α), and the dashed line gives l -µ-1 (Thm B.18), shifted vertically to better compare the slope in log scale (i.e. the exponent of the polynomial dynamics). (See footnote 4 for why we plot the dynamics this way). We see that the our asymptotic prediction is very accurate for the sequence of e (l) that starts with e (0) = 0.99, the closest to e * for each α, while other lines only slowly converge to the same exponent (which is the slope in the log-log plot). This is to be expected based on the proof of Thm B.18. For α = .9, the e (0) = .99 line upticks at around 10 3 and then turn into NaNs due to numerical instability. (b) Colored lines are • (0) /• (l) for • = χ, χ b , χ w (we are not taking logs in addition to plotting in log-log scale like in Fig. <ref type="figure">C</ref>.15). The dashed lines are our asymptotic predictions for the dynamics with corresponding colors, based on Thm B.21, again shifted appropriately to easily compare slope visually. We see that for every alpha our asymptotic predictions are highly accurate. For both (a) and (b), we did not show α = 1 case as ReLU FRN runs into numerical issues quickly (i.e. with even for 100 layers) because of exponential explosions in p (l) and χ (l) as predicted by Thms B.16 and B.20, so we cannot expect to empirically verify the precise predicted asymptotics. All plots are made with parameters</p><formula xml:id="formula_28">σ 2 v = 1.5, σ 2 a = .</formula><formula xml:id="formula_29">Â(0) b =Â(l) b Â(0) w =Â(l) w l R l R + 1 l R ¡ ®=(1 ¡ ®) α = .8</formula><formula xml:id="formula_30">Â(0) b =Â(l) b Â(0) w =Â(l) w l R l R + 1 l R ¡ ®=(1 ¡ ®) α = .7</formula><formula xml:id="formula_31">Â(0) b =Â(l) b Â(0) w =Â(l) w l R l R + 1 l R ¡ ®=(1 ¡ ®) α = .6</formula><formula xml:id="formula_32">Â(0) b =Â(l) b Â(0) w =Â(l) w l R l R + 1 l R ¡ ®=(1 ¡ ®) α = .55</formula><formula xml:id="formula_33">Â(0) b =Â(l) b Â(0) w =Â(l) w l R l R + 1 l R ¡ ®=(1 ¡ ®) α = .51</formula><formula xml:id="formula_34">Â(0) b =Â(l) b Â(0) w =Â(l) w l R l R + 1 l R ¡ ®=(1 ¡ ®)</formula><formula xml:id="formula_35">σ 2 v = 1.5, σ 2 a = .5, σ 2 w = 1.69, σ 2 b = .49; only α is varied.</formula><p>Table A.1: Glossary of Symbols. "Mean normalized" is abbreviated "m.n." Symbol Meaning Ref σ • standard deviation of trainable parameter • x (l)</p><p>activation vector/input vector h (l)  hidden vector N width (same across all layers) p (l)  m.n. squared length of activation vector x (l) 3.3 q (l) m.n. squared length of hidden vector h (l) 3.3</p><formula xml:id="formula_36">γ (l) m.n. dot product x (l) • x (l) 3.4 λ (l) m.n. dot product h (l) • h (l) 3.4 s (l) m.n. squared distance x (l) -x (l) 2 3.4 e (l)</formula><p>cosine distance γ (l) / p (l) p (l) 3.4 e * limit value of e (l) as l → ∞ c (l)  cosine distance λ (l) / q (l) q (l) 3.4 χ (l)  m.n. gradient squared norm w.r.t. x (l) 3.5 Theorem B.2. Suppose φ is tanh-like. Assume RRN architecture.</p><formula xml:id="formula_37">χ (l) • m.n. gradient squared norm w.r.t. trainable parameter • 3.5 φ variable nonlinearity R → R ψ α α-ReLU 5.2 V variance integral transform 5.3 W covariance integral transform 5.3 δ * e (l) converges like Θ(l -δ * ) in tanh FRN B.11 A leading coeff of log χ (0) /χ (L) in tanh FRN B.13 R log χ (0) /χ (L) ∼ R log L for (α &lt; 1)-ReLU B.20 J α kernel function of α-ReLU C.30</formula><p>• If σ w = 0, then p (l) = lVφ(σ 2 b ) + p (0) and q (l) = σ 2 b .</p><p>• If σ w &gt; 0, lim l→∞ p (l) /l = 1 and lim l→∞ q (l) /(σ 2 w l) = 1. If φ = tanh, then we can obtain more terms of the asymptotic expansions:</p><formula xml:id="formula_38">p (l) = l -2Cσ -1 w l 1/2 -C 2 σ -2 w log l + O(1) q (l) = σ 2 w l -2Cσ w l 1/2 -C 2 log l + O(1) as l → ∞, where C = 2/π.</formula><p>Theorem B.3. Suppose φ is antisymmetric. Then in an RRN, λ and γ satisfy the recurrence</p><formula xml:id="formula_39">λ = σ 2 w γ + σ 2 b γ = Wφ(q, λ) + γ.</formula><p>Theorem B.4. Suppose φ is a tanh-like nonlinearity in an RRN. Assume e (0) &lt; 1.</p><formula xml:id="formula_40">• If σ w = 0, then γ (l) = lWφ(σ 2 b , σ 2 b ) + γ (0) = lVφ(σ 2 b ) + γ (0)</formula><p>and λ (l) = σ 2 b , so that e (l) → 1 and 1 -e (l) = Θ(l -1 ). As a result, s (l) = p (l) (1 -e (l) ) = Θ(1).</p><p>• If σ w &gt; 0, then γ (l) = Θ(l 2 π ), and e (l) → 0 like Θ(l 2 π -1 ). Thus s (l) = Θ(p (l) ) = Θ(l).</p><p>Theorem B.5. For any nonlinearity φ in an RRN, under assumptions Axiom 3.1 and Axiom 3.2, whenever φ2 (ζ) has finite variance for Gaussian variable ζ, χ = (σ 2 w V φ(q) + 1)χ, χ b = χV φ(q), χ w = χV φ(q)p.</p><p>Theorem B.6. For φ = tanh in an RRN,</p><p>• If σ w = 0, χ (m) = χ (l) for all l, m.</p><formula xml:id="formula_41">• If σ w &gt; 0, log(χ (m) /χ (l) ) = A( √ l - √ m) + B(log l -log m) + O(1)</formula><p>where A = 4 Theorem B.7. Suppose φ = tanh. Then in an RRN</p><formula xml:id="formula_42">• If σ w = 0, χ (l) b = χ (L) V φ(σ 2 b ) and χ (l) w = χ (L) V φ(σ 2 b )((l -1)Vφ(σ 2 b ) + p (0)</formula><p>), where L is the last layer.</p><formula xml:id="formula_43">• If σ w &gt; 0, log(χ (m) b /χ (l) b ) = A( √ l - √ m) + B b (log l -log m) + O(1) log(χ (m) w /χ (l) w ) = A( √ l - √ m) + B w (log l -log m) + O(1)</formula><p>where A = 4 </p><formula xml:id="formula_44">q = σ 2 w p + σ 2 b p = σ 2 v Vφ(q) + σ 2 a + p</formula><p>Theorem B.9. Suppose φ is tanh-like. Assume the FRN architecture.</p><formula xml:id="formula_45">• If σ w = 0, then p (l) = (σ 2 v Vφ(σ 2 b ) + σ 2 a )l + p (0)</formula><p>, and q (l) = σ 2 b .</p><formula xml:id="formula_46">• If σ w &gt; 0, then p (l) = b 0 l + b 1 l 1/2 + b 2 log l + O(1)</formula><p>, where</p><formula xml:id="formula_47">b 0 = σ 2 v + σ 2 a b 1 = -2Cσ 2 v σ -1 w σ 2 v + σ 2 a b 2 = -C 2 σ 4 v σ -2 w (σ 2 v + σ 2 a ) 2 and C = 2 π . Additionally, q (l) = σ 2 w b 0 l + σ 2 w b 1 l 1/2 + σ 2 w b 2 log l + O(1).</formula><p>Theorem B.10. For any nonlinearity φ, in an FRN</p><formula xml:id="formula_48">λ = σ 2 w γ + σ 2 b γ = σ 2 v Wφ(q, λ) + σ 2 a + γ</formula><p>Theorem B.11. Assume φ = tanh in an FRN. Suppose e (0) &lt; 1.</p><formula xml:id="formula_49">• If σ w = 0, then λ (l) = σ 2 b and γ (l) = l(σ 2 v Wφ(σ 2 b , σ 2 b ) + σ 2 a ) + γ (0) = l(σ 2 v Vφ(σ 2 b ) + σ 2</formula><p>a )+γ (0) . Thus e (l) → 1 and 1-e (l) = Θ(l -1 ). As a result, s (l) = p (l) (1-e (l) ) = Θ(1). • If σ w &gt; 0, then e (l) converges to the unique fixed point e * = 1 determined by the equation</p><formula xml:id="formula_50">e * = 1 σ 2 v + σ 2 a [σ 2 v 2 π arcsin (e * ) + σ 2 a ].</formula><p>Furthermore, e (l) converges to e * polynomially:</p><formula xml:id="formula_51">|e (l) -e * | is Θ(l -δ * ),</formula><p>where</p><formula xml:id="formula_52">δ * := 1 - 2 π 1 1 -(e * ) 2 σ 2 v σ 2 v + σ 2 a ∈ [ 2 π -1,<label>1 2</label></formula><p>)</p><formula xml:id="formula_53">Since e * &lt; 1, s (l) = Θ(p (l) ) = Θ(l).</formula><p>Theorem B.12. For any nonlinearity φ in an FRN, under assumptions Axiom 3.1 and Axiom 3.2, whenever φ(ζ) 2 has finite variance for Gaussian variable ζ,</p><formula xml:id="formula_54">χ = (σ 2 v σ 2 w V φ(q) + 1)χ, χ b = σ 2 v χV φ(q), χ w = σ 2 v χV φ(q)p, χ v = χVφ(q), χ a = χ</formula><p>Theorem B.13. Assume φ = tanh in an FRN.</p><p>• If σ w = 0, χ (m) = χ (l) for all l, m.</p><formula xml:id="formula_55">• If σ w &gt; 0, then for l ≥ m ≥ 0, log(χ (m) /χ (l) ) = A( √ l - √ m) + B(log l -log m) + O(1)</formula><p>where Theorem B.14. Suppose φ = tanh in an FRN.</p><formula xml:id="formula_56">A = 4 3 2 π σ 2 v σ w σ 2 v + σ 2 a B = 4 9π σ 4 v σ 2 v + σ 2 a 3 σ 2 v + σ 2 a -σ 2 w</formula><formula xml:id="formula_57">• If σ w = 0, then χ (l) b = σ 2 v χ (L) V φ(σ 2 b ) χ (l) w = σ 2 v χ (L) V φ(σ 2 b )((σ 2 v Vφ(σ 2 b ) + σ 2 a )(l -1) + p (0) ) χ (l) v = χ (L) Vφ(σ 2 b ) χ (l) a = χ (L) .</formula><p>• If σ w &gt; 0, then for l ≥ m ≥ 0, log(χ</p><formula xml:id="formula_58">(m) b /χ (l) b ) = A( √ l - √ m) + B b (log l -log m) + O(1) log(χ (m) w /χ (l) w ) = A( √ l - √ m) + B w (log l -log m) + O(1) log(χ (m) a /χ (l) a ) = A( √ l - √ m) + B(log l -log m) + O(1) log(χ (m) v /χ (l) v ) = A( √ l - √ m) + B(log l -log m) + O(1)</formula><p>where</p><formula xml:id="formula_59">A = 4 3 2 π σ 2 v σw √ σ 2 v +σ 2 a and B = 4 9π σ 4 v σ 2 v +σ 2 a 3 σ 2 v +σ 2 a</formula><p>-σ 2 w are as in Thm B.13 and</p><formula xml:id="formula_60">B b = B + 1 2 and B w = B -1 2 . B.2 α-ReLU Lemma B.15. If α &gt; -1 2 , then Vψ α (q) = c α q α , where c α = 1 √ π 2 α-1 Γ α + 1 2 .</formula><p>Note that if α ≤ -1 2 , then Vψ α (q) is not defined (its defining integral does not converge).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.1 Full Residual Network</head><p>By Thm B.8 and Lemma B.15, we have the length recurrences</p><formula xml:id="formula_61">q = σ 2 w p + σ 2 b p = σ 2 v c α q α + σ 2 a + p</formula><p>Theorem B.16. Suppose we have the nonlinearity φ = ψ α . The in an FRN:</p><formula xml:id="formula_62">If α = 1, then p (l) = Θ((1 + σ 2 v σ 2 w /2) l ),</formula><p>with the hidden constant depending on the initial condition. If 0 &lt; α &lt; 1, then p (l) = Θ(l Similarly, by Thm B.10, if q = q , then Theorem B.17. Suppose φ = ψ 1 . Then in an FRN, e (l) → 1 and</p><formula xml:id="formula_63">1 1-α ). More precisely, lim l→∞ p/l 1 1-α = [σ 2 v σ 2α w c α (1 -α)] 1 1-α .</formula><formula xml:id="formula_64">λ = σ 2 w γ + σ 2 b γ = σ 2 v q α Wψ α (1, c) + σ 2 a + γ</formula><formula xml:id="formula_65">1 -e (l) ∼ [ 1 4 σ 2 v σ 2 w B -1 U l] -2 for B = 1+σ 2 v σ 2 w /2 and U = 2 √ 2</formula><p>3π . As a result, s (l) = (1-e (l) )p (l) = Θ(l -2 exp(Θ(l))) = exp(Θ(l)). Theorem B.18. Suppose φ = ψ α for 0 &lt; α &lt; 1 in an FRN. Then e converges to the unique nonunit fixed point e * of J α , and |e * -e (l) | is Θ(l -µ ), where µ = (1 -Jα (e * ))/(1 -α). Additionally, s (l) = Θ(p (l) ) = Θ(l 1/(1-α) ). 2 (1 -α), but we have no proof for it. Based on this conjecture, we see there is a "discontinuity" of µ at α = 1: µ → 0 as α → 1, but for α = 1, the actual convergence dynamics has exponent -2 by Thm B.17.</p><p>Because of the following theorem, we cannot expect the equations of Thm B.12 to hold for α ≤ 3  4 . Theorem B.19. Suppose we have the nonlinearity ψ α in an FRN. Var( ψα (ζ) 2 ) diverges for any Gaussian variable ζ with mean 0 if α ≤ 3  4 but is finite if α &gt; 3 4 .</p><p>Theorem B.20. Suppose we have the nonlinearity</p><formula xml:id="formula_66">ψ α in an FRN. If α = 1, then χ (l-m) = χ (l) 1 2 σ 2 v σ 2 w + 1 m . If α ∈ ( 3 4 , 1), then χ (l-m) = Θ(1)χ (l) (l/(l -m)) R for R = α 2</formula><p>(1-α)(2α-1) , where the constants in Θ(1) do not depend on l or m. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>This exponent</head><formula xml:id="formula_67">χ (l-m) v = Θ(1)χ (l) B l , χ (l-m) a = Θ(1)χ (l) B m .</formula><p>where graphs the exponent α 1-α -R in terms of α. We see that on [0.5, 1], the maximum of this exponent is at α = 1.</p><formula xml:id="formula_68">B = 1 + σ 2 v σ 2 w /2. If φ = ψ α in an FRN, for α &lt; 1, then for l ≥ m ≥ 0, χ (l-m) b = Θ(1)χ (l) l R (l -m) -R-1 , χ (l-m) w = Θ(1)χ (l) l R (l -m) α 1-α -R , χ (l-m) v = Θ(1)χ (l) l R (l -m) α 1-α -R , χ (l-m) a = Θ(1)χ (l) (l/(l -m)) R .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Proofs</head><p>A brief note about notation: We use ∼ to denote both how a random variable is sampled (ex: x ∼ N (0, 1) for a Gaussian x) and how a function behaves asymptotically, i.e. f (x) ∼ g(x) as x → a iff lim x→a f (x)/g(x) = 1. Context should be enough to differentiate between these two cases. We in addition use to denote asymptotic expansion. For example, if {α i } i≥0 is a sequence of strictly decreasing reals and {β i } i≥0 is a sequence of nonzero reals, then</p><formula xml:id="formula_69">f (x) i≥0 β i (x -ξ) αi means that as x → ξ, f (x) - N i=0 β i (x -ξ) αi = Θ((x -ξ) α N +1 ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Preliminary Lemmas</head><p>Lemma C.1. We have</p><formula xml:id="formula_70">σ 2 w γ + σ 2 b σ 2 w p + σ 2 b = e(1 + O(γ -1 )).</formula><p>regardless of whether e (l) = γ (l) /p (l) converges.</p><p>But suppose e (l) = γ (l) /p (l) → e * . If e * &lt; 1, then</p><formula xml:id="formula_71">σ 2 w γ + σ 2 b σ 2 w p + σ 2 b = e(1 + Θ(γ -1 ))</formula><p>.</p><formula xml:id="formula_72">If e * = 1, then σ 2 w γ + σ 2 b σ 2 w p + σ 2 b = e(1 + Θ( p -1 )),</formula><p>where = 1 -e.</p><p>Proof.</p><formula xml:id="formula_73">Write M = σ 2 b /σ 2 w . σ 2 w γ + σ 2 b σ 2 w p + σ 2 b = e(1 + 1 + M γ -1 1 + M p -1 ) = e(1 + M (γ -1 -p -1 ) + O(p -1 (γ -1 -p -1 ))).</formula><p>In any situation, γ -1 -p -1 = O(γ -1 ) because γ ≤ p, so this gives the first statement. If e * exists and e * &lt; 1, then γ -1 -p -1 = Θ(γ -1 ), which yields the second statement. If e * exists and e * = 1,</p><formula xml:id="formula_74">then γ -1 -p -1 = p -1 ((1 -) -1 -1) = p -1 ( + O( 2 )) = Θ( p -1 ).</formula><p>For any function f that is (k + 1)-times differentiable in a neighborhood of 0, we have the asymptotic expansion</p><formula xml:id="formula_75">f (z) = k n=0 d n f dz n (0) z n n! + O(z k+1 ), as z → 0.</formula><p>Since</p><formula xml:id="formula_76">d n d(1/q) n q 1/2 Vφ(q) q→∞ = (-1) n 2 n √ 2π ∞ -∞ φ 2 (z)z 2n dz</formula><p>whenever the RHS is integrable, we have Lemma C.2. Suppose φ 2 (z)z 2n is integrable over z ∈ R for all 0 ≤ n ≤ N + 1. Then Vφ(q) = q -1/2 ( N n=0 C n q -n + O(q -N -1 )) as q → ∞, where</p><formula xml:id="formula_77">C n := (-1) n 2 n n! √ 2π ∞ -∞ φ 2 (z)z 2n dz.</formula><p>Note that sech d (z) = Θ(e -d|z| ) for z → ∞ as long as d &gt; 0, so that C n from the above result converges when φ = sech d . Therefore</p><p>Lemma C.3. Let d &gt; 0. We have V sech d (q) q -1/2 n≥0 C n q -n , where</p><formula xml:id="formula_78">C n := (-1) n 2 n n! √ 2π ∞ -∞ sech 2d (z)z 2n dz.</formula><p>As corollaries, we obtain the following asymptotics.</p><p>Lemma C.4. V ṫanh(q) = 2 3 2 π q -1/2 + Θ(q -3/2 ) as q → ∞.</p><p>Proof. Use Lemma C.3 along with the fact that ṫanh(z) = sech 2 (z) and sech 4 z dz = 2 3 tanh z + 1 2 sech 2 z tanh z.</p><p>Lemma C.5. 1 -V tanh(q) = 2 π q -1/2 + Θ(q -3/2 ) as q → ∞.</p><p>Proof. Use Lemma C.3 along with the fact that 1 -tanh 2 (z) = sech 2 (z) and sech 2 z dz = tanh z.</p><p>Lemma C.6. sech 2 (t) ≥ exp(-t 2 ) for all t, with equality iff t = 0.</p><p>Proof. The lower bound is equivalent to</p><formula xml:id="formula_79">2 ≥ e t-t 2 /2 + e -t-t 2 /2</formula><p>The RHS has derivative (1 -t)e t-t 2 /2 -(1 + t)e -t-t 2 /2 . This is 0 iff</p><formula xml:id="formula_80">1 -t 1 + t = e -2t</formula><p>which has a solution 0 and in general can only have solution t ∈ (-1, 1) (by considering the sign of the LHS). Since each side is analytic in t ∈ (-1, 1), we expand</p><formula xml:id="formula_81">log 1 -t 1 + t = log e -2t log(1 -t) -log(1 + t) = -2t (-t -t 2 -• • • ) -(t -t 2 + • • • ) = -2t -2t -2t 3 -• • • = -2t</formula><p>which shows that the only solution is t = 0. A simple plot shows that t = 0 is a maximum, where the bound in question achieves equality.</p><p>Lemma C.7. Suppose φ = tanh. Then V φ(q) ≥ 1 √ 4q+1 .</p><p>As a sanity check, Lemma C.4 shows that V φ(q) ∼ C 0 q 1/2 where C 0 ≈ .5319, which is above the .5 in this lemma.</p><p>Proof. By Lemma C.6,  </p><formula xml:id="formula_82">V φ(q) = dµ(z) φ2 ( √ qz) ≥ 1 √ 2π dz exp(-z 2 /2 -2qz 2 ) = 1 √ 2π dz exp(-(4q + 1)z 2 /2) = 1 √ 4q + 1 .</formula><formula xml:id="formula_83">Σ(M, N, d) =              Θ(1) if d &lt; -1 log N + O(1) if d = -1 N d+1 d+1 + O(1) if -1 &lt; d &lt; 0 N -M + 1 if d = 0 1 d+1 N d+1 + 1 2 N d + O(N max(0,d-1) ) if d &gt; 0</formula><formula xml:id="formula_84">a+1 a z d -a d dz = 1 d + 1 ((a + 1) d+1 -a d ) = (a d + d 2 a d-1 + • • • ) -a d = d 2 a d-1 + Θ(a d-2 ).</formula><p>where the hidden constants in Θ depend only on d (and in fact this term vanishes if d = 1). Thus</p><formula xml:id="formula_85">Σ(M, N, d) = N +1 M z d dz - N a=M [ d 2 a d-1 + Θ(a d-2 )] = 1 d + 1 ((N + 1) d+1 -M d+1 ) - d 2 Σ(M, N, d -1) + Θ(Σ(M, N, d -2)) If -1 &lt; d &lt; 0, then Σ(M, N, d -1) = Θ(1), so that Σ(M, N, d) = (N +1) d+1 d+1 + O(1) = N d+1 d+1 + O(1). If d &gt; 0 and d = 1, then Σ(M, N, d -1) = N d d , so that Σ(M, N, d) = 1 d + 1 N d+1 + N d + Θ(N max(0,d-1) ) - 1 2 N d + Θ(Σ(M, N, d -2)) = 1 d + 1 N d+1 + 1 2 N d + O(N max(0,d-1) ).</formula><p>We can obtain more terms in the expansion for higher d via the Euler-Maclaurin formula, but this suffices for our purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Dynamics Zoo</head><p>This section deduces the asymptotic behaviors of some sequences governed by recurrence equations.</p><p>For the most part, the leading term of their asymptotic expansions is as one would expect from the corresponding differential equation. However, in some cases we need subleading terms for later results. They require slightly more nuanced reasoning. First we present a technical lemma. Lemma C.9. Let F : R × N → R be a function such that for a subset U ⊆ R, and for all z, z ∈ U, z ≥ z =⇒ F (z, n) ≥ F (z , n) for every n. Suppose sequences a (l) , b (l) , c (l) satisfy</p><p>• a (l+1) = F (a (l) , l) for all l;</p><p>• b (l+1) ≤ F (b (l) , l) for all l above a constant K b .</p><p>• c (l+1) ≥ F (c (l) , l) for all l above a constant K c . and furthermore, a (l) , b (l) , c (l) all fall into U for l above a constant K U .</p><p>If for some m</p><formula xml:id="formula_86">≥ max(K b , K U ), b (m) ≤ a (m) , then b (l) ≤ a (l) , ∀l ≥ m. Similarly, if for some n ≥ max(K c , K U ), c (n) ≥ a (n) , then c (l) ≥ a (l) , ∀l ≥ n. Proof. For the first claim: b (m) ≤ a (m) =⇒ b (m+1) ≤ F (b (m) , m) ≤ F (a (m) , m) = a (m+1) .</formula><p>Here the last inequality used the monotonicity of F . Induction gives the desired result.</p><p>It's similar for the second claim, where the inductive step is</p><formula xml:id="formula_87">c (m) ≥ a (m) =⇒ c (m+1) ≥ F (c (m) , m) ≥ F (a (m) , m) = a (m+1) .</formula><p>Lemma C.10. Suppose (l) satisfies the recurrence</p><formula xml:id="formula_88">(l) = (l-1) (1 + δ l β ). for some nonzero constant δ ∈ R independent of l. • If β &gt; 1, then (l) = Θ(1). • If β = 1, then (l) = Θ(l δ ). • If 0 &lt; β &lt; 1, then (l) = exp( δ 1-β l 1-β + Θ(l ψ1(1-2β) ))</formula><p>, where ψ 1 (x) = max(0, x) is the ReLU function.</p><p>Proof. We have log (l) = log (l-1) + log(1 + δ/l β ) = log (l-1) + δ/l β + Θ(δ 2 /l 2β ) for large l. If β &gt; 1, then l l -β converges, and</p><formula xml:id="formula_89">log (l) = log (0) -Θ(1) (l) = Θ(1). If β = 1, then log (l) = log (0) + δ log l + Θ(1) (l) = Θ(l δ ). If β &lt; 1, then log (l) = log (0) + δ 1 -β l 1-β + Θ(l 1-2β ) (l) = exp( δ 1 -β l 1-β + Θ(l ψ1(1-2β) )).</formula><p>Lemma C.11. Suppose (l) = Cl -α + (l-1) (1 + δ/l β ) for α ∈ R, C = 0, and δ = 0. Then</p><formula xml:id="formula_90">• If β &gt; 1, then -(l) = Θ(l 1-α ) if α ∈ (0, 1); -(l) = Θ(log l) if α = 1; -(l) = Θ(1) if α &gt; 1.</formula><p>Proof. Consider the differential equation</p><formula xml:id="formula_91">ẋµ = -µx β+1 µ /t</formula><p>for constant µ has solution x µ = [β(µ log t + C)] -1/β for some constant C determined by initial condition. Note that</p><formula xml:id="formula_92">-µx µ (t) β+1 /t ≤ x µ (t + 1) -x µ (t) ≤ -µx µ (t + 1) β+1 /(t + 1) = -(1 -o(t -1 ))µx µ (t) β+1 /t.</formula><p>For any small enough α &gt; 0, we apply Lemma C.9 with F ( , l) = -µ β+1 /l (which is monotonic in for small enough ), c (l) = x µ (l), and b (l) = x µ-α (l) to obtain</p><formula xml:id="formula_93">x µ-α (l) ≤ (l) ≤ x µ (l)</formula><p>for large enough l and appropriately chosen initial conditions. This shows that (l) = Θ(log l -1/β ) Taking α → 0, we also obtain the leading coefficient (l) ∼ [βµ log l] -1/β .</p><p>Lemma C.13. Suppose a sequence u (l) is governed by the equation</p><formula xml:id="formula_94">u (l) -u (l-1) = A(u (l-1) + B) α ,</formula><p>where α ∈ [0, 1) and A &gt; 0. Then</p><formula xml:id="formula_95">u (l) = K 1 l 1 1-α -K 2 l α 1-α log l + o(l α 1-α log l), where K 1 = [A(1 -α)] 1 1-α and K 2 = 1 2 A 1 1-α (1 -α) α 1-α -1 α.</formula><p>Proof. Leading term. The differential equation</p><formula xml:id="formula_96">ẋA,B = A(x A,B + B) α has solution x A,B (l) = [A(1 -α)(l + S)]<label>1</label></formula><p>1-α -B for some constant S. Since ẋA,B is monotonic, we have (writing x = x A,B for brevity)</p><formula xml:id="formula_97">A(x A,B (l) + B) α = ẋA,B (l) ≤ x A,B (l + 1) -x A,B (l) ≤ ẋA,B (l + 1) ≤ (A + o(1))(x A,B (l) + B) α</formula><p>for large enough l. We apply Lemma C.9 with F (x, l) = x + A(x + B) α (which is monotonic in x for large x), c (l) = x A,B (l), and b (l) = x A-,B (l) to obtain</p><formula xml:id="formula_98">x A-,B (l) ≤ u (l) ≤ x A,B (l)</formula><p>for large enough l and appropriate initial conditions. Therefore lim u (l) /l</p><formula xml:id="formula_99">1 1-α ∈ [[(A -)(1 - α)] 1 1-α , [A(1 -α)] 1 1-α ].</formula><p>Taking → 0 gives the leading term.</p><formula xml:id="formula_100">Subleading term. Now let v (l) := u (l) -ℵl 1 1-a , where ℵ = [A(1 -α)] 1 1-α . Then we have the recurrence v (l+1) + ℵ(l + 1) 1 1-α -v (l) -ℵl 1 1-α = A(v (l) + ℵl 1 1-α + B) α v (l+1) -v (l) + ℵ( 1 1 -α l α 1-α + 1 2 ( 1 1 -α )( α 1 -α )l α 1-α -1 + Θ(l α 1-α -2 )) = A[ℵ α l α 1-α + α(v (l) + B)ℵ α-1 l -1 + Θ((v (l) + B)l -1-1 1-α )] v (l+1) -v (l) = α 1 -α v (l) l -1 - 1 2 ℵ( 1 1 -α )( α 1 -α )l α 1-α -1 + g(l)</formula><p>for some g(l) = O(l α 1-α -2 + l -1 ) and where, to get the last equation, we have used Aα α = 1 1-α ℵ to cancel the l α 1-α term and simplified αAℵ α-1 = α 1-α . For any J &gt; 0, the differential equation vJ</p><formula xml:id="formula_101">(l) = α 1-α v J (l)l -1 -Jl α 1-α -1 has solution v J (l) = C[l(1 -α)] α 1-α -Jl α 1-α log l. Note that the functions F J (z, n) = z + α 1-α zn -1 -Jn α 1-α -1 and G J (z, n) = F J (z, n) + g(n) is monotonic in z (for positive n).</formula><p>For large l, we also have vJ (l) and F J (v J (l), l) = v J (l) + vJ (l) decreasing in l. Thus for any &gt; 0 and l large enough </p><formula xml:id="formula_102">G J+ (v J (l), l) ≤ F J+ /2 (v J (l), l) ≤ v J (l)+ vJ (l+1) ≤ v J (l+1) ≤ F J (v J (l), l) ≤ G J-(v J (l), l).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Now define v</head><formula xml:id="formula_103">(l) = u (l) -[A(1 -α)l] 1 1-α ,</formula><p>and similar to the proof of Lemma C.13, we find</p><formula xml:id="formula_104">v (l+1) -v (l) = α 1 -α v (l) l -1 -Kl α 1-α -1 + C + g(l)</formula><p>where</p><formula xml:id="formula_105">K = 1 2 A 1 1-α (1 -α) α 1-α -1 α and g(l) = O(l α 1-α -2 + l -1 ). If α 1-α &gt; 1 ⇐⇒ α &gt; 1 2 , then C + g(l) = o(l α 1-α -1</formula><p>) and we can proceed as in the proof of Lemma C.13 to find v (l) ∼ Kl <ref type="formula" target="#formula_52">1</ref>), then by using the differential equation vJ (l) = α 1-α v J (l)l -1 + J to approximate the difference equation solution and applying Lemma C.9 as in the proof of Lemma C.13, we obtain v (l) (l) ∼ C(1-α)</p><formula xml:id="formula_106">α 1-α log l. If α 1-α = 1 ⇐⇒ α = 1 and K = C, then v (l+1) -v (l) = α 1-α v (l) l -1 -(K -C)l α 1-α -1 + g(l), so that the technique used in Lemma C.13 would obtain v (l) ∼ (K -C)l α 1-α log l = (K -C)l log l. If α 1-α &lt; 1 ⇐⇒ α &lt; 1 2 , then v (l+1) -v (l) = α 1-α v (l) l -1 + C + o(</formula><p>1-2α l.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Forward Dynamical Equations</head><p>Here we derive the recurrences governing the forward length and correlation quantities p, q, λ, γ.</p><p>We start with reduced residual networks.</p><p>Lemma B.1. Suppose φ is antisymmetric. Then in an RRN, p and q satisfy the recurrence q = σ 2 w p + σ 2 b p = Vφ(q) + p.</p><p>Proof. We have</p><formula xml:id="formula_107">q = h 2 j = i (w ji x i + b j ) 2 = b 2 j + i w 2 ji x 2 i + 2 i w ji x i b j + 2 j =l w ji w li x 2 i</formula><p>But w ji , w li , x, and b j form an independency, so the last two sums are 0, and the terms in the first sum split multiplicatively. Therefore</p><formula xml:id="formula_108">q = σ 2 b + i w 2 ji x 2 i = σ 2 b + N • σ 2 w N p = σ 2 b + σ 2 w p.</formula><p>For the recurrence of p, we have</p><formula xml:id="formula_109">p = x 2 i = (φ(h i ) + x i ) 2 = φ(h i ) 2 + x 2 i + 2 φ(h i )x i</formula><p>As N → ∞, the coefficient w ii of x i in h i has vanishing covariance, so h i and x i become independent. Therefore φ(h i )x i = φ(h i ) x i . Because h i is the sum of a large number of independent random variables, by CLT, h i is a Gaussian with mean i w ji x i + b j = 0 since w ji = b j = 0. Our antisymmetry assumption on φ then implies φ(h i ) = 0. Therefore,</p><formula xml:id="formula_110">p = φ(h i ) 2 + x 2 i = Vφ(q) + p as desired.</formula><p>Theorem B.3. Suppose φ is antisymmetric. Then in an RRN, λ and γ satisfy the recurrence</p><formula xml:id="formula_111">λ = σ 2 w γ + σ 2 b γ = Wφ(q, λ) + γ.</formula><p>Proof. Similar to Lemma B.1. Now, for the full residual networks, the proofs are similar, but we no longer need to assume that φ is antisymmetric because of the randomization via the extra sets of weights. Theorem B.8. For any nonlinearity φ in an FRN,</p><formula xml:id="formula_112">q = σ 2 w p + σ 2 b p = σ 2 v Vφ(q) + σ 2 a + p Proof. q = h 2 j = (w i j x i + b j ) 2 = (w i j x i ) 2 + b 2 j = σ 2 w x 2 i + σ 2 b = σ 2 w p + σ 2 b p = x 2 i = (v j i φ(h j ) + x i + a i ) 2 = σ 2 v φ(h i ) 2 + x 2 i + σ 2 a = σ 2 v Vφ(q) + σ 2 a + p</formula><p>where in the third equality for p, we are now using the independence of v j i from all other variables to cancel out the terms, whereas before we had to rely on φ being antisymmetric.</p><p>Theorem B.10. For any nonlinearity φ, in an FRN</p><formula xml:id="formula_113">λ = σ 2 w γ + σ 2 b γ = σ 2 v Wφ(q, λ) + σ 2 a + γ</formula><p>Proof. Similar to Thm B.8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Backward Dynamical Equations</head><p>Here we derive the recurrences governing the gradient quantities χ and χ • for different •, all under the gradient independence assumption. Write β </p><formula xml:id="formula_114">χ = (σ 2 w V φ(q) + 1)χ, χ b = χV φ(q), χ w = χV φ(q)p.</formula><p>Proof. For a reduced residual network, we have the following derivative computation:</p><formula xml:id="formula_115">∂x i ∂x j = δ ji + φ(h i ) ∂h i ∂x j , ∂x i ∂h j = δ ji φ(h j ), ∂h i ∂x j = w ij , ∂h i ∂w ij = x j , ∂h i ∂b j = δ ij .</formula><p>Then</p><formula xml:id="formula_116">β j = β j + i β i φ(h i ) ∂h i ∂x j = β j + i β i φ(h i )w ij β 2 j = [β j + i β i φ(h i )w ij ] 2 = β 2 j + i β 2 i φ2 (h i )(w ij ) 2 + 2 i&lt;k β i β k φ(h i )w ij φ(h k )w kj + 2 i β j β i φ(h i )w ij</formula><p>The last two terms of the above vanish as w ij is independent from w kj , h i , h k and β i , β j , β k by Axiom 3.2, and w ij = 0.</p><p>Therefore, applying Axiom 3.1,</p><formula xml:id="formula_117">β 2 j = σ 2 w β 2 j φ2 (h i ) + β 2 j = (σ 2 w V φ(q) + 1) β 2 j</formula><p>We similarly have</p><formula xml:id="formula_118">∂E ∂b j = i ∂E ∂x i ∂x i ∂h j = β j φ(h j ), since ∂x i ∂h j = δ ji φ(h j ) ∂E ∂b j 2 = β 2 j φ(h j ) 2 = β 2 j V φ(q), by Axiom 3.2(b); ∂E ∂w ji = i ∂E ∂x i ∂x i ∂h j ∂h j ∂w ji = β j φ(h j )x i , since ∂x i ∂h j = δ ji φ(h j ) ∂E ∂w ji 2 = β 2 j φ2 (h j )x 2 i = β 2 j V φ(q)p, by Axiom 3.2(b)</formula><p>In the last equation we have also used the fact that as N → ∞, h j and x i become independent (they are jointly Gaussian and their correlation w 2 ji goes to 0 with N ).</p><p>Theorem B.12. For any nonlinearity φ in an FRN, under assumptions Axiom 3.1 and Axiom 3.2, whenever φ(ζ) 2 has finite variance for Gaussian variable ζ,</p><formula xml:id="formula_119">χ = (σ 2 v σ 2 w V φ(q) + 1)χ, χ b = σ 2 v χV φ(q), χ w = σ 2 v χV φ(q)p, χ v = χVφ(q), χ a = χ</formula><p>Proof. For the full residual network, we have the following derivative computations:</p><formula xml:id="formula_120">∂x i ∂x j = δ ji + k v ik φ(h k ) ∂h k ∂x j , ∂x i ∂h j = v ij φ(h j ), ∂h i ∂x j = w ij , ∂h i ∂w ij = x j , ∂h i ∂b i = 1, ∂x i ∂v ik = φ(h k ), ∂x i ∂a i = 1.</formula><p>Again let β j = ∂E ∂xj . Then</p><formula xml:id="formula_121">β j = i β i (δ ji + k v ik φ(h k ) ∂h k ∂x j ) = i β i (δ ji + k v ik φ(h k )w kj )</formula><p>Thus,</p><formula xml:id="formula_122">β 2 j = [ i β i (δ ji + k v ik φ(h k )w kj )] 2 = β 2 j + i,k v 2 ik w 2 kj V φ(q) β 2 i = β 2 j (1 + σ 2 v σ 2 w V φ(q))</formula><p>where in the second equality we applied the independence argument as in the proof of Thm B.5, leveraging Axiom 3.2, and in the third equality we used Axiom 3.1 to get β 2 i = β 2 j . The other computations are similar to the proof of Thm B.12.</p><p>C.5 Tanh: Reduced Residual Network C.5.1 Forward Dynamics Theorem B.2. Suppose φ is tanh-like. Assume RRN architecture.</p><formula xml:id="formula_123">• If σ w = 0, then p (l) = lVφ(σ 2 b ) + p (0) and q (l) = σ 2 b .</formula><p>• If σ w &gt; 0, lim l→∞ p (l) /l = 1 and lim l→∞ q (l) /(σ 2 w l) = 1. If φ = tanh, then we can obtain more terms of the asymptotic expansions:</p><formula xml:id="formula_124">p (l) = l -2Cσ -1 w l 1/2 -C 2 σ -2 w log l + O(1) q (l) = σ 2 w l -2Cσ w l 1/2 -C 2 log l + O(1)</formula><p>as l → ∞, where C = 2/π.</p><p>Proof. The case with σ w = 0 is trivial. We assume σ w &gt; 0 from here on.</p><p>p and q are asymptotically linear with l. We first show that, for any ω &lt; 1,</p><formula xml:id="formula_125">l + p (0) ≥ p (l) ≥ ωl and σ 2 w (l + p (0) ) + σ 2 b ≥ q (l) ≥ σ 2 w ω(l -1) + σ 2 b , Lemma C.16. Let φ is antisymmetric. Then for τ ∈ [0, π/2], Wφ(q, q cos τ ) = lim t→τ 1 π sin t w ≥|w| dw dw Υ(w, w ; τ )φ( √ q √ 2 (w + w ))φ( √ q √ 2 (w -w)) = 1 π ∞ 0 r dre -r 2 /2 π 0 dθΣ( √ qr, θ; τ ) = 1 π ∞ 0 s dsq -1 e -s 2 q -1 /2 π 0 dθΣ(s, θ; τ ) = 1 π π 0 dθ ∞ 0 dse -s 2 q -1 /2 ∂ ∂s Σ(s, θ; τ )</formula><p>where Υ(w, w ; τ</p><formula xml:id="formula_126">) := e -1 2 ( w 2 1-c + (w ) 2 1+c ) -e -1 2 ( (w ) 2 1-c + w 2 1+c</formula><p>) with c = cos τ , and Σ(s, θ; τ ) := φ(s sin θ)φ(s sin(θ -τ )).</p><p>Of course, in the above lemma, the limit in the first equation is only necessary when τ = 0 or τ = π/2.</p><p>Proof. Let c := cos τ and</p><formula xml:id="formula_127">Γ := Wφ(q, cq) = 1 2πq √ 1 -c 2 dz exp(-z T Σ -1 z/2)φ(z)φ(z ),</formula><p>where Σ = q cq cq q .</p><p>Our proof will have two portions: Symmetrization of the Γ integral and trigonometric change of variables for evaluation.</p><formula xml:id="formula_128">Symmetrization. Σ is diagonalized by Ω = 1 √ 2q -1 1 1 1 , Σ = Ω T Diag(1 -c, 1 + c)Ω.</formula><p>By a change of variable w = Ωz, so that dw = q -1 dz, we have</p><formula xml:id="formula_129">Γ = 1 2π √ 1 -c 2 dw exp(-w T Diag(1 -c, 1 + c) -1 w/2)φ( √ q √ 2 (w -w))φ( √ q √ 2 (w + w )) = 1 2π √ 1 -c 2 dw dw e -1 2 ( w 2 1-c + (w ) 2 1+c ) φ( √ q √ 2 (w -w))φ( √ q √ 2 (w + w ))</formula><p>By a change of variable swapping w with w , we get</p><formula xml:id="formula_130">Γ = - 1 2π √ 1 -c 2 dw dw e -1 2 ( (w ) 2 1-c + w 2 1+c ) φ( √ q √ 2 (w + w ))φ( √ q √ 2 (w -w)) Thus 2Γ = 1 2π √ 1 -c 2 dw dw Υ(w, w ; τ )φ( √ q √ 2 (w + w ))φ( √ q √ 2 (w -w))</formula><p>where</p><formula xml:id="formula_131">Υ(w, w ; τ ) = e -1 2 ( w 2 1-c + (w ) 2 1+c ) -e -1 2 ( (w ) 2 1-c + w 2 1+c ) .</formula><p>Note that, by the antisymmetry of φ, the integrand K := Υ(w, w ; τ )φ(. . .)φ(. . .) above has the symmetries K(w, w ) = K(w , w) = K(w, -w ), and is everywhere nonnegative.  This gives the first equation in the lemma.</p><formula xml:id="formula_132">Polar Coordinates. Let w √ 1-c = r cos θ, w √ 1+c = r sin θ, so that w = r cos θ √ 1 -c = √ 2r cos θ sin τ 2 w = r sin θ √ 1 + c = √ 2r sin θ cos τ 2 dw dw = 1 -c 2 r dr dθ = (sin 2 τ )r dr dθ. Then A := w ≥|w| e -( w 2 1-c + (w ) 2 1+c )/2 φ( q/2(w + w ))φ( q/2(w -w)) dw dw = sin 2 τ ∞ 0 r dre -r 2 /2 π-τ /2 τ /2 dθφ( √ qr sin(θ + τ /2))φ( √ qr sin(θ -τ /2)).</formula><p>Similarly, let w</p><formula xml:id="formula_133">√ 1+c = r cos θ, w √ 1-c = r sin θ, so that w = r cos θ √ 1 + c = √ 2r cos θ cos τ 2 w = r sin θ √ 1 -c = √ 2r sin θ sin τ 2 dw dw = 1 -c 2 r dr dθ = (sin 2 τ )r dr dθ, and B = w ≥|w| e -( w 2 1+c + (w ) 2 1-c )/2 φ( q/2(w + w ))φ( q/2(w -w)) dw dw = -sin 2 τ ∞ 0 r dre -r 2 /2 π/2+τ /2 π/2-τ /2 dθφ( √ qr cos(θ + τ /2))φ( √ qr cos(θ -τ /2)) = -sin 2 τ ∞ 0 r dre -r 2 /2 τ /2 -τ /2</formula><p>dθφ( √ qr sin(θ + τ /2))φ( qr sin(θ -τ /2)).</p><formula xml:id="formula_134">Thus Γ = 1 π √ 1 -c 2 (A -B) = 1 π ∞ 0 r dre -r 2 /2 π-τ /2 -τ /2 dθφ( √ qr sin(θ + τ /2))φ( √ qr sin(θ -τ /2)) = 1 π ∞ 0 r dre -r 2 /2 π 0 dθφ( √ qr sin(θ))φ( √ qr sin(θ -τ )).</formula><p>This gives the second equation in the lemma, and a change of variables s = √ qr gives the third.</p><p>For the fourth equality, we start from the third equality, and apply integration by parts:</p><formula xml:id="formula_135">1 π ∞ 0 s dsq -1 e -s 2 q -1 /2 π 0 dθΣ(s, θ; τ ) = 1 π π 0 dθ ∞ 0 dssq -1 e -s 2 q -1 /2 Σ(s, θ; τ ) = 1 π π 0 dθ -e -s 2 q -1 /2 Σ(s, θ; τ ) ∞ s=0 + ∞ 0 dse -s 2 q -1 /2 ∂ ∂s Σ(s, θ; τ ) = 1 π π 0 dθ ∞ 0 dse -s 2 q -1 /2 ∂ ∂s Σ(s, θ; τ ).</formula><p>where the last equality follows because Σ(0, θ; τ ) = 0 and e -s 2 q -1 /2 → 0 as s → ∞.</p><p>In the following lemmas, the "2" is not important, and can be any arbitrary finite or infinite value.</p><p>Lemma C.17. Suppose a function f</p><formula xml:id="formula_136">: (0, 2) → R is C k on (0, 2). If lim x↓0 f (i) (x)</formula><p>exists and is finite for every i ∈ [0, k], then f can be extended to [0, 2) such that one sided ith derivatives exist at 0 for all i ∈ [0, k].</p><formula xml:id="formula_137">Proof. Consider f (i) (0) := f (i) (1) - 1 0 f (i+1) (x) dx for i ∈ [0, k -1], which naturally is also equal to f (i) ( ) -0 f (i+1) (x) dx for any &gt; 0. Certainly f (i) (x) → f (i) (0)</formula><p>as x → 0 if this limit exists -and by assumption it does, for 0 ≤ i ≤ k -1. Therefore, we can define the extension of f (i) to x = 0 to be f (i) (0) := f (i) (0). But we need to check that for i</p><formula xml:id="formula_138">∈ [0, k -1]. lim →0 1 (f (i) ( ) -f (i) (0)) = f (i+1) (0)</formula><p>so that all one sided ith derivatives exist. But</p><formula xml:id="formula_139">1 (f (i) ( ) -f (i) (0)) = 1 0 f (i+1) (x) dx = f (i+1) (0) + 1 0 (f (i+1) (x) -f (i) (0))I(x ∈ [0, ]) dx Since lim x↓0 f (i+1) (x) = f (i+1) (0), f (i+1) (x)-f (i+1) (0)</formula><p>is bounded for small x, and by dominated convergence,</p><formula xml:id="formula_140">1 0 (f (i+1) (x) -f (i) (0))I(x ∈ [0, ]) dx → 1 0 0 dx = 0 as → 0. Thus lim →0 1 (f (i) ( ) -f (i) (0)) = f (i+1) (0) as desired. Lemma C.18. If f : [0, 2) → R is C k on (0,</formula><p>2) and has one sided derivatives at 0 up to order k, then</p><formula xml:id="formula_141">f ( ) = f (0) + f (1) (0) + • • • + i-1 (i -1)! f (i-1) (0) + O( i )</formula><p>for any i ≤ k.</p><p>Proof. We have</p><formula xml:id="formula_142">f ( ) = f (0) + 0 f (1) (x) dx = f (0) + f (1) (0) + 0 f (1) (x) -f (1) (0) dx = f (0) + f (1) (0) + 0 x0 0 f (2) (x 2 ) dx 2 dx 1 = f (0) + f (1) (0) + 2 2 f (2) (0) + 0 x1 0 f (2) (x 2 ) -f (2) (0) dx 2 dx 1 . . . f ( ) = f (0) + f (1) (0) + • • • + i-1 (i -1)! f (i-1) (0) + 0 dx 1 x1 0 dx 2 • • • xi-1 0 dx i f (i) (x i )</formula><p>for any i ≤ k. It suffices then to bound the size of the integral. Since</p><formula xml:id="formula_143">f (i) (x) → f (i) (0) as x ↓ 0 by assumption, |f (i) (x i )| is bounded by some constant C on the integration region A := {(x 1 , . . . , x i ) : ≥ x 1 ≥ • • • ≥ x i } for small enough . Therefore, 0 dx 1 x1 0 dx 2 • • • xi-1 0 dx i f (i) (x i ) = f (i) (x i )I( x ∈ A) d x ≤ C|A| = Θ( i ).</formula><p>As a corollary, Lemma C.19. If f : (0, 2) → R is smooth on (0, 2) and lim x→0 f (i) (x) exists and is finite for all i, then f can be extended to [0, 2) and be one-sided smooth at 0, and</p><formula xml:id="formula_144">f ( ) = f (0) + f (1) (0) + • • • + i-1 (i -1)! f (i-1) (0) + O( i )</formula><p>for any i. Lemma C.20. Let φ = tanh. For any fixed c, Wφ(q, cq) is smooth (infinitely differentiable) on q ∈ (0, ∞). As a function of Q := q -1 , it can be extended smoothly to the point Q = 0, so that</p><formula xml:id="formula_145">Wφ(q, cq) = lim q →∞ Wφ(q , cq ) + q -1 lim q →∞ ∂Wφ(q , cq )/∂(q ) -1 + • • • + q -i+1 (i -1)! lim q →∞ ∂ i-1 Wφ(q , cq )/∂(q ) -i+1 + O(q -i )</formula><p>for any i ≥ 0. Furthermore, for c bounded away from 1, the constants hidden O can be taken independent of c.</p><p>Proof. Smoothness on (0, ∞). By the third equation of Lemma C.16, for Q ∈ (0, ∞) ⇐⇒ q ∈ (0, ∞),</p><formula xml:id="formula_146">1 π ∞ 0 s ds ∂ n ∂Q n Qe -s 2 Q/2 π 0 dθ|φ(s sin θ)φ(s sin(θ -τ ))| ≤ ∞ 0 s ds ∂ n ∂Q n Qe -s 2 Q/2 &lt; ∞,</formula><p>so by Leibniz's integral rule and a simple induction, all derivatives of Wφ(q, cq) against Q exists for any Q ∈ (0, ∞).</p><p>Extension to Q = 0. By Lemma C.19, it suffices to show that the limit of ∂ k Wφ(q,cq)</p><p>∂Q k exists and is finite as Q → 0, for all k. Let τ = arccos c. By the fourth equation of Lemma C.16, we have explicitly</p><formula xml:id="formula_147">∂ k Wφ(q, cq) ∂Q k = 1 π π 0 dθ ∞ 0 ds( -s 2 /2) k e -s 2 Q/2 ∂ ∂s Σ(s, θ; τ ) = (-2) -k π π 0 dθ ∞ 0 ds s 2k e -s 2 Q/2 ∂ ∂s Σ(s, θ; τ )</formula><p>for any Q ∈ (0, ∞). Note that for φ = tanh, φ = sech 2 , ∂ ∂s Σ(s, θ; τ ) = sin θ φ(s sin θ)φ(s sin(θ -τ )) + sin(θ -τ )φ(s sin θ) φ(s sin(θ -τ )).</p><p>We split the integral of ∂ k Wφ ∂Q k as follows:</p><formula xml:id="formula_148">∂ k Wφ(q, cq) ∂Q k = (-2) -k π π 0 dθ ∞ 0 ds s 2k e -s 2 Q/2 sin θ φ(s sin θ)φ(s sin(θ -τ )) + (-2) -k π π 0 dθ ∞ 0 ds s 2k e -s 2 Q/2 sin(θ -τ )φ(s sin θ) φ(s sin(θ -τ ))</formula><p>We show that for each piece, the limit as Q → 0 exists and is finite, for any k. This will prove the smooth extendability of Wφ to Q = 0. We will do this for the first piece; the second is similar.</p><p>For Q &gt; 0, the integrand is absolutely integrable, so we may switch the integrals. We now try to bound the inner integral by an exponentially decreasing term e -sµ for some µ; clearly, by monotone convergence on the outer integral as Q → 0, this would show the limit of the integral exists and is finite.</p><p>Because φ is odd and φ is even, the inner integrand is negative on θ ∈ [0, τ ) and positive on θ ∈ (τ, π].</p><p>We will break up the inner integral as follows, for some fixed &gt; 0 satisfying τ -&gt; 0 independent of s (recall τ ∈ (0, π/2]). For the other part: is finite as Q → 0, by monotone convergence.</p><p>Independence of constant hidden in O((q ) -i ). The constant hidden is a function of the chosen above, which depend on τ , but only to the extent that it must satisfy τ -&gt; 0. As long as we are interested in a set C of c that is bounded away from 1, the corresponding set of τ is bounded away from 0, so can be taken to be some number smaller than all of the corresponding τ .</p><p>Lemma C.21. Suppose φ is tanh-like. Then for c ∈ [0, 1],</p><p>Wφ(q, cq) ≤ 2 π arcsin(c),</p><p>and weakly increases to this upper bound as q → ∞. Furthermore,</p><p>• If c = 0 or 1, then equality holds regardless of q.</p><p>• If c ∈ (0, 1) is held constant, 2 π arcsin(c) -Wφ(q, cq) = Θ(q -1 ), where the hidden constants in Θ depend on c. But the constants can be made independent of c if c ∈ [ , 1 -] for some &gt; 0.</p><p>Proof. The cases of c = 0 or 1 are obvious by the definition of W. So from here on we assume c ∈ (0, 1). Wtanh(0, q, q, q cos 1) 2 π arcsin(cos 1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure C.11:</head><p>We verify empirically that the subleading term in W tanh(q, cq) is linear in q -1 , for constant c. Indeed, observe that the curve of of W tanh intersects the y-axis at an angle.</p><p>Let τ := arccos c. By the first equation of Lemma C.16 and the assumption that φ is tanh-like, it is immediate that Wφ(q, cq) is nondecreasing in q. By dominated convergence, using the second equation of Lemma C.16, we get</p><formula xml:id="formula_149">lim q→∞ Wφ(q, cq) = 1 π ∞ 0 r dre -r 2 /2 (π -2τ ) = π -2τ π = 2 π arcsin c.</formula><p>Then the convergence rate is O(q -1 ) by Lemma C.20 and Taylor's theorem. Thus to show the convergence rate is Θ(q -1 ), it suffices to show that D := ∂Wφ(q,cq) ∂Q &lt; 0. But this is apparent from the first equation of Lemma C.16: For τ ∈ (0, π/2),</p><formula xml:id="formula_150">D = 1 π sin τ w ≥|w| dw dw Υ(w, w ; τ )(- 1 2 √ 2 Q -3/2 )</formula><p>× [ φ( q/2(w + w ))φ( q/2(w -w))</p><p>+ φ( q/2(w + w )) φ( q/2(w -w))] &lt; 0 since Υ is positive on the integration domain, and φ and φ are both positive for positive arguments, by the assumption of φ being tanh-like.</p><p>Independence of the constants in Θ(q -1 ) from c when c ∈ [ , 1 -]. By Lemma C.20, the upper constant can be made independent from c. Since D is monotonically decreasing in c (or monotonically increasing in τ ) and |D| is monotonically increasing in c (or monotonically decreasing in τ ), we have</p><formula xml:id="formula_151">|D| &gt; |D| c=</formula><p>, which can be taken to be the lower constant in Θ(q -1 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. C</head><p>.11 verifies empirically that the subleading term in W tanh(q, cq) is linear in q -1 , for constant c.</p><p>Theorem B.4. Suppose φ is a tanh-like nonlinearity in an RRN. Assume e (0) &lt; 1.</p><formula xml:id="formula_152">• If σ w = 0, then γ (l) = lWφ(σ 2 b , σ 2 b ) + γ (0) = lVφ(σ 2 b ) + γ (0)</formula><p>and λ (l) = σ 2 b , so that e (l) → 1 and 1 -e (l) = Θ(l -1 ). As a result, s (l) = p (l) (1 -e (l) ) = Θ(1).</p><p>• If σ w &gt; 0, then γ (l) = Θ(l 2 π ), and e (l) → 0 like Θ(l 2 π -1 ). Thus s (l) = Θ(p (l) ) = Θ(l).</p><p>Proof. We have by Lemma C.21, γ = 2 π arcsin(λ/q) -Θ(q -1 ) + γ. -Θ(q -1 ) + γ.</p><p>We claim that γ (l) → ∞ as l → ∞. Otherwise, there is some C such that γ (l) ≤ C for all l. For large enough l, p (l) ≥ ωl for any ω &lt; 1 and arcsin C σ 2 w p (l-1) +σ 2 b = Θ(1/l) by linearization of arcsin. Thus γ (l) = Θ(log l), but this contradicts our assumption that γ is bounded. This proves our claim.</p><p>Therefore, for large enough l,</p><formula xml:id="formula_153">σ 2 w γ + σ 2 b σ 2 w p + σ 2 b = γ/p + Θ(l -1</formula><p>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. C</head><p>.12 shows 2 π arcsin x vs x. One sees that 1 is an unstable fixed point; if e &lt; 1 -, then 2 π arcsin e &lt; 1 --δ for some δ. Thus c drops monotonically until some threshold under which the linearization of arcsin, arcsin x = x + Θ(x 3 ), is applicable. So for large enough l,</p><formula xml:id="formula_154">γ -γ = 2 π arcsin(γ/p + Θ(l -1 )) -Θ(l -1 ) = 2 π γ/p + O(l -1 )</formula><p>As p (l) ∼ l by Thm B.2, this difference equation has solution γ = Ω(l • If σ w = 0, χ (m) = χ (l) for all l, m.</p><formula xml:id="formula_155">• If σ w &gt; 0, log(χ (m) /χ (l) ) = A( √ l - √ m) + B(log l -log m) + O(1)</formula><p>where A = 4 Proof. The σ w = 0 case is obvious. We will assume σ w &gt; 0 from here on.</p><p>Let </p><formula xml:id="formula_156">p (l) = b 0 l + b 1 l 1/2 + b 2 log l + O(1). Then for D = 2</formula><formula xml:id="formula_157">q -1/2 = σ -1 w b -1/2 0 l -1/2 (1 -b 1 b -1 0 2 -1 l -1/2 -b 2 b -1 0 2 -1 l -1 log l + O(l -1 )) V φ(q) = Dq -1/2 + Θ(q -3/2 ) = Dσ -1 w b -1/2 0 l -1/2 (1 -b 1 b -1 0 2 -1 l -1/2 -b 2 b -1 0 2 -1 l -1 log l + O(l -1 )) log(BV φ(q) + 1) = BDσ -1 w b -1/2 0 l -1/2 -(BDσ -1 w b -3/2 0 b 1 2 -1 + B 2 D 2 σ -2 w b -1 0 2 -1 )l -1 + Θ(l -3/2 log l) l r=1 log(BV φ(q (r) ) + 1) = 2BDσ -1 w b -1/2 0 l 1/2 -(BDσ -1 w b -3/2 0 b 1 2 -1 + B 2 D 2 σ -2 w b -1 0 2 -1 ) log l + O(1)</formula><p>In our case, we have b</p><formula xml:id="formula_158">0 = 1, b 1 = -2Cσ -1 w , b 2 = C 2 σ -2 w , B = σ 2 w , C = 2 π , which gives l r=1 log(BV φ(q (r) ) + 1) = 4 3 2 π σ w l 1/2 + ( 4 3π -σ 2 w 4 9π ) log l + O(1).</formula><p>so that</p><formula xml:id="formula_159">χ (m) /χ (l) = exp 4 3 2 π σ w ( √ l - √ m) + ( 4 3π -σ 2 w 4 9π )(log l -log m) + O(1)</formula><p>Theorem B.7. Suppose φ = tanh. Then in an RRN</p><formula xml:id="formula_160">• If σ w = 0, χ (l) b = χ (L) V φ(σ 2 b ) and χ (l) w = χ (L) V φ(σ 2 b )((l -1)Vφ(σ 2 b ) + p (0)</formula><p>), where L is the last layer.</p><formula xml:id="formula_161">• If σ w &gt; 0, log(χ (m) b /χ (l) b ) = A( √ l - √ m) + B b (log l -log m) + O(1) log(χ (m) w /χ (l) w ) = A( √ l - √ m) + B w (log l -log m) + O(1)</formula><p>where A = 4 Proof. The σ w = 0 case is obvious. We will assume σ w &gt; 0 from here on.</p><p>As in the proof of Thm B.6,</p><formula xml:id="formula_162">V φ(q) = Dσ -1 w b -1/2 0 l -1/2 + Θ(l -1 )</formula><p>where D = 2 </p><formula xml:id="formula_163">log(χ (m) w /χ (l) w ) = 4 3 2 π σ w ( √ l - √ m) + ( 4 3π + 1 2 -σ 2 w 4 9π )(log l -log m) + O(1)</formula><p>C.6 Tanh: Full Residual Network C.6.1 Forward Dynamics Theorem B.9. Suppose φ is tanh-like. Assume the FRN architecture.</p><formula xml:id="formula_164">• If σ w = 0, then p (l) = (σ 2 v Vφ(σ 2 b ) + σ 2 a )l + p (0) , and q (l) = σ 2 b . • If σ w &gt; 0, then p (l) = b 0 l + b 1 l 1/2 + b 2 log l + O(1), where b 0 = σ 2 v + σ 2 a b 1 = -2Cσ 2 v σ -1 w σ 2 v + σ 2 a b 2 = -C 2 σ 4 v σ -2 w (σ 2 v + σ 2 a ) 2 and C = 2 π . Additionally, q (l) = σ 2 w b 0 l + σ 2 w b 1 l 1/2 + σ 2 w b 2 log l + O(1).</formula><p>Proof. The σ w = 0 case is obvious. We will assume σ w &gt; 0 from here on.</p><p>As in Thm B.2, p will have expansion p</p><formula xml:id="formula_165">= b 0 l + b 1 l 1/2 + b 2 log l + O(1). Then, for C = 2 π , q -1/2 = σ -1 w b -1/2 0 l -1/2 (1 -b 1 b -1 0 2 -1 l -1/2 -b 2 b -1 0 2 -1 l -1 log l + O(l -1 )) l r=1 Vφ(q (r) ) = l r=1 1 -C(q (r) ) -1/2 + Θ((q (r) ) -3/2 ) = l -2Cσ -1 w b -1/2 0 l 1/2 + Cσ -1 w b 1 b -3/2 0 2 -1 log l + O(1) p (l) = σ 2 v l r=1 +σ 2 a l = (σ 2 v + σ 2 a )l -2Cσ 2 v σ -1 w b -1/2 0 l 1/2 + Cσ 2 v σ -1 w b 1 b -3/2 0 2 -1 log l + O(1)</formula><p>which yields</p><formula xml:id="formula_166">b 0 = σ 2 v + σ 2 a b 1 = -2Cσ 2 v σ -1 w b -1/2 0 = -2Cσ 2 v σ -1 w σ 2 v + σ 2 a b 2 = -C 2 σ 4 v σ -2 w (σ 2 v + σ 2 a ) 2 Lemma C.22. Suppose φ is tanh-like. Then γ ≤ σ 2 v 2 π arcsin (λ/q) + σ 2 a + γ,<label>and</label></formula><formula xml:id="formula_167">σ 2 v 2 π arcsin (λ/q) + σ 2 a + γ -γ = Θ(q -1 ).</formula><p>Proof. Similar to the proof of Lemma C.21.</p><p>Lemma C.23. Let u * ∈ [0, 1). Let f t : [0, 1) → [0, 1] be a continuous function for each t ∈ N, to each of which we associate two numbers 0 ≤ a t ≤ u * ≤ b t ≤ 1. Suppose for each t, f t (u) &gt; u for all u ∈ [0, a t ) and f t (u) &lt; u for all u ∈ (b t , 1). Assume that for each u, f t (u) -u → 0 as t → ∞ uniformly over u. If a t u * and b t u * , then for any u 0 ∈ [0, 1), the dynamics u t = f t (u t-1 ) has a limiting point. Furthermore, either u t → u * or u t eventually converges monotonically (decreasing or increasing) to a limit point.</p><p>Proof. Fix a u 0 ∈ [0, 1). If u t → u * then we are done. Otherwise, suppose there is a neighborhood [u * -, u * + ] such that for an infinite sequence t 1 , t 2 , . . ., u ti ∈ [u * -, u * + ]. WLOG assume u ti &lt; u *for all i and (t i ) i is the sequence of all ts that satisfy this inequality. If (t i ) i contains {s : s ≥ N } for some N , then for some M &gt; N , for every t &gt; M , a t &gt; u * -&gt; u t . By assumption, u t is monotonic for all t &gt; M but is bounded above. Thus u t has a fixed point û ≤ u *as desired. Now assume there are infinite is such that t i -1 = t i-1 (i.e. t i -1 is not part of the sequence (t i ) i ). We will show that this case is contradictory. Take T large enough such that a t &gt; u * -/2 and |f t (u) -u| &lt; /4 for all u and for all t ≥ T (T exists by premise). Let j be the smallest index such that t j &gt; T and t j -1 = t j-1 . By the definition of j, u tj</p><formula xml:id="formula_168">-1 ≥ u * -. If u tj -1 ≥ u * -/2, then by definition of T , u * -&gt; u tj = f tj (u tj -1 ) &gt; u tj -1 -/4 &gt; u * -3 /4 &gt; u * -, a contradiction. If u * -≤ u tj -1 ≤ u * -/2, then by the definition of T , u tj -1 ≤ a tj -1 so that u tj = f tj (u tj -1 ) &gt; u tj -1 ≥ u * -, a contradiction.</formula><p>The "furthermore" claim is clear from our proof above.</p><p>Theorem B.11. Assume φ = tanh in an FRN. Suppose e (0) &lt; 1.</p><formula xml:id="formula_169">• If σ w = 0, then λ (l) = σ 2 b and γ (l) = l(σ 2 v Wφ(σ 2 b , σ 2 b ) + σ 2 a ) + γ (0) = l(σ 2 v Vφ(σ 2 b ) + σ<label>2</label></formula><p>a )+γ (0) . Thus e (l) → 1 and 1-e (l) = Θ(l -1 ). As a result, s (l) = p (l) (1-e (l) ) = Θ(1).</p><p>• If σ w &gt; 0, then e (l) converges to the unique fixed point e * = 1 determined by the equation</p><formula xml:id="formula_170">e * = 1 σ 2 v + σ 2 a [σ 2 v 2 π arcsin (e * ) + σ 2 a ].</formula><p>Furthermore, e (l) converges to e * polynomially: |e (l) -e * | is Θ(l -δ * ), where</p><formula xml:id="formula_171">δ * := 1 - 2 π 1 1 -(e * ) 2 σ 2 v σ 2 v + σ 2 a ∈ [ 2 π -1,<label>1 2 )</label></formula><p>Since e * &lt; 1, s (l) = Θ(p (l) ) = Θ(l).</p><p>Proof. The σ w = 0 case is obvious. We will assume σ w &gt; 0 from here on.</p><p>If σ a = 0, then e * as defined above is 0, and e = γ p decreases as Θ(l 2 π -1 ) to 0, by the same reason as before.</p><p>So from now on suppose σ a &gt; 0. We apply Lemma C.23 first to show that e converges. We have</p><formula xml:id="formula_172">σ 2 v Wφ(q, cq) + σ 2 a = ep -ep = ep -ep + ep -ep = (e -e)p + e(p -p) = (p -p)[(e -e) p p -p + e] σ 2 v Wφ(q, cq) + σ 2 a σ 2 v Vφ(q) + σ 2 σ 2 v +σ 2 a -u + o(1) &gt; 0 (resp.</formula><p>&lt; 0) on larger and larger intervals [0, a l ] ∩ J k (resp. [b l , 1) ∩ J k ). This proves all the preconditions for Lemma C.23, which yields that I k converges to a limit point. As this argument is independent of k, we have that for all e (0) ∈ [0, 1), e (l) converges. Now we solve for the limit point.</p><p>Suppose e has limit point e † (possibly different from e * described in the theorem); if we express γ (l) = (e † + (l) )p (l) , then As l → ∞, c ∼ e → e † , and Wφ(q, e † q) → 2 π arcsin(e † ), and Vφ(q) → 1. Additionally, p/(pp) = Θ(l) and = o(1) so that -= o(l -1 ). Then we have, taking limits l → ∞,</p><formula xml:id="formula_173">σ 2 v Wφ(q, cq) + σ 2 a = γ -γ = (e † + )p -(e † + )p = e † (p -p) + p -p σ 2 v Wφ(q, cq) + σ 2 a σ 2 v Vφ(q) + σ 2 a = e † + + ( -) p p -p</formula><formula xml:id="formula_174">σ 2 v 2 π arcsin(e † ) + σ 2 a σ 2 v + σ 2 a = e † .</formula><p>Since f l (as defined above) repels points away from 1, the only solution for e † when e (0) &lt; 1 is e † = e * as specified in the theorem statement.</p><p>We defer the proof of the convergence rate to e * to Thm C.25.</p><p>Lemma C.24. Let e * be the stable fixed point determined by σ a and σ v . Then as long as</p><formula xml:id="formula_175">σ v &gt; 0, 2 π 1 1 -(e * ) 2 σ 2 v σ 2 v + σ 2 a ∈ ( 1 2 ,<label>2 π ]</label></formula><p>Proof. Write ρ := 1 -e * Substituting ρ into the expression in question, it follows that we want to show</p><formula xml:id="formula_176">2 π (1 -e * 2 ) -1/2 (1 + ρ) -1 = 2 π (1 -e * 2 ) -1/2 1 -2 π arcsin e * 1 -e * -1 ∈ ( 1 2 , 2 π ] for e * ∈ [0, 1) (the endpoint at 1 is not included since σ v &gt; 0. But this is 2 π (1 -e * ) 1/2 (1 + e * ) -1/2 (1 - 2 π arcsin e * ) -1 .</formula><p>Set g(e * ) to be this expression. We could proceed by finding critical points, but a simple plot Fig. <ref type="figure">C</ref>.13 shows that g is decreasing on [0, 1), with extremal values at the end points:</p><formula xml:id="formula_177">g(e * ) ∈ [ lim e * →1</formula><p>g(e * ), g(0)), for e * ∈ [0, 1).</p><p>Obviously g(0) = 2 π . For the limit, we note that arcsin e * has an asymptotic expansion π</p><formula xml:id="formula_178">2 - √ 2(1 - e) 1/2 + Θ((1 -e) 3/2 ) at 1, so that (1 -e * ) 1/2 (1 -2 π arcsin e * ) -1 → π 2 √ 2</formula><p>, and g(e * ) → 1 2 as e * → 1.</p><p>Theorem C.25. If e (0) &lt; 1, then |e (l) -e * | is Ω(l -δ * -ε ) and O(l -δ * +ε ) for any ε &gt; 0, where</p><formula xml:id="formula_179">δ * := 1 - 2 π 1 1 -(e * ) 2 σ 2 v σ 2 v + σ 2 a ∈ [1 - 2 π ,<label>1 2 )</label></formula><p>,</p><p>where the bounds on the right follow from Lemma C.24. Proof. Define ω(q, c) = 2 π arcsin(c) -W tanh(q, cq). By Lemma C.21, for large enough l, c is close to e * bounded away from 0 or 1, so that ω(q, c) = Θ(q -1 ) with the constant hidden in Θ independent of c. Additionally, by Lemma C.5, 1 -V tanh(q) = Θ(q -1/2 ). Therefore,</p><formula xml:id="formula_180">(e * + )p = σ 2 v ( 2 π arcsin(e * + ) -ω(q, c)) + σ 2 a + γ = σ 2 v 2 π [arcsin(e * ) + 1 -(e * ) 2 + Θ( 2 )] -Θ(l -1 ) + σ 2 a + γ = e * (σ 2 v + σ 2 a ) + (e * + )p + σ 2 v 2 π 1 -(e * ) 2 + Θ( 2 ) -Θ(l -1 ) e * (p -p -σ 2 v -σ 2 a ) = p -p + σ 2 v 2 π 1 -(e * ) 2 + Θ( 2 ) -Θ(l -1 ) e * σ 2 v (Vφ(q) -1) = p -p + σ 2 v 2 π 1 -(e * ) 2 + Θ( 2 ) -Θ(l -1 ) = 1 p (e * σ 2 v (1 -Vφ(q)) + Θ( 2 ) -Θ(l -1 ) + (p + σ 2 v 2 π 1 1 -(e * ) 2 )) = Θ(l -3/2 ) + (1 -δ (l) /l)</formula><p>where</p><formula xml:id="formula_181">δ (l) = l p (σ 2 v Vφ(q) + σ 2 a -σ 2 v 2 π 1 1 -(e * ) 2 ) + Θ( /l) = (1 + Θ(l -1/2 ))(σ 2 v (1 -Θ(l -1/2 )) + σ 2 a -σ 2 v 2 π 1 1 -(e * ) 2 )/(σ 2 v + σ 2 a ) + Θ( /l) = δ * + O(l -1/2 ),</formula><p>where δ * := 1-</p><formula xml:id="formula_182">2 π 1 √ 1-(e * ) 2 σ 2 v σ 2 v +σ 2 a</formula><p>, which is positive by Lemma C.24. By taking the δ of Lemma C.11 to be δ * + ε or δ * -ε respectively for lower and upper bounding the dynamics of (l) , the solution (l) is Ω(l -δ * -ε ) and O(l -δ * +ε ) for any ε &gt; 0 since 1 2 &gt; δ * .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.6.2 Backward Dynamics</head><p>Theorem B.13. Assume φ = tanh in an FRN.</p><p>• If σ w = 0, χ (m) = χ (l) for all l, m. Proof. The σ w = 0 case is obvious. We will assume σ w &gt; 0 from here on.</p><p>As in the proof of Thm B.6, log(χ (m) /χ (l) ) = 2BDσ Proof. Similar to Thm B.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.7 α-ReLU: Full Residual Network</head><p>The following can be checked readily Lemma B.15. If α &gt; -1 2 , then Vψ α (q) = c α q α , where c α = 1 √ π 2 α-1 Γ α + 1 2 .</p><p>Since ψα = αψ α-1 , we have as a corollary, Lemma C.26. If α &gt; 1 2 , then V ψα (q) = α 2 c α-1 q α-1 .</p><p>As a special case, when α = 1, c α = 1 2 . The following is a trivial computation, but useful for many simplifications. By the standard method of characteristic equation, we get that p (l) = A + CB l where A = -</p><formula xml:id="formula_183">σ 2 a +σ 2 b σ 2 v σ 2 v σ 2 w , B = 1 + σ 2 v σ 2 w</formula><p>2 , and C is a coefficient determined by initial conditions. Theorem C.29. Suppose α &lt; 1. We have the following asymptotic expansion</p><formula xml:id="formula_184">p (l) = K 1 l 1 1-α + R(l)</formula><p>where the remainder term</p><formula xml:id="formula_185">R(l) ∼      -K 2 l α 1-α log l if α &gt; 1 2 (C -K 2 )l log l if α = 1 2 and K 2 = C C(1-α) 1-2α l if α &lt; 1 2</formula><p>where  Note that the integrand is symmetric in u and v. Thus, if V = {(u, v) : u, v ≥ 0 &amp; v ≥ u}, then 2πc α J α (θ) = 2 csc θ V du dve -(u 2 +v 2 -2uv cos θ)/2 sin 2 θ u α v α . Now make the change of variables from V to {(p, q) : q ≥ 2 √ p}: p = uv dp = v du + u dv q = u + v dq = du + dv dp dq = (v -u) du dv du dv = (q 2 -4p) -1/2 dp dq so that we have</p><formula xml:id="formula_186">K 1 = [σ 2 v σ 2α w c α (1 -α)] 1 1-α , K 2 = 1 2 [σ 2 v c α σ 2α w ] 1 1-α (1 -α)</formula><formula xml:id="formula_187">2πc α J α (θ) = 2 csc θ ∞ 0 dpe p(1+cos θ) csc 2 θ p α ∞ 2 √ p dqe -q 2 csc 2 θ (q 2 -4p) -1/2 .</formula><p>The inner integral in q can be expressed in terms of K 0 by a change of variable x = q 2 /2 √ p: Proof. We will prove this claim for θ &lt; 1, and by continuity this also proves the case θ = 1. As remarked above, K 0 (z) = K0 (z) + z -1 K0 (z). Thus -dx[cos θe x cos θ x α + αe x cos θ x α-1 ] K0</p><p>-dx[cos θe x cos θ x α-1 + (α -1)e x cos θ x α-2 ]K 0</p><p>Asymptotically, K 0 (z) ∼ π 2z e -z as z → ∞ and K 0 (z) ∼ -ln(z) as z 0, and K0 (z) ∼ -π 2z e -z as z → ∞ and K0 (z) ∼ -z -1 as z 0. Thus, as α &gt; 1, K0 e x cos θ x α | ∞ 0 = -lim x→∞ π/2e -x(1-cos θ) x α-1 + lim</p><p>x 0 e x cos θ x α-1 = 0 K 0 e x cos θ x α-1 | ∞ 0 = -lim x→∞ π/2e -x(1-cos θ) x α-2 + lim</p><p>x 0 e x cos θ x α-1 ln x = 0 So L α (θ) = -cos θL α-1 (θ) -(α -1)L α-2 (θ) -dx[cos θe x cos θ x α + αe x cos θ x α-1 ] K0</p><p>Via another integration by parts, the integral on the right is cos θe x cos θ x α K 0 | ∞ 0 + αe x cos θ x α-1 K 0 | ∞ 0 -dx[cos 2 θe x cos θ x α + 2α cos θe x cos θ x α-1 + α(α -1)e x cos θ x α-2 ]K 0 = -[cos 2 θL α (θ) + 2α cos θL α-1 (θ) + α(α -1)L α-2 (θ)]</p><p>Now, Vφ(q)γ -1 p -1 = Θ(l -1 1-α -1 ). By using the dynamics of Lemma C.11 to upper and lower bound our dynamics, we have (l) = Ω(l -µ-), O(l -µ+ ) for any &gt; 0, where µ = min((1υ)/(1 -α), 1/(1 -α)) = (1 -υ)/(1 -α).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.7.2 Backward Dynamics</head><p>Lemma C.37. Suppose random variable X ∼ N (0, σ 2 ), and Y = ψ -β (X) for some β &gt; 0, where ψ α is α-ReLU. Then for ξ &gt; 0, Y has density</p><formula xml:id="formula_188">Pr[Y ∈ [ξ, ξ + dξ]] = 1 β √ 2πσ 2 ξ -1 β -1 e -ξ -2/β /2σ 2 .</formula><p>At ξ = 0, Y has density given by a Dirac delta of mass 1 2 . Furthermore, Y has finite second moment iff β &lt; 1 2 .</p><p>Proof. We have</p><formula xml:id="formula_189">Pr[Y ∈ [ξ, ∞)] = Pr[X ∈ [0, ξ -1/β ]] = 1 √ 2πσ<label>2</label></formula><p>ξ -1/β 0 e -x 2 /2σ 2 dx.</p><p>Differentiating the RHS against ξ using Leibniz's rule, we get</p><formula xml:id="formula_190">d Pr[Y ∈ [ξ, ∞)]/dξ = 1 √ 2πσ 2 e -ξ -2/β /2σ 2 d dξ ξ -1/β = -1 β √ 2πσ 2 ξ -1 β -1 e -ξ -2/β /2σ 2 .</formula><p>Negating both sides gives the density f Y of Y for ξ &gt; 0. For ξ = 0, observe that lim ξ→0 f Y (ξ) = 0 because, while ξ -1 β -1 blows up polynomially, e -ξ -2/β /2σ 2 blows up exponentially. Thus the contribution of Y 's mass at Y = 0 from X &gt; 0 is 0. On the other hand, all X &lt; 0 gets mapped to Y = 0, so f Y (0) = 1 2 δ 0 , where δ 0 is the Dirac delta. For the second assertion, observe that</p><formula xml:id="formula_191">f Y (ξ) ∼ 1 β √ 2πσ 2 ξ -1 β -1 as ξ → ∞. Thus, ξ 2 f Y (ξ) is integrable iff 2 -1 β -1 &lt; -1 ⇐⇒ β &lt; 1 2 .</formula><p>Theorem B.19. Suppose we have the nonlinearity ψ α in an FRN. Var( ψα (ζ) 2 ) diverges for any Gaussian variable ζ with mean 0 if α ≤ 3 4 but is finite if α &gt; 3 4 .</p><p>Proof. Note that ψα ∝ ψ α-1 , so it suffices to show that Var(ψ α-1 (ζ) (1-α)(2α-1) , where the constants in Θ(1) do not depend on l or m.</p><p>Proof. If α = 1, then</p><formula xml:id="formula_192">χ = χ(1 + 1 2 σ 2 v σ 2 w ).</formula><p>So χ (l-m) /χ (l) = Θ(1)B m for B = 1 + 1 2 σ 2 v σ 2 w .</p><p>If 1 2 &lt; α &lt; 1, then χ/χ -1 is Proof. The proof is similar to that of Thm B.7.</p><formula xml:id="formula_193">σ 2 v σ 2 w V φ(q) = σ 2 v σ 2 w α 2 c α-1 q α-1 = σ 2 v σ 2 w α 2 c α-1 (σ 2 w p) α-1 + Θ(p α-2 ) = σ 2 v σ 2α w α 2 c α-1 (K 1 l 1 1-α -K 2 l</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc>A.1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Our equations predict the relevant quantities very well in practice. These plots make the comparison between prediction and measurements for the full resnet with tanh activation, with σ 2 v = 1.5, σ 2 a = .5, σ 2 w = 1.69, σ 2 b = .49. Left-to-right: (a) p (l) and γ (l) against layer l for 200 layers. (b) e (l) = γ (l) /p (l) against l for 200 layers. Both (a) and (b) trace out curves for different initial conditions. (c) Different gradient quantities against l for 50 layers. From left to right the layer number l decreases, following the direction of backpropagation. Notice that the gradient increases in norm as l → 1. All three figures exhibit smooth curves, which are theoretical estimates, and irregular curves with shades around them, which indicate empirical means and standard deviations (both of which taken in regular scale, not log scale). (a) and (b) are made with 20 runs of resnets of width 1000. (c) is made with 25 runs of resnets of width 250.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Left-to-right: (a) Plots of e * and δ * against σa/σv. (b)In log-log scale: the dashed line is l -δ * -1 , and the colored lines are e (l) -e (l-1) for different initial conditions e (0) . That they become parallel at about l = 400 on verifies that e (l) = Θ(l -δ * ).4 (c) In log-log scale: The dashed line is A √ l (A given in Thm B.13), and the colored lines are log(•(1) /• (l) ) for • = χ, χ b , χ w . That they all converge together starting around l = 1000 indicates that the approximation in Thm B.13 is very good for large l.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>3 2πv +σ 2 a</head><label>32</label><figDesc>σ w in the RRN case and A = in the FRN case (Thm B.6 and Thm B.13</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><figDesc>Fig. A.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>3 ,</head><label>3</label><figDesc>where R = 4); see Fig. B.8. These exponents are verified empirically in Fig. A.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: From left to right, top to bottom: (a) and (b): σ 2 w , L, and test set accuracy of a grid of tanh reduced (left) and full (right) resnets trained on MNIST. Color indicates performance, with ligher colors indicating higher accuracy on test set. Other than the values on the axes, we have fixed σ 2 b = σ 2 a = 1 2 and σ 2 v = 1. The white dotted lines are given by σ 2 w L = C, where C = 170 on the left and C = 145 on the right. We see that both dotted lines accurately predict the largest optimal σw for each depth L. (c) Varying the ratio σ 2 a /σ 2 v while fixing σv/ 1 + σ 2 a /σ 2 v , and thus fixing A, the leading constant of log χ (0) /χ (L) . (d) in log-log scale: Heatmap gives the test accuracies of ReLU FRN for varying σ 2 w and L. Curves give level sets for the log ratios log s (L) /s (0) ≈ log p (L) /p (0) ≈ log χ (0) /χ (L) = L log(1 + σ 2 v σ 2 w /2). (e) Red heatmap shows the test accuracies of a grid of α-ReLU FRN with varying α and L as shown, but with all σ•s fixed. The white dashed curve gives a typical contour line of L R = const, where R =</figDesc><graphic coords="9,303.43,176.68,111.31,103.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>eFigure A. 1 :</head><label>1</label><figDesc>Figure A.1: Empirical vs theoretical dynamics for p(l) , e(l) , and different gradient quantities for α-ReLU, with format similar to Fig.1. We refer to each figure on each row from left to right as (a), (b), and (c). Note that in the α = 1 case, figure (a) (p (l) and γ(l) for different initial values) has log scale y-axis and (a) and (b) have x-axis ranging from 1 to 50, while for other α, (a) has normal y-axis and (a) and (b) have x-axis ranging from 1 to 200. We do so because the norm of the activation vector in a typical ReLU resnet blows up into NaN at around layer 90, while this is not a problem for α &lt; 1. Our theoretical predictions track the average of empirical values closely for forward quantities p (l) , γ(l) , and e (l) for all α, but variance is extremely large for e (l) at α = 1; it also predicts the average gradient norm accurately for α = 1 to α = .7 (despite the fact that we should not expect so for α ≤ .75 due to exploding variance (Thm B.19)), although variance is large for α = 1 at earlier layers (i.e. later layers w.r.t backpropagation). However it consistently and significantly overestimates the average gradient norm for α = .6 to α = .5, where the variance is so large that one standard deviation below the mean results in negative values. All plots are made with parameters σ 2 v = 1.5, σ 2 a = .5, σ 2 w = 1.69, σ 2 b = .49; only α is varied. All figures exhibit smooth curves, which are theoretical estimates, and irregular curves with shades around them, which indicate empirical means and standard deviations (both of which taken in regular scale, not log scale). For each α, figures (a) and (b) are made with 20 runs of resnets of width 1000. (c) is made with 25 runs of resnets of width 250.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><figDesc>Figure A.1: Empirical vs theoretical dynamics for p(l) , e(l) , and different gradient quantities for α-ReLU, with format similar to Fig.1. We refer to each figure on each row from left to right as (a), (b), and (c). Note that in the α = 1 case, figure (a) (p (l) and γ(l) for different initial values) has log scale y-axis and (a) and (b) have x-axis ranging from 1 to 50, while for other α, (a) has normal y-axis and (a) and (b) have x-axis ranging from 1 to 200. We do so because the norm of the activation vector in a typical ReLU resnet blows up into NaN at around layer 90, while this is not a problem for α &lt; 1. Our theoretical predictions track the average of empirical values closely for forward quantities p (l) , γ(l) , and e (l) for all α, but variance is extremely large for e (l) at α = 1; it also predicts the average gradient norm accurately for α = 1 to α = .7 (despite the fact that we should not expect so for α ≤ .75 due to exploding variance (Thm B.19)), although variance is large for α = 1 at earlier layers (i.e. later layers w.r.t backpropagation). However it consistently and significantly overestimates the average gradient norm for α = .6 to α = .5, where the variance is so large that one standard deviation below the mean results in negative values. All plots are made with parameters σ 2 v = 1.5, σ 2 a = .5, σ 2 w = 1.69, σ 2 b = .49; only α is varied. All figures exhibit smooth curves, which are theoretical estimates, and irregular curves with shades around them, which indicate empirical means and standard deviations (both of which taken in regular scale, not log scale). For each α, figures (a) and (b) are made with 20 runs of resnets of width 1000. (c) is made with 25 runs of resnets of width 250.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><figDesc>= 0: 01 e(0) = 0: 21 e(0) = 0: 40 e(0) = 0: 60 e(0) = 0: 79 e(0) = 0: 99</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure A. 2 :</head><label>2</label><figDesc>Figure A.2: We verify the exponents of the forward and backward dynamics for α-ReLU FRN. For each row, the figures are labeled (a) and (b) from left to right. The format is the same as in Fig. C.17. All figures are in log-log scale. (a) We exhibit our theoretical dynamics of the cosine distance e (l) based on the recurrences Thm B.8 and Thm B.10 for different initial conditions e(0) . We draw |e (l) -e (l-1) | for each of these dynamics in colored solid lines. We predict that each dynamic is Θ(l -µ ), where µ = (1 -Jα(e * ))/(1 -α), and the dashed line gives l -µ-1 (Thm B.18), shifted vertically to better compare the slope in log scale (i.e. the exponent of the polynomial dynamics). (See footnote 4 for why we plot the dynamics this way). We see that the our asymptotic prediction is very accurate for the sequence of e (l) that starts with e (0) = 0.99, the closest to e * for each α, while other lines only slowly converge to the same exponent (which is the slope in the log-log plot). This is to be expected based on the proof of Thm B.18. For α = .9, the e (0) = .99 line upticks at around 10 3 and then turn into NaNs due to numerical instability. (b) Colored lines are • (0) /• (l) for • = χ, χ b , χ w (we are not taking logs in addition to plotting in log-log scale like in Fig.C.15). The dashed lines are our asymptotic predictions for the dynamics with corresponding colors, based on Thm B.21, again shifted appropriately to easily compare slope visually. We see that for every alpha our asymptotic predictions are highly accurate. For both (a) and (b), we did not show α = 1 case as ReLU FRN runs into numerical issues quickly (i.e. with even for 100 layers) because of exponential explosions in p (l) and χ (l) as predicted by Thms B.16 and B.20, so we cannot expect to empirically verify the precise predicted asymptotics. All plots are made with parameters σ 2 v = 1.5, σ 2 a = .5, σ 2 w = 1.69, σ 2 b = .49; only α is varied.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>1 .</head><label>1</label><figDesc>Suppose φ is antisymmetric. Then in an RRN, p and q satisfy the recurrence q = σ 2 w p + σ 2 b p = Vφ(q) + p.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>3 2π σ w and B = 4</head><label>34</label></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>3 2π 4 3π -σ 2 w 4 9π</head><label>344</label><figDesc>σ w (same as A in Thm B.6) and B b = B + 1 2 , B w = B -1 2 , with B = (same as B in Thm B.6). B.1.2 Full Residual Network Theorem B.8. For any nonlinearity φ in an FRN,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>5 p ¡ b0l ¡ b1 p l b2logl + 5 : 4 Figure B. 3 :</head><label>5543</label><figDesc>Figure B.3: Empirical verification of Thm B.9.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Fig. B. 4</head><label>4</label><figDesc>Fig. B.4 shows empirical verification of the asymptotic expansion of χ for various values of σ • s.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>6 , 3 ℸ 7 , 2 ℸ 8 , 6 ℸσv = 0. 9 , 1 ℸ 7 , 4 ℸ 8 , 9 ℸFigure B. 4 :</head><label>6372869174894</label><figDesc>Figure B.4: Empirical verification of the asymptotic expansion of χ for various values of σ•s. Note that we have chosen all small values for σ•s. For larger values, the constant term in Thm B.13 begins to dominate (primarily because of the expansion log(1 + x) = x + Θ(x 2 ) has large Θ term when x is large), and χ behaves more like exp(Θ(l)) up to depth 1000.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Fig. B. 5</head><label>5</label><figDesc>Fig. B.5 empirically verifies the asymptotics for α = 1 for various σ v and σ w .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>2 Figure B. 5 :Figure B. 6 :</head><label>256</label><figDesc>Figure B.5: Verification of the exponential asymptotics of p (l) when α = 1. The lines of each color correspond to different (σw, σv) pairs, which are given in the legend. The solid lines are given by the recurrences Thm B.8, and the dashed lines are given by our asymptotics (1 + σ 2 v σ 2 w /2) l (Thm B.16). Note that the y-axis is in log-scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>Fig. B. 6</head><label>6</label><figDesc>Fig. B.6 verifies empirically that e * is indeed the fixed point of e (l) . Fig. A.2 verifies empirically the convergence rate l -µ . Fig. B.7 plots Jα (e * ) and µ versus α. It certainly looks like µ = 12 (1 -α), but we have no proof for it. Based on this conjecture, we see there is a "discontinuity" of µ at α = 1: µ → 0 as α → 1, but for α = 1, the actual convergence dynamics has exponent -2 by Thm B.17.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>α 2 ( 1 Figure B. 7 :Figure B. 8 :</head><label>2178</label><figDesc>Figure B.7: (a) A plot of Jα(e * ) versus α. (b) A plot of the exponent µ of the dynamics of |e * -e (l) | (see Thm B.18)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>Fig. A. 2</head><label>2</label><figDesc>Fig. A.2 verifies the backward asymptotic dynamics empirically for different α &lt; 1. Fig. B.8(b) graphs the exponent α1-α -R in terms of α. We see that on [0.5, 1], the maximum of this exponent is at α = 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head>Fig. C.9 demonstrates Lemma C. 7 .Lemma C. 8 .</head><label>78</label><figDesc>Fig. C.9 demonstrates Lemma C.7. Lemma C.8. Let d ∈ R and 1 &lt; M &lt; N with N -M ∈ Z ≥0 . Set Σ(M, N, d) := N a=M a d . If we fix M and let N → ∞,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head>Figure C. 9 : 1 √ 1 √</head><label>911</label><figDesc>Figure C.9: Illustration of Lemma C.7: V φ(q) vs 1 √ 4q+1 for φ = tanh. This bound is very tight, and for most purposes, 1 √ 4q+1 can be taken as a good approximation of V φ(q).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_31"><head>1 1 -</head><label>1</label><figDesc>α , which gives the result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_32"><head>5 .</head><label>5</label><figDesc>For any nonlinearity φ in an RRN, under assumptions Axiom 3.1 and Axiom 3.2, whenever φ2 (ζ) has finite variance for Gaussian variable ζ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_33"><figDesc>Fig. C.10 displays a contour plot of K for typical values of q and c. So Γ = 1 π √ 1 -c 2 w ≥|w| dw dw K(w, w ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_34"><head>Figure C. 10 :</head><label>10</label><figDesc>Figure C.10: The integrand of Γ after symmetrization. Here c = .2 and q = 100 and φ = tanh.</figDesc><graphic coords="33,219.77,72.00,172.47,129.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_35"><figDesc>e -s 2 Q/2 sin θ φ(s sin θ)φ(s sin(θ -τ )) θ φ(s sin θ)φ(s sin(θ -τ ))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_36"><head>π 0 dθ</head><label>0</label><figDesc>sin θ φ(s sin θ)φ(s sin(θ -τ ))sin θ φ(s sin θ)φ(s sin(θ -τ )) + πdθ sin θ φ(s sin θ)φ(s sin(θ -τ )) Now because φ(z) = sech 2 (z) ≤ 2e -z , and sin θ ≥ sin on θ ∈ [ , π -], πdθ sin θ φ(s sin θ)φ(s sin(θ -τ ))π -2 ) exp(-s sin ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_37"><head>0 dss 2k e -s 2 Q 2 -</head><label>022</label><figDesc>dθ sin θ φ(s sin θ)φ(s sin(θ -τ )) = 0 sin(π -θ) φ(s sin π -θ)φ(s sin(π -θ -τ )) d(π -θ) = 0 dθ sin θ φ(s sin θ)φ(s sin θ + τ ) sin θ φ(s sin θ)φ(s sin(θ -τ )) = 0 dθ sin θ φ(s sin θ)[φ(s sin(τ + θ)) -φ(s sin(τ -θ))] But by intermediate value theorem, φ(s sin(τ +θ))-φ(s sin(τ -θ)) = 2θ∂φ(s sin(τ +θ))/∂θ| θ=ψ = 2θ φ(s sin(τ + ψ))s cos(τ + ψ) for some ψ ∈ [-θ, θ]. By the assumption on , φ(s sin(τ + θ))φ(s sin(τ -θ)) ≤ 2 φ(s sin(τ -))s cos(τ -). Then 0 dθ sin θ φ(s sin θ)[φ(s sin(τ + θ)) -φ(s sin(τ -θ))] ≤ 0 dθ sin θ φ(s sin θ)2 φ(s sin(τ -))s cos(τ -) ≤ 2 φ(s sin(τ -))s cos(τ -)O(1) Because τ -&gt; 0 by assumption on , and because φ(z) = exp(-Θ + (z)), this quantity is exp(-Θ + (z)), as desired (here Θ + denotes a positive quantity). Thus π 0 dθ sin θ φ(s sin θ)φ(s sin(θ -τ )) sin θ φ(s sin θ)φ(s sin(θ -τ )) + πdθ sin θ φ(s sin θ)φ(s sin(θ -τ )) = exp(-Θ + (s)) and similarly for the other piece of ∂ k Wφ ∂Q k , so that ∞ 0 dss 2k e -s 2 Q/2 π 0 dθ sin θ φ(s sin θ)φ(s sin(θ -τ )) = ∞ Θ+(z) → ∞ 0 dss 2k e -Θ+(z)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_39"><head>Since q = σ 2 w p + σ 2 b= 2 π arcsin σ 2 w γ + σ 2 b σ 2 w p + σ 2 b</head><label>22</label><figDesc>by Thm B.2, and λ = σ 2 w γ + σ 2 b by Thm B.3, γ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_40"><head>2 π -), O(l 2 π</head><label>22</label><figDesc>+ ) for any by using the dynamics of Lemma C.11 to upper and lower bound this difference equation.C.5.2 Backward DynamicsTheorem B.6. For φ = tanh in an RRN,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_41"><head>3 2π</head><label>3</label><figDesc>σ w and B = 4 3π -</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_42"><head>3 2π</head><label>3</label><figDesc>, we have (implicitly applying Lemma C.4 and Lemma C.8),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_43"><head>3 2π 4 3π -σ 2 w 4 9π</head><label>344</label><figDesc>σ w (same as A in Thm B.6) and B b = B + 1 2 , B w = B -1 2 , with B = (same as B in Thm B.6).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_44"><head>3 2π</head><label>3</label><figDesc>. Thus by Thm B.5, log(χ (m) /χ (l) l -log m) + O(1) l -log m) + O(1) Similarly, since p = l + Θ( √ l) by Thm B.2, we have</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_45"><head>5 xFigure C. 12 : 1 σ 2 v +σ 2 a [σ 2 v 2 π</head><label>512122</label><figDesc>Figure C.12: Graph of y(e) = 1 σ 2 v +σ 2 a</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_46"><head>σ 2 a σ 2 v.</head><label>2</label><figDesc>By definition of e * , we get e * = (1 -ρ) 2 π arcsin e * + ρ ρ = = e * -2 π arcsin e *</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_47"><head>Figure C. 13 :</head><label>13</label><figDesc>Figure C.13: Plot of g(e * ) in the proof of Lemma C.24</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_48"><head>•</head><figDesc>If σ w &gt; 0, then for l ≥ m ≥ 0, log(χ (m) /χ (l) ) = A( √ l -√ m) + B(log l -log m) + O(1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_49"><head>b 1 2 - 1 +where B = σ 2 v σ 2 w 2 v + σ 2 a b 2 = -C 2 σ 4 v σ - 2 w (σ 2 v + σ 2 a ) 2 .v +σ 2 a -σ 2 w</head><label>212222222222</label><figDesc>B 2 D 2 σ -2 w b -1 0 2 -1 )(log l -log m) + O(1)with C = 2 π . This simplifies to the desired form.Theorem B.14. Suppose φ = tanh in an FRN.• If σ w = 0, then χ (l) b = σ 2 v χ (L) V φ(σ 2 b ) χ (l) w = σ 2 v χ (L) V φ(σ 2 b )((σ 2 v Vφ(σ 2 b ) + σ 2 a )(l -1) + p (0) ) χ (l) v = χ (L) Vφ(σ 2 b ) χ (l) a = χ (L) . • If σ w &gt; 0, then for l ≥ m ≥ 0, B b (log l -log m) + O(1) log(χ (m) w /χ (l) w ) = A( √ l -√ m) + B w (log l -log m) + O(1) log(χ (m) a /χ (l) a ) = A( √ l -√ m) + B(log l -log m) + O(1) log(χ (m) v /χ (l) v ) = A( √ l -√ m) + B(log l -log m) + O(1)are as in Thm B.13 and B b = B + 1 2 and B w = B -1 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_50"><head>Figure C. 14 :</head><label>14</label><figDesc>Figure C.14: Verification of leading term of Thm C.28 for α = 0.55.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_51"><figDesc>Lemma C.27. c α+1 /c α = 2α + 1.C.7.1 Forward DynamicsTheorem C.28. Suppose we have the nonlinearity φ = ψ 1 . Then p (l) = Θ((1 + σ 2 v σ 2 w /2) l ), with the hidden constant depending on the initial condition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_52"><head>α 1 -</head><label>1</label><figDesc>α -1 α and C = σ 2 a .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_53"><head>Fig. C.</head><figDesc>Fig. C.14 verifies the leading coefficient and the exponent of the leading term. Proof. The difference equation governing the evolution of p is pp = A(p + B) α + C where A = σ 2 v c α σ 2α w , B = σ 2 b /σ 2 w , and C = σ 2 a . Then Lemma C.15 yields the result. Thm C.29 combined with Thm C.28 gives the following result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_54"><head>2πc α J α (θ) = 2 csc θ ∞ 0 dpe p(1+cos θ) csc 2 θ p α 1 2 e</head><label>02</label><figDesc>-p csc 2 θ K 0 (p csc 2 θ) = csc θ ∞ 0 dpK 0 (p csc 2 θ)e p cos θ csc 2 θ p α = sin 2α+1 θ ∞ 0 dxK 0 (x)e x cos θ x α Define L α (θ) = 2πc α J α (θ) csc 2α+1 θ = ∞ 0 dxK 0 (x)e x cos θ x α . Lemma C.32. If α &gt; 1, then L α (θ) = csc 2 θ[(2α -1) cos θL α-1 (θ) + (α -1) 2 L α-2 (θ)].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_55"><figDesc>(x) + x -1 K0 (x))e x cos θ x α = K0 e x cos θ x α | ∞ 0 + K 0 e x cos θ x α-1 | ∞ 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_56"><head>1 2 ⇐⇒ α &gt; 3 4 .σ 2 v σ 2 w + 1 m</head><label>141</label><figDesc>2 ) = Var(ψ 2α-2 (ζ)) is infinite for ζ ∼ N (0, σ 2 ). By Lemma C.37 with β = 2 -2α, ψ 2α-2 (ζ) has finite variance iff β &lt; Theorem B.20. Suppose we have the nonlinearity ψ α in an FRN. If α = 1, then χ (l-m) = χ (l) 1 2 . If α ∈ ( 3 4 , 1), then χ (l-m) = Θ(1)χ (l) (l/(l -m)) R for R = α 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_57"><head>α 1 - 2 1 1 l 3 ) = σ 2 v σ 2α w α 2 c α- 1 K α-1 1 l 1 = α 2 ( 1 1 1σ 2 v σ 2 w / 2 .</head><label>121311121122</label><figDesc>α log l + o(l α 1-α log l)) α-1 + Θ(l α--α ) by Thm C.29 = σ 2 v σ 2α w α 2 c α-1 [K α-1 -1 + Θ(l -2 log l)] + O(l --1 + Θ(l -2 log l) = Rl -1 + Θ(l -2 log l) where R = σ 2 v σ 2α w α 2 c α-1 K α-1 -α)(2α-1) and K 1 = [σ 2 v σ 2α w c α (1 -α)] -α . So χ = χ exp(Rl -1 + Θ(l -2 log l)) χ (l-m) = Θ(1)χ (l) l l -m R as desired. Theorem B.21. If φ = ψ 1 in an FRN, then for l ≥ m ≥ 0, χ (l-m) b = Θ(1)χ (l) B m , χ (l-m) w = Θ(1)χ (l) B l , χ (l-m) v = Θ(1)χ (l) B l , χ (l-m) a = Θ(1)χ (l) B m .whereB = 1 + If φ = ψ α in an FRN, for α &lt; 1, then for l ≥ m ≥ 0, χ (l-m) b = Θ(1)χ (l) l R (l -m) -R-1 , χ (l-m) w = Θ(1)χ (l) l R (l -m) α 1-α -R , χ (l-m) v = Θ(1)χ (l) l R (l -m)α 1-α -R , χ (l-m) a = Θ(1)χ (l) (l/(l -m)) R .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Main Recurrences</figDesc><table><row><cell>Antisymmetric/RRN</cell><cell>Any/FRN</cell></row><row><cell>q</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Summary of Main Dynamics Results. Note that while χ (l) is exponential for ReLU/FRN, the gradients with respect to weight parameters have norms (χ w and χ v ) constant in l (Thm B.21).</figDesc><table><row><cell>χ</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>Note that in practice, to avoid the diverging gradient ψα(x) → ∞ as x → 0, we can use a tempered version Ψα(x) of α-ReLU, defined by Ψα(x) = (x + ) αα on x &gt; 0 and 0 otherwise, for some small &gt; 0. The conclusions of this paper on ψα should hold similarly for Ψα as well.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>Daniely et al.<ref type="bibr" target="#b2">[3]</ref> called the version of Wφ with fixed ρ = 1 the "dual function" of φ.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>A more natural visualization is to graph e (l) -e * versus l -δ * , but because of floating point precision, e (l) -e * doesn't converge to 0, but a small number close to 0, so that the log-log plot wouldn't look like what is expected.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>Our derivations actually apply to all α ∈ ( 1 2 , 1], where at α = 1 2 , the expected norm of the gradient diverges within our mean field formalism. However, at α ≤<ref type="bibr" target="#b2">3</ref> 4 , the variance of the gradient already diverges (Thm B.19), so we cannot expect the empirical values to agree with our theoretical predictions. But in fact, empirically our theoretical predictions seem to form an upper bound on the gradient norms (see Fig. A.1).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>the contour for p (l) is similar, but its slopes are slightly off from the heatmap contours.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>Thanks to <rs type="person">Jeffrey Ling</rs> for early exploration experiments and help with the initial draft. Thanks to <rs type="person">Felix Wong</rs> for offering his wisdom and experience working in statistical physics.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>• If β = 1, then</p><p>Furthermore, for β = -δ = 1, (l) ∼ l -1 if α &gt; 2, (l) ∼ l 1-α if α &lt; 2, and (l) ∼ l δ log l if α = 2.</p><p>Proof. We can unwind the recurrence to get </p><p>Now suppose β = 1. By Lemma C.10, we get</p><p>where the constants hidden inside the Θ are the same in every term of the sum. If α &gt; 1 -δ, then m -δ-α = o(m -1 ), so that l m=1 m -δ-α = Θ(1), and (l) = Θ(l δ ) + (0) Θ(l δ ) = Θ(l δ ).</p><p>On the other hand, if α &lt; 1 -δ, then l m=1 m -δ-α = Θ(l 1-δ-α ). So (l) = Θ(l 1-α ) + (0) Θ(l δ ) = Θ(l 1-α ).</p><p>If α = 1 -δ, then l m=1 m -δ-α = Θ(log l). So (l) = Θ(l δ log l) + (0) Θ(l δ ) = Θ(l δ log l).</p><p>Finally, if β ∈ (0, 1), then</p><p>1-β m 1-β +Θ(m 1-2β ) + e δ 1-β l 1-β +Θ(l 1-2β )</p><p>The case of δ = -1 telescopes, so that the upper and lower constants hidden in Θ can both be taken to be 1.</p><p>Lemma C.12. Suppose for some β &gt; 0, a sequence (l) satisfies</p><p>Then (l) ∼ (βµ log l) -1/β . Now apply Lemma C.9 with</p><p>, with appropriately chosen initial conditions. This yields lim l→∞ v (l) /(l α 1-α log l) ∈ [-K -, -K + ] for every &gt; 0, and there it must be equal to K. We have thus obtained the asymptotic expansion</p><p>Lemma C.14. Suppose a sequence u (l) is governed by the equation</p><p>where α &gt; 1 and</p><p>Proof. Similar to Lemma C.13.</p><p>Lemma C.15. Suppose a sequence u (l) is governed by the equation</p><p>where α ∈ (0, 1). Then</p><p>where</p><p>Proof. u is bounded below by the dynamics v (l) -v (l-1) = A(v (l-1) + B) α and bounded above by the dynamics w (l) -w (l-1) = (A + o(1))(w (l-1) + B) α . By Lemma C.13, both v and w are asymptotic to u (l) ∼ [A(1 -α)l] so that p (l) ∼ l and q (l) ∼ σ 2 w l. The upper bounds are trivial, given Vφ(q) ≤ 1 for any q. We show the lower bounds for any ω &lt; 1.</p><p>For any &gt; 0, define ℵ by φ 2 (ℵ ) = exp(-). Then</p><p>where the second inequality follows from an overestimate of the Pr[z ∈ [-ℵ , ℵ ]] via the mode of N (0, q).</p><p>For any q ≥ q (0) , Vφ(q) is then lower bounded by</p><p>Thus p (l) and q (l) are unbounded with l.</p><p>Furthermore, as q → ∞, the lower bound exp(-) 1 -2ℵ √ 2πq goes to exp(-), for any . Therefore, for any ω &lt; 1, p (l) ≥ ωl and q (l) ≥ σ 2 w ω(l -1) + σ 2 b . Asymptotic expansion. Now we repeat the following to get each successive asymptotic term of p (l) and q (l) : We plug in the current asymptotic form of q (l) into V tanh(q) = 1 -Cq -1/2 + Θ(q -3/2 ) (Lemma C.5), where C = 2/π. Next we take the sum q (l) = l r=1 V tanh(q (r) ), which yields one more term in the asymptotic expansion of p than the last round. We then repeat until we get only constant terms.</p><p>The following exhibits a trace of this procedure, where in the summation step for q (l) , we implicitly apply</p><p>which is what we want. </p><p>with the hidden constant depending on the initial condition. If</p><p>By <ref type="bibr" target="#b1">[2]</ref>, we know that Wψ α (q, qc) = Vψ α (q)J α (c), where J α (c) = J α (arccos c) and</p><p>and any c ∈ (0, 1), even though Vψ α is only defined for α &gt; -1/2. α is Gaussian-integrable, which is precisely when α &gt; -1/2. We can compute J α (1) = Wψ α (q, q)/Vψ α (q) = 1, and</p><p>2 ) . We record these observations as a lemma. Lemma C.30. J α (c) is an increasing and convex function for each α &gt; -1/2 on c ∈ [0, 1]. </p><p>The zeroth Bessel function of the second kind is defined by K 0 (z) = ∞ 1 e -zx (x 2 -1) -1/2 dx. It is one of the fundamental solutions to the homogeneous differential equation x 2 ẏ + x ẏ -x 2 y = 0. The following lemma shows that J α can be expressed in terms of K 0 . Lemma C.31. For any α &gt; -1, J α (θ) = 1 2πcα sin 2α+1 θ ∞ 0 dxK 0 (x)e x cos θ x α</p><p>Proof. Cho and Saul <ref type="bibr" target="#b1">[2]</ref> gave the expression</p><p>where the evaluation terms vanish just like before. Altogether, we have</p><p>As a corollary we get Lemma C.33. Suppose α &gt; 1. Then J α (θ) = cos θJ α-1 (θ) + (α -1) 2 (2α -1) -1 (2α -3) -1 sin 2 θJ α-2 (θ)</p><p>The derivative of J α (θ) turns out to be quite simple.</p><p>Lemma C.34. Suppose α &gt; 0. Then</p><p>Proof. We will prove the first formula. The second follows from chain rule. By Lemma C.31,</p><p>As α + 1 &gt; 1, by Lemma C.33, this is</p><p>Thus Jα (1) = α 2 (2α -1) -1 J α-1 (1) = α 2 (2α -1) -1 for any α &gt; 0 by Lemma C.30. For 1/2 &lt; α ≤ 1, Jα (1) ≥ 1 with equality iff α = 1, and for α = 1/2, Jα (1) = ∞ &gt; 1 by continuity of Jα (c) in α. Because for α &gt; -1/2, J α is increasing and convex on [0, 1] and J α (0) &gt; 0 by Lemma C.30, J α intersects identity at a unique point away from 1 when α ∈ [1/2, 1). We record this as a theorem. Theorem C.35. For α ∈ [1/2, 1), J α (c) = c has two solutions: an unstable solution at 1 ("unstable" meaning Jα (1) &gt; 1) and a stable solution in e * ∈ (0, 1) ("stable" meaning Jα (e * ) &lt; 1).</p><p>This result confirms that pictures presented in Fig. <ref type="figure">C</ref>.17b,c are qualitatively correct, that there are indeed stable fixed points of J α away from 1. Theorem B.17. Suppose φ = ψ 1 . Then in an FRN, e (l) → 1 and  but e ≥ J 1 (e) &gt; e as noted above. Thus by monotone convergence e converges, and e * = 1 is the only possible fixed point.</p><p>Let the content of the bracket on the RHS be ℵ. We have pq</p><p>), but because p is exponentially decreasing, this means = Θ(1) and does not converge to 0 -this is a contradiction. Therefore, = ω(p -1 ), and</p><p>Using Lemma C.14 to upper and lower bound our dynamics, we get that</p><p>Lemma C.36. Let φ be any nonlinearity. Suppose Wφ(r, rd) = Vφ(r)K(d) for some twice differentiable function K(d) independent of q, where K(1) = 1 naturally. Suppose further that • K is nondecreasing.</p><p>Let (l) := e (l) -e * and suppose e (0) &lt; 1. If γ (l) → ∞ and Vφ(q (l) ) → ∞, then (l) → 0 and satisfies</p><p>Proof. First we note that because e * is the only stable fixed point of the dynamics x → K(x), with the basin of attraction [0, 1), we can show e (l) → e * as in the proof of Thm B.11 (using Lemma C.23).</p><p>Write V (l) := Vφ(q (l) ). We first show that e (l) → e * . When l is large,</p><p>If γ (l) is bounded for all l, then e → 0 because p (l) → ∞. Since K(c) &gt; 0 for c ∈ [0, 1] and V (l) → ∞, we have that in the limit l → ∞, lim l→∞ e = 0 = K(lim l→∞ e) = K(0) (by the continuity of K), which is impossible by our assumptions. Thus γ (l) → ∞, and we have lim l→∞ e = K(lim l→∞ e). By our assumptions, e * is the only stable fixed point of K with basin of attraction [0, 1), so this shows that e → e * as desired.</p><p>Now we derive the equation in question. Note that c = e(1 + Θ(γ -1 )) because e * &lt; 1. We use the Taylor expansion K(e * + ) = e * + δ + O( <ref type="formula">2</ref>).</p><p>Theorem B.18. Suppose φ = ψ α for 0 &lt; α &lt; 1 in an FRN. Then e converges to the unique nonunit fixed point e * of J α , and |e * -e (l) | is Θ(l -µ ), where µ = (1 -Jα (e * ))/(1 -α). Additionally, s (l) = Θ(p (l) ) = Θ(l 1/(1-α) ).</p><p>Proof. We apply Lemma C.36. We first check the conditions of the lemma, with K = J α . The following conditions were already verified.</p><p>• J α has a fixed point e * less than but very close to 1, where its slope is υ := Jα (e * ) &lt; 1. • J α is nondecreasing (Lemma C.30). Furthermore, from its integral formula (Eq. ( )), we see easily that J α is smooth at e * &lt; 1.</p><p>We also proved the following</p><p>1-α l 1 1-α (Thm C.29) and γ (l) is asymptotically a constant fraction of p (l) (Lemma C.36), so both go to ∞.</p><p>• Vψ α (q) = c α q α = c α (σ 2 w p + σ 2 b ) α = Θ(l α/(1-α) ), so goes to ∞. (Lemma B.15) Thus, for υ = J(e * ),</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Real-time computation at the edge of chaos in recurrent neural networks</title>
		<author>
			<persName><forename type="first">Nils</forename><surname>Bertschinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Natschlger</surname></persName>
		</author>
		<idno type="DOI">10.1162/089976604323057443</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<idno type="ISSN">0899-7667</idno>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1413" to="1436" />
			<date type="published" when="2004-07">July 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Kernel methods for deep learning</title>
		<author>
			<persName><forename type="first">Youngmin</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning" />
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="342" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on Expressivity</title>
		<author>
			<persName><forename type="first">Amit</forename><surname>Daniely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2253" to="2261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v9/glorot10a.html" />
	</analytic>
	<monogr>
		<title level="m">PMLR</title>
		<imprint>
			<date type="published" when="2010-03">March 2010</date>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<ptr target="http://www.cv-foundation.org/openaccess/content_iccv_2015/html/He_Delving_Deep_into_ICCV_2015_paper.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<ptr target="https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html" />
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Random synaptic feedback weights support error backpropagation for deep learning</title>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cownden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><forename type="middle">B</forename><surname>Tweed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><forename type="middle">J</forename><surname>Akerman</surname></persName>
		</author>
		<idno type="DOI">10.1038/ncomms13276</idno>
		<ptr target="https://www.nature.com/articles/ncomms13276" />
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<idno type="ISSN">2041-1723</idno>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13276</biblScope>
			<date type="published" when="2016-11">November 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Exponential expressivity in deep neural networks through transient chaos</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhaneil</forename><surname>Lahiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maithreyi</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3360" to="3368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">On the expressive power of deep neural networks</title>
		<author>
			<persName><forename type="first">Maithra</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05336</idno>
		<idno>arXiv: 1606.05336</idno>
		<ptr target="http://arxiv.org/abs/1606.05336" />
		<imprint>
			<date type="published" when="2016-06">June 2016</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><surname>Sohl-Dickstein</surname></persName>
		</author>
		<ptr target="https://openreview.net/pdf?id=H1W1UN9gg" />
		<title level="m">Deep Information Propagation</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
