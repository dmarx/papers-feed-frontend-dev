<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Stereo4D: Learning How Things Move in 3D from Internet Stereo Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Linyi</forename><surname>Jin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Richard</forename><surname>Tucker</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhengqi</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Fouhey</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Noah</forename><surname>Snavely</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">equal contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aleksander</forename><surname>Holynski</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">equal contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Google</forename><surname>Deepmind</surname></persName>
						</author>
						<title level="a" type="main">Stereo4D: Learning How Things Move in 3D from Internet Stereo Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1C477668F198BD084C2F6AF4ABF3D19D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Figure <ref type="figure">1</ref>. There is currently no scalable source of data for real-world, ground truth 3D motion paired with video. We present a framework for mining such data from existing stereoscopic videos on the Internet, in the form of 3D point clouds with long-range world-space trajectories. Our framework fuses and filters camera poses, dense depth maps, and 2D motion trajectories to produce high-quality, pseudo-metric point clouds with long-term 3D motion trajectories, pictured above, for hundreds of thousands of video clips. We show how this data is useful in learning a model that reasons about both 3D shape and motion in imagery.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Simultaneously predicting and understanding geometry and motion-that is, dynamic 3D content-from images is a fundamental building block for computer vision, with applications ranging from robotic interaction and scene reconstruction to novel view synthesis of dynamic scenes. While recent work has made remarkable progress in predicting static 3D structure from images <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b98">99,</ref><ref type="bibr" target="#b104">105]</ref>, modeling realworld 3D motion-people gesturing, balls bouncing, leaves rustling in the wind-remains a critical unsolved challenge for building truly general models of the visual world.</p><p>Recent breakthroughs in AI, from large language models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b88">89]</ref> to image generation <ref type="bibr" target="#b72">[73]</ref> and static 3D reconstruction <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b98">99,</ref><ref type="bibr" target="#b104">105]</ref>, demonstrate a consistent pattern: large amounts of high-quality, realistic training data and scalable architectures enable dramatic performance improvements. In the realm of 3D reasoning, prior works <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b98">99,</ref><ref type="bibr" target="#b103">104]</ref> have shown the value of large-scale training data for strong zero-shot generalization within singleview or two-view static scene settings. But applying this same formula to dynamic 3D scenes (i.e. moving 3D structure) requires a corresponding large-scale dataset consisting of diverse visual content paired with corresponding ground-truth 3D motion trajectories. Obtaining such data presents unique challenges. While there are synthetic datasets <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b114">115]</ref>, these often fail to capture the distribution of real-world content and the nuanced patterns of real-world motion. Traditional approaches to gathering real motion data, such as motion capture systems or multi-view camera arrays <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b42">43]</ref> are accurate, but difficult to scale and limited in the diversity of scenes they can capture.</p><p>We identify online stereoscopic fisheye videos (often referred to as VR180 videos) as an untapped source of such data. These videos, designed to capture immersive VR experiences, provide wide field-of-view stereo imagery with a standardized stereo baseline. We present a pipeline that carefully combines state-of-the-art methods for stereo depth estimation and video tracking along with structure-frommotion methods optimized for dynamic scenes. By combining our system with careful filtering and quality control, we show that we can extract over 100K video sequences, each containing high-quality 3D point clouds with per-point long-term trajectories (see Fig. <ref type="figure">1</ref>), as well as all other intermediate quantities: depth maps, camera poses, images, and 2D correspondences. We additionally show the utility of the dataset by training DynaDUSt3R, an extension to DUSt3R that can predict high-quality 3D structure and motion from challenging image pairs. Our contributions include: (1) a framework for obtaining real-world, dynamic, and pseudo-metric 4D reconstructions and camera poses at scale from existing online video; (2) DynaDUSt3R, a method that takes a pair of frames from any real-world video, and predicts a pair of 3D point clouds and the corresponding 3D motion trajectories that connect them in time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>2D and 3D motion data. There has been tremendous progress on the task of motion estimation from images and videos, and in particular for 2D image-space correspondence estimation. Most state-of-the-art methods use neural networks trained on ground truth data to predict these correspondences directly from images. While these approaches require large training datasets, synthetic data from graphics engines <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b82">83,</ref><ref type="bibr" target="#b114">115]</ref> has proven surprisingly effective at generalizing to real-world data, likely because the core task, low-level textural correspondence, is similar between the two domains.</p><p>However, the same cannot be said for 3D motion estimation, since predicting both 3D structure and motion is usually more ambiguous and can require specific prior knowledge about the real world and how it moves. To help address this domain gap, a number of real-world datasets have been proposed. The KITTI <ref type="bibr" target="#b26">[27]</ref> and Waymo <ref type="bibr" target="#b85">[86]</ref> datasets in-clude real-world autonomous driving sequences with stereo and motion annotations derived from LiDAR and odometry information, but only focus on the relatively closed domain of street scenes, whereas our data depicts more diverse in-the-wild scenarios. A number of annotated smallerscale datasets, such as TAPVid <ref type="bibr" target="#b15">[16]</ref>, TAPVid3D <ref type="bibr" target="#b45">[46]</ref>, and Dycheck <ref type="bibr" target="#b24">[25]</ref>, have been proposed, primarily serving as evaluation datasets for benchmarking depth estimation, 3D reconstruction, and 3D motion estimation approaches. WSVD <ref type="bibr" target="#b96">[97]</ref> and NVDS <ref type="bibr" target="#b99">[100]</ref> are stereo video datasets that include disparity maps derived from optical flow. While their source content is similar, our method provides richer 3D annotations beyond time-independent disparity maps, such as 3D camera parameters and long-term 3D motion tracks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Static and dynamic scene reconstruction</head><p>The problem of reconstructing a static 3D scene has been studied for decades. Traditional 3D reconstruction methods tackle this problem by first estimating camera parameters via Structure-from-Motion (SfM) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b81">82,</ref><ref type="bibr" target="#b86">87]</ref> or SLAM <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b59">60]</ref>. Dense scene geometry can then be estimated through Multi-view Stereo (MVS) <ref type="bibr">[10, 22-24, 36, 77, 106, 107]</ref> followed by surface reconstruction algorithms <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b39">40]</ref>. More recently, deep neural networkbased approaches have shown promising results in improving camera localization accuracy or scene reconstruction through intermediate representations such as depth maps <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b87">88,</ref><ref type="bibr" target="#b90">91,</ref><ref type="bibr" target="#b92">93]</ref>, radiance fields <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b101">102]</ref>, or 3D scene coordinates <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b98">99,</ref><ref type="bibr" target="#b110">111]</ref>. However, these methods assume the input images to be observations of a static environment, and therefore produce inaccurate geometry and camera poses for dynamic scenes.</p><p>Reconstructing dynamic scenes is more challenging since scene and object motions violate the multi-view constraints used to reconstruct static scenes. As a result, many prior works require RGBD input <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b61">62]</ref> or only recover sparse geometry <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b80">81,</ref><ref type="bibr" target="#b95">96]</ref>. Several recent works tackle this problem from monocular input through video depth maps <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b112">113,</ref><ref type="bibr" target="#b113">114]</ref>, time-varying radiance fields <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b97">98]</ref>, or generative priors <ref type="bibr" target="#b102">[103]</ref>.</p><p>Monocular and stereo depth. Recent works on singleview depth prediction have shown strong zero-shot generalization to in-the-wild domains by training deep neural networks on diverse RGBD datasets <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b103">104,</ref><ref type="bibr" target="#b104">105,</ref><ref type="bibr" target="#b107">108,</ref><ref type="bibr" target="#b108">109]</ref>. However, producing temporally consistent and metric depth from video is still challenging. To tackle this, recent works use test-time optimization <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b113">114]</ref> or end-to-end learning with temporal attention <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b99">100]</ref>. On the other hand, stereo images or videos are also popular input modalities for obtaining reliable metric depth maps, and various stereo matching algorithms have been proposed <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b84">85,</ref><ref type="bibr" target="#b93">94,</ref><ref type="bibr" target="#b100">101,</ref><ref type="bibr" target="#b109">110,</ref><ref type="bibr" target="#b111">112]</ref>. Building on these advancements, . Data processing pipeline. Our method starts with VR180 (wide-angle, stereoscopic) videos, and estimates metric stereo depth, 2D point tracks, and camera poses. These quantities allow the tracks to be lifted to 3D where they are filtered and denoised to produce world-space, metric 3D point trajectories.</p><p>our method bridges ideas from monocular video depth estimation and stereo video processing. We use a light-weight optimization step and extend them to stereo inputs for more consistent motion estimation in metric space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Creating a dataset of 4D scenes</head><p>A core contribution of this work is a pipeline for extracting high-quality, pseudo-metric, 3D data from online stereoscopic fisheye videos (known as VR180 videos). Highresolution, wide field of view VR180 videos can be found readily online. We show that this data is ideal for deriving rich dynamic 3D information that can power models for predicting geometry and motion from imagery.</p><p>Concretely, each instance of data starts as an N frame stereo video consisting of left-right image pairs I i and I ′ i indexed by frame index i ∈ [1, N ]. We convert these stereo pairs to a dynamic 3D point cloud with K points in a world-space coordinate frame, where each point, indexed by j ∈ [1, K], has a time-varying position p j i . As part of the process of generating this dynamic point cloud, we also extract a number of auxiliary quantities: (1) per-frame camera extrinsics, (the left camera's position c i and orientation R i ), (2) rig calibration for the stereo video giving the position c r and orientation R r of the right camera relative to the left camera, and (3) a per-frame disparity map D i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data Processing Pipeline</head><p>At a high level, our pipeline for converting a stereoscopic video into a dynamic point cloud involves estimating camera poses, stereo disparity, and 2D tracks, fusing these quantities into a consistent 3D coordinate frame, and performing several filtering operations to ensure temporal consistent, high-quality reconstructions (Fig. <ref type="figure" target="#fig_0">2</ref>). In this section, we describe in detail the key components of this process.</p><p>SfM. We start by processing the sequence of stereo frames I i ↔ I ′ i to produce camera pose estimates (c i , R i ). We first use a SLAM method to divide the video into shots, as in <ref type="bibr" target="#b115">[116]</ref>. For each shot, we run an incremental SfM algorithm similar to COLMAP <ref type="bibr" target="#b75">[76]</ref>. We initialize the stereo rig calibration (c r , R r ) to a rectified stereo pair with baseline 6.3cm, but optimize for the calibration in bundle adjustment. In practice, we found that the exact stereo pair orientation can vary significantly from its nominal configuration and that optimizing the rig was critical for good results.</p><p>Depth Estimation. We next estimate a per-frame disparity map, operating on each frame independently. In particular, we use the estimated camera rig calibration c r , R r to create rectified stereo pairs from the stereo fisheye video and estimate the per-frame disparity D i with RAFT <ref type="bibr" target="#b82">[83,</ref><ref type="bibr" target="#b83">84,</ref><ref type="bibr" target="#b89">90]</ref>.</p><p>3D Track Estimation and Optimization. We extract longrange 2D point trajectories using BootsTAP <ref type="bibr" target="#b16">[17]</ref>. Using the camera poses c i , R i and disparity maps D i , we unproject these tracked points into 3D space, turning each 2D track j into a 3D motion trajectory p j 1 , . . . , p j N across all frames. In general, each point will usually only be tracked in a subset of frames, but for simplicity, we describe the formulation as if all points are always visible. Moreover, since subsequent steps are done independently per-track, we drop the superscript j in future references.</p><p>Since stereo depth estimation is performed per-frame, the initial disparity estimates (and therefore, the 3D track positions) are likely to exhibit high-frequency temporal jitter. To compensate for potentially inconsistent disparity estimates, we formulate an optimization strategy that solves for a per-frame scalar offset δ i ∈ R that moves each point p i along the ray from camera location c i to p i at frame i. This ray is denoted r i = (p i -c i )/||p i -c i ||, and we refer to the updated location as p ′ i = p i + δ i r i .</p><p>To ensure static points remain stationary while moving tracks maintain realistic, smooth motion, avoiding abrupt depth changes frame by frame, we design an optimization objective comprising three terms: a static loss L static , a dynamic loss L dynamic , and a regularization loss L reg . The static loss L static minimizes jitter by encouraging points to remain close to each other in world space:</p><formula xml:id="formula_0">L static = N i=1 N j=1 ∥p ′ i -p ′ j ∥ 2 N ′2 p (1)</formula><p>where</p><formula xml:id="formula_1">N ′ p = N i=1 ∥p ′ i ∥/N</formula><p>is a normalizing factor. The dynamic loss term reduces jitter by minimizing acceleration along the camera ray through a discrete Laplacian operator:</p><formula xml:id="formula_2">L dynamic = N i=1 ∆∈W p ′ i+∆ -2p ′ i + p ′ i-∆ ⊤ r i 2<label>(2)</label></formula><p>where the acceleration along the ray is calculated over multiple window sizes W = {1, 3, 5}.</p><p>The two loss terms are weighted by a track-dependent function, σ(m), where m is a measure of the motion magnitude of the track. Motion is measured in 2D rather than 3D because distant points can appear to have a larger 3D motion due to noise amplification at low disparities. Specifically, we project the 3D motion trajectory between time i -w o and the current time i into 2D image-space at time i, and calculate the track's motion magnitude m as the 90 th percentile of the track's trail length across all frames. The track trail length for a frame is measured by projected 3D points along the track to the current frame as if the camera is static in a window of w o = 16 frames,</p><formula xml:id="formula_3">m = Percentile 90 i=1:N max w=1:wo ∥π i (p i ) -π i (p i-w )∥ (3)</formula><p>where π i (•) ∈ R 2 gives the projected pixel location of a 3D point on camera c i 's image plane. The weighting function σ(m) is defined as σ(m) = 1 1+exp(m-m0) where m 0 = 20. Finally, to encourage faithfulness to the originally estimated disparities, we regularize the displacements in disparity space:</p><formula xml:id="formula_4">L reg = λ reg T i=1 1 δ i + ∥p i -c i ∥ - 1 ∥p i -c i ∥ 2 ,<label>(4)</label></formula><p>where use of disparity space reflects the fact that the measurements themselves originate as disparities. Practically, the impact of the use of disparity is that larger deviations are tolerated at more distant points, where depth is intrinsically more uncertain.</p><p>The full objective function is We set λ reg = 10 -4 and optimize Eqn. 5 using Adam with a learning rate of 0.05 for 100 steps. The effect of track optimization is shown in Fig. <ref type="figure" target="#fig_1">3</ref>. The optimized motion is smoother and does not contain high frequency noise.</p><formula xml:id="formula_5">min {δi} N i=1 σ(m)L static + (1 -σ(m))L dynamic + L reg . (<label>5</label></formula><formula xml:id="formula_6">)</formula><p>Implementation details. Shot-selection. Rather than work with the full video, we break the footage into discrete, trackable shots using ORB-SLAM2's stereo estimation mode <ref type="bibr" target="#b60">[61]</ref> following <ref type="bibr" target="#b117">[118]</ref>. Field of View. While estimating pose, we use a 140 • FoV fisheye format, which we found to capture more of the (usually static) background and less of the (often dynamic) foreground, yielding more reliable camera poses. Stereo Confidence Checks. We discard pixels where the y-component of RAFT flow is more than 1 pixel (since rectified stereo pairs should have perfectly horizontal motion) and where the stereo cycle consistency error is more than 1 pixel (since such pixels are unreliable). Dense 2D tracks. To get dense tracks, we run BootsTAP with dense query points: for every 10th frame, we uniformly initialize 128 × 128 query points on frames of resolution 512 × 512. We then prune redundant tracks that overlap on the same pixel. Drifting tracks. Since 2D tracks can drift on textureless regions, we discard moving 3D tracks that correspond to certain semantic categories (e.g., "walls", "building", "road", "earth", "sidewalk"), detected by DeepLabv3 <ref type="bibr" target="#b12">[13]</ref> on ADE20K classes <ref type="bibr" target="#b115">[116,</ref><ref type="bibr" target="#b116">117]</ref>.</p><p>Filtering details. A fraction of the video clips that are processed may be unsuitable because they either (1) are not videos, and are entirely static images, (2) contain crossfades, or (3) have text or other synthetic graphics. To discard text and title sequences, we avoid creating video clips from the start and ends of the source videos. We identify cross-fades by running SIFT <ref type="bibr" target="#b56">[57]</ref> matching through the video at multiple temporal scales and discarding video clips with static camera but with fewer than 5 SIFT matches between frames that are 5 seconds apart.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Stereo4D Dataset</head><p>Fig. <ref type="figure" target="#fig_2">4</ref> illustrates a subset of videos and reconstructions from a dataset processed with the above pipeline, encompassing more than 100K clips capturing everyday scenes and activities. To visualize the range of content, we used an automatic captioning system to generate captions for the dataset and created a word cloud (Fig. <ref type="figure" target="#fig_3">5</ref>) highlighting the most frequently observed objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Learning a prior on how things move</head><p>We now describe our method for predicting dynamic 3D point clouds from pairs of images, and how we train it with our Stereo4D data. Our model is based on DUSt3R <ref type="bibr" target="#b98">[99]</ref>, which predicts a 3D point cloud for a static scene from images. Given two input images, it uses a ViT-based architecture <ref type="bibr" target="#b17">[18]</ref> to extract image features and uses a transformerbased decoder to cross-attend features from two images, and then use a downstream pointmap decoder to output pointmaps for the two images, aligned in the first image's coordinate frame.</p><p>DynaDUSt3R model. While DUSt3R focuses on static scene structure, our proposed DynaDUSt3R method, illustrated in Fig. <ref type="figure" target="#fig_4">6</ref>, works with dynamic scenes by adding a motion head that predicts how the points move between two frames. As input, DynaDUSt3R accepts two images: I 0 at time t 0 , and I 1 at time t 1 (where t 0 and t 1 may be seconds apart). It also accepts an intermediate query time t q ∈ [0, 1]; the motion head is asked to predict 3D scene flow from the two input frames to query time t q , as described below. Like DUSt3R, DynaDUSt3R begins by encoding the images with a shared ViT and cross-attention decoder, producing global features G 0 and G 1 for I 0 and I 1 , respectively. Each feature embedding can be converted into geometry using DUSt3R's point head: e.g., for image I 0 , the point head produces a pointmap P 0 ∈ R H×W ×3 representing the geometry at time t 0 , as well as a point confidence map C 0 point ∈ R H×W . Each point cloud is predicted in the coordinate frame of I 0 , but at the time of its respective image (so, the two point clouds may differ due to scene motion).</p><p>We add a separate motion head in parallel to the original point head, to predict a map of 3D displacement vectors (that is, a scene flow map, which we refer to as a motion map) for each pointmap. The motion map should displace each input frame to an intermediate time t q ∈ [0, 1], where t q = 0 corresponds to t 0 , the time of I 0 , and similarly for t q = 1, t 1 , and I 1 . The motivation for predicting motion to an intermediate time (inclusive of the endpoints) is twofold: first, it leads to a more general prediction task where we can predict a full motion trajectory between two frames, and second, it allows us to use partial ground truth 3D trajecto- ries as supervision; not all trajectories may span all the way from t 0 to t 1 , but may span through some intermediate time.</p><p>For each image I v (with v ∈ {0, 1}), the network outputs a 3D motion map M v→tq for the corresponding pointmap from t v to t q with corresponding motion confidence map C v mot ∈ R H×W . This prediction is based on the global feature G v as well as an embedding of the query time emb(t q ). We use positional embedding <ref type="bibr" target="#b94">[95]</ref> to encode time t q to a 128-D vector and inject it to the motion features in the motion head via linear projection layers. Training objective. We use the same confidence-aware scale-invariant 3D regression loss as in DUSt3R. We first normalize both the predicted and ground truth pointmaps using scale factors z = norm(P 0 , P 1 ) and z = norm( P0 , P1 ), respectively (where a bar, e.g., P0 , denotes a ground truth quantity, and where 'norm' computes the average distance between a set of points and the world origin). We scale the motion maps with the same scales z and z. Following DUSt3R, we compute a Euclidean distance loss on the pointmap, setting</p><formula xml:id="formula_7">L point to v∈{0,1} i∈D v C v point,i 1 z P v i - 1 z Pv i -αp log C v point,i<label>(6)</label></formula><p>where D v corresponds to the valid pixels where ground truth is defined and α p is a weighting hyperparameter. We additionally compute a Euclidean distance loss on the position after motion, which encourages the network to learn correct displacements. This loss L motion is defined as</p><formula xml:id="formula_8">v∈{0,1} i∈D v C v mot,i 1 z P v→tq i - 1 z Pv→tq i -αm log C v mot,i ,<label>(7)</label></formula><p>where</p><formula xml:id="formula_9">P v→tq i = P v i + M v→tq i .</formula><p>Training details. We initialize our network with DUSt3R weights and initialize the motion head with the same weights as the point head. We finetune for 49k iterations, with batch size 64, learning rate 2.5e-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We conduct a series of experiments to validate the effectiveness of our proposed data and techniques. First, we evaluate our proposed real-world Stereo4D data mined from VR180 videos on the DynaDUSt3R task. In particular, we compare models that are individually trained with our realworld data and with synthetic data, and we show that our data enables model learning more accurate 3D motion priors (Sec. 5.1). Second, we show that our trained model that adapts DUSt3R has strong generalization to in-the-wild images of dynamic scenes, and enables accurate predictions of underlying geometry (Sec. 5.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">3D motion prediction</head><p>Baselines and metrics. To evaluate the efficacy of our data paradigm on motion prediction, we primarily compare Dy-naDUSt3R trained on Stereo4D to the same network trained on a synthetic dataset, PointOdyssey <ref type="bibr" target="#b114">[115]</ref>. PointOdyssey contains ground truth depth maps and 3D motion tracks rendered from an animation engine; we supervise the model with this data using the same hyperparameter settings as described above. During inference, given two video frames sampled from a video of a dynamic scene, we compare 3D end-point-error (EPE) between ground truth and predicted 3D motion vectors. We also compute the fraction of 3D points that have motion within 5 cm and 10 cm compared to ground truth (δ 0.05 3D , δ 0.10 3D ), following <ref type="bibr" target="#b91">[92,</ref><ref type="bibr" target="#b97">98]</ref>. Since our model outputs point clouds up to an unknown scale, we align each prediction with the ground truth through a median scale estimate. We evaluate models trained on each of these two data sources on a held-out Stereo4D test-set, as well as on Arial Digital Twin (ADT) <ref type="bibr" target="#b63">[64]</ref> data containing scene motion, processed by the TapVid3D benchmark <ref type="bibr" target="#b45">[46]</ref>. As test data, we randomly sample pairs of frames that are at most 30 frames apart from both Stereo4D and ADT.  Quantitative results. We show numerical results for twoframe 3D motion prediction in Tab. 1. DynaDUSt3R trained on real-world data achieves better generalization and outperforms the baseline trained on PointOdyssey significantly across all metrics. This suggests the potential of our data for more effective learning of real-world 3D motion priors.</p><p>Qualitative results. Fig. <ref type="figure" target="#fig_6">7</ref> shows example results for three dynamic scenes in our Stereo4D test set, including visualizations of 3D point clouds and motion tracks. Dy-naDUSt3R produces accurate 3D shape and motion tracks over the timespan defined by the two input images. Despite the inputs being two sparse images, our architecture enables querying intermediate motion states, resulting in continuous and potentially non-linear motion trajectories.</p><p>We also qualitatively compare predicted 3D motion tracks between DynaDUSt3R networks trained on Stereo4D and on PointOdyssey, by projecting their predicted 3D motion vectors into 2D image space. Fig. <ref type="figure" target="#fig_7">8</ref> and Fig. <ref type="figure" target="#fig_8">9</ref> show comparisons on the Stereo4D and ADT test set respectively. DynaDUSt3R trained on Stereo4D produces more accurate 3D motion estimates for both static and moving objects. For instance, DynaDUSt3R trained on PointOdyssey produces non-zero motion for the stationary street banner and erroneous motions for the walking people in Fig. <ref type="figure" target="#fig_7">8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Structure prediction</head><p>Baseline and metrics. We evaluate the quality of predicted 3D structure by comparing depth maps predicted by DUSt3R <ref type="bibr" target="#b98">[99]</ref>, MonST3R <ref type="bibr" target="#b110">[111]</ref>, and DynaDUSt3R trained on Stereo4D or PointOdyssey. DUSt3R is designed to predict aligned point clouds from two input images of a static scene. MonST3R, a concurrent approach, extends DUSt3R to handle dynamic scenes by predicting time-varying point clouds without modeling motion.</p><p>We evaluate predicted depth accuracy on the Bonn [63] dataset and our held-out test set, where we sample two views that are 30 frames apart from a video. Since we focus on the two-frame case, we do not apply any postoptimization to the network outputs. In addition, since all methods predict 3D point clouds in the coordinate frame of the first image, we include the two points clouds predicted from both input frames in our evaluation. We use standard depth metrics, including absolute relative error (Abs Rel) and percentage of inlier points δ &lt; 1.25, following prior work <ref type="bibr" target="#b99">[100,</ref><ref type="bibr" target="#b110">111]</ref>. We use the same median alignment as before to align the predicted depth map with the ground truth.</p><p>Quantitative comparisons. We show quantitative comparisons of depth predicted by different methods in Tab. 2. Dy-naDUSt3R trained on Stereo4D outperforms all other baselines by a large margin. In particular, we demonstrate improved depth prediction on the unseen Bonn dataset.</p><p>Qualitative comparisons. We provide additional visual comparisons in Fig. <ref type="figure" target="#fig_9">10</ref>, where we visualize ground truth 3D point clouds and predictions from our approach and the other three baselines at different input time steps. DuST3R predicts inaccurate depth relationships for the two moving people, while MonsT3R and DynaDUSt3R trained on PointOdyssey both predict distorted scene geometry. In contrast, our model trained on Stereo4D produces 3D structure that most closely resembles the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion and Conclusion</head><p>Limitations. Our data curation pipeline and trained model have limitations. The quality of the long-range 3D motion</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stereo4D</head><p>Bonn <ref type="bibr" target="#b62">[63]</ref> Method Abs Rel↓ δ &lt; 1.25↑ Abs Rel↓ δ &lt; 1.25↑ </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.">More qualitative examples on track optimization</head><p>In Fig. <ref type="figure" target="#fig_15">16</ref>, we illustrate estimated tracks for a video sequence featuring a forward-moving camera and vehicles driving towards the camera. Our initial 3D tracks derived directly from RAFT depth, BootsTAP 2D tracks, and SfM camera pose, show significant jitter for both dynamic (vehicle) and static (ground) points. However, after applying our track optimization, the ground points produce stable, static tracks, and vehicle tracks become smooth and coherent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Dataset curation details 9.1. Equirectangular videos</head><p>The raw videos that we collect (see examples in Fig. <ref type="figure" target="#fig_14">15</ref>) are natively stored in a cropped equirectangular format, which differs from a full 360 • equirectangular projection as the horizontal field of view of the cropped format typically spans 180 • -half of a full sphere. These videos often contain metadata specifying the horizontal and vertical field of view. For instance, metadata for a typical video might specify start yaw = -90.0 • , end yaw = 90.0 • , start tilt = -90.0 • , end tilt = 90.0 • ; Since many VR180 videos are designed for an immersive VR experience, they are typically viewed with headsets. Hence, the baseline between the left and right  cameras typically closely matches the average human eye distance of 6.3 cm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2.">SfM</head><p>For ease of processing with standard 3D computer vision pipelines, and to benefit from the wide FoV of the input videos, we convert the videos from their native format (equirectangular projections) to a fisheye format for camera pose estimation. We use a 140 • field of view for these fisheye-projected videos, because many equirectangular videos have a black fade-out/feathering/vignetting effect applied at the boundary, as shown in Fig. <ref type="figure" target="#fig_14">15</ref>. We found that using wider FoV frames significantly improves camera pose estimation in dynamic scenes. When using narrow FoV projections, dynamic objects are more likely to occupy a large fraction of the frame; when these dynamic foreground objects are rich in features, they can confuse camera tracking algorithms, leading to inaccurate camera poses that track the dynamic object rather than producing true camera motion with respect to the environment. In contrast, wide-angle fisheye videos capture more background regions, which tend to have stable features for tracking, yielding more reliable camera poses. We first use ORB-SLAM2's stereo estimation mode <ref type="bibr" target="#b60">[61]</ref> to identify trackable sequences within the videos, utilizing the method devised by Zhou et al. to divide videos into discrete, trackable shots <ref type="bibr" target="#b117">[118]</ref>. For each given shot, consisting of frames (I i , . . . , I n ), we estimate camera poses and rig calibration via an incremental global bundle adjustment algorithm similar to COLMAP <ref type="bibr" target="#b75">[76]</ref>. We initialize the stereo rig calibration to be that of a rectified stereo pair with baseline 6.3 cm, but optimize for the calibration as part of the bundle adjustment process, as in practice the stereo rig can vary significantly from its nominal configuration. This process yields a camera position c i and orientation R i for each frame i (defined as the pose of the left camera), and a position c r and orientation R r for the right camera relative to the left (assumed to be constant throughout the shot).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.3.">Depth estimation</head><p>Depth estimation is first performed on a per-frame basis, with disparity maps computed independently for each frame.</p><p>We use the estimated camera rig calibration c r , R r to rectify the original high resolution equirectangular video frames, ensuring that (1) the left and right views have centered principal points, (2) are oriented perpendicular to the baseline, and (3) pointing in a parallel direction. We then convert the equirectangular videos to perspective projections for downstream predictions.</p><p>Disparity is estimated from optical flow <ref type="bibr" target="#b83">[84,</ref><ref type="bibr" target="#b89">90]</ref> between the rectified left and right frames. The x-component of the optical flow is used as disparity, which is converted to met-ric depth using:</p><formula xml:id="formula_10">Depth = baseline × f disparity .<label>(8)</label></formula><p>Here baseline = 0.063m, and f is the frame's focal length.</p><p>Outlier Rejection. Several criteria are applied to filter out unreliable pixels: Inconsistency between left and right eyes: Disparity is rejected if the optical flow fails a cycleconsistency check with an error exceeding one pixel. Depth values exceeding 20 meters are considered invalid. Estimating accurate depth beyond a certain range requires sub-pixel disparity estimation, and therefore the resulting depths are usually very noisy. Negative flow values that shouldn't occur, but can, often due to errors in textureless regions. Large vertical flow: pixels with a y-component of flow exceeding one pixel are removed (as in our rectified stereo pairs correspondences should have the same y-value, and violating that epipolar constraint indicates uncertain matches). Occlusion boundaries: Depth gradients exceeding a threshold (threshold = 0.3) indicate occlusion boundaries and are rejected. For a pixel location (x, y), depth gradients are computed as:</p><formula xml:id="formula_11">grad x = |Depth(x + 1, y) -Depth(x -1, y)|, grad y = |Depth(x, y + 1) -Depth(x, y -1)|.</formula><p>Pixels are rejected if grad x &gt; threshold × Depth(x, y) or grad y &gt; threshold × Depth(x, y).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.4.">2D tracks</head><p>We extract long-range 2D point trajectories using Boot-sTAP <ref type="bibr" target="#b16">[17]</ref>. We run tracking on the left-eye video only. For every 10 frames, we uniformly initialize query points on image with stride 4. We then remove duplicated queries if earlier tracks fall within 1 pixel of a query point.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Initial 3D tracks Stereo video</head><p>Optimized 3D tracks When converting the equirectangular videos to perspective projections, we use two FoVs: 60 • and 120 • . Both perspective videos are set to a resolution of 512×512, the maximum supported by BootsTAP. The 60 • projection offers a higher sampling rate in scene units, which improves the accuracy of depth estimation and 2D tracks when measured in meters. Additionally, it has smaller perspective distortion near the image boundaries. In contrast, the 120 • projection provides wider coverage, ensuring longer 2D tracks across the videos. This trade-off allows us to balance data quality with spatial coverage for downstream tasks, e.g. DynaDUSt3R.</p><p>We take the union of the 3D tracks derived from each of these videos for DynaDUSt3R training supervision.</p><p>10. DynaDUSt3R training details.</p><p>Dataloader. During training, we randomly sample two frames from the training videos that are at most 60 frames apart, at times t 0 and t 1 , (t 0 &lt; t 1 ). Additionally, we also sample one auxiliary frame in between, at time t aux , t 0 &lt; t aux &lt; t 1 , for additional track supervision between the two input frames. During training, we add data augmentation by applying random crops and color jitter to the input images and cropping the ground truth pointmap and motionmap accordingly.</p><p>Training. The network takes input the two RGB images as well as query times t q = {0, 1, taux-t0 t1-t0 } and predicts the pointmaps for the two input views and motionmaps for each query t q . We supervise the network with losses defined in Eqn. 6 and 7. We initialize our network with the DUSt3R weights and initialize the motion head with the same weights as the point head. We finetune for 49k iterations with batch size 64, learning rate 2.5 × 10 -5 , and optimized by Adam with weight decay 0.95.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2</head><label>2</label><figDesc>Figure2. Data processing pipeline. Our method starts with VR180 (wide-angle, stereoscopic) videos, and estimates metric stereo depth, 2D point tracks, and camera poses. These quantities allow the tracks to be lifted to 3D where they are filtered and denoised to produce world-space, metric 3D point trajectories.</figDesc><graphic coords="3,155.78,289.83,170.57,126.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Effect of track optimization. Comparing motion trajectories before and after track optimization, we see that optimization resolves the high frequency jitter along camera rays, affecting both static and dynamic content. After optimization, static content has static tracks, and dynamic tracks are less noisy.</figDesc><graphic coords="4,441.02,62.67,126.71,112.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Diverse motion: Stereo4D captures a wide variety of types of moving objects, from swimming fish, to walking pedestrians, moving vehicles, and a farmer sowing seeds. It includes source videos captured with both stationary (left) and moving (right) cameras.</figDesc><graphic coords="5,420.93,166.13,185.65,92.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Diverse scene content: A word cloud of captioned frames from our dataset shows our data is diverse, including a variety of common objects seen in videos.</figDesc><graphic coords="5,58.50,310.38,236.25,113.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. DynaDUSt3R architecture. Given two images (I0, I1) of a dynamic scene and a desired target time tq, the images are passed through a ViT encoder and transformer decoder. The resulting features are processed by (1) a pointmap head that predicts 3D points in the coordinate frame of I0, and (2) a 3D motion head that predicts the motion of all points to the target time tq. A double outline indicates a new component compared to DUSt3R.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>with weight decay 0. 95 .</head><label>95</label><figDesc>During training, we randomly sample pairs of video frames that are at most 60 frames apart. The weight for the confidence loss in Eqn 6-7 is α m = α p = 0.2. The model is trained on tracks extracted from both 60 • FoV videos for (higher quality) and 120 • FoV videos for (larger coverage).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Testing on held out examples from Stereo4D. We visualize image pairs and corresponding dynamic 3D point clouds predicted by DynaDUSt3R. It recovers accurate 3D shape and complex scene motion for objects such as people breakdancing and cows walking.</figDesc><graphic coords="7,313.65,72.05,335.62,187.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Qualitative comparisons, 3D motion on the Stereo4D. We compare variants of DynaDUSt3R trained on different data sources. The PointOdyssey-trained model incorrectly predicts significant 3D motion on static elements such as the building wall and the banners near the streetlight, while the Stereo4D-trained model correctly predicts these elements as stationary. The Stereo4D model also makes more precise motion predictions for dynamic objects, such as humans with large movements (bottom row).</figDesc><graphic coords="7,66.47,326.66,130.74,130.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Qualitative comparisons of predicted 3D motion on ADT [64]. DynaDUSt3R trained on Stereo4D produces more accurate 3D motion compared to training on PointOdyssey.</figDesc><graphic coords="7,202.70,326.66,130.74,130.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Qualitative comparison, 3D structure on Bonn [63]. From left to right, we show an input image pair, predictions from different methods, and the ground truth geometry. The top two rows show 3D point clouds from two viewpoints, where we show the union of the pointmaps for the two input time steps. The bottom row shows the corresponding disparity for two input images. Compared to all the other methods, DynaDUSt3R trained on Stereo4D achieves better 3D structure predictions with finer details.</figDesc><graphic coords="8,464.95,140.51,140.03,78.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Camera statistics from Stereo4D. We measure the difference (in meters) of camera poses between the start and end frame of each video clip as calculated by SfM.</figDesc><graphic coords="14,317.25,280.90,236.24,115.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. Scene motion statistics from Stereo4D. We measure the motion in terms of pixel displacement projected onto the image frame. For each video, we measure the percentage of tracks that have 3D trail length greater than 50 pixels. The 3D trail length is measured by Eqn. 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 13 .</head><label>13</label><figDesc>Figure 13. More qualitative results on Stereo4D test set. Extending Fig. 7, we visualize image pairs and corresponding dynamic 3D point clouds predicted by DynaDUSt3R trained on Stereo4D. Our method recovers accurate 3D shape and complex scene motion.</figDesc><graphic coords="15,45.64,144.33,178.98,115.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 14 .</head><label>14</label><figDesc>Figure 14. More qualitative comparisons of 3D motion in the Stereo4D test set. Extending Fig. 8, we compare variants of DynaDUSt3R trained on different data sources. The Stereo4D-trained model also makes more precise motion predictions than the PointOdyssey-trained model.</figDesc><graphic coords="16,58.50,459.70,66.72,66.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 15 .</head><label>15</label><figDesc>Figure 15. Example equirectangular stereo videos collected from the internet.</figDesc><graphic coords="16,58.50,581.22,166.84,83.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 16 .</head><label>16</label><figDesc>Figure<ref type="bibr" target="#b15">16</ref>. Effect of Track Optimization. We compare 3D tracks on a challenging walking tour video sequence. In this clip (left), the camera moves forward while vehicles drive toward the camera. We visualize the results across 16 frames, showing 3D trails left by both dynamic and static points. Middle: Our initial 3D tracks, created directly from RAFT, BootsTAP and SfM camera pose, also exhibit significant jitter for both dynamic (vehicle) and static (ground) points. Right: After applying our track optimization, the ground points yield stable, static tracks, and vehicle tracks become smooth and coherent.</figDesc><graphic coords="17,-199.05,3.34,645.33,362.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>EPE 3D ↓ δ 0.05 3D ↑ δ 0.10 3D ↑ DynaDUSt3R (PointOdyssey) 0.6191 11.61 20.25 0.3126 8.56 18.03 DynaDUSt3R (Stereo4D) 0.1110 65.07 75.18 0.1231 51.98 65.20 Synthetic</figDesc><table><row><cell></cell><cell>Stereo4D</cell><cell>ADT</cell></row><row><cell>Method</cell><cell>EPE 3D ↓ δ 0.05 3D ↑ δ 0.10 3D ↑</cell></row></table><note><p>5, optimized by Adam</p><p>vs. Real Training Data. Compared to synthetic data (PointOdyssey [115]), training on Stereo4D improves DynaDUSt3R's ability to generalize to real-world motion.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Quantitative comparison, depth maps.</figDesc><table><row><cell>DUSt3R [99]</cell><cell>0.2696</cell><cell>67.77</cell><cell>0.1098</cell><cell>84.93</cell></row><row><cell cols="2">MonST3R [111] 0.1939</cell><cell>72.56</cell><cell>0.0721</cell><cell>92.60</cell></row><row><cell cols="2">Ours (PtOdyssey) 0.3858</cell><cell>61.87</cell><cell>0.0691</cell><cell>95.94</cell></row><row><cell>Ours (Stereo4D)</cell><cell>0.1032</cell><cell>87.93</cell><cell>0.0653</cell><cell>96.02</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>DynaDUSt3R</cell></row><row><cell cols="5">trained on our Stereo4D data surpasses the performance of the</cell></row><row><cell cols="5">model trained on PointOdyssey [115], as well as DUSt3R and</cell></row><row><cell cols="4">MonST3R under challenging sparse view settings.</cell><cell></cell></row><row><cell cols="5">tracks depends on the accuracy of optical flow and 2D point</cell></row><row><cell cols="5">tracking and may degrade for distant background regions</cell></row><row><cell cols="5">or objects occluded for long periods. Additionally, Dy-</cell></row><row><cell cols="5">naDUSt3R is a non-generative model that only operates on</cell></row><row><cell cols="5">two-frame inputs. Extending our model to video input by</cell></row><row><cell cols="5">adopting an extra global optimization [111] or integrating</cell></row><row><cell cols="5">generative priors for modeling ambiguous motion content</cell></row><row><cell cols="3">is a promising future direction.</cell><cell></cell><cell></cell></row><row><cell cols="5">Conclusion. We presented a pipeline for mining high-</cell></row><row><cell cols="5">quality 4D data from Internet stereoscopic videos. Our</cell></row><row><cell cols="5">framework automatically annotates each real-world video</cell></row><row><cell cols="5">sequence with camera parameters, 3D point clouds, and</cell></row><row><cell cols="5">long-range 3D motion trajectories by consolidating dif-</cell></row><row><cell cols="5">ferent noisy structure and motion estimates derived from</cell></row><row><cell cols="5">videos. Furthermore, we show that training a variant of</cell></row><row><cell cols="5">DUSt3R on our real-world 4D data enables more accurate</cell></row><row><cell cols="5">learning of 3D structure and motion in dynamic scenes, out-</cell></row><row><cell cols="2">performing other baselines.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Stereo4D Statistics</head><p>We collected around 110k clips from 6,493 Internet VR180 videos. Fig. <ref type="figure">11</ref> shows the camera translation distribution between the first and last frame of each clip. In Fig. <ref type="figure">12</ref>, we measure the motion in terms of pixel displacement projected onto the image frame. Measuring motion in pixelspace emphasizes motion that occurs closer to the camera, since such motion yields larger pixel displacements, while naturally de-emphasizing motion further from the camera.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Josh</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lama</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilge</forename><surname>Akkaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florencia</forename><surname>Leoni Aleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janko</forename><surname>Altenschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Altman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.08774</idno>
		<title level="m">Shyamal Anadkat, et al. Gpt-4 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Building rome in a day</title>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasutaka</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Depth discontinuities by pixel-to-pixel stereo</title>
		<author>
			<persName><forename type="first">Stan</forename><surname>Birchfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Codeslam-learning a compact, optimisable representation for dense visual slam</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bloesch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Czarnowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Leutenegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Depth pro: Sharp monocular metric depth in less than a second</title>
		<author>
			<persName><forename type="first">Aleksei</forename><surname>Bochkovskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amaël</forename><surname>Delaunoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcel</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.02073</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deepdeform: Learning non-rigid rgbd reconstruction with semi-supervised data</title>
		<author>
			<persName><forename type="first">Aljaz</forename><surname>Bozic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zollhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Accelerated coordinate encoding: Learning to relocalize in minutes using rgb and poses</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommaso</forename><surname>Cavallari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName><surname>Prisacariu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Scene coordinate reconstruction: Posing of image collections via incremental learning of a relocalizer</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Wynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommaso</forename><surname>Cavallari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Áron</forename><surname>Monszpart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniyar</forename><surname>Turmukhambetov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName><surname>Prisacariu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A naturalistic open source movie for optical flow evaluation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Garrett</forename><forename type="middle">B</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Using multiple hypotheses to improve depth-maps for multi-view stereo</title>
		<author>
			<persName><forename type="first">George</forename><surname>Neill Df Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Vogiatzis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Hernández</surname></persName>
		</author>
		<author>
			<persName><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Orb-slam3: An accurate open-source library for visual, visual-inertial, and multimap slam</title>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Elvira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan J Gómez</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José Mm</forename><surname>Montiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">D</forename><surname>Tardós</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pyramid stereo matching network</title>
		<author>
			<persName><forename type="first">Jia-Ren</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong-Sheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A volumetric method for building complex models from range images</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIG-GRAPH</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Monoslam: Real-time single camera slam</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">D</forename><surname>Andrew J Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Molton</surname></persName>
		</author>
		<author>
			<persName><surname>Stasse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Tap-vid: A benchmark for tracking any point in a video</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankush</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larisa</forename><surname>Markeeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adria</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">BootsTAP: Bootstrapped training for tracking any point</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pauline</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilara</forename><surname>Gokay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Skanda</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankush</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Heyward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ignacio</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">João</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale. ICLR</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Direct sparse odometry</title>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Colmap-free 3d gaussian splatting</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sifei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amey</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Accurate, dense, and robust multiview stereopsis</title>
		<author>
			<persName><forename type="first">Yasutaka</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Towards internet-scale multi-view stereo</title>
		<author>
			<persName><forename type="first">Yasutaka</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Massively parallel multiview stereopsis by surface normal diffusion</title>
		<author>
			<persName><forename type="first">Silvano</forename><surname>Galliani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katrin</forename><surname>Lasinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Monocular dynamic view synthesis: A reality check</title>
		<author>
			<persName><forename type="first">Hang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruilong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shubham</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cat3d: Create anything in 3d with multi-view diffusion models</title>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Holynski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Henzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Brussee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Martin-Brualla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pratul</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJRR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Westbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Triantafyllos</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kumar</forename><surname>Ashutosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Baiyya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhant</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bikram</forename><surname>Boote</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Byrne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zach</forename><surname>Chavis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fu-Jen</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Crane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avijit</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Escobar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristhian</forename><surname>Forigua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abrham</forename><surname>Gebreselasie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Haresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Md</forename><forename type="middle">Mohaiminul</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suyog</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rawal</forename><surname>Khirodkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devansh</forename><surname>Kukreja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia-Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sagnik</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongsen</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Effrosyni</forename><surname>Mavroudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Ragusa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santhosh</forename><surname>Kumar Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luigi</forename><surname>Seminara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arjun</forename><surname>Somayazulu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihui</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinxu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinzhu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryosuke</forename><surname>Furuta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristina</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prince</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiabo</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weslie</forename><surname>Khoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anush</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sach</forename><surname>Lakhavani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brighid</forename><surname>Meredith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Austin</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oluwatumininu</forename><surname>Oguntola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaqing</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Penny</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shraman</forename><surname>Pramanick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Merey</forename><surname>Ramazanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fiona</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiran</forename><surname>Somasundaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Audrey</forename><surname>Southerland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masatoshi</forename><surname>Tateno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takuma</forename><surname>Yagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingfei</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xitong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zecheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cindy</forename><surname>Shengxin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gedas</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dima</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><forename type="middle">Maria</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonino</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judy</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">V</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyun</forename><forename type="middle">Soo</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">M</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoichi</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manolis</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianbo</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><forename type="middle">Zheng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName><surname>Wray</surname></persName>
		</author>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Kubric: a scalable dataset generator</title>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francois</forename><surname>Belletti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yilun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Duckworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Gnanapragasam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Golemo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Herrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhijit</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Lagun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Issam</forename><surname>Laradji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">(</forename><surname>Hsueh-Ti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">)</forename><surname>Derek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henning</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yishu</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<editor>
			<persName><forename type="first">Derek</forename><surname>Nowrouzezahrai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Cengiz</forename><surname>Oztireli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Etienne</forename><surname>Pot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Noha</forename><surname>Radwan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Daniel</forename><surname>Rebain</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sara</forename><surname>Sabour</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Matan</forename><surname>Sajjadi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Vincent</forename><surname>Sela</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Austin</forename><surname>Sitzmann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Deqing</forename><surname>Stone</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Suhani</forename><surname>Sun</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ziyu</forename><surname>Vora</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tianhao</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kwang</forename><forename type="middle">Moo</forename><surname>Wu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Fangcheng</forename><surname>Yi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andrea</forename><surname>Zhong</surname></persName>
		</editor>
		<editor>
			<persName><surname>Tagliasacchi</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Particle video revisited: Tracking through occlusions using point trajectories</title>
		<author>
			<persName><forename type="first">Adam</forename><forename type="middle">W</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Real-time correlation-based stereo vision with reduced border errors</title>
		<author>
			<persName><forename type="first">Heiko</forename><surname>Hirschmüller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Peter R Innocent</surname></persName>
		</author>
		<author>
			<persName><surname>Garibaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Reducing drift in structure from motion using extended features</title>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Holynski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Geraghty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan-Michael</forename><surname>Frahm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Sweeney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Surface reconstruction from unorganized points</title>
		<author>
			<persName><forename type="first">Hugues</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Derose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Duchamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Mc-Donald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Werner</forename><surname>Stuetzle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Wenbo</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangjun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sijie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><surname>Depthcrafter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.02095</idno>
		<title level="m">Generating consistent long depth sequences for open-world videos</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Humanrf: High-fidelity neural radiance fields for humans in motion</title>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Is ¸ık</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Rünz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markos</forename><surname>Georgopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taras</forename><surname>Khakhulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Starck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lourdes</forename><surname>Agapito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
		<idno>2023. 2</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multi-view reconstruction preserving weakly-supported surfaces</title>
		<author>
			<persName><forename type="first">Michal</forename><surname>Jancosek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Matchstereo-videos: Bidirectional alignment for consistent dynamic stereo matching</title>
		<author>
			<persName><forename type="first">Junpeng</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Panoptic studio: A massively multiview system for social motion capture</title>
		<author>
			<persName><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Nabbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shohei</forename><surname>Nobuhara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dynamicstereo: Consistent dynamic depth from stereo videos</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Karaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ignacio</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Poisson surface reconstruction</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Michael Kazhdan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugues</forename><surname>Bolitho</surname></persName>
		</author>
		<author>
			<persName><surname>Hoppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourth Eurographics symposium on Geometry processing</title>
		<meeting>the fourth Eurographics symposium on Geometry processing</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Repurposing diffusion-based image generators for monocular depth estimation</title>
		<author>
			<persName><forename type="first">Bingxin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Obukhov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nando</forename><surname>Metzger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodrigo</forename><forename type="middle">Caye</forename><surname>Daudt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">End-to-end learning of geometry and context for deep stereo regression</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hayk</forename><surname>Martirosyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saumitro</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abraham</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Bry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Nersemble: Multi-view radiance field reconstruction of human heads</title>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Kirschstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenhan</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Giebenhain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Segment-based stereo matching using belief propagation and a self-adapting dissimilarity measure</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Klaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Sormann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konrad</forename><surname>Karner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Robust consistent video depth estimation</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuejian</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Skanda</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ignacio</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Heyward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">João</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Tapvid-3d: A benchmark for tracking any point in 3d. In NeurIPS, 2024. 2, 6</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijia</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
		<author>
			<persName><surname>Mosca</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.17421</idno>
		<title level="m">Dynamic gaussian fusion from casual videos via 4d motion scaffolds</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Leroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yohann</forename><surname>Cabon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jérôme</forename><surname>Revaud</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.09756</idno>
		<title level="m">Grounding image matching in 3d with mast3r</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Megadepth: Learning single-view depth prediction from internet photos</title>
		<author>
			<persName><forename type="first">Zhengqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning the depths of moving people by watching frozen people</title>
		<author>
			<persName><forename type="first">Zhengqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tali</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Forrester</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019-02">2019. 2</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Neural scene flow fields for space-time view synthesis of dynamic scenes</title>
		<author>
			<persName><forename type="first">Zhengqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Dynibar: Neural dynamic image-based rendering</title>
		<author>
			<persName><forename type="first">Zhengqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Forrester</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Temporally consistent online depth estimation in dynamic scenes</title>
		<author>
			<persName><forename type="first">Zhaoshuo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><forename type="middle">H</forename><surname>Francis X Creighton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganesh</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName><surname>Unberath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">Zhengqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Forrester</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linyi</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vickie</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Holynski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.04463</idno>
		<title level="m">Megasam: Accurate, fast, and robust structure and motion from casual dynamic videos</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Bundle-adjusting neural radiance fields</title>
		<author>
			<persName><forename type="first">Chen-Hsuan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Chiu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting><address><addrLine>Barf</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Robust dynamic radiance fields</title>
		<author>
			<persName><forename type="first">Yu-Lun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Meuleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung-Yu</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayush</forename><surname>Saraf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changil</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yung-Yu</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Sift-the scale invariant feature transform</title>
		<author>
			<persName><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Consistent video depth estimation</title>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Matzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Kopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Orb-slam: a versatile and accurate monocular slam system</title>
		<author>
			<persName><forename type="first">Raul</forename><surname>Mur-Artal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><forename type="middle">Martinez</forename><surname>Montiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">D</forename><surname>Tardos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">ORB-SLAM: a versatile and accurate monocular SLAM system</title>
		<author>
			<persName><forename type="first">Raúl</forename><surname>Mur-Artal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M M</forename><surname>Montiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">D</forename><surname>Tardós</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Robotics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Dynamicfusion: Reconstruction and tracking of non-rigid scenes in real-time</title>
		<author>
			<persName><forename type="first">Dieter</forename><surname>Richard A Newcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">M</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">ReFusion: 3D Reconstruction in Dynamic Environments for RGB-D Cameras Exploiting Residuals</title>
		<author>
			<persName><forename type="first">E</forename><surname>Palazzolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lottes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Giguère</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IROS</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Aria digital twin: A new benchmark dataset for egocentric 3d machine perception</title>
		<author>
			<persName><forename type="first">Xiaqing</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Charron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongqian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Whelan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omkar</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuheng</forename><surname>Carl Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Cascade residual learning: A two-stage convolutional neural network for stereo matching</title>
		<author>
			<persName><forename type="first">Jiahao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Sj Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengxi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiong</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR Workshops</title>
		<meeting>CVPR Workshops</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">3d reconstruction of a moving point from a series of 2d projections</title>
		<author>
			<persName><forename type="first">Hyun</forename><surname>Soo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Park</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Takaaki</forename><surname>Shiratori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Nerfies: Deformable neural radiance fields</title>
		<author>
			<persName><forename type="first">Keunhong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Utkarsh</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sofien</forename><surname>Bouaziz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Martin-Brualla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<author>
			<persName><forename type="first">Keunhong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Utkarsh</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Hedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sofien</forename><surname>Bouaziz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Martin-Brualla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13228</idno>
		<title level="m">Hypernerf: A higherdimensional representation for topologically varying neural radiance fields</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Camp: Camera preconditioning for neural radiance fields</title>
		<author>
			<persName><forename type="first">Keunhong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Henzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Martin-Brualla</surname></persName>
		</author>
		<idno>2023. 2</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">UniDepth: Universal monocular metric depth estimation</title>
		<author>
			<persName><forename type="first">Luigi</forename><surname>Piccinelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yung-Hsu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mattia</forename><surname>Segu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Visual modeling with a hand-held camera</title>
		<author>
			<persName><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Vergauwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Verbiest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Cornelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Tops</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reinhard</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Detailed realtime urban 3d reconstruction from video</title>
		<author>
			<persName><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Nistér</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J-M</forename><surname>Frahm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Akbarzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippos</forename><surname>Mordohai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Clipp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Engels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Gallup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S-J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Merrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Movie gen: A cast of media foundation models</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Zohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andros</forename><surname>Tjandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Animesh</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ann</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chih-Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ching-Yao</forename><surname>Chuang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.13720</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Towards robust monocular depth estimation: Mixing datasets for zero-shot crossdataset transfer</title>
		<author>
			<persName><forename type="first">René</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katrin</forename><surname>Lasinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Vision transformers for dense prediction</title>
		<author>
			<persName><forename type="first">René</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Structure-from-motion revisited</title>
		<author>
			<persName><forename type="first">L</forename><surname>Johannes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan-Michael</forename><surname>Schonberger</surname></persName>
		</author>
		<author>
			<persName><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Pixelwise view selection for unstructured multi-view stereo</title>
		<author>
			<persName><forename type="first">Enliang</forename><surname>Johannes L Schönberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan-Michael</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Frahm</surname></persName>
		</author>
		<author>
			<persName><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Learning temporally consistent video depth from video diffusion priors</title>
		<author>
			<persName><forename type="first">Jiahao</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanbo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youmin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiyi</forename><surname>Liao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.01493</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Dytanvo: Joint refinement of visual odometry and motion segmentation in dynamic environments</title>
		<author>
			<persName><forename type="first">Shihao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yilin</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenshan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICRA</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Extranerf: Visibility-aware view extrapolation of neural radiance fields with diffusion models</title>
		<author>
			<persName><forename type="first">Meng-Li</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Chiu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Boyice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Holynski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Forrester</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janne</forename><surname>Kontkanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Kronecker-markov prior for dynamic 3d reconstruction</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Photo tourism: exploring photo collections in 3d</title>
		<author>
			<persName><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Autoflow: Learning a better training set for optical flow</title>
		<author>
			<persName><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Vlasic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Herrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Krainin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huiwen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramin</forename><surname>Zabih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>William T Freeman</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Disentangling architecture and training for optical flow</title>
		<author>
			<persName><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Herrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fitsum</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Stereo matching using belief propagation</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan-Ning</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Scalability in perception for autonomous driving: Waymo open dataset</title>
		<author>
			<persName><forename type="first">Pei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrik</forename><surname>Kretzschmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xerxes</forename><surname>Dotiwalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelien</forename><surname>Chouard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijaysai</forename><surname>Patnaik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Tsui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Caine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<author>
			<persName><forename type="first">Chris</forename><surname>Sweeney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Holynski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.03539</idno>
		<title level="m">Structure from motion for panorama-style videos</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<author>
			<persName><forename type="first">Chengzhou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.04807</idno>
		<title level="m">Ba-net: Dense bundle adjustment network</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Gemini: a family of highly capable multimodal models</title>
		<author>
			<persName><forename type="first">Gemini</forename><surname>Team</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johan</forename><surname>Schalkwyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anja</forename><surname>Hauth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Millican</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.11805</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Raft: Recurrent all-pairs field transforms for optical flow</title>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Teed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020">2020. 3, 2</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Droid-slam: Deep visual slam for monocular, stereo, and rgb-d cameras</title>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Teed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Raft-3d: Scene flow using rigid-motion embeddings</title>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Teed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Deep patch visual odometry</title>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Teed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lahav</forename><surname>Lipson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">A hierarchical symmetric stereo algorithm using dynamic programming</title>
		<author>
			<persName><forename type="first">Geert</forename><surname>Van Meerbergen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Vergauwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Spatiotemporal bundle adjustment for dynamic 3d reconstruction</title>
		<author>
			<persName><forename type="first">Minh</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaser</forename><surname>Srinivasa G Narasimhan</surname></persName>
		</author>
		<author>
			<persName><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">Web stereo video supervision for depth prediction from dynamic scenes</title>
		<author>
			<persName><forename type="first">Chaoyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<author>
			<persName><forename type="first">Qianqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vickie</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.13764</idno>
		<title level="m">Shape of motion: 4d reconstruction from a single video</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Dust3r: Geometric 3d vision made easy</title>
		<author>
			<persName><forename type="first">Shuzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Leroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yohann</forename><surname>Cabon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Chidlovskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerome</forename><surname>Revaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2008">2024. 1, 2, 5, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Neural video depth stabilizer</title>
		<author>
			<persName><forename type="first">Yiran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiguo</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2008">2023. 2, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Sea-raft: Simple, efficient, accurate raft for optical flow</title>
		<author>
			<persName><forename type="first">Yihan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lahav</forename><surname>Lipson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Nerfiller: Completing scenes via generative 3d inpainting</title>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Holynski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<author>
			<persName><forename type="first">Rundi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Trevithick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changxi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Holynski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2411.18613</idno>
		<title level="m">Cat4d: Create anything in 4d with multi-view video diffusion models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Depth anything: Unleashing the power of large-scale unlabeled data</title>
		<author>
			<persName><forename type="first">Lihe</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<author>
			<persName><forename type="first">Lihe</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.09414</idno>
		<title level="m">Depth anything v2</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Mvsnet: Depth inference for unstructured multiview stereo</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Recurrent mvsnet for high-resolution multi-view stereo depth inference</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianwei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019-02">2019. 2</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Learning to recover 3d scene shape from a single image</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Metric3d: Towards zero-shot metric 3d prediction from a single image</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhipeng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaixuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Ga-net: Guided aggregation net for end-toend stereo matching</title>
		<author>
			<persName><forename type="first">Feihu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Prisacariu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019-02">2019. 2</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<monogr>
		<author>
			<persName><forename type="first">Junyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Herrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junhwa</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Forrester</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.03825</idno>
		<title level="m">MonST3R: A simple approach for estimating geometry in the presence of motion</title>
		<imprint>
			<date type="published" when="2008">2024. 2, 7, 8</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Temporalstereo: Efficient spatial-temporal stereo matching network</title>
		<author>
			<persName><forename type="first">Youmin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Consistent depth of moving objects in video</title>
		<author>
			<persName><forename type="first">Zhoutong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Forrester</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tali</forename><surname>Dekel</surname></persName>
		</author>
		<idno>2021. 2</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Structure and motion from casual videos</title>
		<author>
			<persName><forename type="first">Zhoutong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Forrester</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Pointodyssey: A large-scale synthetic dataset for long-term point tracking</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><forename type="middle">W</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bokui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gordon</forename><surname>Wetzstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2008">2023. 2, 6, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Semantic understanding of scenes through the ade20k dataset</title>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Stereo magnification: Learning view synthesis using multiplane images</title>
		<author>
			<persName><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Fyffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
