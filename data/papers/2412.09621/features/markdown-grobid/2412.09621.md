# Stereo4D: Learning How Things Move in 3D from Internet Stereo Videos

## Abstract

## 

Figure [1](#). There is currently no scalable source of data for real-world, ground truth 3D motion paired with video. We present a framework for mining such data from existing stereoscopic videos on the Internet, in the form of 3D point clouds with long-range world-space trajectories. Our framework fuses and filters camera poses, dense depth maps, and 2D motion trajectories to produce high-quality, pseudo-metric point clouds with long-term 3D motion trajectories, pictured above, for hundreds of thousands of video clips. We show how this data is useful in learning a model that reasons about both 3D shape and motion in imagery.

## Introduction

Simultaneously predicting and understanding geometry and motion-that is, dynamic 3D content-from images is a fundamental building block for computer vision, with applications ranging from robotic interaction and scene reconstruction to novel view synthesis of dynamic scenes. While recent work has made remarkable progress in predicting static 3D structure from images [[5,](#b4)[99,](#b98)[105]](#b104), modeling realworld 3D motion-people gesturing, balls bouncing, leaves rustling in the wind-remains a critical unsolved challenge for building truly general models of the visual world.

Recent breakthroughs in AI, from large language models [[1,](#b0)[89]](#b88) to image generation [[73]](#b72) and static 3D reconstruction [[5,](#b4)[99,](#b98)[105]](#b104), demonstrate a consistent pattern: large amounts of high-quality, realistic training data and scalable architectures enable dramatic performance improvements. In the realm of 3D reasoning, prior works [[49,](#b48)[74,](#b73)[75,](#b74)[99,](#b98)[104]](#b103) have shown the value of large-scale training data for strong zero-shot generalization within singleview or two-view static scene settings. But applying this same formula to dynamic 3D scenes (i.e. moving 3D structure) requires a corresponding large-scale dataset consisting of diverse visual content paired with corresponding ground-truth 3D motion trajectories. Obtaining such data presents unique challenges. While there are synthetic datasets [[9,](#b8)[19,](#b18)[29,](#b28)[115]](#b114), these often fail to capture the distribution of real-world content and the nuanced patterns of real-world motion. Traditional approaches to gathering real motion data, such as motion capture systems or multi-view camera arrays [[28,](#b27)[35,](#b34)[38,](#b37)[43]](#b42) are accurate, but difficult to scale and limited in the diversity of scenes they can capture.

We identify online stereoscopic fisheye videos (often referred to as VR180 videos) as an untapped source of such data. These videos, designed to capture immersive VR experiences, provide wide field-of-view stereo imagery with a standardized stereo baseline. We present a pipeline that carefully combines state-of-the-art methods for stereo depth estimation and video tracking along with structure-frommotion methods optimized for dynamic scenes. By combining our system with careful filtering and quality control, we show that we can extract over 100K video sequences, each containing high-quality 3D point clouds with per-point long-term trajectories (see Fig. [1](#)), as well as all other intermediate quantities: depth maps, camera poses, images, and 2D correspondences. We additionally show the utility of the dataset by training DynaDUSt3R, an extension to DUSt3R that can predict high-quality 3D structure and motion from challenging image pairs. Our contributions include: (1) a framework for obtaining real-world, dynamic, and pseudo-metric 4D reconstructions and camera poses at scale from existing online video; (2) DynaDUSt3R, a method that takes a pair of frames from any real-world video, and predicts a pair of 3D point clouds and the corresponding 3D motion trajectories that connect them in time.

## Related work

2D and 3D motion data. There has been tremendous progress on the task of motion estimation from images and videos, and in particular for 2D image-space correspondence estimation. Most state-of-the-art methods use neural networks trained on ground truth data to predict these correspondences directly from images. While these approaches require large training datasets, synthetic data from graphics engines [[9,](#b8)[19,](#b18)[29,](#b28)[30,](#b29)[59,](#b58)[83,](#b82)[115]](#b114) has proven surprisingly effective at generalizing to real-world data, likely because the core task, low-level textural correspondence, is similar between the two domains.

However, the same cannot be said for 3D motion estimation, since predicting both 3D structure and motion is usually more ambiguous and can require specific prior knowledge about the real world and how it moves. To help address this domain gap, a number of real-world datasets have been proposed. The KITTI [[27]](#b26) and Waymo [[86]](#b85) datasets in-clude real-world autonomous driving sequences with stereo and motion annotations derived from LiDAR and odometry information, but only focus on the relatively closed domain of street scenes, whereas our data depicts more diverse in-the-wild scenarios. A number of annotated smallerscale datasets, such as TAPVid [[16]](#b15), TAPVid3D [[46]](#b45), and Dycheck [[25]](#b24), have been proposed, primarily serving as evaluation datasets for benchmarking depth estimation, 3D reconstruction, and 3D motion estimation approaches. WSVD [[97]](#b96) and NVDS [[100]](#b99) are stereo video datasets that include disparity maps derived from optical flow. While their source content is similar, our method provides richer 3D annotations beyond time-independent disparity maps, such as 3D camera parameters and long-term 3D motion tracks.

## Static and dynamic scene reconstruction

The problem of reconstructing a static 3D scene has been studied for decades. Traditional 3D reconstruction methods tackle this problem by first estimating camera parameters via Structure-from-Motion (SfM) [[2,](#b1)[32,](#b31)[71,](#b70)[72,](#b71)[72,](#b71)[76,](#b75)[82,](#b81)[87]](#b86) or SLAM [[11,](#b10)[15,](#b14)[20,](#b19)[60]](#b59). Dense scene geometry can then be estimated through Multi-view Stereo (MVS) [[10, 22-24, 36, 77, 106, 107]](#) followed by surface reconstruction algorithms [[14,](#b13)[33,](#b32)[40]](#b39). More recently, deep neural networkbased approaches have shown promising results in improving camera localization accuracy or scene reconstruction through intermediate representations such as depth maps [[4,](#b3)[54,](#b53)[79,](#b78)[88,](#b87)[91,](#b90)[93]](#b92), radiance fields [[21,](#b20)[26,](#b25)[55,](#b54)[69,](#b68)[80,](#b79)[102]](#b101), or 3D scene coordinates [[7,](#b6)[8,](#b7)[48,](#b47)[99,](#b98)[111]](#b110). However, these methods assume the input images to be observations of a static environment, and therefore produce inaccurate geometry and camera poses for dynamic scenes.

Reconstructing dynamic scenes is more challenging since scene and object motions violate the multi-view constraints used to reconstruct static scenes. As a result, many prior works require RGBD input [[6,](#b5)[62]](#b61) or only recover sparse geometry [[66,](#b65)[81,](#b80)[96]](#b95). Several recent works tackle this problem from monocular input through video depth maps [[45,](#b44)[113,](#b112)[114]](#b113), time-varying radiance fields [[25,](#b24)[47,](#b46)[51,](#b50)[52,](#b51)[56,](#b55)[67,](#b66)[68,](#b67)[98]](#b97), or generative priors [[103]](#b102).

Monocular and stereo depth. Recent works on singleview depth prediction have shown strong zero-shot generalization to in-the-wild domains by training deep neural networks on diverse RGBD datasets [[41,](#b40)[49,](#b48)[50,](#b49)[70,](#b69)[74,](#b73)[75,](#b74)[104,](#b103)[105,](#b104)[108,](#b107)[109]](#b108). However, producing temporally consistent and metric depth from video is still challenging. To tackle this, recent works use test-time optimization [[58,](#b57)[114]](#b113) or end-to-end learning with temporal attention [[34,](#b33)[45,](#b44)[78,](#b77)[100]](#b99). On the other hand, stereo images or videos are also popular input modalities for obtaining reliable metric depth maps, and various stereo matching algorithms have been proposed [[3,](#b2)[12,](#b11)[31,](#b30)[37,](#b36)[39,](#b38)[42,](#b41)[44,](#b43)[53,](#b52)[65,](#b64)[85,](#b84)[94,](#b93)[101,](#b100)[110,](#b109)[112]](#b111). Building on these advancements, . Data processing pipeline. Our method starts with VR180 (wide-angle, stereoscopic) videos, and estimates metric stereo depth, 2D point tracks, and camera poses. These quantities allow the tracks to be lifted to 3D where they are filtered and denoised to produce world-space, metric 3D point trajectories.

our method bridges ideas from monocular video depth estimation and stereo video processing. We use a light-weight optimization step and extend them to stereo inputs for more consistent motion estimation in metric space.

## Creating a dataset of 4D scenes

A core contribution of this work is a pipeline for extracting high-quality, pseudo-metric, 3D data from online stereoscopic fisheye videos (known as VR180 videos). Highresolution, wide field of view VR180 videos can be found readily online. We show that this data is ideal for deriving rich dynamic 3D information that can power models for predicting geometry and motion from imagery.

Concretely, each instance of data starts as an N frame stereo video consisting of left-right image pairs I i and I ′ i indexed by frame index i ∈ [1, N ]. We convert these stereo pairs to a dynamic 3D point cloud with K points in a world-space coordinate frame, where each point, indexed by j ∈ [1, K], has a time-varying position p j i . As part of the process of generating this dynamic point cloud, we also extract a number of auxiliary quantities: (1) per-frame camera extrinsics, (the left camera's position c i and orientation R i ), (2) rig calibration for the stereo video giving the position c r and orientation R r of the right camera relative to the left camera, and (3) a per-frame disparity map D i .

## Data Processing Pipeline

At a high level, our pipeline for converting a stereoscopic video into a dynamic point cloud involves estimating camera poses, stereo disparity, and 2D tracks, fusing these quantities into a consistent 3D coordinate frame, and performing several filtering operations to ensure temporal consistent, high-quality reconstructions (Fig. [2](#fig_0)). In this section, we describe in detail the key components of this process.

SfM. We start by processing the sequence of stereo frames I i ↔ I ′ i to produce camera pose estimates (c i , R i ). We first use a SLAM method to divide the video into shots, as in [[116]](#b115). For each shot, we run an incremental SfM algorithm similar to COLMAP [[76]](#b75). We initialize the stereo rig calibration (c r , R r ) to a rectified stereo pair with baseline 6.3cm, but optimize for the calibration in bundle adjustment. In practice, we found that the exact stereo pair orientation can vary significantly from its nominal configuration and that optimizing the rig was critical for good results.

Depth Estimation. We next estimate a per-frame disparity map, operating on each frame independently. In particular, we use the estimated camera rig calibration c r , R r to create rectified stereo pairs from the stereo fisheye video and estimate the per-frame disparity D i with RAFT [[83,](#b82)[84,](#b83)[90]](#b89).

3D Track Estimation and Optimization. We extract longrange 2D point trajectories using BootsTAP [[17]](#b16). Using the camera poses c i , R i and disparity maps D i , we unproject these tracked points into 3D space, turning each 2D track j into a 3D motion trajectory p j 1 , . . . , p j N across all frames. In general, each point will usually only be tracked in a subset of frames, but for simplicity, we describe the formulation as if all points are always visible. Moreover, since subsequent steps are done independently per-track, we drop the superscript j in future references.

Since stereo depth estimation is performed per-frame, the initial disparity estimates (and therefore, the 3D track positions) are likely to exhibit high-frequency temporal jitter. To compensate for potentially inconsistent disparity estimates, we formulate an optimization strategy that solves for a per-frame scalar offset δ i ∈ R that moves each point p i along the ray from camera location c i to p i at frame i. This ray is denoted r i = (p i -c i )/||p i -c i ||, and we refer to the updated location as p ′ i = p i + δ i r i .

To ensure static points remain stationary while moving tracks maintain realistic, smooth motion, avoiding abrupt depth changes frame by frame, we design an optimization objective comprising three terms: a static loss L static , a dynamic loss L dynamic , and a regularization loss L reg . The static loss L static minimizes jitter by encouraging points to remain close to each other in world space:

$L static = N i=1 N j=1 ∥p ′ i -p ′ j ∥ 2 N ′2 p (1)$where

$N ′ p = N i=1 ∥p ′ i ∥/N$is a normalizing factor. The dynamic loss term reduces jitter by minimizing acceleration along the camera ray through a discrete Laplacian operator:

$L dynamic = N i=1 ∆∈W p ′ i+∆ -2p ′ i + p ′ i-∆ ⊤ r i 2(2)$where the acceleration along the ray is calculated over multiple window sizes W = {1, 3, 5}.

The two loss terms are weighted by a track-dependent function, σ(m), where m is a measure of the motion magnitude of the track. Motion is measured in 2D rather than 3D because distant points can appear to have a larger 3D motion due to noise amplification at low disparities. Specifically, we project the 3D motion trajectory between time i -w o and the current time i into 2D image-space at time i, and calculate the track's motion magnitude m as the 90 th percentile of the track's trail length across all frames. The track trail length for a frame is measured by projected 3D points along the track to the current frame as if the camera is static in a window of w o = 16 frames,

$m = Percentile 90 i=1:N max w=1:wo ∥π i (p i ) -π i (p i-w )∥ (3)$where π i (•) ∈ R 2 gives the projected pixel location of a 3D point on camera c i 's image plane. The weighting function σ(m) is defined as σ(m) = 1 1+exp(m-m0) where m 0 = 20. Finally, to encourage faithfulness to the originally estimated disparities, we regularize the displacements in disparity space:

$L reg = λ reg T i=1 1 δ i + ∥p i -c i ∥ - 1 ∥p i -c i ∥ 2 ,(4)$where use of disparity space reflects the fact that the measurements themselves originate as disparities. Practically, the impact of the use of disparity is that larger deviations are tolerated at more distant points, where depth is intrinsically more uncertain.

The full objective function is We set λ reg = 10 -4 and optimize Eqn. 5 using Adam with a learning rate of 0.05 for 100 steps. The effect of track optimization is shown in Fig. [3](#fig_1). The optimized motion is smoother and does not contain high frequency noise.

$min {δi} N i=1 σ(m)L static + (1 -σ(m))L dynamic + L reg . (5$$)$Implementation details. Shot-selection. Rather than work with the full video, we break the footage into discrete, trackable shots using ORB-SLAM2's stereo estimation mode [[61]](#b60) following [[118]](#b117). Field of View. While estimating pose, we use a 140 • FoV fisheye format, which we found to capture more of the (usually static) background and less of the (often dynamic) foreground, yielding more reliable camera poses. Stereo Confidence Checks. We discard pixels where the y-component of RAFT flow is more than 1 pixel (since rectified stereo pairs should have perfectly horizontal motion) and where the stereo cycle consistency error is more than 1 pixel (since such pixels are unreliable). Dense 2D tracks. To get dense tracks, we run BootsTAP with dense query points: for every 10th frame, we uniformly initialize 128 × 128 query points on frames of resolution 512 × 512. We then prune redundant tracks that overlap on the same pixel. Drifting tracks. Since 2D tracks can drift on textureless regions, we discard moving 3D tracks that correspond to certain semantic categories (e.g., "walls", "building", "road", "earth", "sidewalk"), detected by DeepLabv3 [[13]](#b12) on ADE20K classes [[116,](#b115)[117]](#b116).

Filtering details. A fraction of the video clips that are processed may be unsuitable because they either (1) are not videos, and are entirely static images, (2) contain crossfades, or (3) have text or other synthetic graphics. To discard text and title sequences, we avoid creating video clips from the start and ends of the source videos. We identify cross-fades by running SIFT [[57]](#b56) matching through the video at multiple temporal scales and discarding video clips with static camera but with fewer than 5 SIFT matches between frames that are 5 seconds apart.  

## Stereo4D Dataset

Fig. [4](#fig_2) illustrates a subset of videos and reconstructions from a dataset processed with the above pipeline, encompassing more than 100K clips capturing everyday scenes and activities. To visualize the range of content, we used an automatic captioning system to generate captions for the dataset and created a word cloud (Fig. [5](#fig_3)) highlighting the most frequently observed objects.

## Learning a prior on how things move

We now describe our method for predicting dynamic 3D point clouds from pairs of images, and how we train it with our Stereo4D data. Our model is based on DUSt3R [[99]](#b98), which predicts a 3D point cloud for a static scene from images. Given two input images, it uses a ViT-based architecture [[18]](#b17) to extract image features and uses a transformerbased decoder to cross-attend features from two images, and then use a downstream pointmap decoder to output pointmaps for the two images, aligned in the first image's coordinate frame.

DynaDUSt3R model. While DUSt3R focuses on static scene structure, our proposed DynaDUSt3R method, illustrated in Fig. [6](#fig_4), works with dynamic scenes by adding a motion head that predicts how the points move between two frames. As input, DynaDUSt3R accepts two images: I 0 at time t 0 , and I 1 at time t 1 (where t 0 and t 1 may be seconds apart). It also accepts an intermediate query time t q ∈ [0, 1]; the motion head is asked to predict 3D scene flow from the two input frames to query time t q , as described below. Like DUSt3R, DynaDUSt3R begins by encoding the images with a shared ViT and cross-attention decoder, producing global features G 0 and G 1 for I 0 and I 1 , respectively. Each feature embedding can be converted into geometry using DUSt3R's point head: e.g., for image I 0 , the point head produces a pointmap P 0 ∈ R H×W ×3 representing the geometry at time t 0 , as well as a point confidence map C 0 point ∈ R H×W . Each point cloud is predicted in the coordinate frame of I 0 , but at the time of its respective image (so, the two point clouds may differ due to scene motion).

We add a separate motion head in parallel to the original point head, to predict a map of 3D displacement vectors (that is, a scene flow map, which we refer to as a motion map) for each pointmap. The motion map should displace each input frame to an intermediate time t q ∈ [0, 1], where t q = 0 corresponds to t 0 , the time of I 0 , and similarly for t q = 1, t 1 , and I 1 . The motivation for predicting motion to an intermediate time (inclusive of the endpoints) is twofold: first, it leads to a more general prediction task where we can predict a full motion trajectory between two frames, and second, it allows us to use partial ground truth 3D trajecto- ries as supervision; not all trajectories may span all the way from t 0 to t 1 , but may span through some intermediate time.

For each image I v (with v ∈ {0, 1}), the network outputs a 3D motion map M v→tq for the corresponding pointmap from t v to t q with corresponding motion confidence map C v mot ∈ R H×W . This prediction is based on the global feature G v as well as an embedding of the query time emb(t q ). We use positional embedding [[95]](#b94) to encode time t q to a 128-D vector and inject it to the motion features in the motion head via linear projection layers. Training objective. We use the same confidence-aware scale-invariant 3D regression loss as in DUSt3R. We first normalize both the predicted and ground truth pointmaps using scale factors z = norm(P 0 , P 1 ) and z = norm( P0 , P1 ), respectively (where a bar, e.g., P0 , denotes a ground truth quantity, and where 'norm' computes the average distance between a set of points and the world origin). We scale the motion maps with the same scales z and z. Following DUSt3R, we compute a Euclidean distance loss on the pointmap, setting

$L point to v∈{0,1} i∈D v C v point,i 1 z P v i - 1 z Pv i -αp log C v point,i(6)$where D v corresponds to the valid pixels where ground truth is defined and α p is a weighting hyperparameter. We additionally compute a Euclidean distance loss on the position after motion, which encourages the network to learn correct displacements. This loss L motion is defined as

$v∈{0,1} i∈D v C v mot,i 1 z P v→tq i - 1 z Pv→tq i -αm log C v mot,i ,(7)$where

$P v→tq i = P v i + M v→tq i .$Training details. We initialize our network with DUSt3R weights and initialize the motion head with the same weights as the point head. We finetune for 49k iterations, with batch size 64, learning rate 2.5e-  

## Experiments

We conduct a series of experiments to validate the effectiveness of our proposed data and techniques. First, we evaluate our proposed real-world Stereo4D data mined from VR180 videos on the DynaDUSt3R task. In particular, we compare models that are individually trained with our realworld data and with synthetic data, and we show that our data enables model learning more accurate 3D motion priors (Sec. 5.1). Second, we show that our trained model that adapts DUSt3R has strong generalization to in-the-wild images of dynamic scenes, and enables accurate predictions of underlying geometry (Sec. 5.2).

## 3D motion prediction

Baselines and metrics. To evaluate the efficacy of our data paradigm on motion prediction, we primarily compare Dy-naDUSt3R trained on Stereo4D to the same network trained on a synthetic dataset, PointOdyssey [[115]](#b114). PointOdyssey contains ground truth depth maps and 3D motion tracks rendered from an animation engine; we supervise the model with this data using the same hyperparameter settings as described above. During inference, given two video frames sampled from a video of a dynamic scene, we compare 3D end-point-error (EPE) between ground truth and predicted 3D motion vectors. We also compute the fraction of 3D points that have motion within 5 cm and 10 cm compared to ground truth (δ 0.05 3D , δ 0.10 3D ), following [[92,](#b91)[98]](#b97). Since our model outputs point clouds up to an unknown scale, we align each prediction with the ground truth through a median scale estimate. We evaluate models trained on each of these two data sources on a held-out Stereo4D test-set, as well as on Arial Digital Twin (ADT) [[64]](#b63) data containing scene motion, processed by the TapVid3D benchmark [[46]](#b45). As test data, we randomly sample pairs of frames that are at most 30 frames apart from both Stereo4D and ADT.  Quantitative results. We show numerical results for twoframe 3D motion prediction in Tab. 1. DynaDUSt3R trained on real-world data achieves better generalization and outperforms the baseline trained on PointOdyssey significantly across all metrics. This suggests the potential of our data for more effective learning of real-world 3D motion priors.

Qualitative results. Fig. [7](#fig_6) shows example results for three dynamic scenes in our Stereo4D test set, including visualizations of 3D point clouds and motion tracks. Dy-naDUSt3R produces accurate 3D shape and motion tracks over the timespan defined by the two input images. Despite the inputs being two sparse images, our architecture enables querying intermediate motion states, resulting in continuous and potentially non-linear motion trajectories.

We also qualitatively compare predicted 3D motion tracks between DynaDUSt3R networks trained on Stereo4D and on PointOdyssey, by projecting their predicted 3D motion vectors into 2D image space. Fig. [8](#fig_7) and Fig. [9](#fig_8) show comparisons on the Stereo4D and ADT test set respectively. DynaDUSt3R trained on Stereo4D produces more accurate 3D motion estimates for both static and moving objects. For instance, DynaDUSt3R trained on PointOdyssey produces non-zero motion for the stationary street banner and erroneous motions for the walking people in Fig. [8](#fig_7).

## Structure prediction

Baseline and metrics. We evaluate the quality of predicted 3D structure by comparing depth maps predicted by DUSt3R [[99]](#b98), MonST3R [[111]](#b110), and DynaDUSt3R trained on Stereo4D or PointOdyssey. DUSt3R is designed to predict aligned point clouds from two input images of a static scene. MonST3R, a concurrent approach, extends DUSt3R to handle dynamic scenes by predicting time-varying point clouds without modeling motion.

We evaluate predicted depth accuracy on the Bonn [63] dataset and our held-out test set, where we sample two views that are 30 frames apart from a video. Since we focus on the two-frame case, we do not apply any postoptimization to the network outputs. In addition, since all methods predict 3D point clouds in the coordinate frame of the first image, we include the two points clouds predicted from both input frames in our evaluation. We use standard depth metrics, including absolute relative error (Abs Rel) and percentage of inlier points δ < 1.25, following prior work [[100,](#b99)[111]](#b110). We use the same median alignment as before to align the predicted depth map with the ground truth.

Quantitative comparisons. We show quantitative comparisons of depth predicted by different methods in Tab. 2. Dy-naDUSt3R trained on Stereo4D outperforms all other baselines by a large margin. In particular, we demonstrate improved depth prediction on the unseen Bonn dataset.

Qualitative comparisons. We provide additional visual comparisons in Fig. [10](#fig_9), where we visualize ground truth 3D point clouds and predictions from our approach and the other three baselines at different input time steps. DuST3R predicts inaccurate depth relationships for the two moving people, while MonsT3R and DynaDUSt3R trained on PointOdyssey both predict distorted scene geometry. In contrast, our model trained on Stereo4D produces 3D structure that most closely resembles the ground truth.

## Discussion and Conclusion

Limitations. Our data curation pipeline and trained model have limitations. The quality of the long-range 3D motion

## Stereo4D

Bonn [[63]](#b62) Method Abs Rel↓ δ < 1.25↑ Abs Rel↓ δ < 1.25↑ 

## More qualitative examples on track optimization

In Fig. [16](#fig_15), we illustrate estimated tracks for a video sequence featuring a forward-moving camera and vehicles driving towards the camera. Our initial 3D tracks derived directly from RAFT depth, BootsTAP 2D tracks, and SfM camera pose, show significant jitter for both dynamic (vehicle) and static (ground) points. However, after applying our track optimization, the ground points produce stable, static tracks, and vehicle tracks become smooth and coherent.

## Dataset curation details 9.1. Equirectangular videos

The raw videos that we collect (see examples in Fig. [15](#fig_14)) are natively stored in a cropped equirectangular format, which differs from a full 360 • equirectangular projection as the horizontal field of view of the cropped format typically spans 180 • -half of a full sphere. These videos often contain metadata specifying the horizontal and vertical field of view. For instance, metadata for a typical video might specify start yaw = -90.0 • , end yaw = 90.0 • , start tilt = -90.0 • , end tilt = 90.0 • ; Since many VR180 videos are designed for an immersive VR experience, they are typically viewed with headsets. Hence, the baseline between the left and right  cameras typically closely matches the average human eye distance of 6.3 cm.

## SfM

For ease of processing with standard 3D computer vision pipelines, and to benefit from the wide FoV of the input videos, we convert the videos from their native format (equirectangular projections) to a fisheye format for camera pose estimation. We use a 140 • field of view for these fisheye-projected videos, because many equirectangular videos have a black fade-out/feathering/vignetting effect applied at the boundary, as shown in Fig. [15](#fig_14). We found that using wider FoV frames significantly improves camera pose estimation in dynamic scenes. When using narrow FoV projections, dynamic objects are more likely to occupy a large fraction of the frame; when these dynamic foreground objects are rich in features, they can confuse camera tracking algorithms, leading to inaccurate camera poses that track the dynamic object rather than producing true camera motion with respect to the environment. In contrast, wide-angle fisheye videos capture more background regions, which tend to have stable features for tracking, yielding more reliable camera poses. We first use ORB-SLAM2's stereo estimation mode [[61]](#b60) to identify trackable sequences within the videos, utilizing the method devised by Zhou et al. to divide videos into discrete, trackable shots [[118]](#b117). For each given shot, consisting of frames (I i , . . . , I n ), we estimate camera poses and rig calibration via an incremental global bundle adjustment algorithm similar to COLMAP [[76]](#b75). We initialize the stereo rig calibration to be that of a rectified stereo pair with baseline 6.3 cm, but optimize for the calibration as part of the bundle adjustment process, as in practice the stereo rig can vary significantly from its nominal configuration. This process yields a camera position c i and orientation R i for each frame i (defined as the pose of the left camera), and a position c r and orientation R r for the right camera relative to the left (assumed to be constant throughout the shot).

## Depth estimation

Depth estimation is first performed on a per-frame basis, with disparity maps computed independently for each frame.

We use the estimated camera rig calibration c r , R r to rectify the original high resolution equirectangular video frames, ensuring that (1) the left and right views have centered principal points, (2) are oriented perpendicular to the baseline, and (3) pointing in a parallel direction. We then convert the equirectangular videos to perspective projections for downstream predictions.

Disparity is estimated from optical flow [[84,](#b83)[90]](#b89) between the rectified left and right frames. The x-component of the optical flow is used as disparity, which is converted to met-ric depth using:

$Depth = baseline × f disparity .(8)$Here baseline = 0.063m, and f is the frame's focal length.

Outlier Rejection. Several criteria are applied to filter out unreliable pixels: Inconsistency between left and right eyes: Disparity is rejected if the optical flow fails a cycleconsistency check with an error exceeding one pixel. Depth values exceeding 20 meters are considered invalid. Estimating accurate depth beyond a certain range requires sub-pixel disparity estimation, and therefore the resulting depths are usually very noisy. Negative flow values that shouldn't occur, but can, often due to errors in textureless regions. Large vertical flow: pixels with a y-component of flow exceeding one pixel are removed (as in our rectified stereo pairs correspondences should have the same y-value, and violating that epipolar constraint indicates uncertain matches). Occlusion boundaries: Depth gradients exceeding a threshold (threshold = 0.3) indicate occlusion boundaries and are rejected. For a pixel location (x, y), depth gradients are computed as:

$grad x = |Depth(x + 1, y) -Depth(x -1, y)|, grad y = |Depth(x, y + 1) -Depth(x, y -1)|.$Pixels are rejected if grad x > threshold × Depth(x, y) or grad y > threshold × Depth(x, y).

## 2D tracks

We extract long-range 2D point trajectories using Boot-sTAP [[17]](#b16). We run tracking on the left-eye video only. For every 10 frames, we uniformly initialize query points on image with stride 4. We then remove duplicated queries if earlier tracks fall within 1 pixel of a query point.  

## Initial 3D tracks Stereo video

Optimized 3D tracks When converting the equirectangular videos to perspective projections, we use two FoVs: 60 • and 120 • . Both perspective videos are set to a resolution of 512×512, the maximum supported by BootsTAP. The 60 • projection offers a higher sampling rate in scene units, which improves the accuracy of depth estimation and 2D tracks when measured in meters. Additionally, it has smaller perspective distortion near the image boundaries. In contrast, the 120 • projection provides wider coverage, ensuring longer 2D tracks across the videos. This trade-off allows us to balance data quality with spatial coverage for downstream tasks, e.g. DynaDUSt3R.

We take the union of the 3D tracks derived from each of these videos for DynaDUSt3R training supervision.

10. DynaDUSt3R training details.

Dataloader. During training, we randomly sample two frames from the training videos that are at most 60 frames apart, at times t 0 and t 1 , (t 0 < t 1 ). Additionally, we also sample one auxiliary frame in between, at time t aux , t 0 < t aux < t 1 , for additional track supervision between the two input frames. During training, we add data augmentation by applying random crops and color jitter to the input images and cropping the ground truth pointmap and motionmap accordingly.

Training. The network takes input the two RGB images as well as query times t q = {0, 1, taux-t0 t1-t0 } and predicts the pointmaps for the two input views and motionmaps for each query t q . We supervise the network with losses defined in Eqn. 6 and 7. We initialize our network with the DUSt3R weights and initialize the motion head with the same weights as the point head. We finetune for 49k iterations with batch size 64, learning rate 2.5 × 10 -5 , and optimized by Adam with weight decay 0.95.

![Figure2. Data processing pipeline. Our method starts with VR180 (wide-angle, stereoscopic) videos, and estimates metric stereo depth, 2D point tracks, and camera poses. These quantities allow the tracks to be lifted to 3D where they are filtered and denoised to produce world-space, metric 3D point trajectories.]()

![Figure 3. Effect of track optimization. Comparing motion trajectories before and after track optimization, we see that optimization resolves the high frequency jitter along camera rays, affecting both static and dynamic content. After optimization, static content has static tracks, and dynamic tracks are less noisy.]()

![Figure 4. Diverse motion: Stereo4D captures a wide variety of types of moving objects, from swimming fish, to walking pedestrians, moving vehicles, and a farmer sowing seeds. It includes source videos captured with both stationary (left) and moving (right) cameras.]()

![Figure 5. Diverse scene content: A word cloud of captioned frames from our dataset shows our data is diverse, including a variety of common objects seen in videos.]()

![Figure 6. DynaDUSt3R architecture. Given two images (I0, I1) of a dynamic scene and a desired target time tq, the images are passed through a ViT encoder and transformer decoder. The resulting features are processed by (1) a pointmap head that predicts 3D points in the coordinate frame of I0, and (2) a 3D motion head that predicts the motion of all points to the target time tq. A double outline indicates a new component compared to DUSt3R.]()

![During training, we randomly sample pairs of video frames that are at most 60 frames apart. The weight for the confidence loss in Eqn 6-7 is α m = α p = 0.2. The model is trained on tracks extracted from both 60 • FoV videos for (higher quality) and 120 • FoV videos for (larger coverage).]()

![Figure 7. Testing on held out examples from Stereo4D. We visualize image pairs and corresponding dynamic 3D point clouds predicted by DynaDUSt3R. It recovers accurate 3D shape and complex scene motion for objects such as people breakdancing and cows walking.]()

![Figure 8. Qualitative comparisons, 3D motion on the Stereo4D. We compare variants of DynaDUSt3R trained on different data sources. The PointOdyssey-trained model incorrectly predicts significant 3D motion on static elements such as the building wall and the banners near the streetlight, while the Stereo4D-trained model correctly predicts these elements as stationary. The Stereo4D model also makes more precise motion predictions for dynamic objects, such as humans with large movements (bottom row).]()

![Figure 9. Qualitative comparisons of predicted 3D motion on ADT [64]. DynaDUSt3R trained on Stereo4D produces more accurate 3D motion compared to training on PointOdyssey.]()

![Figure 10. Qualitative comparison, 3D structure on Bonn [63]. From left to right, we show an input image pair, predictions from different methods, and the ground truth geometry. The top two rows show 3D point clouds from two viewpoints, where we show the union of the pointmaps for the two input time steps. The bottom row shows the corresponding disparity for two input images. Compared to all the other methods, DynaDUSt3R trained on Stereo4D achieves better 3D structure predictions with finer details.]()

![Figure 11. Camera statistics from Stereo4D. We measure the difference (in meters) of camera poses between the start and end frame of each video clip as calculated by SfM.]()

![Figure 12. Scene motion statistics from Stereo4D. We measure the motion in terms of pixel displacement projected onto the image frame. For each video, we measure the percentage of tracks that have 3D trail length greater than 50 pixels. The 3D trail length is measured by Eqn. 3.]()

![Figure 13. More qualitative results on Stereo4D test set. Extending Fig. 7, we visualize image pairs and corresponding dynamic 3D point clouds predicted by DynaDUSt3R trained on Stereo4D. Our method recovers accurate 3D shape and complex scene motion.]()

![Figure 14. More qualitative comparisons of 3D motion in the Stereo4D test set. Extending Fig. 8, we compare variants of DynaDUSt3R trained on different data sources. The Stereo4D-trained model also makes more precise motion predictions than the PointOdyssey-trained model.]()

![Figure 15. Example equirectangular stereo videos collected from the internet.]()

![Figure16. Effect of Track Optimization. We compare 3D tracks on a challenging walking tour video sequence. In this clip (left), the camera moves forward while vehicles drive toward the camera. We visualize the results across 16 frames, showing 3D trails left by both dynamic and static points. Middle: Our initial 3D tracks, created directly from RAFT, BootsTAP and SfM camera pose, also exhibit significant jitter for both dynamic (vehicle) and static (ground) points. Right: After applying our track optimization, the ground points yield stable, static tracks, and vehicle tracks become smooth and coherent.]()

![EPE 3D ↓ δ 0.05 3D ↑ δ 0.10 3D ↑ DynaDUSt3R (PointOdyssey) 0.6191 11.61 20.25 0.3126 8.56 18.03 DynaDUSt3R (Stereo4D) 0.1110 65.07 75.18 0.1231 51.98 65.20 Synthetic]()

![Quantitative comparison, depth maps.]()

