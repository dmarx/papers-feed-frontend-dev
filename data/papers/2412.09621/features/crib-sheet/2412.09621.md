- **Objective**: Develop a framework to extract 3D motion data from online stereoscopic videos, enabling the understanding of dynamic 3D content.
  
- **Key Contributions**:
  - A pipeline for obtaining real-world, dynamic, pseudo-metric 4D reconstructions from online video.
  - Introduction of DynaDUSt3R, a model predicting 3D structure and motion from image pairs.

- **Data Source**: Utilizes online stereoscopic fisheye videos (VR180) as a scalable source for 3D motion data.

- **Pipeline Overview**:
  1. **Input**: N-frame stereo video pairs \( I_i \) and \( I'_i \).
  2. **Output**: Dynamic 3D point cloud with K points, each with time-varying position \( p_{j,i} \).

- **Camera Pose Estimation**:
  - Uses Structure-from-Motion (SfM) to estimate camera poses \( (c_i, R_i) \).
  - Initial calibration \( (c_r, R_r) \) set to a rectified stereo pair with a baseline of 6.3 cm.

- **Depth Estimation**:
  - Per-frame disparity map \( D_i \) estimated using RAFT.

- **3D Track Estimation**:
  - Long-range 2D point trajectories extracted using BootsTAP.
  - 3D motion trajectories \( p_j^1, \ldots, p_j^N \) computed from 2D tracks and camera poses.

- **Optimization Strategy**:
  - Formulates an optimization for per-frame scalar offsets \( \delta_i \) to reduce temporal jitter.
  - Objective function combines:
    - Static loss \( L_{\text{static}} \): Minimizes jitter among static points.
    - Dynamic loss \( L_{\text{dynamic}} \): Reduces acceleration along the camera ray.
    - Regularization loss \( L_{\text{reg}} \): Encourages faithfulness to original disparity estimates.

- **Loss Functions**:
  - Static loss:
    \[
    L_{\text{static}} = \sum_{i=1}^{N} \sum_{j=1}^{N} \frac{\|p'_i - p'_j\|^2}{N'_p^2}
    \]
  - Dynamic loss:
    \[
    L_{\text{dynamic}} = \sum_{i=1}^{N} \Delta \in W \|p'_{i+\Delta} - 2p'_i + p'_{i-\Delta}\|^2
    \]
  - Regularization loss:
    \[
    L_{\text{reg}} = \lambda_{\text{reg}} \sum_{i=1}^{T} \left( \frac{1}{\delta_i} + \frac{\|p_i - c_i\| - 1}{\|p_i - c_i\|^2} \right)
    \]

- **Full Objective Function**:
  \[
  \min_{\delta_i} \sum_{i=1}^{N} \sigma(m)L_{\text{static}} + (1 - \sigma(m))L_{\text{dynamic}} + L_{\text{reg}}
  \]

- **Implementation Details**:
  - Shot selection using ORB-SLAM2 for discrete, trackable shots.
  - Field of view set to 140Â° for reliable camera pose estimation.
  - Stereo confidence checks to discard unreliable pixels.

- **Utility of Dataset**: The extracted data supports training models for predicting geometry and motion, enhancing applications in computer vision.