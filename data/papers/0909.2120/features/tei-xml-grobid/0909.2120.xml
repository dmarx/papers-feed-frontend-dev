<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">APPROXIMATE MAXIMIZERS OF INTRICACY FUNCTIONALS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2009-09-13">13 Sep 2009</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">J</forename><surname>Buzzi</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratoire de Probabilités et Modèles Aléatoires (CNRS U.M.R</orgName>
								<orgName type="institution">U.F.R. Mathematiques</orgName>
								<address>
									<addrLine>7599) and Université Paris 6 -Pierre et Marie Curie Case 188, 4 place Jussieu</addrLine>
									<postCode>75252 cedex 05</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">L</forename><surname>Zambotti</surname></persName>
							<email>lorenzo.zambotti@upmc.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratoire de Probabilités et Modèles Aléatoires (CNRS U.M.R</orgName>
								<orgName type="institution">U.F.R. Mathematiques</orgName>
								<address>
									<addrLine>7599) and Université Paris 6 -Pierre et Marie Curie Case 188, 4 place Jussieu</addrLine>
									<postCode>75252 cedex 05</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">APPROXIMATE MAXIMIZERS OF INTRICACY FUNCTIONALS</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2009-09-13">13 Sep 2009</date>
						</imprint>
					</monogr>
					<idno type="MD5">4D350798461EA0989C96824E06EE2229</idno>
					<idno type="arXiv">arXiv:0909.2120v2[math.PR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>2000 Mathematics Subject Classification. 94A17</term>
					<term>92B30</term>
					<term>60C05 Entropy</term>
					<term>Complexity</term>
					<term>Maximization</term>
					<term>Discrete probability</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>G. Edelman, O. Sporns, and G. Tononi introduced in theoretical biology the neural complexity of a family of random variables. This functional is a special case of intricacy, i.e., an average of the mutual information of subsystems whose weights have good mathematical properties. Moreover, its maximum value grows at a definite speed with the size of the system.</p><p>In this work, we compute exactly this speed of growth by building "approximate maximizers" subject to an entropy condition. These approximate maximizers work simultaneously for all intricacies. We also establish some properties of arbitrary approximate maximizers, in particular the existence of a threshold in the size of subsystems of approximate maximizers: most smaller subsystems are almost equidistributed, most larger subsystems determine the full system.</p><p>The main ideas are a random construction of almost maximizers with a high statistical symmetry and the consideration of entropy profiles, i.e., the average entropies of sub-systems of a given size. The latter gives rise to interesting questions of probability and information theory.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1. Introduction 1.1. Neural Complexity, a measure of complexity from theoretical biology. In <ref type="bibr" target="#b15">[16]</ref>, G. Edelman, O. Sporns and G. Tononi introduced the so called neural complexity of a family of random variables. It is defined as an average of mutual information between any subfamily and its complement, see below. It has been considered from a theoretical and experimental point of view by a number of authors, see e.g. <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>.</p><p>In order to define the neural complexity, we need to recall two classical definitions. If X is a random variable taking values in a finite space E, then its entropy is defined by H(X) := -x∈E P X (x) log(P X (x)), P X (x) := P(X = x).</p><p>Given two random variables defined over the same probability space, the mutual information between X and Y is MI(X, Y ) := H(X) + H(Y ) -H(X, Y ).</p><p>We refer to the Appendix for a review of the main properties of the entropy and the mutual information. Edelman, Sporns and Tononi consider systems formed by a finite family X = (X i ) i∈I and define the following concept of complexity. For any S ⊂ I, they divide the system into two subsystems:</p><formula xml:id="formula_0">X S := (X i , i ∈ S), X S c := (X i , i ∈ S c ),</formula><p>where S c := I\S. Then they compute the mutual information MI(X S , X S c ) and consider the sum</p><formula xml:id="formula_1">I(X) := 1 |I| + 1 S⊂I 1 |I| |S| MI(X S , X S c ),<label>(1.1)</label></formula><p>where |I| denotes the cardinality of I. Note that I(X) is really a function of the law of X.</p><p>As shown in <ref type="bibr" target="#b1">[2]</ref>, one can define more general functionals</p><formula xml:id="formula_2">I c (X) := S⊂I c I S MI(X S , X S c ),</formula><p>which have similar properties, provided the properties of "exchangeability" and "weak additivity" still hold, see Sec. 2. The resulting functionals have been called intricacies in <ref type="bibr" target="#b1">[2]</ref>. Using a super-additivity argument, we showed in <ref type="bibr" target="#b1">[2]</ref> that the maximum value of any intricacy over systems with a given size grows linearly with the size. In this paper, we compute exactly this speed of growth by building "approximate maximizers", i.e., families of an increasing number of random variables taking value in a fixed set and achieving, in the limit, the maximum intricacy per variable. Moreover, we shall construct in this paper a sequence of simultaneous approximate maximizers for all intricacies.</p><p>Our construction is probabilistic in a fundamental way. We shall show that maximizers should approximately satisfy strong symmetries (see Theorem 1.6), that cannot be satisfied exactly (Lemma 3.8). We shall exhibit a random sequence of systems, which satisfy such symmetries in law, and approximately satisfy the same symmetries almost surely.</p><p>If the family (X i ) i∈I is completely deterministic or, on the contrary, independent, then every mutual information vanishes and therefore I (X) = 0. As these examples suggest, large values of I require compromising between randomness and mutual dependence, i.e., to have non-trivial correlation between X S and X S c for many subsets S. This explains why maximizing this functional is not a trivial problem. 1.2. Main Results. For the sake of simplicity, we state our results in this introduction only for the neural complexity (1.1), deferring the analogous results for arbitrary intricacies to Section 5.</p><p>First, we need some notations. The integers N ≥ 1 and d ≥ 2 will denote respectively the cardinality of the family (X i ) i∈I and of the range of each X i . Moreover,</p><p>• Λ N,d := {0, . . . , d -1} N is the set of configurations, i.e., of possible values for the random vector X; • X (d, N) is the set of all Λ N,d -valued random variables X, which we shall identify with M(d, N), set of all probability measures on Λ N,d .</p><p>In particular, we write indifferently H(X) and H(µ), as well as I(X) and I(µ). Of course, entropy and intricacy are in fact functions of the law µ of X and not of the (random) values of X.</p><p>Let us state our main results in the case of the neural complexity:</p><p>Theorem 1.1. Let I (X) be the neural complexity (1.1) of Edelman-Sporns-Tononi.</p><p>(1) We have for all µ ∈ M(d, N), setting</p><formula xml:id="formula_3">x µ := H(µ) N log d , I(µ) N log d ≤ x µ (1 -x µ ) ≤ 1 4 .<label>(1.2)</label></formula><p>(2) The maximum value of the intricacy at fixed size</p><formula xml:id="formula_4">I(d, N) := max X∈X (d,N ) I(X) = max µ∈M(d,N ) I(µ) satisfies: lim N →∞ I(d, N) N = log d 4 .</formula><p>(3) For any x ∈ [0, 1], there exists a sequence µ N ∈ M(d, N) approaching the upper bound of point (1), i.e., satisfying:</p><formula xml:id="formula_5">lim N →+∞ H(µ N ) N log d = x, lim N →+∞ I(µ N ) N log d = x(1 -x).<label>(1.3)</label></formula><p>Remark 1.2. We shall actually prove this theorem for arbitrary intricacies (see Theorem 5.1). More precisely, and perhaps unexpectedly, we shall build, for each d ≥ 2, a sequence µ N ∈ M(d, N) satisfying, simultaneously for all intricacies I c , lim</p><formula xml:id="formula_6">N →∞ I c (µ N ) N = lim N →∞ max µ∈M(d,N ) I c (µ) N ,</formula><p>see Remark 5.5 below.</p><p>Remark 1.3. All of the above is new, though numerical experiments by previous authors [16, Fig. <ref type="figure" target="#fig_3">1</ref>] had suggested the concavity and the symmetry of the maximal intricacy given the entropy, but not its quadratic form.</p><p>While the upper bound (1.2) follows from direct computations, the existence of sequences (µ N ) N satistying (1.3) is much less trivial and is the main result of this paper. As shown in Theorem 1.6 below, such sequences must exhibit a nontrivial behavior, combining a large amount of local independence and of non-trivial correlation on a global level.</p><p>The existence of approximate x-maximizers, i.e., sequences µ N ∈ M(d, N) satisfying (1.3), follows in our approach from a probabilistic construction: we shall prove that uniform distributions on appropriately chosen random sparse supports will have almost surely the desired properties: see Proposition 4.3 below.</p><p>In the course of the proof, we also obtain rather detailed information on the structure of approximate x-maximizers. A key notion is the following one. Definition 1.4. Given X ∈ X (d, N), its entropy profile is the function h</p><formula xml:id="formula_7">X : [0, 1] → [0, 1] such that h X (0) = 0, h X k N = 1 N k S⊂I, |S|=k H(X S ) N log d , k ∈ I := {1, . . . , N} and h X is affine on each interval k-1 N , k N , k ∈ I. Theorem 1.5. For x ∈ [0, 1], let the ideal profile be h * x (t) = x ∧ t = min{x, t}.</formula><p>Then for any sequence µ N ∈ M(d, N) of approximate x-maximizers, we have</p><formula xml:id="formula_8">h µ N -h * x sup := sup t∈[0,1] |h µ N (t) -h * x (t)| → 0 as N → ∞.</formula><p>In particular, for any sequence µ N ∈ M(d, N) of approximate maximizers, i.e., such that lim N →∞ I(µ N )/N = log d/4, we have:</p><formula xml:id="formula_9">lim N →+∞ H(µ N ) N log d = 1/2, lim N →+∞ h µ N -h * 1/2 sup = 0.</formula><p>Again, we prove in fact a version of this result for all intricacies, see Theorem 5.2 below.</p><p>If (µ N ) N is a sequence of approximate x-maximizers and X N ∈ X (d, N) has law µ N , we say that (X N ) N is also a sequence of approximate x-maximizers.</p><p>A corollary of the convergence of entropy profiles is the existence of a threshold in the behavior of typical subsystems of approximate x-maximizers: if |S| ≤ xN, then X N S is almost uniform, which corresponds to local independence; if |S| ≥ xN, then the whole family X N is almost a function of X N S , which corresponds to strong global correlation. Recall that H(Y | Z) is the conditional entropy of Y given Z, see the Appendix below.</p><p>Theorem 1.6. Let (X N ) N be an approximate x-maximizer. Let y ∈]0, 1[ and set</p><formula xml:id="formula_10">k N := ⌊yN⌋. Consider the N k N sub-systems X S of X N of size k N = |S|.</formula><p>For all ǫ &gt; 0, if N is large enough, then except for at most ǫ N k N of such subsets S, the following holds:</p><formula xml:id="formula_11">• if y ≤ x: X S is almost uniform: 1 -ǫ ≤ H(X S ) |S| log d ≤ 1;</formula><p>• if y ≥ x: X S almost determines the whole system</p><formula xml:id="formula_12">X N : 0 ≤ H(X N |X S ) N log d ≤ ǫ.</formula><p>Again, we prove a more general version of this result in Theorem 5.6 below.</p><p>1.3. Strategy of Proof and Organization of the paper. The main ideas of the proofs of the two theorems are a probabilistic construction of the sequence maximizers and the consideration of the entropy profiles h X defined above. As we indicated, in fact we analyze arbitrary intricacies generalizing neural complexity.</p><p>In section 2 we recall the notion of intricacy as a family of functionals over finite sets of discrete random variables satisfying exchangeability and weak additivity and give simple examples. In section 3 we give upper bounds on the intricacies of arbitrary systems of given size and entropy. In section 4 we prove the main results by means of a probabilistic construction of random approximate maximizers. In section 5 we collect our results for arbitrary intricacies. An Appendix contains basic facts from entropy theory for the convenience of the reader.</p><p>1.4. Further questions. The bound x(1x) in (1.3) is symmetric with respect to x = 1/2 and independent of d ≥ 2. We do not know whether these simple properties, which extend to arbitrary intricacies (see Theorem 5.1), can be proved directly, e.g.: does there exist a duality operation in X (N, d) exchanging systems with entropy xN log d and (1x)N log d while preserving their intricacy? Can one deduce from a system in X (d, N) with entropy H and intricacy I a system in X (d ′ , N) with entropy (log d ′ / log d)H and intricacy (log d ′ / log d)I?</p><p>This work has focused on properties of systems with size tending to infinity. Notice that we know very little on the exact maximizers for fixed size beyond the constraints on their entropy contained in our main results. Because of the invariance properties of intricacy (see Lemma 2.8 and the following comment), exact maximizers are nonunique but we do not even know if there are only finitely many of them.</p><p>Our construction of approximate maximizers is probabilistic. Could it be done deterministically? Would the corresponding algorithms possess a computational complexity related to the complexity that intricacies are supposed to describe? Our construction is global but could systems with maximum intricacy be built by a local approach, i.e., a "biologically reasonable" building process, using some type of local rules and/or evolution? That is, does there exist a "reasonable" self-map T : M(d, N) → M(d, N) such that the neural complexity of T n (µ) converges to the maximum as n → ∞ for "many" µ ∈ M(d, M).</p><p>Our work also leads to interesting probabilistic constructions and questions in the theory of entropy and information. For instance: Problem. Describe the set of functions h : {0, . . . , N} → R obtained from picking X ∈ X (d, N) and setting h(k) to be the average entropy of X S where S ranges over the subsets of {1, . . . , N} with cardinality k.</p><p>Basic properties of the entropy (recalled in the Appendix) imply that h(0</p><formula xml:id="formula_13">) = 0, 0 ≤ h(k + 1) -h(k) ≤ log d for 0 ≤ k &lt; N and h(k + ℓ) -h(k) ≤ h(j + ℓ) -h(j) for 0 ≤ j ≤ k ≤ k + ℓ ≤ N.</formula><p>However we shall show that not all such functions h arise from some X ∈ X (d, N), see Lemma 3.8. See <ref type="bibr" target="#b7">[8]</ref> for a closely related question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Intricacy</head><p>2.1. Definition. In this paper, a system is a finite collection (X i ) i∈I of random variables, each X i taking value in the same finite set V . Without loss of generality, we assume that V = {0, . . . , d -1} for all i ∈ I and some d ≥ 2 (d should be thought of as a convenient normalization) and I is a set of positive integers. We let M = d≥2 M(d) = d≥2,N ≥1 M(d, N) be the set of the corresponding laws, that is, the probability measures on {0, . . . , d-1} I for each finite subset I ⊂ N * := {1, 2, 3, . . . }.</p><p>For S ⊂ I, we denote X S := (X i , i ∈ S). In <ref type="bibr" target="#b1">[2]</ref>, we defined the following family of functionals over such systems (more precisely: over their laws) formalizing (and slightly generalizing) the neural complexity of Edelman-Sporns-Tononi <ref type="bibr" target="#b15">[16]</ref>: where S c := I \ S. The corresponding mutual information functional is I c : M → R defined by:</p><formula xml:id="formula_14">I c (X) := S⊂I c I S MI (X S , X S c ) .</formula><p>By convention, MI (X ∅ , X I ) = MI (X I , X ∅ ) = 0. An intricacy, is a mutual information function satisfying:</p><p>(1) exchangeability (invariance by permutations): if I, J ⊂⊂ N * and φ :</p><formula xml:id="formula_15">I → J is a bijection, then I c (X) = I c (Y ) for any X := (X i ) i∈I , Y := (X φ -1 (j) ) j∈J ;</formula><p>(2) weak additivity: for any two independent sub-systems (X i ) i∈I , (Y j ) j∈J (defined on the same probability space):</p><formula xml:id="formula_16">I c (X, Y ) = I c (X) + I c (Y ). I c is non-null if some coefficient c I S with S / ∈ {∅, I} is not zero.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Classification of intricacies. In section 3 of [2] the following has been proved</head><p>Proposition 2.2. A mutual information functional I c determines its coefficients uniquely and the following equivalences hold:</p><p>• I c is exchangeable if and only if c I S depends only on |I| and |S|; • an exchangeable I c is weakly additive if and only if there exists a random variable W c → [0, 1] such that W c and 1 -W c have the same law and</p><formula xml:id="formula_17">c N k = E (1 -W c ) N -k W k c = [0,1] x k (1 -x) N -k λ c (dx),<label>(2.2)</label></formula><p>where λ c is the law of W c . • an exchangeable weakly additive I c is non-null iff λ c (]0, 1[) &gt; 0, in which case all coefficients c I S are non-zero.</p><p>In this paper we consider only non-null intricacies I c .</p><p>Example 2.3. The intricacy I of Edelman-Sporns-Tononi is defined by the coefficients:</p><formula xml:id="formula_18">c I S = 1 |I| + 1 1 |I| |S| (2.3)</formula><p>and it is easy to see that in this case (2.2) holds with W c a uniform variable over [0, 1], see Lemma 3.8 in <ref type="bibr" target="#b1">[2]</ref>. For 0 &lt; p &lt; 1, the symmetric p-intricacy I p is defined by</p><formula xml:id="formula_19">c I S = 1 2 p |S| (1 -p) |I\S| + (1 -p) |S| p |I\S|</formula><p>and in this case W c is uniform on {p, 1 -p}. For p = 1/2, this yields the uniform intricacy I U (X) with:</p><formula xml:id="formula_20">c I S = 2 -|I|</formula><p>, and W c = 1/2 almost surely. All these functionals are clearly non-null and exchangeable.</p><p>Remark 2.4. The global 1/(|I| + 1) factor in (2.3) is not present in <ref type="bibr" target="#b15">[16]</ref>, which did not compare systems of different sizes. However it is necessary in order to have weak additivity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Simple examples.</head><p>Let X i take values in {0, . . . , d -1} for all i ∈ I, a finite subset of N * .</p><p>Example 2.5. If the variables X i are independent then each mutual information is zero and therefore:</p><formula xml:id="formula_21">I c (X) = 0. Example 2.6. If each X i is a.s. equal to a constant c i in {0, . . . , d -1}, then, for any S = ∅, H(X S ) = 0. Hence, I c (X) = 0. Example 2.7. If X 1 is uniform on {0, . . . , d -1} and X i = X 1 for all i ∈ I, then, for any S = ∅, H(X S ) = log d and, if additionally S c = ∅, H(X S X S c ) = 0 so that each mutual information MI(X S ; X S c ) is log d. Hence, I c (X) = S⊂I\{∅,I} c I S • log d = 1 -c I ∅ -c I I log d ≤ log d.</formula><p>Examples 2.5 and 2.6 correspond to, respectively, maximal and minimal total entropy. In these extreme cases I c = 0. Example 2.7 has positive total entropy and intricacy (if all c I S are non zero). However, the values of the intricacy grow very slowly with |I| in these examples: they stay bounded. We shall see however how to build systems (X i ) i∈I ∈ X (d, I) which realize much larger values of I c , namely of the order of |I|.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.4.</head><p>Invariance properties of intricacies. We have the following obvious invariances of intricacies.</p><p>Lemma 2.8. The intricacies are invariant under the following group actions on X (d, N) for some N, d ≥ 1:</p><p>(1) the group S N of permutations on {1, . . . , N} acting on X (d, N) by: (σX</p><formula xml:id="formula_22">) i = X σ -1 (i) , ∀ i = 1, . . . , N.</formula><p>(2) the Nth power (S d ) N of the permutation group on {0, . . . , d -1} acting on X (d, N) by: (σX</p><formula xml:id="formula_23">) i = σ i • X i , ∀ i = 1, . . . , N.</formula><p>In particular, for N, d ≥ 2, the maximum of I c over X ∈ X (d, N) cannot be achieved at a single probability measure on Λ d,N = {0, . . . , d -1} N . Indeed, if it were the case, then this measure would be invariant under the group action (2) above. However, this action is transitive on Λ d,N . Therefore the measure would be equidistributed on this set. Hence the maximizer would be a family of independent variables, for which the intricacies are all zero. This is a contradiction whenever N, d ≥ 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Upper bounds on intricacies</head><p>In <ref type="bibr" target="#b1">[2]</ref>, it was proved that I c (X) &lt; N log d/2 if X ∈ X (d, N). By comparison with "ideal entropy profiles" defined below, we prove sharper upper bounds for systems with given size and entropy.</p><p>3.1. Definitions. We define the ideal entropy profile and the corresponding intricacy values both for finite size and in the limit N → ∞. We also introduce an adapted norm to measure the distance between profiles.</p><p>Let I c be some intricacy. It is convenient to use the following probabilistic representation of the coefficients c based on the the random variable W c with law λ c defined by (2.2). Let (Y i ) i≥1 be a sequence of i.i.d. uniform random variables on [0, 1] and let</p><formula xml:id="formula_24">D N := N k=1 ½ (Y k ≤Wc) , β N := D N N , N ≥ 1. (3.1)</formula><p>Conditionally on W c , D N is a binomial variable with parameters (N, W c ). In particular, for all g : N → R, by (2.2)</p><formula xml:id="formula_25">E (g (D N )) = [0,1] N k=0 N k x k (1 -x) N -k g(k) λ c (dx) = N k=0 c N k N k g(k),<label>(3.2)</label></formula><p>and therefore, for all bounded Borel f :</p><formula xml:id="formula_26">[0, 1] → R E (f (β N )) = N k=0 c N k N k f k N . (3.3)</formula><p>We recall the Definition 1.4 of the entropy profile of X ∈ X (d, N): h X (0) = 0,</p><formula xml:id="formula_27">h X k N = 1 N k S⊂I, |S|=k H(X S ) N log d , k ∈ I := {1, . . . , N}</formula><p>and h X is affine on each interval k-1 N , k N , k ∈ I. We can now define the ideal profiles and their intricacies. Definition 3.1. For x ∈ [0, 1] and N ≥ 1, the ideal entropy profile is</p><formula xml:id="formula_28">h * x (t) := t ∧ x = min{t, x}<label>(3.4)</label></formula><p>and the corresponding (normalized) intricacies are, for finite N:</p><formula xml:id="formula_29">i c N (x) := 2 N k=0 c N k N k h * x (k/N) -x = 2 E (x ∧ β N ) -x (3.5)</formula><p>and, for N → ∞:</p><formula xml:id="formula_30">i c (x) := 2 1 0 (t ∧ x) λ c (dt) -x = 2 E(x ∧ W c ) -x. (3.6)</formula><p>We remark that the ideal profile h * x does not depend on the intricacy I c . Finally, we define a family of norms. For all bounded Borel f :</p><formula xml:id="formula_31">[0, 1] → R, let f c,N := N k=0 c N k N k |f (k/N)| = E (|f (β N )|) .<label>(3.7)</label></formula><p>Remark 3.2. For the particular cases of Example 2.3 we have more explicit expressions. For the Edelman-Sporns-Tononi neural complexity, the above reduces to i(x) = x(1x), x ∈ [0, 1], for the uniform intricacy</p><formula xml:id="formula_32">i U (x) = min{x, 1 -x}, x ∈ [0, 1],</formula><p>and for the symmetric p-intricacy</p><formula xml:id="formula_33">i p (x) = min{x, 1 -x, p, 1 -p}, x ∈ [0, 1].</formula><p>3.2. Upper bounds and distance from the ideal profile. In this section we prove the following upper bounds Proposition 3.3. Let I c be an intricacy.</p><p>(</p><formula xml:id="formula_34">) i c : [0, 1] → [0, 1] is a concave function admitting the Lipschitz constant 1 and symmetric about 1/2: i c (1 -x) = i c (x). Moreover, i c (1/2) = max x∈[0,1] i c (x). (2) |i c (x) -i c N (x)| ≤ 1/ √ N . (3) All systems X ∈ X (d, N) with H(X) N log d = x satisfy: I c (X) N log d = i c N (x) -h X -h * x c,N ≤ i c N (x). (3.8) (4) If X N ∈ X (d, N) and lim N →∞ H(X N ) N log d = x, then lim sup N →∞ I c (X N ) N log d ≤ i c (x). (3.9)<label>1</label></formula><p>Observe that to show that i c (x) is indeed the value of the limit (3.9), rather than a mere upper bound, requires to prove the existence of sequences saturating the inequality. This is deferred to the next section. Before proving Proposition 3.3, we need some preliminary material which will also be useful later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.3.</head><p>The functions i c N (x) and i c (x). We consider the first two points of the proposition, beginning with the convergence of i c N (x) → i c (x).</p><p>Lemma 3.4. For all x ∈ [0, 1]</p><formula xml:id="formula_35">|i c N (x) -i c (x)| ≤ 1 2 √ N , N ≥ 1.</formula><p>Proof. We use the probabilistic representations (3.6) and (3.5) and we obtain</p><formula xml:id="formula_36">|i c N (x) -i c (x)| ≤ E (|h * x (β N ) -h * x (W c )|) ≤ E (|β N -W c |) ≤ E |β N -W c | 2 .</formula><p>Since D N = Nβ N is, conditionally on W c , a binomial variable with parameters (N, W c ), we have that</p><formula xml:id="formula_37">E |β N -W c | 2 = E (Var (β N | W c )) = E W c (1 -W c ) N ≤ 1 4N<label>(3.10)</label></formula><p>and the result is proven. Now, we analyze the limit function i c (x).</p><p>Lemma 3.5.</p><p>(</p><formula xml:id="formula_38">) i c (x) = E(min{x, 1 -x, W c , 1 -W c }) for all x ∈ [0, 1]. (2) The function i c : [0, 1] → [0, 1] is 1-Lipschitz and concave. The distributional second derivative of i c is -2λ c . (3) i c (x) = i c (1 -x) for all x ∈ [0, 1]. (4) i c achieves its maximum at x = 1/2 and i c (1/2) = E(W c ∧ (1 -W c )).<label>1</label></formula><p>(5) i c is maximum only at x = 1/2 if and only if 1/2 belongs to the support of λ c .</p><p>Proof. First, for all x, a ∈ [0, 1]</p><formula xml:id="formula_39">x ∧ a + x ∧ (1 -a) -x = min{x, 1 -x, a, 1 -a}. (3.11)</formula><p>Indeed, one can assume a ≤ 1a and then check the above in the three cases:</p><p>x ≤ a, a ≤ x ≤ 1a and x ≥ 1a. Since W c has same law as 1 -W c , point (1) and (3) follow. Concavity, 1-Lipschitz continuity and symmetry w.r.t. 1/2 follow easily. Moreover, an integration by parts shows that for all ϕ ∈ C ∞ (R) with compact support contained in (0, 1):</p><formula xml:id="formula_40">[0,1] ϕ ′′ (x) i c (x) dx = 2 [0,1] [0,1] ϕ ′′ (x) x ∧ t dx λ c (dt) - [0,1] ϕ ′′ (x) x dx = 2 [0,1] t 0 ϕ ′′ (x) x dx + 1 t ϕ ′′ (x) tdx λ c (dt) -0 = 2 [0,1] [ϕ ′ (t)t -ϕ(t) -tϕ ′ (t)] λ c (dt) = -2 [0,1] ϕ(t) λ c (dt), proving that (d/dx) 2 i c = -2λ c as distributions. Point (2) is proved. Since i c is concave and i c (x) = i c (1 -x) then i c (x) ≤ i c (1/2) = E(W c ∧ (1 -W c )) for all x ∈ [0, 1]. Point (4) is proved.</formula><p>Let us now assume x &lt; 1/2 (the other case being similar) so that</p><formula xml:id="formula_41">x = x ∧ (1 -x). Set w = W c ∧ (1 -W c ). Then, by (3.11) i c (1/2) -i c (x) = E (w -x ∧ w) = E (w -x) ½ x&lt;w) .</formula><p>Hence, i c (x) &lt; i c (1/2) if and only if P(x &lt; w) &gt; 0, i.e. 1/2 is the unique maximum point if and only if λ c (]x, 1x[) &gt; 0 for all x &lt; 1/2. This proves the last point. These sets are endowed with the partial order: h ≤ g if and only if h(t) ≤ g(t) for all t ∈ [0, 1]. Each Γ x has a unique maximal element: the previously introduced ideal entropy profile, h * x (t) = t ∧ x. Lemma 3.6. For any X ∈ X (d, N), the entropy profile h X , defined according to Def. 1.4, belongs to Γ.</p><p>Proof. Let X ∈ X(d, N). Setting I := {1, . . . , N} and</p><formula xml:id="formula_42">H k := 1 N k S⊂I, |S|=k H(X S ), k = 0, 1, . . . , N we must prove that 0 = H 0 ≤ H 1 ≤ • • • ≤ H N = H(X), H k+1 -H k ≤ log d, 0 ≤ k &lt; N.</formula><p>The equalities H 0 = 0 and H N = H(X) are obvious. Let 0 ≤ k &lt; N and compute:</p><formula xml:id="formula_43">H k+1 = 1 N k+1 |S|=k+1 H(X S ) = 1 N k+1 |S|=k 1 k + 1 i∈S c H(X S∪{i} ) ≤ k!(N -k -1)! N! |S|=k (N -k)(H(X S ) + log d) = H k + log d</formula><p>where H(X S∪{i} ) ≤ H(X S ) + H(X i ) by (A.3) and H(X i ) ≤ log d by (A.1). The same computation, since H(X S∪{i} ) ≥ H(X S ) by (A.2), proves</p><formula xml:id="formula_44">H k ≤ H k+1 . Let for any h ∈ Γ G c N (h) := 2 N k=0 c N k N k h(k/N) -h(1) = 2 E (h (β N )) -h(1). Lemma 3.7. Fix x ∈ [0, 1]. (1) For all X ∈ X (d, N) G c N (h X ) = I(X) N log d . (2) h * x is the unique maximizer of G c N in Γ x and G c N (h * x ) = i c N (x). (3) For arbitrary h ∈ Γ x , we have h -h * x c,N = |G c N (h) -G c N (h * x )|. (3.12) Proof. Since MI(X, Y ) = H(X) + H(Y ) -H(X, Y ), c I S = c I S c</formula><p>, and S c I S = 1, we obtain</p><formula xml:id="formula_45">I c (X) = 2 N k=0 c N k |S|=k H(X S ) -H(X).</formula><p>Hence, the intricacy can be computed from the entropy profile:</p><formula xml:id="formula_46">I c (X) N log d = 2 N k=0 c N k N k h X (k/n) -h X (1) = G c N (h X )<label>(3.13)</label></formula><p>and ( <ref type="formula" target="#formula_1">1</ref>) is proved. A direct computation yields for arbitrary h ∈ Γ x :</p><formula xml:id="formula_47">|G c N (h * x ) -G c N (h)| = G c N (h * x ) -G c N (h) = 2 N k=0 c N k N k (h * x (k/N) -h(k/N)) = 2 N k=0 c N k N k |h * x (k/N) -h(k/N)| = h * x -h c,N ,</formula><p>since each term is non-negative. This proves <ref type="bibr" target="#b2">(3)</ref>.</p><p>Observe that G c N : Γ x → R is monotone non-decreasing. Hence, setting x = H(X)</p><p>N log d and recalling (3.5)</p><formula xml:id="formula_48">I c (X) = G c N (h X ) ≤ sup h∈Γx G c N (h) = G c N (h * x ) = 2E (x ∧ β N ) -x = i c N (x).</formula><p>Moreover, I c being non-null, all c I S are positive, G c N is increasing and h * x is a maximizer. Uniqueness of the maximizer in Γ x follows from (3.12), and point (2) is proved.</p><p>3.5. Proof of Proposition 3.3. Formula (3.8) follows from Lemma 3.7, since for</p><formula xml:id="formula_49">H(X) N log d = x I c (X) N log d = G c N (h X ) = G c N (h * x ) + G c N (h X ) -G c N (h * x ) = i c N (x) -h X -h * x c,N .</formula><p>To prove (3.9), it is enough to use (3.8), together with the continuity of i c and the uniform convergence of i c N → i c . Proposition 3.3 is proved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.6.</head><p>No system with the ideal profile. We turn to the problem of maximizing I c over X (d, N) at fixed N for a prescribed value of the entropy H(X). The above results show that a system X ∈ X (d, N) such that h X (k/N) = h * x (k/N) (k = 0, 1, . . . , N) with x = H(X)</p><p>N log d would be an exact maximizer. However, the next Lemma shows that such X cannot exist except if K or N -K are bounded, independently of N. Thus, all we can hope is to find systems which approach the ideal profile. This will be done in section 4. </p><formula xml:id="formula_50">[0, N], H(Y σ(1) , . . . , Y σ(k) ) log d = k ∧ H, ∀ σ ∈ S N , ∀k = 1, . . . , N, then H or N -H ≤ H * .</formula><p>Proof. Let K := ⌊H⌋ and K := ⌈H⌉. Without loss of generality, we assume that K ≥ 3 and we proceed by contradiction. Let us condition on the variables (X 3 , . . . , X K ) (in the following paragraphs we simply write "conditional" for "conditional on (X 3 , . . . , X K )). By assumption:</p><p>• (X 1 , X 2 ) belongs to Z := {0, . . . , d -1} 2 ;</p><p>• each X i , K &lt; i ≤ N, is a function of X 1 , X 2 as the conditional entropy of (X 1 , X 2 , X i ) is not bigger than that of (X 1 , X 2 ). Moreover, the conditional entropy of X i is log d. Hence, each such X i defines a partition Z i of Z into d subsets. • For any pair i = j in {1, 2, K + 1, . . . , N}, (X i , X j ) has conditional entropy (H -K + 2) log d, strictly greater than that of X i or X j , both equal to log d.</p><p>In particular, Z i = Z j .</p><p>Thus, we have an injection from {1, 2, K + 1, . . . , N} into the set of partitions of Z into d subsets. This implies:</p><formula xml:id="formula_51">N -K + 2 ≤ d 2 + d -1 d -1 . Thus N -H ≤ H * (d) := d 2 +d-1 d-1</formula><p>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Random Construction of approximate maximizers</head><p>Motivated by (3.9), we introduce the following Definition 4.1. Let I c be some intricacy and let x ∈ [0, 1] and d ≥ 2.</p><p>(1) The entropy-intricacy function I c (d, x) is:</p><formula xml:id="formula_52">sup lim sup N →+∞ I c (µ N ) N log d : µ N ∈ M(d, N), lim N →+∞ H(µ N ) N log d = x . (4.1) (2) A sequence of systems X N ∈ X (d, N), N ≥ 2, is an approximate x- maximizer for I c if lim N →∞ H(X N ) N log d = x and lim N →∞ I c (X N ) N log d = I c (d, x).</formula><p>(3) (X N ) N is an approximate maximizer for</p><formula xml:id="formula_53">I c if lim N →∞ I c (X N ) N log d = max x∈[0,1] I c (d, x). Proposition 3.3 established that I c (d, x) ≤ i c (x). Proposition 4.</formula><p>3 in this section shows that this inequality is in fact an equality.</p><p>In the rest of this section we construct approximate x-maximizers by choosing uniform distributions on random supports with the appropriate size: since H(µ N ) N log d must be close to x and µ N is uniform, then the size of the (random) support of µ N must be close to d x , see (A.1). It turns out that this simple construction yields the desired results. Remark 4.2. In <ref type="bibr" target="#b1">[2]</ref> we have given a different definition of I c (d, x), namely</p><formula xml:id="formula_54">I c (d, x) := lim N →∞ 1 N log d sup I c (X) : X ∈ X (d, N), H(X) N log d -x ≤ δ N</formula><p>for any sequence (δ N ) N of non-negative numbers converging to zero. It is easy to see that this definition and (4.1) actually coincide. We consider a family (W i ) i∈Λ d,M of i.i.d. variables, each uniformly distributed on Λ d,N , defined on a probability space (Ω, F , P). We define a random probability measure on Λ d,N</p><formula xml:id="formula_55">µ N,M (x) := d -M i∈Λ d,M ½ (x=W i ) , x ∈ Λ d,N . (4.2)</formula><p>In what follows we consider random variables X N,M on (Ω, F , P) such that</p><formula xml:id="formula_56">P X N,M = x (W i ) i∈Λ d,M = µ N,M (x), x ∈ Λ d,N . (4.3)</formula><p>In other words,</p><formula xml:id="formula_57">conditionally on (W i ) i∈Λ d,M , X N,M has law µ N,M .</formula><p>We are going to prove the following Remark 4.4. We stress that in the following we denote</p><formula xml:id="formula_58">I c (X N,M ) = I c (µ N,M ), H(X N,M ) = H(µ N,M ) (4.6)</formula><p>and that all these expressions are random variables which depend on (W i ) i∈Λ d,M . In other words, (4.6) indicates entropy and intricacy of the law of X N,M conditionally on (W i ) i∈Λ d,M . This abuse of notation seems necessary, to keep notation reasonably readable. See also Remark 4.6 below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.2.</head><p>Average intricacy of sparse random configurations. We recall that N ≥ 2, M is an integer between 1 and N and S N denotes the set of permutations of {1, . . . , N}. By Lemma 3.7,</p><formula xml:id="formula_59">I c (X N,M ) = 2 N k=0 c N k N k h X (k/N) -h X (1)</formula><p>, hence we get:</p><formula xml:id="formula_60">E I c (X N,M ) = 1 N! σ∈S N 2 N k=1 c N k N k E H(X N,M {σ(1),...,σ(k)} ) -E H(X N,M ) .</formula><p>(4.7) We are going to simplify this expression by exploiting the symmetries of our construction.</p><p>Lemma 4.5. The random vector X N,M = (X N,M 1 , . . . , X N,M N ) ∈ X (d, N) is exchangeable, i.e. for all σ ∈ S N and any Φ :</p><formula xml:id="formula_61">Λ d,N → R E Φ(X N,M σ(1) , . . . , X N,M σ(N ) ) = E Φ(X N,M 1 , . . . , X N,M N ) .</formula><p>Proof. Note that every σ ∈ S N induces a permutation Σ</p><formula xml:id="formula_62">σ : Λ d,N → Λ d,N Σ σ (x 1 , . . . , x N ) = (x σ(1) , . . . , x σ(N ) ), x ∈ Λ d,N .</formula><p>In particular, (X</p><formula xml:id="formula_63">N,M σ(1) , . . . , X N,M σ(N ) ) = Σ σ (X N,M ) has, conditionally on (W i ) i∈Λ d,M , dis- tribution Σ * σ µ N,M (x) := d -M i∈Λ d,M ½ (x=Σσ(W i )) , x ∈ Λ d,N .</formula><p>However, (Σ σ (W i )) i∈Λ d,M has the same distribution as (W i ) i∈Λ d,M . Therefore we conclude.</p><p>Remark 4.6. Notice that Σ * σ µ N,M has same law as µ N,M , but in general the two measures are not a.s. equal. In other words, (X</p><formula xml:id="formula_64">N,M 1 , . . . , X N,M n ) is exchangeable but not exchangeable conditionally on (W i ) i∈Λ d,M .</formula><p>In particular, for all k ∈ {1, . . . , N} and σ</p><formula xml:id="formula_65">∈ S N E H(X N,M {σ(1),...,σ(k)} ) = E H(X N,M {1,...,k} ) ,<label>(4.8)</label></formula><p>and we obtain by (4.7) </p><formula xml:id="formula_66">E I c (X N,M ) = 2 N k=1 c N k N k E H(X N,M {1,...,k} ) -E H(X N,M ) . (<label>4</label></formula><formula xml:id="formula_67">Then d M • ν(y) is a binomial variable with parameters (d M , d -k ).</formula><p>Proof. Notice that, conditionally on</p><formula xml:id="formula_68">(W i ) i∈Λ d,M , X N,M {1,...,k} = (X N,M 1 , . . . , X N,M k ) ∈ Λ d,k has distribution P X N,M {1,...,k} = y (W i ) i∈Λ d,M = z∈Λ d,N-k µ N,M (y, z) = d -M i∈Λ d,M z∈Λ d,N-k ½ ((y,z)=W i ) ,</formula><p>where</p><formula xml:id="formula_69">y ∈ Λ d,k and (y, z) ∈ Λ d,k × Λ d,N -k = Λ d,N . For fixed y ∈ Λ d,k</formula><p>, the family</p><formula xml:id="formula_70">T i := z∈Λ d,N-k ½ ((y,z)=W i ) , i ∈ Λ d,M is an i.i.d. family of Bernoulli variables with parameter d -k . Indeed, if Π N →k : Λ d,N → Λ d,k is the natural projection, then the law of Π N →k (W i ) is uniform on Λ d,k , so that P(T i = 1) = P(Π N →k (W i ) = y) = d -k . Hence, for all y ∈ Λ d,k , d M • ν(y) = i∈Λ d,M</formula><p>T i is the sum of d M independent Bernoulli variables with parameter d -k , i.e. a binomial variable with parameters (d M , d -k ).</p><p>Let us denote from now on by B k a binomial variable with parameters (d</p><formula xml:id="formula_71">M , d -k ). Set ϕ(x) := - x log x log d , ∀x &gt; 0, ϕ(0) := 0.</formula><p>Notice that the function ψ(x)</p><formula xml:id="formula_72">:= -(1 + x) log(1 + x) + x + x 2 2 satisfies ψ(0) = ψ ′ (0) = 0, ψ ′′ (x) ≥ 0, ∀x ≥ 0, so that ψ(x) ≥ 0 for x ≥ 0. Moreover, ϕ(1 + x) ≥ 0 if x ∈ [-1, 0]. Hence, for all x ≥ -1, ϕ(1 + x) ≥ -½ (x&gt;0) x + x 2 /2 log d .<label>(4.11)</label></formula><p>Now, by (4.10) H X N,M {1,...,k} = -</p><formula xml:id="formula_73">y∈Λ k,N ν(y) log ν(y),</formula><p>then we obtain by Lemma 4.7 that</p><formula xml:id="formula_74">h k := 1 log d E H X N,M {1,...,k} = d k E ϕ B k d -M .<label>(4.12)</label></formula><p>Lemma 4.8. We have, for any 0 ≤ k ≤ M,</p><formula xml:id="formula_75">h k = k + E ϕ(B k d k-M ) = M + d k-M E (ϕ(B k )) .</formula><p>Proof. These identities follow from the formulae:</p><formula xml:id="formula_76">E(B k ) = d M -k , ϕ(d -j ) = jd -j and ϕ(αx) = αϕ(x) + xϕ(α) for α &gt; 0 applied to ϕ(B k d k-M • d -k ) and ϕ(B k • d -M ).</formula><p>Lemma 4.9. Away from k = M, the entropy is nearly constant:</p><formula xml:id="formula_77">k -2d k-M 2 ≤ h k ≤ k, k = 1, . . . , M, M -d M -k ≤ h k ≤ M, k = M + 1, . . . , N.</formula><p>Proof. The upper bounds are easy. Indeed, for k ≤ M one uses (A.3), while for k &gt; M we notice that the support of µ N,M has cardinality at most d M , and apply Let us consider now the regime k &gt; M. We have</p><formula xml:id="formula_78">(A.1) to conclude. Recall that B k is binomial with parameters (d M , d -k ). Then E(B k ) = d M -k and Var(B k ) = d M d -k (1 -d -k ). If we define J k := B k • d k-M -1 then we obtain E J 2 k = d 2(k-M ) Var(B k ) = d k-M -d -M ≤ d k-M . Hence, using (4.11) we get E ϕ(B k • d k-M ) = E (ϕ(1 + J k )) ≥ - 1 log d E ½ (J k &gt;0) J k + J 2 k 2 ≥ -E |J k | + J 2 k ≥ - E (J 2 k ) + E J 2 k ≥ -2d k-M 2 , since E(|J k |) ≤ E(J<label>2</label></formula><formula xml:id="formula_79">h k = d k E(ϕ(B k d -M )) = M + d k-M E (ϕ(B k )) . If B k ∈ {0, 1} then ϕ(B k ) = 0. Note that (4.11) implies that ϕ(B k ) ≥ -(B k - 1) 2 -(B k -1)</formula><p>as the right hand side is zero whenever B k = 0, 1 and less than</p><formula xml:id="formula_80">-(B k -1) 2 /2 -(B k -1) otherwise. Thus, d k-M E (ϕ(B k )) ≥ -d k-M E (B k -1) 2 + (B k -1) = -d k-M E B 2 k -B k = -d k-M d M -k + d 2(M -k) -d M -2k -d M -k ≥ -d M -k + d -k ≥ -d M -k .</formula><p>By Lemma 4.8 we obtain the lower bound for k &gt; M. </p><formula xml:id="formula_81">-2d -(N -M ) ≤ E (2 (D N ∧ M) -M) - E I c (X N,M ) log d ≤ E 4α -|D N -M | . (4.13)</formula><p>Proof. By (4.9), (4.12) and (3.2),</p><formula xml:id="formula_82">E I c (X N,M ) log d = 2 N k=1 c N k N k h k -h N = 2 E(h D N ) -h N .</formula><p>So,</p><formula xml:id="formula_83">E I c (X N,M ) log d -E(2(D N ∧ M) -M) = 2E(h D N -D N ∧ M) + M -h N</formula><p>We conclude by Lemma 4.9.</p><p>Lemma 4.11. Let x ∈ ]0, 1[ and M := ⌊xN⌋. Then for α := d 1/2 &gt; 1 and some constant C &gt; 0 we have</p><formula xml:id="formula_84">E α -|D N -M | ≤ C √ N , ∀ N ≥ 1. (4.14)</formula><p>Proof. To ease notation, in this proof we drop the subscript c from W c . By (2.2) and (3.2), we have that</p><formula xml:id="formula_85">P(D N = k) = N k c N k = N k E W k (1 -W ) N -k , k = 0, . . . , N.<label>(4.15)</label></formula><p>We claim that for all 1 ≤ k ≤ ⌊ N 2 ⌋ we have P(D N = k -1) ≤ P(D N = k).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Indeed,</head><formula xml:id="formula_86">P(D N = k) -P(D N = k -1) = = N! k! (N -k + 1)! E W k-1 (1 -W ) N -k [(N + 1 -k)W -k(1 -W )] . = N! k! (N -k + 1)! E W k-1 (1 -W ) N -k [(N + 1)W -k] .</formula><p>By the symmetry of λ c w.r.t. 1/2 we have, since 2k ≤ N + 1,</p><formula xml:id="formula_87">E W k-1 (1 -W ) N -k [(N + 1)W -k] = 2 -N +1 N + 1 2 -k P(W = 1/2)+ + E W k-1 (1 -W ) N -k [(N + 1)W -k] ½ (W &gt;1/2) + + E W N -k (1 -W ) k-1 [(N + 1)(1 -W ) -k] ½ (W &gt;1/2) ≥ E (W (1 -W )) k-1 ½ (W &gt;1/2) • • ((N + 1)W -k) W N -2k+1 + ((N + 1)(1 -W ) -k) (1 -W ) N -2k+1 .</formula><p>This expectation is nonnegative. Indeed, the first term is nonnegative as 2k ≤ N + 1 and W &gt; 1/2. We assume the second term to be negative, otherwise we are done.</p><p>As 1 -W ≤ W , we get:</p><formula xml:id="formula_88">P(D N = k) -P(D N = k -1) = = E ½ W &gt;1/2 ((N + 1)W -k + (N + 1)(1 -W ) -k)(1 -W ) N -2k+1 ≥ E ½ W &gt;1/2 ((N + 1) -2k)W N -2k+1 ≥ 0, proving the claim. We set L := ⌊ N 2 ⌋. Then N -L ≥ L ≥ N 2 -</formula><p>1 and by (4.15) we obtain for all 0</p><formula xml:id="formula_89">≤ k ≤ N N k c N k ≤ N L c N L = N! L!(N -L)! E W L (1 -W ) N -L ≤ N! L!(N -L)! E W L (1 -W ) L ≤ N! L!(N -L)! 4 -L ≤ N! L!(N -L)! 2 -N +2 . By Stirling's formula n! = √ 2πn(n/e) n (1 + O(1/n)), there is a constant C ≥ 0 such that 2 -N N! L!(N -L)! ≤ C N -1 2 , N → +∞, L = N 2 .</formula><p>Then, we obtain for some constants</p><formula xml:id="formula_90">C 1 , C 2 ≥ 0 E α -|D N -M | = N k=0 N k c N k α -|k-M | ≤ C 1 N -1 2 N k=0 α -|k-M | ≤ C 1 N -1 2 2 +∞ k=0 α -k = C 2 N -1 2 ,</formula><p>and the proof is finished.</p><p>Proof of Proposition 4.3. Let x ∈ ]0, 1[ and M := ⌊xN⌋ ≥ 1 (N is large). By Lemma 4.9 for k = N</p><formula xml:id="formula_91">E M N - H(X N,M ) N log d = E M N - H(X N,M ) N log d = E M -h N N ≤ d M -N N ≤ d -N (1-x) .</formula><p>Thus,</p><formula xml:id="formula_92">N ≥1 E M N - H(X N,M ) N log d &lt; +∞,<label>(4.16</label></formula><p>) therefore a.s.</p><formula xml:id="formula_93">N ≥1 M N - H(X N,M ) N log d &lt; +∞,</formula><p>and in particular a.s.</p><p>lim</p><formula xml:id="formula_94">N →+∞ M N - H(X N,M ) N log d = 0.</formula><p>(2) Let x ∈ [0, 1] and let (X N ) N be an approximate x-maximizer for I c . Then:</p><formula xml:id="formula_95">lim N →∞ sup supp(λc) |h X N -h * x | = 0. (5.2)</formula><p>In particular, if x ∈ supp(λ c ) then</p><formula xml:id="formula_96">lim N →∞ sup [0,1] |h X N -h * x | = 0. (<label>5.3)</label></formula><p>(3) If x ∈ supp(λ c ) then an approximate x-maximizer (X N ) N for I c is an approximate x-maximizer for any other intricacy I c ′ .</p><p>Remark 5.3. The extra assumption about the support of λ c cannot be dropped. Indeed, for point (1) of Theorem, observe that, in the case of the p-symmetric intricacy, 1/2 is not in the support of 1 2 (δ p + δ 1-p ) for p = 1/2 and approximate maximizers of I p satisfy only</p><formula xml:id="formula_97">p ≤ lim inf N H(X N ) N log d ≤ lim sup N H(X N ) N log d ≤ 1 -p.</formula><p>The entropy may accumulate on any point on the interval [p, 1p].</p><p>Notice however that for many intricacies, including the neural complexiy, the support of λ c is the whole interval, making this assumption satisfied for all x ∈ [0, 1]. Remark 5.5. Let X N,M the random system constructed in (4.2) and (4.3), with M := ⌊xN⌋, x ∈ ]0, 1[. For the Edelman-Sporns-Tononi intricacy I we have that the support of the associated probability measure λ is [0, 1], since it is the Lebesgue measure by Lemma 3.8 of <ref type="bibr" target="#b1">[2]</ref>. Since (X N,M ) N is a.s. an approximate x-maximizer for I by Proposition 4.3, then by point (3) of Theorem 5.2 a.s. this sequence is an approximate x-maximizer simultaneously for all intricacies I c . This has the following consequence for approximate maximizers (i.e., without entropy constraints). An approximate maximizer for some intricacy I c where 1/2 / ∈ supp(λ c ) is not necessarily an approximate maximizer for another intricacy. But an approximate 1/2-maximizer for any intricacy is automatically an approximate maximizer for all intricacies.</p><p>Proof of Theorem 5.2. Let us set for simplicity of notation:</p><formula xml:id="formula_98">x N := H(X N ) N log d , I N := I c (X N ) N log d .<label>(5.5)</label></formula><p>If 1/2 is in the support of λ c , then it is the unique point where i c (x) achieves its maximum. Then, Theorem 5.1 implies that no x = 1/2 can be an accumulation point of x N , N ≥ 1. Thus an approximate maximizer is an approximate 1/2-maximizer.</p><p>It is now enough to prove point <ref type="bibr" target="#b1">(2)</ref>. By definition of approximate x-maximizers, x N → x and I N → i c (x). Using point (2) of Proposition 3.3, it follows that |I Ni c N (x)| → 0 and by (3.8) we have h X Nh * x c,N → 0. Notice now that for any K-Lipschitz function f : [0, 1] → R, by (3.10)</p><formula xml:id="formula_99">|E(f (β N )) -E(f (W c ))| ≤ K √ N As entropy profiles are 1-Lipschitz, we obtain |h X N -h * x | dλ c → 0.</formula><p>As all functions (h X Nh * x ) N are 2-Lipschitz, (5.2) follows by a routine argument.</p><formula xml:id="formula_100">Assuming now x ∈ supp(λ c ), lim N →∞ h X N (x) = h * x (x) = x. On the one hand, as h X N (0) = 0 and h X N is 1-Lipschitz, it follows that the convergence lim N →∞ h X N (t) = h * x (t) = t occurs for all t ∈ [0, x]. On the other hand, all h X N being non-decreasing, h X N (x) = x ≤ h X N (t) ≤ h X N (1) → x.</formula><p>Hence the previous convergence occurs for all x ∈ [0, 1], proving (5.3). Finally, let us prove point <ref type="bibr" target="#b2">(3)</ref>. By (5.3) the profiles h X N converge to h * x uniformly on [0, 1]. Let I c ′ be any other intricacy. By uniform convergence we have h</p><formula xml:id="formula_101">X N - h * x c ′ ,N ≤ sup [0,1] |h X N -h * x | → 0. By (3.8) and Lemma 3.4 I c ′ (X) N log d = i c ′ N (x) -h X -h * x c ′ ,N → i c ′ (x), N → +∞.</formula><p>By Theorem 5.1, i c ′ (x) = I c ′ (d, x) and therefore (X N ) N is an approximate xmaximizer for I c ′ .</p><p>We have the following consequence for approximate x-maximizers. We recall that H(Y | Z) denotes the conditional entropy, see the Appendix. Theorem 5.6. Suppose that x ∈ supp(λ c ) and let (X N ) N be an approximate xmaximizer for some 0</p><formula xml:id="formula_102">≤ x ≤ 1. Then (1) If y ∈ ]0, x] then for all ε &gt; 0 lim N →+∞ 1 N ⌊yN ⌋ # S ⊂ {1, . . . , N} : |S| = ⌊yN⌋, (1 -ε)|S| log d &lt; H(X N S ) ≤ |S| log d = 1.</formula><p>(</p><formula xml:id="formula_103">) If y ∈ [x, 1[ then for all ε &gt; 0 lim N →+∞ 1 N ⌊yN ⌋ #{S ⊂ {1, . . . , N} : |S| = ⌊yN⌋, H(X N | X N S ) &lt; εxN log d} = 1.<label>2</label></formula><p>This result can be loosely interpreted as follows: as N → +∞, (1) if y ∈ ]0, x] then for almost all subsets S with |S| = ⌊yN⌋, X S is almost uniform;</p><p>(2) if y ∈ [x, 1[ then for almost all subsets S with |S| = ⌊yN⌋, X is almost a function of X S . This follows from the relation between entropy and conditional entropy on one side and independence versus dependence on the other side, see the Appendix.</p><p>Proof of Theorem 5.6. Let y ∈ ]0, 1[. By <ref type="bibr">(5.3)</ref>, h X N (y) → h * x(y) = x ∧ y as N → +∞. By the definition 1.4 of h X N , we obtain, setting k N := ⌊yN⌋,</p><formula xml:id="formula_104">1 N k N |S|=k N h * x (y) - H(X S ) N log d = h * x (y) - 1 N k N |S|=k N H(X S ) N log d = h * x (y) -h X N (y) → 0,</formula><p>since all terms in the sum are non-negative by Lemma 3.6. Let Z N , defined on (Ω, F , P), be a random subset of {1, . . . , N} defined by</p><formula xml:id="formula_105">P(Z N = S) = 1 N k N , if |S| = k N .</formula><p>Then the above formula can be rewritten as follows lim</p><formula xml:id="formula_106">N →+∞ E h * x (y) - H(X Z N ) N log d = 0.</formula><p>Since L 1 convergence implies convergence in probability, we obtain This readily implies the Theorem, by recalling that H(X N | X N S ) = H(X N ) -H(X N S ) and that H(X N )</p><p>N log d → x by assumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Entropy</head><p>In this Appendix, we recall needed facts from basic information theory. The main object is the entropy functional which may be said to quantify the randomness of a random variable. We refer to <ref type="bibr" target="#b2">[3]</ref> for more background.</p><p>Let X be a random variable taking values in a finite space E. We define the entropy of X H(X) := -x∈E P X (x) log(P X (x)), P X (x) := P(X = x),</p><p>where we adopt the convention 0 • log(0) = 0 • log(+∞) = 0.</p><p>We recall that 0 ≤ H(X) ≤ log |E|, (A.1)</p><p>More precisely, H(X) is minimal iff X is a constant, it is maximal iff X is uniform over E. To prove (A.1), just notice that since ϕ ≥ 0 and ϕ(x) = 0 if and only if If we have a E-valued random variable X and a F -valued random variable Y defined on the same probability space, with E and F finite, we can consider the vector (X, Y ) as a E × F -valued random variable The entropy of (X, Y ) is then H(X, Y ) :=x,y P (X,Y ) (x, y) log(P (X,Y ) (x, y)), P (X,Y ) (x, y) := P(X = x, Y = y).</p><p>This entropy H(X, Y ) does not only depends on the (separate) laws of X and Y but on the extent to which the "randomness of the two variables is shared". The following notions formalize this idea. We first claim that it is nonnegative.</p><p>Remark that P X (x) and P Y (y), defined in the obvious way, are the marginal laws of P (X,Y ) (x, y), i.e. P X (x) = y P (X,Y ) (x, y), P Y (y) =</p><p>x P (X,Y ) (x, y).</p><p>In particular, P X (x) ≥ P (X,Y ) (x, y) for all x, y. Therefore </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Definition 2 . 1 .</head><label>21</label><figDesc>A system of coefficients is a collection of numbers c := (c I S : I ⊂⊂ N * , S ⊂ I), i.e., I ranges over the finite subsets of N * , for all I and all S ⊂ I:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>3. 4 .</head><label>4</label><figDesc>Intricacy as a function of the profile. Let us setΓ := {h : [0, 1] → [0, 1] : h(0) = 0, t → h(t)is non-decreasing and 1-Lipschitz} and for any real number x ∈ [0, 1] Γ x := {h ∈ Γ : h(1) = x} .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Lemma 3 . 8 .</head><label>38</label><figDesc>For each d ≥ 2, there exists H * = H * (d) &lt; ∞ with the following property. If N ≥ 1 and Y 1 , . . . , Y N are random variables taking values in {0, . . . , d -1} and defined on the same probability space such that, for some real number H ∈</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>4. 1 .</head><label>1</label><figDesc>Sparse random configurations. Let N ≥ 2 and 0 ≤ M ≤ N be integers. We denote Λ d,n := {0, . . . , d -1} n , ∀ n ≥ 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Proposition 4 . 3 .</head><label>43</label><figDesc>For integers N ≥ 1, 0 ≤ M ≤ N, let X M,N be the random systems defined above. Let x ∈ [0, 1]. For any intricacy I c we have, a.s. and in L 1 limN →+∞ I c (µ N,⌊xN ⌋ ) N log d = i c (x) (4.4)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>. 9 ) 4 . 7 .</head><label>947</label><figDesc>Lemma Let y ∈ Λ d,k , k ∈ {1, . . . , N} and set ν(y) := z∈Λ d,N-k µ N,M (y, z).(4.10)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>M 2 ≤</head><label>2</label><figDesc>k ) by Cauchy-Schwartz and d kd k-M ≤ 1. By Lemma 4.8 we obtain the desired lower bound for k ≤ M.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>4. 3 .</head><label>3</label><figDesc>Estimation of the expected Intricacy. Lemma 4.10. Let x ∈ ]0, 1[, M := ⌊xN⌋ and α := d 1/2 &gt; 1. For all N ≥ 2, using the notation (3.1),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Remark 5 . 4 .</head><label>54</label><figDesc>In the setting of point (2) of Theorem 5.2, if x does not belong to the support of λ c , then one can prove with similar arguments that limN →∞ sup [0,a]∪[b,1] |h X Nh * x | = 0 (5.4) where a := sup([0, x] ∩ supp(λ c )), b := inf([x, 1] ∩ supp(λ c )), with the convention sup ∅ := 0 and inf ∅ := 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>x</head><figDesc>∈ {0, 1}, and by strict convexity of x → ϕ(x) = x log x and Jensen's inequalitylog |E| -H(X) = 1 |E| x∈E P X (x) |E| (log(P X (x)) + log |E|) = 1 |E| x∈E ϕ (P X (x) |E|) ≥ ϕ 1 |E| x∈E P X (x) |E| = ϕ(1) = 0, with log |E| -H(X) = 0 if and only if P X (x) |E| is constant in x ∈ E.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>A. 1 .</head><label>1</label><figDesc>Conditional Entropy. The conditional entropy of X given Y is:H(X | Y ) := H(X, Y ) -H(Y ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>PP</head><figDesc>(X,Y ) (x, y) log P (X,Y ) (x, y) P X (x) (X,Y ) (x, y) log P (X,Y ) (x, y) ≥ -x P X (x) log P X (x) = H(X), i.e., H(X|Y ) ≥ 0, proving the claim. ThereforeH(X, Y ) ≥ max{H(X), H(Y )}. (A.2) Moreover H(X, Y ) = H(X), i.e. H(Y |X) = 0, if and only if P (X,Y ) (x, y) = P X (x) whenever P (X,Y ) (x, y) = 0, which means that Y is a function of X. On the other hand, H(X, Y ) ≤ H(X) + H(Y ) (A.3) with equality, i.e., H(Y |X) = H(Y ), if and only if X and Y are independent. This can be shown by considering the Kullback-Leibler divergence or relative entropy:I := x,y P (X,Y ) (x, y) log P (X,Y ) (x, y) P X (x) P Y (y) .Since log(•) is concave, by Jensen's inequality-I ≤ log x,y P (X,Y ) (x, y) P X (x) P Y (y) P (X,Y ) (x, y) = logx,yP X (x) P Y (y) = 0.By strict concavity, I = 0 if and only if P (X,Y ) (x, y) = P X (x) P Y (y) for all x, y, i.e., whenever X and Y are independent. By the above considerations, H(X | Y ) ∈ [0, H(X)] is a measure of the uncertainty associated with X if Y is known. It is minimal iff X is a function of Y and it maximal iff X and Y are independent.A.2. Mutual Information. Finally, we recall the notion of mutual information between two random variables X and Y defined on the same probability space:MI(X, Y ) := H(X) + H(Y ) -H(X, Y ) = H(X) -H(X | Y ) = H(Y ) -H(Y | X) = x,y P (X,Y ) (x, y) log P (X,Y ) (x, y) P X (x) P Y (y) .This quantity is a measure of the common randomness of X and Y . By (A.2) and (A.3) we have MI(X, Y ) ∈ [0, min{H(X), H(Y )}]. MI(X, Y ) is minimal (zero) iff X, Y are independent and maximal, i.e. equal to min{H(X), H(Y )}, iff one variable is a function of the other.</figDesc></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Setting x N := H(X N,M ) N log d , we have obtained lim N →+∞</p><p>x N = lim</p><p>a.s. and in L 1 , namely we have proven <ref type="bibr">(4.5)</ref>. Now, let us set observe that, by Proposition 3.3, this gives</p><p>→ 0 again a.s. and in L 1 . On the other hand, by <ref type="bibr">(3.5)</ref> and by Lemmas 4.10, 4.11</p><p>Arguing as above, it follows that I c (X N,M ) N log d → i c (x) a.s. and in L 1 . This proves (4.4) and concludes the proof of Proposition 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results for Arbitrary Intricacies</head><p>We now collect our results to state the generalizations of Theorems 1.1 and 1.5 for arbitrary intricacies. We consider some non-null intricacy I c . Let λ c be the associated probability measure on [0, 1] according to Proposition 2.2. Recall from Def. 4.1 that the corresponding entropy-intricacy function I c (d, x) is:</p><p>We also recall that i c (x) and i c N (x) have been defined in eq. (3.5) and (3.6). Theorem 5.1.</p><p>(1) For any N ≥ 1, X ∈ X (d, N),</p><p>(</p><p>for all x ∈ [0, 1]. Theorem 5.1 immediately follows from Propositions 3.3 and 4.3. We now consider the convergence of entropy profiles to the ideal profiles for approximate maximizers, i.e., the generalization of Theorem 1.5: Theorem 5.2. Let I c be an intricacy.</p><p>(1) If 1/2 is in the support of λ c , then any approximate maximizer (X N ) N for I c satisfies: </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural complexity and structural connectivity</title>
		<author>
			<persName><forename type="first">L</forename><surname>Barnett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bullock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. E</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page">51914</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A probabilistic study of neural complexity</title>
		<author>
			<persName><forename type="first">J</forename><surname>Buzzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zambotti</surname></persName>
		</author>
		<ptr target="http://fr.arxiv.org/abs/0908.1006" />
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thomas</surname></persName>
		</author>
		<title level="m">Elements of Information Theory</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Degeneracy and complexity in biological systems</title>
		<author>
			<persName><forename type="first">G</forename><surname>Edelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl. Acad. Sci. USA</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="13763" to="13768" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Greven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Warnecke</surname></persName>
		</author>
		<author>
			<persName><surname>Entropy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Princeton University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Analytical description of the evolution of neural networks: learning rules and complexity</title>
		<author>
			<persName><forename type="first">K</forename><surname>Holthausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Breidbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biol. Cybern</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="169" to="176" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Characterizing functional hippocampal pathways in a brain-based device as it solves a spatial memory task</title>
		<author>
			<persName><forename type="first">J</forename><surname>Krichmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Edelman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl. Acad. Sci. USA</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="2111" to="2116" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A new class of non-Shannon-type inequalities for entropies</title>
		<author>
			<persName><forename type="first">K</forename><surname>Makarychev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Makarychev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romashchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vereshchagin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in Information and Systems</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="147" to="166" />
			<date type="published" when="2002">2002</date>
			<publisher>International Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Theories and measures of consciousness: an extended framework</title>
		<author>
			<persName><forename type="first">A</forename><surname>Seth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Izhikevich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Reeke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Edelman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl. Acad. Sci. USA</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page" from="10799" to="10804" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Seth</forename><surname>Anil</surname></persName>
		</author>
		<title level="m">Models of consciousness</title>
		<imprint>
			<publisher>Scholarpedia</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">1328</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dynamical complexity in small-world networks of spiking neurons</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Shanahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. E</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page">41924</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Connectivity and complexity: the relationship between neuroanatomy and brain dynamics</title>
		<author>
			<persName><forename type="first">O</forename><surname>Sporns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tononi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Edelman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">8-9</biblScope>
			<biblScope unit="page" from="909" to="922" />
			<date type="published" when="2000-10">2000 Oct-Nov</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Networks analysis, complexity, and brain function</title>
		<author>
			<persName><forename type="first">O</forename><surname>Sporns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Complexity</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="56" to="60" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">O</forename><surname>Sporns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Complexity</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">1623</biblScope>
			<date type="published" when="2007">2007</date>
			<publisher>Scholarpedia</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Talagrand</surname></persName>
		</author>
		<title level="m">Spin Glasses: A Challenge for Mathematicians</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A measure for brain complexity: relating functional segregation and integration in the nervous system</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tononi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sporns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Edelman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl. Acad. Sci. USA</title>
		<meeting>Natl. Acad. Sci. USA</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="page" from="5033" to="5037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A complexity measure for selective matching of signals by the brain</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tononi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sporns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Edelman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl. Acad. Sci. USA</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="3422" to="3427" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Measures of degeneracy and redundancy in biological networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tononi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sporns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Edelman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl. Acad. Sci. USA</title>
		<meeting>Natl. Acad. Sci. USA</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="3257" to="3262" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
