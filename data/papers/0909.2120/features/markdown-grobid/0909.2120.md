# APPROXIMATE MAXIMIZERS OF INTRICACY FUNCTIONALS

## Abstract

## 

G. Edelman, O. Sporns, and G. Tononi introduced in theoretical biology the neural complexity of a family of random variables. This functional is a special case of intricacy, i.e., an average of the mutual information of subsystems whose weights have good mathematical properties. Moreover, its maximum value grows at a definite speed with the size of the system.

In this work, we compute exactly this speed of growth by building "approximate maximizers" subject to an entropy condition. These approximate maximizers work simultaneously for all intricacies. We also establish some properties of arbitrary approximate maximizers, in particular the existence of a threshold in the size of subsystems of approximate maximizers: most smaller subsystems are almost equidistributed, most larger subsystems determine the full system.

The main ideas are a random construction of almost maximizers with a high statistical symmetry and the consideration of entropy profiles, i.e., the average entropies of sub-systems of a given size. The latter gives rise to interesting questions of probability and information theory.

## 

1. Introduction 1.1. Neural Complexity, a measure of complexity from theoretical biology. In [[16]](#b15), G. Edelman, O. Sporns and G. Tononi introduced the so called neural complexity of a family of random variables. It is defined as an average of mutual information between any subfamily and its complement, see below. It has been considered from a theoretical and experimental point of view by a number of authors, see e.g. [[1,](#b0)[4,](#b3)[5,](#b4)[6,](#b5)[7,](#b6)[9,](#b8)[10,](#b9)[11,](#b10)[12,](#b11)[13,](#b12)[14,](#b13)[16,](#b15)[17,](#b16)[18]](#b17).

In order to define the neural complexity, we need to recall two classical definitions. If X is a random variable taking values in a finite space E, then its entropy is defined by H(X) := -x∈E P X (x) log(P X (x)), P X (x) := P(X = x).

Given two random variables defined over the same probability space, the mutual information between X and Y is MI(X, Y ) := H(X) + H(Y ) -H(X, Y ).

We refer to the Appendix for a review of the main properties of the entropy and the mutual information. Edelman, Sporns and Tononi consider systems formed by a finite family X = (X i ) i∈I and define the following concept of complexity. For any S ⊂ I, they divide the system into two subsystems:

$X S := (X i , i ∈ S), X S c := (X i , i ∈ S c ),$where S c := I\S. Then they compute the mutual information MI(X S , X S c ) and consider the sum

$I(X) := 1 |I| + 1 S⊂I 1 |I| |S| MI(X S , X S c ),(1.1)$where |I| denotes the cardinality of I. Note that I(X) is really a function of the law of X.

As shown in [[2]](#b1), one can define more general functionals

$I c (X) := S⊂I c I S MI(X S , X S c ),$which have similar properties, provided the properties of "exchangeability" and "weak additivity" still hold, see Sec. 2. The resulting functionals have been called intricacies in [[2]](#b1). Using a super-additivity argument, we showed in [[2]](#b1) that the maximum value of any intricacy over systems with a given size grows linearly with the size. In this paper, we compute exactly this speed of growth by building "approximate maximizers", i.e., families of an increasing number of random variables taking value in a fixed set and achieving, in the limit, the maximum intricacy per variable. Moreover, we shall construct in this paper a sequence of simultaneous approximate maximizers for all intricacies.

Our construction is probabilistic in a fundamental way. We shall show that maximizers should approximately satisfy strong symmetries (see Theorem 1.6), that cannot be satisfied exactly (Lemma 3.8). We shall exhibit a random sequence of systems, which satisfy such symmetries in law, and approximately satisfy the same symmetries almost surely.

If the family (X i ) i∈I is completely deterministic or, on the contrary, independent, then every mutual information vanishes and therefore I (X) = 0. As these examples suggest, large values of I require compromising between randomness and mutual dependence, i.e., to have non-trivial correlation between X S and X S c for many subsets S. This explains why maximizing this functional is not a trivial problem. 1.2. Main Results. For the sake of simplicity, we state our results in this introduction only for the neural complexity (1.1), deferring the analogous results for arbitrary intricacies to Section 5.

First, we need some notations. The integers N ≥ 1 and d ≥ 2 will denote respectively the cardinality of the family (X i ) i∈I and of the range of each X i . Moreover,

• Λ N,d := {0, . . . , d -1} N is the set of configurations, i.e., of possible values for the random vector X; • X (d, N) is the set of all Λ N,d -valued random variables X, which we shall identify with M(d, N), set of all probability measures on Λ N,d .

In particular, we write indifferently H(X) and H(µ), as well as I(X) and I(µ). Of course, entropy and intricacy are in fact functions of the law µ of X and not of the (random) values of X.

Let us state our main results in the case of the neural complexity:

Theorem 1.1. Let I (X) be the neural complexity (1.1) of Edelman-Sporns-Tononi.

(1) We have for all µ ∈ M(d, N), setting

$x µ := H(µ) N log d , I(µ) N log d ≤ x µ (1 -x µ ) ≤ 1 4 .(1.2)$(2) The maximum value of the intricacy at fixed size

$I(d, N) := max X∈X (d,N ) I(X) = max µ∈M(d,N ) I(µ) satisfies: lim N →∞ I(d, N) N = log d 4 .$(3) For any x ∈ [0, 1], there exists a sequence µ N ∈ M(d, N) approaching the upper bound of point (1), i.e., satisfying:

$lim N →+∞ H(µ N ) N log d = x, lim N →+∞ I(µ N ) N log d = x(1 -x).(1.3)$Remark 1.2. We shall actually prove this theorem for arbitrary intricacies (see Theorem 5.1). More precisely, and perhaps unexpectedly, we shall build, for each d ≥ 2, a sequence µ N ∈ M(d, N) satisfying, simultaneously for all intricacies I c , lim

$N →∞ I c (µ N ) N = lim N →∞ max µ∈M(d,N ) I c (µ) N ,$see Remark 5.5 below.

Remark 1.3. All of the above is new, though numerical experiments by previous authors [16, Fig. [1](#fig_3)] had suggested the concavity and the symmetry of the maximal intricacy given the entropy, but not its quadratic form.

While the upper bound (1.2) follows from direct computations, the existence of sequences (µ N ) N satistying (1.3) is much less trivial and is the main result of this paper. As shown in Theorem 1.6 below, such sequences must exhibit a nontrivial behavior, combining a large amount of local independence and of non-trivial correlation on a global level.

The existence of approximate x-maximizers, i.e., sequences µ N ∈ M(d, N) satisfying (1.3), follows in our approach from a probabilistic construction: we shall prove that uniform distributions on appropriately chosen random sparse supports will have almost surely the desired properties: see Proposition 4.3 below.

In the course of the proof, we also obtain rather detailed information on the structure of approximate x-maximizers. A key notion is the following one. Definition 1.4. Given X ∈ X (d, N), its entropy profile is the function h

$X : [0, 1] → [0, 1] such that h X (0) = 0, h X k N = 1 N k S⊂I, |S|=k H(X S ) N log d , k ∈ I := {1, . . . , N} and h X is affine on each interval k-1 N , k N , k ∈ I. Theorem 1.5. For x ∈ [0, 1], let the ideal profile be h * x (t) = x ∧ t = min{x, t}.$Then for any sequence µ N ∈ M(d, N) of approximate x-maximizers, we have

$h µ N -h * x sup := sup t∈[0,1] |h µ N (t) -h * x (t)| → 0 as N → ∞.$In particular, for any sequence µ N ∈ M(d, N) of approximate maximizers, i.e., such that lim N →∞ I(µ N )/N = log d/4, we have:

$lim N →+∞ H(µ N ) N log d = 1/2, lim N →+∞ h µ N -h * 1/2 sup = 0.$Again, we prove in fact a version of this result for all intricacies, see Theorem 5.2 below.

If (µ N ) N is a sequence of approximate x-maximizers and X N ∈ X (d, N) has law µ N , we say that (X N ) N is also a sequence of approximate x-maximizers.

A corollary of the convergence of entropy profiles is the existence of a threshold in the behavior of typical subsystems of approximate x-maximizers: if |S| ≤ xN, then X N S is almost uniform, which corresponds to local independence; if |S| ≥ xN, then the whole family X N is almost a function of X N S , which corresponds to strong global correlation. Recall that H(Y | Z) is the conditional entropy of Y given Z, see the Appendix below.

Theorem 1.6. Let (X N ) N be an approximate x-maximizer. Let y ∈]0, 1[ and set

$k N := ⌊yN⌋. Consider the N k N sub-systems X S of X N of size k N = |S|.$For all ǫ > 0, if N is large enough, then except for at most ǫ N k N of such subsets S, the following holds:

$• if y ≤ x: X S is almost uniform: 1 -ǫ ≤ H(X S ) |S| log d ≤ 1;$• if y ≥ x: X S almost determines the whole system

$X N : 0 ≤ H(X N |X S ) N log d ≤ ǫ.$Again, we prove a more general version of this result in Theorem 5.6 below.

1.3. Strategy of Proof and Organization of the paper. The main ideas of the proofs of the two theorems are a probabilistic construction of the sequence maximizers and the consideration of the entropy profiles h X defined above. As we indicated, in fact we analyze arbitrary intricacies generalizing neural complexity.

In section 2 we recall the notion of intricacy as a family of functionals over finite sets of discrete random variables satisfying exchangeability and weak additivity and give simple examples. In section 3 we give upper bounds on the intricacies of arbitrary systems of given size and entropy. In section 4 we prove the main results by means of a probabilistic construction of random approximate maximizers. In section 5 we collect our results for arbitrary intricacies. An Appendix contains basic facts from entropy theory for the convenience of the reader.

1.4. Further questions. The bound x(1x) in (1.3) is symmetric with respect to x = 1/2 and independent of d ≥ 2. We do not know whether these simple properties, which extend to arbitrary intricacies (see Theorem 5.1), can be proved directly, e.g.: does there exist a duality operation in X (N, d) exchanging systems with entropy xN log d and (1x)N log d while preserving their intricacy? Can one deduce from a system in X (d, N) with entropy H and intricacy I a system in X (d ′ , N) with entropy (log d ′ / log d)H and intricacy (log d ′ / log d)I?

This work has focused on properties of systems with size tending to infinity. Notice that we know very little on the exact maximizers for fixed size beyond the constraints on their entropy contained in our main results. Because of the invariance properties of intricacy (see Lemma 2.8 and the following comment), exact maximizers are nonunique but we do not even know if there are only finitely many of them.

Our construction of approximate maximizers is probabilistic. Could it be done deterministically? Would the corresponding algorithms possess a computational complexity related to the complexity that intricacies are supposed to describe? Our construction is global but could systems with maximum intricacy be built by a local approach, i.e., a "biologically reasonable" building process, using some type of local rules and/or evolution? That is, does there exist a "reasonable" self-map T : M(d, N) → M(d, N) such that the neural complexity of T n (µ) converges to the maximum as n → ∞ for "many" µ ∈ M(d, M).

Our work also leads to interesting probabilistic constructions and questions in the theory of entropy and information. For instance: Problem. Describe the set of functions h : {0, . . . , N} → R obtained from picking X ∈ X (d, N) and setting h(k) to be the average entropy of X S where S ranges over the subsets of {1, . . . , N} with cardinality k.

Basic properties of the entropy (recalled in the Appendix) imply that h(0

$) = 0, 0 ≤ h(k + 1) -h(k) ≤ log d for 0 ≤ k < N and h(k + ℓ) -h(k) ≤ h(j + ℓ) -h(j) for 0 ≤ j ≤ k ≤ k + ℓ ≤ N.$However we shall show that not all such functions h arise from some X ∈ X (d, N), see Lemma 3.8. See [[8]](#b7) for a closely related question.

## Intricacy

2.1. Definition. In this paper, a system is a finite collection (X i ) i∈I of random variables, each X i taking value in the same finite set V . Without loss of generality, we assume that V = {0, . . . , d -1} for all i ∈ I and some d ≥ 2 (d should be thought of as a convenient normalization) and I is a set of positive integers. We let M = d≥2 M(d) = d≥2,N ≥1 M(d, N) be the set of the corresponding laws, that is, the probability measures on {0, . . . , d-1} I for each finite subset I ⊂ N * := {1, 2, 3, . . . }.

For S ⊂ I, we denote X S := (X i , i ∈ S). In [[2]](#b1), we defined the following family of functionals over such systems (more precisely: over their laws) formalizing (and slightly generalizing) the neural complexity of Edelman-Sporns-Tononi [[16]](#b15): where S c := I \ S. The corresponding mutual information functional is I c : M → R defined by:

$I c (X) := S⊂I c I S MI (X S , X S c ) .$By convention, MI (X ∅ , X I ) = MI (X I , X ∅ ) = 0. An intricacy, is a mutual information function satisfying:

(1) exchangeability (invariance by permutations): if I, J ⊂⊂ N * and φ :

$I → J is a bijection, then I c (X) = I c (Y ) for any X := (X i ) i∈I , Y := (X φ -1 (j) ) j∈J ;$(2) weak additivity: for any two independent sub-systems (X i ) i∈I , (Y j ) j∈J (defined on the same probability space):

$I c (X, Y ) = I c (X) + I c (Y ). I c is non-null if some coefficient c I S with S / ∈ {∅, I} is not zero.$
## Classification of intricacies. In section 3 of [2] the following has been proved

Proposition 2.2. A mutual information functional I c determines its coefficients uniquely and the following equivalences hold:

• I c is exchangeable if and only if c I S depends only on |I| and |S|; • an exchangeable I c is weakly additive if and only if there exists a random variable W c → [0, 1] such that W c and 1 -W c have the same law and

$c N k = E (1 -W c ) N -k W k c = [0,1] x k (1 -x) N -k λ c (dx),(2.2)$where λ c is the law of W c . • an exchangeable weakly additive I c is non-null iff λ c (]0, 1[) > 0, in which case all coefficients c I S are non-zero.

In this paper we consider only non-null intricacies I c .

Example 2.3. The intricacy I of Edelman-Sporns-Tononi is defined by the coefficients:

$c I S = 1 |I| + 1 1 |I| |S| (2.3)$and it is easy to see that in this case (2.2) holds with W c a uniform variable over [0, 1], see Lemma 3.8 in [[2]](#b1). For 0 < p < 1, the symmetric p-intricacy I p is defined by

$c I S = 1 2 p |S| (1 -p) |I\S| + (1 -p) |S| p |I\S|$and in this case W c is uniform on {p, 1 -p}. For p = 1/2, this yields the uniform intricacy I U (X) with:

$c I S = 2 -|I|$, and W c = 1/2 almost surely. All these functionals are clearly non-null and exchangeable.

Remark 2.4. The global 1/(|I| + 1) factor in (2.3) is not present in [[16]](#b15), which did not compare systems of different sizes. However it is necessary in order to have weak additivity.

## Simple examples.

Let X i take values in {0, . . . , d -1} for all i ∈ I, a finite subset of N * .

Example 2.5. If the variables X i are independent then each mutual information is zero and therefore:

$I c (X) = 0. Example 2.6. If each X i is a.s. equal to a constant c i in {0, . . . , d -1}, then, for any S = ∅, H(X S ) = 0. Hence, I c (X) = 0. Example 2.7. If X 1 is uniform on {0, . . . , d -1} and X i = X 1 for all i ∈ I, then, for any S = ∅, H(X S ) = log d and, if additionally S c = ∅, H(X S X S c ) = 0 so that each mutual information MI(X S ; X S c ) is log d. Hence, I c (X) = S⊂I\{∅,I} c I S • log d = 1 -c I ∅ -c I I log d ≤ log d.$Examples 2.5 and 2.6 correspond to, respectively, maximal and minimal total entropy. In these extreme cases I c = 0. Example 2.7 has positive total entropy and intricacy (if all c I S are non zero). However, the values of the intricacy grow very slowly with |I| in these examples: they stay bounded. We shall see however how to build systems (X i ) i∈I ∈ X (d, I) which realize much larger values of I c , namely of the order of |I|.

## 2.4.

Invariance properties of intricacies. We have the following obvious invariances of intricacies.

Lemma 2.8. The intricacies are invariant under the following group actions on X (d, N) for some N, d ≥ 1:

(1) the group S N of permutations on {1, . . . , N} acting on X (d, N) by: (σX

$) i = X σ -1 (i) , ∀ i = 1, . . . , N.$(2) the Nth power (S d ) N of the permutation group on {0, . . . , d -1} acting on X (d, N) by: (σX

$) i = σ i • X i , ∀ i = 1, . . . , N.$In particular, for N, d ≥ 2, the maximum of I c over X ∈ X (d, N) cannot be achieved at a single probability measure on Λ d,N = {0, . . . , d -1} N . Indeed, if it were the case, then this measure would be invariant under the group action (2) above. However, this action is transitive on Λ d,N . Therefore the measure would be equidistributed on this set. Hence the maximizer would be a family of independent variables, for which the intricacies are all zero. This is a contradiction whenever N, d ≥ 2.

## Upper bounds on intricacies

In [[2]](#b1), it was proved that I c (X) < N log d/2 if X ∈ X (d, N). By comparison with "ideal entropy profiles" defined below, we prove sharper upper bounds for systems with given size and entropy.

3.1. Definitions. We define the ideal entropy profile and the corresponding intricacy values both for finite size and in the limit N → ∞. We also introduce an adapted norm to measure the distance between profiles.

Let I c be some intricacy. It is convenient to use the following probabilistic representation of the coefficients c based on the the random variable W c with law λ c defined by (2.2). Let (Y i ) i≥1 be a sequence of i.i.d. uniform random variables on [0, 1] and let

$D N := N k=1 ½ (Y k ≤Wc) , β N := D N N , N ≥ 1. (3.1)$Conditionally on W c , D N is a binomial variable with parameters (N, W c ). In particular, for all g : N → R, by (2.2)

$E (g (D N )) = [0,1] N k=0 N k x k (1 -x) N -k g(k) λ c (dx) = N k=0 c N k N k g(k),(3.2)$and therefore, for all bounded Borel f :

$[0, 1] → R E (f (β N )) = N k=0 c N k N k f k N . (3.3)$We recall the Definition 1.4 of the entropy profile of X ∈ X (d, N): h X (0) = 0,

$h X k N = 1 N k S⊂I, |S|=k H(X S ) N log d , k ∈ I := {1, . . . , N}$and h X is affine on each interval k-1 N , k N , k ∈ I. We can now define the ideal profiles and their intricacies. Definition 3.1. For x ∈ [0, 1] and N ≥ 1, the ideal entropy profile is

$h * x (t) := t ∧ x = min{t, x}(3.4)$and the corresponding (normalized) intricacies are, for finite N:

$i c N (x) := 2 N k=0 c N k N k h * x (k/N) -x = 2 E (x ∧ β N ) -x (3.5)$and, for N → ∞:

$i c (x) := 2 1 0 (t ∧ x) λ c (dt) -x = 2 E(x ∧ W c ) -x. (3.6)$We remark that the ideal profile h * x does not depend on the intricacy I c . Finally, we define a family of norms. For all bounded Borel f :

$[0, 1] → R, let f c,N := N k=0 c N k N k |f (k/N)| = E (|f (β N )|) .(3.7)$Remark 3.2. For the particular cases of Example 2.3 we have more explicit expressions. For the Edelman-Sporns-Tononi neural complexity, the above reduces to i(x) = x(1x), x ∈ [0, 1], for the uniform intricacy

$i U (x) = min{x, 1 -x}, x ∈ [0, 1],$and for the symmetric p-intricacy

$i p (x) = min{x, 1 -x, p, 1 -p}, x ∈ [0, 1].$3.2. Upper bounds and distance from the ideal profile. In this section we prove the following upper bounds Proposition 3.3. Let I c be an intricacy.

(

$) i c : [0, 1] → [0, 1] is a concave function admitting the Lipschitz constant 1 and symmetric about 1/2: i c (1 -x) = i c (x). Moreover, i c (1/2) = max x∈[0,1] i c (x). (2) |i c (x) -i c N (x)| ≤ 1/ √ N . (3) All systems X ∈ X (d, N) with H(X) N log d = x satisfy: I c (X) N log d = i c N (x) -h X -h * x c,N ≤ i c N (x). (3.8) (4) If X N ∈ X (d, N) and lim N →∞ H(X N ) N log d = x, then lim sup N →∞ I c (X N ) N log d ≤ i c (x). (3.9)1$Observe that to show that i c (x) is indeed the value of the limit (3.9), rather than a mere upper bound, requires to prove the existence of sequences saturating the inequality. This is deferred to the next section. Before proving Proposition 3.3, we need some preliminary material which will also be useful later.

## 3.3.

The functions i c N (x) and i c (x). We consider the first two points of the proposition, beginning with the convergence of i c N (x) → i c (x).

Lemma 3.4. For all x ∈ [0, 1]

$|i c N (x) -i c (x)| ≤ 1 2 √ N , N ≥ 1.$Proof. We use the probabilistic representations (3.6) and (3.5) and we obtain

$|i c N (x) -i c (x)| ≤ E (|h * x (β N ) -h * x (W c )|) ≤ E (|β N -W c |) ≤ E |β N -W c | 2 .$Since D N = Nβ N is, conditionally on W c , a binomial variable with parameters (N, W c ), we have that

$E |β N -W c | 2 = E (Var (β N | W c )) = E W c (1 -W c ) N ≤ 1 4N(3.10)$and the result is proven. Now, we analyze the limit function i c (x).

Lemma 3.5.

(

$) i c (x) = E(min{x, 1 -x, W c , 1 -W c }) for all x ∈ [0, 1]. (2) The function i c : [0, 1] → [0, 1] is 1-Lipschitz and concave. The distributional second derivative of i c is -2λ c . (3) i c (x) = i c (1 -x) for all x ∈ [0, 1]. (4) i c achieves its maximum at x = 1/2 and i c (1/2) = E(W c ∧ (1 -W c )).1$(5) i c is maximum only at x = 1/2 if and only if 1/2 belongs to the support of λ c .

Proof. First, for all x, a ∈ [0, 1]

$x ∧ a + x ∧ (1 -a) -x = min{x, 1 -x, a, 1 -a}. (3.11)$Indeed, one can assume a ≤ 1a and then check the above in the three cases:

x ≤ a, a ≤ x ≤ 1a and x ≥ 1a. Since W c has same law as 1 -W c , point (1) and (3) follow. Concavity, 1-Lipschitz continuity and symmetry w.r.t. 1/2 follow easily. Moreover, an integration by parts shows that for all ϕ ∈ C ∞ (R) with compact support contained in (0, 1):

$[0,1] ϕ ′′ (x) i c (x) dx = 2 [0,1] [0,1] ϕ ′′ (x) x ∧ t dx λ c (dt) - [0,1] ϕ ′′ (x) x dx = 2 [0,1] t 0 ϕ ′′ (x) x dx + 1 t ϕ ′′ (x) tdx λ c (dt) -0 = 2 [0,1] [ϕ ′ (t)t -ϕ(t) -tϕ ′ (t)] λ c (dt) = -2 [0,1] ϕ(t) λ c (dt), proving that (d/dx) 2 i c = -2λ c as distributions. Point (2) is proved. Since i c is concave and i c (x) = i c (1 -x) then i c (x) ≤ i c (1/2) = E(W c ∧ (1 -W c )) for all x ∈ [0, 1]. Point (4) is proved.$Let us now assume x < 1/2 (the other case being similar) so that

$x = x ∧ (1 -x). Set w = W c ∧ (1 -W c ). Then, by (3.11) i c (1/2) -i c (x) = E (w -x ∧ w) = E (w -x) ½ x<w) .$Hence, i c (x) < i c (1/2) if and only if P(x < w) > 0, i.e. 1/2 is the unique maximum point if and only if λ c (]x, 1x[) > 0 for all x < 1/2. This proves the last point. These sets are endowed with the partial order: h ≤ g if and only if h(t) ≤ g(t) for all t ∈ [0, 1]. Each Γ x has a unique maximal element: the previously introduced ideal entropy profile, h * x (t) = t ∧ x. Lemma 3.6. For any X ∈ X (d, N), the entropy profile h X , defined according to Def. 1.4, belongs to Γ.

Proof. Let X ∈ X(d, N). Setting I := {1, . . . , N} and

$H k := 1 N k S⊂I, |S|=k H(X S ), k = 0, 1, . . . , N we must prove that 0 = H 0 ≤ H 1 ≤ • • • ≤ H N = H(X), H k+1 -H k ≤ log d, 0 ≤ k < N.$The equalities H 0 = 0 and H N = H(X) are obvious. Let 0 ≤ k < N and compute:

$H k+1 = 1 N k+1 |S|=k+1 H(X S ) = 1 N k+1 |S|=k 1 k + 1 i∈S c H(X S∪{i} ) ≤ k!(N -k -1)! N! |S|=k (N -k)(H(X S ) + log d) = H k + log d$where H(X S∪{i} ) ≤ H(X S ) + H(X i ) by (A.3) and H(X i ) ≤ log d by (A.1). The same computation, since H(X S∪{i} ) ≥ H(X S ) by (A.2), proves

$H k ≤ H k+1 . Let for any h ∈ Γ G c N (h) := 2 N k=0 c N k N k h(k/N) -h(1) = 2 E (h (β N )) -h(1). Lemma 3.7. Fix x ∈ [0, 1]. (1) For all X ∈ X (d, N) G c N (h X ) = I(X) N log d . (2) h * x is the unique maximizer of G c N in Γ x and G c N (h * x ) = i c N (x). (3) For arbitrary h ∈ Γ x , we have h -h * x c,N = |G c N (h) -G c N (h * x )|. (3.12) Proof. Since MI(X, Y ) = H(X) + H(Y ) -H(X, Y ), c I S = c I S c$, and S c I S = 1, we obtain

$I c (X) = 2 N k=0 c N k |S|=k H(X S ) -H(X).$Hence, the intricacy can be computed from the entropy profile:

$I c (X) N log d = 2 N k=0 c N k N k h X (k/n) -h X (1) = G c N (h X )(3.13)$and ( [1](#formula_1)) is proved. A direct computation yields for arbitrary h ∈ Γ x :

$|G c N (h * x ) -G c N (h)| = G c N (h * x ) -G c N (h) = 2 N k=0 c N k N k (h * x (k/N) -h(k/N)) = 2 N k=0 c N k N k |h * x (k/N) -h(k/N)| = h * x -h c,N ,$since each term is non-negative. This proves [(3)](#b2).

Observe that G c N : Γ x → R is monotone non-decreasing. Hence, setting x = H(X)

N log d and recalling (3.5)

$I c (X) = G c N (h X ) ≤ sup h∈Γx G c N (h) = G c N (h * x ) = 2E (x ∧ β N ) -x = i c N (x).$Moreover, I c being non-null, all c I S are positive, G c N is increasing and h * x is a maximizer. Uniqueness of the maximizer in Γ x follows from (3.12), and point (2) is proved.

3.5. Proof of Proposition 3.3. Formula (3.8) follows from Lemma 3.7, since for

$H(X) N log d = x I c (X) N log d = G c N (h X ) = G c N (h * x ) + G c N (h X ) -G c N (h * x ) = i c N (x) -h X -h * x c,N .$To prove (3.9), it is enough to use (3.8), together with the continuity of i c and the uniform convergence of i c N → i c . Proposition 3.3 is proved.

## 3.6.

No system with the ideal profile. We turn to the problem of maximizing I c over X (d, N) at fixed N for a prescribed value of the entropy H(X). The above results show that a system X ∈ X (d, N) such that h X (k/N) = h * x (k/N) (k = 0, 1, . . . , N) with x = H(X)

N log d would be an exact maximizer. However, the next Lemma shows that such X cannot exist except if K or N -K are bounded, independently of N. Thus, all we can hope is to find systems which approach the ideal profile. This will be done in section 4. 

$[0, N], H(Y σ(1) , . . . , Y σ(k) ) log d = k ∧ H, ∀ σ ∈ S N , ∀k = 1, . . . , N, then H or N -H ≤ H * .$Proof. Let K := ⌊H⌋ and K := ⌈H⌉. Without loss of generality, we assume that K ≥ 3 and we proceed by contradiction. Let us condition on the variables (X 3 , . . . , X K ) (in the following paragraphs we simply write "conditional" for "conditional on (X 3 , . . . , X K )). By assumption:

• (X 1 , X 2 ) belongs to Z := {0, . . . , d -1} 2 ;

• each X i , K < i ≤ N, is a function of X 1 , X 2 as the conditional entropy of (X 1 , X 2 , X i ) is not bigger than that of (X 1 , X 2 ). Moreover, the conditional entropy of X i is log d. Hence, each such X i defines a partition Z i of Z into d subsets. • For any pair i = j in {1, 2, K + 1, . . . , N}, (X i , X j ) has conditional entropy (H -K + 2) log d, strictly greater than that of X i or X j , both equal to log d.

In particular, Z i = Z j .

Thus, we have an injection from {1, 2, K + 1, . . . , N} into the set of partitions of Z into d subsets. This implies:

$N -K + 2 ≤ d 2 + d -1 d -1 . Thus N -H ≤ H * (d) := d 2 +d-1 d-1$.

## Random Construction of approximate maximizers

Motivated by (3.9), we introduce the following Definition 4.1. Let I c be some intricacy and let x ∈ [0, 1] and d ≥ 2.

(1) The entropy-intricacy function I c (d, x) is:

$sup lim sup N →+∞ I c (µ N ) N log d : µ N ∈ M(d, N), lim N →+∞ H(µ N ) N log d = x . (4.1) (2) A sequence of systems X N ∈ X (d, N), N ≥ 2, is an approximate x- maximizer for I c if lim N →∞ H(X N ) N log d = x and lim N →∞ I c (X N ) N log d = I c (d, x).$(3) (X N ) N is an approximate maximizer for

$I c if lim N →∞ I c (X N ) N log d = max x∈[0,1] I c (d, x). Proposition 3.3 established that I c (d, x) ≤ i c (x). Proposition 4.$3 in this section shows that this inequality is in fact an equality.

In the rest of this section we construct approximate x-maximizers by choosing uniform distributions on random supports with the appropriate size: since H(µ N ) N log d must be close to x and µ N is uniform, then the size of the (random) support of µ N must be close to d x , see (A.1). It turns out that this simple construction yields the desired results. Remark 4.2. In [[2]](#b1) we have given a different definition of I c (d, x), namely

$I c (d, x) := lim N →∞ 1 N log d sup I c (X) : X ∈ X (d, N), H(X) N log d -x ≤ δ N$for any sequence (δ N ) N of non-negative numbers converging to zero. It is easy to see that this definition and (4.1) actually coincide. We consider a family (W i ) i∈Λ d,M of i.i.d. variables, each uniformly distributed on Λ d,N , defined on a probability space (Ω, F , P). We define a random probability measure on Λ d,N

$µ N,M (x) := d -M i∈Λ d,M ½ (x=W i ) , x ∈ Λ d,N . (4.2)$In what follows we consider random variables X N,M on (Ω, F , P) such that

$P X N,M = x (W i ) i∈Λ d,M = µ N,M (x), x ∈ Λ d,N . (4.3)$In other words,

$conditionally on (W i ) i∈Λ d,M , X N,M has law µ N,M .$We are going to prove the following Remark 4.4. We stress that in the following we denote

$I c (X N,M ) = I c (µ N,M ), H(X N,M ) = H(µ N,M ) (4.6)$and that all these expressions are random variables which depend on (W i ) i∈Λ d,M . In other words, (4.6) indicates entropy and intricacy of the law of X N,M conditionally on (W i ) i∈Λ d,M . This abuse of notation seems necessary, to keep notation reasonably readable. See also Remark 4.6 below.

## 4.2.

Average intricacy of sparse random configurations. We recall that N ≥ 2, M is an integer between 1 and N and S N denotes the set of permutations of {1, . . . , N}. By Lemma 3.7,

$I c (X N,M ) = 2 N k=0 c N k N k h X (k/N) -h X (1)$, hence we get:

$E I c (X N,M ) = 1 N! σ∈S N 2 N k=1 c N k N k E H(X N,M {σ(1),...,σ(k)} ) -E H(X N,M ) .$(4.7) We are going to simplify this expression by exploiting the symmetries of our construction.

Lemma 4.5. The random vector X N,M = (X N,M 1 , . . . , X N,M N ) ∈ X (d, N) is exchangeable, i.e. for all σ ∈ S N and any Φ :

$Λ d,N → R E Φ(X N,M σ(1) , . . . , X N,M σ(N ) ) = E Φ(X N,M 1 , . . . , X N,M N ) .$Proof. Note that every σ ∈ S N induces a permutation Σ

$σ : Λ d,N → Λ d,N Σ σ (x 1 , . . . , x N ) = (x σ(1) , . . . , x σ(N ) ), x ∈ Λ d,N .$In particular, (X

$N,M σ(1) , . . . , X N,M σ(N ) ) = Σ σ (X N,M ) has, conditionally on (W i ) i∈Λ d,M , dis- tribution Σ * σ µ N,M (x) := d -M i∈Λ d,M ½ (x=Σσ(W i )) , x ∈ Λ d,N .$However, (Σ σ (W i )) i∈Λ d,M has the same distribution as (W i ) i∈Λ d,M . Therefore we conclude.

Remark 4.6. Notice that Σ * σ µ N,M has same law as µ N,M , but in general the two measures are not a.s. equal. In other words, (X

$N,M 1 , . . . , X N,M n ) is exchangeable but not exchangeable conditionally on (W i ) i∈Λ d,M .$In particular, for all k ∈ {1, . . . , N} and σ

$∈ S N E H(X N,M {σ(1),...,σ(k)} ) = E H(X N,M {1,...,k} ) ,(4.8)$and we obtain by (4.7) 

$E I c (X N,M ) = 2 N k=1 c N k N k E H(X N,M {1,...,k} ) -E H(X N,M ) . (4$$Then d M • ν(y) is a binomial variable with parameters (d M , d -k ).$Proof. Notice that, conditionally on

$(W i ) i∈Λ d,M , X N,M {1,...,k} = (X N,M 1 , . . . , X N,M k ) ∈ Λ d,k has distribution P X N,M {1,...,k} = y (W i ) i∈Λ d,M = z∈Λ d,N-k µ N,M (y, z) = d -M i∈Λ d,M z∈Λ d,N-k ½ ((y,z)=W i ) ,$where

$y ∈ Λ d,k and (y, z) ∈ Λ d,k × Λ d,N -k = Λ d,N . For fixed y ∈ Λ d,k$, the family

$T i := z∈Λ d,N-k ½ ((y,z)=W i ) , i ∈ Λ d,M is an i.i.d. family of Bernoulli variables with parameter d -k . Indeed, if Π N →k : Λ d,N → Λ d,k is the natural projection, then the law of Π N →k (W i ) is uniform on Λ d,k , so that P(T i = 1) = P(Π N →k (W i ) = y) = d -k . Hence, for all y ∈ Λ d,k , d M • ν(y) = i∈Λ d,M$T i is the sum of d M independent Bernoulli variables with parameter d -k , i.e. a binomial variable with parameters (d M , d -k ).

Let us denote from now on by B k a binomial variable with parameters (d

$M , d -k ). Set ϕ(x) := - x log x log d , ∀x > 0, ϕ(0) := 0.$Notice that the function ψ(x)

$:= -(1 + x) log(1 + x) + x + x 2 2 satisfies ψ(0) = ψ ′ (0) = 0, ψ ′′ (x) ≥ 0, ∀x ≥ 0, so that ψ(x) ≥ 0 for x ≥ 0. Moreover, ϕ(1 + x) ≥ 0 if x ∈ [-1, 0]. Hence, for all x ≥ -1, ϕ(1 + x) ≥ -½ (x>0) x + x 2 /2 log d .(4.11)$Now, by (4.10) H X N,M {1,...,k} = -

$y∈Λ k,N ν(y) log ν(y),$then we obtain by Lemma 4.7 that

$h k := 1 log d E H X N,M {1,...,k} = d k E ϕ B k d -M .(4.12)$Lemma 4.8. We have, for any 0 ≤ k ≤ M,

$h k = k + E ϕ(B k d k-M ) = M + d k-M E (ϕ(B k )) .$Proof. These identities follow from the formulae:

$E(B k ) = d M -k , ϕ(d -j ) = jd -j and ϕ(αx) = αϕ(x) + xϕ(α) for α > 0 applied to ϕ(B k d k-M • d -k ) and ϕ(B k • d -M ).$Lemma 4.9. Away from k = M, the entropy is nearly constant:

$k -2d k-M 2 ≤ h k ≤ k, k = 1, . . . , M, M -d M -k ≤ h k ≤ M, k = M + 1, . . . , N.$Proof. The upper bounds are easy. Indeed, for k ≤ M one uses (A.3), while for k > M we notice that the support of µ N,M has cardinality at most d M , and apply Let us consider now the regime k > M. We have

$(A.1) to conclude. Recall that B k is binomial with parameters (d M , d -k ). Then E(B k ) = d M -k and Var(B k ) = d M d -k (1 -d -k ). If we define J k := B k • d k-M -1 then we obtain E J 2 k = d 2(k-M ) Var(B k ) = d k-M -d -M ≤ d k-M . Hence, using (4.11) we get E ϕ(B k • d k-M ) = E (ϕ(1 + J k )) ≥ - 1 log d E ½ (J k >0) J k + J 2 k 2 ≥ -E |J k | + J 2 k ≥ - E (J 2 k ) + E J 2 k ≥ -2d k-M 2 , since E(|J k |) ≤ E(J2$$h k = d k E(ϕ(B k d -M )) = M + d k-M E (ϕ(B k )) . If B k ∈ {0, 1} then ϕ(B k ) = 0. Note that (4.11) implies that ϕ(B k ) ≥ -(B k - 1) 2 -(B k -1)$as the right hand side is zero whenever B k = 0, 1 and less than

$-(B k -1) 2 /2 -(B k -1) otherwise. Thus, d k-M E (ϕ(B k )) ≥ -d k-M E (B k -1) 2 + (B k -1) = -d k-M E B 2 k -B k = -d k-M d M -k + d 2(M -k) -d M -2k -d M -k ≥ -d M -k + d -k ≥ -d M -k .$By Lemma 4.8 we obtain the lower bound for k > M. 

$-2d -(N -M ) ≤ E (2 (D N ∧ M) -M) - E I c (X N,M ) log d ≤ E 4α -|D N -M | . (4.13)$Proof. By (4.9), (4.12) and (3.2),

$E I c (X N,M ) log d = 2 N k=1 c N k N k h k -h N = 2 E(h D N ) -h N .$So,

$E I c (X N,M ) log d -E(2(D N ∧ M) -M) = 2E(h D N -D N ∧ M) + M -h N$We conclude by Lemma 4.9.

Lemma 4.11. Let x ∈ ]0, 1[ and M := ⌊xN⌋. Then for α := d 1/2 > 1 and some constant C > 0 we have

$E α -|D N -M | ≤ C √ N , ∀ N ≥ 1. (4.14)$Proof. To ease notation, in this proof we drop the subscript c from W c . By (2.2) and (3.2), we have that

$P(D N = k) = N k c N k = N k E W k (1 -W ) N -k , k = 0, . . . , N.(4.15)$We claim that for all 1 ≤ k ≤ ⌊ N 2 ⌋ we have P(D N = k -1) ≤ P(D N = k).

## Indeed,

$P(D N = k) -P(D N = k -1) = = N! k! (N -k + 1)! E W k-1 (1 -W ) N -k [(N + 1 -k)W -k(1 -W )] . = N! k! (N -k + 1)! E W k-1 (1 -W ) N -k [(N + 1)W -k] .$By the symmetry of λ c w.r.t. 1/2 we have, since 2k ≤ N + 1,

$E W k-1 (1 -W ) N -k [(N + 1)W -k] = 2 -N +1 N + 1 2 -k P(W = 1/2)+ + E W k-1 (1 -W ) N -k [(N + 1)W -k] ½ (W >1/2) + + E W N -k (1 -W ) k-1 [(N + 1)(1 -W ) -k] ½ (W >1/2) ≥ E (W (1 -W )) k-1 ½ (W >1/2) • • ((N + 1)W -k) W N -2k+1 + ((N + 1)(1 -W ) -k) (1 -W ) N -2k+1 .$This expectation is nonnegative. Indeed, the first term is nonnegative as 2k ≤ N + 1 and W > 1/2. We assume the second term to be negative, otherwise we are done.

As 1 -W ≤ W , we get:

$P(D N = k) -P(D N = k -1) = = E ½ W >1/2 ((N + 1)W -k + (N + 1)(1 -W ) -k)(1 -W ) N -2k+1 ≥ E ½ W >1/2 ((N + 1) -2k)W N -2k+1 ≥ 0, proving the claim. We set L := ⌊ N 2 ⌋. Then N -L ≥ L ≥ N 2 -$1 and by (4.15) we obtain for all 0

$≤ k ≤ N N k c N k ≤ N L c N L = N! L!(N -L)! E W L (1 -W ) N -L ≤ N! L!(N -L)! E W L (1 -W ) L ≤ N! L!(N -L)! 4 -L ≤ N! L!(N -L)! 2 -N +2 . By Stirling's formula n! = √ 2πn(n/e) n (1 + O(1/n)), there is a constant C ≥ 0 such that 2 -N N! L!(N -L)! ≤ C N -1 2 , N → +∞, L = N 2 .$Then, we obtain for some constants

$C 1 , C 2 ≥ 0 E α -|D N -M | = N k=0 N k c N k α -|k-M | ≤ C 1 N -1 2 N k=0 α -|k-M | ≤ C 1 N -1 2 2 +∞ k=0 α -k = C 2 N -1 2 ,$and the proof is finished.

Proof of Proposition 4.3. Let x ∈ ]0, 1[ and M := ⌊xN⌋ ≥ 1 (N is large). By Lemma 4.9 for k = N

$E M N - H(X N,M ) N log d = E M N - H(X N,M ) N log d = E M -h N N ≤ d M -N N ≤ d -N (1-x) .$Thus,

$N ≥1 E M N - H(X N,M ) N log d < +∞,(4.16$) therefore a.s.

$N ≥1 M N - H(X N,M ) N log d < +∞,$and in particular a.s.

lim

$N →+∞ M N - H(X N,M ) N log d = 0.$(2) Let x ∈ [0, 1] and let (X N ) N be an approximate x-maximizer for I c . Then:

$lim N →∞ sup supp(λc) |h X N -h * x | = 0. (5.2)$In particular, if x ∈ supp(λ c ) then

$lim N →∞ sup [0,1] |h X N -h * x | = 0. (5.3)$(3) If x ∈ supp(λ c ) then an approximate x-maximizer (X N ) N for I c is an approximate x-maximizer for any other intricacy I c ′ .

Remark 5.3. The extra assumption about the support of λ c cannot be dropped. Indeed, for point (1) of Theorem, observe that, in the case of the p-symmetric intricacy, 1/2 is not in the support of 1 2 (δ p + δ 1-p ) for p = 1/2 and approximate maximizers of I p satisfy only

$p ≤ lim inf N H(X N ) N log d ≤ lim sup N H(X N ) N log d ≤ 1 -p.$The entropy may accumulate on any point on the interval [p, 1p].

Notice however that for many intricacies, including the neural complexiy, the support of λ c is the whole interval, making this assumption satisfied for all x ∈ [0, 1]. Remark 5.5. Let X N,M the random system constructed in (4.2) and (4.3), with M := ⌊xN⌋, x ∈ ]0, 1[. For the Edelman-Sporns-Tononi intricacy I we have that the support of the associated probability measure λ is [0, 1], since it is the Lebesgue measure by Lemma 3.8 of [[2]](#b1). Since (X N,M ) N is a.s. an approximate x-maximizer for I by Proposition 4.3, then by point (3) of Theorem 5.2 a.s. this sequence is an approximate x-maximizer simultaneously for all intricacies I c . This has the following consequence for approximate maximizers (i.e., without entropy constraints). An approximate maximizer for some intricacy I c where 1/2 / ∈ supp(λ c ) is not necessarily an approximate maximizer for another intricacy. But an approximate 1/2-maximizer for any intricacy is automatically an approximate maximizer for all intricacies.

Proof of Theorem 5.2. Let us set for simplicity of notation:

$x N := H(X N ) N log d , I N := I c (X N ) N log d .(5.5)$If 1/2 is in the support of λ c , then it is the unique point where i c (x) achieves its maximum. Then, Theorem 5.1 implies that no x = 1/2 can be an accumulation point of x N , N ≥ 1. Thus an approximate maximizer is an approximate 1/2-maximizer.

It is now enough to prove point [(2)](#b1). By definition of approximate x-maximizers, x N → x and I N → i c (x). Using point (2) of Proposition 3.3, it follows that |I Ni c N (x)| → 0 and by (3.8) we have h X Nh * x c,N → 0. Notice now that for any K-Lipschitz function f : [0, 1] → R, by (3.10)

$|E(f (β N )) -E(f (W c ))| ≤ K √ N As entropy profiles are 1-Lipschitz, we obtain |h X N -h * x | dλ c → 0.$As all functions (h X Nh * x ) N are 2-Lipschitz, (5.2) follows by a routine argument.

$Assuming now x ∈ supp(λ c ), lim N →∞ h X N (x) = h * x (x) = x. On the one hand, as h X N (0) = 0 and h X N is 1-Lipschitz, it follows that the convergence lim N →∞ h X N (t) = h * x (t) = t occurs for all t ∈ [0, x]. On the other hand, all h X N being non-decreasing, h X N (x) = x ≤ h X N (t) ≤ h X N (1) → x.$Hence the previous convergence occurs for all x ∈ [0, 1], proving (5.3). Finally, let us prove point [(3)](#b2). By (5.3) the profiles h X N converge to h * x uniformly on [0, 1]. Let I c ′ be any other intricacy. By uniform convergence we have h

$X N - h * x c ′ ,N ≤ sup [0,1] |h X N -h * x | → 0. By (3.8) and Lemma 3.4 I c ′ (X) N log d = i c ′ N (x) -h X -h * x c ′ ,N → i c ′ (x), N → +∞.$By Theorem 5.1, i c ′ (x) = I c ′ (d, x) and therefore (X N ) N is an approximate xmaximizer for I c ′ .

We have the following consequence for approximate x-maximizers. We recall that H(Y | Z) denotes the conditional entropy, see the Appendix. Theorem 5.6. Suppose that x ∈ supp(λ c ) and let (X N ) N be an approximate xmaximizer for some 0

$≤ x ≤ 1. Then (1) If y ∈ ]0, x] then for all ε > 0 lim N →+∞ 1 N ⌊yN ⌋ # S ⊂ {1, . . . , N} : |S| = ⌊yN⌋, (1 -ε)|S| log d < H(X N S ) ≤ |S| log d = 1.$(

$) If y ∈ [x, 1[ then for all ε > 0 lim N →+∞ 1 N ⌊yN ⌋ #{S ⊂ {1, . . . , N} : |S| = ⌊yN⌋, H(X N | X N S ) < εxN log d} = 1.2$This result can be loosely interpreted as follows: as N → +∞, (1) if y ∈ ]0, x] then for almost all subsets S with |S| = ⌊yN⌋, X S is almost uniform;

(2) if y ∈ [x, 1[ then for almost all subsets S with |S| = ⌊yN⌋, X is almost a function of X S . This follows from the relation between entropy and conditional entropy on one side and independence versus dependence on the other side, see the Appendix.

Proof of Theorem 5.6. Let y ∈ ]0, 1[. By [(5.3)](#), h X N (y) → h * x(y) = x ∧ y as N → +∞. By the definition 1.4 of h X N , we obtain, setting k N := ⌊yN⌋,

$1 N k N |S|=k N h * x (y) - H(X S ) N log d = h * x (y) - 1 N k N |S|=k N H(X S ) N log d = h * x (y) -h X N (y) → 0,$since all terms in the sum are non-negative by Lemma 3.6. Let Z N , defined on (Ω, F , P), be a random subset of {1, . . . , N} defined by

$P(Z N = S) = 1 N k N , if |S| = k N .$Then the above formula can be rewritten as follows lim

$N →+∞ E h * x (y) - H(X Z N ) N log d = 0.$Since L 1 convergence implies convergence in probability, we obtain This readily implies the Theorem, by recalling that H(X N | X N S ) = H(X N ) -H(X N S ) and that H(X N )

N log d → x by assumption.

## Appendix A. Entropy

In this Appendix, we recall needed facts from basic information theory. The main object is the entropy functional which may be said to quantify the randomness of a random variable. We refer to [[3]](#b2) for more background.

Let X be a random variable taking values in a finite space E. We define the entropy of X H(X) := -x∈E P X (x) log(P X (x)), P X (x) := P(X = x),

where we adopt the convention 0 • log(0) = 0 • log(+∞) = 0.

We recall that 0 ≤ H(X) ≤ log |E|, (A.1)

More precisely, H(X) is minimal iff X is a constant, it is maximal iff X is uniform over E. To prove (A.1), just notice that since ϕ ≥ 0 and ϕ(x) = 0 if and only if If we have a E-valued random variable X and a F -valued random variable Y defined on the same probability space, with E and F finite, we can consider the vector (X, Y ) as a E × F -valued random variable The entropy of (X, Y ) is then H(X, Y ) :=x,y P (X,Y ) (x, y) log(P (X,Y ) (x, y)), P (X,Y ) (x, y) := P(X = x, Y = y).

This entropy H(X, Y ) does not only depends on the (separate) laws of X and Y but on the extent to which the "randomness of the two variables is shared". The following notions formalize this idea. We first claim that it is nonnegative.

Remark that P X (x) and P Y (y), defined in the obvious way, are the marginal laws of P (X,Y ) (x, y), i.e. P X (x) = y P (X,Y ) (x, y), P Y (y) =

x P (X,Y ) (x, y).

In particular, P X (x) ≥ P (X,Y ) (x, y) for all x, y. Therefore 

![A system of coefficients is a collection of numbers c := (c I S : I ⊂⊂ N * , S ⊂ I), i.e., I ranges over the finite subsets of N * , for all I and all S ⊂ I:]()

![Intricacy as a function of the profile. Let us setΓ := {h : [0, 1] → [0, 1] : h(0) = 0, t → h(t)is non-decreasing and 1-Lipschitz} and for any real number x ∈ [0, 1] Γ x := {h ∈ Γ : h(1) = x} .]()

![For each d ≥ 2, there exists H * = H * (d) < ∞ with the following property. If N ≥ 1 and Y 1 , . . . , Y N are random variables taking values in {0, . . . , d -1} and defined on the same probability space such that, for some real number H ∈]()

![Sparse random configurations. Let N ≥ 2 and 0 ≤ M ≤ N be integers. We denote Λ d,n := {0, . . . , d -1} n , ∀ n ≥ 1.]()

![For integers N ≥ 1, 0 ≤ M ≤ N, let X M,N be the random systems defined above. Let x ∈ [0, 1]. For any intricacy I c we have, a.s. and in L 1 limN →+∞ I c (µ N,⌊xN ⌋ ) N log d = i c (x) (4.4)]()

![Lemma Let y ∈ Λ d,k , k ∈ {1, . . . , N} and set ν(y) := z∈Λ d,N-k µ N,M (y, z).(4.10)]()

![k ) by Cauchy-Schwartz and d kd k-M ≤ 1. By Lemma 4.8 we obtain the desired lower bound for k ≤ M.]()

![Estimation of the expected Intricacy. Lemma 4.10. Let x ∈ ]0, 1[, M := ⌊xN⌋ and α := d 1/2 > 1. For all N ≥ 2, using the notation (3.1),]()

![In the setting of point (2) of Theorem 5.2, if x does not belong to the support of λ c , then one can prove with similar arguments that limN →∞ sup [0,a]∪[b,1] |h X Nh * x | = 0 (5.4) where a := sup([0, x] ∩ supp(λ c )), b := inf([x, 1] ∩ supp(λ c )), with the convention sup ∅ := 0 and inf ∅ := 1.]()

![∈ {0, 1}, and by strict convexity of x → ϕ(x) = x log x and Jensen's inequalitylog |E| -H(X) = 1 |E| x∈E P X (x) |E| (log(P X (x)) + log |E|) = 1 |E| x∈E ϕ (P X (x) |E|) ≥ ϕ 1 |E| x∈E P X (x) |E| = ϕ(1) = 0, with log |E| -H(X) = 0 if and only if P X (x) |E| is constant in x ∈ E.]()

![Conditional Entropy. The conditional entropy of X given Y is:H(X | Y ) := H(X, Y ) -H(Y ).]()

![(X,Y ) (x, y) log P (X,Y ) (x, y) P X (x) (X,Y ) (x, y) log P (X,Y ) (x, y) ≥ -x P X (x) log P X (x) = H(X), i.e., H(X|Y ) ≥ 0, proving the claim. ThereforeH(X, Y ) ≥ max{H(X), H(Y )}. (A.2) Moreover H(X, Y ) = H(X), i.e. H(Y |X) = 0, if and only if P (X,Y ) (x, y) = P X (x) whenever P (X,Y ) (x, y) = 0, which means that Y is a function of X. On the other hand, H(X, Y ) ≤ H(X) + H(Y ) (A.3) with equality, i.e., H(Y |X) = H(Y ), if and only if X and Y are independent. This can be shown by considering the Kullback-Leibler divergence or relative entropy:I := x,y P (X,Y ) (x, y) log P (X,Y ) (x, y) P X (x) P Y (y) .Since log(•) is concave, by Jensen's inequality-I ≤ log x,y P (X,Y ) (x, y) P X (x) P Y (y) P (X,Y ) (x, y) = logx,yP X (x) P Y (y) = 0.By strict concavity, I = 0 if and only if P (X,Y ) (x, y) = P X (x) P Y (y) for all x, y, i.e., whenever X and Y are independent. By the above considerations, H(X | Y ) ∈ [0, H(X)] is a measure of the uncertainty associated with X if Y is known. It is minimal iff X is a function of Y and it maximal iff X and Y are independent.A.2. Mutual Information. Finally, we recall the notion of mutual information between two random variables X and Y defined on the same probability space:MI(X, Y ) := H(X) + H(Y ) -H(X, Y ) = H(X) -H(X | Y ) = H(Y ) -H(Y | X) = x,y P (X,Y ) (x, y) log P (X,Y ) (x, y) P X (x) P Y (y) .This quantity is a measure of the common randomness of X and Y . By (A.2) and (A.3) we have MI(X, Y ) ∈ [0, min{H(X), H(Y )}]. MI(X, Y ) is minimal (zero) iff X, Y are independent and maximal, i.e. equal to min{H(X), H(Y )}, iff one variable is a function of the other.]()

