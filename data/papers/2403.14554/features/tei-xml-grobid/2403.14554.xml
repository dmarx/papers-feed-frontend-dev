<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Gaussian Frosting: Editable Complex Radiance Fields with Real-Time Rendering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Antoine</forename><surname>Gu√©don</surname></persName>
							<idno type="ORCID">0009-0001-3107-4454</idno>
							<affiliation key="aff0">
								<orgName type="department">Ecole des Ponts</orgName>
								<orgName type="institution" key="instit1">LIGM</orgName>
								<orgName type="institution" key="instit2">Univ Gustave Eiffel</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
							<idno type="ORCID">0000-0001-9985-4433</idno>
							<affiliation key="aff0">
								<orgName type="department">Ecole des Ponts</orgName>
								<orgName type="institution" key="instit1">LIGM</orgName>
								<orgName type="institution" key="instit2">Univ Gustave Eiffel</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Gaussian Frosting: Editable Complex Radiance Fields with Real-Time Rendering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5B8399124DF4BA775F18C0EE35F50C49</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Gaussian Splatting</term>
					<term>Mesh</term>
					<term>Differentiable rendering</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p><ref type="url" target="https://anttwo.github.io/frosting/">https://anttwo.github.io/frosting/</ref> (a) Rendering three different scenes with Frosting: Bicycle, Buzz, and Kitten. (b) Composition: Buzz is riding a giant kitten jumping over a bench.</p><p>(c) Fuzzy details rendered with Frosting -occlusions are correctly rendered (d) Rendering with SuGaR [11] -the fur does not occlude the legs correctly</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fig. <ref type="figure">1</ref>: We propose to represent surfaces by a mesh covered with a "Frosting" layer of varying thickness and made of 3D Gaussians. This representation captures both complex volumetric effects created by fuzzy materials such as the cat's hair or grass as well as flat surfaces. Built from RGB images only, it can be rendered in real-time and animated using traditional animation tools. In the example above, we were able to animate both Buzz and the kitten, changing their original pose (a) while preserving high-quality rendering (b): Contrary to SuGaR, very fine and fuzzy details such as the kitten's hair can be seen covering Buzz's legs in a realistic way (c).  A thicker layer is shown with a brighter value. Our method automatically builds a thick Frosting layer for fuzzy areas such as the fur of the red panda plush, and a thin Frosting layer for flat surfaces such as the table or the floor. Adapting the thickness of the Frosting layer allows for allocating more Gaussians in areas where more volumetric rendering is necessary near the surface, resulting in an efficient distribution of Gaussians in the scene. As we demonstrate in the paper, using an adaptive thickness results in higher performance than using a predefined constant thickness, and reduces artifacts when animating the mesh.</p><p>1 Introduction 3D Gaussian Splatting (3DGS) <ref type="bibr" target="#b16">[17]</ref> has recently conquered the field of 3D reconstruction and image-based rendering. By representing a scene with a large set of tiny Gaussians, 3DGS allows for fast reconstruction and rendering, while nicely capturing fine details and complex light effects. Compared to earlier neural rendering methods such as NeRFs <ref type="bibr" target="#b22">[23]</ref>, 3DGS is much more efficient for both the reconstruction and rendering stages by a large margin. However, like NeRFs, Vanilla 3DGS does not allow easy edition of the reconstructed scene. The Gaussians are unstructured, disconnected from each other, and it is not clear how a designer can manipulate them, for example to animate the scene. Very recently, SuGaR <ref type="bibr" target="#b10">[11]</ref> showed how to extract a mesh from the output of 3DGS. Then, by constraining Gaussians to stay on the mesh, it is possible to edit the scene with traditional Computer Graphics tools for manipulating meshes. But by flattening the Gaussians onto the mesh, SuGaR loses the rendering quality possible with 3DGS for fuzzy materials and volumetric effects.</p><p>In this paper, we introduce Gaussian Frosting-or Frosting, for short-a hybrid representation of 3D scenes that is editable as a mesh while providing a rendering quality at least equal, sometimes superior to 3DGS. The key idea of Frosting is to augment a mesh with a layer containing Gaussians. The thickness of the Frosting layer adapts locally to the material of the scene: The layer should be thin for flat surfaces to avoid undesirable volumetric effects, and thicker around fuzzy materials for realistic rendering. As shown in Figure <ref type="figure" target="#fig_1">2</ref>, using the Frosting representation, we can not only retrieve a highly accurate editable mesh but also render complex volumetric effects in real-time.</p><p>Frosting is reminiscent of the Adaptive Shells representation <ref type="bibr" target="#b39">[40]</ref>, which relies on two explicit triangle meshes extracted from a Signed Distance Function to control volumetric effects. Still, Frosting allies a rendering quality superior to the quality of Adaptive Shells to the speed efficiency of 3DGS for reconstruction and rendering, while being easily editable as it depends on a single mesh.</p><p>While the Frosting representation is simple, it is challenging to define the local thickness of its layer. To extract the mesh, we essentially rely on SuGaR, which we improved with a technique (described in the supplementary material) to automatically tune a critical hyperparameter. To estimate the Frosting thickness, we introduce a method to define an inner and outer bound for the Frosting layer at each vertex of this mesh based on the Gaussians initially retrieved by 3DGS around the mesh. Finally, we populate the Frosting layer with randomly sampled Gaussians and optimize these Gaussians constrained to stay within the layer. We also propose a simple method to automatically adjust in real-time the parameters of the Gaussians when animating the mesh.</p><p>In summary, we propose a simple yet powerful surface representation that captures complex volumetric effects, can be edited with traditional tools, and can be rendered in real-time. We also propose a method to build this representation from images, based on recent developments on Gaussian Splatting. We will release our code including a viewer usable in browser as an additional contribution. We also changed the pose of the characters by using the rigging tool in Blender (b).</p><p>Similarly to surface-based methods like SuGaR <ref type="bibr" target="#b10">[11]</ref>, Frosting can be used for editing and compositing scenes, but allows for better rendering of complex volumetric effects and fuzzy materials, such as hair or grass.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The goal of image-based rendering (IBR) is to create a representation of a scene from a given set of images in order to generate new images of the scene. Different types of scene representations have been proposed, ranging from explicit and editable ones like triangle meshes or point clouds, to implicit or non-editable ones like voxel grids, multiplane images, or neural implicit functions. Volumetric IBR methods. A recent breakthrough in IBR is Neural Radiance Fields (NeRF) <ref type="bibr" target="#b22">[23]</ref>, which uses a multilayer perceptron (MLP) to model a continuous volumetric function of density and color. NeRF can render novel views with high quality and view-dependent effects, by using volumetric ray tracing. However, NeRF is slow and memory hungry. Several works have tried to improve NeRF's efficiency and training speed by using discretized volumetric representations like voxel grids and hash tables to store learnable features that act as inputs for a much smaller MLP <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b44">45]</ref>, or to improve rendering performance by using hierarchical sampling strategies <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b45">46]</ref>. Other works have also proposed to modify NeRF's representation of radiance and include an explicit lighting model to increase the rendering quality for scenes with specular materials <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b46">47]</ref>. However, most volumetric methods rely on implicit representations that are not suited to editing compared to triangle meshes, for which most standard graphics hardware and software are tailored. Surface-based IBR methods. Triangle meshes have been a popular 3D representation for generating novel views of scenes <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b40">41]</ref> after Structure-from-motion (SfM) <ref type="bibr" target="#b34">[35]</ref> and multi-view stereo (MVS) <ref type="bibr" target="#b9">[10]</ref> have enabled 3D reconstruction of surfaces. Deep-learning-based mesh representations <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref> have also been used for improving view synthesis using explicit surface meshes; However, even though mesh-based methods allow for very efficient rendering, they have trouble capturing complex and very fine geometry as well as fuzzy materials.</p><p>Hybrid IBR methods. Some methods use a hybrid volumetric representation to recover surface meshes that are suitable for downstream graphics applications while efficiently modeling view-dependent appearance. Specifically, some works optimize a Neural Radiance Field in which the density is replaced by an implicit signed distance function (SDF), which provides a stronger regularization on the underlying geometry <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42]</ref>. However, most of these methods are not aimed at real-time rendering. To mitigate this issue, other approaches greatly accelerate rendering by "baking" the rendering computation into the extracted mesh after optimization with a dedicated view-dependent appearance model <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b42">43]</ref>. Even though these surface-based methods encode the surface using a volumetric function represented by an MLP during optimization, they struggle in capturing fine details or fuzzy materials compared to volumetric methods.</p><p>Adaptive Shells <ref type="bibr" target="#b39">[40]</ref> is a recent method that achieves a significant improvement in rendering quality by using a true hybrid surface-volumetric approach that restricts the volumetric rendering of NeRFs to a thin layer around the object. This layer is bounded by two explicit meshes, which are extracted after optimizing an SDF-based radiance field. The layer's variable thickness also improves the rendering quality compared to a single flat mesh. This method combines the high-quality rendering of a full volumetric approach with the editability of a surface-based approach by manipulating the two meshes that define the layer. However, Adaptive Shells depends on a neural SDF <ref type="bibr" target="#b38">[39]</ref>, which has some limitations in its ability to reconstruct precise surfaces, and requires more than 8 hours to optimize a single synthetic scene, which is much longer than the recent Gaussian Splatting methods.</p><p>Gaussian Splatting. Gaussian Splatting <ref type="bibr" target="#b16">[17]</ref> is a new volumetric representation inspired by point cloud-based radiance fields <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b31">32]</ref> which is very fast to optimize and allows for real-time rendering with very good quality. One of its greatest strengths is its explicit 3D representation, which enables editing tasks as each Gaussian exists individually and can be easily adjusted in real-time. Some appearance editing and segmentation methods have been proposed <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b43">44]</ref>, but the lack of structure in the point cloud makes it almost impossible for a 3D artist or an animator to easily modify, sculpt or animate the raw representation. The triangle mesh remains the standard 3D structure for these applications. A recent work, SuGaR <ref type="bibr" target="#b10">[11]</ref>, extends this framework by aligning the Gaussians with the surface and extracting a mesh from them. Gaussians are finally flattened and pinned on the surface of the mesh, which provides a hybrid representation combining the editability of a mesh with the high-quality rendering of Gaussian Splatting. However, SuGaR remains a surface-based representation with limited capacity in reconstructing and rendering fuzzy materials and volumetric effects, resulting in a decrease in performance compared to vanilla Gaussian Splatting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">3D Gaussian Splatting and Surface Reconstruction</head><p>Our method relies on the original 3D Gaussian Splatting (3DGS) method <ref type="bibr" target="#b16">[17]</ref> for initialization and on SuGaR <ref type="bibr" target="#b10">[11]</ref> to align Gaussians with the surface of the scene and facilitate the extraction of a mesh. We briefly describe 3DGS and SuGaR in this section before describing our method in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">3D Gaussian Splatting</head><p>3DGS represents the scene as a large set of Gaussians. Each Gaussian g is equipped with a mean ¬µ g ‚àà R 3 and a positive-definite covariance matrix Œ£ g ‚àà R 3√ó3 . The covariance matrix is parameterized by a scaling vector s g ‚àà R 3 and a quaternion q g ‚àà R 4 encoding the rotation of the Gaussian.</p><p>In addition, each Gaussian has a view-dependent radiance represented by an opacity Œ± g ‚àà [0, 1] and a set of spherical harmonics coordinates defining the colors emitted for all directions. To render an image from a given viewpoint, a rasterizer "splats" the 3D Gaussians into 2D Gaussians parallel to the image plane and blends the splats depending on their opacity and depth. This rendering is extremely fast, which is one of the advantages of 3DGS over volumetric rendering as in NeRFs for example <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>Gaussian Splatting can be seen as an approximation of the traditional volumetric rendering of radiance fields with the following density function d, computed as the sum of the Gaussian values weighted by their alpha-blending coefficients at any 3D point p ‚àà R 3 :</p><formula xml:id="formula_0">d(p) = g Œ± g exp - 1 2 (p -¬µ g ) T Œ£ -1 g (p -¬µ g ) .<label>(1)</label></formula><p>We initialize our Gaussian Frosting method using a vanilla 3DGS optimization: Gaussians are initialized using the point cloud produced by an SfM <ref type="bibr" target="#b34">[35]</ref> algorithm like COLMAP <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>, required to compute camera poses. The Gaussians' parameters (3D means, scaling vectors, quaternions, opacities, and spherical harmonics coordinates) are then optimized to make the renderings match the ground truth images of the scene, using a rendering loss that only consists in a combination of a pixel-wise L1 distance and a more structural D-SSIM term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">SuGaR Mesh Extraction</head><p>Vanilla 3DGS does not have regularization explicitly encouraging Gaussians to align with the true surface of the scene. Our Gaussian Frosting representation relies on a mesh that approximates this surface, in order to be editable by traditional tools. To obtain this mesh, we rely on the method proposed in SuGaR <ref type="bibr" target="#b10">[11]</ref>, which we improve by automatically selecting a critical hyperparameter.</p><p>SuGaR proposes a regularization term encouraging the alignment of the 3D Gaussians with the true surface of the scene during the optimization of Gaussian Splatting, as well as a mesh extraction method. After enforcing the regularization, the optimization provides Gaussians that are mostly aligned with the We refer to these Gaussians as unconstrained. We then regularize these Gaussians to enforce their alignement with the surface, and extract a mesh that will serve as a basis for the Frosting. Next, we use the misalignment of surface-aligned Gaussians to identify areas where more volumetric rendering is needed, and we build search intervals Ji around the mesh's vertices vi. Finally, we use the density function of the unconstrained Gaussians to refine the intervals, resulting in a Frosting layer. We finally sample a novel, densified set of Gaussians inside the layer.</p><p>surface albeit not perfectly: We noticed that in practice, a large discrepancy between the regularized Gaussians and the extracted mesh indicates the presence of fuzzy materials or surfaces that require volumetric rendering. We thus exploit this discrepancy as a cue to evaluate where the Frosting should be thicker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Creating a Frosting Layer from Images</head><p>In this section, we describe our Gaussian Frosting creation method: First, we extract an editable surface with optimal resolution using SuGaR. We then detail how we use this surface-based model to go back to a volumetric but editable representation built around the mesh. This representation adapts to the complexity of the scene and its need for more volumetric effects. Finally, we describe how we parameterize and refine this representation. An overview is provided Figure <ref type="figure" target="#fig_3">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Forward Process: From Volume to Surface</head><p>We start by optimizing an unconstrained Gaussian Splatting representation for a short period of time to let Gaussians position themselves. We will refer to such Gaussians as unconstrained. We save these Gaussians aside, and apply the regularization term from SuGaR to enforce the alignment of the Gaussians with the real surface. We will refer to these Gaussians as regularized.</p><p>Once we obtain the regularized Gaussians, we extract a surface mesh from the Gaussian Splatting representation. This surface mesh serves as a basis for our representation. Like SuGaR <ref type="bibr" target="#b10">[11]</ref>, we then sample points on the visible level set of the Gaussian splatting density function, and apply Poisson reconstruction.</p><p>(a) Using the predefined, large parameter D as in SuGaR <ref type="bibr" target="#b10">[11]</ref> (b) Using our automatically computed D that adapts to the complexity of the 3DGS Fig. <ref type="figure">5</ref>: Comparison of meshes extracted by SuGaR from the Shelly dataset without and with our improvement that automatically tunes the octree depth D in Poisson reconstruction depending on the complexity of the scene. Our technique (bottom) drastically reduces surface artifacts for many scenes, such as the holes and the ellipsoidal bumps on the surface when using the default values from <ref type="bibr" target="#b10">[11]</ref> (top).</p><p>In the supplementary material, we describe our technique to automatically estimate a good value for a critical hyperparameter used by Poisson reconstruction, namely the octree depth D. As we will show in the Experiments section, selecting the right value for D when applying Poisson reconstruction can drastically improve both the quality of the mesh and the rendering performance of our model. Figure <ref type="figure">5</ref> illustrates this point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Backward Process: From Surface to Volume</head><p>After extracting a base mesh, we build a Frosting layer with a variable thickness and containing Gaussians around this mesh. We want this layer to be thicker in areas where more volumetric rendering is necessary near the surface, such as fuzzy material like hair or grass for example. On the contrary, this layer should be very thin near the parts of the scene that corresponds to well-defined flat surfaces, such as wood or plastic for example.</p><p>As illustrated in Figure <ref type="figure" target="#fig_4">6</ref>, to define this layer, we introduce two values Œ¥ in i and Œ¥ out i for each vertex v i of the extracted base mesh M. This gives two surfaces with vertices (v i + Œ¥ in i n i ) i and (v i + Œ¥ out i n i ) i respectively, where n i is the mesh normal at vertex v i . These two surfaces define the inner and outer bounds of the Frosting layer. Note that we do not have to build them explicitly as they directly depend on the base mesh and the Œ¥ in i 's and Œ¥ out i 's. To find good values for the Œ¥ in i 's and Œ¥ out i 's, we initially tried using directly the unconstrained Gaussians, i.e., the Gaussians obtained before applying the regularization term from SuGaR. Unfortunately, without regularization, Gaussian Splatting tends to retrieve a thick layer of Gaussians even for "non-fuzzy" surfaces, which would result in excessively large values for Œ¥ in i and Œ¥ out i . Moreover, the unconstrained Gaussians generally contain many transparent floaters and other outlier Gaussians. Such Gaussians could also bias the shifts toward unnecessarily large values. On the other hand, using only the regularized Gaussians to setup the Œ¥ in i 's and Œ¥ out i 's could miss fuzzy areas since these Gaussians are made flatter by the regularization.</p><p>Our solution is thus to consider both the unconstrained and the regularized Gaussians. More exactly, we estimate the Frosting thickness from the thickness of the unconstrained Gaussians by looking for their isosurfaces, BUT, to make sure we consider the isosurfaces close to the scene surface, we search for the isosurfaces close to the regularized Gaussians: Even under the influence of the regularization term from SuGaR, Gaussians do not align well with the geometry around surfaces with fuzzy details. As a consequence, the local thickness of the regularized Gaussians is a cue on how fuzzy the material is.</p><p>Figure <ref type="figure" target="#fig_4">6</ref> illustrates what we do to fix the Œ¥ in i 's and Œ¥ out i 's. To restrict the search, we define a first interval I i = [-3œÉ i , 3œÉ i ] for each vertex v i , where œÉ i is the standard deviation in the direction of n i of the regularized Gaussian the closest to v i . I i is the confidence interval for the 99.7 confidence level of the 1D Gaussian function of t along the normal. Fuzzy parts result in general in large I i . We could use the I i 's to restrict the search for the isosurfaces of the unconstrained Gaussians. A more reliable search interval J i is obtained by looking for the isosurfaces of the regularized Gaussians along n i within I i :</p><formula xml:id="formula_1">œµ in i = inf(T ) , œµ out i = sup(T ) , with T = {t ‚àà I i | d r (v i + tn i ) ‚â• Œª} ,<label>(2)</label></formula><p>where d r is the density function as defined in Eq. ( <ref type="formula" target="#formula_0">1</ref>) for the regularized Gaussians. In practice, we use an isosurface level Œª = 0.01, i.e., close to zero. We use œµ in i and œµ out i to define interval J i :</p><formula xml:id="formula_2">J i = œµ mid i -kœµ half i , œµ mid i + kœµ half i</formula><p>, with œµ mid i = (œµ in + œµ out )/2 and œµ half i = (œµ out -œµ in )/2. We take k = 3 as it gives an interval large enough to include most of the unconstrained Gaussians while rejecting the outlier Gaussians. Finally, we can compute the inner and outer shifts Œ¥ in i and Fig. <ref type="figure">7</ref>: Rendering complex scenes with Frosting. First row: Renderings, Second row: recovered normal maps, Third row: estimated Frosting thickness. Note that the Frosting is thick on fuzzy materials such as the hair and the grass, as expected, and very thin on flat surfaces such as the table on the fourth column.</p><p>Œ¥ out i as:</p><formula xml:id="formula_3">Œ¥ in i = inf(V ) , Œ¥ out i = sup(V ) , with V = {t ‚àà J i | d u (v i + tn i ) ‚â• Œª} .<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Frosting Optimization</head><p>Once we constructed the outer and inner bounds of the Frosting layer, we initialize a densified set of Gaussians inside this layer and optimize them using 3DGS rendering loss as the unconstrained Gaussians. To make sure the Gaussians stay inside the frosting layer during optimization, we introduce a new parameterization of the Gaussians. Moreover, this parameterization will make possible to easily adjust the Gaussians' parameters when editing the scene.</p><p>Parameterization. Let us consider a triangular face of the base mesh M, with vertices denoted by v 0 , v 1 , and v 2 and their corresponding normals n 0 , n 1 , and n 2 . After extracting inner and outer shifts from unconstrained Gaussians, we obtain six new vertices (v i + Œ¥ in i n i ) i=0,1,2 and (v i + Œ¥ out i n i ) i=0,1,2 that respectively belong to the inner and outer bounds of the frosting. Specifically, these six vertices delimit an irregular triangular prism. We will refer to such polyhedrons as "prismatic cells". We parameterize the 3D mean ¬µ g ‚àà R 3 of a Gaussian g ‚àà G located inside a prismatic cell with a set of six barycentric coordinates split into two subsets (b</p><formula xml:id="formula_4">(i) g ) i=0,1,2 and (Œ≤ (i) g ) i=0,1,2 , such that ¬µ g = 2 i=0 b (i) g v i + Œ¥ out i n i + Œ≤ (i) g v i + Œ¥ in i n i ,<label>(4)</label></formula><p>with barycentric coordinates verifying 2 i=0 (b</p><formula xml:id="formula_5">(i) g + Œ≤ (i) g ) = 1.</formula><p>Using barycentric coordinates enforces Gaussians to stay inside their corresponding prismatic cell, and guarantees the stability of our representation during optimization. In practice, we apply a softmax activation on the parameters to optimize to obtain barycentric coordinates that sum up to 1.</p><p>Initialization. For a given budget N of Gaussians provided by the user, we initialize N Gaussians in the scene by sampling N 3D centers ¬µ g in the frosting layer. Specifically, for sampling a single Gaussian, we first randomly select a prismatic cell with a probability proportional to its volume. Then, we sample random coordinates that sum up to 1. This sampling allows for allocating more Gaussians in areas with fuzzy and complex geometry, where more volumetric rendering is needed. However, flat parts in the layer may also need a large number of Gaussians to recover texture details. Therefore, in practice, we instantiate N/2 Gaussians with uniform probabilities in the prismatic cells, and N/2 Gaussians with probabilities proportional to the volume of the cell.</p><p>We initialize the colors of the Gaussians with the color of the closest Gaussian in the unconstrained representation. However, we do not use the unconstrained Gaussians to initialize opacity, rotation, and scaling factors, as in practice, following the strategy from 3DGS <ref type="bibr" target="#b16">[17]</ref> for these parameters provides better performance: We suppose the positions and configuration of the Gaussians inside the Frosting layer are already a good initialization, and resetting opacities, scaling factors and rotations helps Gaussians to take a fresh start, avoiding a potential local minimum encountered by previous unconstrained Gaussians.</p><p>Our representation allows for a much better control over the number of Gaussians than the original Gaussian Splatting densification process, as it is up to the user to decide on a number of Gaussians to instantiate in the frosting layer. These Gaussians will be spread in the entire frosting in a very efficient way, adapting to the need for volumetric rendering in the entire scene.</p><p>Optimizing the Gaussian Frosting. We reload the unconstrained Gaussians and apply our method for computing the inner and outer bounds of the Frosting. Then, for a given budget of N Gaussians, we initialize N Gaussians in the Frosting and optimize the representation while keeping the number of Gaussians constant. Note that compared to Vanilla 3DGS, this allows to control precisely the number of Gaussians.</p><p>Editing, Deforming, and Animating the Frosting. When deforming the base mesh, the positions of Gaussians automatically adjust in the frosting layer thanks to the use of the barycentric coordinates. To automatically adjust the rotation and scaling factors of the Gaussians, we propose a strategy different from the surface-based adjustment from SuGaR: In a given prismatic cell with center c and vertices v i for 0 ‚â§ i &lt; 5, we first estimate the local transformation at each vertex v i by computing the rotation and rescaling of the vector (cv i ). Then, we use the barycentric coordinates of a Gaussian g to compute an average transformation at point ¬µ g from the transformation of all 6 vertices, and we adjust the rotation and scaling factors of g by applying this average transformation. Please note that the spherical harmonics are also adjusted in practice, to ensure the consistency of the emitted radiance depending on the averaged rotation applied to the Gaussian. We provide more details about this automatic adjustment of Gaussian parameters in the supplementary material. 5 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Implementation Details</head><p>We implemented our method with PyTorch <ref type="bibr" target="#b25">[26]</ref> and optimized the representations on a single GPU Nvidia Tesla V100 SXM2 32 Go. Optimizing a full, editable Frosting model takes between 45 and 90 minutes on a single GPU, depending on the complexity of the scene. This optimization is much faster than the most similar approach to Frosting in the literature, namely Adaptive Shells <ref type="bibr" target="#b39">[40]</ref>, that requires 8 hours on a single GPU for a synthetic scene, and 1.7 times more iterations for a real scene.</p><p>Extracting the surface mesh. When reconstructing real scenes, we follow the approach from vanilla 3DGS <ref type="bibr" target="#b16">[17]</ref> and first use COLMAP to estimate the camera poses and extract a point cloud for initialization. For synthetic scenes with known camera poses, we just use a random point cloud for initialization. Then, we optimize an unconstrained Gaussian Splatting representation for 7,000 iterations. We save these Gaussians aside and apply the regularization term from SuGaR until iteration 15,000. We finally compute an optimal depth parameter D with Œ≥ = 100 and extract a mesh from the regularized Gaussians by applying Poisson surface reconstruction as described in <ref type="bibr" target="#b10">[11]</ref>.</p><p>Optimizing the Gaussian Frosting. Given a budget of N Gaussians, we initialize N Gaussians in the Frosting layer and optimize them for 15,000 additional iterations, which gives a total of 30,000 iterations, similarly to 3DGS <ref type="bibr" target="#b16">[17]</ref>. Vanilla 3DGS optimization generally produces between 1 and 5 million Gaussians. In practice, we use N =5 million for real scenes and N =2 million for synthetic scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Real-Time Rendering in Complex Scenes</head><p>To evaluate the quality of Frosting's rendering, we compute the standard metrics PSNR, SSIM and LPIPS <ref type="bibr" target="#b47">[48]</ref> and compare to several baselines, some of them focusing only on Novel View Synthesis <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b44">45]</ref> and others relying on an editable representation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b42">43]</ref>, just like Frosting. We compute metrics on several challenging datasets containing synthetic and real scenes.</p><p>Shelly. We first compare Frosting to state-of-the-art methods on the dataset Shelly introduced in Adaptive Shells <ref type="bibr" target="#b39">[40]</ref>. Shelly includes six synthetic scenes with challenging fuzzy materials that surface-based approaches struggle to reconstruct accurately. As we show in Table <ref type="table">1</ref> and Figure <ref type="figure" target="#fig_5">8</ref>, Frosting outperforms every other methods for all three metrics. Frosting even outperforms with a wide margin vanilla Gaussian Splatting, which is free from any surface constraints and only focuses on optimizing the rendering quality. Indeed, the sampling of Gaussians inside the Frosting layer provides a much more efficient densification of Gaussians than the strategy proposed in 3DGS <ref type="bibr" target="#b16">[17]</ref>, targeting the challenging fuzzy areas close to the surface and allocating more Gaussians where volumetric rendering is needed.</p><p>NeRFSynthetic. Table <ref type="table">1</ref> provides a comparison on the NeRFSynthetic dataset <ref type="bibr" target="#b22">[23]</ref>, which consists in eight synthetic scenes. Frosting performs the best among the editable methods, surpassing SuGaR <ref type="bibr" target="#b10">[11]</ref>, and achieves results on par with vanilla 3DGS and other radiance field methods.</p><p>Table <ref type="table">1</ref>: Quantitative evaluation of rendering quality on the synthetic datasets Shelly <ref type="bibr" target="#b39">[40]</ref> and NeRFSynthetic <ref type="bibr" target="#b22">[23]</ref>. Frosting is the best among all methods, outperforming even non-editable models that only focus on rendering. Contrary to an unconstrained 3D Gaussian Splatting <ref type="bibr" target="#b16">[17]</ref>, our representation allows for densifying Gaussians more efficiently by targeting challenging and fuzzy areas.</p><formula xml:id="formula_6">Shelly NeRFSynthetic PSNR ‚Üë SSIM ‚Üë LPIPS ‚Üì PSNR ‚Üë SSIM ‚Üë LPIPS ‚Üì NeRF [</formula><p>23] 31.27 0.893 0.157 31.01 0.947 0.081 NeuS [39] 29.98 0.893 0.158 ---Mip-NeRF [1] 32.59 0.899 0.148 33.09 0.961 0.043 I-NGP [24] 33.22 0.922 0.125 33.18 --3DGS [17] 37.66 0.958 0.066 33.32 0.970 0.030 MobileNeRF [7] 31.62 0.911 0.129 30.90 0.947 0.062 Adaptive Shells [40] 36.02 0.954 0.079 31.84 0.957 0.056 SuGaR [11] 36.33 0.954 0.059 32.40 0.964 0.033 Frosting (Ours) 39.84 0.977 0.033 33.03 0.967 0.029</p><p>Mip-NeRF 360. We also compare Frosting to state-of-the-art approaches on the real scenes from the Mip-NeRF 360 dataset <ref type="bibr" target="#b1">[2]</ref>. This dataset contains images from seven challenging real scenes, but was captured with ideal lighting condition and provides really good camera calibration data and initial SfM points.</p><p>Results are available in Table <ref type="table" target="#tab_1">2</ref> and Figure <ref type="figure">7</ref>. Frosting reaches the best performance among all editable methods, and obtains worse but competitive results compared to vanilla Gaussian Splatting. When Gaussian Splatting is given a very good initialization with a large amount of SfM points, the benefits from the Gaussian Frosting densification are not as effective, and optimizing Gaussians without additional constraints as in 3DGS slightly improves performance.</p><p>Additional real scenes. We finally compare Frosting to the baselines with captures of real scenes that present variations in exposure or white balance.</p><p>To this end, we follow the approach from 3DGS <ref type="bibr" target="#b16">[17]</ref> and select the same two subsets of two scenes from Tanks&amp;Temples (Truck and Train) and Deep Blending (Playroom and Dr. Johnson). We also evaluate a few methods on a custom dataset that consists of four casual captures made with a smartphone (we call these scenes SleepyCat, Buzz, RedPanda, and Knight), illustrated in Figures <ref type="figure" target="#fig_1">2</ref> and <ref type="figure">7</ref>. Results are available in Table <ref type="table" target="#tab_2">3</ref>. In these more realistic scenarios, Frosting achieves once again similar or better performance than unconstrained Gaussian Splatting even though it is an editable representation that relies on a single, animatable mesh.   We were able to animate the sculpture in the left image using the rigging tool in Blender.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Editing, Compositing, and Animating Gaussian Frosting</head><p>As shown in Figure <ref type="figure">1</ref>, Figure <ref type="figure" target="#fig_2">3</ref> and Figure <ref type="figure" target="#fig_6">9</ref>, our Frosting representation automatically adapts when editing, rescaling, deforming, combining or animating base meshes. Frosting offers editing, composition and animation capabilities similar to surface-based approaches like SuGaR <ref type="bibr" target="#b10">[11]</ref>, but achieves much better performance thanks to its frosting layer with variable thickness that adapts to the volumetric effects and fuzzy materials in the scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Study: Octree Depth</head><p>To demonstrate how our technique for automatically computing the optimal octree depth D for Poisson reconstruction improves the performance of Frosting, we provide in Table <ref type="table">4</ref> a comparison in rendering performance between our full model, and a version of Frosting that uses the same predefined depth parameter as SuGaR. This technique results in equivalent or better rendering performance with much fewer triangles.</p><p>Table <ref type="table">4</ref>: Ablation for two different depth computation methods used for the octree in Poisson surface reconstruction. We compare the rendering performance between using a predefined depth with a high value as in <ref type="bibr" target="#b10">[11]</ref> and our automatically computed depth. Our technique systematically selects an optimal depth depending on the complexity of the scene, avoiding artifacts in the mesh, and resulting in equivalent or better rendering performance with a much smaller average number of triangles.</p><p>NeRFSynthetic Shelly We compare the rendering performance (PSNR ‚Üë) in synthetic and real scenes depending on how we compute and refine the thickness of the Frosting layer. Specifically, we first show that using an adaptive thickness improves performance over a constant thickness. Even though using a large constant thickness improves performance in scenes with very fuzzy materials like Shelly <ref type="bibr" target="#b39">[40]</ref>, this lowers performance in scenes with flat surfaces and generates artifacts when editing the scene, as shown in Figure <ref type="figure">10</ref>. By contrast, our method adapts automatically to the type of surfaces.</p><formula xml:id="formula_7">PSNR ‚Üë SSIM ‚Üë LPIPS ‚Üì Ntri ‚Üì ‚Üë SSIM ‚Üë LPIPS ‚Üì Ntri ‚Üì Predefined depth D =</formula><p>We also demonstrate that refining the thickness using the unconstrained Gaussians is necessary to achieve top performance.</p><p>Shelly Mip-NeRF 360 Average Indoor Outdoor Average Constant thickness (Small) 39.03 30.36 25.50 28.28 Constant thickness (Medium) 39.67 30.19 25.54 28.20 Constant thickness (Large) 40.00 30.06 25.48 28.10 Using Regularized Gaussians only (Œ¥ in/out = œµ in/out ) 39.34 30.42 25.57 28.34 Using Regularized and Unconstrained Gaussians (Full method) 39.84 30.49 25.57 28.38</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Ablation Study: Thickness of the Frosting Layer</head><p>We also provide in Table <ref type="table" target="#tab_3">5</ref> an ablation study comparing different strategies for computing the thickness of the Frosting layer. Specifically, we first evaluate the rendering performance of a Frosting layer with constant thickness. We repeat the experiment for small, medium and large thickness values, using different quantiles of our inner and outer shifts Œ¥ in and Œ¥ out for computing the constant thickness. We show that using an adaptive thickness improves performance over a constant thickness, as (a) some fuzzy materials need a thicker frosting to be accurately rendered, and (b) some flat surfaces are better rendered with a very thin frosting. As a consequence, even though using a large constant thickness improves performance in scenes with very fuzzy materials like Shelly <ref type="bibr" target="#b39">[40]</ref>, it lowers performance in scenes with flat surfaces. Moreover, using an adaptive thickness rather than a constant thickness with a large value helps to greatly reduce artifacts, as we demonstrate in Figure <ref type="figure">10</ref>.</p><p>We also show that using unconstrained Gaussians to refine the thickness of the Frosting is necessary to achieve top performance. To this end, we skip the Fig. <ref type="figure">10</ref>: Comparison with a constant thickness. Our strategy to compute an adaptive thickness for Frosting is essential to maintain optimal performance while avoiding artifacts when editing the scene. As shown in the right image, using a constant thickness may produce artifacts when animating a character: When using a constant, large thickness in this scene, Gaussians located near the right knee of the knight participate in reconstructing the right hand, which produces artifacts when moving the hand.</p><p>refinement process and evaluate the rendering performance of a Frosting layer with Œ¥ in = œµ in and Œ¥ out = œµ out . This results in lower performance, as shown in Table <ref type="table" target="#tab_3">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We proposed a simple yet powerful surface representation with many advantages over current representations together with a method to extract it from images. One limitation of our implementation is the simple deformation model as it is piecewise linear. It should be however simple to replace it with a more sophisticated, physics-based deformation model. Another limitation is that our models are larger than vanilla Gaussian Splatting models since we have to include the barycentric coordinates and the mesh vertices. Recent works about compressing 3DGS could help. We believe that the Frosting representation can be useful beyond image-based rendering. It could for example be used in more general Computer Graphics applications to render complex materials in real-time. a large but very detailed shape can be reconstructed using Gaussians with large size, if these Gaussians are close to each other. On the contrary, whatever their size, if the centers of the Gaussian are too far from each other, then the rendered geometry will look rough.</p><p>Consequently, to first evaluate the geometrical complexity of a scene, we propose to compute, for each Gaussian g in the scene, the distance between g and its nearest neighbor Gaussian. We use these distances to define the following geometrical complexity score CS:</p><formula xml:id="formula_8">CS = Q 0.1 min g ‚Ä≤ Ã∏ =g ‚à•¬µ g -¬µ g ‚Ä≤ ‚à• 2 L g‚ààG ,<label>(5)</label></formula><p>where G is the set of all 3D Gaussians in the scene, L is the length of the longest edge of the bounding box of the point cloud to use in Poisson reconstruction, and Q 0.1 is the function that returns the 0.1-quantile of a list. We use the 0.1-quantile rather than the average because Gaussians that have a neighbor close to them generally encode details in the scene, which provide a much more reliable and less noisy criterion than using the overall average. We also use a quantile rather than a minimum to be robust to extreme values. In short, this complexity score CS is a canonical distance between the closest Gaussians in the scene, i.e., the distance between neighbor Gaussians that reconstruct details in the scene. Since the normalized length of a cell in the octree is 2 -D and this score represents a canonical normalized distance between Gaussians representing details in the scene, we can compute a natural optimal depth D for the Poisson reconstruction algorithm:</p><formula xml:id="formula_9">D = ‚åä-log 2 (Œ≥ √ó CS)‚åã ,<label>(6)</label></formula><p>where Œ≥ &gt; 0 is a hyperparameter that does not depend on the scene and its geometrical complexity. This formula guarantees that the size of the cells is as close as possible but greater than Œ≥ √ó CS. Decreasing the value of Œ≥ increases the resolution of the reconstruction. But for a given Œ≥, whatever the dataset or the complexity of the scene, this formula enforces the scene to be reconstructed with a similar level of smoothness. Choosing Œ≥ is therefore much easier than having to tune D as it is not dependent on the scene. In practice, we use Œ≥ = 100 for all the scenes. Our experiments validates that this method to fix D results in greater rendering performance.</p><p>8 Initializing the frosting layer 8.1 Sampling Gaussians in the frosting layer Sampling more Gaussians in thicker parts of the frosting. For a given budget N of Gaussians provided by the user, we initialize N Gaussians in the scene by sampling N 3D centers ¬µ g in the frosting layer. Specifically, for sampling a single Gaussian, we first randomly select a prismatic cell with a probability proportional to its volume. Then, we sample random coordinates that sum up to 1. This sampling allows for allocating more Gaussians in areas with fuzzy and complex geometry, where more volumetric rendering is needed. However, flat parts in the layer may also need a large number of Gaussians to recover texture details. Therefore, in practice, we instantiate N/2 Gaussians with uniform probabilities in the prismatic cells, and N/2 Gaussians with probabilities proportional to the volume of the cell.</p><p>Contracting volumes in unbounded scenes. In real unbounded scenes, 3D Gaussians located far away from the center of the scene can have a significantly large volume despite their limited participation in the final rendering. This can lead to an unnecessarily large number of Gaussians being sampled in the frosting layer far away from the training camera poses. To address this issue, we propose distributing distant Gaussians proportionally to disparity (inverse distance) rather than distance.</p><p>When sampling Gaussians in practice, we start by contracting the volumes of the prismatic cells. We achieve this by applying a continuous transformation f : R 3 ‚Üí R 3 to the vertices of the outer and inner bounds of the frosting layer. Then, we compute the volumes of the resulting "contracted" prismatic cells and use these adjusted volumes for sampling Gaussians within the frosting layer, as previously described. The transformation function f aims to contract the volume of prismatic cells located far away from the center of the scene. We define f using a formula similar to the contraction transformation introduced in Mip-NeRF 360 <ref type="bibr" target="#b1">[2]</ref>:</p><formula xml:id="formula_10">f (x) = x if ‚à•x -c‚à• ‚â§ l c + l √ó 2 -l ‚à•x-c‚à• x-c ‚à•x-c‚à• if ‚à•x -c‚à• &gt; l ,<label>(7)</label></formula><p>where c ‚àà R 3 is the center of the bounding box containing all training camera positions, and l ‚àà R + is equal to half the length of the diagional of the same bounding box. We choose the bounding box of the camera positions as our reference scale because both 3D Gaussian Splatting <ref type="bibr" target="#b16">[17]</ref> and SuGaR <ref type="bibr" target="#b10">[11]</ref> use this same reference for scaling learning rates and distinguishing foreground from background in unbounded scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Avoiding self-intersections in the frosting layer</head><p>In the main paper, we define the inner and outer bounds of the frosting layer by adding inner and outer shifts Œ¥ in i and Œ¥ out i to the vertices v i of the base mesh. This results in two bounding surfaces with vertices v i +Œ¥ in i and v i +Œ¥ out i . In practice, we wish to minimize self-intersections within the frosting layer, specifically avoiding prismatic cells intersecting with each other.</p><p>While self-intersections do not directly impact rendering quality, they can lead to artifacts during scene editing or animation. Consider the scenario where different cells intersect. In such cases, moving a specific triangle of the base mesh may not affect all Gaussians intersecting the surrounding cell: Some Gaussians may belong to prismatic cells associated with different triangles, resulting in artifacts due to their failure to follow local motion or scene edits.</p><p>To mitigate self-intersections, we adopt an indirect approach for initializing the shifts Œ¥ in and Œ¥ out . Instead of using the final computed values directly, we start with shifts equal to zero and progressively increase them until reaching their final values. As soon as an inner vertex (or outer vertex) of a prismatic cell is detected to intersect another cell, we stop further increases in its inner shift (or outer shift). This straightforward process significantly reduces self-intersections in the frosting layer while maintaining rendering performance.</p><p>By following this approach, we ensure that the frosting layer remains free from unwanted artifacts while preserving efficient rendering capabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Adjusting Gaussians' parameters for edition</head><p>When editing or animating the scene, we automatically adjust Gaussians' parameters. Specifically, in a given prismatic cell with center c and six vertices v i for 0 ‚â§ i &lt; 6, we first estimate the local transformation at each vertex v i by computing the rotation and rescaling of the vector (c -v i ).</p><p>To compute the local rotations at vertex v i , we use an axis-angle representation where the axis angle is the normalized cross-product between the previous and the current values of the vector (c -v i ). The local rescaling transformation at vertex v i is computed as the transformation that scales along axis (c -v i ) with the appropriate factor but leaves other axes unchanged.</p><p>To update the scaling factors and rotation of a Gaussian g, we first apply each of these six transformations on the three main axes of the Gaussian. We then average the resulting axes using the barycentric coordinates of the center ¬µ g of the Gaussian. We finally orthonormalize the three resulting axes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc>arXiv:2403.14554v1 [cs.CV] 21 Mar 2024</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Visualization of the thickness of the Frosting layer on an example.A thicker layer is shown with a brighter value. Our method automatically builds a thick Frosting layer for fuzzy areas such as the fur of the red panda plush, and a thin Frosting layer for flat surfaces such as the table or the floor. Adapting the thickness of the Frosting layer allows for allocating more Gaussians in areas where more volumetric rendering is necessary near the surface, resulting in an efficient distribution of Gaussians in the scene. As we demonstrate in the paper, using an adaptive thickness results in higher performance than using a predefined constant thickness, and reduces artifacts when animating the mesh.</figDesc><graphic coords="2,34.02,34.02,167.75,94.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Fig.3: Scene composition. Using mesh editing tools in Blender, we were able to combine various elements from multiple scenes (a) to build a whole new scene (c). We also changed the pose of the characters by using the rigging tool in Blender (b). Similarly to surface-based methods like SuGaR<ref type="bibr" target="#b10">[11]</ref>, Frosting can be used for editing and compositing scenes, but allows for better rendering of complex volumetric effects and fuzzy materials, such as hair or grass.</figDesc><graphic coords="4,85.89,119.84,145.24,81.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Fig.4: Creating a Layer of Gaussian Frosting. To build our proposed Frosting representation, we start by optimizing a Gaussian Splatting representation using a rendering loss without any additional constraint, to let Gaussians position themselves. We refer to these Gaussians as unconstrained. We then regularize these Gaussians to enforce their alignement with the surface, and extract a mesh that will serve as a basis for the Frosting. Next, we use the misalignment of surface-aligned Gaussians to identify areas where more volumetric rendering is needed, and we build search intervals Ji around the mesh's vertices vi. Finally, we use the density function of the unconstrained Gaussians to refine the intervals, resulting in a Frosting layer. We finally sample a novel, densified set of Gaussians inside the layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: How we define the inner and outer bounds of the Frosting layer. See text in Section 4.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Close-up views of fuzzy materials from the Shelly dataset [40] reconstructed with vanilla Gaussian Splatting [17] (center) and Frosting (right).</figDesc><graphic coords="12,34.02,144.79,107.20,54.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 :</head><label>9</label><figDesc>Fig.9: Examples of animation with Frosting. We were able to animate the sculpture in the left image using the rigging tool in Blender.</figDesc><graphic coords="15,2.43,195.78,178.26,100.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><figDesc>Original pose(b) Adaptive thickness (ours) (c) Constant thickness (baseline)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Quantitative evaluation of rendering quality on the Mip-NeRF 360 dataset<ref type="bibr" target="#b1">[2]</ref>. Frosting is best among the methods that recover an editable Radiance Field with explicit meshes, and achieves performance comparable to NeRF methods and vanilla 3D Gaussian Splatting.</figDesc><table><row><cell></cell><cell cols="3">Indoor scenes</cell><cell cols="3">Outdoor scenes</cell><cell cols="3">Average on all scenes</cell></row><row><cell cols="3">No mesh (except Frosting)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Plenoxels [45]</cell><cell>24.83</cell><cell>0.766</cell><cell>0.426</cell><cell>22.02</cell><cell>0.542</cell><cell>0.465</cell><cell>23.62</cell><cell>0.670</cell><cell>0.443</cell></row><row><cell>INGP-Base [24]</cell><cell>28.65</cell><cell>0.840</cell><cell>0.281</cell><cell>23.47</cell><cell>0.571</cell><cell>0.416</cell><cell>26.43</cell><cell>0.725</cell><cell>0.339</cell></row><row><cell>INGP-Big [24]</cell><cell>29.14</cell><cell>0.863</cell><cell>0.242</cell><cell>23.57</cell><cell>0.602</cell><cell>0.375</cell><cell>26.75</cell><cell>0.751</cell><cell>0.299</cell></row><row><cell>Mip-NeRF 360 [2]</cell><cell cols="2">31.58 0.914</cell><cell>0.182</cell><cell>25.79</cell><cell>0.746</cell><cell>0.247</cell><cell cols="2">29.09 0.842</cell><cell>0.210</cell></row><row><cell>3DGS [17]</cell><cell>30.41</cell><cell>0.920</cell><cell>0.189</cell><cell cols="3">26.40 0.805 0.173</cell><cell cols="3">28.69 0.870 0.182</cell></row><row><cell>Frosting (Ours)</cell><cell cols="2">30.49 0.925</cell><cell>0.190</cell><cell>25.57</cell><cell>0.765</cell><cell>0.225</cell><cell>28.38</cell><cell>0.856</cell><cell>0.205</cell></row><row><cell>With mesh</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MobileNeRF [7]</cell><cell>25.74</cell><cell>0.757</cell><cell>0.453</cell><cell>22.90</cell><cell>0.524</cell><cell>0.463</cell><cell>24.52</cell><cell>0.657</cell><cell>0.457</cell></row><row><cell>NeRFMeshing [27]</cell><cell>23.83</cell><cell>-</cell><cell>-</cell><cell>22.23</cell><cell>-</cell><cell>-</cell><cell>23.15</cell><cell>-</cell><cell>-</cell></row><row><cell>BakedSDF [43]</cell><cell>27.20</cell><cell>0.845</cell><cell>0.300</cell><cell>23.40</cell><cell>0.577</cell><cell>0.351</cell><cell>25.57</cell><cell>0.730</cell><cell>0.321</cell></row><row><cell>B.O. Grids [28]</cell><cell>27.71</cell><cell>0.873</cell><cell>0.227</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Adaptive Shells [40] 29.19</cell><cell>0.872</cell><cell>0.285</cell><cell>23.17</cell><cell>0.606</cell><cell>0.389</cell><cell>26.61</cell><cell>0.758</cell><cell>0.330</cell></row><row><cell>SuGaR [11]</cell><cell>29.43</cell><cell>0.910</cell><cell>0.216</cell><cell>24.40</cell><cell>0.699</cell><cell>0.301</cell><cell>27.27</cell><cell>0.820</cell><cell>0.253</cell></row><row><cell>Frosting (Ours)</cell><cell cols="3">30.49 0.925 0.190</cell><cell cols="3">25.57 0.765 0.225</cell><cell cols="3">28.38 0.856 0.205</cell></row></table><note><p>PSNR ‚Üë SSIM ‚Üë LPIPS ‚Üì PSNR ‚Üë SSIM ‚Üë LPIPS ‚Üì PSNR ‚Üë SSIM ‚Üë LPIPS ‚Üì</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Quantitative evaluation of rendering quality on real scenes from Tanks&amp;Temples<ref type="bibr" target="#b18">[19]</ref>, Deep Blending<ref type="bibr" target="#b11">[12]</ref> and our custom dataset. Our representation performs the best among the surface-based methods, and achieves similar or better performance than unconstrained 3DGS and other non-editable methods.</figDesc><table><row><cell>Plenoxels [45]</cell><cell>21.07</cell><cell>0.719</cell><cell>0.379</cell><cell>23.06</cell><cell>0.794</cell><cell>0.510</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>INGP-Base [24]</cell><cell>21.72</cell><cell>0.723</cell><cell>0.330</cell><cell>23.62</cell><cell>0.796</cell><cell>0.423</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>INGP-Big [24]</cell><cell>21.92</cell><cell>0.744</cell><cell>0.304</cell><cell>24.96</cell><cell>0.817</cell><cell>0.390</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Mip-NeRF 360 [2] 22.22</cell><cell>0.758</cell><cell>0.257</cell><cell>29.40</cell><cell>0.901</cell><cell>0.244</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>3DGS [17]</cell><cell>23.14</cell><cell>0.841</cell><cell>0.183</cell><cell>29.41</cell><cell>0.903</cell><cell>0.243</cell><cell>34.17</cell><cell>0.944</cell><cell>0.165</cell></row><row><cell>SuGaR [11]</cell><cell>21.58</cell><cell>0.795</cell><cell>0.219</cell><cell>29.41</cell><cell>0.893</cell><cell>0.267</cell><cell>32.05</cell><cell>0.930</cell><cell>0.180</cell></row><row><cell>Frosting (Ours)</cell><cell>23.13</cell><cell>0.836</cell><cell>0.174</cell><cell>29.62</cell><cell>0.900</cell><cell>0.236</cell><cell>33.82</cell><cell>0.945</cell><cell>0.149</cell></row></table><note><p>Tanks&amp;Temples<ref type="bibr" target="#b18">[19]</ref> </p><p>Deep Blending<ref type="bibr" target="#b11">[12]</ref> </p><p>Custom dataset</p><p>PSNR ‚Üë SSIM ‚Üë LPIPS ‚Üì PSNR ‚Üë SSIM ‚Üë LPIPS ‚Üì PSNR ‚Üë SSIM ‚Üë LPIPS ‚Üì</p><p>(</p><p>a) Original pose Edited pose (c) Edited pose</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Comparing different strategies for computing the thickness of the Frosting layer.</figDesc><table><row><cell></cell><cell>10 [11] 31.63</cell><cell>0.959</cell><cell>0.041 &gt;1 M 39.85 0.975</cell><cell>0.035 K</cell></row><row><cell>Depth D ‚â§ 10</cell><cell cols="4">33.03 0.967 0.029 863 K 39.84 0.977 0.033 203 K</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>In this supplementary material, we provide the following elements:</p><p>-A description of our method to improve the surface reconstruction from SuGaR <ref type="bibr" target="#b10">[11]</ref>.</p><p>-Additional details about our strategy to initialize the Frosting layer and automatically adjust Gaussians' parameters when deforming, editing, or animating our representation.</p><p>We also provide a video that offers an overview of the approach and showcases additional qualitative results. Specifically, the video demonstrates how Frosting can be used to edit, combine or animate Gaussian Splatting representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Improving surface reconstruction</head><p>We improve the surface reconstruction method from SuGaR by proposing a way to automatically adjust the hyperparameter of the Poisson surface reconstruction <ref type="bibr" target="#b15">[16]</ref> stage.</p><p>Poisson surface reconstruction first recovers an underlying occupancy field œá : R 3 ‚Üí [0, 1] and applies a marching algorithm on œá, which allows for a much better mesh reconstruction than the density function. This approach allows for high scalability as the marching algorithm is applied only in voxels located close to the point cloud.</p><p>To estimate œá, Poisson surface reconstruction discretizes the scene into 2 D √ó 2 D √ó2 D cells by adapting an octree with depth D ‚àà N to the input samples. D is a hyperparameter provided by the user: The higher D, the higher the resolution of the mesh.</p><p>By default, SuGaR <ref type="bibr" target="#b10">[11]</ref> uses a large depth D = 10 for any scene, as it guarantees a high level of details. However, if the resolution is too high with respect to the complexity of the geometry and the size of the details in the scene, the shapes of the Gaussians become visible as ellipsoidal bumps on the surface of the mesh, and create incorrect bumps or self-intersections. More importantly, holes can also appear in the geometry when D is too large with regards to the density of the Gaussians and the sampled point cloud.</p><p>We therefore introduce a method to automatically select D. A simple strategy would be to adjust the depth of the octree such that the size of a cell is approximately equal or larger than the average size of the Gaussians in the scene, normalized by the spatial extent of the point cloud used for reconstruction. Unfortunately, this does not work well in practice: We found that whatever the scene (real or synthetic) or the number of Gaussians to represent it, Gaussian Splatting optimization systematically converges toward a varied collection of Gaussian sizes, so that there is no noticeable difference or pattern in the distribution of sizes between scenes.</p><p>We noticed that the distance between Gaussians is much more representative of the geometrical complexity of the scene and thus a reliable cue to fix D. Indeed,</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Martin-Brualla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<title level="m">Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>International Conference on Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Verbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">NeRD: Neural Reflectance Decomposition from Image Collections</title>
		<author>
			<persName><forename type="first">M</forename><surname>Boss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P A</forename><surname>Lensch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Unstructured Lumigraph Rendering</title>
		<author>
			<persName><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mcmillan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gortler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cohen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>ACM SIGGRAPH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">TensoRF: Tensorial Radiance Fields</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<idno>arXiv Preprint</idno>
		<title level="m">GaussianEditor: Swift and Controllable 3D Editing with Gaussian Splatting</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">MobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient Neural Field Rendering on Mobile Architectures</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tagliasacchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">NeuMesh: Learning Disentangled Neural Mesh-Based Implicit Field for Geometry and Texture Editing</title>
		<author>
			<persName><forename type="first">Chong</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bangbang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Junyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hujun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yinda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhaopeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guofeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improving Neural Implicit Surfaces Geometry with Patch Warping</title>
		<author>
			<persName><forename type="first">F</forename><surname>Darmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bascle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Devaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Monasse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aubry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-View Stereo for Community Photo Collections</title>
		<author>
			<persName><forename type="first">M</forename><surname>Goesele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">SuGaR: Surface-Aligned Gaussian Splatting for Efficient 3D Mesh Reconstruction and High-Quality Mesh Rendering</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gu√©don</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.12775</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deep Blending for Free-Viewpoint Image-Based Rendering</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Frahm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Drettakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>ACM SIGGRAPH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Baking Neural Radiance Fields for Real-Time View Synthesis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Point&apos;n Move: Interactive Scene Object Manipulation on Gaussian Splatting Radiance Fields</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<idno>arXiv Preprint</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">ReLU Fields: The Little Non-Linearity That Could</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karnewar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ritschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mitra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>ACM SIGGRAPH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Poisson Surface Reconstruction</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Kazhdan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bolitho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hoppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eurographics</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">3D Gaussian Splatting for Real-Time Radiance Field Rendering</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kerbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kopanas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leimk√ºhler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Drettakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>ACM SIGGRAPH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kerr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<idno>arXiv Preprint</idno>
		<title level="m">GARField: Group Anything with Radiance Fields</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Tanks and Temples: Benchmarking Large-Scale Scene Reconstruction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Knapitsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>ACM SIGGRAPH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Point-Based Neural Rendering with Per-View Optimization</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kopanas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leimk√ºhler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Drettakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">NeROIC: Neural Rendering of Objects from Online Image Collections</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Olszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Achlioptas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tulyakov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>ACM SIGGRAPH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neuralangelo: High-Fidelity Neural Surface Reconstruction</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Unberath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
		<title level="m">NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis. European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Instant Neural Graphics Primitives with a Multiresolution Hash Encoding</title>
		<author>
			<persName><forename type="first">T</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schied</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Keller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>ACM SIGGRAPH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">UNISURF: Unifying Neural Implicit Surfaces and Radiance Fields for Multi-View Reconstruction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Oechsle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">PyTorch: An Imperative Style, High-Performance Deep Learning Library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Rakotosaona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Arroyo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<title level="m">NeRFMeshing: Distilling Neural Radiance Fields into Geometrically-Accurate 3D Meshes</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>International Conference on 3D Vision</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Binary Opacity Grids: Capturing Fine Geometric Detail for Mesh-Based View Synthesis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Reiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Garbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Verbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">KiloNeRF: Speeding Up Neural Radiance Fields with Thousands of Tiny MLPs</title>
		<author>
			<persName><forename type="first">C</forename><surname>Reiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Free View Synthesis</title>
		<author>
			<persName><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Stable View Synthesis</title>
		<author>
			<persName><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">ADOP: Approximate Differentiable One-Pixel Point Rendering</title>
		<author>
			<persName><forename type="first">D</forename><surname>R√ºckert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stamminger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>ACM SIGGRAPH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Structure-from-Motion Revisited</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Sch√∂nberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pixelwise View Selection for Unstructured Multi-View Stereo</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Sch√∂nberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Photo Tourism: Exploring Photo Collections in 3D</title>
		<author>
			<persName><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>ACM SIGGRAPH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">NeRV: Neural Reflectance and Visibility Fields for Relighting and View Synthesis</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Direct Voxel Grid Optimization: Super-Fast Convergence for Radiance Fields Reconstruction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance Fields</title>
		<author>
			<persName><forename type="first">D</forename><surname>Verbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zickler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-View Reconstruction</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Komura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Adaptive Shells for Efficient Neural Radiance Field Rendering</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nimier-David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gojcic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>ACM SIGGRAPH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Surface Light Fields for 3D Photography</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>Azuma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Aldinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Duchamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Salesin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Stuetzle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>ACM SIGGRAPH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Volume Rendering of Neural Implicit Surfaces</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yariv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kasten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">BakedSDF: Meshing Neural SDFs for Real-Time View Synthesis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yariv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Reiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Verbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>ACM SIGGRAPH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ke</surname></persName>
		</author>
		<idno>arXiv Preprint</idno>
		<title level="m">Gaussian Grouping: Segment and Edit Anything in 3D Scenes</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Plenoxels: Radiance Fields Without Neural Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fridovich-Keil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">PlenOctrees For Real-Time Rendering of Neural Radiance Fields</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">PhySG: Inverse Rendering with Spherical Gaussians for Physics-Based Material Editing and Relighting</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The Unreasonable Effectiveness of Deep Features as a Perceptual Metric</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
