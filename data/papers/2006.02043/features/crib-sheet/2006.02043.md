- **Hierarchical Forecasting (HF)**: A method for generating coherent forecasts across different aggregation levels in a hierarchy (e.g., SKU demand at store, regional, and national levels).

- **Coherence in Forecasting**: The property that forecasts at lower levels sum to the forecasts at higher levels, ensuring alignment in decision-making.

- **Traditional Approaches**:
  - **Bottom-Up (BU)**: Forecasts generated at the lowest level and aggregated upwards. 
    - Formula: \( G = [0_{m_k \times (m - m_k)} | I_{m_k}] \)
  - **Top-Down (TD)**: Forecasts generated at the top level and disaggregated downwards.
    - Disaggregation methods: 
      - \( p_j = \frac{1}{n} \sum_{t=1}^{n} Y_{j,t} \) (historical proportions)
      - \( p_j = \frac{\sum_{t=1}^{n} Y_{j,t}}{\sum_{t=1}^{n} Y_t} \) (relative proportions)
  - **Middle-Out (MO)**: Forecasts produced at an intermediate level, with disaggregation and aggregation for lower and higher levels.

- **Combination Approaches (COM)**: Utilize information from all levels to produce coherent forecasts, often resulting in improved accuracy.

- **Limitations of Traditional Methods**: Myopic focus on specific levels, leading to potential loss of useful information from other levels.

- **Proposed Machine Learning (ML) Approach**:
  - **Non-linear Combination**: Allows for more general combinations of base forecasts compared to linear methods.
  - **Dual Objectives**: Aims for both empirical forecasting accuracy and coherence during the training phase.
  - **Selective Combination**: Automatically combines forecasts without needing all forecasts for each series and level.

- **Machine Learning Algorithms Used**:
  - **Random Forests (RF)**: Decision tree-based algorithm that captures non-linear relationships.
  - **XGBoost (XGB)**: An efficient implementation of gradient boosting for regression tasks.

- **Evaluation**: The proposed method is benchmarked against state-of-the-art methods using datasets from tourism and retail, demonstrating superior accuracy and bias reduction.

- **Key Notations**:
  - \( m \): Total number of series in the hierarchy.
  - \( m_i \): Total number of series for level \( i \).
  - \( k \): Total number of levels in hierarchy.
  - \( n \): Number of observations in each series.
  - \( Y_{x,t} \): \( t \)-th observation of series \( Y_x \).
  - \( \hat{Y}_{x,n}(h) \): \( h \)-step-ahead independent base forecast of series \( Y_x \).
  - \( \hat{Y}_{i,t}(h) \): \( h \)-step-ahead forecast at level \( i \).
  - \( \hat{Y}_n(h) \): \( h \)-step-ahead independent base forecast of all series.
  - \( \tilde{Y}_n(h) \): Final reconciled forecasts of all series.

- **Hierarchical Structure Representation**:
  - Expressed as \( Y_t = S Y_{k,t} \), where \( S \) is a summing matrix that aggregates bottom-level series.

- **Performance Metrics**: Evaluated based on accuracy and bias, with results indicating ML approaches outperforming linear methods in various scenarios.