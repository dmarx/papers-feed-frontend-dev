# Zyda: A 1.3T Dataset for Open Language Modeling

## Abstract

## 

The size of large language models (LLMs) has scaled dramatically in recent years and their computational and data requirements have surged correspondingly. State-of-the-art language models, even at relatively smaller sizes, typically require training on at least a trillion tokens. This rapid advancement has eclipsed the growth of open-source datasets available for large-scale LLM pretraining. In this paper, we introduce Zyda (Zyphra Dataset), a dataset under a permissive license comprising 1.3 trillion tokens, assembled by integrating several major respected open-source datasets into a single, high-quality corpus. We apply rigorous filtering and deduplication processes, both within and across datasets, to maintain and enhance the quality derived from the original datasets. Our evaluations show that Zyda not only competes favorably with other open datasets like Dolma, FineWeb, and RefinedWeb, but also substantially improves the performance of comparable models from the Pythia suite. Our rigorous data processing methods significantly enhance Zyda's effectiveness, outperforming even the best of its constituent datasets when used independently.

## Introduction

Over the last five years, large language models (LLMs) have been undergoing an extremely rapid growth in scale, cost, and capabilities [(Vaswani et al., 2017;](#b32)[Radford et al., 2019;](#b20)[Brown et al., 2020;](#b4)[Team et al., 2023;](#b28)[Achiam et al., 2023;](#b1)[Sevilla et al., 2022)](#b23). This development was fueled by the LLM scaling laws [(Hestness et al., 2017;](#b7)[Kaplan et al., 2020;](#b11)[Hoffmann et al., 2022)](#b8) that established a relationship between the attainable loss and model size, dataset size and compute budget based on systematic experiments. Highlighting how performance increases with model size, these scaling laws provide guidance for how to optimally allocate resources for model size and dataset size given a fixed compute budget and provide concrete and fairly accurate predictions about the final loss and downstream capabilities of these models. Specifically, the "Chinchilla scaling laws" [(Hoffmann et al., 2022)](#b8) suggest that equal scaling of parameters and data is required to train a "compute-optimal" model. However, as models become increasingly widely deployed, the majority of the total FLOPs are spent in inference, and not in pretraining. Therefore, the focus has been shifting towards "inference-optimal" models, which are much smaller and are trained on significantly more tokens than the Chinchilla scaling laws would recommend [(Touvron et al., 2023;](#b31)[Jiang et al., 2023)](#b10). These smaller models require significantly fewer forward-pass FLOPs and significantly less GPU VRAM for inference, which has caused them to become extremely important in the open-source LLM community, where the ability of a model to fit inside the VRAM of consumer GPUs is extremely important.

These trends have resulted in a significant increase in the ratio of total number of training tokens to model parameter count. State-of-the-art LLMs went from a 300B:175B ratio with GPT3 and a 540B:760B ratio with PaLM to a 12T:132B ratio with DBRX (MosaicML, 2024) and a 15T:8B ratio with Llama3. Research has also begun to focus strongly on dataset quality as a determinant of final model performance. While dataset quality has always been acknowledged as a factor, early models such as GPT3 [(Brown et al., 2020)](#b4) were trained on largely unfiltered web-scraped data. Modern dataset processing techniques have become more advanced and rely on sophisticated pipelines of syntactic filtering, deduplication, and potentially semantic clustering and classification before a final dataset is produced. It has been consistently demonstrated that both filtering datasets [(Rae et al., 2021;](#b21)[Elazar et al., 2023;](#b5)[Raffel et al., 2020)](#b22) based on heuristic quality signals as well as aggressively deduplicating datasets [(Lee et al., 2021)](#b14) offer significant improvements to the per-token average loss decrease during training and to model performance for a fixed compute budget. Beyond this, using more advanced methods such as utilizing semantic clustering to eliminate semantically duplicated data [(Abbas et al., 2023;](#b0)[Tirumala et al., 2024)](#b30), using trained classifiers to rank quality [(Xie et al., 2023;](#b34)[Ilyas et al., 2022)](#b9), and discovering specific useful data subsets [(Xie et al., 2024)](#b33) have been investigated to further improve model quality for a fixed data and compute budget.

Open-source models have, to some extent, kept up with the frontier in performance during this period of unprecedented scaling. While these models [(Touvron et al., 2023;](#b31)[Team et al., 2024;](#b29)[Jiang et al., 2023)](#b10) are almost always 'open-weights', they usually give little to no information about the datasets they are trained on. In part due to this, openly available datasets have tended to lag significantly behind the frontier in both scale and quality. If open models are to become competitive with the state of the art, they will need extremely large, general, and high-quality open datasets which can be utilized straightforwardly to train such models, allowing practitioners to focus on other aspects of performance such as model scaling and architecture. Moreover, large high-quality datasets may encourage dataset standardization which can allow for fair and meaningful comparisons between different potential architectures and training methods.

In this paper, we take a step towards this vision by releasing Zyda -an open, permissively licensed dataset of 1.3 trillion tokens. Zyda was created by combining major permissively licensed open datasets which are recognized as high-quality within the community. Beyond simple collation, we performed extensive and thorough additional filtering and both intra-and inter-dataset deduplication on these datasets, in addition to any processing they originally underwent. Specifically, we design, test, and tune a novel filtering pipeline which is significantly more comprehensive than prior works. This pipeline utilizes a wide range of filters from a number of sources [(Raffel et al., 2020;](#b22)[Soboleva et al., 2023;](#b25)[Kudugunta et al., 2024;](#b13)[Penedo et al., 2023)](#b19) and also introduces a number of novel ones, all of which were extensively manually tested and tuned to ensure efficacy.

Building atop the deduplication pipeline from [(Soboleva et al., 2023)](#b25), we optimize it for highly parallel processing and carefully tune our deduplication hyperparameters with manual inspections to ensure a good false-positive rate. Inter-dataset deduplication is crucial, since simply combining existing datasets does not remove the duplicate data which is found in both datasets independently. We performed cross-dataset deduplication and found large numbers of cross-dataset duplicates, which is not surprising given that almost all existing open-source datasets were ultimately derived from Common Crawl â€  and filtered in similar ways.

We find that models trained on Zyda perform strongly on language modelling tasks, significantly outperforming models trained upon Dolma as well as the Pile (see Fig. [5a](#) left panel). Further, we find that StarCoder interferes with language modelling abilities, and when we remove the StarCoder subset, Zyda outperforms all its constituent datasets, including RefinedWeb which is known to be a particularly high quality dataset. While these comparisons are done at equi-token level, Zyda contains double the amount of tokens as RefinedWeb, thus enabling significantly better performance if the entire dataset is trained upon. We believe this improvement is based upon our postprocessing pipeline -especially the heavy deduplication both within and across datasets. Zyda is openly available on Huggingface at [https://huggingface.co/datasets/Zyphra/Zyda](https://huggingface.co/datasets/Zyphra/Zyda) and all of our dataset processing code is open-source and is available at [https://github.com/Zyphra/Zyda_ processing](https://github.com/Zyphra/Zyda_processing). 2 Dataset Composition and processing

## Composition

Our dataset comprises almost all currently accessible large-scale LLM pretraining datasets with permissive licenses. These include: The Pile (Gao et al., 2020), SlimPajama [(Soboleva et al., 2023)](#b25), RefinedWeb [(Penedo et al., 2023)](#b19), C4 [(Raffel et al., 2020)](#b22), PeS2o [(Soldaini and Lo, 2023)](#b27), arxiv_s2orc_parsed [(Kenney, 2023)](#b12), and StarCoder [(Li et al., 2023)](#b15). The Pile, SlimPajama, and RefinedWeb are general language modelling datasets which focus on text data and are primarily derived from Common Crawl and a few other sources. PeS2o is a dataset focused on scientific writing and is primarily comprised of arXiv and journal papers. StarCoder is a dataset focused on code and is comprised of code scraped and filtered from github.

## Filtering

Prior to deduplication, following common practice [(Gao et al., 2020;](#b5)[Rae et al., 2021)](#b21), we performed heuristic syntactic filtering for quality and to remove low-quality data, such as meaningless strings, large quantity of random numbers, as well as pornographic or otherwise objectionable content. Our filtering pipeline consisted of two stages: (1) substring replacement and (2) document-level filtering and removal.

For the first stage, we deployed regexes that replace certain substrings with more sanitized versions. This was primarily to fix common formatting issues that we noticed in our original analyses of the datasets. Examples included excessively long sequences of dashes, full-stops, '\r' characters, and other punctuation characters which presumably arose from quirks of formatting or the processing pipelines that lead to these documents. Such strings appeared with reasonable frequency and often were found amid otherwise unobjectionable documents, so we did not wish to simply remove documents in which they appeared.

Typically, we replaced large numbers of repeated characters Dataset Rows Initial (Millions) Rows Removed (Millions) Percent filtered RefinedWeb 968.000 21.350 2.21% SlimPajama 590.395 21.045 3.56% C4-EN 364.869 4.234 1.16% Pile-Uncopyrighted 177.010 21.536 12.17% peS2o 38.811 0.044 0.11% arXiv 1.672 0.189 11.33% Table 1: Number of document and percentage of all documents removed from each subcomponent dataset by our filtering process.

with a single or just a few characters. For instance, we replaced large numbers of linebreaks '\n' and carriage returns '\r' with just a single linebreak or carriage return. Similarly, for long sequences of dashes, we replaced them with a a single dash. For the second stage, we performed document-level filtering based on a set of syntactic heuristics which can be cheaply computed from the raw text of a document. If the threshold of the filter was exceeded, the whole document was removed from the dataset. These filters broadly fell into three categories: (1) removing syntactically broken or otherwise gibberish documents, (2) removing semantically meaningless documents, and (3) removing meaningful but objectionable content. Examples of the first kind of filter include methods like filtering based on the proportion of alphanumeric characters in a document, which at high proportions uniformly corresponds to documents comprised of entirely gibberish strings generated either through broken preprocessing or through unknown processes on the internet. Semantically meaningless documents include documents full of seemingly random numbers, cryptographic strings, and lists of seemingly unrelated URLs. Objectionable content included primarily pornographic and offensive content, which we removed with specialized word lists.

Our general philosophy in filtering was to not filter excessively and keep false positives (i.e. good documents removed) relatively low. We manually tested and tuned each of the filters presented here on the Pile dataset. We tuned the filter thresholds so that we obtained a false-positive threshold of about 20% -that is, 20% of the filtered documents were seemingly unobjectionable, while 80% were obviously harmful and it was correct to remove them. This provides a reasonable trade-off between excessive filtering while still removing the majority of content the filter was aimed at reducing. For every filter, we spent significant time manually tuning the threshold and other parameters, then looking at the filtered outputs and attempting to identify true vs false positives. We primarily performed this tuning on the Pile dataset and performed only sanity checking of the filter outputs on the other datasets. However, we believe that many of the categories of low-quality text likely have similar distributions between datasets, since so many of them are derived from the same source. A full list and description of all filters can be found in Appendix E.1. All datasets were filtered using the same rules except for StarCoder, which we exempted since it consists entirely of code which has a significantly different distribution than the primarily text data in the other datasets. Additionally, the StarCoder authors performed a thorough code-specific filtering of the dataset before release. Table [1](#) shows the proportion and number of rows removed from each dataset by our filtering process. We observe that the primary source of data removed by our filters is the Pile and arXiv.  To identify duplicates we used Locality Sensitive Hashing (LSH) based on MinHash signatures [(Broder, 1997)](#b3). This technique allows fast approximate identification of duplicate candidates based on Jaccard similarity of sets of n-grams S n (â€¢) in documents. For example, using MinHash(A) = {min h 1 (S n (A)), . . . , min h k (S n (A))} and MinHash(B) = {min h 1 (S n (B)), . . . , min h k (S B )} for k hash functions, we can measure the resemblance of documents A and B using:

## Deduplication

$1 k k i=1 1[min h i (S n (A)) = min h i (S n (B))].(1)$In the rest of the paper, we refer to LSH-x% as the LSH with at least x% resemblance, as expressed by Equation [1](#formula_0). We deduplicated each dataset both against itself and against the other datasets in our full dataset. We built the LSH index by inserting the components in the following order: first Pileuncopyrighted, then C4-en, peS2o, arxiv_s2orc_parsed, RefinedWeb, SlimPajama, and finally StarCoder.

For our deduplication pipeline, we used 13-grams â€¡ based on words to form our n-gram subsets and a minhash signature size of 128. Before generating 13-grams, we performed NFC normalization, conversion to lower case, and removal of punctuation and consecutive spaces, newlines, tabs in the middle and in the beginning and end of the strings. We performed deduplication at two Jaccard similarity thresholds: 40% and 80% (documents are considered duplicates if their similarity measure is equal or greater than the threshold). The parameters of LSH index were optimized to minimize the rates of false positives (FP) and false negatives (FN): for the 40% threshold, minhash indices were split into 32 bands each with a range of 4, while for the 80% threshold, we used 9 bands with a range of 13. From these parameters we can derive the following false-positive and false-negative rates: for 40% threshold, the false-negative rate is 3.4% and false-positive rate is 5.4%, for 80% threshold, the false-negative rate is 3.3% and false-positive rate is 2.5%. Table [2](#tab_1) summarizes how many tokens (using gpt-neox tokenizer) were removed at different thresholds. Our raw dataset consists of 2T tokens, and we end up with 1.5T for 80% threshold and 1.3T for 40% threshold.

After identifying duplicate pairs using the LSH minhash technique, we clustered documents into a graph of connected components with the nodes being documents and the edges connecting duplicate pairs. We then kept only one document from the cluster while removing the rest. To determine which document to keep, we sorted the documents in the clusters by their dataset of origin and kept the highest-ranking one according to the following order: 1) StarCoder; 2) RefinedWeb; 3) peS2o; 4) arXiv; 5) C4; 6) Pile-uncopyrighted; 7) SlimPajama. We chose StarCoder as the highest-ranking dataset because it was specifically designed for code, so we hypothesized that any duplicate code snippets are likely to be of the highest quality from this source. We chose the rest of the ranking based on heuristic assessments of quality.

We then performed random sampling of duplicates in the clusters to manually explore examples. The largest clusters usually contained either short low-quality documents or documents with widely distributed texts, such as license agreements, advertisements, etc. We did notice that at 40% threshold LSH minhash was performing a qualitatively different kind of deduplication than at 80%: while at 80% most duplicates looked very similar, at 40% we started seeing duplicates across formats, â€¡ We chose 13-gram based on what [(Gao et al., 2020;](#b5)[Soldaini et al., 2024)](#b26) use, which is a common choice of n-gram. Other choices of n can be successfully applied to deduplicate text. For example, [(Penedo et al., 2023](#b19)[(Penedo et al., , 2024) )](#b18)   especially between peS2o and arXiv components of our dataset, where we observed, for instance, the same paper but formatted in two different ways.

Since the LSH minhash algorithm only performs approximate deduplication, we also sampled 4.8 million duplicate pairs to estimate the actual false-positive rate based on Jaccard and edit similarities.

We define the edit similarity between two documents as their edit distance divided by maximum length of two documents. We found good agreements between theoretical and estimated false-positive rate based on Jaccard similarities, while the estimated false-positive rate based on edit similarity for the 40% threshold was even lower than the theoretical estimate at 3.1%. Figures [2a](#fig_2) and [2b](#fig_2) show the distribution of edit and Jaccard similarities for the 40% threshold version of our dataset: As expected, the vast majority of identified duplicates are above the threshold (marked as red dash line), and for the edit similarity metric, the distributions is skewed toward higher values (which is expected, since it corresponds to a lower false-positive rate). To compare the 40% and the 80% versions of Zyda, we trained 1.4B transformers on 50B tokens sampled from each version of Zyda. We found that 40% Zyda performed slightly better (Fig. [5b](#)). Due to this, we chose to release the 40% version as our primary Zyda dataset.

## Performance

We perform a series of training runs of small transformer models to assess the performance of our dataset vs alternatives such as Pile, and RefinedWeb, and Dolma. We first compare the performance of models trained on our dataset with the well-known Pythia suite of models [(Biderman et al., 2023)](#b2). We match the architecture, hyperparameters, and training process of the Pythia suite. We compare against the 410M Pythia model size. Like Pythia, we train for 300B tokens at this model scale. This provides a direct apples-to-apples comparison of Zyda against the Pile. We demonstrate that across the range of evals reported in the Pythia paper, that Zyda-trained models significantly outperforms the Pile. We especially observe gains in reasoning evaluations such as ARC and PIQA.

For a fair comparison, we ensure that these comparisons are equi-token. We note that Zyda contains 1.3T tokens, while the Pile contains only 300B, and thus we expect that models trained on the full Zyda dataset would substantially outperform models trained on the full Pile, such as Pythia, even more than we do so here.

Next we perform a preliminary scaling analysis of our dataset against the Pile and Pythia on a range of scales from 160M to 1.4B, all trained on the full Pile and 300B tokens of Zyda. We show that the advantage of Zyda appears to increase with scale. We suggest that this is because the increased dataset quality is not 'visible' to small models as they are capacity limited to only extract the larger modes of variance in the data, whereas larger models can additionally 'see' finer modes of variance where our increased dataset quality provides an advantage. A small model may simply be unable to  (b) The effect of deduplication threshold at either 40% or 80% similarity. We observe a slight advantage for 40% although this is not consistent across evaluation metrics.

Figure [5](#): Comparison of Zyda with alternative datasets, and across deduplication LSH We first observe that Zyda at the 50B token stage performs slightly worse than RefinedWeb but still better than SlimPajama C4, and Dolma. A priori, this is not surprising since Zyda contains both of these as subsets. The performance decrease of Zyda (compared to RefinedWed) is attributed to the presence of StarCoder. With StarCoder removed, Zyda outperforms all other datasets and indeed performs better than any of its subsets -a surprising result which speaks to the importance of our filtering and deduplication pipeline. It appears that, at least at this scale, the competition between language and code introduced by having a significant fraction of code inhibits performance on standard language modelling evaluations. We find that a comparable open dataset -Dolma [Soldaini et al. (2024)](#b26) -performs worse than both Zyda and most individual subsets of Zyda. It is also important to note that RefinedWeb only contains 600B tokens to Zyda's 1.3T and hence we anticipate Zyda would greatly outperform RefinedWeb if the model was trained to the end on each dataset. RefinedWeb is a strong and already well-filtered dataset which also forms a significant proportion of Zyda, and is thus a strong baseline. Zyda also contains a number of less performant datasets such as the Pile and C4. Despite this, Zyda's additional filtering and deduplication steps boost Zyda's performance such that the full dataset performs better than the raw version of its strongest subset. We can also detect this by noting that the raw Zyda dataset performs averagely at about the level of SlimPajama. Ultimately, Zyda appears to be a strong pretraining dataset at the 1T token scale containing only pretraining web data. Since it appears to decrease performance, we recommend removing StarCoder from Zyda for small models which need to be language focused instead of code focused since, at this scale, code appears to cause interference with the model's language abilities. The performance of Zyda has also been empirically validated through our language modelling effort Zamba [(Glorioso et al., 2024)](#b6), a 7B hybrid SSM-transformer which performs very strongly per pretraining token, and was trained on an early version of Zyda.

## Related Work

Concurrently with the rise of open-source large language models, there has been an increase in the number of open-source and widely available datasets for large-scale language model pretraining. Early works in this vein include C4 [(Raffel et al., 2020)](#b22), derived directly from Common Crawl, and The Pile [(Gao et al., 2020)](#b5) which was manually constructed from a number of disparate sources. Other, more recently released large datasets such as SlimPajama [(Soboleva et al., 2023)](#b25), RefinedWeb [(Penedo et al., 2023)](#b19) and Dolma [(Soldaini et al., 2024)](#b26) have undergone more thorough filtering and deduplication -a process which we build upon in this work. We have also built upon works that described their filtering pipelines without releasing the dataset. Of especial use was Gopher [(Rae et al., 2021)](#b21) and RedPajama (TogetherAI, 2023), which inspired a number of our own filters.

Most similar to our approach are RefinedWeb and Dolma. The RefinedWeb authors, as part of the Falcon LLM team, created a dataset of several trillion tokens using a filtering and deduplication pipeline similar to ours, but they only released publicly approximately 600B tokens of their full dataset. The Dolma dataset is also similar to our approach and releases several trillion tokens, derived from a mixture of Common Crawl and some existing datasets such as C4 and PeS2o, which are also included in our dataset. Hovewer, Dolma does not cross-deduplicate data between its component datasets -a process we found likely to be important since we identified many duplicate documents across datasets. We additionally find that Zyda outperforms Dolma at equi-token language modelling evaluations by a significant margin. Concurrently, FineWeb [(Penedo et al., 2024)](#b18) was also released, which offers 15T tokens directly from Common Crawl under an open license. Since their Common Crawl datasets appear to come from a different source, it seems likely that Zyda and FineWeb can be productively merged. Zyda consolidates all the primary prior open-datasets into one place and subjects them to a uniform quality filtering and deduplication which can then be augmented with FineWeb's additional Common Crawl tokens. Additionally, unlike these works which directly filter from Common Crawl, we aim to instead collect and collate existing open-source datasets under one banner, and to uniformly filter and deduplicate them against one another so as to create a dataset of known high-quality text which can be used to perform training at trillion-token scales. We hope that this seed dataset can then be expanded as additional datasets are released by the open-source community.

We built our deduplication pipeline upon SlimPajama's existing open-source libraries, although we ended up significantly modifying them and performed extensive manual tuning and optimization. Our initial filters were inspired by the filter list accompanying the RedPajama dataset. However, after preliminary testing, we removed some of their filters which we judged were ineffective and integrated new ones, either from other papers such as C4, Gopher, and MADLAD-400 [(Kudugunta et al., 2024)](#b13), or of our own invention. A full list and description of the filters we used, as well as qualitative impressions gained during testing them, can be found in Appendix E.1.

## Discussion

In this paper, we have presented Zyda, a unified dataset released under a permissive license, comprising most of the largest and highest quality existing open-source datasets available. Upon these, we have performed extensive additional filtering, beyond what was originally applied, in addition to thorough intra-and inter-dataset deduplication. Our aim with this work is to create a growing and extendable dataset which can be easily used by practitioners to train trillion-token scale language models while also consolidating and unifying the efforts made by disparate open-source groups who have released their datasets. Ultimately, we hope that our work can provide an "off-the-shelf" accessible trillion-scale high-quality pretraining dataset which can be used by groups aiming to pretrain their own LLMs.

While not performing new collection from Common Crawl, we believe that our work is an important and valuable step towards the creation of large scale high quality open datasets given that there exist a number of high quality existing datasets but few to none of them individually reach the scale necessary for training state-of-the-art models. Collating, filtering, and deduplicating the existing datasets needed to create a trillion token dataset is nontrivial work and extremely important to raise the quality of the dataset and prevent significant amounts of inter-dataset duplicates. This latter operation of deduplication between datasets is extremely important given the degree of duplicated documents we discovered in common open-source datasets.

Ultimately, we believe that this dataset is only the first step towards building an open dataset competitive with state-of-the-art models at the largest scales. While our dataset combines the primary filtered open-source datasets available, the datasets used to train the largest models today are already an order of magnitude larger. The recent release of FineWeb [(Penedo et al., 2024)](#b18) which contains 15T Common Crawl tokens is exciting and effectively removes the first bottleneck -sourcing the tokens -to the creation of large effective open datasets to LLM pretraining, yet likely the best proprietary datasets still retain significant quality advantages. While many of the secrets to high quality filtering remain hidden behind the veil of the largest labs, there is now a growing amount of academic work showing methods for how dataset quality can be significantly improved beyond the simple filtering and deduplication pipeline we have implemented here. We believe that a strong pretraining mixture can likely be created by merging Zyda and FineWeb together which can be used to approach the training token count of the largest closed datasets.

While we have performed thorough filtering and deduplication to remove low quality documents and many duplicates, we have only yet taken the first step in improving dataset quality. A large number of methods can still be employed to further improve its quality, at the cost of greater compute requirements. Such methods include training semantic classifiers to detect high or low quality data [(Xie et al., 2023;](#b34)[Ilyas et al., 2022)](#b9), performing passes over the dataset with language models to filter based on the perplexity of the data [(Marion et al., 2023)](#b17), performing clustering on the dataset to remove outlier or repetitive data [(Tirumala et al., 2024)](#b30), augmenting the original data with synthetic or rephrased data [(Maini et al., 2024)](#b16), and many other approaches. We hope that future work explores these and other avenues and continues to extend and improve the underlying open datasets available for pretraining so as to push forward the frontier of performance accessible with open datasets.

## A Limitations

While we believe Zyda is a highly effective dataset for language modelling, and this is supported by our ablation studies, and our empirical successes with the Zamba model [(Glorioso et al., 2024)](#b6). However, except our ablations comparisons against the Pythia suite, all our other ablations were only trained on 50 billion tokens which may not be enough to predict dataset quality over the full course of training over one or more epochs on the dataset. We limited our ablations to 50B tokens on a 1.4B model primarily out of compute and cost considerations. Additionally, while we performed a small scaling suite for the Pythia models, we did not perform this for other ablations, focusing on the 1.4B scale. It is possible that the scaling properties of different datasets differ so tht some datasets may perform better with larger or smaller models, and we cannot discount that this could change the rank ordering of datasets by performance.

## B Ablation experimental details

For our 50B ablations, we used a 1.4B standard transformer model. The architecture and trainng hyperparameters are presented in Table [3](#tab_2). We used the Swiglu activation function on the MLP layers and trained with RoPE position embeddings. For the Pythia comparisons, we trained on 300B tokens (the same size as full Pile), using the model and training hyperparameters reported in [Biderman et al. (2023)](#b2).

All models were trained on Nvidia H100 DGX systems. Most ablations were run on 1-2 DGX nodes. We used the MegatronLM [(Shoeybi et al., 2019)](#b24) framework for training the models used in these ablation studies.

## C Societal impacts

We release Zyda with the aim of providing a dataset to make language model training at the trilliontoken scale more accessible and effective. Empowering disparate and less-resourced groups to create such language models may have a variety of societal impacts. While enabling more actors to train effective language models may lead to greater decentralization of power and more equal access to the technology, it also opens the door to potential misuse. While we have endeavoured with our filtering to remove harmful and personally identifying content from Zyda, we cannot guarantee that Zyda cannot be used to train language models which could cause harm.  

## D Ablation performance by evals

## E Additional details for dataset processing E.1 Additional Filtering Details

In this Appendix, we describe the filters that were implemented in our filtering pipeline as well as additional filters that we experimented with which we did not find useful. We also present a number of qualitative insights we found while exploring Zyda in depth during our manual analysis and tuning of the filters.

The filters that were used in the final pipeline were as follows:

â€¢ Filter by min length: This filtered out all documents shorter than a given length (we choose 100 characters). In practice, almost all documents shorter than this length consisted of short strings of broken formatting, ads and spam, single links, or unintelligble broken code snippets (primarily client-side javascript and sometimes CSS). This filter appeared highly effective with few false positives.

â€¢ Filtering by max mean word length: This filter removes documents which have too large a proportion of words of exceptional length. We found that this primarily removed short code snippets with verbose variables in CamelCase as well as malformed documents without spaces or with exceptionally long unusual strings such as hashes, cryptographic keys, and general weirdness.

â€¢ Filtering by minimum mean word length: This filter removed documents which too large a proportion of words with too short a length. We found in practice that this tended to remove short documents which were 'broken' in some sense -usually consisting of either individual random characters or short bursts of random gibberish characters.

â€¢ Filtering by fraction of alphanumeric characters: This filter primarily removed documents consisting of non-alphanumeric characters. In practice, almost all such documents appeared to be gibberish with small fractions of unobjectionable code as false positives. We do not believe that this filter is suitable for full code datasets.

â€¢ Filter by fraction numerical: This filter removed documents consisting primarily of numbers. We detected a large number of such documents, most of which appeared to be stored data from various souces, most without any kind of context which would make it easy for a language model to understand what the data is meant to represent.

â€¢ Filtering by fraction 'xml': This filter removed egregious xml documents which primarily appeared to be endless sequences of xml tags with little actual content.

â€¢ Filter by lorem ipsum: This filter removed lorem ipsum text, of which we found a small amount in the Pile.

â€¢ Filter by 'http://' and 'https://' and 'www.': this filter removed pages with too high a fraction of website links. In practice, this removed documents which either consisted entirely of a single random URL (which could be long enough to survive our min length filter), or else documents with large proportions of URLs. Such documents were almost entirely ads or spam of various kinds, or else isolated links bars from websites. This filter appeared highly effective at removing such content in practice.

â€¢ Filter by fraction '<' and '>': This filter removed a small amounts of documents consisting almost entirely of these symbols, as well as many poorly parsed webpages with truly excessive amounts of HTML tags without much appreciable content.

â€¢ Filtering by ':': This was used to filter for the opening of a python dictionary or JSON key or value. We used this to remove documents that were composed of an excessive fraction of JSON data. We found that in practice such documents primarily consisted of either unintelligble data or extremely repetitive CSS UI attributes.

â€¢ Filtering by pornographic word list: We found that using a relatively simple word list filtered out a large quantity of pornographic content from the Pile. This mostly consisted of adverts or titles from porn sites with a dash of porn ads. It also filtered out a fair amount of erotica fiction which may not necessarily be low quality. After experimenting with existing approaches, we elected to create a custom porn filter based only on a few keywords which can run fast through the dataset. We then filter both on the absolute number of such terms appearing in the document as well as on the fraction of the document comprised of these terms. We found that approximately 0.5% of The Pile appears to be pornographic content. â€¢ Filter by profanity word list: This filtered out documents based on a high proportion of common (English) profanity. We found that this primarily filtered out short documents consisting of low quality reddit comments or other social media postings. Notably, almost all documents removed by this filter were extremely short. â€¢ Filter by Chinese porn signals: This filter was derived from [Kudugunta et al. (2024)](#b13) and used a special word-list that they had derived. Upon inspection this did appear to filter out mostly chinese text and included very little English. However we did not analyze this in depth as we focused on English monolingual language modelling. â€¢ Filter by cursed string fraction: We filtered using the 'cursed regex' of [Kudugunta et al. (2024)](#b13) which they claimed captured many undesirable strings. We found that this primarily filtered for ads, spam, and lorem ipsum text. This filter required quite a bit of manual tuning as the settings described in the paper were too aggressive -e.g. removing any line with the word 'Facebook', or 'download' -which clearly matches to many perfectly unobjectionable texts.

There was also a number of filters that we tried and found ineffective, including a number of recommendations from prior works. These include:

â€¢ Filtering by max length: We found that this was actively unhelpful and that almost all long documents appeared high quality. Usually they were either full books, or else concatenated encyclopedia entries or legal cases. We suspect that such data is actually high quality and helpful for the model. â€¢ Filtering by fraction of upper case characters, as suggested in RedPajama. We found that while this filter caught some spam it also caught large amounts of unobjectionable text. Such text included notices and long-form advertisements from news papers, legal texts, and text which just seemed to be randomly have been converted to upper case. Since we could not find a threshold which had a sufficiently low false-positive level while also filtering out an appreciable number of documents, we ended up dropping this filter. â€¢ We found that removing all documents with many "{" as done in [Kudugunta et al. (2024)](#b13) and with sentences ended without punctuation [Raffel et al. (2020)](#b22) primarily removed code which was otherwise non-objectionable. â€¢ We found that removing documents without at least 2 standard english words such as "the, of" etc as proposed in [Rae et al. (2021)](#b21) tended to remove large fractions of otherwise unobjectionable code â€¢ We found that filtering based on a large number of repeated characters was not effective and had many false positives. This usually meant either texts with many 0s in it -for instance numbers reported to a high degree of precision, and good text just with messed up formatting -i.e. many repeating dashes or slashes or tabs. There were not many files with extremely large amounts of repeating characters that we found. For instance, we found many legal cases with extremely large numbers of dashes, tabs, carriage-returns and xa0 characters which we filtered based on. Instead we opted to replace many repeated characters with a much smaller number of the same character. We hope this fixed many elements of the formatting without removing whole documents of good text which just happened to have been badly formatted during text extraction. â€¢ We found that filtering by PII count was not particularly useful. We found that filtering by IP addresses as done in RefinedWeb [(Penedo et al., 2023)](#b19) primarily removes tutorial networking code using example IP addresses which seems likely to be helpful for the model. Filtering aggressively by email and phone numbers tends to remove large numbers of otherwise useful social media and blog postings, however less aggressive filters which require a large fraction of such PII does remove some ads and spam. 

## E.3 Duplicates sources

We computed the distribution of sources of duplicates across the components (see Figure [7](#fig_8)). On Figure [7](#fig_8) we plot the proportion of duplicates that are coming from different datasets: e.g., for Pile we identified roughly 90 million documents duplicated in the Pile itself and other datasets, and the figure tells us that 35% of those are coming from itself, 24% from StarCoder, 17% from SlimPajama, and 23% from the rest of the components. We observe extremely high number of duplicates in arXiv are coming from peS2o since they have the same provenance. We also observe that our approach correctly identified a lot of documents in SlimPajama are present in C4, since C4 was included in RedPajama (the parent dataset of SlimPajama  

## E.4 Estimating empirical false positive rate

To estimate the true false positive rate beyond the theoretical calculation, we sampled 4.8 million duplicates pairs (see Figure [8a](#fig_9) for the dustribution of lengths), and computed their empirical edit and Jaccard similarities, as well as manually inspected a number of examples. On Figure [8b](#fig_9) we plot a dependence on document length of what we call a cumulative FP rate: given length l, cumulative FP rate is the FP rate amoung duplicates for which the length is at most l. We found that the length of the document had a strong effect on the proportion of false positives with short documents being more likely to be categorized as false positives than longer ones. This is likely due to our use of 13-grams for similarity matching as well as the fact that noise fluctuations in similarity are of large scale relative to the threshold at small lengths.

(a) Cumulative distribution function of duplicate documents lengths on a log scale. Red dashed lines correspond to median length. We observe that the vast majority of documents tend to sit between 10 2 and 10 4 words in length.

(b) Cumulative FP rate as a function of duplicate document length. The red dashed line corresponds to the theoretical 3.1% FP rate computed on all pairs of the sample. We observe that for short documents the empirical and theoretical FP rates diverge before converging to the theoretical prediction for long documents 

![Figure 1: The proportion of different dataset subsets in Zyda. The primary proportion is RefinedWeb, followed by SlimPajama and StarCoder.]()

![of edit similarity distance of selected duplicate documents, red dash line marks 40% threshold.(b) Distribution of Jaccard similarity distance of selected duplicate documents, red dash line marks 40% threshold.]()

![Figure 2: Document similarity distances for Zyda dataset]()

![Figure 3: Aggregate evaluation scores across training steps for a 1.4B model trained on 50B tokens of Zyda and comparable datasets. Zyda and especially Zyda without starcoder outperform strong open baselines such as FineWeb, and Dolma. Aggregate scores is the mean of arc-challenge, arc-easy, boolq, openbookqa, piqa, sciq, and winogrande. Scores smoothed using a window size of 5.]()

![(a) Aggregate evaluation performance of models trained on Zyda versus Pile. Aggregate score is the mean of the evaluation scores for ARC-c, ARC-e, LogiQA, PIQA, SciQ, and WinoGrande (b) Evaluation scores for 1.4B Pythia trained on Zyda and Pile.]()

![Figure4: We match the Pythia suite in architecture and training hyperparameters. We observe that Zyda outperforms Pile on evaluations and that this advantage increases with scale, which we believe to be due to reduced noise on standard evals as model performance improves. All models were trained for 300B tokens on either Pile or Zyda.]()

![Figure 6: Scores broken out by specific evaluation of our ablation studies comparing Zyda to alternative datasets.]()

![Figure 7: Proportion of duplications for all datasets for LSH 40%.]()

![Figure 8: Statistics of the sample of 4.8 million duplicate documents.]()

![successfully use 5-grams for deduplication Initial]()

![Model and training hyperparameters for the 1.4B transformer models trained in our ablation experiments.]()

![Filtering counts for pile-uncopyrighted]()

![). Big proportion of duplicates in RefinedWeb, C4 overlap with each other probably due the fact that they are derivative of Common Crawl.Filtering counts for C4-EN]()

![Filtering counts for peS2o]()

![Filtering counts for arXiv]()

![Filtering counts for RefinedWeb]()

![Filtering counts for SlimPajama]()

â€  https://commoncrawl.org/

