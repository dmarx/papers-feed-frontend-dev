<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Zyda: A 1.3T Dataset for Open Language Modeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-09-03">3 Sep 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yury</forename><surname>Tokpanov</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zyphra</forename><surname>Beren Millidge</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zyphra</forename><surname>Paolo</surname></persName>
							<email>paolo@zyphra.com</email>
						</author>
						<author>
							<persName><forename type="first">Glorioso</forename><surname>Zyphra</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jonathan</forename><forename type="middle">Pilault</forename><surname>Zyphra</surname></persName>
							<email>jonathan@zyphra.com</email>
						</author>
						<author>
							<persName><forename type="first">Adam</forename><forename type="middle">Ibrahim</forename><surname>Zyphra</surname></persName>
						</author>
						<author>
							<persName><forename type="first">James</forename><forename type="middle">Whittington</forename><surname>Zyphra</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Quentin</forename><forename type="middle">Anthony</forename><surname>Zyphra</surname></persName>
							<email>quentin@zyphra.com</email>
						</author>
						<title level="a" type="main">Zyda: A 1.3T Dataset for Open Language Modeling</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-09-03">3 Sep 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">AD2C9E74A757D1D5758C8EE376DBF404</idno>
					<idno type="arXiv">arXiv:2406.01981v2[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The size of large language models (LLMs) has scaled dramatically in recent years and their computational and data requirements have surged correspondingly. State-of-the-art language models, even at relatively smaller sizes, typically require training on at least a trillion tokens. This rapid advancement has eclipsed the growth of open-source datasets available for large-scale LLM pretraining. In this paper, we introduce Zyda (Zyphra Dataset), a dataset under a permissive license comprising 1.3 trillion tokens, assembled by integrating several major respected open-source datasets into a single, high-quality corpus. We apply rigorous filtering and deduplication processes, both within and across datasets, to maintain and enhance the quality derived from the original datasets. Our evaluations show that Zyda not only competes favorably with other open datasets like Dolma, FineWeb, and RefinedWeb, but also substantially improves the performance of comparable models from the Pythia suite. Our rigorous data processing methods significantly enhance Zyda's effectiveness, outperforming even the best of its constituent datasets when used independently.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Over the last five years, large language models (LLMs) have been undergoing an extremely rapid growth in scale, cost, and capabilities <ref type="bibr" target="#b32">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b20">Radford et al., 2019;</ref><ref type="bibr" target="#b4">Brown et al., 2020;</ref><ref type="bibr" target="#b28">Team et al., 2023;</ref><ref type="bibr" target="#b1">Achiam et al., 2023;</ref><ref type="bibr" target="#b23">Sevilla et al., 2022)</ref>. This development was fueled by the LLM scaling laws <ref type="bibr" target="#b7">(Hestness et al., 2017;</ref><ref type="bibr" target="#b11">Kaplan et al., 2020;</ref><ref type="bibr" target="#b8">Hoffmann et al., 2022)</ref> that established a relationship between the attainable loss and model size, dataset size and compute budget based on systematic experiments. Highlighting how performance increases with model size, these scaling laws provide guidance for how to optimally allocate resources for model size and dataset size given a fixed compute budget and provide concrete and fairly accurate predictions about the final loss and downstream capabilities of these models. Specifically, the "Chinchilla scaling laws" <ref type="bibr" target="#b8">(Hoffmann et al., 2022)</ref> suggest that equal scaling of parameters and data is required to train a "compute-optimal" model. However, as models become increasingly widely deployed, the majority of the total FLOPs are spent in inference, and not in pretraining. Therefore, the focus has been shifting towards "inference-optimal" models, which are much smaller and are trained on significantly more tokens than the Chinchilla scaling laws would recommend <ref type="bibr" target="#b31">(Touvron et al., 2023;</ref><ref type="bibr" target="#b10">Jiang et al., 2023)</ref>. These smaller models require significantly fewer forward-pass FLOPs and significantly less GPU VRAM for inference, which has caused them to become extremely important in the open-source LLM community, where the ability of a model to fit inside the VRAM of consumer GPUs is extremely important.</p><p>These trends have resulted in a significant increase in the ratio of total number of training tokens to model parameter count. State-of-the-art LLMs went from a 300B:175B ratio with GPT3 and a 540B:760B ratio with PaLM to a 12T:132B ratio with DBRX (MosaicML, 2024) and a 15T:8B ratio with Llama3. Research has also begun to focus strongly on dataset quality as a determinant of final model performance. While dataset quality has always been acknowledged as a factor, early models such as GPT3 <ref type="bibr" target="#b4">(Brown et al., 2020)</ref> were trained on largely unfiltered web-scraped data. Modern dataset processing techniques have become more advanced and rely on sophisticated pipelines of syntactic filtering, deduplication, and potentially semantic clustering and classification before a final dataset is produced. It has been consistently demonstrated that both filtering datasets <ref type="bibr" target="#b21">(Rae et al., 2021;</ref><ref type="bibr" target="#b5">Elazar et al., 2023;</ref><ref type="bibr" target="#b22">Raffel et al., 2020)</ref> based on heuristic quality signals as well as aggressively deduplicating datasets <ref type="bibr" target="#b14">(Lee et al., 2021)</ref> offer significant improvements to the per-token average loss decrease during training and to model performance for a fixed compute budget. Beyond this, using more advanced methods such as utilizing semantic clustering to eliminate semantically duplicated data <ref type="bibr" target="#b0">(Abbas et al., 2023;</ref><ref type="bibr" target="#b30">Tirumala et al., 2024)</ref>, using trained classifiers to rank quality <ref type="bibr" target="#b34">(Xie et al., 2023;</ref><ref type="bibr" target="#b9">Ilyas et al., 2022)</ref>, and discovering specific useful data subsets <ref type="bibr" target="#b33">(Xie et al., 2024)</ref> have been investigated to further improve model quality for a fixed data and compute budget.</p><p>Open-source models have, to some extent, kept up with the frontier in performance during this period of unprecedented scaling. While these models <ref type="bibr" target="#b31">(Touvron et al., 2023;</ref><ref type="bibr" target="#b29">Team et al., 2024;</ref><ref type="bibr" target="#b10">Jiang et al., 2023)</ref> are almost always 'open-weights', they usually give little to no information about the datasets they are trained on. In part due to this, openly available datasets have tended to lag significantly behind the frontier in both scale and quality. If open models are to become competitive with the state of the art, they will need extremely large, general, and high-quality open datasets which can be utilized straightforwardly to train such models, allowing practitioners to focus on other aspects of performance such as model scaling and architecture. Moreover, large high-quality datasets may encourage dataset standardization which can allow for fair and meaningful comparisons between different potential architectures and training methods.</p><p>In this paper, we take a step towards this vision by releasing Zyda -an open, permissively licensed dataset of 1.3 trillion tokens. Zyda was created by combining major permissively licensed open datasets which are recognized as high-quality within the community. Beyond simple collation, we performed extensive and thorough additional filtering and both intra-and inter-dataset deduplication on these datasets, in addition to any processing they originally underwent. Specifically, we design, test, and tune a novel filtering pipeline which is significantly more comprehensive than prior works. This pipeline utilizes a wide range of filters from a number of sources <ref type="bibr" target="#b22">(Raffel et al., 2020;</ref><ref type="bibr" target="#b25">Soboleva et al., 2023;</ref><ref type="bibr" target="#b13">Kudugunta et al., 2024;</ref><ref type="bibr" target="#b19">Penedo et al., 2023)</ref> and also introduces a number of novel ones, all of which were extensively manually tested and tuned to ensure efficacy.</p><p>Building atop the deduplication pipeline from <ref type="bibr" target="#b25">(Soboleva et al., 2023)</ref>, we optimize it for highly parallel processing and carefully tune our deduplication hyperparameters with manual inspections to ensure a good false-positive rate. Inter-dataset deduplication is crucial, since simply combining existing datasets does not remove the duplicate data which is found in both datasets independently. We performed cross-dataset deduplication and found large numbers of cross-dataset duplicates, which is not surprising given that almost all existing open-source datasets were ultimately derived from Common Crawl † and filtered in similar ways.</p><p>We find that models trained on Zyda perform strongly on language modelling tasks, significantly outperforming models trained upon Dolma as well as the Pile (see Fig. <ref type="figure">5a</ref> left panel). Further, we find that StarCoder interferes with language modelling abilities, and when we remove the StarCoder subset, Zyda outperforms all its constituent datasets, including RefinedWeb which is known to be a particularly high quality dataset. While these comparisons are done at equi-token level, Zyda contains double the amount of tokens as RefinedWeb, thus enabling significantly better performance if the entire dataset is trained upon. We believe this improvement is based upon our postprocessing pipeline -especially the heavy deduplication both within and across datasets. Zyda is openly available on Huggingface at <ref type="url" target="https://huggingface.co/datasets/Zyphra/Zyda">https://huggingface.co/datasets/Zyphra/Zyda</ref> and all of our dataset processing code is open-source and is available at <ref type="url" target="https://github.com/Zyphra/Zyda_processing">https://github.com/Zyphra/Zyda_ processing</ref>. 2 Dataset Composition and processing</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Composition</head><p>Our dataset comprises almost all currently accessible large-scale LLM pretraining datasets with permissive licenses. These include: The Pile (Gao et al., 2020), SlimPajama <ref type="bibr" target="#b25">(Soboleva et al., 2023)</ref>, RefinedWeb <ref type="bibr" target="#b19">(Penedo et al., 2023)</ref>, C4 <ref type="bibr" target="#b22">(Raffel et al., 2020)</ref>, PeS2o <ref type="bibr" target="#b27">(Soldaini and Lo, 2023)</ref>, arxiv_s2orc_parsed <ref type="bibr" target="#b12">(Kenney, 2023)</ref>, and StarCoder <ref type="bibr" target="#b15">(Li et al., 2023)</ref>. The Pile, SlimPajama, and RefinedWeb are general language modelling datasets which focus on text data and are primarily derived from Common Crawl and a few other sources. PeS2o is a dataset focused on scientific writing and is primarily comprised of arXiv and journal papers. StarCoder is a dataset focused on code and is comprised of code scraped and filtered from github.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Filtering</head><p>Prior to deduplication, following common practice <ref type="bibr" target="#b5">(Gao et al., 2020;</ref><ref type="bibr" target="#b21">Rae et al., 2021)</ref>, we performed heuristic syntactic filtering for quality and to remove low-quality data, such as meaningless strings, large quantity of random numbers, as well as pornographic or otherwise objectionable content. Our filtering pipeline consisted of two stages: (1) substring replacement and (2) document-level filtering and removal.</p><p>For the first stage, we deployed regexes that replace certain substrings with more sanitized versions. This was primarily to fix common formatting issues that we noticed in our original analyses of the datasets. Examples included excessively long sequences of dashes, full-stops, '\r' characters, and other punctuation characters which presumably arose from quirks of formatting or the processing pipelines that lead to these documents. Such strings appeared with reasonable frequency and often were found amid otherwise unobjectionable documents, so we did not wish to simply remove documents in which they appeared.</p><p>Typically, we replaced large numbers of repeated characters Dataset Rows Initial (Millions) Rows Removed (Millions) Percent filtered RefinedWeb 968.000 21.350 2.21% SlimPajama 590.395 21.045 3.56% C4-EN 364.869 4.234 1.16% Pile-Uncopyrighted 177.010 21.536 12.17% peS2o 38.811 0.044 0.11% arXiv 1.672 0.189 11.33% Table 1: Number of document and percentage of all documents removed from each subcomponent dataset by our filtering process.</p><p>with a single or just a few characters. For instance, we replaced large numbers of linebreaks '\n' and carriage returns '\r' with just a single linebreak or carriage return. Similarly, for long sequences of dashes, we replaced them with a a single dash. For the second stage, we performed document-level filtering based on a set of syntactic heuristics which can be cheaply computed from the raw text of a document. If the threshold of the filter was exceeded, the whole document was removed from the dataset. These filters broadly fell into three categories: (1) removing syntactically broken or otherwise gibberish documents, (2) removing semantically meaningless documents, and (3) removing meaningful but objectionable content. Examples of the first kind of filter include methods like filtering based on the proportion of alphanumeric characters in a document, which at high proportions uniformly corresponds to documents comprised of entirely gibberish strings generated either through broken preprocessing or through unknown processes on the internet. Semantically meaningless documents include documents full of seemingly random numbers, cryptographic strings, and lists of seemingly unrelated URLs. Objectionable content included primarily pornographic and offensive content, which we removed with specialized word lists.</p><p>Our general philosophy in filtering was to not filter excessively and keep false positives (i.e. good documents removed) relatively low. We manually tested and tuned each of the filters presented here on the Pile dataset. We tuned the filter thresholds so that we obtained a false-positive threshold of about 20% -that is, 20% of the filtered documents were seemingly unobjectionable, while 80% were obviously harmful and it was correct to remove them. This provides a reasonable trade-off between excessive filtering while still removing the majority of content the filter was aimed at reducing. For every filter, we spent significant time manually tuning the threshold and other parameters, then looking at the filtered outputs and attempting to identify true vs false positives. We primarily performed this tuning on the Pile dataset and performed only sanity checking of the filter outputs on the other datasets. However, we believe that many of the categories of low-quality text likely have similar distributions between datasets, since so many of them are derived from the same source. A full list and description of all filters can be found in Appendix E.1. All datasets were filtered using the same rules except for StarCoder, which we exempted since it consists entirely of code which has a significantly different distribution than the primarily text data in the other datasets. Additionally, the StarCoder authors performed a thorough code-specific filtering of the dataset before release. Table <ref type="table">1</ref> shows the proportion and number of rows removed from each dataset by our filtering process. We observe that the primary source of data removed by our filters is the Pile and arXiv.  To identify duplicates we used Locality Sensitive Hashing (LSH) based on MinHash signatures <ref type="bibr" target="#b3">(Broder, 1997)</ref>. This technique allows fast approximate identification of duplicate candidates based on Jaccard similarity of sets of n-grams S n (•) in documents. For example, using MinHash(A) = {min h 1 (S n (A)), . . . , min h k (S n (A))} and MinHash(B) = {min h 1 (S n (B)), . . . , min h k (S B )} for k hash functions, we can measure the resemblance of documents A and B using:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Deduplication</head><formula xml:id="formula_0">1 k k i=1 1[min h i (S n (A)) = min h i (S n (B))].<label>(1)</label></formula><p>In the rest of the paper, we refer to LSH-x% as the LSH with at least x% resemblance, as expressed by Equation <ref type="formula" target="#formula_0">1</ref>. We deduplicated each dataset both against itself and against the other datasets in our full dataset. We built the LSH index by inserting the components in the following order: first Pileuncopyrighted, then C4-en, peS2o, arxiv_s2orc_parsed, RefinedWeb, SlimPajama, and finally StarCoder.</p><p>For our deduplication pipeline, we used 13-grams ‡ based on words to form our n-gram subsets and a minhash signature size of 128. Before generating 13-grams, we performed NFC normalization, conversion to lower case, and removal of punctuation and consecutive spaces, newlines, tabs in the middle and in the beginning and end of the strings. We performed deduplication at two Jaccard similarity thresholds: 40% and 80% (documents are considered duplicates if their similarity measure is equal or greater than the threshold). The parameters of LSH index were optimized to minimize the rates of false positives (FP) and false negatives (FN): for the 40% threshold, minhash indices were split into 32 bands each with a range of 4, while for the 80% threshold, we used 9 bands with a range of 13. From these parameters we can derive the following false-positive and false-negative rates: for 40% threshold, the false-negative rate is 3.4% and false-positive rate is 5.4%, for 80% threshold, the false-negative rate is 3.3% and false-positive rate is 2.5%. Table <ref type="table" target="#tab_1">2</ref> summarizes how many tokens (using gpt-neox tokenizer) were removed at different thresholds. Our raw dataset consists of 2T tokens, and we end up with 1.5T for 80% threshold and 1.3T for 40% threshold.</p><p>After identifying duplicate pairs using the LSH minhash technique, we clustered documents into a graph of connected components with the nodes being documents and the edges connecting duplicate pairs. We then kept only one document from the cluster while removing the rest. To determine which document to keep, we sorted the documents in the clusters by their dataset of origin and kept the highest-ranking one according to the following order: 1) StarCoder; 2) RefinedWeb; 3) peS2o; 4) arXiv; 5) C4; 6) Pile-uncopyrighted; 7) SlimPajama. We chose StarCoder as the highest-ranking dataset because it was specifically designed for code, so we hypothesized that any duplicate code snippets are likely to be of the highest quality from this source. We chose the rest of the ranking based on heuristic assessments of quality.</p><p>We then performed random sampling of duplicates in the clusters to manually explore examples. The largest clusters usually contained either short low-quality documents or documents with widely distributed texts, such as license agreements, advertisements, etc. We did notice that at 40% threshold LSH minhash was performing a qualitatively different kind of deduplication than at 80%: while at 80% most duplicates looked very similar, at 40% we started seeing duplicates across formats, ‡ We chose 13-gram based on what <ref type="bibr" target="#b5">(Gao et al., 2020;</ref><ref type="bibr" target="#b26">Soldaini et al., 2024)</ref> use, which is a common choice of n-gram. Other choices of n can be successfully applied to deduplicate text. For example, <ref type="bibr" target="#b19">(Penedo et al., 2023</ref><ref type="bibr" target="#b18">(Penedo et al., , 2024) )</ref>   especially between peS2o and arXiv components of our dataset, where we observed, for instance, the same paper but formatted in two different ways.</p><p>Since the LSH minhash algorithm only performs approximate deduplication, we also sampled 4.8 million duplicate pairs to estimate the actual false-positive rate based on Jaccard and edit similarities.</p><p>We define the edit similarity between two documents as their edit distance divided by maximum length of two documents. We found good agreements between theoretical and estimated false-positive rate based on Jaccard similarities, while the estimated false-positive rate based on edit similarity for the 40% threshold was even lower than the theoretical estimate at 3.1%. Figures <ref type="figure" target="#fig_2">2a</ref> and <ref type="figure" target="#fig_2">2b</ref> show the distribution of edit and Jaccard similarities for the 40% threshold version of our dataset: As expected, the vast majority of identified duplicates are above the threshold (marked as red dash line), and for the edit similarity metric, the distributions is skewed toward higher values (which is expected, since it corresponds to a lower false-positive rate). To compare the 40% and the 80% versions of Zyda, we trained 1.4B transformers on 50B tokens sampled from each version of Zyda. We found that 40% Zyda performed slightly better (Fig. <ref type="figure">5b</ref>). Due to this, we chose to release the 40% version as our primary Zyda dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Performance</head><p>We perform a series of training runs of small transformer models to assess the performance of our dataset vs alternatives such as Pile, and RefinedWeb, and Dolma. We first compare the performance of models trained on our dataset with the well-known Pythia suite of models <ref type="bibr" target="#b2">(Biderman et al., 2023)</ref>. We match the architecture, hyperparameters, and training process of the Pythia suite. We compare against the 410M Pythia model size. Like Pythia, we train for 300B tokens at this model scale. This provides a direct apples-to-apples comparison of Zyda against the Pile. We demonstrate that across the range of evals reported in the Pythia paper, that Zyda-trained models significantly outperforms the Pile. We especially observe gains in reasoning evaluations such as ARC and PIQA.</p><p>For a fair comparison, we ensure that these comparisons are equi-token. We note that Zyda contains 1.3T tokens, while the Pile contains only 300B, and thus we expect that models trained on the full Zyda dataset would substantially outperform models trained on the full Pile, such as Pythia, even more than we do so here.</p><p>Next we perform a preliminary scaling analysis of our dataset against the Pile and Pythia on a range of scales from 160M to 1.4B, all trained on the full Pile and 300B tokens of Zyda. We show that the advantage of Zyda appears to increase with scale. We suggest that this is because the increased dataset quality is not 'visible' to small models as they are capacity limited to only extract the larger modes of variance in the data, whereas larger models can additionally 'see' finer modes of variance where our increased dataset quality provides an advantage. A small model may simply be unable to  (b) The effect of deduplication threshold at either 40% or 80% similarity. We observe a slight advantage for 40% although this is not consistent across evaluation metrics.</p><p>Figure <ref type="figure">5</ref>: Comparison of Zyda with alternative datasets, and across deduplication LSH We first observe that Zyda at the 50B token stage performs slightly worse than RefinedWeb but still better than SlimPajama C4, and Dolma. A priori, this is not surprising since Zyda contains both of these as subsets. The performance decrease of Zyda (compared to RefinedWed) is attributed to the presence of StarCoder. With StarCoder removed, Zyda outperforms all other datasets and indeed performs better than any of its subsets -a surprising result which speaks to the importance of our filtering and deduplication pipeline. It appears that, at least at this scale, the competition between language and code introduced by having a significant fraction of code inhibits performance on standard language modelling evaluations. We find that a comparable open dataset -Dolma <ref type="bibr" target="#b26">Soldaini et al. (2024)</ref> -performs worse than both Zyda and most individual subsets of Zyda. It is also important to note that RefinedWeb only contains 600B tokens to Zyda's 1.3T and hence we anticipate Zyda would greatly outperform RefinedWeb if the model was trained to the end on each dataset. RefinedWeb is a strong and already well-filtered dataset which also forms a significant proportion of Zyda, and is thus a strong baseline. Zyda also contains a number of less performant datasets such as the Pile and C4. Despite this, Zyda's additional filtering and deduplication steps boost Zyda's performance such that the full dataset performs better than the raw version of its strongest subset. We can also detect this by noting that the raw Zyda dataset performs averagely at about the level of SlimPajama. Ultimately, Zyda appears to be a strong pretraining dataset at the 1T token scale containing only pretraining web data. Since it appears to decrease performance, we recommend removing StarCoder from Zyda for small models which need to be language focused instead of code focused since, at this scale, code appears to cause interference with the model's language abilities. The performance of Zyda has also been empirically validated through our language modelling effort Zamba <ref type="bibr" target="#b6">(Glorioso et al., 2024)</ref>, a 7B hybrid SSM-transformer which performs very strongly per pretraining token, and was trained on an early version of Zyda.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Concurrently with the rise of open-source large language models, there has been an increase in the number of open-source and widely available datasets for large-scale language model pretraining. Early works in this vein include C4 <ref type="bibr" target="#b22">(Raffel et al., 2020)</ref>, derived directly from Common Crawl, and The Pile <ref type="bibr" target="#b5">(Gao et al., 2020)</ref> which was manually constructed from a number of disparate sources. Other, more recently released large datasets such as SlimPajama <ref type="bibr" target="#b25">(Soboleva et al., 2023)</ref>, RefinedWeb <ref type="bibr" target="#b19">(Penedo et al., 2023)</ref> and Dolma <ref type="bibr" target="#b26">(Soldaini et al., 2024)</ref> have undergone more thorough filtering and deduplication -a process which we build upon in this work. We have also built upon works that described their filtering pipelines without releasing the dataset. Of especial use was Gopher <ref type="bibr" target="#b21">(Rae et al., 2021)</ref> and RedPajama (TogetherAI, 2023), which inspired a number of our own filters.</p><p>Most similar to our approach are RefinedWeb and Dolma. The RefinedWeb authors, as part of the Falcon LLM team, created a dataset of several trillion tokens using a filtering and deduplication pipeline similar to ours, but they only released publicly approximately 600B tokens of their full dataset. The Dolma dataset is also similar to our approach and releases several trillion tokens, derived from a mixture of Common Crawl and some existing datasets such as C4 and PeS2o, which are also included in our dataset. Hovewer, Dolma does not cross-deduplicate data between its component datasets -a process we found likely to be important since we identified many duplicate documents across datasets. We additionally find that Zyda outperforms Dolma at equi-token language modelling evaluations by a significant margin. Concurrently, FineWeb <ref type="bibr" target="#b18">(Penedo et al., 2024)</ref> was also released, which offers 15T tokens directly from Common Crawl under an open license. Since their Common Crawl datasets appear to come from a different source, it seems likely that Zyda and FineWeb can be productively merged. Zyda consolidates all the primary prior open-datasets into one place and subjects them to a uniform quality filtering and deduplication which can then be augmented with FineWeb's additional Common Crawl tokens. Additionally, unlike these works which directly filter from Common Crawl, we aim to instead collect and collate existing open-source datasets under one banner, and to uniformly filter and deduplicate them against one another so as to create a dataset of known high-quality text which can be used to perform training at trillion-token scales. We hope that this seed dataset can then be expanded as additional datasets are released by the open-source community.</p><p>We built our deduplication pipeline upon SlimPajama's existing open-source libraries, although we ended up significantly modifying them and performed extensive manual tuning and optimization. Our initial filters were inspired by the filter list accompanying the RedPajama dataset. However, after preliminary testing, we removed some of their filters which we judged were ineffective and integrated new ones, either from other papers such as C4, Gopher, and MADLAD-400 <ref type="bibr" target="#b13">(Kudugunta et al., 2024)</ref>, or of our own invention. A full list and description of the filters we used, as well as qualitative impressions gained during testing them, can be found in Appendix E.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>In this paper, we have presented Zyda, a unified dataset released under a permissive license, comprising most of the largest and highest quality existing open-source datasets available. Upon these, we have performed extensive additional filtering, beyond what was originally applied, in addition to thorough intra-and inter-dataset deduplication. Our aim with this work is to create a growing and extendable dataset which can be easily used by practitioners to train trillion-token scale language models while also consolidating and unifying the efforts made by disparate open-source groups who have released their datasets. Ultimately, we hope that our work can provide an "off-the-shelf" accessible trillion-scale high-quality pretraining dataset which can be used by groups aiming to pretrain their own LLMs.</p><p>While not performing new collection from Common Crawl, we believe that our work is an important and valuable step towards the creation of large scale high quality open datasets given that there exist a number of high quality existing datasets but few to none of them individually reach the scale necessary for training state-of-the-art models. Collating, filtering, and deduplicating the existing datasets needed to create a trillion token dataset is nontrivial work and extremely important to raise the quality of the dataset and prevent significant amounts of inter-dataset duplicates. This latter operation of deduplication between datasets is extremely important given the degree of duplicated documents we discovered in common open-source datasets.</p><p>Ultimately, we believe that this dataset is only the first step towards building an open dataset competitive with state-of-the-art models at the largest scales. While our dataset combines the primary filtered open-source datasets available, the datasets used to train the largest models today are already an order of magnitude larger. The recent release of FineWeb <ref type="bibr" target="#b18">(Penedo et al., 2024)</ref> which contains 15T Common Crawl tokens is exciting and effectively removes the first bottleneck -sourcing the tokens -to the creation of large effective open datasets to LLM pretraining, yet likely the best proprietary datasets still retain significant quality advantages. While many of the secrets to high quality filtering remain hidden behind the veil of the largest labs, there is now a growing amount of academic work showing methods for how dataset quality can be significantly improved beyond the simple filtering and deduplication pipeline we have implemented here. We believe that a strong pretraining mixture can likely be created by merging Zyda and FineWeb together which can be used to approach the training token count of the largest closed datasets.</p><p>While we have performed thorough filtering and deduplication to remove low quality documents and many duplicates, we have only yet taken the first step in improving dataset quality. A large number of methods can still be employed to further improve its quality, at the cost of greater compute requirements. Such methods include training semantic classifiers to detect high or low quality data <ref type="bibr" target="#b34">(Xie et al., 2023;</ref><ref type="bibr" target="#b9">Ilyas et al., 2022)</ref>, performing passes over the dataset with language models to filter based on the perplexity of the data <ref type="bibr" target="#b17">(Marion et al., 2023)</ref>, performing clustering on the dataset to remove outlier or repetitive data <ref type="bibr" target="#b30">(Tirumala et al., 2024)</ref>, augmenting the original data with synthetic or rephrased data <ref type="bibr" target="#b16">(Maini et al., 2024)</ref>, and many other approaches. We hope that future work explores these and other avenues and continues to extend and improve the underlying open datasets available for pretraining so as to push forward the frontier of performance accessible with open datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Limitations</head><p>While we believe Zyda is a highly effective dataset for language modelling, and this is supported by our ablation studies, and our empirical successes with the Zamba model <ref type="bibr" target="#b6">(Glorioso et al., 2024)</ref>. However, except our ablations comparisons against the Pythia suite, all our other ablations were only trained on 50 billion tokens which may not be enough to predict dataset quality over the full course of training over one or more epochs on the dataset. We limited our ablations to 50B tokens on a 1.4B model primarily out of compute and cost considerations. Additionally, while we performed a small scaling suite for the Pythia models, we did not perform this for other ablations, focusing on the 1.4B scale. It is possible that the scaling properties of different datasets differ so tht some datasets may perform better with larger or smaller models, and we cannot discount that this could change the rank ordering of datasets by performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Ablation experimental details</head><p>For our 50B ablations, we used a 1.4B standard transformer model. The architecture and trainng hyperparameters are presented in Table <ref type="table" target="#tab_2">3</ref>. We used the Swiglu activation function on the MLP layers and trained with RoPE position embeddings. For the Pythia comparisons, we trained on 300B tokens (the same size as full Pile), using the model and training hyperparameters reported in <ref type="bibr" target="#b2">Biderman et al. (2023)</ref>.</p><p>All models were trained on Nvidia H100 DGX systems. Most ablations were run on 1-2 DGX nodes. We used the MegatronLM <ref type="bibr" target="#b24">(Shoeybi et al., 2019)</ref> framework for training the models used in these ablation studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Societal impacts</head><p>We release Zyda with the aim of providing a dataset to make language model training at the trilliontoken scale more accessible and effective. Empowering disparate and less-resourced groups to create such language models may have a variety of societal impacts. While enabling more actors to train effective language models may lead to greater decentralization of power and more equal access to the technology, it also opens the door to potential misuse. While we have endeavoured with our filtering to remove harmful and personally identifying content from Zyda, we cannot guarantee that Zyda cannot be used to train language models which could cause harm.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Ablation performance by evals</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Additional details for dataset processing E.1 Additional Filtering Details</head><p>In this Appendix, we describe the filters that were implemented in our filtering pipeline as well as additional filters that we experimented with which we did not find useful. We also present a number of qualitative insights we found while exploring Zyda in depth during our manual analysis and tuning of the filters.</p><p>The filters that were used in the final pipeline were as follows:</p><p>• Filter by min length: This filtered out all documents shorter than a given length (we choose 100 characters). In practice, almost all documents shorter than this length consisted of short strings of broken formatting, ads and spam, single links, or unintelligble broken code snippets (primarily client-side javascript and sometimes CSS). This filter appeared highly effective with few false positives.</p><p>• Filtering by max mean word length: This filter removes documents which have too large a proportion of words of exceptional length. We found that this primarily removed short code snippets with verbose variables in CamelCase as well as malformed documents without spaces or with exceptionally long unusual strings such as hashes, cryptographic keys, and general weirdness.</p><p>• Filtering by minimum mean word length: This filter removed documents which too large a proportion of words with too short a length. We found in practice that this tended to remove short documents which were 'broken' in some sense -usually consisting of either individual random characters or short bursts of random gibberish characters.</p><p>• Filtering by fraction of alphanumeric characters: This filter primarily removed documents consisting of non-alphanumeric characters. In practice, almost all such documents appeared to be gibberish with small fractions of unobjectionable code as false positives. We do not believe that this filter is suitable for full code datasets.</p><p>• Filter by fraction numerical: This filter removed documents consisting primarily of numbers. We detected a large number of such documents, most of which appeared to be stored data from various souces, most without any kind of context which would make it easy for a language model to understand what the data is meant to represent.</p><p>• Filtering by fraction 'xml': This filter removed egregious xml documents which primarily appeared to be endless sequences of xml tags with little actual content.</p><p>• Filter by lorem ipsum: This filter removed lorem ipsum text, of which we found a small amount in the Pile.</p><p>• Filter by 'http://' and 'https://' and 'www.': this filter removed pages with too high a fraction of website links. In practice, this removed documents which either consisted entirely of a single random URL (which could be long enough to survive our min length filter), or else documents with large proportions of URLs. Such documents were almost entirely ads or spam of various kinds, or else isolated links bars from websites. This filter appeared highly effective at removing such content in practice.</p><p>• Filter by fraction '&lt;' and '&gt;': This filter removed a small amounts of documents consisting almost entirely of these symbols, as well as many poorly parsed webpages with truly excessive amounts of HTML tags without much appreciable content.</p><p>• Filtering by ':': This was used to filter for the opening of a python dictionary or JSON key or value. We used this to remove documents that were composed of an excessive fraction of JSON data. We found that in practice such documents primarily consisted of either unintelligble data or extremely repetitive CSS UI attributes.</p><p>• Filtering by pornographic word list: We found that using a relatively simple word list filtered out a large quantity of pornographic content from the Pile. This mostly consisted of adverts or titles from porn sites with a dash of porn ads. It also filtered out a fair amount of erotica fiction which may not necessarily be low quality. After experimenting with existing approaches, we elected to create a custom porn filter based only on a few keywords which can run fast through the dataset. We then filter both on the absolute number of such terms appearing in the document as well as on the fraction of the document comprised of these terms. We found that approximately 0.5% of The Pile appears to be pornographic content. • Filter by profanity word list: This filtered out documents based on a high proportion of common (English) profanity. We found that this primarily filtered out short documents consisting of low quality reddit comments or other social media postings. Notably, almost all documents removed by this filter were extremely short. • Filter by Chinese porn signals: This filter was derived from <ref type="bibr" target="#b13">Kudugunta et al. (2024)</ref> and used a special word-list that they had derived. Upon inspection this did appear to filter out mostly chinese text and included very little English. However we did not analyze this in depth as we focused on English monolingual language modelling. • Filter by cursed string fraction: We filtered using the 'cursed regex' of <ref type="bibr" target="#b13">Kudugunta et al. (2024)</ref> which they claimed captured many undesirable strings. We found that this primarily filtered for ads, spam, and lorem ipsum text. This filter required quite a bit of manual tuning as the settings described in the paper were too aggressive -e.g. removing any line with the word 'Facebook', or 'download' -which clearly matches to many perfectly unobjectionable texts.</p><p>There was also a number of filters that we tried and found ineffective, including a number of recommendations from prior works. These include:</p><p>• Filtering by max length: We found that this was actively unhelpful and that almost all long documents appeared high quality. Usually they were either full books, or else concatenated encyclopedia entries or legal cases. We suspect that such data is actually high quality and helpful for the model. • Filtering by fraction of upper case characters, as suggested in RedPajama. We found that while this filter caught some spam it also caught large amounts of unobjectionable text. Such text included notices and long-form advertisements from news papers, legal texts, and text which just seemed to be randomly have been converted to upper case. Since we could not find a threshold which had a sufficiently low false-positive level while also filtering out an appreciable number of documents, we ended up dropping this filter. • We found that removing all documents with many "{" as done in <ref type="bibr" target="#b13">Kudugunta et al. (2024)</ref> and with sentences ended without punctuation <ref type="bibr" target="#b22">Raffel et al. (2020)</ref> primarily removed code which was otherwise non-objectionable. • We found that removing documents without at least 2 standard english words such as "the, of" etc as proposed in <ref type="bibr" target="#b21">Rae et al. (2021)</ref> tended to remove large fractions of otherwise unobjectionable code • We found that filtering based on a large number of repeated characters was not effective and had many false positives. This usually meant either texts with many 0s in it -for instance numbers reported to a high degree of precision, and good text just with messed up formatting -i.e. many repeating dashes or slashes or tabs. There were not many files with extremely large amounts of repeating characters that we found. For instance, we found many legal cases with extremely large numbers of dashes, tabs, carriage-returns and xa0 characters which we filtered based on. Instead we opted to replace many repeated characters with a much smaller number of the same character. We hope this fixed many elements of the formatting without removing whole documents of good text which just happened to have been badly formatted during text extraction. • We found that filtering by PII count was not particularly useful. We found that filtering by IP addresses as done in RefinedWeb <ref type="bibr" target="#b19">(Penedo et al., 2023)</ref> primarily removes tutorial networking code using example IP addresses which seems likely to be helpful for the model. Filtering aggressively by email and phone numbers tends to remove large numbers of otherwise useful social media and blog postings, however less aggressive filters which require a large fraction of such PII does remove some ads and spam. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Duplicates sources</head><p>We computed the distribution of sources of duplicates across the components (see Figure <ref type="figure" target="#fig_8">7</ref>). On Figure <ref type="figure" target="#fig_8">7</ref> we plot the proportion of duplicates that are coming from different datasets: e.g., for Pile we identified roughly 90 million documents duplicated in the Pile itself and other datasets, and the figure tells us that 35% of those are coming from itself, 24% from StarCoder, 17% from SlimPajama, and 23% from the rest of the components. We observe extremely high number of duplicates in arXiv are coming from peS2o since they have the same provenance. We also observe that our approach correctly identified a lot of documents in SlimPajama are present in C4, since C4 was included in RedPajama (the parent dataset of SlimPajama  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4 Estimating empirical false positive rate</head><p>To estimate the true false positive rate beyond the theoretical calculation, we sampled 4.8 million duplicates pairs (see Figure <ref type="figure" target="#fig_9">8a</ref> for the dustribution of lengths), and computed their empirical edit and Jaccard similarities, as well as manually inspected a number of examples. On Figure <ref type="figure" target="#fig_9">8b</ref> we plot a dependence on document length of what we call a cumulative FP rate: given length l, cumulative FP rate is the FP rate amoung duplicates for which the length is at most l. We found that the length of the document had a strong effect on the proportion of false positives with short documents being more likely to be categorized as false positives than longer ones. This is likely due to our use of 13-grams for similarity matching as well as the fact that noise fluctuations in similarity are of large scale relative to the threshold at small lengths.</p><p>(a) Cumulative distribution function of duplicate documents lengths on a log scale. Red dashed lines correspond to median length. We observe that the vast majority of documents tend to sit between 10 2 and 10 4 words in length.</p><p>(b) Cumulative FP rate as a function of duplicate document length. The red dashed line corresponds to the theoretical 3.1% FP rate computed on all pairs of the sample. We observe that for short documents the empirical and theoretical FP rates diverge before converging to the theoretical prediction for long documents </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The proportion of different dataset subsets in Zyda. The primary proportion is RefinedWeb, followed by SlimPajama and StarCoder.</figDesc><graphic coords="3,167.40,72.00,277.18,162.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><figDesc>of edit similarity distance of selected duplicate documents, red dash line marks 40% threshold.(b) Distribution of Jaccard similarity distance of selected duplicate documents, red dash line marks 40% threshold.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Document similarity distances for Zyda dataset</figDesc><graphic coords="4,108.00,469.81,198.00,148.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Aggregate evaluation scores across training steps for a 1.4B model trained on 50B tokens of Zyda and comparable datasets. Zyda and especially Zyda without starcoder outperform strong open baselines such as FineWeb, and Dolma. Aggregate scores is the mean of arc-challenge, arc-easy, boolq, openbookqa, piqa, sciq, and winogrande. Scores smoothed using a window size of 5.</figDesc><graphic coords="6,167.40,72.00,277.20,166.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><figDesc>(a) Aggregate evaluation performance of models trained on Zyda versus Pile. Aggregate score is the mean of the evaluation scores for ARC-c, ARC-e, LogiQA, PIQA, SciQ, and WinoGrande (b) Evaluation scores for 1.4B Pythia trained on Zyda and Pile.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: We match the Pythia suite in architecture and training hyperparameters. We observe that Zyda outperforms Pile on evaluations and that this advantage increases with scale, which we believe to be due to reduced noise on standard evals as model performance improves. All models were trained for 300B tokens on either Pile or Zyda.</figDesc><graphic coords="7,108.00,72.00,194.04,145.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Scores broken out by specific evaluation of our ablation studies comparing Zyda to alternative datasets.</figDesc><graphic coords="14,108.00,476.39,178.20,106.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Proportion of duplications for all datasets for LSH 40%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Statistics of the sample of 4.8 million duplicate documents.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>successfully use 5-grams for deduplication Initial</figDesc><table><row><cell>Dataset</cell><cell>Raw Tokens</cell><cell cols="2">LSH 80% tokens LSH 40% tokens</cell></row><row><cell>RefinedWeb</cell><cell>579,958,873,663</cell><cell>569,412,954,030</cell><cell>564,793,074,285</cell></row><row><cell>SlimPajama</cell><cell>605,453,814,327</cell><cell>355,300,814,841</cell><cell>242,280,662,390</cell></row><row><cell>C4-en</cell><cell>174,402,215,716</cell><cell>156,848,709,800</cell><cell>117,511,570,406</cell></row><row><cell>Pile-Uncopyrighted</cell><cell>253,107,739,551</cell><cell>115,280,920,159</cell><cell>82,918,673,353</cell></row><row><cell>peS2o</cell><cell>57,171,106,720</cell><cell>56,264,262,881</cell><cell>53,376,291,935</cell></row><row><cell>arXiv</cell><cell>24,261,321,215</cell><cell>19,523,122,996</cell><cell>4,703,637,744</cell></row><row><cell>StarCoder</cell><cell>260,865,965,654</cell><cell>257,327,664,673</cell><cell>231,285,458,516</cell></row><row><cell>Total:</cell><cell cols="3">1,955,221,036,846 1,529,958,449,380 1,296,869,368,629</cell></row></table><note><p>and final number of tokens before and after filtering and deduplication with two thresholds (80% and 40%)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Model and training hyperparameters for the 1.4B transformer models trained in our ablation experiments.</figDesc><table><row><cell>Hyperparameter</cell><cell>Value</cell></row><row><cell>Number of Layers</cell><cell>24</cell></row><row><cell>Hidden Dimension</cell><cell>248</cell></row><row><cell>Number of Attention Heads</cell><cell>16</cell></row><row><cell>Batch Size</cell><cell>512</cell></row><row><cell>Max Learning Rage</cell><cell>2.0e-4</cell></row><row><cell>LR Decay Schedule</cell><cell>Cosine</cell></row><row><cell>Minimum LR</cell><cell>2.0e-5</cell></row><row><cell>Weight Decay</cell><cell>0.1</cell></row><row><cell>Adam Beta2</cell><cell>0.95</cell></row><row><cell>LR Warmup</cell><cell>0.01</cell></row><row><cell>Gradient Clipping</cell><cell>1.0</cell></row><row><cell>Training Precision</cell><cell>BF16</cell></row><row><cell>Activation Function</cell><cell>Swiglu</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Filtering counts for pile-uncopyrighted</figDesc><table><row><cell>E.2 Number of documents removed by each filter per dataset</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>). Big proportion of duplicates in RefinedWeb, C4 overlap with each other probably due the fact that they are derivative of Common Crawl.Filtering counts for C4-EN</figDesc><table><row><cell>Filter</cell><cell>Num Documents Removed</cell></row><row><cell>min_length</cell><cell>2,090,400</cell></row><row><cell>min_mean_word_length</cell><cell>570,427</cell></row><row><cell>max_fraction for pattern https://</cell><cell>537,657</cell></row><row><cell>max_fraction for word list cursed_substrings.json</cell><cell>361,818</cell></row><row><cell>max_mean_word_length</cell><cell>288,769</cell></row><row><cell>max_fraction_numerical</cell><cell>206,738</cell></row><row><cell>max_PII_items_count</cell><cell>79,543</cell></row><row><cell>max_fraction for word list profanity_word_list.json</cell><cell>34,400</cell></row><row><cell>max_fraction for pattern ":</cell><cell>23,955</cell></row><row><cell>max_fraction for pattern www.</cell><cell>15,789</cell></row><row><cell>max_fraction_non_alphanumeric</cell><cell>13,111</cell></row><row><cell>max_count for pattern xml</cell><cell>3,411</cell></row><row><cell>max_count for word list zh_pornsignals.json</cell><cell>2,933</cell></row><row><cell>max_fraction for word list sexual_word_list.json</cell><cell>2,528</cell></row><row><cell>max_fraction for pattern &lt;</cell><cell>2,512</cell></row><row><cell>max_repeated_substrings</cell><cell>418</cell></row><row><cell>max_count for pattern &lt;?xml version=</cell><cell>86</cell></row><row><cell>max_count for word list sexual_word_list.json</cell><cell>2</cell></row><row><cell>Filter</cell><cell>Num Documents Removed</cell></row><row><cell>max_fraction for word list sexual_word_list.json</cell><cell>19,749</cell></row><row><cell>max_PII_items_count</cell><cell>7,570</cell></row><row><cell>min_mean_word_length</cell><cell>7,366</cell></row><row><cell>max_count for word list sexual_word_list.json</cell><cell>4,833</cell></row><row><cell>max_fraction for pattern https://</cell><cell>1,314</cell></row><row><cell>max_fraction for pattern &lt;</cell><cell>1,076</cell></row><row><cell>max_fraction for pattern ":</cell><cell>698</cell></row><row><cell>max_fraction for word list cursed_substrings.json</cell><cell>669</cell></row><row><cell>max_repeated_substrings</cell><cell>205</cell></row><row><cell>max_count for pattern xml</cell><cell>151</cell></row><row><cell>max_mean_word_length</cell><cell>139</cell></row><row><cell>max_count for word list zh_pornsignals.json</cell><cell>137</cell></row><row><cell>max_count for pattern &lt;?xml version=</cell><cell>25</cell></row><row><cell>max_fraction for word list profanity_word_list.json</cell><cell>21</cell></row><row><cell>max_fraction_numerical</cell><cell>17</cell></row><row><cell>max_fraction_non_alphanumeric</cell><cell>6</cell></row><row><cell>max_count for pattern lorem ipsum</cell><cell>2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Filtering counts for peS2o</figDesc><table><row><cell>Filter</cell><cell>Num Documents Removed</cell></row><row><cell>min_mean_word_length</cell><cell>175,161</cell></row><row><cell>max_PII_items_count</cell><cell>11,901</cell></row><row><cell>max_repeated_substrings</cell><cell>752</cell></row><row><cell>max_fraction for word list cursed_substrings.json</cell><cell>615</cell></row><row><cell>max_fraction_numerical</cell><cell>502</cell></row><row><cell>max_count for pattern xml</cell><cell>166</cell></row><row><cell>max_count for word list zh_pornsignals.json</cell><cell>113</cell></row><row><cell>max_count for word list sexual_word_list.json</cell><cell>67</cell></row><row><cell>max_fraction for pattern ":</cell><cell>32</cell></row><row><cell>max_count for pattern &lt;?xml version=</cell><cell></cell></row><row><cell>max_fraction for pattern https://</cell><cell>24</cell></row><row><cell>max_mean_word_length</cell><cell>18</cell></row><row><cell>max_fraction_non_alphanumeric</cell><cell>15</cell></row><row><cell>min_length</cell><cell>2</cell></row><row><cell>max_count for pattern lorem ipsum</cell><cell>2</cell></row><row><cell>max_fraction for word list sexual_word_list.json</cell><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Filtering counts for arXiv</figDesc><table><row><cell>Filter</cell><cell>Num Documents Removed</cell></row><row><cell>min_length</cell><cell>11,527,798</cell></row><row><cell>min_mean_word_length</cell><cell>3,085,862</cell></row><row><cell>max_fraction for word list sexual_word_list.json</cell><cell>2,037,579</cell></row><row><cell>max_fraction_numerical</cell><cell>1,436,988</cell></row><row><cell>max_PII_items_count</cell><cell>826,732</cell></row><row><cell>max_fraction for word list cursed_substrings.json</cell><cell>794,444</cell></row><row><cell>max_count for word list sexual_word_list.json</cell><cell>492,413</cell></row><row><cell>max_count for word list zh_pornsignals.json</cell><cell>357,466</cell></row><row><cell>max_fraction for pattern ":</cell><cell>286,229</cell></row><row><cell>max_fraction for word list profanity_word_list.json</cell><cell>260,606</cell></row><row><cell>max_fraction for pattern &lt;</cell><cell>140,598</cell></row><row><cell>max_mean_word_length</cell><cell>28,146</cell></row><row><cell>max_fraction_non_alphanumeric</cell><cell>28,036</cell></row><row><cell>max_count for pattern xml</cell><cell>24,755</cell></row><row><cell>max_fraction for pattern https://</cell><cell>7,807</cell></row><row><cell>max_count for pattern &lt;?xml version=</cell><cell>7,381</cell></row><row><cell>max_repeated_substrings</cell><cell>3,090</cell></row><row><cell>max_fraction for pattern www.</cell><cell>2,686</cell></row><row><cell>max_count for pattern lorem ipsum</cell><cell>1,061</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Filtering counts for RefinedWeb</figDesc><table><row><cell>Filter</cell><cell>Num Documents Removed</cell></row><row><cell>min_mean_word_length</cell><cell>14,682,212</cell></row><row><cell>max_fraction for pattern https://</cell><cell>1,808,881</cell></row><row><cell>min_length</cell><cell>847,055</cell></row><row><cell>max_fraction for pattern &lt;</cell><cell>704,490</cell></row><row><cell>max_fraction_numerical</cell><cell>689,546</cell></row><row><cell>max_fraction for pattern ":</cell><cell>66,8174</cell></row><row><cell>max_PII_items_count</cell><cell>570,952</cell></row><row><cell>max_mean_word_length</cell><cell>348,773</cell></row><row><cell>max_fraction for word list cursed_substrings.json</cell><cell>286,527</cell></row><row><cell>max_count for pattern xml</cell><cell>143,725</cell></row><row><cell>max_fraction_non_alphanumeric</cell><cell>112,351</cell></row><row><cell>max_count for pattern &lt;?xml version=</cell><cell>43,069</cell></row><row><cell>max_count for word list sexual_word_list.json</cell><cell>40,855</cell></row><row><cell>max_fraction for word list sexual_word_list.json</cell><cell>30,420</cell></row><row><cell>max_fraction for word list profanity_word_list.json</cell><cell>28,863</cell></row><row><cell>max_fraction for pattern www.</cell><cell>18,936</cell></row><row><cell>max_count for word list zh_pornsignals.json</cell><cell>10,128</cell></row><row><cell>max_repeated_substrings</cell><cell>6,555</cell></row><row><cell>max_count for pattern lorem ipsum</cell><cell>3,231</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Filtering counts for SlimPajama</figDesc><table><row><cell>Duplicate Proportion</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>† https://commoncrawl.org/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Semdedup: Data-efficient learning at web-scale through semantic deduplication</title>
		<author>
			<persName><forename type="first">A</forename><surname>Abbas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tirumala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Simig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Morcos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.09540</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Akkaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Aleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Altenschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Anadkat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.08774</idno>
		<title level="m">Gpt-4 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pythia: A suite for analyzing large language models across training and scaling</title>
		<author>
			<persName><forename type="first">S</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schoelkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">G</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>O'brien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hallahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">S</forename><surname>Prashanth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Raff</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2397" to="2430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the resemblance and containment of documents</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Z</forename><surname>Broder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. Compression and Complexity of SEQUENCES 1997</title>
		<meeting>Compression and Complexity of SEQUENCES 1997</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
	<note>Cat. No. 97TB100171</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">What&apos;s in my big data?</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Elazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bhagia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Magnusson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ravichander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Suhr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Walsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Groeneveld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Soldaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nabeshima</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00027</idno>
	</analytic>
	<monogr>
		<title level="m">The pile: An 800gb dataset of diverse text for language modeling</title>
		<imprint>
			<date type="published" when="2020">2023. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Glorioso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tokpanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Whittington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pilault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Millidge</surname></persName>
		</author>
		<title level="m">Zamba: A compact 7b ssm hybrid model</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Hestness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ardalani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kianinejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M A</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.00409</idno>
		<title level="m">Deep learning scaling is predictable, empirically</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D L</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.15556</idno>
		<title level="m">Training compute-optimal large language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Leclerc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.00622</idno>
		<title level="m">Datamodels: Predicting predictions from training data</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bamford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Chaplot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bressand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lengyel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Saulnier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.06825</idno>
		<title level="m">Mistral 7b</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08361</idno>
		<title level="m">Scaling laws for neural language models</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Kenney</surname></persName>
		</author>
		<ptr target="https://huggingface.co/datasets/ArtifactAI/arxiv_s2orc_parsed" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Madlad-400: A multilingual and document-level large audited dataset</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kudugunta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Caswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ippolito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nystrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.06499</idno>
		<title level="m">Deduplicating training data makes language models better</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">B</forename><surname>Allal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kocetkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Akiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.06161</idno>
		<title level="m">Starcoder: may the source be with you! arXiv preprint</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Maini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.16380</idno>
		<title level="m">Rephrasing the web: A recipe for compute and data-efficient language modeling</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">When less is more: Investigating data pruning for pretraining llms at scale</title>
		<author>
			<persName><forename type="first">M</forename><surname>Marion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Üstün</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pozzobon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fadaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hooker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.04564.MosaicML</idno>
		<ptr target="https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm" />
		<imprint>
			<date type="published" when="2023">2023. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Introducing dbrx: A new state-of-the-art open llm</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">G</forename><surname>Penedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kydlíček</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Von Werra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<ptr target="https://huggingface.co/datasets/HuggingFaceFW/fineweb" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Penedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Malartic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hesslow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cojocaru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cappelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Alobeidli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pannier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Almazrouei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Launay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.01116</idno>
		<title level="m">The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aslanides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Young</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.11446</idno>
		<title level="m">Scaling language models: Methods, analysis &amp; insights from training gopher</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Compute trends across three eras of machine learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sevilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Heim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Besiroglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hobbhahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Villalobos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08053</idno>
		<title level="m">Megatronlm: Training multi-billion parameter language models using model parallelism</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Slimpajama: A 627b token cleaned and deduplicated version of redpajama</title>
		<author>
			<persName><forename type="first">D</forename><surname>Soboleva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Al-Khateeb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Steeves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hestness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Soldaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bhagia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Authur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bogin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chandu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dumas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Elazar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.00159</idno>
		<title level="m">Dolma: An open corpus of three trillion tokens for language model pretraining research</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">peS2o (Pretraining Efficiently on S2ORC) Dataset</title>
		<author>
			<persName><forename type="first">L</forename><surname>Soldaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lo</surname></persName>
		</author>
		<ptr target="https://github.com/allenai/pes2o" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
		<respStmt>
			<orgName>Allen Institute for AI. ODC-By</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Team</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schalkwyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hauth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.11805</idno>
		<title level="m">Gemini: a family of highly capable multimodal models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Team</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mesnard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hardin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dadashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhupatiraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rivière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Love</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.08295</idno>
		<title level="m">Gemma: Open models based on gemini research and technology</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Redpajama-data-v2: An open dataset with 30 trillion tokens for training large language models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tirumala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Simig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Morcos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems, 36. TogetherAI</title>
		<imprint>
			<date type="published" when="2023">2024. 2023</date>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
	<note>Improving llm pretraining via document de-duplication and diversification</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhosale</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.09288</idno>
		<title level="m">Llama 2: Open foundation and fine-tuned chat models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Doremi: Optimizing data mixtures speeds up language model pretraining</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Author Contributions Yury -Led dataset processing and deduplication pipeline. Led ablation and performance experiments</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="34201" to="34227" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>Data selection for language models via importance resampling Contributed to paper writing, figures, and edits</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Led dataset filtering pipeline. Contributed to project conceptualization</title>
		<author>
			<persName><surname>Beren</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Wrote primary draft of paper and contributed to figures and edits</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jonathan</forename></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Contributed to paper writing and edited the paper draft. Adam -Contributed to the paper draft</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName><surname>James</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Contributed comments to the paper draft</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Quentin -Contributed to paper writing, creating figures, and edits</title>
		<imprint/>
	</monogr>
	<note>Contributed to project conceptualization</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
