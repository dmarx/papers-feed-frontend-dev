{
  "arxivId": "2406.01981",
  "title": "Zyda: A 1.3T Dataset for Open Language Modeling",
  "authors": "Yury Tokpanov, Beren Millidge, Paolo Glorioso, Jonathan Pilault, Adam Ibrahim, James Whittington, Quentin Anthony",
  "abstract": "The size of large language models (LLMs) has scaled dramatically in recent\nyears and their computational and data requirements have surged\ncorrespondingly. State-of-the-art language models, even at relatively smaller\nsizes, typically require training on at least a trillion tokens. This rapid\nadvancement has eclipsed the growth of open-source datasets available for\nlarge-scale LLM pretraining. In this paper, we introduce Zyda (Zyphra Dataset),\na dataset under a permissive license comprising 1.3 trillion tokens, assembled\nby integrating several major respected open-source datasets into a single,\nhigh-quality corpus. We apply rigorous filtering and deduplication processes,\nboth within and across datasets, to maintain and enhance the quality derived\nfrom the original datasets. Our evaluations show that Zyda not only competes\nfavorably with other open datasets like Dolma, FineWeb, and RefinedWeb, but\nalso substantially improves the performance of comparable models from the\nPythia suite. Our rigorous data processing methods significantly enhance Zyda's\neffectiveness, outperforming even the best of its constituent datasets when\nused independently.",
  "url": "https://arxiv.org/abs/2406.01981",
  "issue_number": 641,
  "issue_url": "https://github.com/dmarx/papers-feed/issues/641",
  "created_at": "2025-01-04T06:52:48.608606",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 8,
  "last_read": "2025-01-04T06:53:18.650339",
  "last_visited": "2024-12-30T20:04:46.858Z",
  "main_tex_file": null,
  "published_date": "2024-06-04T05:47:17Z",
  "arxiv_tags": [
    "cs.CL",
    "cs.AI"
  ]
}