# Enforcing Demographic Coherence: A Harms Aware Framework for Reasoning about Private Data Release

## Abstract

## 

The technical literature about data privacy largely consists of two complementary approaches: formal definitions of conditions sufficient for privacy preservation and attacks that demonstrate privacy breaches. Differential privacy is an accepted standard in the former sphere. However, differential privacy's powerful adversarial model and worst-case guarantees may make it too stringent in some situations, especially when achieving it comes at a significant cost to data utility. Meanwhile, privacy attacks aim to expose real and worrying privacy risks associated with existing data release processes but often face criticism for being unrealistic. Moreover, the literature on attacks generally does not identify what properties are necessary to defend against them.

We address the gap between these approaches by introducing demographic coherence, a condition inspired by privacy attacks that we argue is necessary for data privacy. This condition captures privacy violations arising from inferences about individuals that are incoherent with respect to the demographic patterns in the data. Our framework focuses on confidence rated predictors, which can in turn be distilled from almost any data-informed process. Thus, we capture privacy threats that exist even when no attack is explicitly being carried out. Our framework not only provides a condition with respect to which data release algorithms can be analysed but suggests natural experimental evaluation methodologies that could be used to build practical intuition and make tangible assessment of risks. Finally, we argue that demographic coherence is weaker than differential privacy: we prove that every differentially private data release is also demographically coherent, and that there are demographically coherent algorithms which are not differentially private.

## Introduction

The collection of data and dissemination of aggregated statistics is a key function of government and civil society, driving critical data-driven decision making processes, e.g., democratic apportionment, collective resource allocation, and documenting ongoing social ills. Indeed, data has become an indispensable modern tool for producing knowledge. However, the collection of personal data-particularly mass scale collection-introduces the potential for the inappropriate disclosure of information that individuals might prefer to remain private. Thus, data curators must carefully apply privacy protection mechanisms to their data, ideally without compromising the utility of the eventual data release. The study of privacy preserving data releases started with attacks that compellingly demonstrated that data releases which had not taken steps to ensure privacy could be weaponized for harm, e.g., Latanya Sweeney's infamous re-identification of the Massachusetts Governor Bill Weld's medical records [[52]](#b47). Since then, there has been a robust literature describing increasingly sophisticated attacks which continue to motivate efforts towards privacy preserving data release [[40,](#b35)[10,](#)[9,](#)[12]](#). While these attacks have proved convincing enough to shift data protection practices in many fields, attack demonstrations do not provide a clear path towards designing data protection mechanisms themselves, even in the form of "prevent all attacks like this one"; the attack demonstrations do not take on the task of distilling a set of agreed upon properties that make the attack convincing. In the aftermath of these attacks, the research community has developed a set of formal approaches that aim to provide robust privacy guarantees. While early attempts, like k-anonymity, proved inadequate, differential privacy [[24]](#b19) has recently emerged as an accepted standard, seeing deployment in both industry [[27,](#b22)[5,](#b4)[21,](#b16)[53]](#b48) and government [[1]](#b0). These formal approaches are often seen as sufficient for ensuring data subject's privacy, in that they are "one-size-fits-all," i.e., data curators can apply best practice protections without needing to consider the intricacies of each deployment.

In practice, however, there can be significant barriers to applying differential privacy, which stem from the need to strike a delicate balance between the benefits of privacy preservation and its cost to utility [[4]](#b3). Moreover, the generality of sufficient conditions means that they naturally lend themselves to being very abstract, which can make it far too easy to lose sight of the concrete privacy harms they are intended to prevent [[17]](#b12).

The deficiencies inherent in each of the existing approaches compels us to explore an intermediary design philosophy: necessary conditions. Within this approach, we can formally define (possibly many) properties that any private data release should guarantee without needing to provide a single, unifying, sufficient condition. These necessary conditions can be seen as giving formal procedures for recognizing when an attacker has inflicted harm. Specifying necessary conditions promises to be an approach that simultaneously embraces the formality of sufficient conditions, while being just as concrete and convincing as attacks. Thinking in terms of necessary conditions has always been implicit in the practice of differential privacy (albeit, usually informally), where selecting the "best" privacy parameters ε, δ, for a deployment requires a trade-off with other metrics, such as accuracy. This makes it necessary to understand how small the parameters must be for the prevention of concrete privacy harms. Therefore, necessary conditions can provide a concrete methodology for justifying parameter choices by identifying parameter regimes that could enable specific harms.

A new necessary condition: Demographic Coherence. In this work, we design a novel necessary privacy notion rooted in three key insights: [(1)](#b0) privacy harms are increasingly going to come in the form of inferences at the hands of predictive algorithms. [1](#foot_0) That is, we should be interested in the predictions that these algorithms make about people-and the decisions organizations may make based on these predictions-even when predictive algorithms are not intentionally designed with causing harm in mind; [(2)](#b1) we should consider the confidence with which an algorithm can make predictions, because simply increasing the confidence that an individual or a group has a certain attribute may be enough to result in harm; and

(3) The harms associated with breaches of privacy are not experienced uniformly among members of a population. This means that, if not defined carefully, an aggregate measure across an entire population could easily 'hide' effects on vulnerable subgroups by averaging them away.

Our resulting notion, which we call demographic coherence, is intentionally designed to be ergonomic[foot_1](#foot_1) in many different contexts. For example, we provide sufficient formalism to enable rigorous analysis and provable realization, all while keeping the specific harms against which demographic coherence protects compellingly salient. Additionally, we provide a vision as to how demographic coherence can support the type of intuition building required to set real-world parameters.

## Our Contributions

In this work we make the following contributions:

-Demographic Coherence. In this work we introduce demographic coherence, an analytical framework for reasoning about the privacy provided by data release algorithms. Demographic coherence has the following qualities:

-Captures predictive harms. Demographic coherence builds on conceptual tools from generalization [[55,](#b50)[23,](#b18)[18,](#b13)[34,](#b29)[8]](#b7) and multicalibration [[30,](#b25)[41]](#b36) to [(1)](#b0) evaluate the risk of predictive harms distributionally without relying on measuring accuracy with respect to an unknown (and possibly unknowable) ground truth and (2) evaluate the risk of predictive harms local to the different subgroups within a population. Evaluating risks distributionally allows the framework to remain applicable even when ground truth is unavailable, and evaluating risks for different subgroups allows the framework to identify effects specific to vulnerable subgroups.

-Lends itself to experimental auditing. Demographic coherence has a natural translation to an experimental setup for comparing the effects of various algorithms for privacy preserving data release. In addition, demographic coherence is measured by computing a distance metric over two distributions, which facilitates quantification of the concrete risk. In this work we study an instantiation of demographic coherence measured using Wasserstein distance.

-Lends itself to analytical arguments. Finally, the formalism we build supports rigorous analytical arguments about algorithms. For example, we show that all algorithms with bounded max information are also coherence enforcing.

-Demographic coherence enforcement is achievable. We prove that demographic coherence enforcement is achievable, showing parameter conversions under which any pure differentially private (pure-DP) algorithm and any approximate differentially private (approx-DP) algorithm enforce demographic coherence. For an overview of these theorems, see Section 2.

## Related Work

The study of privacy-preserving data release broadly falls into two categories: demonstration of potential harms via concrete attacks, and the development of formal methodologies that provide robust guarantees. These two approaches provide complementary insights. Formal approaches provide a concrete path to implementing privacy-protections, and the motivation for their use is derived from attacks. In particular differential privacy provably protects against membership inference (e.g., [[32,](#b27)[26,](#b21)[50,](#b45)[56]](#b51)), reconstruction (e.g., [[22,](#b17)[14,](#b9)[29,](#b24)[11]](#)), and reidentification (e.g., [[52,](#b47)[42]](#b37)), as shown by Dwork et. al. [[25]](#b20). In practice, however, there are fundamental challenges in using attacks to guide the many choices one must make when implementing privacy protections. These challenges arise from ( [1](#formula_18)) identifying successful attacks, (2) identifying realistic attacks, and (3) determining the privacy protections necessary to prevent the attacks being considered.

Evaluating the success of an attack. In using attacks to motivate formal methodologies, one must start by demonstrating the extent of potential vulnerabilities. For example, membership inference is an attack that relates directly to the definition of differential privacyhowever, the potential to infer membership in a dataset isn't a convincing vulnerability in the case of large data collection efforts like the US decennial Census. Therefore, differential privacy frequently derives its motivations from re-identification and reconstruction attacks. Still, the success of these attacks is difficult to evaluate. [3](#foot_2) In recent work, Dick et al. implemented a reconstruction attack [[20]](#b15) along with robust evaluations of its success. Their work has since been cited by the US Census Bureau's chief scientist as evidence that "database reconstruction does compromise confidentiality" [[39]](#b34). The key insight in their evaluation comparing the results of the reconstruction to a baseline in which reconstruction is conducted with complete access to the distribution underlying the data. While the intuition behind this work-that an attack is much more concerning if it reveals more than what could be learned from a detailed knowledge of the distributional properties-applies to many attack paradigms, the baseline considered in their work is specific to reconstruction attacks. Reconstruction attacks are not always possible to carry out, and, furthermore, conducting a reconstruction attack assumes malicious intent in a way that may or may not be convincing to all stakeholders. We introduce the demographic coherence framework which extends this intuition to the evaluation of a more general class of attacks.

Another place where the efficacy of specific attacks is measured via comparison to baselines is the literature on auditing differentially private algorithms (e.g., [[37,](#b32)[36,](#b31)[44,](#b39)[51]](#b46)). Here, attacks are carried out on existing systems, and the efficacy of the attack is used to measure the maximum level of "effective privacy" that the system confers.

Identifying realistic attacks. Research into conducting privacy attacks makes a variety of assumptions about the setting in which those attacks could be conducted, including the goal of the attacker, the power of the attacker, the type of system attacked, etc. These assumptions can radically change the extent to which an attack should be considered a realistic threat against real-world data releases; attacks that require unrealistic assumptions may not be concrete threats. The works of Rigaki & Garcia [[46]](#b41), Salem et al. [[49]](#b44), and Cummings et al. [[17]](#b12) classify existing attack strategies by adversarial resources and goals in order to provide a structure for evaluating privacy risks. In addition to this, Cohen [[13]](#b8) and Giomi et al. [[28]](#b23) take a different approach, appealing to the law to determine the goals of a realistic attacker. Specifically, they contextualize the attacks they consider by tying them to existing privacy law. Still, individual attacks, even if successful and realistic, don't provide a clear path forward in terms of designing protections.

Identifying necessary conditions. Some prior work has started to identify necessary conditions for achieving privacy. Cohen & Nissim [[15]](#b10) introduce a necessary condition, called "predicate singling out," inspired by the GDPR notion of singling out. Balle et. al. [[6]](#b5) introduce an alternative necessary condition called "reconstruction robustness," which is closely related to reconstruction attacks. Cummings et. al. [[17]](#b12) build on the notion of reconstruction robustness, extending it to a weaker adversarial setting. Our framework extends this general approach but applies to a much broader class of attacks-namely, any attacks from which a confidence rated predictor could be distilled.

Recent work by Cohen et al. [[16]](#b11) also recognizes the need to bridge the gap between formal privacy guarantees and practical attacks. Building on definitions in prior work [[6,](#b5)[15,](#b10)[17]](#b12) they introduce "narcissus resiliency," a framework for establishing precise conditions under which an algorithm prevents various classes of existing attacks, including reconstruction attacks, singling out attacks, and membership inference attacks. Our definition defines invulnerability against a different type of privacy loss, providing complementary insights in the form of necessary conditions that can be considered alongside their definitions. Specifically, we believe that it is important to consider demographic coherence alongside their notion of narcissus singling out; the latter captures an important property that the former does not. (An algorithm that chooses a small subset of the data to publish in the clear does not meet the definition of singling out security even though it may be demographic coherence enforcing if the subset is small enough.) Another key difference between our works is that the narcissus framework does not naturally lend itself to concrete experimental evaluation, whereas demographic coherence is intentionally designed with this use case in mind.

Finally, most of the works discussed above measure the success of an attack via its accuracy (i.e., is the information extracted about the data subject true?). We observe that harm is not necessarily predicated on accuracy, and we design demographic coherence to be intentionally independent of accuracy. One impact of this choice is that demographic coherence is a more natural fit for settings in which ground truth is difficult or impossible to measure.

## Overview of Technical Results

In Section 5, we show parameter conversions under which any pure-DP algorithm, and any approx-DP algorithm enforces demographic coherence. Here, we present informal statements of these technical results.

We start by presenting a simplified definition of coherence enforcement (Definition 3, 4). (This presentation is meant to allow the informal statements of our technical results. For a formal presentation, see Section 4. Additionally, the concept of enforcing demographic coherence emerges from careful consideration of several key principles, which are discussed in detail in Section 3.) Informally, a coherence-enforcing A guarantees that predictors trained using its private reports will be demographically coherent. [4](#foot_3)Definition 1 (Informal Version of Definitions 3 and 4 ). Consider a data universe X , and a data-curation algorithm A : X * → Y. We say that A enforces (α, β)-demographic coherence, if for all algorithms L : Y → (X → [-1, 1]) that use the report produced by the curator to create a confidence-rated predictor h : X → [-1, 1], the following condition is satisfied. For all datasets X,

$Pr Xa,X b $ ← -X,Ra←A(Xa),h←L(Ra) [dist(h(X a ), h(X b )) ≥ α] ≤ β,$where X a , X b represent a random split of the dataset X into halves, report R a is produced by the data-curator using only X a , and h is created by running algorithm L on the report, h(X a ) represents the empirical distribution of predictions made on X a , and dist(•, •) represents a metric distance between distributions. Here, β is the probability that h is not demographically coherent, and α represents how close the distributions of h(X a ) and h(X b ) are required to be.

The formal definition of coherence enforcement is more intricate than the one above. One key technical distinction is that the restriction on predictor h applies not only to the full sets X a and X b , but also across different subpopulations in those sets. For the remainder of this section, we specify the distance metric dist(•, •) as Wasserstein-1 distance between distributions. In this context, we say that and algorithm A enforces Wasserstein-coherence.

The following theorem is an informal statement of Theorem 6, which argues that any data-curation algorithm with bounded max-information [[23]](#b18) (a notion that mathematically captures the dependence of algorithms' outputs to their inputs) also enforces Wasserstein coherence.

Theorem 1 (Informal Version of Theorem 6).

$Let n ∈ N,ζ > 0, β ∈ (0, 1), α ∈ (0, 2].$Consider a data curation algorithm A : X n/2 → Y with bounded max-information

$I β/2 ∞ (A, n/2) < ζ. Then, A enforces (α, β)-demographic coherence provided that n ≥ k • ζ+ln(1/β) α 2$for some constant k.

We leverage the connection between differential privacy and max-information to show the exact parameter conversion under which differentially private algorithms enforce demographic coherence. Theorem 2 is an informal statement of Theorem 7, the result for pure-DP. For the approximate-DP result, we point the reader to Theorem 8 in Section 5.

Theorem 2 (Informal Version of [Theorem 7)](#). Let n ∈ N, β, ε ∈ (0, 1), α ∈ (0, 2].

Consider an ε-DP data curation algorithm A : X n/2 → Y. Then, A enforces (α, β)demographic coherence provided that ε ≤ k • α ln(1/β) for some constant k. This theorem should be understood as follows: a data curator identifies (possibly experimentally) regimes for α and β that they find to be "too risky" for a data release (with respect to demographic coherence). That curator can then use this theorem to suggest a value of ε such that, if they were to use differential privacy as their privatization mechanism, the resulting data release would achieve their desired constraints. While the parameter conversion in Theorem 7 would likely result in a value of ε that is too small for most use cases, we expect this to be inherent to a black-box conversion of differential privacy to the enforcement of demographic coherence. We leave it as an important open question to identify other ways of achieving our definition, including non-black-box uses of DP-algorithms for obtaining better coherence enforcement guarantees.

## A Walk Through Our Definition

In order to clearly motivate and explain the choices embedded within our definition, we incrementally build up our approach in this section; for the formal definition see Section 4.

Notation and Conventions. Assume that the data curator has collected a sample X from the overall population of interest. We make no requirements on the relative sizes of X and the population such that our framework can be used broadly-even in Census-like circumstances, in which the goal is to sample the entire population. Our ultimate goal is to reason about a data curator A who uses X to generate a privacy-preserving release R. [5](#foot_4)

## Predictive Harms

Within our framework, we characterize the adversary as a party interested in making predictions about individuals, e.g., if people have some particular stigmatized feature or are going to buy a product if they are targeted with an advertisement. We formalize this conceptual approach by considering an arbitrary algorithm L used by the adversary to design the predictor h. [6](#foot_5) We choose to measure privacy risk in terms of predictive harms for the following reasons:

-Predictors are commonplace: The predictions made by machine learning models increasingly have direct impacts on people's daily lives. Diagnostic models are being tested as potential aides for medical experts [[3]](#b2), and increasingly complex and opaque models are used to "match" job candidates with prospective employees [[35,](#b30)[2,](#b1)[45]](#b40) in order to increase the odds that an individual ends up with a lucrative job. Even complex infrastructures, like those used in digital advertising, can be seen as predictive models that are attempting to classify individuals into target audiences. The fact that predictive algorithms are increasingly commonplace-and the decisions they make concretely impact our daily lives-makes them a very believable source of harm.

-Harmful predictors need not be maliciously produced: By considering the impact of predictors, we free ourselves from needing to see the adversary as intentionally trying to cause harm and instead can refocus on the (perhaps accidental) harms that a data release has the potential to cause.

A conceptual concern about considering predictive harms is whether we are explicitly ruling out particular, important types of adversarial behavior that are attempting to extract information (e.g., reconstruction attacks). However, we note that by discussing predictors we are only limiting the input/output behavior of the adversary's product, and not how the predictor is produced. For example, our framework could capture an adversary that runs a known reconstruction algorithm (e.g., [[19]](#b14)) and then makes predictions about individuals based on the produced table. In this way, our approach highlights the ways in which existing approaches could be used when applied in decision-making contexts. Looking ahead, in theoretically analyzing our framework we will universally quantify over algorithms to preserve generality, which means that reconstruction-based approaches-or other known malicious approaches to data extraction-are naturally captured.

Still, in choosing to concretize the type of our adversary, we do risk failing to consider a different type of attacker with inconsistent goals. Definitions created with this philosophy can be extremely helpful at establishing necessary conditions for ensuring privacy, but do not claim to be sufficient. On the other hand, our approach helps highlight a specific way in which data is likely to be weaponized in the real world.

Incoherent predictions. Within this work we focus on capturing predictive harms that occur specifically by virtue of a data subject appearing in a dataset. To give a concrete example, consider the now classic case of Narayanan and Shmatikov's re-identification of Netflix users within an anonymized data release using public IMDB data [[43]](#b38). In this setting, we might consider an adversary interested in learning a predictor which predicts queerness (e.g., imagine the adversary is operating in a regime in which queerness is criminalized or highly stigmatized). Now, imagine two similar 7 individuals Asahi and Blair; each intentionally avoids being perceived as queer, and in particular does not provide ratings on movies with queer themes on their public IMDB profiles. Assume that based on a random sample, one of them (e.g., Asahi) has their movie ratings released by Netflix and the other (e.g., Blair) does not. A predictor that is likely to guess that Asahi is queer when they are present in the dataset but would not have guessed they are queer otherwise indicates that the predictor was able to extract some information about Asahi from the data release. Given the assumption that we claim that Asahi and Blair are similar, this would also be true of a predictor that guessed that Asahi is queer while Blair is not.

Importantly, this is true even if it's not clear exactly what form leakage takes or if the prediction as to their queerness is inaccurate. We call predictors that act in this way "demographically incoherent." There are two important (if unintuitive) subtleties that immediately emerge from this description of incoherent predictions:

(1) Harmful predictors need not be accurate: Incoherent predictions focus on the behavior of the predictor independent of accuracy. Within the example above, it is not important if Asahi is actually queer, it is enough that the predictor guesses that Asahi is queer because of their presence in the data. This is because we envision our predictor being used to make some real-world decision, e.g., limiting the opportunities available to Asahi due to their perceived queerness. As such, the prediction's accuracy is a secondary concern.

(2) Measuring confidence is critical: When considering the ways in which data releases can be translated into real-world harms, it's important to recognize that enabling an adversary to make high confidence predictions about private attributes is a problem. Importantly, this means that we should not require that the adversary can predict private attributes with 100% certainty in order for it to be considered harmful. Indeed, there is no particular cut-off threshold for certainty at which point it is natural to consider a harm occurring for all contexts. In turning to predictors as our adversarial strategy, we naturally arrive in a context within which notions of confidence have been extensively explored. Specifically, our approach considers confidence-rated predictors h, allowing us to directly reckon with predictive uncertainty.

We note that there are other pathological predictors which do not indicate privacy-loss, and are therefore not considered demographically incoherent. For example, a predictor may make guesses that are entirely random or guess that everyone in the population has some feature, i.e., make predictions that do not depend on the characteristics of individuals. The challenge then is to detect demographically incoherent predictors, whose behavior indicates privacy leakage, without depending on accuracy and without accidentally measuring variance in behavior that is not dataset dependent.

## An Experiment to Detect Demographically Incoherent Predictors

With intuition about our class of "bad" predictors in hand, we now turn our attention to designing an experiment for detecting algorithms that produce them. In this discussion, we will defer to the concrete ways in which we will measure the demographic incoherence, and first focus on the experiment itself-that is, first we will decide what values we should measure, and then proceed to deciding how to do that measurement.

Because a symptom of incoherent predictors is differing performance on in-sample and out-of-sample individuals, it is clear that a comparison is required. However, it is not immediately obvious what the "right" comparison should actually be. In fact, some natural approaches fall short of our goals. As such, we walk through two seemingly natural, but flawed, experiments before discussing our final choice. Recall that the data curator has a dataset X and will be releasing a privacy-preserving report R.

(1) Comparing before and after a data release: A very natural approach would be to compare the performance of a predictor created before a data release with one created after. i.e., comparing the performance (on individuals in X) of h 0 produced by an algorithm L with access to the adversary's pre-existing, auxiliary information Aux to a predictor h 1 produced by L with access to both Aux and the report R. [8](#foot_7) Such a comparison, intuitively, should isolate exactly the predictive changes associated with releasing R.

Where this approach fails is that it does not recognize that there should be a difference between the predictors h 0 and h 1 over the inputs in X. After all, if there was no difference between h 0 and h 1 , there would be no value whatsoever in releasing R! As such, this comparison is necessarily conflating potential "bad" types of predictions that releasing R enables with the "good" types of predictions that motivated the release of R in the first place.

(2) Comparing to the base population: The next most natural approach would be to compare the behavior of a single predictor h on individuals in X with that on individuals in the rest of the population. For example, by comparing its behavior on another similarly sampled dataset Y . This improves on our previous approach because we might expect that a "good" learning algorithm uses the dataset to learn about the population at large instead of revealing specifics about individuals.

While this approach gets to the core of our interests, it has an important flaw. Technically speaking, we can not assert that a real world sampling procedure has access to the base population distribution. i.e., one cannot assume that two real world datasets are i.i.d samples from the same distribution. Also, we could conceivably be in a situation where the entire population of interest is contained in X, leaving no one in X against which we could compare. Therefore, keeping in mind the ergonomics of our definition in a concrete deployment scenario, this approach also falls short of our goals.

Our approach. We build off the second approach above by taking control of the randomness used to separate the two comparison populations. Specifically, we split the dataset X into two uniformly selected halves, X a and X b . We then use the data curation algorithm to generate the report R using only X a , holding X b in reserve as our "test" data set. We then test the behavior of a predictor h, designed based on the report R. Specifically, we compare the predictions of h on individuals in X a and X b . This approach "fixes" our second failed attempt by moving the assumptions about the randomness used in sampling X-something over which we have no control-into the randomness we use to split X into X a and X bsomething over which we do have control. We say that a data release is demographically incoherent with respect to X if its predictions on members of X a are noticeably different than the predictions it makes on members of X b (who necessarily have similar demographic distributions, given the uniform split.)

## Measuring the Incoherence of a Predictor

Finally, we discuss how to compare the behavior of h on X a and X b without relying on accuracy. Formally, we consider real-valued confidence-rated predictors h :

$X → [-1, 1]$which predict something about individuals. To capture the fact that these predictions are confidence rated, h outputs values in [0, 1] when it predicts the attribute is likely to be true, and values in [-1, 0] when it predicts the attribute likely to be false, with a higher absolute value representing higher confidence.

For any such predictor, we will consider h(X a ) and h(X b ) as representing the uniform distribution over the predictions of h on X a and X b respectively. Comparing these distributions allows us to reason about the general behavior of the predictor h on X a and X b without considering accuracy of predictions on individuals. In order to get a more granular understanding of the behavior of h we further formalize the intuition of making comparisons over "similar" individuals in X a and X b as explained below.

Measuring a difference with respect to "similar" individuals. In our motivating discussion of incoherent predictions, our representative individuals Asahi and Blair were assumed to be "similar" to one another. To formalise this intuition, we ask that a predictor is demographically coherent not only on the population as a whole, but also on recognizable subgroups from the population, e.g., men, women, college freshmen, middle-school teachers etc. . .[foot_8](#foot_8) For each of these subgroups of the population, the things that bind them together make them similar, in some particular sense. By operationalizing our earlier intuition in this way, we ask that the demographic coherence property holds not only over some particular notion of similarity, but rather over many notions of similarity at the same time. It also has the following technical and social benefits: (1) From a technical perspective, considering only the full population might hide incoherent decisions within sub-populations that effectively "cancel out." That is, there might be a right-shift in one group that masks a left-shift in a different group, each shift effectively "disappearing" in the collective distribution over all individuals. (2) From a social perspective, there may be particularly important groups within the population for whom we want to ensure coherent predictors for normative reasons. For example, if X is a Census-like dataset, we may want to ensure that there are not subgeographies on which incoherent predictors are allowed. Similarly, we may want to ensure that there aren't legally protected categories (e.g., race, sex, religion, etc. . . ) on whom incoherent predictions are allowed.

The lens of a predictor. Consider the adversary using the Netflix dataset to learn a predictor for queerness. We assume that at the time of making predictions, the adversary sees a public user profile (their IMDB ratings) which contains only some of the user information that was contained in the dataset. To formalize this intuition we introduce the lens ρ of the predictor, which indicates the attributes contained in the dataset which the predictor can "see." We then compare the behavior of h on members of X a and X b as seen through the lens ρ.[foot_9](#foot_9) Choosing a metric. In this work, we recommend instantiating the demographic coherence experiment with the distance metric of Wasserstein distance (Definition 5), also known as earth-mover's distance, when measuring demographic coherence (or lack thereof). Intuitively, this metric measures the minimum amount of work that it requires to deform one probability distribution into another. For example, if one visualizes a probability distribution as a mount of dirt, Wasserstein distance measures the effort required to move enough dirt to make one mount look like the other (thus, earth-mover's distance). Unlike Total Variation distance, which only measures the distance between the distribution curves, Wasserstein distance is greater with a higher shift in confidence; the importance of measuring confidence is one of the insights we highlight in Section 3.1. Another advantage of the Wasserstein distance is that it has been widely studied and used in theoretical and empirical statistics, and so there is a rich mathematical toolkit that one can borrow from when reasoning about it.

We recognize that there may be other measurement metrics that could be applied to the demographic coherence experiment that might highlight risk in different ways, and encourage this as important follow-up work.

## Formal Definition

This section presents the formal definitions corresponding to our framework. For a discussion about the various choices made here, see Section 3.

Section 4.1 contains a glossary of the notation we use, Section 4.2 formally defines the notion of incoherence that we measure, Section 4.3 defines what it means for an algorithm to be demographically incoherent, and Section 4.4 defines what it means for a data curation algorithm to be coherence enforcing.

## Notation

• We define a data universe X = ({0, 1} * ∪ ⊥) * , where each feature of the data can also take the value ⊥ (a.k.a. null).

• We denote a dataset consisting of n records from X by dataset X ∈ X n .[foot_10](#foot_10)

• We consider a collection C of sub-populations C ⊆ X .

• For any dataset X and sub-population C ∈ C, we define X| C def = X ∩ C to be the restriction of X to the sub-population C, i.e., the members of the dataset X that belong to sub-population C.

• We define a lens ρ as a set of features from X .

• For a lens ρ, we define π ρ (X) to be the data in X restricted to the features in the lens. That is, for every feature represented by ρ, the sets π ρ (X) and X are exactly the same, and for features not represented by ρ, the entries in π ρ (X) always have the value ⊥.

• For any fixed predictor h : X → [-1, 1], define the distribution h(X) as the uniform distribution over the predictions of h on X. That is, the distribution h(X) has cumulative distribution function

$cdf h(X) (p) = Pr x $ ← -X [h(x) ≤ p].$
## Measuring Demographic Incoherence of Predictors

In this section we define the notion of incoherence that we measure over predictors. This measurement is used informally in the demographic coherence experiment DemCoh (Figure [1](#)).

Definition 2 (Demographically Incoherent Predictor). Let X be a data universe, let X a , X b ∈ X n/2 be datasets consisting of n/2 data entries each, let C be a collection of sub-populations C ⊆ X , let the lens ρ be a set of features from X , and let h : X → [-1, 1] be a confidence rated predictor. Let d(•, •) represent some metric distance between probability distributions.

We say that the h has α-incoherent predictions with respect to X a , X b , ρ, and C, if for some

$C ∈ C, d h(πρ (X a | C )), h(πρ (X b | C )) > α$(In contrast, h has α-coherent predictions with respect to X a , X b , ρ, and

$C, if for all C ∈ C, it satisfies d h(πρ (X a | C )), h(πρ (X b | C )) ≤ α)$In other words, the predictions of h are incoherent if there is some sub-population C that witnesses a big distance between its predictions on two sets X a , and X b . Note that this definition distills a notion of 'incoherence' only once we fix some assumptions on X a , X b , perhaps that they are drawn from similar distributions.

## Demographic Coherence

Consider a data universe X , an algorithm L :

$X n/2 → (X → [-1, 1]$) which uses a dataset of size n/2 to produce a confidence-rated predictor h : X → [-1, 1]. Let C be a collection of sub-populations C ⊆ X , let the lens ρ be a set of features from X , Let dist(•, •) represent some metric distance between probability distributions.

In this section, we formally define when the algorithm L is demographically coherent. We start by defining the demographic coherence experiment DemCoh which checks the demographic coherence of L with respect to on a specific dataset X, a collection C, a lens ρ, and a distance metric dist(•, •). This experiment works similarly to the description under our approach in Section 3.2, expect we are testing the coherence of an algorithm that gets the dataset in the clear. In Section 4.4, we will use the notion of demographic coherence to define when a data curator is coherence enforcing.

In the DemCoh experiment, the input dataset X is split into sets X a , X b where X b is held in reserve as a "test" set. Then the algorithm L, with input X a , is used to produce a predictor h. Finally, the predictor h is checked for demographic incoherence (Def 2) with respect to X a , X b , ρ, and C.

$DemCoh L,X,C,ρ,dist(•,•) (α)$Input:

An algorithm L : Split Data:

$X n/2 → (X → [-1, 1]), A dataset X ∈ X n , a collection C of subgroups C ⊆ X ,$$I $ ← -{S ⊆ [n] : |S| = n/2} X a = (x i ) i∈I X b = (x i ) i∈[n]\I Compute Predictor(X a ): h ← L(X a )$Incoherence Condition:

$Set b = 0 if there exists C ∈ C such that: dist h(πρ (X a | C )), h(πρ (X b | C )) > α$Else set b = 1

## Figure 1: Demographic Coherence Experiment

A natural formalization of the intuition we developed in Section 3 would say that an algorithm L produces (α, β)-demographically coherent predictions with respect to collections C and lens ρ if the following holds for all datasets X:

$Pr[DemCoh L,X,C,ρ (α) = 0] ≤ β.$However, this definition cannot be realized with respect to arbitrary categories C and all datasets, because the sampling experiment in and of itself introduces some incoherence. For example, consider a category C that is men over 60, and a dataset that contains only two people in this category. There exists a predictor that predicts -1 on one of them and 1 on the other. With probability 1/2, these two men end up in X a and X b respectively, and the Wasserstein distance between the predictors' distributions on X a | C and X b | C is 2. Hence, for our definition to be meaningful and achievable, we need that the datasets considered are sufficiently representative that the incoherence due to sampling does not dominate. Thus, in order to make the definition achievable, we define demographic coherence with respect to a size constraint.

To remove the need for the parameter γ, one could redefine X a | C by "zeroing out" members of X a that are not in C instead of taking the intersection X a ∩ C. This would effectively result in asking the predictor h in Definition 2, and Figure [1](#) to make "dummy" predictions, with no information, for every member of X a and X b not belonging to C. While such a definition may be more mathematically elegant, we believe that the explicit failure point represented by γ in our defintion is important for interpretability and ease of use-especially by non-experts. For the same reason, it is important to have an explicit collection C, and lens ρ, even though the eventual theorems one can prove may only hold for large choices of γ, C, and ρ. Definition 3. (Demographic Coherence). Consider a data universe X , and an algorithm L : X n/2 → (X → [-1, 1]) which uses a dataset of size n/2 to produce a fixed confidence-rated predictor h : X → [-1, 1]. Let C be a collection of sub-populations C ⊆ X , let the lens ρ be a set of features from X , Let dist(•, •) represent some metric distance between probability distributions. We say that L produces (α, β)-demographically coherent predictions with respect to collection C, size-constraint γ, and lens ρ if the following holds:

$For all X ∈ X n , C * = {C ∈ C | |C ∩ X| ≥ γ} Pr[DemCoh L,X,C * ,ρ (α) = 0] ≤ β.$
## Coherence Enforcing Algorithms

We finally define coherence-enforcing algorithms by reference to the definition of demographic coherence (Definition 3). Specifically, a data curator A is coherence enforcing if, any algorithm L can be rendered demographically coherent simply by filtering its inputs through the data curator A, without many any changes to the algorithm itself. Definition 4. (Coherence Enforcing Algorithms). Consider data universes X , Y, a collection C of sub-populations C ⊆ X , a lens ρ, and an algorithm A : X n/2 → Y. Let dist(•, •) represent some metric distance between probability distributions. We say that A enforces (α, β)-demographic coherence with respect to collection C, sizeconstraint γ, and lens ρ if:

$For any algorithm L : Y → (X → [-1, 1]), the combined algorithm L • A : X n/2 → (X → [-1, 1]$) satisfies (α, β)-demographic coherence with respect to the collection C, size-constraint γ, and lens ρ.

## Instantiating Demographic Coherence with a Metric

In the definitions above, we do not use a specific metric. The metric we will use to instantiate these definitions in the following sections is the Wasserstein-1 metric defined below. Definition 5. Let P, Q represent distributions over a discrete subset S ⊆ R. Then, the 1-Wasserstein distance between P, Q is defined as

$dist W 1 (P, Q) = inf π i∈S j∈S x i -x j 1 π(x i , x j ),$where the infimum is over all joint distributions π on the product space S × S with marginals P and Q respectively.

If an algorithm is (α, β)-demographically-coherent as per Definition 3 with this Wasserstein metric instantiation, we say that it is (α, β)-Wasserstein-demographically-coherent (or (α, β)-Wasserstein-coherent for short.) Similarly, if an algorithm enforces (α, β)-demographiccoherence as per Definition 4 with this Wasserstein metric, then we say it enforces (α, β)-Wasserstein-demographic-coherence (or it enforces (α, β)-Wasserstein-coherence for short.)

## Algorithms that Enforce Wasserstein Demographic Coherence

Now, we show that Wasserstein coherence enforcement is instantiable. Firstly, in Section 5.1, we go over some technical preliminaries necessary to state and prove our results, Then, in Section 5.2, we prove Theorem 6 showing that algorithms with bounded max-information are coherence enforcing. Building on this, in Section 5.3, we leverage the connection between differential privacy and max-information to prove Theorem 7 which says that pure-DP algorithms enforce demographic coherence, and Theorem 8 which is says that approx-DP algorithms enforce demographic coherence as well.

## Technical Preliminaries

For a recap of the notation used, see Section 4.1.

Definition 6 (Differential Privacy [[24]](#b19)). Let n ∈ N. A randomized algorithm A : X n → Y is (ǫ, δ)-differentially private if for all subsets Y ⊆ Y of the output space, and for all neighboring datasets X, X ′ ∈ X n (i.e. X -X ′ 0 ≤ 1), we have that

$Pr[A(X) ∈ Y ] ≤ e ε Pr[A(X ′ ) ∈ Y ] + δ$If δ = 0, we refer to the algorithm as satisfying pure differential privacy (pure-DP), whereas δ > 0 corresponds to approximate differential privacy (approx-DP).

Definition 7 (Max-information of random variables [[23]](#b18)). Let X and Y be jointly distributed random variables over the domain (X , Y). The β-approximate max-information between X and Y , denoted by

$I β ∞ (X; Y ) is I β ∞ (X; Y ) = ln    sup T ⊆(X ×Y) Pr[(X,Y )∈T ]>β Pr[(X, Y ) ∈ T ] -β Pr[X ⊗ Y ∈ T ]   $Definition 8 (Max-information of algorithms [[23]](#b18)).[foot_11](#foot_11) Fix n ∈ N, β > 0. Let X be a finite data universe of size m. Let S be a sample of size n chosen without replacement from X . Let A : X n → Y be an algorithm.

Then we define the max-information of the algorithm as follows:

$I β ∞ (A, n) = I β ∞ (S, A(S)))(1)$The following definition of order-invariant algorithms appears as a technical assumption in some of our theorem statements. [13](#foot_12) This is a minimal assumption because any non-orderinvariant algorithm can be made order-invariant by simply pre-processing the dataset with a sorting or shuffling operation.

Definition 9 (Order-invariant algorithm). An algorithm A : X m → Y, is order invariant if for all X ∈ X m , the distribution of the random variable A(X) does not depend on the order of the elements of X.

The proofs of the following theorems connecting differential privacy and max-information can be found in Appendix B.

Theorem 3. (Pure-DP =⇒ Bounded Max-Information) Fix n ∈ N, ε > 0 and let X be a data universe of size at least n. Let A : X n/2 → Y be an order-invariant ε-DP algorithm. Then for any γ > 0,

$I γ ∞ (A, n/2) ≤ ε 2 n/4 + ε n ln(2/γ)/4.$The following theorem is a generalized version of that in [[47]](#b42). The proof follows theirs, with the following key distinctions: (1) it applies to sampling without replacement (2) It carefully tracks constants and (3) It maintains flexibility in setting parameters. We anticipate that this version of the result might be independently useful.

Theorem 4. (Approx-DP =⇒ Bounded Max-Information, Generalised) Let A : X n → Y be an (order-invariant) (ε, δ)-differentially private algorithm for ε ∈ (0, 1/2], δ ∈ (0, ε). For δ ∈ (0, ε/15], t > 0, and β(t, δ) = e -t 2 /2 + n 2δ δ + 2 δ+2δ

1-e -3ε

we have

$I β ∞ (A, n) ≤ n   347 δ + 75 δ ε 2 + 24 δ2 ε + 240ε 2   + 6tε √ n. Corollary 1. (Approx-DP =⇒ Bounded Max-Information, Specific) Fix n ∈ N, for ε ∈ (0, 1/2], γ ∈ (0, 1], δ ∈ (0, ε 2 γ 2 (120n) 2 ]$and let X be a data universe of size at least n. Let A : X n → Y be an (order-invariant) (ε, δ)-differentially private algorithm. We have that

$I γ ∞ (A, n) ≤ 265ε 2 n + 12ε n ln(2/γ).$Definition 10 (Hypergeometric distribution). Fix 0 < a, s ≤ b. Consider a population of b items of which a items have a special property P . Consider s items sampled without replacement from b. The distribution of the number of items in s with property P is called the hypergeometric distribution parameterized by b, a, s (denoted by H(b, a, s)).

Theorem 5 [([33]](#)). Let K have a hypergeometric distribution H(b, a, s). Then for every γ ≥ 2,

$Pr[K > s a b + γ] < e -2c(γ 2 -1) Pr[K < s a b -γ] < e -2c(γ 2 -1)$where

$c = max 1 s + 1 + 1 b -s + 1 , 1 a + 1 + 1 b -a + 1 .$
## Bounded Max-Information Algorithms are Coherence Enforcing

$Theorem 6. Let n ∈ N,ζ > 0, β ∈ (0, 1), α ∈ (0, 1]. Let dist W 1 (•, •) represent the Wasserstein-1 distance metric.$Consider a collection C of sub-populations C ⊆ X , a lens ρ, and an algorithm A : X n/2 → Y with bounded max-information

$I β/2|C| ∞ (A, n/2) < ζ.$Then A enforces (α, β)-Wasserstein-coherence with respect to collection C, lens ρ, and size constraint γ, where γ = max 8.

3 • (ζ + ln(16|C|/β)) α 2 , 36 ln(3/α) α 2 , 16.6 • (ζ + ln(16|C|/β)), 5.3 α , 80 .

The intuition behind the result in Theorem 6 is that the output of an algorithm with bounded max-information does not contain too much specific information about the input dataset. This intuition is leveraged to prove Lemma 1, which is the main lemma underlying this result. Theorem 6 follows from this lemma by an appropriate setting of parameters.

$Lemma 1. Let η, ζ > 0, α ∈ (0, 1). Let dist W 1 (•,$•) represent the Wasserstein-1 distance metric. Consider a sub-population C ⊆ X , a lens ρ, and an algorithm A : X n/2 → Y with bounded max-information

$I η ∞ (A, n/2) < ζ. For all algorithms L : Y → {X → [-1, 1]}, datasets X ∈ X n , µ > 0,$as long as |X ∩ C| ≥ max 4.15 • ln(4/µ) α 2 , 16/3 α , 8.3 • ln(4/µ), 40 , we have Pr Xa←X,h←L•A(Xa)

$[dist W 1 (h(πρ (X a | C )), h(πρ (X b | C ))) > α] ≤ 2µ(|X ∩ C| + 1) • e ζ + η.$Here X a and X b denote a random split of the dataset X as in the DemCoh L•A,X,C * ,ρ (α) experiment in Figure [1](#).

Proof sketch of Lemma 1: Considering a particular subpopulation C ⊆ X , We need to show for all algorithms L : Y → {X → [-1, 1]}, all datasets X ∈ X n , µ > 0 that with high probability over a choice of split X a , X b $ ← -X, and predictor h ← L • A(X a ) as in the DemCoh L•A,X,C * ,ρ (α) experiment in Figure [1](#), the following holds:

$dist W 1 (h(πρ (X a | C )), h(πρ (X b | C ))) < α.$Note that instead of the split X a , X b which predictor h depends on, if we consider an independent split S, S $ ← -X, then we could hope to use a concentration inequality to get the bound we desire. (The proof of Claim 2 below redefines the sampling process in a way that allows us to use a concentration bound of Hush and Scovel (Theorem 5) for the hypergeometric distribution to prove such a result.)

With the goal of analysing an independent split, we combine the fact that bounded max-information is preserved under post-processing with the intuition that the output of an algorithm with bounded max-information does not contain too much specific information about the input dataset to decouple the predictive hypothesis h from the dataset X a in the following way (in Claim 1):

$dist W 1 (h(πρ (X a | C )), h(πρ (X b | C ))) ≈ dist W 1 (h(πρ (S| C )), g(πρ S| C ))$Proof of Lemma 1. Fix any arbitrary lens ρ. The proof proceeds in two claims. First, in Claim 1, we use the definition of max-information to replace X a , X b with an independently chosen half-sample S and its complement S = X \ S.

Claim 1. Consider η, α, ζ > 0 and a fixed dataset X ∈ X n . Consider a data-curation algorithm A : X n/2 → Y with bounded max-information, I η ∞,P (A, n/2) ≤ ζ, and an algorithm L : Y → {X → [-1, 1]} that uses the data report to create a predictor. Independently choose two random half samples X a , S ← X, and let sets X b = X \ X a , S = X \ S. Finally let h ← L(X a ). Then, we have that

$Pr Xa,h [dist W 1 (h(πρ (X a | C )), h(πρ (X b | C ))) > α] ≤ Pr S,Xa,h [dist W 1 (h(πρ (S| C )), h(πρ S| C )) > α] • e ζ + η Then, in Claim 2, we bound Pr[dist W 1 g(πρ (S| C )), h π ρ S| C$> α] for any confidence rated predictor g : X → [-1, 1] that is produced independently of S.

Claim 2. Let α ∈ (0, 1), let S be a sample of size n/2 drawn uniformly without replacement from X, let S = X \ S, and let g : X → [-1, 1] be any confidence rated predictor. For any µ > 0, when |X ∩ C| ≥ max{ 4.15  α 2 ln(4/µ), 5.3 α , 8.3 ln(4/µ), 40}, we have that

$Pr dist W 1 g(πρ (S| C )), g(πρ S| C ) > α ≤ 2(1 + |X ∩ C|)µ.$
## Putting these claims together, we get that Pr

$Xa,h [dist W 1 (h(πρ (X a | C )), h(πρ (X b | C ))) > α] ≤ 2µ(|X ∩ C| + 1) • e ζ + η.$
## Now we proceed to prove Claims 1 and 2.

Proof of Claim 1. First, note that since the algorithm L postprocesses the report output by the data curator, by the fact that max-information is preserved under postprocessing, it inherits its max-information. Let A * be the combined algorithm L•A. Then by the definition of max-information, and since (X a , A * (S)) is distributed exactly the same as (S, A * (X a )),

$I η ∞ (X a ; A * (X a )) = dist η ∞ X a , A * (X a ) || X a , A * (S) = dist η ∞ X a , A * (X a ) || S, A * (X a ) .$we have that for all

$T such that Pr[(X a , A * (X a ) ∈ T ] > η, log Pr[(X a , A * (X a )) ∈ T ] -η Pr[(S, A * (X a ))) ∈ T ] ≤ ζ.$Given C, we can post-process a pair

$(X a , A * (X a )) to compute (h(πρ (X a | C ))) and (h(πρ (X b | C ))). This is because h ← A * (X a ) and X b = X \ X a .$Applying the same post-processing to (S, A * (X a )) yields (h(πρ (S| C ))) and (h(πρ S| C )).

$Let T = {(S, h) | dist W 1 (h(πρ (S| C )), h(πρ S| C )) > α}.Then if Pr[(X a , A * (X a )) ∈ T ] > η, log Pr[(X a , A * (X a )) ∈ T ] -η Pr[(S, A * (X a ))) ∈ T ] ≤ ζ.$
## This means that if

$Pr[dist W 1 (h(πρ (X a | C )), h(πρ (X b | C ))) > α] > η, log Pr[dist W 1 (h(πρ (X a | C )), h(πρ (X b | C ))) > α] -η Pr[dist W 1 (h(πρ (S| C )), h(πρ S| C )) > α] ≤ ζ.$We can rearrange the above equation to get that

$Pr[dist W 1 (h(πρ (X a | C )), h(πρ (X b | C ))) > α] ≤ Pr[dist W 1 (h(πρ (S| C )), h(πρ S| C )) > α] • e ζ + η,$as required.

Proof of Claim 2. Let α, µ ∈ (0, 1) (the statement holds trivially for µ > 1), and suppose |X ∩ C| ≥ max{ 4.15 α 2 ln(4/µ), 5.3 α , 8.3 ln(4/µ), 40}. By the definition of the distance metric we have the following:

$dist W 1 g(πρ (S| C )), g(πρ S| C ) = 1 -1 |cdf g(πρ(S| C )) (g) -cdf g(πρ(S| C )) (g)| dg.(3)$To bound this value, we first prove the following for a fixed y ∈ [-1, 1].

$Pr |cdf g(πρ(S| C )) (y) -cdf g(πρ(S| C )) (y)| ≥ α ≤ µ.(4$) Then, we observe that there are at most |X ∩ C| + 1 effectively different values of y we need to consider with respect to any fixed g and C. (For every realization of S, S, cdf g(πρ(S| C )) can only change for values of y on which cdf g(πρ(X| C )) changes. These values correspond to the partitioning of [-1, 1] into intervals induced by applying g • π ρ (•) to the elements in X ∩ C.) By union bounding over these |X ∩ C| + 1 effectively different values of y, Equation (4) gives us the following. Pr sup y∈[-1,1]

$|cdf g(πρ(S| C )) (y) -cdf g(πρ(S| C )) (y)| ≥ α ≤ (1 + |X ∩ C|)µ.(5)$Finally, substituting the bound from Equation ( [5](#formula_43)) in Equation ( [3](#formula_41)) proves the lemma .

To show Equation ( [4](#formula_42)), we redefine the sampling process in a way that will allow us to use Theorem 5, a concentration result for the hypergeometric distribution (See Definition 10 for a definition of this distribution): Consider an urn consisting of n balls. Among those n balls, m are marked with a red stripe, representing membership in C ∩ X. Among the m red-striped balls, t are further marked with a blue stripe, representing x ∈ C ∩ X such that g(πρ (x)) ≤ y for the value of y being considered. Consider the experiment where we sample n/2 balls without replacement, and define the joint pair of random variables (V, W ) where V counts the number of red-striped balls in the sample, (i.e., the number of sampled points that are in X ∩ C) and W counts the number of (red and) blue-striped balls in the sample, (i.e., the number of sampled points x that are in X ∩ C and satisfy g(πρ (x)) ≤ y.

The random variables W and V follow hypergeometric distributions as follows:

$V ∼ H(n, m, n/2) (W |V = v) ∼ H(m, t, v).$Observe that the absolute value of the CDF difference we are trying to bound is equal to

$W V -t-W$m-V by definition. Let E 1 be the event that the number of red-striped balls in the sample is close to its expected value (i.e., |V -m/2| < m/4). Then applying Theorem 5 and using m > 40 and m > 8.3 ln(4/µ) we have that

$Pr[E 1 ] < 2 exp -2 m + 1 • (m/4) 2 -1 ≤ 2 exp -2 1.025m • 0.99(m/4) 2 = 2 exp 1.98 16.4 m < µ/2.$Now let us condition on a realization V = v. Given this, let E 2 be the event that the number of blue-striped balls in the sample is close to its expected value (i.e., W -tv m ≤ ζ). Then applying Theorem 5 for ζ ≥ 2 and c as in the theorem:

$Pr[E 2 V = v] < 2 exp -2c • ζ 2 -1 . Observe that c = max 1 v + 1 + 1 m -v + 1 , 1 t + 1 + 1 m -t + 1 ≥ 1 t + 1 + 1 m -t + 1 ≥ 2 m 2 + 1 . Therefore, Pr[E 2 V = v] < 2 exp -4 m 2 + 1 • ζ 2 -1 .(6)$Assume that both events E 1 and E 2 hold. Then in this case we will argue that:

$W V - t -W m -V < mζ m 2 /4 -γ 2 = α.(7)$We will then substitute the derived value of ζ into Equation [6](#formula_47)to show that

$Pr[E 2 V = v] < µ/2.$To this end, observe that if the number of blue-striped balls in the sample is within ζ of the expected value, tV m , then the number of blue-striped balls in the sample is also within ζ of its own expected value, t(m-V ) m . This is because the balls can only be in one of these two sets. Therefore, when both events E 1 and E 2 hold, we have that:

$(t -W ) - t(m -V ) m < ζ.$Therefore,

$W V - t -W m -V < E[W ] + ζ • 1 V -E[t -W ] -ζ 1 m -V because |W - tV m | < ζ = tV m + ζ • 1 V - t(m -V ) m -ζ • 1 m -V = mζ V (m -V ) < mζ m 2 /4 -γ 2 (because |V -m/2| < γ) . Similarly, t -W m -V - W V < mζ m 2 /4 -γ 2 .$This gives us Equation 7. We can now set α = mζ m 2 /4-γ 2 to get that ζ = 3mα 16 . Now, substituting this ζ value back into Equation 6 and using m > 40, m > 16/3α, and m > 4.15 α 2 ln(4/µ) we have the following:

$Pr[E 2 V = v] < exp -4 m 2 + 1 • ζ 2 -1 = 2 exp -4 m 2 + 1 • 9m 2 α 2 16 2 -1 ≤ 2 exp -8 1.05m • 0.9 • 9m 2 α 2 16 2 = 2 exp 8.1α 2 m 33.6 < µ/2.$Finally, we get the following:

$Pr W V - t -W m -V > α ≤ Pr[E 1 ∨ E 2 ] ≤ µ.$With Claims 1 and 2 now proved, this concludes the proof of Theorem 6.

Proof of Theorem 6. In the remaining part of this section, we use Lemma 1 to prove that any data curation algorithm A with bounded max-information also enforces wassersteincoherence.

Proof Of Theorem 6. Fix η > 0. Fix any subpopulation C ∈ C. Consider any dataset X.

Then for any algorithm L : Y → {X → [-1, 1]}, dataset X ∈ X n , and µ > 0, such that

$|X ∩ C| ≥ max 4.15 • ln(4/µ) α 2 , 5.3 α , 8.3 • ln(4/µ), 40 ,(8)$we have the following (by Lemma 1)

$Pr Xa,X b ←X,h←L•A(Xa) dist W 1 h(πρ (X a | C )), h(πρ (X b | C )) > α ≤ 2(|X ∩ C| + 1)µ • e ζ + η (9)$where the choice of split X a , X b $ ← -X and the computed predictor h ← L(X a ) are as in the DemCoh L•A,X,C * ,ρ (α) experiment in Figure [1](#).

We need to show, instead, that for

$γ = max 8.3 • (ζ + ln(16|C|/β)) α 2 , 36 ln((3/α) α 2 , 16.6 • (ζ + ln(16|C|/β)), 5.3 α ,80$, and all algorithms L : Y → (X → [-1, 1]), all datasets X ∈ X n , the probability the following holds for all C ∈ C (such that |X| C | ≥ γ,) is low:

$dist W 1 h π ρ (X a | C ) , h π ρ (X b | C ) > α,$where the split X a , X b $ ← -X and predictor h ← L(X a ) are as in Figure [1](#).

To that end, we start by considering the fixed subpopulation C and setting µ = η/ 2(|X ∩ C| + 1) • e ζ (ensuring that the RHS of Equation ( [9](#)) is 2 η ). The main content in the proof will be arguing that the following lower bound on the size of |X ∩ C| implies the condition in Equation [(8)](#b7). We can then union bound over all sub-populations in C to get the theorem.

$|X ∩ C| ≥ max 8.3 • (ζ + ln(16/η)) α 2 , 36 ln(3/α) α 2 , 16.6 • (ζ + ln(16/η)), 5.3 α , 80 .(10)$Substituting the value of µ back in Equation ( [8](#formula_54)), the first term in the max corresponds to the condition

$|X ∩ C| ≥ 4.15 • ln(8(|X ∩ C| + 1)e ζ /η)$α 2 which can also be written as:

$|X ∩ C| ≥ 4.15 • ln((|X ∩ C| + 1) + g α 2 where g = 4.15 • (ζ + ln(8/η)). Assume |X∩C| 2 ≥ 4.15•ln((|X∩C|+1) α 2$. Then, as long as |X ∩ C| ≥ 2g α 2 , the condition is satisfied. Now, using the fact that |X ∩ C| ≥ 80, we have that ln((|X ∩ C| + 1) ≤ 1.01 ln((|X ∩ C| + 1), which implies that it suffices for |X ∩ C| ≥ 9•ln((|X∩C|)  . Similarly, to be larger than the third term in the max in Equation ( [8](#formula_54)), we need

$|X ∩ C| ≥ 8.3 • ln(8(|X ∩ C| + 1)e ζ /η)$which can also be written as

$|X ∩ C| ≥ 8.3 • ln((|X ∩ C| + 1) + g where g = 8.3 • (ζ + ln(8/η)). Assume |X∩C| 2 ≥ 8.3 • ln((|X ∩ C| + 1)$. Then, as long as |X ∩ C| ≥ 2g, the condition is satisfied. Now, using the fact that |X ∩ C| ≥ 80, we have that ln((|X ∩ C| + 1) ≤ 1.01 ln((|X ∩ C| + 1), which implies that it suffices for |X ∩ C| ≥ 18 • ln((|X ∩ C|),which is true as long as |X ∩ C| ≥ 80, hence this case is taken care of.

Hence, for a single subpopulation C we have argued that it is sufficient that

$|X ∩ C| ≥ max 8.3 • (ζ + ln(8/η)) α 2 , 36 ln(3/α) α 2 , 16.6 • (ζ + ln(8/η)), 5.3 α , 80 .(11)$Setting η = β/2|C|, and applying a union bound on Equation (9) (over all subpopulations in the class) then gives the theorem.

## Differentially Private Algorithms Enforce Demographic Coherence

In this section, we argue that our definition of demographic coherence can be achieved via differentially private algorithms. We do this by adapting known connections between differential privacy and max-information, and Theorem 6 connecting max-information and demographic coherence.

The proofs for pure differential privacy and approximate differential privacy follow a similar flavor. Firstly, we adapt known connections between (pure and approximate) differential privacy and max-information to the setting of sampling without replacement. Then, we use Theorem 6 (connecting bounded max-information to demographic coherence) to argue that differential privacy implies demographic coherence.

$Theorem 7. [Pure-DP Enforces Wasserstein Coherence] Fix any ε, β ∈ (0, 1], α ∈ (0, 1], n ∈ N.$Let C be a collection of subpopulations C ∈ X * . Consider an order-invariant ε-DP algorithm A : X n/2 → Y. Fix any lens ρ. Then, A enforces (α, β)-Wasserstein-coherence with respect to collection C, lens ρ, and size constraint γ, where

$γ = max 8.3 • (ε 2 n/4 + ε n ln(4|C|/β)/2 + ln(16|C|/β)) α 2 , 36 ln((3/α) α 2 , 16.6 • (ε 2 n/4 + ε n ln(4|C|/β)/2 + ln(16|C|/β)), 5.3 α , 80 .(12)$Proof. Fix β > 0. By Theorem 11 connecting differential privacy and max-information, we have that we have that,

$I β/2|C| ∞ (A, n/2) ≤ ε 2 n/4 + ε n ln(4|C|/β)/4.$Applying Theorem 6 and substituting the bound on max-information then completes the proof.

Theorem 8. [Approx-DP Enforces Wasserstein Coherence] Fix any β ∈ (0, 1], α ∈ (0, 1], n ∈ N. Let ε ∈ (0, 1  2 ], and δ ∈ (0, ε 2 β 2 (120n) 2 |C| 2 ] Let C be a collection of subpopulations C ∈ X * . Consider an order-invariant[foot_13](#foot_13) (ε, δ)-DP algorithm A : X n/2 → Y. Fix any lens ρ. Then, A enforces (α, β)-Wasserstein-coherence with respect to collection C, lens ρ, and size constraint γ, where γ = max 8.

3 • (265ε 2 n + 12ε n ln(4|C|/β) + ln(32|C|/β)) α 2 , 36 ln((3/α) α 2 , 16.6 • (265ε 2 n + 12ε n ln(4|C|/β) + ln(32|C|/β)), 16/3 α , 80 .

Proof. Fix β > 0 and γ = β 2|C| . By Corollary 2 connecting differential privacy and maxinformation, we have that we have that, as long as δ ∈ (0,

$ε 2 β 2 (120n) 2 |C| 2 ] I β/2|C| ∞ (A, n/2) ≤ 265 2 ε 2 n + 12ε n 2 ln(4|C|/β).$Applying Theorem 6 and substituting the bound on max-information then completes the proof.

## A Comparison to Perfect Generalization

In this section we include a comparison between demographic coherence (Definition 3) and the notion of perfect generalization introduced by Cummings et al. [[18]](#b13). (See also the work of Bassily and Freund [[7]](#b6) that independently introduced a generalization thereof.) This notion was originally meant to capture generalization under post-processing but has since been shown to be closely related to other desirable properties as well (e.g., replicability [[8]](#b7)). Our framework shares conceptual similarities with this definition, but the technical details differ in important ways.

The following comparison uses the definition of sample perfect generalization (Definition 11) from [[8]](#b7) which is roughly equivalent to the original definition from [[18]](#b13). Intuitively, a mechanism M running on i.i.d. samples from some distribution (sample) perfectly generalizes if the distribution of its output does not depend "too much" on specific realization of its sampled input. That is, its output distributions when run on two i.i.d samples from any distribution are indistinguishable.

Definition 11 (Sample perfect generalization [[8,](#b7)[Def 3.4]](#) ). An algorithm A : X m → Y is said to be (β, ǫ, δ)-sample perfectly generalizing if, for every distribution D over X , with probability at least 1 -β over the draw of two i.

$i.d. samples X a , X b ∼ D m , A(X a ) ≈ ǫ,δ A(X b ),$where ≈ ǫ,δ denotes ǫ, δ indistinguishability. Definition 3 has several noticeable syntactic differences when compared to Definition 11. First, demographic coherence is defined within a specific framework that explicitly lays out the entire data release pipeline, a design choice that intentionally lends itself to concrete intuition (and experimental evaluation) of data release. However, this still leaves open the possibility that the core statistical guarantee of demographic coherence is roughly equivalent to perfect generalization. In other words, it may still be the case that demographic coherence is simply a different way to describe the protections offered by perfect generalization; as we see below, this is not the case. Second, while "closeness" in the definition of perfect generalization is required for distributions over the entire sets X a , X b , the "closeness" in the definition of demographic coherence is required for distributions over the sets X a | C , X b | C for subpopulations C ⊆ X from some collection C. For the sake of drawing a more direct comparison here, we collapse this difference by comparing sample perfect generalization to demographic coherence with C = {X }.

A third difference in the definitions is is the choice of sets X a , X b that the comparison is made with respect to, i.e., a random partition of a fixed dataset in demographic coherence vs. i.i.d. draws from a distribution in the case of perfect generalization. The choice of random partitioning in our framework is made to ensure concreteness and applicability in census-like settings but it is chosen intentionally to maintain both intuitive and quantitative similarities to i.i.d. sampling. Thus, we view this as more of a difference in interpretability and applicability of the definitions, rather than one about their underlying guarantees.

The main difference between the two definitions, thus, is in how how "closeness" is measured-as spelled out in Figure [2](#fig_1).

$An algorithm A : X n/2 → Y is: 1. (β, ε, δ)-sample perfectly generalizing if ∀ distributions D over X , with probability at least 1 -β over X a , X b ∼ D n/2 : A(X a ) ≈ ε,δ A(X b ) 2. (α, β)-coherence enforcing if ∀ datasets X ∈ X n , learners L : Y → (X → [-1, 1]$), with probability at least 1 -β over the random split X a ∪ X b = X and the coins of A, L:

$dist W (h a (X a ), h a (X b )) ≤ α$where h a ← L • A(X a ) is a confidence rated predictor, and h a (X i ) is the distribution induced by randomly choosing x ∼ X i and computing h a (x). Perfect generalization asks that w.h.p. over independent samples, A produces indistinguishable distributions over reports R a ← A(X a ) and R b ← A(X b ). Meanwhile, coherence enforcement asks that w.h.p. L • A(X a ) produces a confidence rated predictor h a : X → [-1, 1] which has "similar" predictions on X a and X b (a property enforced by A). That is, the comparison in perfect generalization is on the behavior of the algorithm A itself, while the comparison in demographic coherence is on the likely behavior of a realized hypothesis h a that is produced only over the report R a ← A(X a ).

Since coherence enforcement limits the set of algorithms against which indistinguishability applies, one might expect that perfectly generalizing algorithms also enforce demographic coherence, and indeed, Theorem 6 proves this to be true. However, the converse need not be true-implying that demographic coherence is a relaxation of perfect generalization. In particular, the example below shows a set X such that no confidence rated predictor h : X → [-1, 1] violates this property. In this case, all data curators are vacuously coherence enforcing.

Consider a data curator A : {0, 1} n/2 → {0, 1} n/2 that (deterministically) publishes its input in the clear. This is clearly not perfectly generalizing as the distribution of the report X a is a point mass that (for reasonable choices of X, with high probability) is distinct from the distribution of the report X b .

Meanwhile, considering a dataset X ∈ {0, 1} n and a random split X a , X b ← X of the dataset, there are two possible predictors h : {0, 1} → [-1, 1] that witnesses the highest possible Wasserstein distance when run on X a vs. X b . That is, without loss of generality h is either h(0) = -1, h(1) = 1 or h(0) = 1, h(1) = -1. In either case, h cannot be improved even by seeing X a in the clear. So, any data curation algorithm in this scenario, is coherence enforcing since the data itself doesn't have enough complexity to allow for a violation of demographic coherence. Note that the absence of information correlated with the bits contained in the dataset X (e.g., time, location, computer system) is crucial to this example.

## B Differential Privacy implies Bounded Max-Information: Sampling without Replacement

Prior work shows that for datasets sampled i.i.d., differentially private algorithms have bounded max-information [[23,](#b18)[47]](#b42). In this appendix we prove Theorem 11 and Theorem 12, which are analogs of those theorems for sampling without replacement.

## B.1 Preliminaries

First we state Theorem 9, a version of McDiarmid's inequality that applies to the case of sampling without replacement. This result follows from Lemma 2 in [[54]](#b49). This result in used in the proof of Theorem 11, which says that pure-DP algorithms have bounded maxinformation (even in the case of sampling without replacement.)

Definition 12. A function f : X m → Y, is called order invariant if for all X ∈ X m , the value of the function f (X) does not depend on the order of the elements of X.

Theorem 9 (McDiarmid's for sampling without replacement [[54]](#b49)). Let f : X n → Y be an an order invariant function with global sensitivity ∆ > 0. Let X be a data universe of size n, let S be a sample of size m chosen without replacement from X . Then for t ≥ 0,

$Pr S [f (S) -E[f (S)] ≥ t] ≤ exp - 2t 2 m∆ 2 • n -1/2 n -m • 1 - 1 2 max(m, n -m)$In particular, for m = n/2 and n ≥ 3,

$Pr[f (S) -E[f (S)] ≥ t] ≤ exp - 4t 2 n∆ 2$Next we state some lemmas that are used in the proof of Theorem 12 and Corollary 2 which say that approximate-DP algorithms have bounded max-information (even in the case of sampling without replacement.) Definition 13 (Point-wise indistinguishibility [[38]](#b33)). Two random variables A, B are pointwise (ε, δ)-indistinguishable if with probability at least 1 -δ over a ∼ P (A):

$e -ǫ Pr [B = a] ≤ Pr [A = a] ≤ e ǫ Pr [B = a] .$Lemma 2 (Indistinguishability implies Pointwise Indistinguishability [[38]](#b33)). Let A, B be two random variables. If A ≈ ε,δ B then A and B are pointwise 2ε, 2δ  1-e -ε -indistinguishable.

Lemma 3 (Conditioning Lemma [[38]](#b33)). Suppose that (A, B) ≈ ε,δ (A ′ , B ′ ). Then for every δ > 0, the following holds:

$Pr t∼P (B) A| B=t ≈ 3ǫ, δ A ′ | B ′ =t ≥ 1 - 2δ δ - 2δ 1 -e -ε .$Theorem 10 (Azuma's Inequality). Let C 1 , • • • , C n be a sequence of random variables such that for every i ∈ [n], we have

$Pr [|C i | ≤ α] = 1$and for every fixed prefix C i-1

$1 = c i-1 1 , we have E C i |c i-1 1 ≤ γ,$then for all t ≥ 0, we have

$Pr n i=1 C i > nγ + t √ nα ≤ e -t 2 /2 .$
## B.2 Pure-DP =⇒ Bounded Max-Information

In this appendix we state Theorem 11, which is an analog of Theorem from [[23]](#b18). The proof of this theorem works exactly as in [[23]](#b18), except replacing the application of McDiarmid's Lemma with a version of McDiarmid's for sampling without replacement (Theorem 9) which we state in Appendix B.1.

Theorem 11. (Pure-DP =⇒ Bounded Max-Information) Fix n ∈ N, ε > 0 and let X be a data universe of size at least n. Let A : X n/2 → Y be an order-invariant ε-DP algorithm.

Then for any γ > 0,

$I γ ∞ (A, n/2) ≤ ε 2 n/4 + ε n ln(2/γ)/4.$
## B.3 (ε, δ)-DP =⇒ Bounded Max-Information

In this appendix we prove Theorem 12, which is an analog of Theorem 1 from Rogers et al. [[47]](#b42). In fact, the following proof is almost exactly the proof of them from [[47]](#b42) with the following differences: (1) we compute all the constants exactly and avoid using asymptotic notation, and (2) we keep the tunable parameters in the final version of the theorem to obtain the most flexible result that we can. Finally, we set the parameters in Theorem 12 to get Corollary 2, which is used in the proof of Theorem 8.

Theorem 12. (Approx-DP =⇒ Bounded Max-Information, Generalised) Let A : X n → Y be an (order-invariant) (ε, δ)-differentially private algorithm for ε ∈ (0, 1/2], δ ∈ (0, ε). For δ ∈ (0, ε/15], t > 0, and β(t, δ) = e -t 2 /2 + n 2δ δ + 2 δ+2δ

1-e -3ε

we have

$I β ∞ (A, n) ≤ n   347 δ + 75 δ ε 2 + 24 δ2 ε + 240ε 2   + 6tε √ n. Corollary 2. (Approx-DP =⇒ Bounded Max-Information, Specific) Fix n ∈ N, for ε ∈ (0, 1/2], γ ∈ (0, 1], δ ∈ (0, ε 2 γ 2 (120n) 2 ]$and let X be a data universe of size at least n. Let A : X n → Y be an (order-invariant) (ε, δ)-differentially private algorithm. We have that

$I γ ∞ (A, n) ≤ 265ε 2 n + 12ε n ln(2/γ).$We will sometimes abbreviate conditional probabilities of the form Pr [X = x | A = a] as Pr [X = x | a] when the random variables are clear from context. Further, for any x ∈ X n and a ∈ Y, we define

$Z i (a, x [i] ) def = log Pr X i = x i | a, x [i-1] Pr X i = x i | x [i-1] .(14)$Z(a, x)

$def = log Pr x [A(x) = a, X = x] Pr [A = a] • Pr [X = x] = n i=1 Z i (a, x [i] )(15)$If we can bound Z(a, x) with high probability over (a, x) ∼ p(A(X), X), then we can bound the approximate max-information by using the following lemma: Lemma 4 ([23, [Lemma 18]](#)).

## Pr log Pr

$x [A(x) = a, X = x] Pr [A = a] • Pr [X = x] ≥ k ≤ β =⇒ I β ∞ (A(X); X) ≤ k.$To bound Z(a, x) with high probability over (a, x) ∼ p(A(X), X) we will apply Azuma's inequality (Theorem 10) to the sum of the Z i (a, x [i] )'s. For this we must first argue that each Z i (a, x [i] ) term is bounded with high probability: Claim 3. Let δ > 0, and δ ′′ def = 2 δ 1-e -3ε . If A is (ε, δ)-differentially private and, X ∈ X n is sampled without replacement from a finite universe X , then for each i ∈ [n], and each prefix x [i-1] ∈ X i-1 and answer a, we have:

$Pr x i ∼X i |x [i-1] log Pr X i = x i | a, x [i-1] Pr X i = x i | x [i-1] ≤ 6ε ≥ 1 -δ ′′ Proof. Whenever X i | x [i-1] and X i | a,x [i-1] are 3ǫ, δ -indistinguishable, Lemma 2 tells us that X i | x [i-1] and X i | a,x [i-1] are point-wise (6ǫ, δ ′′ )-indistinguishable. i.e., given that X i | x [i-1] and X i | a,x [i-1] are 3ǫ, δ -indistinguishable, we have that Pr x i ∼X i |x [i-1] log Pr X i = x i | a, x [i-1] Pr X i = x i | x [i-1] ≤ 6ε ≥ 1 -δ ′′ Claim 4. Let δ > 0, δ ′ def = 2δ δ + 2δ$1-e -ε , and δ ′′ def = 2 δ 1-e -3ε . If A is (ε, δ)-differentially private and, X ∈ X n is sampled without replacement from a finite universe X , then for each i ∈ [n], and each prefix x [i-1] ∈ X i-1 we have:

$Pr x i ∼X i |x [i-1] a∼A|x [i-1] log Pr X i = x i | a, x [i-1] Pr X i = x i | x [i-1] ≤ 6ε ≥ 1 -δ ′ -δ ′′ Proof.$For this proof, we use Claim 3 and then show for each i ∈ [n], and prefix

$x [i-1] ∈ X i-1 , Pr a∼p A|x [i-1] X i | x [i-1] ≈ 3ε, δ X i | a,x [i-1] ≥ 1 -δ ′ .$We use the differential privacy guarantee on A to show that (A,

$X i )| x [i-1] ≈ ε,δ A| x [i-1] ⊗ X i | x [i-1]$. The above equation then follows directly from the conditioning lemma Lemma 3.

Fix any set O ⊆ Y × X and prefix x [i-1] ∈ X i-1 . From the differential privacy of A, and the order-invariance of the algorithm, we get the following (where the first inequality follows from DP.):

$Pr (A(X), X i ) ∈ O | x [i-1] = x i ∼X i |x [i-1] Pr X i = x i | x [i-1] Pr (A(X), x i ) ∈ O | x [i-1] , x i ≤ x i ∼X i |x [i-1] Pr X i = x i | x [i-1] e ε Pr (A(X), x i ) ∈ O | x [i-1] , t i + δ ∀t i ∼ X i | x [i-1] = x i ,t i ∼X i |x [i-1] Pr X i = t i | x [i-1] Pr X i = x i | x [i-1] e ε Pr (A(X), x i ) ∈ O | x [i-1] , t i + δ = x i ∼X i |x [i-1] Pr X i = x i | x [i-1] e ε Pr (A(X), x i ) ∈ O | x [i-1] + δ ≤ e ε   x i ∼X i |x [i-1] Pr X i = x i | x [i-1] Pr A(X), X i ) ∈ O | x [i-1]   + δ = e ε Pr A(X) ⊗ X i ∈ O | x [i-1] + δ15$Applying a very similar argument, will give us that

$Pr A(X) ⊗ X i ∈ O | x [i-1] ≤ e ε Pr (A(X), X i ) ∈ O | x [i-1] + δ.$Having shown a high probability bound on the terms Z i , our next step is to bound their expectation so that we can continue towards our goal of applying Azuma's inequality.

We will use the following shorthand notation for conditional expectation:

$E Z i (A, X [i] ) | a, x [i-1] , |Z i | ≤ 6ε def = E Z i (A, X [i] ) | A = a, X [i-1] = x [i-1] , |Z i (A, X [i] )| ≤ 6ε ,$Lemma 5. Let A be (ε, δ)-differentially private and, X ∈ X n be sampled without replacement from a finite universe X . Let ε ∈ (0, 1/2] and δ ∈ (0, ε/15],

$X i | x [i-1] ≈ 3ε, δ X i | a,x [i-1] =⇒ E Z i (A, X [i] ) | a, x [i-1] , |Z i | ≤ 6ε = O(ε 2 + δ).$More precisely, E Z i (A,

$X [i] ) | a, x [i-1] , |Z i | ≤ 6ε ≤ ν( δ)$, where ν( δ) is defined in [(16)](#b11).

$Proof. Let S def = {x i | a, x [i-1] , |Z i | < 6ε}.$Given an outcome and prefix (a,

$x [i-1] ) such that X i | x [i-1] ≈ 3ε, δ X i | a,x [i-1] ,$we have the following by definition:

$E Z i (A, X [i] ) | a, x [i-1] , |Z i | ≤ 6ε = x i ∈S Pr X i = x i | a, x [i-1] , |Z i | ≤ 6ε Z i (a, x [i] ) = x i ∈S Pr X i = x i | a, x [i-1] , |Z i | ≤ 6ε log Pr[X i =x i |a,x [i-1]] Pr[X i =x i |x [i-1]]$Claim 5.

$x i ∈S Pr X i = x i | x [i-1] log Pr[X i =x i |a,x [i-1]] Pr[X i =x i |x [i-1]] ≤ log 1-Pr[X i / ∈S|a,x [i-1]] 1-Pr[X i / ∈S|x [i-1]]$Proof.

$x i ∈S Pr X i = x i | a, x [i-1] log Pr[X i =x i |a,x [i-1]] Pr[X i =x i |x [i-1]] = Pr X i ∈ S | x [i-1] x i ∈S Pr[X i =x i |x [i-1]] Pr[X i ∈S|x [i-1]] log Pr[X i =x i |a,x [i-1]] Pr[X i =x i |x [i-1]] ≤ x i ∈S Pr[X i =x i |x [i-1]] Pr[X i ∈S|x [i-1]] log Pr[X i =x i |a,x [i-1]] Pr[X i =x i |x [i-1]] ≤ log x i ∈S Pr[X i =x i |a,x [i-1]] Pr[X i ∈S|x [i-1]] ≤ log Pr[X i ∈S|a,x [i-1]] Pr[X i ∈S|x [i-1]] = log 1-Pr[X i / ∈S|a,x [i-1]] 1-Pr[X i / ∈S|x [i-1]]$The first inequality follows form the fact that all probabilities are less than one. The second inequality follows from noticing that x i ∈S

$Pr[X i =x i |x [i-1]]$Pr[X i ∈S|x [i-1]] = 1 and applying Jensen's inequality.

Let

$Pr X i / ∈ {x i | a, x [i-1],|Z i |<6ε } | a, x [i-1] = Pr X i / ∈ S | a, x [i-1] def = q. Note that, because X i | x [i-1] ≈ 3ε, δ X i | a,x [i-1]$, we have for δ > 0:

$Pr X i / ∈ S | x [i-1$] ≤ e 3ε Pr X i / ∈ S | a, x [i-1] + δ = e 3ε q + δ Note that q ≤ δ ′′ by Claim 3. Now, we can bound the following:

$x i ∈S Pr X i = x i | x [i-1] log Pr[X i =x i |a,x [i-1]] Pr[X i =x i |x [i-1]] ≤ log 1-Pr[X i / ∈S|a,x [i-1]] 1-Pr[X i / ∈S|x [i-1]]$≤ log(1 -q) -log(1 -(e 3ε q + δ))

≤ log(e) • (-q + e 3ε q + δ + 2(e 3ε q + δ) 2 )

= log(e) • ((e 3ε -1)q + δ + 2(e 3ε q + δ) 2 )

$def = τ ( δ)$where the second inequality follows by using the inequality (-x -2x 2 ) log(e) ≤ log(1 -x) ≤ -x log(e) for 0 < x ≤ 1/2, and as (e 3ε q + δ) ≤ 1/2 for ε and δ bounded as in the lemma statement.

We use the results above to to upper bound the expectation we wanted:

$E Z i (A, X [i] ) | a, x [i-1] , |Z i | ≤ 6ε ≤ xi∈S Pr X i = x i | a, x [i-1] , |Z i | ≤ 6ε log Pr[Xi=xi|a,x [i-1]] Pr[Xi=xi|x [i-1] ] - xi∈S Pr X i = x i | x [i-1] log Pr[Xi=xi|a,x [i-1]] Pr[Xi=xi|x [i-1]] + τ ( δ) = xi∈S Pr X i = x i | a, x [i-1] , |Z i | ≤ 6ε -Pr X i = x i | x [i-1] log Pr[Xi=xi|a,x [i-1]] Pr[Xi=xi|x [i-1]] + τ ( δ) ≤ |Zi|≤6ε 6ε xi∈S |Pr X i = x i | a, x [i-1] , |Z i | ≤ 6ε -Pr X i = x i | x [i-1] | + τ ( δ) ≤ Def of S, Claim 3 6ε xi∈S Pr X i = x i | x [i-1] max e 6ε Pr |Z i | ≤ 6ε | a, x [i-1] -1, 1 - e -6ε Pr |Z i | ≤ 6ε | a, x [i-1]$+ τ ( δ)

$≤ Claim 3 6ε e 6ε$1-2 δ 1-e -3ε -1 + τ ( δ) ≤ Substituting for τ ( δ) 6ε e 6ǫ 1 + 4 δ 1-e -3ǫ -1 + log(e) • ((e 3ε -1)q + δ + 2(e 3ε q + δ) 2 ) ≤ Upper bound for q 6ε e 6ǫ 1 + 4 (1 -e -3ε ) + 8 δe 3ε + δ + 2 δ2 = b 24εe 6ε + 2e 3ε -2 + 8 δe 6ε

(1 -e -3ε ) + 8 δe 3ε + δ + 2 δ2 + 6ε(e 6ε -1) = δ 1 -e -3ε 2e 3ε (4e 3ε (3ε + δ 1 -e -3ε )) + 4 δ + 1) -2 + δ + 2 δ2 + 6ε(e 6ε -1)

$≤ e -3ε$≤1-1.5ε for ε∈[0,0.5] 2 δ 1.5ε e 3ε 4e 3ε (3ε + δ 1.5ε )) + 4 δ + 1 + δ -2 1.5ε + 2 δ + 1 + 6ε(e 6ε -1) ≤ 8 e 6ε δ 1.5ε 3ε + δ 1.5ε + 2 δ 1.5ε e 3ε 4 δ + 1 + δ -2 1.5ε + 2 δ + 1 + 6ε(e 6ε -1) ≤ e 3ε ≤1+7ε,e 6ε ≤1+40ε for ε∈[0,0.5] 8 (1 + 40ε) δ 1.5ε 3ε + δ 1.5ε + 2 (1 + 7ε) δ 1.5ε 4 δ + 1 + δ -2 1.5ε + 2 δ + 1 + 6ε(40ε) ≤ (8 + 320ε) δ 1.5ε 3ε + δ 1.5ε + (2 + 14ε) 1.5ε 4 δ2 + δ -2 δ 1.5ε + 2 δ2 + δ + 240ε 2 ≤ ε<0.5 168 δ 1.5ε (3ε + δ 1.5ε ) + 8 1.5 δ2 ε + 56 1.5 δ2 + 14 1.5 δ + 2 δ2 + δ + 240ε 2 ≤ ε≤0.5 347 δ + 75 δ ε 2 + 24 δ2 ε + 240ε 2 def = ν( δ)

Finally, we need to apply Azuma's inequality (stated in Theorem 10) to a set of variables that are bounded with probability 1, not just with high probability. Towards this end, we now define (1) the sets G i ( δ) and G ≤i ( δ) of "good" tuples of outcomes and databases, and (2) a variable T i that will match Z i for "good events", and will be zero otherwise-and hence, is always bounded:

$G i ( δ) = (a, x [i] ) |Z i (a, x [i] )| ≤ 6ε & X i | x [i-1] ≈ 3ε, δ X i | a,x [i-1] ,(17)$G ≤i ( δ) = (a,

$x [i] ) : (a, x 1 ) ∈ G 1 ( δ), • • • , (a, x [i] ) ∈ G i ( δ)(18)$T i (a,

$x [i] ) = Z i (a, x [i] ) if (a, x [i] ) ∈ G ≤i ( δ) 0 otherwise(19)$Note that the variables T i indeed satisfy the requirements of Azuma's inequality. The first condition, Pr |T i (A, X [i] )| ≤ 6ε = 1 holds by definition, and the second holds because of Lemma 5.

We are now ready to prove our main theorem.

![a lens ρ, and a distance metric dist(•, •).]()

![Consider the inequality |X∩C| ln((|X∩C|) ≥ c. Note that the left hand side is an increasing function of |X ∩ C|. Let |X ∩ C| ≥ 2c ln c. Then, we get that |X∩C| ln((|X∩C|) ≥ 2c ln c ln(2c ln c) , and some arithmetic shows that for c ≥ 9 (which is true whenever α ≤ 1), the right hand side is indeed larger than c. Hence, it is additionally sufficient that |X ∩ C| ≥ 36 ln((3/α)]()

![]()

![Figure 2: Comparing the definition of sample perfect generalization to a simplified definition of demographic coherence.]()

![e -3ǫ -1 + log(e) • (e 3εe -3ε ) 2 + e -3ε = b= δ 1-e -3ε 6ε e 6ǫ (1 + 4b) -1 + log(e) • b 2e 3ε -2 + 8 δe 6ε]()

In this work we intentionally us the term "algorithm" broadly to capture, e.g., informal decision-making process made by humans that might not be explicitly codified as algorithms in the traditional sense.

We use ergonomic in this context to mean ease of use by many different stakeholders. We intentionally move away from the term "usable," as this typically focuses only on end-users and we are interested in ease of use from a more diverse set of communities.

e.g., reconstruction of features like gender can be carried out simply from knowing population statistics rather than breaking anonymity[[48]](#b43).

In reality, the property of demographic coherence applies to algorithms L that use private reports to design predictors.

In our formal experiment, we actually suppress the formal object of the report. Specifically, we reason directly about the composition of some data processing algorithm A with an arbitrary algorithm L, rather than making the report an explicit object that is then passed to L.

A more complex interpretation could also cover an approach to prediction that takes into account a social decision-making process. While this interpretation is beyond the technical scope of our work, it may be interesting to consider in future work.

The notion of similarity is obviously a loaded one, as the ways in which two individuals are similar or different depend on the types of predictions being made about them. We eventually handle this by quantifying over many notions of similarity. For the sake of this motivation, it is enough to assume that the similarity of these individuals is meaningful with respect to the characteristic being predicted about them.

As this approach sketch is mainly to motivate our final approach below, we gloss over some formalities in this description. For example, how do we know that L does not act differently when provided one input (Aux) and two inputs (Aux and R)?

We borrow this conceptual approach from[[31]](#b26).

The predictor may also have side information about individuals not contained in the dataset, which can be formally included in the description of the adversarial algorithm L.

In a real-world scenario, X might be sampled from some underlying population, but our definition does not require this and makes no assumptions about how it might be done.

This definition, for sampling without replacement, is slightly different than the original one.

Since it is an assumption in the version of McDiarmid's Inequality for sampling without replacement (Theorem 9) that we use.

Order-invariance can be relaxed by multiplying ε by 2 in the γ value, and dividing by 2 in the range of δ.

In the step where we apply DP, the parameters will double if the algorithm is not order-invariant.

