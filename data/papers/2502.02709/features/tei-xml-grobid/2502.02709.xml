<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Enforcing Demographic Coherence: A Harms Aware Framework for Reasoning about Private Data Release</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-02-04">4 Feb 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mark</forename><surname>Bun</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Marco</forename><surname>Carmosino</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Palak</forename><surname>Jain</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Gabriel</forename><surname>Kaptchuk</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Satchit</forename><surname>Sivakumar</surname></persName>
						</author>
						<title level="a" type="main">Enforcing Demographic Coherence: A Harms Aware Framework for Reasoning about Private Data Release</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-02-04">4 Feb 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">E9B2A4A86A934B6EB38D7C066F36C2C1</idno>
					<idno type="arXiv">arXiv:2502.02709v1[cs.CR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The technical literature about data privacy largely consists of two complementary approaches: formal definitions of conditions sufficient for privacy preservation and attacks that demonstrate privacy breaches. Differential privacy is an accepted standard in the former sphere. However, differential privacy's powerful adversarial model and worst-case guarantees may make it too stringent in some situations, especially when achieving it comes at a significant cost to data utility. Meanwhile, privacy attacks aim to expose real and worrying privacy risks associated with existing data release processes but often face criticism for being unrealistic. Moreover, the literature on attacks generally does not identify what properties are necessary to defend against them.</p><p>We address the gap between these approaches by introducing demographic coherence, a condition inspired by privacy attacks that we argue is necessary for data privacy. This condition captures privacy violations arising from inferences about individuals that are incoherent with respect to the demographic patterns in the data. Our framework focuses on confidence rated predictors, which can in turn be distilled from almost any data-informed process. Thus, we capture privacy threats that exist even when no attack is explicitly being carried out. Our framework not only provides a condition with respect to which data release algorithms can be analysed but suggests natural experimental evaluation methodologies that could be used to build practical intuition and make tangible assessment of risks. Finally, we argue that demographic coherence is weaker than differential privacy: we prove that every differentially private data release is also demographically coherent, and that there are demographically coherent algorithms which are not differentially private.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The collection of data and dissemination of aggregated statistics is a key function of government and civil society, driving critical data-driven decision making processes, e.g., democratic apportionment, collective resource allocation, and documenting ongoing social ills. Indeed, data has become an indispensable modern tool for producing knowledge. However, the collection of personal data-particularly mass scale collection-introduces the potential for the inappropriate disclosure of information that individuals might prefer to remain private. Thus, data curators must carefully apply privacy protection mechanisms to their data, ideally without compromising the utility of the eventual data release. The study of privacy preserving data releases started with attacks that compellingly demonstrated that data releases which had not taken steps to ensure privacy could be weaponized for harm, e.g., Latanya Sweeney's infamous re-identification of the Massachusetts Governor Bill Weld's medical records <ref type="bibr" target="#b47">[52]</ref>. Since then, there has been a robust literature describing increasingly sophisticated attacks which continue to motivate efforts towards privacy preserving data release <ref type="bibr" target="#b35">[40,</ref><ref type="bibr">10,</ref><ref type="bibr">9,</ref><ref type="bibr">12]</ref>. While these attacks have proved convincing enough to shift data protection practices in many fields, attack demonstrations do not provide a clear path towards designing data protection mechanisms themselves, even in the form of "prevent all attacks like this one"; the attack demonstrations do not take on the task of distilling a set of agreed upon properties that make the attack convincing. In the aftermath of these attacks, the research community has developed a set of formal approaches that aim to provide robust privacy guarantees. While early attempts, like k-anonymity, proved inadequate, differential privacy <ref type="bibr" target="#b19">[24]</ref> has recently emerged as an accepted standard, seeing deployment in both industry <ref type="bibr" target="#b22">[27,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b16">21,</ref><ref type="bibr" target="#b48">53]</ref> and government <ref type="bibr" target="#b0">[1]</ref>. These formal approaches are often seen as sufficient for ensuring data subject's privacy, in that they are "one-size-fits-all," i.e., data curators can apply best practice protections without needing to consider the intricacies of each deployment.</p><p>In practice, however, there can be significant barriers to applying differential privacy, which stem from the need to strike a delicate balance between the benefits of privacy preservation and its cost to utility <ref type="bibr" target="#b3">[4]</ref>. Moreover, the generality of sufficient conditions means that they naturally lend themselves to being very abstract, which can make it far too easy to lose sight of the concrete privacy harms they are intended to prevent <ref type="bibr" target="#b12">[17]</ref>.</p><p>The deficiencies inherent in each of the existing approaches compels us to explore an intermediary design philosophy: necessary conditions. Within this approach, we can formally define (possibly many) properties that any private data release should guarantee without needing to provide a single, unifying, sufficient condition. These necessary conditions can be seen as giving formal procedures for recognizing when an attacker has inflicted harm. Specifying necessary conditions promises to be an approach that simultaneously embraces the formality of sufficient conditions, while being just as concrete and convincing as attacks. Thinking in terms of necessary conditions has always been implicit in the practice of differential privacy (albeit, usually informally), where selecting the "best" privacy parameters ε, δ, for a deployment requires a trade-off with other metrics, such as accuracy. This makes it necessary to understand how small the parameters must be for the prevention of concrete privacy harms. Therefore, necessary conditions can provide a concrete methodology for justifying parameter choices by identifying parameter regimes that could enable specific harms.</p><p>A new necessary condition: Demographic Coherence. In this work, we design a novel necessary privacy notion rooted in three key insights: <ref type="bibr" target="#b0">(1)</ref> privacy harms are increasingly going to come in the form of inferences at the hands of predictive algorithms. <ref type="foot" target="#foot_0">1</ref> That is, we should be interested in the predictions that these algorithms make about people-and the decisions organizations may make based on these predictions-even when predictive algorithms are not intentionally designed with causing harm in mind; <ref type="bibr" target="#b1">(2)</ref> we should consider the confidence with which an algorithm can make predictions, because simply increasing the confidence that an individual or a group has a certain attribute may be enough to result in harm; and</p><p>(3) The harms associated with breaches of privacy are not experienced uniformly among members of a population. This means that, if not defined carefully, an aggregate measure across an entire population could easily 'hide' effects on vulnerable subgroups by averaging them away.</p><p>Our resulting notion, which we call demographic coherence, is intentionally designed to be ergonomic<ref type="foot" target="#foot_1">foot_1</ref> in many different contexts. For example, we provide sufficient formalism to enable rigorous analysis and provable realization, all while keeping the specific harms against which demographic coherence protects compellingly salient. Additionally, we provide a vision as to how demographic coherence can support the type of intuition building required to set real-world parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Our Contributions</head><p>In this work we make the following contributions:</p><p>-Demographic Coherence. In this work we introduce demographic coherence, an analytical framework for reasoning about the privacy provided by data release algorithms. Demographic coherence has the following qualities:</p><p>-Captures predictive harms. Demographic coherence builds on conceptual tools from generalization <ref type="bibr" target="#b50">[55,</ref><ref type="bibr" target="#b18">23,</ref><ref type="bibr" target="#b13">18,</ref><ref type="bibr" target="#b29">34,</ref><ref type="bibr" target="#b7">8]</ref> and multicalibration <ref type="bibr" target="#b25">[30,</ref><ref type="bibr" target="#b36">41]</ref> to <ref type="bibr" target="#b0">(1)</ref> evaluate the risk of predictive harms distributionally without relying on measuring accuracy with respect to an unknown (and possibly unknowable) ground truth and (2) evaluate the risk of predictive harms local to the different subgroups within a population. Evaluating risks distributionally allows the framework to remain applicable even when ground truth is unavailable, and evaluating risks for different subgroups allows the framework to identify effects specific to vulnerable subgroups.</p><p>-Lends itself to experimental auditing. Demographic coherence has a natural translation to an experimental setup for comparing the effects of various algorithms for privacy preserving data release. In addition, demographic coherence is measured by computing a distance metric over two distributions, which facilitates quantification of the concrete risk. In this work we study an instantiation of demographic coherence measured using Wasserstein distance.</p><p>-Lends itself to analytical arguments. Finally, the formalism we build supports rigorous analytical arguments about algorithms. For example, we show that all algorithms with bounded max information are also coherence enforcing.</p><p>-Demographic coherence enforcement is achievable. We prove that demographic coherence enforcement is achievable, showing parameter conversions under which any pure differentially private (pure-DP) algorithm and any approximate differentially private (approx-DP) algorithm enforce demographic coherence. For an overview of these theorems, see Section 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Related Work</head><p>The study of privacy-preserving data release broadly falls into two categories: demonstration of potential harms via concrete attacks, and the development of formal methodologies that provide robust guarantees. These two approaches provide complementary insights. Formal approaches provide a concrete path to implementing privacy-protections, and the motivation for their use is derived from attacks. In particular differential privacy provably protects against membership inference (e.g., <ref type="bibr" target="#b27">[32,</ref><ref type="bibr" target="#b21">26,</ref><ref type="bibr" target="#b45">50,</ref><ref type="bibr" target="#b51">56]</ref>), reconstruction (e.g., <ref type="bibr" target="#b17">[22,</ref><ref type="bibr" target="#b9">14,</ref><ref type="bibr" target="#b24">29,</ref><ref type="bibr">11]</ref>), and reidentification (e.g., <ref type="bibr" target="#b47">[52,</ref><ref type="bibr" target="#b37">42]</ref>), as shown by Dwork et. al. <ref type="bibr" target="#b20">[25]</ref>. In practice, however, there are fundamental challenges in using attacks to guide the many choices one must make when implementing privacy protections. These challenges arise from ( <ref type="formula" target="#formula_18">1</ref>) identifying successful attacks, (2) identifying realistic attacks, and (3) determining the privacy protections necessary to prevent the attacks being considered.</p><p>Evaluating the success of an attack. In using attacks to motivate formal methodologies, one must start by demonstrating the extent of potential vulnerabilities. For example, membership inference is an attack that relates directly to the definition of differential privacyhowever, the potential to infer membership in a dataset isn't a convincing vulnerability in the case of large data collection efforts like the US decennial Census. Therefore, differential privacy frequently derives its motivations from re-identification and reconstruction attacks. Still, the success of these attacks is difficult to evaluate. <ref type="foot" target="#foot_2">3</ref> In recent work, Dick et al. implemented a reconstruction attack <ref type="bibr" target="#b15">[20]</ref> along with robust evaluations of its success. Their work has since been cited by the US Census Bureau's chief scientist as evidence that "database reconstruction does compromise confidentiality" <ref type="bibr" target="#b34">[39]</ref>. The key insight in their evaluation comparing the results of the reconstruction to a baseline in which reconstruction is conducted with complete access to the distribution underlying the data. While the intuition behind this work-that an attack is much more concerning if it reveals more than what could be learned from a detailed knowledge of the distributional properties-applies to many attack paradigms, the baseline considered in their work is specific to reconstruction attacks. Reconstruction attacks are not always possible to carry out, and, furthermore, conducting a reconstruction attack assumes malicious intent in a way that may or may not be convincing to all stakeholders. We introduce the demographic coherence framework which extends this intuition to the evaluation of a more general class of attacks.</p><p>Another place where the efficacy of specific attacks is measured via comparison to baselines is the literature on auditing differentially private algorithms (e.g., <ref type="bibr" target="#b32">[37,</ref><ref type="bibr" target="#b31">36,</ref><ref type="bibr" target="#b39">44,</ref><ref type="bibr" target="#b46">51]</ref>). Here, attacks are carried out on existing systems, and the efficacy of the attack is used to measure the maximum level of "effective privacy" that the system confers.</p><p>Identifying realistic attacks. Research into conducting privacy attacks makes a variety of assumptions about the setting in which those attacks could be conducted, including the goal of the attacker, the power of the attacker, the type of system attacked, etc. These assumptions can radically change the extent to which an attack should be considered a realistic threat against real-world data releases; attacks that require unrealistic assumptions may not be concrete threats. The works of Rigaki &amp; Garcia <ref type="bibr" target="#b41">[46]</ref>, Salem et al. <ref type="bibr" target="#b44">[49]</ref>, and Cummings et al. <ref type="bibr" target="#b12">[17]</ref> classify existing attack strategies by adversarial resources and goals in order to provide a structure for evaluating privacy risks. In addition to this, Cohen <ref type="bibr" target="#b8">[13]</ref> and Giomi et al. <ref type="bibr" target="#b23">[28]</ref> take a different approach, appealing to the law to determine the goals of a realistic attacker. Specifically, they contextualize the attacks they consider by tying them to existing privacy law. Still, individual attacks, even if successful and realistic, don't provide a clear path forward in terms of designing protections.</p><p>Identifying necessary conditions. Some prior work has started to identify necessary conditions for achieving privacy. Cohen &amp; Nissim <ref type="bibr" target="#b10">[15]</ref> introduce a necessary condition, called "predicate singling out," inspired by the GDPR notion of singling out. Balle et. al. <ref type="bibr" target="#b5">[6]</ref> introduce an alternative necessary condition called "reconstruction robustness," which is closely related to reconstruction attacks. Cummings et. al. <ref type="bibr" target="#b12">[17]</ref> build on the notion of reconstruction robustness, extending it to a weaker adversarial setting. Our framework extends this general approach but applies to a much broader class of attacks-namely, any attacks from which a confidence rated predictor could be distilled.</p><p>Recent work by Cohen et al. <ref type="bibr" target="#b11">[16]</ref> also recognizes the need to bridge the gap between formal privacy guarantees and practical attacks. Building on definitions in prior work <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">15,</ref><ref type="bibr" target="#b12">17]</ref> they introduce "narcissus resiliency," a framework for establishing precise conditions under which an algorithm prevents various classes of existing attacks, including reconstruction attacks, singling out attacks, and membership inference attacks. Our definition defines invulnerability against a different type of privacy loss, providing complementary insights in the form of necessary conditions that can be considered alongside their definitions. Specifically, we believe that it is important to consider demographic coherence alongside their notion of narcissus singling out; the latter captures an important property that the former does not. (An algorithm that chooses a small subset of the data to publish in the clear does not meet the definition of singling out security even though it may be demographic coherence enforcing if the subset is small enough.) Another key difference between our works is that the narcissus framework does not naturally lend itself to concrete experimental evaluation, whereas demographic coherence is intentionally designed with this use case in mind.</p><p>Finally, most of the works discussed above measure the success of an attack via its accuracy (i.e., is the information extracted about the data subject true?). We observe that harm is not necessarily predicated on accuracy, and we design demographic coherence to be intentionally independent of accuracy. One impact of this choice is that demographic coherence is a more natural fit for settings in which ground truth is difficult or impossible to measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Overview of Technical Results</head><p>In Section 5, we show parameter conversions under which any pure-DP algorithm, and any approx-DP algorithm enforces demographic coherence. Here, we present informal statements of these technical results.</p><p>We start by presenting a simplified definition of coherence enforcement (Definition 3, 4). (This presentation is meant to allow the informal statements of our technical results. For a formal presentation, see Section 4. Additionally, the concept of enforcing demographic coherence emerges from careful consideration of several key principles, which are discussed in detail in Section 3.) Informally, a coherence-enforcing A guarantees that predictors trained using its private reports will be demographically coherent. <ref type="foot" target="#foot_3">4</ref>Definition 1 (Informal Version of Definitions 3 and 4 ). Consider a data universe X , and a data-curation algorithm A : X * → Y. We say that A enforces (α, β)-demographic coherence, if for all algorithms L : Y → (X → [-1, 1]) that use the report produced by the curator to create a confidence-rated predictor h : X → [-1, 1], the following condition is satisfied. For all datasets X,</p><formula xml:id="formula_0">Pr Xa,X b $ ← -X,Ra←A(Xa),h←L(Ra) [dist(h(X a ), h(X b )) ≥ α] ≤ β,</formula><p>where X a , X b represent a random split of the dataset X into halves, report R a is produced by the data-curator using only X a , and h is created by running algorithm L on the report, h(X a ) represents the empirical distribution of predictions made on X a , and dist(•, •) represents a metric distance between distributions. Here, β is the probability that h is not demographically coherent, and α represents how close the distributions of h(X a ) and h(X b ) are required to be.</p><p>The formal definition of coherence enforcement is more intricate than the one above. One key technical distinction is that the restriction on predictor h applies not only to the full sets X a and X b , but also across different subpopulations in those sets. For the remainder of this section, we specify the distance metric dist(•, •) as Wasserstein-1 distance between distributions. In this context, we say that and algorithm A enforces Wasserstein-coherence.</p><p>The following theorem is an informal statement of Theorem 6, which argues that any data-curation algorithm with bounded max-information <ref type="bibr" target="#b18">[23]</ref> (a notion that mathematically captures the dependence of algorithms' outputs to their inputs) also enforces Wasserstein coherence.</p><p>Theorem 1 (Informal Version of Theorem 6).</p><formula xml:id="formula_1">Let n ∈ N,ζ &gt; 0, β ∈ (0, 1), α ∈ (0, 2].</formula><p>Consider a data curation algorithm A : X n/2 → Y with bounded max-information</p><formula xml:id="formula_2">I β/2 ∞ (A, n/2) &lt; ζ. Then, A enforces (α, β)-demographic coherence provided that n ≥ k • ζ+ln(1/β) α 2</formula><p>for some constant k.</p><p>We leverage the connection between differential privacy and max-information to show the exact parameter conversion under which differentially private algorithms enforce demographic coherence. Theorem 2 is an informal statement of Theorem 7, the result for pure-DP. For the approximate-DP result, we point the reader to Theorem 8 in Section 5.</p><p>Theorem 2 (Informal Version of <ref type="bibr">Theorem 7)</ref>. Let n ∈ N, β, ε ∈ (0, 1), α ∈ (0, 2].</p><p>Consider an ε-DP data curation algorithm A : X n/2 → Y. Then, A enforces (α, β)demographic coherence provided that ε ≤ k • α ln(1/β) for some constant k. This theorem should be understood as follows: a data curator identifies (possibly experimentally) regimes for α and β that they find to be "too risky" for a data release (with respect to demographic coherence). That curator can then use this theorem to suggest a value of ε such that, if they were to use differential privacy as their privatization mechanism, the resulting data release would achieve their desired constraints. While the parameter conversion in Theorem 7 would likely result in a value of ε that is too small for most use cases, we expect this to be inherent to a black-box conversion of differential privacy to the enforcement of demographic coherence. We leave it as an important open question to identify other ways of achieving our definition, including non-black-box uses of DP-algorithms for obtaining better coherence enforcement guarantees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A Walk Through Our Definition</head><p>In order to clearly motivate and explain the choices embedded within our definition, we incrementally build up our approach in this section; for the formal definition see Section 4.</p><p>Notation and Conventions. Assume that the data curator has collected a sample X from the overall population of interest. We make no requirements on the relative sizes of X and the population such that our framework can be used broadly-even in Census-like circumstances, in which the goal is to sample the entire population. Our ultimate goal is to reason about a data curator A who uses X to generate a privacy-preserving release R. <ref type="foot" target="#foot_4">5</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Predictive Harms</head><p>Within our framework, we characterize the adversary as a party interested in making predictions about individuals, e.g., if people have some particular stigmatized feature or are going to buy a product if they are targeted with an advertisement. We formalize this conceptual approach by considering an arbitrary algorithm L used by the adversary to design the predictor h. <ref type="foot" target="#foot_5">6</ref> We choose to measure privacy risk in terms of predictive harms for the following reasons:</p><p>-Predictors are commonplace: The predictions made by machine learning models increasingly have direct impacts on people's daily lives. Diagnostic models are being tested as potential aides for medical experts <ref type="bibr" target="#b2">[3]</ref>, and increasingly complex and opaque models are used to "match" job candidates with prospective employees <ref type="bibr" target="#b30">[35,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b40">45]</ref> in order to increase the odds that an individual ends up with a lucrative job. Even complex infrastructures, like those used in digital advertising, can be seen as predictive models that are attempting to classify individuals into target audiences. The fact that predictive algorithms are increasingly commonplace-and the decisions they make concretely impact our daily lives-makes them a very believable source of harm.</p><p>-Harmful predictors need not be maliciously produced: By considering the impact of predictors, we free ourselves from needing to see the adversary as intentionally trying to cause harm and instead can refocus on the (perhaps accidental) harms that a data release has the potential to cause.</p><p>A conceptual concern about considering predictive harms is whether we are explicitly ruling out particular, important types of adversarial behavior that are attempting to extract information (e.g., reconstruction attacks). However, we note that by discussing predictors we are only limiting the input/output behavior of the adversary's product, and not how the predictor is produced. For example, our framework could capture an adversary that runs a known reconstruction algorithm (e.g., <ref type="bibr" target="#b14">[19]</ref>) and then makes predictions about individuals based on the produced table. In this way, our approach highlights the ways in which existing approaches could be used when applied in decision-making contexts. Looking ahead, in theoretically analyzing our framework we will universally quantify over algorithms to preserve generality, which means that reconstruction-based approaches-or other known malicious approaches to data extraction-are naturally captured.</p><p>Still, in choosing to concretize the type of our adversary, we do risk failing to consider a different type of attacker with inconsistent goals. Definitions created with this philosophy can be extremely helpful at establishing necessary conditions for ensuring privacy, but do not claim to be sufficient. On the other hand, our approach helps highlight a specific way in which data is likely to be weaponized in the real world.</p><p>Incoherent predictions. Within this work we focus on capturing predictive harms that occur specifically by virtue of a data subject appearing in a dataset. To give a concrete example, consider the now classic case of Narayanan and Shmatikov's re-identification of Netflix users within an anonymized data release using public IMDB data <ref type="bibr" target="#b38">[43]</ref>. In this setting, we might consider an adversary interested in learning a predictor which predicts queerness (e.g., imagine the adversary is operating in a regime in which queerness is criminalized or highly stigmatized). Now, imagine two similar 7 individuals Asahi and Blair; each intentionally avoids being perceived as queer, and in particular does not provide ratings on movies with queer themes on their public IMDB profiles. Assume that based on a random sample, one of them (e.g., Asahi) has their movie ratings released by Netflix and the other (e.g., Blair) does not. A predictor that is likely to guess that Asahi is queer when they are present in the dataset but would not have guessed they are queer otherwise indicates that the predictor was able to extract some information about Asahi from the data release. Given the assumption that we claim that Asahi and Blair are similar, this would also be true of a predictor that guessed that Asahi is queer while Blair is not.</p><p>Importantly, this is true even if it's not clear exactly what form leakage takes or if the prediction as to their queerness is inaccurate. We call predictors that act in this way "demographically incoherent." There are two important (if unintuitive) subtleties that immediately emerge from this description of incoherent predictions:</p><p>(1) Harmful predictors need not be accurate: Incoherent predictions focus on the behavior of the predictor independent of accuracy. Within the example above, it is not important if Asahi is actually queer, it is enough that the predictor guesses that Asahi is queer because of their presence in the data. This is because we envision our predictor being used to make some real-world decision, e.g., limiting the opportunities available to Asahi due to their perceived queerness. As such, the prediction's accuracy is a secondary concern.</p><p>(2) Measuring confidence is critical: When considering the ways in which data releases can be translated into real-world harms, it's important to recognize that enabling an adversary to make high confidence predictions about private attributes is a problem. Importantly, this means that we should not require that the adversary can predict private attributes with 100% certainty in order for it to be considered harmful. Indeed, there is no particular cut-off threshold for certainty at which point it is natural to consider a harm occurring for all contexts. In turning to predictors as our adversarial strategy, we naturally arrive in a context within which notions of confidence have been extensively explored. Specifically, our approach considers confidence-rated predictors h, allowing us to directly reckon with predictive uncertainty.</p><p>We note that there are other pathological predictors which do not indicate privacy-loss, and are therefore not considered demographically incoherent. For example, a predictor may make guesses that are entirely random or guess that everyone in the population has some feature, i.e., make predictions that do not depend on the characteristics of individuals. The challenge then is to detect demographically incoherent predictors, whose behavior indicates privacy leakage, without depending on accuracy and without accidentally measuring variance in behavior that is not dataset dependent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">An Experiment to Detect Demographically Incoherent Predictors</head><p>With intuition about our class of "bad" predictors in hand, we now turn our attention to designing an experiment for detecting algorithms that produce them. In this discussion, we will defer to the concrete ways in which we will measure the demographic incoherence, and first focus on the experiment itself-that is, first we will decide what values we should measure, and then proceed to deciding how to do that measurement.</p><p>Because a symptom of incoherent predictors is differing performance on in-sample and out-of-sample individuals, it is clear that a comparison is required. However, it is not immediately obvious what the "right" comparison should actually be. In fact, some natural approaches fall short of our goals. As such, we walk through two seemingly natural, but flawed, experiments before discussing our final choice. Recall that the data curator has a dataset X and will be releasing a privacy-preserving report R.</p><p>(1) Comparing before and after a data release: A very natural approach would be to compare the performance of a predictor created before a data release with one created after. i.e., comparing the performance (on individuals in X) of h 0 produced by an algorithm L with access to the adversary's pre-existing, auxiliary information Aux to a predictor h 1 produced by L with access to both Aux and the report R. <ref type="foot" target="#foot_7">8</ref> Such a comparison, intuitively, should isolate exactly the predictive changes associated with releasing R.</p><p>Where this approach fails is that it does not recognize that there should be a difference between the predictors h 0 and h 1 over the inputs in X. After all, if there was no difference between h 0 and h 1 , there would be no value whatsoever in releasing R! As such, this comparison is necessarily conflating potential "bad" types of predictions that releasing R enables with the "good" types of predictions that motivated the release of R in the first place.</p><p>(2) Comparing to the base population: The next most natural approach would be to compare the behavior of a single predictor h on individuals in X with that on individuals in the rest of the population. For example, by comparing its behavior on another similarly sampled dataset Y . This improves on our previous approach because we might expect that a "good" learning algorithm uses the dataset to learn about the population at large instead of revealing specifics about individuals.</p><p>While this approach gets to the core of our interests, it has an important flaw. Technically speaking, we can not assert that a real world sampling procedure has access to the base population distribution. i.e., one cannot assume that two real world datasets are i.i.d samples from the same distribution. Also, we could conceivably be in a situation where the entire population of interest is contained in X, leaving no one in X against which we could compare. Therefore, keeping in mind the ergonomics of our definition in a concrete deployment scenario, this approach also falls short of our goals.</p><p>Our approach. We build off the second approach above by taking control of the randomness used to separate the two comparison populations. Specifically, we split the dataset X into two uniformly selected halves, X a and X b . We then use the data curation algorithm to generate the report R using only X a , holding X b in reserve as our "test" data set. We then test the behavior of a predictor h, designed based on the report R. Specifically, we compare the predictions of h on individuals in X a and X b . This approach "fixes" our second failed attempt by moving the assumptions about the randomness used in sampling X-something over which we have no control-into the randomness we use to split X into X a and X bsomething over which we do have control. We say that a data release is demographically incoherent with respect to X if its predictions on members of X a are noticeably different than the predictions it makes on members of X b (who necessarily have similar demographic distributions, given the uniform split.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Measuring the Incoherence of a Predictor</head><p>Finally, we discuss how to compare the behavior of h on X a and X b without relying on accuracy. Formally, we consider real-valued confidence-rated predictors h :</p><formula xml:id="formula_3">X → [-1, 1]</formula><p>which predict something about individuals. To capture the fact that these predictions are confidence rated, h outputs values in [0, 1] when it predicts the attribute is likely to be true, and values in [-1, 0] when it predicts the attribute likely to be false, with a higher absolute value representing higher confidence.</p><p>For any such predictor, we will consider h(X a ) and h(X b ) as representing the uniform distribution over the predictions of h on X a and X b respectively. Comparing these distributions allows us to reason about the general behavior of the predictor h on X a and X b without considering accuracy of predictions on individuals. In order to get a more granular understanding of the behavior of h we further formalize the intuition of making comparisons over "similar" individuals in X a and X b as explained below.</p><p>Measuring a difference with respect to "similar" individuals. In our motivating discussion of incoherent predictions, our representative individuals Asahi and Blair were assumed to be "similar" to one another. To formalise this intuition, we ask that a predictor is demographically coherent not only on the population as a whole, but also on recognizable subgroups from the population, e.g., men, women, college freshmen, middle-school teachers etc. . .<ref type="foot" target="#foot_8">foot_8</ref> For each of these subgroups of the population, the things that bind them together make them similar, in some particular sense. By operationalizing our earlier intuition in this way, we ask that the demographic coherence property holds not only over some particular notion of similarity, but rather over many notions of similarity at the same time. It also has the following technical and social benefits: (1) From a technical perspective, considering only the full population might hide incoherent decisions within sub-populations that effectively "cancel out." That is, there might be a right-shift in one group that masks a left-shift in a different group, each shift effectively "disappearing" in the collective distribution over all individuals. (2) From a social perspective, there may be particularly important groups within the population for whom we want to ensure coherent predictors for normative reasons. For example, if X is a Census-like dataset, we may want to ensure that there are not subgeographies on which incoherent predictors are allowed. Similarly, we may want to ensure that there aren't legally protected categories (e.g., race, sex, religion, etc. . . ) on whom incoherent predictions are allowed.</p><p>The lens of a predictor. Consider the adversary using the Netflix dataset to learn a predictor for queerness. We assume that at the time of making predictions, the adversary sees a public user profile (their IMDB ratings) which contains only some of the user information that was contained in the dataset. To formalize this intuition we introduce the lens ρ of the predictor, which indicates the attributes contained in the dataset which the predictor can "see." We then compare the behavior of h on members of X a and X b as seen through the lens ρ.<ref type="foot" target="#foot_9">foot_9</ref> Choosing a metric. In this work, we recommend instantiating the demographic coherence experiment with the distance metric of Wasserstein distance (Definition 5), also known as earth-mover's distance, when measuring demographic coherence (or lack thereof). Intuitively, this metric measures the minimum amount of work that it requires to deform one probability distribution into another. For example, if one visualizes a probability distribution as a mount of dirt, Wasserstein distance measures the effort required to move enough dirt to make one mount look like the other (thus, earth-mover's distance). Unlike Total Variation distance, which only measures the distance between the distribution curves, Wasserstein distance is greater with a higher shift in confidence; the importance of measuring confidence is one of the insights we highlight in Section 3.1. Another advantage of the Wasserstein distance is that it has been widely studied and used in theoretical and empirical statistics, and so there is a rich mathematical toolkit that one can borrow from when reasoning about it.</p><p>We recognize that there may be other measurement metrics that could be applied to the demographic coherence experiment that might highlight risk in different ways, and encourage this as important follow-up work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Formal Definition</head><p>This section presents the formal definitions corresponding to our framework. For a discussion about the various choices made here, see Section 3.</p><p>Section 4.1 contains a glossary of the notation we use, Section 4.2 formally defines the notion of incoherence that we measure, Section 4.3 defines what it means for an algorithm to be demographically incoherent, and Section 4.4 defines what it means for a data curation algorithm to be coherence enforcing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Notation</head><p>• We define a data universe X = ({0, 1} * ∪ ⊥) * , where each feature of the data can also take the value ⊥ (a.k.a. null).</p><p>• We denote a dataset consisting of n records from X by dataset X ∈ X n .<ref type="foot" target="#foot_10">foot_10</ref> </p><p>• We consider a collection C of sub-populations C ⊆ X .</p><p>• For any dataset X and sub-population C ∈ C, we define X| C def = X ∩ C to be the restriction of X to the sub-population C, i.e., the members of the dataset X that belong to sub-population C.</p><p>• We define a lens ρ as a set of features from X .</p><p>• For a lens ρ, we define π ρ (X) to be the data in X restricted to the features in the lens. That is, for every feature represented by ρ, the sets π ρ (X) and X are exactly the same, and for features not represented by ρ, the entries in π ρ (X) always have the value ⊥.</p><p>• For any fixed predictor h : X → [-1, 1], define the distribution h(X) as the uniform distribution over the predictions of h on X. That is, the distribution h(X) has cumulative distribution function</p><formula xml:id="formula_4">cdf h(X) (p) = Pr x $ ← -X [h(x) ≤ p].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Measuring Demographic Incoherence of Predictors</head><p>In this section we define the notion of incoherence that we measure over predictors. This measurement is used informally in the demographic coherence experiment DemCoh (Figure <ref type="figure">1</ref>).</p><p>Definition 2 (Demographically Incoherent Predictor). Let X be a data universe, let X a , X b ∈ X n/2 be datasets consisting of n/2 data entries each, let C be a collection of sub-populations C ⊆ X , let the lens ρ be a set of features from X , and let h : X → [-1, 1] be a confidence rated predictor. Let d(•, •) represent some metric distance between probability distributions.</p><p>We say that the h has α-incoherent predictions with respect to X a , X b , ρ, and C, if for some</p><formula xml:id="formula_5">C ∈ C, d h(πρ (X a | C )), h(πρ (X b | C )) &gt; α</formula><p>(In contrast, h has α-coherent predictions with respect to X a , X b , ρ, and</p><formula xml:id="formula_6">C, if for all C ∈ C, it satisfies d h(πρ (X a | C )), h(πρ (X b | C )) ≤ α)</formula><p>In other words, the predictions of h are incoherent if there is some sub-population C that witnesses a big distance between its predictions on two sets X a , and X b . Note that this definition distills a notion of 'incoherence' only once we fix some assumptions on X a , X b , perhaps that they are drawn from similar distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Demographic Coherence</head><p>Consider a data universe X , an algorithm L :</p><formula xml:id="formula_7">X n/2 → (X → [-1, 1]</formula><p>) which uses a dataset of size n/2 to produce a confidence-rated predictor h : X → [-1, 1]. Let C be a collection of sub-populations C ⊆ X , let the lens ρ be a set of features from X , Let dist(•, •) represent some metric distance between probability distributions.</p><p>In this section, we formally define when the algorithm L is demographically coherent. We start by defining the demographic coherence experiment DemCoh which checks the demographic coherence of L with respect to on a specific dataset X, a collection C, a lens ρ, and a distance metric dist(•, •). This experiment works similarly to the description under our approach in Section 3.2, expect we are testing the coherence of an algorithm that gets the dataset in the clear. In Section 4.4, we will use the notion of demographic coherence to define when a data curator is coherence enforcing.</p><p>In the DemCoh experiment, the input dataset X is split into sets X a , X b where X b is held in reserve as a "test" set. Then the algorithm L, with input X a , is used to produce a predictor h. Finally, the predictor h is checked for demographic incoherence (Def 2) with respect to X a , X b , ρ, and C.</p><formula xml:id="formula_8">DemCoh L,X,C,ρ,dist(•,•) (α)</formula><p>Input:</p><p>An algorithm L : Split Data:</p><formula xml:id="formula_9">X n/2 → (X → [-1, 1]), A dataset X ∈ X n , a collection C of subgroups C ⊆ X ,</formula><formula xml:id="formula_10">I $ ← -{S ⊆ [n] : |S| = n/2} X a = (x i ) i∈I X b = (x i ) i∈[n]\I Compute Predictor(X a ): h ← L(X a )</formula><p>Incoherence Condition:</p><formula xml:id="formula_11">Set b = 0 if there exists C ∈ C such that: dist h(πρ (X a | C )), h(πρ (X b | C )) &gt; α</formula><p>Else set b = 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1: Demographic Coherence Experiment</head><p>A natural formalization of the intuition we developed in Section 3 would say that an algorithm L produces (α, β)-demographically coherent predictions with respect to collections C and lens ρ if the following holds for all datasets X:</p><formula xml:id="formula_12">Pr[DemCoh L,X,C,ρ (α) = 0] ≤ β.</formula><p>However, this definition cannot be realized with respect to arbitrary categories C and all datasets, because the sampling experiment in and of itself introduces some incoherence. For example, consider a category C that is men over 60, and a dataset that contains only two people in this category. There exists a predictor that predicts -1 on one of them and 1 on the other. With probability 1/2, these two men end up in X a and X b respectively, and the Wasserstein distance between the predictors' distributions on X a | C and X b | C is 2. Hence, for our definition to be meaningful and achievable, we need that the datasets considered are sufficiently representative that the incoherence due to sampling does not dominate. Thus, in order to make the definition achievable, we define demographic coherence with respect to a size constraint.</p><p>To remove the need for the parameter γ, one could redefine X a | C by "zeroing out" members of X a that are not in C instead of taking the intersection X a ∩ C. This would effectively result in asking the predictor h in Definition 2, and Figure <ref type="figure">1</ref> to make "dummy" predictions, with no information, for every member of X a and X b not belonging to C. While such a definition may be more mathematically elegant, we believe that the explicit failure point represented by γ in our defintion is important for interpretability and ease of use-especially by non-experts. For the same reason, it is important to have an explicit collection C, and lens ρ, even though the eventual theorems one can prove may only hold for large choices of γ, C, and ρ. Definition 3. (Demographic Coherence). Consider a data universe X , and an algorithm L : X n/2 → (X → [-1, 1]) which uses a dataset of size n/2 to produce a fixed confidence-rated predictor h : X → [-1, 1]. Let C be a collection of sub-populations C ⊆ X , let the lens ρ be a set of features from X , Let dist(•, •) represent some metric distance between probability distributions. We say that L produces (α, β)-demographically coherent predictions with respect to collection C, size-constraint γ, and lens ρ if the following holds:</p><formula xml:id="formula_13">For all X ∈ X n , C * = {C ∈ C | |C ∩ X| ≥ γ} Pr[DemCoh L,X,C * ,ρ (α) = 0] ≤ β.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Coherence Enforcing Algorithms</head><p>We finally define coherence-enforcing algorithms by reference to the definition of demographic coherence (Definition 3). Specifically, a data curator A is coherence enforcing if, any algorithm L can be rendered demographically coherent simply by filtering its inputs through the data curator A, without many any changes to the algorithm itself. Definition 4. (Coherence Enforcing Algorithms). Consider data universes X , Y, a collection C of sub-populations C ⊆ X , a lens ρ, and an algorithm A : X n/2 → Y. Let dist(•, •) represent some metric distance between probability distributions. We say that A enforces (α, β)-demographic coherence with respect to collection C, sizeconstraint γ, and lens ρ if:</p><formula xml:id="formula_14">For any algorithm L : Y → (X → [-1, 1]), the combined algorithm L • A : X n/2 → (X → [-1, 1]</formula><p>) satisfies (α, β)-demographic coherence with respect to the collection C, size-constraint γ, and lens ρ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Instantiating Demographic Coherence with a Metric</head><p>In the definitions above, we do not use a specific metric. The metric we will use to instantiate these definitions in the following sections is the Wasserstein-1 metric defined below. Definition 5. Let P, Q represent distributions over a discrete subset S ⊆ R. Then, the 1-Wasserstein distance between P, Q is defined as</p><formula xml:id="formula_15">dist W 1 (P, Q) = inf π i∈S j∈S x i -x j 1 π(x i , x j ),</formula><p>where the infimum is over all joint distributions π on the product space S × S with marginals P and Q respectively.</p><p>If an algorithm is (α, β)-demographically-coherent as per Definition 3 with this Wasserstein metric instantiation, we say that it is (α, β)-Wasserstein-demographically-coherent (or (α, β)-Wasserstein-coherent for short.) Similarly, if an algorithm enforces (α, β)-demographiccoherence as per Definition 4 with this Wasserstein metric, then we say it enforces (α, β)-Wasserstein-demographic-coherence (or it enforces (α, β)-Wasserstein-coherence for short.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Algorithms that Enforce Wasserstein Demographic Coherence</head><p>Now, we show that Wasserstein coherence enforcement is instantiable. Firstly, in Section 5.1, we go over some technical preliminaries necessary to state and prove our results, Then, in Section 5.2, we prove Theorem 6 showing that algorithms with bounded max-information are coherence enforcing. Building on this, in Section 5.3, we leverage the connection between differential privacy and max-information to prove Theorem 7 which says that pure-DP algorithms enforce demographic coherence, and Theorem 8 which is says that approx-DP algorithms enforce demographic coherence as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Technical Preliminaries</head><p>For a recap of the notation used, see Section 4.1.</p><p>Definition 6 (Differential Privacy <ref type="bibr" target="#b19">[24]</ref>). Let n ∈ N. A randomized algorithm A : X n → Y is (ǫ, δ)-differentially private if for all subsets Y ⊆ Y of the output space, and for all neighboring datasets X, X ′ ∈ X n (i.e. X -X ′ 0 ≤ 1), we have that</p><formula xml:id="formula_16">Pr[A(X) ∈ Y ] ≤ e ε Pr[A(X ′ ) ∈ Y ] + δ</formula><p>If δ = 0, we refer to the algorithm as satisfying pure differential privacy (pure-DP), whereas δ &gt; 0 corresponds to approximate differential privacy (approx-DP).</p><p>Definition 7 (Max-information of random variables <ref type="bibr" target="#b18">[23]</ref>). Let X and Y be jointly distributed random variables over the domain (X , Y). The β-approximate max-information between X and Y , denoted by</p><formula xml:id="formula_17">I β ∞ (X; Y ) is I β ∞ (X; Y ) = ln    sup T ⊆(X ×Y) Pr[(X,Y )∈T ]&gt;β Pr[(X, Y ) ∈ T ] -β Pr[X ⊗ Y ∈ T ]   </formula><p>Definition 8 (Max-information of algorithms <ref type="bibr" target="#b18">[23]</ref>).<ref type="foot" target="#foot_11">foot_11</ref> Fix n ∈ N, β &gt; 0. Let X be a finite data universe of size m. Let S be a sample of size n chosen without replacement from X . Let A : X n → Y be an algorithm.</p><p>Then we define the max-information of the algorithm as follows:</p><formula xml:id="formula_18">I β ∞ (A, n) = I β ∞ (S, A(S)))<label>(1)</label></formula><p>The following definition of order-invariant algorithms appears as a technical assumption in some of our theorem statements. <ref type="foot" target="#foot_12">13</ref> This is a minimal assumption because any non-orderinvariant algorithm can be made order-invariant by simply pre-processing the dataset with a sorting or shuffling operation.</p><p>Definition 9 (Order-invariant algorithm). An algorithm A : X m → Y, is order invariant if for all X ∈ X m , the distribution of the random variable A(X) does not depend on the order of the elements of X.</p><p>The proofs of the following theorems connecting differential privacy and max-information can be found in Appendix B.</p><p>Theorem 3. (Pure-DP =⇒ Bounded Max-Information) Fix n ∈ N, ε &gt; 0 and let X be a data universe of size at least n. Let A : X n/2 → Y be an order-invariant ε-DP algorithm. Then for any γ &gt; 0,</p><formula xml:id="formula_19">I γ ∞ (A, n/2) ≤ ε 2 n/4 + ε n ln(2/γ)/4.</formula><p>The following theorem is a generalized version of that in <ref type="bibr" target="#b42">[47]</ref>. The proof follows theirs, with the following key distinctions: (1) it applies to sampling without replacement (2) It carefully tracks constants and (3) It maintains flexibility in setting parameters. We anticipate that this version of the result might be independently useful.</p><p>Theorem 4. (Approx-DP =⇒ Bounded Max-Information, Generalised) Let A : X n → Y be an (order-invariant) (ε, δ)-differentially private algorithm for ε ∈ (0, 1/2], δ ∈ (0, ε). For δ ∈ (0, ε/15], t &gt; 0, and β(t, δ) = e -t 2 /2 + n 2δ δ + 2 δ+2δ</p><p>1-e -3ε</p><p>we have</p><formula xml:id="formula_20">I β ∞ (A, n) ≤ n   347 δ + 75 δ ε 2 + 24 δ2 ε + 240ε 2   + 6tε √ n. Corollary 1. (Approx-DP =⇒ Bounded Max-Information, Specific) Fix n ∈ N, for ε ∈ (0, 1/2], γ ∈ (0, 1], δ ∈ (0, ε 2 γ 2 (120n) 2 ]</formula><p>and let X be a data universe of size at least n. Let A : X n → Y be an (order-invariant) (ε, δ)-differentially private algorithm. We have that</p><formula xml:id="formula_21">I γ ∞ (A, n) ≤ 265ε 2 n + 12ε n ln(2/γ).</formula><p>Definition 10 (Hypergeometric distribution). Fix 0 &lt; a, s ≤ b. Consider a population of b items of which a items have a special property P . Consider s items sampled without replacement from b. The distribution of the number of items in s with property P is called the hypergeometric distribution parameterized by b, a, s (denoted by H(b, a, s)).</p><p>Theorem 5 <ref type="bibr">([33]</ref>). Let K have a hypergeometric distribution H(b, a, s). Then for every γ ≥ 2,</p><formula xml:id="formula_22">Pr[K &gt; s a b + γ] &lt; e -2c(γ 2 -1) Pr[K &lt; s a b -γ] &lt; e -2c(γ 2 -1)</formula><p>where</p><formula xml:id="formula_23">c = max 1 s + 1 + 1 b -s + 1 , 1 a + 1 + 1 b -a + 1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Bounded Max-Information Algorithms are Coherence Enforcing</head><formula xml:id="formula_24">Theorem 6. Let n ∈ N,ζ &gt; 0, β ∈ (0, 1), α ∈ (0, 1]. Let dist W 1 (•, •) represent the Wasserstein-1 distance metric.</formula><p>Consider a collection C of sub-populations C ⊆ X , a lens ρ, and an algorithm A : X n/2 → Y with bounded max-information</p><formula xml:id="formula_25">I β/2|C| ∞ (A, n/2) &lt; ζ.</formula><p>Then A enforces (α, β)-Wasserstein-coherence with respect to collection C, lens ρ, and size constraint γ, where γ = max 8.</p><p>3 • (ζ + ln(16|C|/β)) α 2 , 36 ln(3/α) α 2 , 16.6 • (ζ + ln(16|C|/β)), 5.3 α , 80 .</p><p>The intuition behind the result in Theorem 6 is that the output of an algorithm with bounded max-information does not contain too much specific information about the input dataset. This intuition is leveraged to prove Lemma 1, which is the main lemma underlying this result. Theorem 6 follows from this lemma by an appropriate setting of parameters.</p><formula xml:id="formula_27">Lemma 1. Let η, ζ &gt; 0, α ∈ (0, 1). Let dist W 1 (•,</formula><p>•) represent the Wasserstein-1 distance metric. Consider a sub-population C ⊆ X , a lens ρ, and an algorithm A : X n/2 → Y with bounded max-information</p><formula xml:id="formula_28">I η ∞ (A, n/2) &lt; ζ. For all algorithms L : Y → {X → [-1, 1]}, datasets X ∈ X n , µ &gt; 0,</formula><p>as long as |X ∩ C| ≥ max 4.15 • ln(4/µ) α 2 , 16/3 α , 8.3 • ln(4/µ), 40 , we have Pr Xa←X,h←L•A(Xa)</p><formula xml:id="formula_29">[dist W 1 (h(πρ (X a | C )), h(πρ (X b | C ))) &gt; α] ≤ 2µ(|X ∩ C| + 1) • e ζ + η.</formula><p>Here X a and X b denote a random split of the dataset X as in the DemCoh L•A,X,C * ,ρ (α) experiment in Figure <ref type="figure">1</ref>.</p><p>Proof sketch of Lemma 1: Considering a particular subpopulation C ⊆ X , We need to show for all algorithms L : Y → {X → [-1, 1]}, all datasets X ∈ X n , µ &gt; 0 that with high probability over a choice of split X a , X b $ ← -X, and predictor h ← L • A(X a ) as in the DemCoh L•A,X,C * ,ρ (α) experiment in Figure <ref type="figure">1</ref>, the following holds:</p><formula xml:id="formula_30">dist W 1 (h(πρ (X a | C )), h(πρ (X b | C ))) &lt; α.</formula><p>Note that instead of the split X a , X b which predictor h depends on, if we consider an independent split S, S $ ← -X, then we could hope to use a concentration inequality to get the bound we desire. (The proof of Claim 2 below redefines the sampling process in a way that allows us to use a concentration bound of Hush and Scovel (Theorem 5) for the hypergeometric distribution to prove such a result.)</p><p>With the goal of analysing an independent split, we combine the fact that bounded max-information is preserved under post-processing with the intuition that the output of an algorithm with bounded max-information does not contain too much specific information about the input dataset to decouple the predictive hypothesis h from the dataset X a in the following way (in Claim 1):</p><formula xml:id="formula_31">dist W 1 (h(πρ (X a | C )), h(πρ (X b | C ))) ≈ dist W 1 (h(πρ (S| C )), g(πρ S| C ))</formula><p>Proof of Lemma 1. Fix any arbitrary lens ρ. The proof proceeds in two claims. First, in Claim 1, we use the definition of max-information to replace X a , X b with an independently chosen half-sample S and its complement S = X \ S.</p><p>Claim 1. Consider η, α, ζ &gt; 0 and a fixed dataset X ∈ X n . Consider a data-curation algorithm A : X n/2 → Y with bounded max-information, I η ∞,P (A, n/2) ≤ ζ, and an algorithm L : Y → {X → [-1, 1]} that uses the data report to create a predictor. Independently choose two random half samples X a , S ← X, and let sets X b = X \ X a , S = X \ S. Finally let h ← L(X a ). Then, we have that</p><formula xml:id="formula_32">Pr Xa,h [dist W 1 (h(πρ (X a | C )), h(πρ (X b | C ))) &gt; α] ≤ Pr S,Xa,h [dist W 1 (h(πρ (S| C )), h(πρ S| C )) &gt; α] • e ζ + η Then, in Claim 2, we bound Pr[dist W 1 g(πρ (S| C )), h π ρ S| C</formula><p>&gt; α] for any confidence rated predictor g : X → [-1, 1] that is produced independently of S.</p><p>Claim 2. Let α ∈ (0, 1), let S be a sample of size n/2 drawn uniformly without replacement from X, let S = X \ S, and let g : X → [-1, 1] be any confidence rated predictor. For any µ &gt; 0, when |X ∩ C| ≥ max{ 4.15  α 2 ln(4/µ), 5.3 α , 8.3 ln(4/µ), 40}, we have that</p><formula xml:id="formula_33">Pr dist W 1 g(πρ (S| C )), g(πρ S| C ) &gt; α ≤ 2(1 + |X ∩ C|)µ.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Putting these claims together, we get that Pr</head><formula xml:id="formula_34">Xa,h [dist W 1 (h(πρ (X a | C )), h(πρ (X b | C ))) &gt; α] ≤ 2µ(|X ∩ C| + 1) • e ζ + η.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Now we proceed to prove Claims 1 and 2.</head><p>Proof of Claim 1. First, note that since the algorithm L postprocesses the report output by the data curator, by the fact that max-information is preserved under postprocessing, it inherits its max-information. Let A * be the combined algorithm L•A. Then by the definition of max-information, and since (X a , A * (S)) is distributed exactly the same as (S, A * (X a )),</p><formula xml:id="formula_35">I η ∞ (X a ; A * (X a )) = dist η ∞ X a , A * (X a ) || X a , A * (S) = dist η ∞ X a , A * (X a ) || S, A * (X a ) .</formula><p>we have that for all</p><formula xml:id="formula_36">T such that Pr[(X a , A * (X a ) ∈ T ] &gt; η, log Pr[(X a , A * (X a )) ∈ T ] -η Pr[(S, A * (X a ))) ∈ T ] ≤ ζ.</formula><p>Given C, we can post-process a pair</p><formula xml:id="formula_37">(X a , A * (X a )) to compute (h(πρ (X a | C ))) and (h(πρ (X b | C ))). This is because h ← A * (X a ) and X b = X \ X a .</formula><p>Applying the same post-processing to (S, A * (X a )) yields (h(πρ (S| C ))) and (h(πρ S| C )).</p><formula xml:id="formula_38">Let T = {(S, h) | dist W 1 (h(πρ (S| C )), h(πρ S| C )) &gt; α}.Then if Pr[(X a , A * (X a )) ∈ T ] &gt; η, log Pr[(X a , A * (X a )) ∈ T ] -η Pr[(S, A * (X a ))) ∈ T ] ≤ ζ.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>This means that if</head><formula xml:id="formula_39">Pr[dist W 1 (h(πρ (X a | C )), h(πρ (X b | C ))) &gt; α] &gt; η, log Pr[dist W 1 (h(πρ (X a | C )), h(πρ (X b | C ))) &gt; α] -η Pr[dist W 1 (h(πρ (S| C )), h(πρ S| C )) &gt; α] ≤ ζ.</formula><p>We can rearrange the above equation to get that</p><formula xml:id="formula_40">Pr[dist W 1 (h(πρ (X a | C )), h(πρ (X b | C ))) &gt; α] ≤ Pr[dist W 1 (h(πρ (S| C )), h(πρ S| C )) &gt; α] • e ζ + η,</formula><p>as required.</p><p>Proof of Claim 2. Let α, µ ∈ (0, 1) (the statement holds trivially for µ &gt; 1), and suppose |X ∩ C| ≥ max{ 4.15 α 2 ln(4/µ), 5.3 α , 8.3 ln(4/µ), 40}. By the definition of the distance metric we have the following:</p><formula xml:id="formula_41">dist W 1 g(πρ (S| C )), g(πρ S| C ) = 1 -1 |cdf g(πρ(S| C )) (g) -cdf g(πρ(S| C )) (g)| dg.<label>(3)</label></formula><p>To bound this value, we first prove the following for a fixed y ∈ [-1, 1].</p><formula xml:id="formula_42">Pr |cdf g(πρ(S| C )) (y) -cdf g(πρ(S| C )) (y)| ≥ α ≤ µ.<label>(4</label></formula><p>) Then, we observe that there are at most |X ∩ C| + 1 effectively different values of y we need to consider with respect to any fixed g and C. (For every realization of S, S, cdf g(πρ(S| C )) can only change for values of y on which cdf g(πρ(X| C )) changes. These values correspond to the partitioning of [-1, 1] into intervals induced by applying g • π ρ (•) to the elements in X ∩ C.) By union bounding over these |X ∩ C| + 1 effectively different values of y, Equation (4) gives us the following. Pr sup y∈[-1,1]</p><formula xml:id="formula_43">|cdf g(πρ(S| C )) (y) -cdf g(πρ(S| C )) (y)| ≥ α ≤ (1 + |X ∩ C|)µ.<label>(5)</label></formula><p>Finally, substituting the bound from Equation ( <ref type="formula" target="#formula_43">5</ref>) in Equation ( <ref type="formula" target="#formula_41">3</ref>) proves the lemma .</p><p>To show Equation ( <ref type="formula" target="#formula_42">4</ref>), we redefine the sampling process in a way that will allow us to use Theorem 5, a concentration result for the hypergeometric distribution (See Definition 10 for a definition of this distribution): Consider an urn consisting of n balls. Among those n balls, m are marked with a red stripe, representing membership in C ∩ X. Among the m red-striped balls, t are further marked with a blue stripe, representing x ∈ C ∩ X such that g(πρ (x)) ≤ y for the value of y being considered. Consider the experiment where we sample n/2 balls without replacement, and define the joint pair of random variables (V, W ) where V counts the number of red-striped balls in the sample, (i.e., the number of sampled points that are in X ∩ C) and W counts the number of (red and) blue-striped balls in the sample, (i.e., the number of sampled points x that are in X ∩ C and satisfy g(πρ (x)) ≤ y.</p><p>The random variables W and V follow hypergeometric distributions as follows:</p><formula xml:id="formula_44">V ∼ H(n, m, n/2) (W |V = v) ∼ H(m, t, v).</formula><p>Observe that the absolute value of the CDF difference we are trying to bound is equal to</p><formula xml:id="formula_45">W V -t-W</formula><p>m-V by definition. Let E 1 be the event that the number of red-striped balls in the sample is close to its expected value (i.e., |V -m/2| &lt; m/4). Then applying Theorem 5 and using m &gt; 40 and m &gt; 8.3 ln(4/µ) we have that</p><formula xml:id="formula_46">Pr[E 1 ] &lt; 2 exp -2 m + 1 • (m/4) 2 -1 ≤ 2 exp -2 1.025m • 0.99(m/4) 2 = 2 exp 1.98 16.4 m &lt; µ/2.</formula><p>Now let us condition on a realization V = v. Given this, let E 2 be the event that the number of blue-striped balls in the sample is close to its expected value (i.e., W -tv m ≤ ζ). Then applying Theorem 5 for ζ ≥ 2 and c as in the theorem:</p><formula xml:id="formula_47">Pr[E 2 V = v] &lt; 2 exp -2c • ζ 2 -1 . Observe that c = max 1 v + 1 + 1 m -v + 1 , 1 t + 1 + 1 m -t + 1 ≥ 1 t + 1 + 1 m -t + 1 ≥ 2 m 2 + 1 . Therefore, Pr[E 2 V = v] &lt; 2 exp -4 m 2 + 1 • ζ 2 -1 .<label>(6)</label></formula><p>Assume that both events E 1 and E 2 hold. Then in this case we will argue that:</p><formula xml:id="formula_48">W V - t -W m -V &lt; mζ m 2 /4 -γ 2 = α.<label>(7)</label></formula><p>We will then substitute the derived value of ζ into Equation <ref type="formula" target="#formula_47">6</ref>to show that</p><formula xml:id="formula_49">Pr[E 2 V = v] &lt; µ/2.</formula><p>To this end, observe that if the number of blue-striped balls in the sample is within ζ of the expected value, tV m , then the number of blue-striped balls in the sample is also within ζ of its own expected value, t(m-V ) m . This is because the balls can only be in one of these two sets. Therefore, when both events E 1 and E 2 hold, we have that:</p><formula xml:id="formula_50">(t -W ) - t(m -V ) m &lt; ζ.</formula><p>Therefore,</p><formula xml:id="formula_51">W V - t -W m -V &lt; E[W ] + ζ • 1 V -E[t -W ] -ζ 1 m -V because |W - tV m | &lt; ζ = tV m + ζ • 1 V - t(m -V ) m -ζ • 1 m -V = mζ V (m -V ) &lt; mζ m 2 /4 -γ 2 (because |V -m/2| &lt; γ) . Similarly, t -W m -V - W V &lt; mζ m 2 /4 -γ 2 .</formula><p>This gives us Equation 7. We can now set α = mζ m 2 /4-γ 2 to get that ζ = 3mα 16 . Now, substituting this ζ value back into Equation 6 and using m &gt; 40, m &gt; 16/3α, and m &gt; 4.15 α 2 ln(4/µ) we have the following:</p><formula xml:id="formula_52">Pr[E 2 V = v] &lt; exp -4 m 2 + 1 • ζ 2 -1 = 2 exp -4 m 2 + 1 • 9m 2 α 2 16 2 -1 ≤ 2 exp -8 1.05m • 0.9 • 9m 2 α 2 16 2 = 2 exp 8.1α 2 m 33.6 &lt; µ/2.</formula><p>Finally, we get the following:</p><formula xml:id="formula_53">Pr W V - t -W m -V &gt; α ≤ Pr[E 1 ∨ E 2 ] ≤ µ.</formula><p>With Claims 1 and 2 now proved, this concludes the proof of Theorem 6.</p><p>Proof of Theorem 6. In the remaining part of this section, we use Lemma 1 to prove that any data curation algorithm A with bounded max-information also enforces wassersteincoherence.</p><p>Proof Of Theorem 6. Fix η &gt; 0. Fix any subpopulation C ∈ C. Consider any dataset X.</p><p>Then for any algorithm L : Y → {X → [-1, 1]}, dataset X ∈ X n , and µ &gt; 0, such that</p><formula xml:id="formula_54">|X ∩ C| ≥ max 4.15 • ln(4/µ) α 2 , 5.3 α , 8.3 • ln(4/µ), 40 ,<label>(8)</label></formula><p>we have the following (by Lemma 1)</p><formula xml:id="formula_55">Pr Xa,X b ←X,h←L•A(Xa) dist W 1 h(πρ (X a | C )), h(πρ (X b | C )) &gt; α ≤ 2(|X ∩ C| + 1)µ • e ζ + η (9)</formula><p>where the choice of split X a , X b $ ← -X and the computed predictor h ← L(X a ) are as in the DemCoh L•A,X,C * ,ρ (α) experiment in Figure <ref type="figure">1</ref>.</p><p>We need to show, instead, that for</p><formula xml:id="formula_56">γ = max 8.3 • (ζ + ln(16|C|/β)) α 2 , 36 ln((3/α) α 2 , 16.6 • (ζ + ln(16|C|/β)), 5.3 α ,<label>80</label></formula><p>, and all algorithms L : Y → (X → [-1, 1]), all datasets X ∈ X n , the probability the following holds for all C ∈ C (such that |X| C | ≥ γ,) is low:</p><formula xml:id="formula_57">dist W 1 h π ρ (X a | C ) , h π ρ (X b | C ) &gt; α,</formula><p>where the split X a , X b $ ← -X and predictor h ← L(X a ) are as in Figure <ref type="figure">1</ref>.</p><p>To that end, we start by considering the fixed subpopulation C and setting µ = η/ 2(|X ∩ C| + 1) • e ζ (ensuring that the RHS of Equation ( <ref type="formula">9</ref>) is 2 η ). The main content in the proof will be arguing that the following lower bound on the size of |X ∩ C| implies the condition in Equation <ref type="bibr" target="#b7">(8)</ref>. We can then union bound over all sub-populations in C to get the theorem.</p><formula xml:id="formula_58">|X ∩ C| ≥ max 8.3 • (ζ + ln(16/η)) α 2 , 36 ln(3/α) α 2 , 16.6 • (ζ + ln(16/η)), 5.3 α , 80 .<label>(10)</label></formula><p>Substituting the value of µ back in Equation ( <ref type="formula" target="#formula_54">8</ref>), the first term in the max corresponds to the condition</p><formula xml:id="formula_59">|X ∩ C| ≥ 4.15 • ln(8(|X ∩ C| + 1)e ζ /η)</formula><p>α 2 which can also be written as:</p><formula xml:id="formula_60">|X ∩ C| ≥ 4.15 • ln((|X ∩ C| + 1) + g α 2 where g = 4.15 • (ζ + ln(8/η)). Assume |X∩C| 2 ≥ 4.15•ln((|X∩C|+1) α 2</formula><p>. Then, as long as |X ∩ C| ≥ 2g α 2 , the condition is satisfied. Now, using the fact that |X ∩ C| ≥ 80, we have that ln((|X ∩ C| + 1) ≤ 1.01 ln((|X ∩ C| + 1), which implies that it suffices for |X ∩ C| ≥ 9•ln((|X∩C|)  . Similarly, to be larger than the third term in the max in Equation ( <ref type="formula" target="#formula_54">8</ref>), we need</p><formula xml:id="formula_61">|X ∩ C| ≥ 8.3 • ln(8(|X ∩ C| + 1)e ζ /η)</formula><p>which can also be written as</p><formula xml:id="formula_62">|X ∩ C| ≥ 8.3 • ln((|X ∩ C| + 1) + g where g = 8.3 • (ζ + ln(8/η)). Assume |X∩C| 2 ≥ 8.3 • ln((|X ∩ C| + 1)</formula><p>. Then, as long as |X ∩ C| ≥ 2g, the condition is satisfied. Now, using the fact that |X ∩ C| ≥ 80, we have that ln((|X ∩ C| + 1) ≤ 1.01 ln((|X ∩ C| + 1), which implies that it suffices for |X ∩ C| ≥ 18 • ln((|X ∩ C|),which is true as long as |X ∩ C| ≥ 80, hence this case is taken care of.</p><p>Hence, for a single subpopulation C we have argued that it is sufficient that</p><formula xml:id="formula_63">|X ∩ C| ≥ max 8.3 • (ζ + ln(8/η)) α 2 , 36 ln(3/α) α 2 , 16.6 • (ζ + ln(8/η)), 5.3 α , 80 .<label>(11)</label></formula><p>Setting η = β/2|C|, and applying a union bound on Equation (9) (over all subpopulations in the class) then gives the theorem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Differentially Private Algorithms Enforce Demographic Coherence</head><p>In this section, we argue that our definition of demographic coherence can be achieved via differentially private algorithms. We do this by adapting known connections between differential privacy and max-information, and Theorem 6 connecting max-information and demographic coherence.</p><p>The proofs for pure differential privacy and approximate differential privacy follow a similar flavor. Firstly, we adapt known connections between (pure and approximate) differential privacy and max-information to the setting of sampling without replacement. Then, we use Theorem 6 (connecting bounded max-information to demographic coherence) to argue that differential privacy implies demographic coherence.</p><formula xml:id="formula_64">Theorem 7. [Pure-DP Enforces Wasserstein Coherence] Fix any ε, β ∈ (0, 1], α ∈ (0, 1], n ∈ N.</formula><p>Let C be a collection of subpopulations C ∈ X * . Consider an order-invariant ε-DP algorithm A : X n/2 → Y. Fix any lens ρ. Then, A enforces (α, β)-Wasserstein-coherence with respect to collection C, lens ρ, and size constraint γ, where</p><formula xml:id="formula_65">γ = max 8.3 • (ε 2 n/4 + ε n ln(4|C|/β)/2 + ln(16|C|/β)) α 2 , 36 ln((3/α) α 2 , 16.6 • (ε 2 n/4 + ε n ln(4|C|/β)/2 + ln(16|C|/β)), 5.3 α , 80 .<label>(12)</label></formula><p>Proof. Fix β &gt; 0. By Theorem 11 connecting differential privacy and max-information, we have that we have that,</p><formula xml:id="formula_66">I β/2|C| ∞ (A, n/2) ≤ ε 2 n/4 + ε n ln(4|C|/β)/4.</formula><p>Applying Theorem 6 and substituting the bound on max-information then completes the proof.</p><p>Theorem 8. [Approx-DP Enforces Wasserstein Coherence] Fix any β ∈ (0, 1], α ∈ (0, 1], n ∈ N. Let ε ∈ (0, 1  2 ], and δ ∈ (0, ε 2 β 2 (120n) 2 |C| 2 ] Let C be a collection of subpopulations C ∈ X * . Consider an order-invariant<ref type="foot" target="#foot_13">foot_13</ref> (ε, δ)-DP algorithm A : X n/2 → Y. Fix any lens ρ. Then, A enforces (α, β)-Wasserstein-coherence with respect to collection C, lens ρ, and size constraint γ, where γ = max 8.</p><p>3 • (265ε 2 n + 12ε n ln(4|C|/β) + ln(32|C|/β)) α 2 , 36 ln((3/α) α 2 , 16.6 • (265ε 2 n + 12ε n ln(4|C|/β) + ln(32|C|/β)), 16/3 α , 80 .</p><p>Proof. Fix β &gt; 0 and γ = β 2|C| . By Corollary 2 connecting differential privacy and maxinformation, we have that we have that, as long as δ ∈ (0,</p><formula xml:id="formula_68">ε 2 β 2 (120n) 2 |C| 2 ] I β/2|C| ∞ (A, n/2) ≤ 265 2 ε 2 n + 12ε n 2 ln(4|C|/β).</formula><p>Applying Theorem 6 and substituting the bound on max-information then completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Comparison to Perfect Generalization</head><p>In this section we include a comparison between demographic coherence (Definition 3) and the notion of perfect generalization introduced by Cummings et al. <ref type="bibr" target="#b13">[18]</ref>. (See also the work of Bassily and Freund <ref type="bibr" target="#b6">[7]</ref> that independently introduced a generalization thereof.) This notion was originally meant to capture generalization under post-processing but has since been shown to be closely related to other desirable properties as well (e.g., replicability <ref type="bibr" target="#b7">[8]</ref>). Our framework shares conceptual similarities with this definition, but the technical details differ in important ways.</p><p>The following comparison uses the definition of sample perfect generalization (Definition 11) from <ref type="bibr" target="#b7">[8]</ref> which is roughly equivalent to the original definition from <ref type="bibr" target="#b13">[18]</ref>. Intuitively, a mechanism M running on i.i.d. samples from some distribution (sample) perfectly generalizes if the distribution of its output does not depend "too much" on specific realization of its sampled input. That is, its output distributions when run on two i.i.d samples from any distribution are indistinguishable.</p><p>Definition 11 (Sample perfect generalization <ref type="bibr" target="#b7">[8,</ref><ref type="bibr">Def 3.4]</ref> ). An algorithm A : X m → Y is said to be (β, ǫ, δ)-sample perfectly generalizing if, for every distribution D over X , with probability at least 1 -β over the draw of two i.</p><formula xml:id="formula_69">i.d. samples X a , X b ∼ D m , A(X a ) ≈ ǫ,δ A(X b ),</formula><p>where ≈ ǫ,δ denotes ǫ, δ indistinguishability. Definition 3 has several noticeable syntactic differences when compared to Definition 11. First, demographic coherence is defined within a specific framework that explicitly lays out the entire data release pipeline, a design choice that intentionally lends itself to concrete intuition (and experimental evaluation) of data release. However, this still leaves open the possibility that the core statistical guarantee of demographic coherence is roughly equivalent to perfect generalization. In other words, it may still be the case that demographic coherence is simply a different way to describe the protections offered by perfect generalization; as we see below, this is not the case. Second, while "closeness" in the definition of perfect generalization is required for distributions over the entire sets X a , X b , the "closeness" in the definition of demographic coherence is required for distributions over the sets X a | C , X b | C for subpopulations C ⊆ X from some collection C. For the sake of drawing a more direct comparison here, we collapse this difference by comparing sample perfect generalization to demographic coherence with C = {X }.</p><p>A third difference in the definitions is is the choice of sets X a , X b that the comparison is made with respect to, i.e., a random partition of a fixed dataset in demographic coherence vs. i.i.d. draws from a distribution in the case of perfect generalization. The choice of random partitioning in our framework is made to ensure concreteness and applicability in census-like settings but it is chosen intentionally to maintain both intuitive and quantitative similarities to i.i.d. sampling. Thus, we view this as more of a difference in interpretability and applicability of the definitions, rather than one about their underlying guarantees.</p><p>The main difference between the two definitions, thus, is in how how "closeness" is measured-as spelled out in Figure <ref type="figure" target="#fig_1">2</ref>.</p><formula xml:id="formula_70">An algorithm A : X n/2 → Y is: 1. (β, ε, δ)-sample perfectly generalizing if ∀ distributions D over X , with probability at least 1 -β over X a , X b ∼ D n/2 : A(X a ) ≈ ε,δ A(X b ) 2. (α, β)-coherence enforcing if ∀ datasets X ∈ X n , learners L : Y → (X → [-1, 1]</formula><p>), with probability at least 1 -β over the random split X a ∪ X b = X and the coins of A, L:</p><formula xml:id="formula_71">dist W (h a (X a ), h a (X b )) ≤ α</formula><p>where h a ← L • A(X a ) is a confidence rated predictor, and h a (X i ) is the distribution induced by randomly choosing x ∼ X i and computing h a (x). Perfect generalization asks that w.h.p. over independent samples, A produces indistinguishable distributions over reports R a ← A(X a ) and R b ← A(X b ). Meanwhile, coherence enforcement asks that w.h.p. L • A(X a ) produces a confidence rated predictor h a : X → [-1, 1] which has "similar" predictions on X a and X b (a property enforced by A). That is, the comparison in perfect generalization is on the behavior of the algorithm A itself, while the comparison in demographic coherence is on the likely behavior of a realized hypothesis h a that is produced only over the report R a ← A(X a ).</p><p>Since coherence enforcement limits the set of algorithms against which indistinguishability applies, one might expect that perfectly generalizing algorithms also enforce demographic coherence, and indeed, Theorem 6 proves this to be true. However, the converse need not be true-implying that demographic coherence is a relaxation of perfect generalization. In particular, the example below shows a set X such that no confidence rated predictor h : X → [-1, 1] violates this property. In this case, all data curators are vacuously coherence enforcing.</p><p>Consider a data curator A : {0, 1} n/2 → {0, 1} n/2 that (deterministically) publishes its input in the clear. This is clearly not perfectly generalizing as the distribution of the report X a is a point mass that (for reasonable choices of X, with high probability) is distinct from the distribution of the report X b .</p><p>Meanwhile, considering a dataset X ∈ {0, 1} n and a random split X a , X b ← X of the dataset, there are two possible predictors h : {0, 1} → [-1, 1] that witnesses the highest possible Wasserstein distance when run on X a vs. X b . That is, without loss of generality h is either h(0) = -1, h(1) = 1 or h(0) = 1, h(1) = -1. In either case, h cannot be improved even by seeing X a in the clear. So, any data curation algorithm in this scenario, is coherence enforcing since the data itself doesn't have enough complexity to allow for a violation of demographic coherence. Note that the absence of information correlated with the bits contained in the dataset X (e.g., time, location, computer system) is crucial to this example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Differential Privacy implies Bounded Max-Information: Sampling without Replacement</head><p>Prior work shows that for datasets sampled i.i.d., differentially private algorithms have bounded max-information <ref type="bibr" target="#b18">[23,</ref><ref type="bibr" target="#b42">47]</ref>. In this appendix we prove Theorem 11 and Theorem 12, which are analogs of those theorems for sampling without replacement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Preliminaries</head><p>First we state Theorem 9, a version of McDiarmid's inequality that applies to the case of sampling without replacement. This result follows from Lemma 2 in <ref type="bibr" target="#b49">[54]</ref>. This result in used in the proof of Theorem 11, which says that pure-DP algorithms have bounded maxinformation (even in the case of sampling without replacement.)</p><p>Definition 12. A function f : X m → Y, is called order invariant if for all X ∈ X m , the value of the function f (X) does not depend on the order of the elements of X.</p><p>Theorem 9 (McDiarmid's for sampling without replacement <ref type="bibr" target="#b49">[54]</ref>). Let f : X n → Y be an an order invariant function with global sensitivity ∆ &gt; 0. Let X be a data universe of size n, let S be a sample of size m chosen without replacement from X . Then for t ≥ 0,</p><formula xml:id="formula_72">Pr S [f (S) -E[f (S)] ≥ t] ≤ exp - 2t 2 m∆ 2 • n -1/2 n -m • 1 - 1 2 max(m, n -m)</formula><p>In particular, for m = n/2 and n ≥ 3,</p><formula xml:id="formula_73">Pr[f (S) -E[f (S)] ≥ t] ≤ exp - 4t 2 n∆ 2</formula><p>Next we state some lemmas that are used in the proof of Theorem 12 and Corollary 2 which say that approximate-DP algorithms have bounded max-information (even in the case of sampling without replacement.) Definition 13 (Point-wise indistinguishibility <ref type="bibr" target="#b33">[38]</ref>). Two random variables A, B are pointwise (ε, δ)-indistinguishable if with probability at least 1 -δ over a ∼ P (A):</p><formula xml:id="formula_74">e -ǫ Pr [B = a] ≤ Pr [A = a] ≤ e ǫ Pr [B = a] .</formula><p>Lemma 2 (Indistinguishability implies Pointwise Indistinguishability <ref type="bibr" target="#b33">[38]</ref>). Let A, B be two random variables. If A ≈ ε,δ B then A and B are pointwise 2ε, 2δ  1-e -ε -indistinguishable.</p><p>Lemma 3 (Conditioning Lemma <ref type="bibr" target="#b33">[38]</ref>). Suppose that (A, B) ≈ ε,δ (A ′ , B ′ ). Then for every δ &gt; 0, the following holds:</p><formula xml:id="formula_75">Pr t∼P (B) A| B=t ≈ 3ǫ, δ A ′ | B ′ =t ≥ 1 - 2δ δ - 2δ 1 -e -ε .</formula><p>Theorem 10 (Azuma's Inequality). Let C 1 , • • • , C n be a sequence of random variables such that for every i ∈ [n], we have</p><formula xml:id="formula_76">Pr [|C i | ≤ α] = 1</formula><p>and for every fixed prefix C i-1</p><formula xml:id="formula_77">1 = c i-1 1 , we have E C i |c i-1 1 ≤ γ,</formula><p>then for all t ≥ 0, we have</p><formula xml:id="formula_78">Pr n i=1 C i &gt; nγ + t √ nα ≤ e -t 2 /2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Pure-DP =⇒ Bounded Max-Information</head><p>In this appendix we state Theorem 11, which is an analog of Theorem from <ref type="bibr" target="#b18">[23]</ref>. The proof of this theorem works exactly as in <ref type="bibr" target="#b18">[23]</ref>, except replacing the application of McDiarmid's Lemma with a version of McDiarmid's for sampling without replacement (Theorem 9) which we state in Appendix B.1.</p><p>Theorem 11. (Pure-DP =⇒ Bounded Max-Information) Fix n ∈ N, ε &gt; 0 and let X be a data universe of size at least n. Let A : X n/2 → Y be an order-invariant ε-DP algorithm.</p><p>Then for any γ &gt; 0,</p><formula xml:id="formula_79">I γ ∞ (A, n/2) ≤ ε 2 n/4 + ε n ln(2/γ)/4.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 (ε, δ)-DP =⇒ Bounded Max-Information</head><p>In this appendix we prove Theorem 12, which is an analog of Theorem 1 from Rogers et al. <ref type="bibr" target="#b42">[47]</ref>. In fact, the following proof is almost exactly the proof of them from <ref type="bibr" target="#b42">[47]</ref> with the following differences: (1) we compute all the constants exactly and avoid using asymptotic notation, and (2) we keep the tunable parameters in the final version of the theorem to obtain the most flexible result that we can. Finally, we set the parameters in Theorem 12 to get Corollary 2, which is used in the proof of Theorem 8.</p><p>Theorem 12. (Approx-DP =⇒ Bounded Max-Information, Generalised) Let A : X n → Y be an (order-invariant) (ε, δ)-differentially private algorithm for ε ∈ (0, 1/2], δ ∈ (0, ε). For δ ∈ (0, ε/15], t &gt; 0, and β(t, δ) = e -t 2 /2 + n 2δ δ + 2 δ+2δ</p><p>1-e -3ε</p><p>we have</p><formula xml:id="formula_80">I β ∞ (A, n) ≤ n   347 δ + 75 δ ε 2 + 24 δ2 ε + 240ε 2   + 6tε √ n. Corollary 2. (Approx-DP =⇒ Bounded Max-Information, Specific) Fix n ∈ N, for ε ∈ (0, 1/2], γ ∈ (0, 1], δ ∈ (0, ε 2 γ 2 (120n) 2 ]</formula><p>and let X be a data universe of size at least n. Let A : X n → Y be an (order-invariant) (ε, δ)-differentially private algorithm. We have that</p><formula xml:id="formula_81">I γ ∞ (A, n) ≤ 265ε 2 n + 12ε n ln(2/γ).</formula><p>We will sometimes abbreviate conditional probabilities of the form Pr [X = x | A = a] as Pr [X = x | a] when the random variables are clear from context. Further, for any x ∈ X n and a ∈ Y, we define</p><formula xml:id="formula_82">Z i (a, x [i] ) def = log Pr X i = x i | a, x [i-1] Pr X i = x i | x [i-1] .<label>(14)</label></formula><p>Z(a, x)</p><formula xml:id="formula_83">def = log Pr x [A(x) = a, X = x] Pr [A = a] • Pr [X = x] = n i=1 Z i (a, x [i] )<label>(15)</label></formula><p>If we can bound Z(a, x) with high probability over (a, x) ∼ p(A(X), X), then we can bound the approximate max-information by using the following lemma: Lemma 4 ([23, <ref type="bibr">Lemma 18]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pr log Pr</head><formula xml:id="formula_84">x [A(x) = a, X = x] Pr [A = a] • Pr [X = x] ≥ k ≤ β =⇒ I β ∞ (A(X); X) ≤ k.</formula><p>To bound Z(a, x) with high probability over (a, x) ∼ p(A(X), X) we will apply Azuma's inequality (Theorem 10) to the sum of the Z i (a, x [i] )'s. For this we must first argue that each Z i (a, x [i] ) term is bounded with high probability: Claim 3. Let δ &gt; 0, and δ ′′ def = 2 δ 1-e -3ε . If A is (ε, δ)-differentially private and, X ∈ X n is sampled without replacement from a finite universe X , then for each i ∈ [n], and each prefix x [i-1] ∈ X i-1 and answer a, we have:</p><formula xml:id="formula_85">Pr x i ∼X i |x [i-1] log Pr X i = x i | a, x [i-1] Pr X i = x i | x [i-1] ≤ 6ε ≥ 1 -δ ′′ Proof. Whenever X i | x [i-1] and X i | a,x [i-1] are 3ǫ, δ -indistinguishable, Lemma 2 tells us that X i | x [i-1] and X i | a,x [i-1] are point-wise (6ǫ, δ ′′ )-indistinguishable. i.e., given that X i | x [i-1] and X i | a,x [i-1] are 3ǫ, δ -indistinguishable, we have that Pr x i ∼X i |x [i-1] log Pr X i = x i | a, x [i-1] Pr X i = x i | x [i-1] ≤ 6ε ≥ 1 -δ ′′ Claim 4. Let δ &gt; 0, δ ′ def = 2δ δ + 2δ</formula><p>1-e -ε , and δ ′′ def = 2 δ 1-e -3ε . If A is (ε, δ)-differentially private and, X ∈ X n is sampled without replacement from a finite universe X , then for each i ∈ [n], and each prefix x [i-1] ∈ X i-1 we have:</p><formula xml:id="formula_86">Pr x i ∼X i |x [i-1] a∼A|x [i-1] log Pr X i = x i | a, x [i-1] Pr X i = x i | x [i-1] ≤ 6ε ≥ 1 -δ ′ -δ ′′ Proof.</formula><p>For this proof, we use Claim 3 and then show for each i ∈ [n], and prefix</p><formula xml:id="formula_87">x [i-1] ∈ X i-1 , Pr a∼p A|x [i-1] X i | x [i-1] ≈ 3ε, δ X i | a,x [i-1] ≥ 1 -δ ′ .</formula><p>We use the differential privacy guarantee on A to show that (A,</p><formula xml:id="formula_88">X i )| x [i-1] ≈ ε,δ A| x [i-1] ⊗ X i | x [i-1]</formula><p>. The above equation then follows directly from the conditioning lemma Lemma 3.</p><p>Fix any set O ⊆ Y × X and prefix x [i-1] ∈ X i-1 . From the differential privacy of A, and the order-invariance of the algorithm, we get the following (where the first inequality follows from DP.):</p><formula xml:id="formula_89">Pr (A(X), X i ) ∈ O | x [i-1] = x i ∼X i |x [i-1] Pr X i = x i | x [i-1] Pr (A(X), x i ) ∈ O | x [i-1] , x i ≤ x i ∼X i |x [i-1] Pr X i = x i | x [i-1] e ε Pr (A(X), x i ) ∈ O | x [i-1] , t i + δ ∀t i ∼ X i | x [i-1] = x i ,t i ∼X i |x [i-1] Pr X i = t i | x [i-1] Pr X i = x i | x [i-1] e ε Pr (A(X), x i ) ∈ O | x [i-1] , t i + δ = x i ∼X i |x [i-1] Pr X i = x i | x [i-1] e ε Pr (A(X), x i ) ∈ O | x [i-1] + δ ≤ e ε   x i ∼X i |x [i-1] Pr X i = x i | x [i-1] Pr A(X), X i ) ∈ O | x [i-1]   + δ = e ε Pr A(X) ⊗ X i ∈ O | x [i-1] + δ<label>15</label></formula><p>Applying a very similar argument, will give us that</p><formula xml:id="formula_90">Pr A(X) ⊗ X i ∈ O | x [i-1] ≤ e ε Pr (A(X), X i ) ∈ O | x [i-1] + δ.</formula><p>Having shown a high probability bound on the terms Z i , our next step is to bound their expectation so that we can continue towards our goal of applying Azuma's inequality.</p><p>We will use the following shorthand notation for conditional expectation:</p><formula xml:id="formula_91">E Z i (A, X [i] ) | a, x [i-1] , |Z i | ≤ 6ε def = E Z i (A, X [i] ) | A = a, X [i-1] = x [i-1] , |Z i (A, X [i] )| ≤ 6ε ,</formula><p>Lemma 5. Let A be (ε, δ)-differentially private and, X ∈ X n be sampled without replacement from a finite universe X . Let ε ∈ (0, 1/2] and δ ∈ (0, ε/15],</p><formula xml:id="formula_92">X i | x [i-1] ≈ 3ε, δ X i | a,x [i-1] =⇒ E Z i (A, X [i] ) | a, x [i-1] , |Z i | ≤ 6ε = O(ε 2 + δ).</formula><p>More precisely, E Z i (A,</p><formula xml:id="formula_93">X [i] ) | a, x [i-1] , |Z i | ≤ 6ε ≤ ν( δ)</formula><p>, where ν( δ) is defined in <ref type="bibr" target="#b11">(16)</ref>.</p><formula xml:id="formula_94">Proof. Let S def = {x i | a, x [i-1] , |Z i | &lt; 6ε}.</formula><p>Given an outcome and prefix (a,</p><formula xml:id="formula_95">x [i-1] ) such that X i | x [i-1] ≈ 3ε, δ X i | a,x [i-1] ,</formula><p>we have the following by definition:</p><formula xml:id="formula_96">E Z i (A, X [i] ) | a, x [i-1] , |Z i | ≤ 6ε = x i ∈S Pr X i = x i | a, x [i-1] , |Z i | ≤ 6ε Z i (a, x [i] ) = x i ∈S Pr X i = x i | a, x [i-1] , |Z i | ≤ 6ε log Pr[X i =x i |a,x [i-1]] Pr[X i =x i |x [i-1]]</formula><p>Claim 5.</p><formula xml:id="formula_97">x i ∈S Pr X i = x i | x [i-1] log Pr[X i =x i |a,x [i-1]] Pr[X i =x i |x [i-1]] ≤ log 1-Pr[X i / ∈S|a,x [i-1]] 1-Pr[X i / ∈S|x [i-1]]</formula><p>Proof.</p><formula xml:id="formula_98">x i ∈S Pr X i = x i | a, x [i-1] log Pr[X i =x i |a,x [i-1]] Pr[X i =x i |x [i-1]] = Pr X i ∈ S | x [i-1] x i ∈S Pr[X i =x i |x [i-1]] Pr[X i ∈S|x [i-1]] log Pr[X i =x i |a,x [i-1]] Pr[X i =x i |x [i-1]] ≤ x i ∈S Pr[X i =x i |x [i-1]] Pr[X i ∈S|x [i-1]] log Pr[X i =x i |a,x [i-1]] Pr[X i =x i |x [i-1]] ≤ log x i ∈S Pr[X i =x i |a,x [i-1]] Pr[X i ∈S|x [i-1]] ≤ log Pr[X i ∈S|a,x [i-1]] Pr[X i ∈S|x [i-1]] = log 1-Pr[X i / ∈S|a,x [i-1]] 1-Pr[X i / ∈S|x [i-1]]</formula><p>The first inequality follows form the fact that all probabilities are less than one. The second inequality follows from noticing that x i ∈S</p><formula xml:id="formula_99">Pr[X i =x i |x [i-1]]</formula><p>Pr[X i ∈S|x [i-1]] = 1 and applying Jensen's inequality.</p><p>Let</p><formula xml:id="formula_100">Pr X i / ∈ {x i | a, x [i-1],|Z i |&lt;6ε } | a, x [i-1] = Pr X i / ∈ S | a, x [i-1] def = q. Note that, because X i | x [i-1] ≈ 3ε, δ X i | a,x [i-1]</formula><p>, we have for δ &gt; 0:</p><formula xml:id="formula_101">Pr X i / ∈ S | x [i-1</formula><p>] ≤ e 3ε Pr X i / ∈ S | a, x [i-1] + δ = e 3ε q + δ Note that q ≤ δ ′′ by Claim 3. Now, we can bound the following:</p><formula xml:id="formula_102">x i ∈S Pr X i = x i | x [i-1] log Pr[X i =x i |a,x [i-1]] Pr[X i =x i |x [i-1]] ≤ log 1-Pr[X i / ∈S|a,x [i-1]] 1-Pr[X i / ∈S|x [i-1]]</formula><p>≤ log(1 -q) -log(1 -(e 3ε q + δ))</p><p>≤ log(e) • (-q + e 3ε q + δ + 2(e 3ε q + δ) 2 )</p><p>= log(e) • ((e 3ε -1)q + δ + 2(e 3ε q + δ) 2 )</p><formula xml:id="formula_103">def = τ ( δ)</formula><p>where the second inequality follows by using the inequality (-x -2x 2 ) log(e) ≤ log(1 -x) ≤ -x log(e) for 0 &lt; x ≤ 1/2, and as (e 3ε q + δ) ≤ 1/2 for ε and δ bounded as in the lemma statement.</p><p>We use the results above to to upper bound the expectation we wanted:</p><formula xml:id="formula_104">E Z i (A, X [i] ) | a, x [i-1] , |Z i | ≤ 6ε ≤ xi∈S Pr X i = x i | a, x [i-1] , |Z i | ≤ 6ε log Pr[Xi=xi|a,x [i-1]] Pr[Xi=xi|x [i-1] ] - xi∈S Pr X i = x i | x [i-1] log Pr[Xi=xi|a,x [i-1]] Pr[Xi=xi|x [i-1]] + τ ( δ) = xi∈S Pr X i = x i | a, x [i-1] , |Z i | ≤ 6ε -Pr X i = x i | x [i-1] log Pr[Xi=xi|a,x [i-1]] Pr[Xi=xi|x [i-1]] + τ ( δ) ≤ |Zi|≤6ε 6ε xi∈S |Pr X i = x i | a, x [i-1] , |Z i | ≤ 6ε -Pr X i = x i | x [i-1] | + τ ( δ) ≤ Def of S, Claim 3 6ε xi∈S Pr X i = x i | x [i-1] max e 6ε Pr |Z i | ≤ 6ε | a, x [i-1] -1, 1 - e -6ε Pr |Z i | ≤ 6ε | a, x [i-1]</formula><p>+ τ ( δ)</p><formula xml:id="formula_105">≤ Claim 3 6ε e 6ε</formula><p>1-2 δ 1-e -3ε -1 + τ ( δ) ≤ Substituting for τ ( δ) 6ε e 6ǫ 1 + 4 δ 1-e -3ǫ -1 + log(e) • ((e 3ε -1)q + δ + 2(e 3ε q + δ) 2 ) ≤ Upper bound for q 6ε e 6ǫ 1 + 4 (1 -e -3ε ) + 8 δe 3ε + δ + 2 δ2 = b 24εe 6ε + 2e 3ε -2 + 8 δe 6ε</p><p>(1 -e -3ε ) + 8 δe 3ε + δ + 2 δ2 + 6ε(e 6ε -1) = δ 1 -e -3ε 2e 3ε (4e 3ε (3ε + δ 1 -e -3ε )) + 4 δ + 1) -2 + δ + 2 δ2 + 6ε(e 6ε -1)</p><formula xml:id="formula_106">≤ e -3ε</formula><p>≤1-1.5ε for ε∈[0,0.5] 2 δ 1.5ε e 3ε 4e 3ε (3ε + δ 1.5ε )) + 4 δ + 1 + δ -2 1.5ε + 2 δ + 1 + 6ε(e 6ε -1) ≤ 8 e 6ε δ 1.5ε 3ε + δ 1.5ε + 2 δ 1.5ε e 3ε 4 δ + 1 + δ -2 1.5ε + 2 δ + 1 + 6ε(e 6ε -1) ≤ e 3ε ≤1+7ε,e 6ε ≤1+40ε for ε∈[0,0.5] 8 (1 + 40ε) δ 1.5ε 3ε + δ 1.5ε + 2 (1 + 7ε) δ 1.5ε 4 δ + 1 + δ -2 1.5ε + 2 δ + 1 + 6ε(40ε) ≤ (8 + 320ε) δ 1.5ε 3ε + δ 1.5ε + (2 + 14ε) 1.5ε 4 δ2 + δ -2 δ 1.5ε + 2 δ2 + δ + 240ε 2 ≤ ε&lt;0.5 168 δ 1.5ε (3ε + δ 1.5ε ) + 8 1.5 δ2 ε + 56 1.5 δ2 + 14 1.5 δ + 2 δ2 + δ + 240ε 2 ≤ ε≤0.5 347 δ + 75 δ ε 2 + 24 δ2 ε + 240ε 2 def = ν( δ)</p><p>Finally, we need to apply Azuma's inequality (stated in Theorem 10) to a set of variables that are bounded with probability 1, not just with high probability. Towards this end, we now define (1) the sets G i ( δ) and G ≤i ( δ) of "good" tuples of outcomes and databases, and (2) a variable T i that will match Z i for "good events", and will be zero otherwise-and hence, is always bounded:</p><formula xml:id="formula_108">G i ( δ) = (a, x [i] ) |Z i (a, x [i] )| ≤ 6ε &amp; X i | x [i-1] ≈ 3ε, δ X i | a,x [i-1] ,<label>(17)</label></formula><p>G ≤i ( δ) = (a,</p><formula xml:id="formula_109">x [i] ) : (a, x 1 ) ∈ G 1 ( δ), • • • , (a, x [i] ) ∈ G i ( δ)<label>(18)</label></formula><p>T i (a,</p><formula xml:id="formula_110">x [i] ) = Z i (a, x [i] ) if (a, x [i] ) ∈ G ≤i ( δ) 0 otherwise<label>(19)</label></formula><p>Note that the variables T i indeed satisfy the requirements of Azuma's inequality. The first condition, Pr |T i (A, X [i] )| ≤ 6ε = 1 holds by definition, and the second holds because of Lemma 5.</p><p>We are now ready to prove our main theorem.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc>a lens ρ, and a distance metric dist(•, •).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>α 2 .</head><label>2</label><figDesc>Consider the inequality |X∩C| ln((|X∩C|) ≥ c. Note that the left hand side is an increasing function of |X ∩ C|. Let |X ∩ C| ≥ 2c ln c. Then, we get that |X∩C| ln((|X∩C|) ≥ 2c ln c ln(2c ln c) , and some arithmetic shows that for c ≥ 9 (which is true whenever α ≤ 1), the right hand side is indeed larger than c. Hence, it is additionally sufficient that |X ∩ C| ≥ 36 ln((3/α)   </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>α 2</head><label>2</label></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparing the definition of sample perfect generalization to a simplified definition of demographic coherence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>δ 1 -( 1 - 8 δ2 e 3ε 1 -</head><label>1181</label><figDesc>e -3ǫ -1 + log(e) • (e 3εe -3ε ) 2 + e -3ε = b= δ 1-e -3ε 6ε e 6ǫ (1 + 4b) -1 + log(e) • b 2e 3ε -2 + 8 δe 6ε</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>In this work we intentionally us the term "algorithm" broadly to capture, e.g., informal decision-making process made by humans that might not be explicitly codified as algorithms in the traditional sense.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>We use ergonomic in this context to mean ease of use by many different stakeholders. We intentionally move away from the term "usable," as this typically focuses only on end-users and we are interested in ease of use from a more diverse set of communities.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>e.g., reconstruction of features like gender can be carried out simply from knowing population statistics rather than breaking anonymity<ref type="bibr" target="#b43">[48]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>In reality, the property of demographic coherence applies to algorithms L that use private reports to design predictors.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>In our formal experiment, we actually suppress the formal object of the report. Specifically, we reason directly about the composition of some data processing algorithm A with an arbitrary algorithm L, rather than making the report an explicit object that is then passed to L.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>A more complex interpretation could also cover an approach to prediction that takes into account a social decision-making process. While this interpretation is beyond the technical scope of our work, it may be interesting to consider in future work.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>The notion of similarity is obviously a loaded one, as the ways in which two individuals are similar or different depend on the types of predictions being made about them. We eventually handle this by quantifying over many notions of similarity. For the sake of this motivation, it is enough to assume that the similarity of these individuals is meaningful with respect to the characteristic being predicted about them.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7"><p>As this approach sketch is mainly to motivate our final approach below, we gloss over some formalities in this description. For example, how do we know that L does not act differently when provided one input (Aux) and two inputs (Aux and R)?</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8"><p>We borrow this conceptual approach from<ref type="bibr" target="#b26">[31]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9"><p>The predictor may also have side information about individuals not contained in the dataset, which can be formally included in the description of the adversarial algorithm L.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_10"><p>In a real-world scenario, X might be sampled from some underlying population, but our definition does not require this and makes no assumptions about how it might be done.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_11"><p>This definition, for sampling without replacement, is slightly different than the original one.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_12"><p>Since it is an assumption in the version of McDiarmid's Inequality for sampling without replacement (Theorem 9) that we use.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_13"><p>Order-invariance can be relaxed by multiplying ε by 2 in the γ value, and dividing by 2 in the range of δ.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_14"><p>In the step where we apply DP, the parameters will double if the algorithm is not order-invariant.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6">Acknowledgments</head><p>We would like to thank <rs type="person">Adam Smith</rs> for insightful comments and feedback that helped improve this work. We thank <rs type="person">Christina Xu</rs> for her contributions to shaping the early phases of this project.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Proof of Theorem 12. For any constant ν, we have:</p><p>We then substitute ν by ν( δ) as defined in Equation ( <ref type="formula">16</ref>), and apply a union bound on Pr (A, X) / ∈ G ≤n ( δ) using Claim 4 to get</p><p>where the two inequalities follow from Claim 4 and Theorem 10, respectively. Therefore,</p><p>From Lemma 4, we have I</p><p>We now prove the Corollary 2, which we use in Section 5. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Census Bureau adopts Differential Privacy</title>
		<author>
			<persName><forename type="first">M</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><surname>Abowd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">S</forename><surname>The</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD &apos;18, page 2867</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD &apos;18, page 2867<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">3 Resume Screening Tools That Every Recruiter Should Know About</title>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Dixon</surname></persName>
		</author>
		<ptr target="https://ideal.com/resume-screening-tools/" />
		<imprint>
			<date type="published" when="2016-11-21">November 2016. 21 January 2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Machine-learningbased disease diagnosis: A comprehensive review</title>
		<author>
			<persName><forename type="first">Md</forename><surname>Manjurul Ahsan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shahana Akter</forename><surname>Luna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zahed</forename><surname>Siddique</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Healthcare</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Practical considerations for differential privacy</title>
		<author>
			<persName><forename type="first">Kareem</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergei</forename><surname>Vassilvitskii</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Apple Differential Privacy Team. Learning with privacy at scale</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Reconstructing training data with informed adversaries</title>
		<author>
			<persName><forename type="first">Borja</forename><surname>Balle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Cherubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Hayes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">43rd IEEE Symposium on Security and Privacy</title>
		<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">May 22-26, 2022. 2022</date>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="1138" to="1156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Typicality-based stability and privacy</title>
		<author>
			<persName><forename type="first">Raef</forename><surname>Bassily</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Freund</surname></persName>
		</author>
		<idno>CoRR, abs/1604.03336</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Stability is stable: Connections between replicability, privacy, and adaptive generalization</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Bun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gaboardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Hopkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><surname>Impagliazzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toniann</forename><surname>Pitassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satchit</forename><surname>Sivakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Sorrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual ACM Symposium on Theory of Computing, STOC 2023</title>
		<editor>
			<persName><forename type="first">Barna</forename><surname>Saha</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rocco</forename><forename type="middle">A</forename><surname>Servedio</surname></persName>
		</editor>
		<meeting>the 55th Annual ACM Symposium on Theory of Computing, STOC 2023<address><addrLine>Orlando, FL, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2023">June 20-23, 2023. 2023</date>
			<biblScope unit="page" from="520" to="527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Attacks on deidentification&apos;s defenses</title>
		<author>
			<persName><forename type="first">Aloni</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">31st USENIX Security Symposium, USENIX Security 2022</title>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Kevin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kurt</forename><surname>Butler</surname></persName>
		</editor>
		<editor>
			<persName><surname>Thomas</surname></persName>
		</editor>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2022">August 10-12, 2022. 2022</date>
			<biblScope unit="page" from="1469" to="1486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Linear program reconstruction in practice</title>
		<author>
			<persName><forename type="first">Aloni</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kobbi</forename><surname>Nissim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Priv. Confidentiality</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Towards formalizing the gdpr&apos;s notion of singling out</title>
		<author>
			<persName><forename type="first">Aloni</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kobbi</forename><surname>Nissim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl. Acad. Sci. USA</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="8344" to="8352" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Data reconstruction: When you see it and when you don</title>
		<author>
			<persName><forename type="first">Edith</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haim</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yishay</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shay</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kobbi</forename><surname>Nissim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Stemmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliad</forename><surname>Tsfadia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.15753</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">t. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">ATTAXON-OMY: unpacking differential privacy guarantees against practical adversaries</title>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Cummings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shlomi</forename><surname>Hod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayshree</forename><surname>Sarathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marika</forename><surname>Swanberg</surname></persName>
		</author>
		<idno>CoRR, abs/2405.01716</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adaptive learning with robust generalization guarantees</title>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Cummings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katrina</forename><surname>Ligett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kobbi</forename><surname>Nissim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwei</forename><forename type="middle">Steven</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Conference on Learning Theory</title>
		<editor>
			<persName><forename type="first">Vitaly</forename><surname>Feldman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alexander</forename><surname>Rakhlin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ohad</forename><surname>Shamir</surname></persName>
		</editor>
		<meeting>the 29th Conference on Learning Theory<address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-23">2016. June 23-26, 2016. 2016</date>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="772" to="814" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Confidence-ranked reconstruction of census microdata from published statistics</title>
		<author>
			<persName><forename type="first">Travis</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terrance</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Vietri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwei</forename><forename type="middle">Steven</forename><surname>Wu</surname></persName>
		</author>
		<idno>CoRR, abs/2211.03128</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Confidence-ranked reconstruction of census microdata from published statistics</title>
		<author>
			<persName><forename type="first">Travis</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terrance</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Vietri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwei</forename><forename type="middle">Steven</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">2218605120</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Collecting telemetry data privately</title>
		<author>
			<persName><forename type="first">Janardhan</forename><surname>Bolin Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><surname>Yekhanin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Revealing information while preserving privacy</title>
		<author>
			<persName><forename type="first">Irit</forename><surname>Dinur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kobbi</forename><surname>Nissim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Second ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems</title>
		<editor>
			<persName><forename type="first">Frank</forename><surname>Neven</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Catriel</forename><surname>Beeri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tova</forename><surname>Milo</surname></persName>
		</editor>
		<meeting>the Twenty-Second ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003">June 9-12, 2003. 2003</date>
			<biblScope unit="page" from="202" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generalization in adaptive data analysis and holdout reuse</title>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaly</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toniann</forename><surname>Pitassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Reingold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Neil</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Daniel</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Roman</forename><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07">2015. December 7-12, 2015. 2015</date>
			<biblScope unit="page" from="2350" to="2358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Calibrating noise to sensitivity in private data analysis</title>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Mcsherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kobbi</forename><surname>Nissim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><forename type="middle">D</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Priv. Confidentiality</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="17" to="51" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Exposed! a survey of attacks on private data</title>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Steinke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Statistics and Its Application</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Robust traceability from trace amounts</title>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><forename type="middle">D</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Steinke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">R</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salil</forename><forename type="middle">P</forename><surname>Vadhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Venkatesan Guruswami, editor, IEEE 56th Annual Symposium on Foundations of Computer Science, FOCS 2015</title>
		<meeting><address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015-10-20">17-20 October, 2015. 2015</date>
			<biblScope unit="page" from="650" to="669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">RAPPOR: Randomized aggregatable privacy-preserving ordinal response</title>
		<author>
			<persName><forename type="first">Úlfar</forename><surname>Erlingsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vasyl</forename><surname>Pihur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Korolova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM CCS 2014</title>
		<editor>
			<persName><forename type="first">Gail-Joon</forename><surname>Ahn</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Moti</forename><surname>Yung</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ninghui</forename><surname>Li</surname></persName>
		</editor>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2014-11">November 2014</date>
			<biblScope unit="page" from="1054" to="1067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A unified framework for quantifying privacy risk in synthetic data</title>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Giomi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franziska</forename><surname>Boenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Wehmeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Borbála</forename><surname>Tasnádi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.10459</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Lower bound on SNARGs in the random oracle model</title>
		<author>
			<persName><forename type="first">Iftach</forename><surname>Haitner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Nukrai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eylon</forename><surname>Yogev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CRYPTO 2022, Part III</title>
		<imprint>
			<date type="published" when="2022-08">August 2022</date>
			<biblScope unit="page" from="97" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multicalibration: Calibration for the (computationally-identifiable) masses</title>
		<author>
			<persName><forename type="first">Úrsula</forename><surname>Hébert-Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">P</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Reingold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><forename type="middle">N</forename><surname>Rothblum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan</title>
		<editor>
			<persName><forename type="first">Jennifer</forename><forename type="middle">G</forename><surname>Dy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</editor>
		<meeting>the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">July 10-15, 2018. 2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="1944" to="1953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multicalibration: Calibration for the (computationally-identifiable) masses</title>
		<author>
			<persName><forename type="first">Úrsula</forename><surname>Hébert-Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">P</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Reingold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><forename type="middle">N</forename><surname>Rothblum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan</title>
		<editor>
			<persName><forename type="first">Jennifer</forename><forename type="middle">G</forename><surname>Dy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</editor>
		<meeting>the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">July 10-15, 2018. 2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="1944" to="1953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Resolving individuals contributing trace amounts of dna to highly complex mixtures using high-density snp genotyping microarrays</title>
		<author>
			<persName><forename type="first">Nils</forename><surname>Homer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Szabolcs</forename><surname>Szelinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margot</forename><surname>Redman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Duggan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Waibhav</forename><surname>Tembe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jill</forename><surname>Muehling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">V</forename><surname>Pearson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dietrich</forename><forename type="middle">A</forename><surname>Stephan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanley</forename><forename type="middle">F</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">W</forename><surname>Craig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS Genetics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2008-08">08 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Concentration of the hypergeometric distribution</title>
		<author>
			<persName><forename type="first">Don</forename><surname>Hush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clint</forename><surname>Scovel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics &amp; Probability Letters</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="127" to="132" />
			<date type="published" when="2005">11 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Reproducibility in learning</title>
		<author>
			<persName><forename type="first">Russell</forename><surname>Impagliazzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toniann</forename><surname>Pitassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Sorrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STOC &apos;22: 54th Annual ACM SIGACT Symposium on Theory of Computing</title>
		<editor>
			<persName><forename type="first">Stefano</forename><surname>Leonardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Anupam</forename><surname>Gupta</surname></persName>
		</editor>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2022">June 20 -24, 2022. 2022</date>
			<biblScope unit="page" from="818" to="831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<orgName type="collaboration">Indeed Employer Content Team</orgName>
		</author>
		<ptr target="https://www.indeed.com/hire/c/info/machine-learning-recruitment" />
		<title level="m">Machine Learning in Recruitment: Revolutionize Your Hiring Process</title>
		<imprint>
			<date type="published" when="2025-01">January 2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Auditing differentially private machine learning: How private is private sgd?</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Jagielski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">R</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alina</forename><surname>Oprea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Marc'aurelio</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Maria-Florina</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hsuan-Tien</forename><surname>Lin</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. December 6-12, 2020, virtual, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Evaluating differentially private machine learning in practice</title>
		<author>
			<persName><forename type="first">Bargav</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">28th USENIX Security Symposium, USENIX Security</title>
		<editor>
			<persName><forename type="first">Nadia</forename><surname>Heninger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Patrick</forename><surname>Traynor</surname></persName>
		</editor>
		<meeting><address><addrLine>Santa Clara, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2019-08-14">2019. August 14-16, 2019. 2019</date>
			<biblScope unit="page" from="1895" to="1912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">On the &apos;semantics&apos; of differential privacy: A bayesian formulation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Shiva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Kasiviswanathan</surname></persName>
		</author>
		<author>
			<persName><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Privacy and Confidentiality</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014-06">Jun. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Database reconstruction does compromise confidentiality</title>
		<author>
			<persName><forename type="first">Sallie</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ann</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">M</forename><surname>Abowd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">2300976120</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">How census data put trans children at risk</title>
		<author>
			<persName><forename type="first">Os</forename><surname>Keyes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abraham</forename><forename type="middle">D</forename><surname>Flaxman</surname></persName>
		</author>
		<ptr target="https://www.scientificamerican.com/article/how-census-data-put-trans-children-at-ris" />
		<imprint>
			<date type="published" when="2022-09">Sep 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multiaccuracy: Black-box post-processing for fairness in classification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amirata</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">Y</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society</title>
		<editor>
			<persName><forename type="first">Vincent</forename><surname>Conitzer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Gillian</forename><forename type="middle">K</forename><surname>Hadfield</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Shannon</forename><surname>Vallor</surname></persName>
		</editor>
		<meeting>the 2019 AAAI/ACM Conference on AI, Ethics, and Society<address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019-01-27">2019. January 27-28, 2019. 2019</date>
			<biblScope unit="page" from="247" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Robust de-anonymization of large sparse datasets</title>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaly</forename><surname>Shmatikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 IEEE Symposium on Security and Privacy (SP 2008)</title>
		<meeting><address><addrLine>Oakland, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2008-05-21">18-21 May 2008. 2008</date>
			<biblScope unit="page" from="111" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Robust de-anonymization of large sparse datasets</title>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaly</forename><surname>Shmatikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 IEEE Symposium on Security and Privacy</title>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="2008-05">May 2008</date>
			<biblScope unit="page" from="111" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Tight auditing of differentially private machine learning</title>
		<author>
			<persName><forename type="first">Milad</forename><surname>Nasr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Steinke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Borja</forename><surname>Balle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Jagielski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Terzis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">32nd USENIX Security Symposium, USENIX Security 2023</title>
		<editor>
			<persName><forename type="first">Joseph</forename><forename type="middle">A</forename><surname>Calandrino</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Carmela</forename><surname>Troncoso</surname></persName>
		</editor>
		<meeting><address><addrLine>Anaheim, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2023">August 9-11, 2023. 2023</date>
			<biblScope unit="page" from="1631" to="1648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Neelie</forename><surname>Verlinden</surname></persName>
		</author>
		<ptr target="https://www.aihr.com/blog/top-pre-employment-assessment-tools/" />
		<title level="m">Top 40+ Pre-Employment Assessment Tools</title>
		<imprint>
			<date type="published" when="2020-07-21">July 2020. 21 January 2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A survey of privacy attacks in machine learning</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>Rigaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastián</forename><surname>García</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="101" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Max-information, differential privacy, and post-selection hypothesis testing</title>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">M</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><forename type="middle">D</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Om</forename><surname>Thakkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 57th Annual Symposium on Foundations of Computer Science, FOCS 2016</title>
		<editor>
			<persName><forename type="first">Irit</forename><surname>Dinur</surname></persName>
		</editor>
		<meeting><address><addrLine>Hyatt Regency, New Brunswick, New Jersey, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="487" to="494" />
		</imprint>
	</monogr>
	<note>October</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The role of chance in the census bureau database reconstruction experiment</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Ruggles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Van Riper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Population Research and Policy Review</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="781" to="788" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Sok: Let the privacy games begin! A unified treatment of data inference privacy in machine learning</title>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Salem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Cherubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Köpf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Paverd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anshuman</forename><surname>Suri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Tople</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Zanella Béguelin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">44th IEEE Symposium on Security and Privacy</title>
		<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2023">May 21-25, 2023. 2023</date>
			<biblScope unit="volume">2023</biblScope>
			<biblScope unit="page" from="327" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Membership inference attacks against machine learning models</title>
		<author>
			<persName><forename type="first">Reza</forename><surname>Shokri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Stronati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Congzheng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaly</forename><surname>Shmatikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Symposium on Security and Privacy</title>
		<meeting><address><addrLine>San Jose, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017">May 22-26, 2017. 2017</date>
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="page" from="3" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Privacy auditing with one (1) training run</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Steinke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milad</forename><surname>Nasr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Jagielski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023</title>
		<editor>
			<persName><forename type="first">Alice</forename><surname>Oh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tristan</forename><surname>Naumann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</editor>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">December 10 -16, 2023, 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A model for protecting privacy</title>
		<author>
			<persName><forename type="first">Latanya</forename><surname>Sweeney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of uncertainty, fuzziness and knowledge-based systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">05</biblScope>
			<biblScope unit="page" from="557" to="570" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Uber releases open source project for differential privacy</title>
		<author>
			<persName><forename type="first">Katie</forename><surname>Tezapsidis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Concentration inequalities for samples without replacement</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">O</forename><surname>Tolstikhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theory of Probability &amp; Its Applications</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="462" to="481" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability and Its Applications</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ya</surname></persName>
		</author>
		<author>
			<persName><surname>Chervonenkis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Privacy risk in machine learning: Analyzing the connection to overfitting</title>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Yeom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irene</forename><surname>Giacomelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Fredrikson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">31st IEEE Computer Security Foundations Symposium, CSF 2018</title>
		<meeting><address><addrLine>Oxford, United Kingdom</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018">July 9-12, 2018. 2018</date>
			<biblScope unit="page" from="268" to="282" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
