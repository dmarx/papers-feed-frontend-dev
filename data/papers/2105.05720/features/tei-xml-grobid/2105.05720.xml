<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads</title>
				<funder ref="#_XYc8XWX">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2022-03-26">26 Mar 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Abhinav</forename><surname>Jangda</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Guodong</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Amir</forename><surname>Hossein</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nodehi</forename><surname>Sabet</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Saeed</forename><surname>Maleki</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Youshan</forename><surname>Miao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Madanlal</forename><surname>Musuvathi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Todd</forename><surname>Mytkowicz</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Olli</forename><surname>Saarikivi</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<settlement>Amherst</settlement>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Ohio State University</orgName>
								<address>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Chinese Academy of Sciences China</orgName>
								<orgName type="institution" key="instit2">University of California</orgName>
								<address>
									<country>Riverside United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution" key="instit1">Microsoft Research China</orgName>
								<orgName type="institution" key="instit2">Microsoft Research</orgName>
								<address>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-03-26">26 Mar 2022</date>
						</imprint>
					</monogr>
					<idno type="MD5">471335AA705D813FF0B6C37509BADC06</idno>
					<idno type="DOI">10.1145/3503222.3507778</idno>
					<idno type="arXiv">arXiv:2105.05720v5[cs.DC]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Distributed Machine Learning</term>
					<term>Collective Communication</term>
					<term>MPI</term>
					<term>CUDA</term>
					<term>Code Generation</term>
					<term>Compiler Optimizations .2</term>
					<term>6.2.2</term>
					<term>and 6.3.2</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent trends towards large machine learning models require both training and inference tasks to be distributed. Considering the huge cost of training these models, it is imperative to unlock optimizations in computation and communication to obtain best performance. However, the current logical separation between computation and communication kernels in machine learning frameworks misses optimization opportunities across this barrier. Breaking this abstraction can provide many optimizations to improve the performance of distributed workloads. However, manually applying these optimizations requires modifying the underlying computation and communication libraries for each scenario, which is both time consuming and error-prone.</p><p>Therefore, we present CoCoNet, which contains (i) a domain specific language to express a distributed machine learning program in the form of computation and communication operations, (ii) a set of semantics preserving transformations to optimize the program, and (iii) a compiler to generate jointly optimized communication and computation GPU kernels. Providing both computation and communication as first class constructs allows users to work on a high-level abstraction and apply powerful optimizations, such as fusion or overlapping of communication and computation. Co-CoNet enabled us to optimize data-, model-and pipeline-parallel workloads in large language models with only a few lines of code. Our experiments show that CoCoNet significantly outperforms state-of-the-art distributed machine learning implementations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>As the trend towards larger machine-learning models continue, from BERT <ref type="bibr" target="#b15">[21]</ref> with 340 million parameters, GPT-2 <ref type="bibr" target="#b35">[41]</ref> with 1.5 billion parameters, to GPT-3 <ref type="bibr" target="#b11">[17]</ref> with 175 billion parameters, model training and inferencing have to be distributed. Moreover, as the computations become resource hungry, optimizing for even the last percentage can have huge benefits in terms of time, energy, and money savings <ref type="bibr" target="#b4">[10,</ref><ref type="bibr" target="#b43">49]</ref>.</p><p>In machine learning systems today, computation and communication are treated as independent abstractions implemented in different libraries. For instance, computation libraries, such as cuBLAS <ref type="bibr">[2]</ref> and cuDNN <ref type="bibr">[3]</ref>, provide optimized tensor algebra operations, while communication libraries, like NVIDIA Collective Communications Library <ref type="bibr" target="#b2">[8]</ref>, provide high-performance implementations of collective communication, such as AllReduce. Machine learning frameworks, such as PyTorch <ref type="bibr" target="#b34">[40]</ref>, call computation and communication kernels from these libraries. Thus, in machine learning applications built atop of such frameworks, the computation and communication operations are invoked separately.</p><p>While this separation allows independent optimization of computation and communication kernels, breaking this abstraction boundary can unlock new optimizations that are otherwise not feasible. These optimizations include the following. Interface optimization eliminates a mismatch between the caller and the callee of an abstraction. For example, a machine learning model's parameters are stored in non-contiguous buffers, one buffer per layer and hence, need to copy all buffers into a single buffer before calling a collective communication like AllReduce. This copy can be avoided if the communication operation takes a list of arrays as input instead of requiring a single buffer. Fusion optimization decreases memory bandwidth usage by generating a single kernel to perform multiple communication and computation operations.</p><p>Reorder optimization moves the computation before or after the communication, thereby either distributing the computation or enabling new fusion possibilities. Finally, overlapping optimization orchestrates multiple computation and communication operations in a fine-grained manner to fully utilize both network and compute resources. We elaborate on this possibility below. In model parallelism, which is one of the distributed machine learning approaches, each layer is distributed across multiple GPUs <ref type="bibr" target="#b41">[47]</ref> and the computation for each layer consists of a matrix multiplication (MatMul) on each node followed by an AllReduce. The existing implementation of model parallelism calls individually optimized library functions for MatMul and AllReduce. However, the implementation cannot utilize both network and computation resources simultaneously because the network is idle during MatMul. We can completely utilize both network and computation resources simultaneously by overlapping the computation of MatMul with the communication of AllReduce in a fine-grained manner. The idea is to slice the output into smaller chunks and start the AllReduce communication on a chunk as soon as the MatMul kernel has computed it. To ensure minimum wait time for the AllReduce kernel, we need to schedule the MatMul kernel to compute chunks in the order the AllReduce kernel communicates them. For instance, in the ring algorithm for AllReduce, the 𝑛 th node sends the chunks to the next node in the order starting from the 𝑛 th chunk. As such, the MatMul kernel on the 𝑛 th node needs to generate the chunks in this order. Furthermore, we need to invoke only one MatMul kernel and AllReduce kernel to avoid the overhead of launching multiple kernels. Figure <ref type="figure" target="#fig_0">1</ref> shows that this fine-grained overlapping of MatMul with AllReduce can hide 80% of the execution time of MatMul and provides 1.36× speedup.</p><p>However, manually writing these optimizations for each scenario is unproductive, for example, the implementation of above overlapping optimization contains ≈2k lines of CUDA code. Thus, in this paper, we show that by carefully designing a language for expressing combinations of computation and communication the benefits of existing machine learning framework's abstraction can be maintained while simultaneously allowing a compiler to apply powerful optimizations. To this effect, we propose CoCoNet 1 for generating co-optimized custom computation and communication kernels. Figure <ref type="figure">2</ref> presents the overview of CoCoNet. CoCoNet includes a domain specific language (DSL) to express programs containing both computation and communication operations. Inspired by Halide <ref type="bibr" target="#b36">[42]</ref>, CoCoNet includes a scheduling language to specify an execution schedule of the program using a set of transformations. CoCoNet's autotuner automatically applies these transformations to optimize a program by breaking the communication and computation boundary. Hence, CoCoNet enables users to quickly generate optimized implementations for specific hardware, topology, and data sizes. CoCoNet's code generator automatically generates high-performance computation and communication kernels from a program and its schedule. We used CoCoNet to optimize data-parallel training, model-parallel inference, and pipelineparallel inference. CoCoNet generated kernels for the Adam <ref type="bibr" target="#b26">[32]</ref> and LAMB <ref type="bibr" target="#b46">[52]</ref> optimizers speeds up the training time of BERT models by upto 1.68× and can train BERT 3.9 Billion parameter models using only data parallelism, which is not possible with state of the arts. CoCoNet's kernels for model parallelism speeds up the inference in BERT 3.9 Billion and GPT-2 8.2 Billion parameter models by upto 1.51×. CoCoNet's optimized pipeline parallelism kernels speeds up inference times in GPT-2 8.2 Billion and GPT-3 175 Billion parameter models by upto 1.77×. Our implementation of CoCoNet is available at <ref type="url" target="https://github.com/parasailteam/coconet">https://github.com/parasailteam/coconet</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">THE COCONET DSL</head><p>The CoCoNet DSL extends the data representation in existing machine learning frameworks and provides constructs to express both computation and communication. The CoCoNet DSL is embedded in C++. Unifying the expression of computation and communication for distributed machine learning in the same DSL is the foundation to enable optimizations across computation and communication.</p><p>In this paper, we follow the MPI <ref type="bibr" target="#b16">[22]</ref> terminology: RANK is the process ID of a distributed process, GROUP is a set of concurrent distributed processes, and WORLD is the GROUP that includes all processes. CoCoNet supports dividing consecutive ranks into one or more process groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Tensor Layout</head><p>CoCoNet extends the concept of a tensor in machine learning frameworks from a single device data into distributed forms. Besides item datatype, like FP32 and FP16, and shape, a CoCoNet tensor also includes a layout that describes the distributed allocation of tensor's data across a set of ranks. There are three layouts for a tensor: sliced, replicated, and local. A sliced tensor is equally distributed among all nodes in a group along a specified dimension with RANK identifying the slice for that process. For example, in Figure <ref type="figure" target="#fig_1">3</ref>, which describes the Megatron-LM <ref type="bibr" target="#b41">[47]</ref> model parallel logic of Self-Attention layer in CoCoNet, w is sliced among all ranks in DSL Computation torch. coconetFunc() CodeGen PyTorch Library Communication Fused Kernels Call Schedule fuseOut = fuse(drop, out) (sumRS, sumAG) = split(sum) outAG,scOut = reorder(sumAG,fuseOut) fusedAR = fuse(sumRS, scOut,outAG) overlapOut = overlap( layer,fusedAR) MatMul Overlapped Tensor w, b; Var layer = MatMul (w, b); Var sum = AllReduce ("+", layer); Var drop = Dropout(sum); Var out = drop + r Autotuner CUDA code fuse, split, reorder, overlap... Transformations DSL Schedules Best DSL Program Best Schedule Model Parallel Self Attention Machine Learning Algorithm Figure 2: Overview of CoCoNet's workflow. First, a user expresses a machine learning algorithm in the DSL that contains both computation (MatMul) and communication (AllReduce). Then, the autotuner applies transformations to optimize the program while keeping the algorithm unchanged, such as fusing AllReduce and Dropout into FusedAllReduce and overlapping this with MatMul. Finally, CoCoNet generates custom communication and computation code, which is available through PyTorch. 1 Tensor w( FP16 , [H , H ] , Sliced (0) , WORLD , RANK ); 2 Tensor b( FP16 , [ H ] , Replicated , WORLD ); 3 Tensor in ( FP16 , [B ,S , H ] , Sliced (2) , WORLD , RANK ); 4 Tensor r( FP16 , [B ,S , H ] , Replicated , WORLD ); 5 6 // layer ( FP16 , [B ,S , H ] , Local , WORLD , RANK ) 7 Var layer = MatMul ( in , w ); 8 // sum ( FP16 , [B ,S , H ] , Replicated , WORLD ) 9 Var sum = AllReduce ( " + " , layer ); // dropout ( FP16 , [B ,S , H ] , Replicated , WORLD ) Var dropout = Dropout ( sum + b , 0.1); // out ( FP16 , [B ,S , H ] , Replicated , WORLD ) Var out = dropout + r ; Execute self_attention ({ w , in ,b , r } , { out }); WORLD in the first dimension and in is sliced in the third dimension.</p><p>A tensor can also be replicated across all ranks in a group where it has the same value on each rank and it does not have a rank identifier. For example, the bias b and the residual connection r are replicated as shown in Figure <ref type="figure" target="#fig_1">3</ref>. A local tensor has same shape on all ranks but different values on all ranks. A local tensor requires RANK to identify the values. For example, in Figure <ref type="figure" target="#fig_1">3</ref>, layer is a local tensor that represents the result of MatMul operation. A Scalar is a zero-dimensional tensor that represents a variable available on all ranks. We discuss the layout of intermediate tensors in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">CoCoNet's Operations</head><p>A CoCoNet program inherits the concept of data-flow graph (DFG) from existing machine learning frameworks with operations as vertices and data dependencies as edges. Operations in CoCoNet can be classified as (i) local computations, such as pointwise computations, matrix multiplication, and convolution, and (ii) cross rank communication operations, such as AllReduce, AllGather, and P2P Send-Recv. Table 1 shows all operations supported by CoCoNet.</p><p>A Var represents the intermediate tensor obtained after performing an operation. In the example of Figure <ref type="figure" target="#fig_1">3</ref>, the linear layer's weight (w) and the input (in) are sliced across all ranks while the bias (b) and residual (r) are replicated on all ranks. A Var's shape and distribution layout are inferred based on the operation and inputs to the operation. For example, line 7 performs a MatMul operation on the input (in) and weights (w). Since MatMul between </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Fused Collective Communication Operations</head><p>CoCoNet enables efficient computations on the output of communication by providing fused collective communication operations, such as FusedAllReduce. Consider the AllReduce in Figure <ref type="figure" target="#fig_1">3</ref> followed by a Dropout (lines 9-11). The abstraction in existing machine learning frameworks requires the output of AllReduce to be stored in memory and then re-loaded by Dropout. FusedAllReduce avoids such stores and loads by directly passing the output of communication to following computations through registers. In addition to the argument of AllReduce, a FusedAllReduce takes computations as extra arguments. Section 5.2 discusses the implementation of Fused Collective Communication Operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Overlapping Operations</head><p>CoCoNet supports overlapping multiple dependent computation and communication operations using the Overlap construct. For example, consecutive MatMul and AllReduce in Figure <ref type="figure" target="#fig_1">3</ref> (lines 7-9) can be overlapped to fully utilize both network and computation resources. Section 5.3 discusses the implementation of this construct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Custom Operations</head><p>In CoCoNet, the implementation of an operator needs to define three key properties of the operator: (i) syntax, (ii) semantics, and (iii) code generation. The syntax of an operator is defined using C++ constructors and the semantics are defined by implementing rules to describe the layout and size of the output tensor based on the input tensors. Finally, the code generation requires implementing a function to generate a call to existing libraries or generate fused GPU kernels. The implementation of syntax and semantics can be achieved in a few lines of code, however, implementing the code generation for complex operations like Matrix Multiplication and Convolution can potentially take hundreds of lines of code. Fortunately, in practice the code generation for complex operations can call an optimized implementation of existing libraries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">COCONET TRANSFORMATIONS</head><p>CoCoNet provides four semantics preserving transformations to optimize a program written in the DSL. All transformations are valid based on rules described in the sections below. CoCoNet automatically checks the validity of each transformation based on these rules and throws an error for an invalid transformation.</p><p>We call an order of transformations a schedule. A user can manually specify the schedule to optimize the program. Additionally, a user can invoke the autotuner to automatically find the best performing schedule for the given problem sizes and the underlying architecture. Below we present each transformation by applying them on the program from Figure <ref type="figure" target="#fig_1">3</ref> and show equivalent CoCoNet programs generated after applying each transformation in Figure <ref type="figure">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Splitting Communication</head><p>The split transformation breaks a collective communication operation into two communication operations. One of the two split policies supported by CoCoNet is AllReduce Split RS-AG splits an AllReduce into a ReduceScatter to produce a sliced tensor and an AllGather on the sliced tensor to return a replicated tensor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Running Example</head><p>The AllReduce in Figure <ref type="figure" target="#fig_1">3</ref> is split into rsSum that does a ReduceScatter on layer and agSum that does an All-Gather on rsSum.</p><p>( rsSum , agSum ) = split ( layer , ARSplitRSAG );</p><p>The program 1 of Figure <ref type="figure">4</ref> is the implementation of this schedule where the input to Dropout is replaced by agSum.</p><p>Validity Since an AllReduce can always be split to a ReduceScatter and an AllGather, this transformation is always valid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Reordering Operations</head><p>The reorder transformation swaps operations with an AllGather or a Broadcast in the DFG of a program. We explain this transformation for AllGather below:</p><p>AllGather Reorder reorders an AllGather with communication and computation operations. This transformation changes the layout of the operations, the input and output of operations, and the input and output of the AllGather. We explain this transformation below using the running example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Running Example</head><p>In Figure <ref type="figure">4</ref>, applying the reorder transformation changes the program 1 to 2 by reordering AllGather (agSum) with computations d and out. The reorder transformation replaces these operations in the DFG with three new operations: scD and scOut, both of which performs sliced computations, and agOut, which gathers the final result of computations.</p><p>( scD , scOut , agOut ) = reorder (d , out , agSum );</p><p>The new sliced computations perform the same operations as original computations with two differences: (i) the output of All-Gather used in the computation is replaced by the input of All-Gather, and (ii) since the input of AllGather is sliced, all tensors input to the computations are also sliced along the same dimension as the input of AllGather. After reorder, scD performs the same computation as d but scD takes rsSum and Slice(r) as input. Therefore, the layout of scOut is also sliced while the computation is same as out. Furthermore, the new AllGather is performed on the outputs of the computations, for example, after reorder, the AllGather (agOut) is performed on scOut. Figure <ref type="figure">5</ref> shows the workflow of this schedule.</p><p>Validity The reorder transformation is valid only if operations being reordered with an AllGather can be sliced along the dimension the AllGather is performed. The rules of slicing an operation depend on the type of operation and the dimensions of inputs to the operations. For example, d and out can be sliced because the computations have the same dimensions as agOut. Section 4 shows how P2P Send can be reordered with an AllGather.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Fusing Operations</head><p>Fusing multiple computations is a common technique used by existing compilers <ref type="bibr" target="#b12">[18,</ref><ref type="bibr" target="#b14">20,</ref><ref type="bibr" target="#b18">24,</ref><ref type="bibr" target="#b21">27,</ref><ref type="bibr" target="#b36">42]</ref>. CoCoNet extends this concept to fuse multiple computations and communications in a single operation and provides this capability using the fuse transformation. Below we explain two fuse policies supported by CoCoNet:</p><p>Computation Fuse fuses a series of computations in a single operation that performs all these operations.</p><p>AllReduce Fuse fuses a series of ReduceScatter, sliced computations, and AllGather operations in a single FusedAllReduce that performs all these operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Running Example</head><p>We can fuse ReduceScatter (rsSum), computations (scD and scOut), and AllGather (agOut) in program 2 of Figure <ref type="figure">4</ref> into a FusedAllReduce to obtain program 3 . fuseAR = fuse ( rsSum , scOut , agOut , ARFuse );</p><p>The comp method of fusedAR specifies the computation to be fused with FusedAllReduce and returned out is the output.</p><p>Validity Fusing multiple operations into one operation is valid only if the dependencies in the DFG after fusion are preserved. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Overlapping Operations</head><p>CoCoNet provides the overlap transformation to overlap a series of producer-consumer operations to utilize multiple resources of hardware simultaneously.</p><p>Running Example In the program 3 of Figure <ref type="figure">4</ref> we overlap the matrix multiplication (layer) with FusedAllReduce (fuseAR) to obtain program in 5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>layerWithAR = overlap ( layer , fusedAR );</head><p>Validity Overlapping multiple operations is valid only when all operations have a producer-consumer relationship between them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Automatic Exploration of Schedules</head><p>CoCoNet provides an autotuner to automatically explore the space of all schedules of a program and return the schedule that provides the best performance for the underlying architecture and input sizes. First, the autotuner fuses all pointwise computations up to a pre-defined threshold to decrease the search space and then exhaustively explores the schedule space in a breadth first search manner. Finally, the autotuner generates code for all schedules in its search space, executes all programs, and returns the schedule with minimum execution time. Table <ref type="table">3</ref> shows that the autotuner takes only a few seconds to explore the schedule space for all workloads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DISTRIBUTED WORKLOADS IN COCONET</head><p>We additionally optimized two distributed machine learning workloads using CoCoNet: (i) parameter update using Adam <ref type="bibr" target="#b26">[32]</ref>, and (ii) point-to-point communication in pipeline parallelism.  average the gradients using AllReduce and then perform computations to update the optimizer state and model parameters. Update updates the values of a tensor and reflects the new values in that position in the DFG (lines 2-3). Figure <ref type="figure" target="#fig_7">6b</ref> presents a schedule that optimizes this by distributing the computation on all ranks in a single kernel. Line 2 fuses all computations in comps. Line 3 splits the AllReduce into a ReduceScatter and an AllGather, such that computations take output of AllGather (agG) as input. Line 5 reorders AllGather with computations, such that, each rank performs computations on a slice of tensors. Line 6 slices optimizer states on all ranks to decrease memory usage and removes corresponding AllGather. Finally, line 7 fuses all operations in a single kernel. <ref type="figure" target="#fig_6">7a</ref> shows a scenario of pipeline parallelism in Megatron-LM with two transformer layers assigned to two groups each with two ranks. Rank 𝑖 in group 𝑗 is shown by ( 𝑗, 𝑖). Each group uses model parallelism within its transformer layer. Pipeline parallelism in Megatron-LM works as follows. First, all ranks in the first group reduce their input using AllReduce to get replicated output. Then  each rank performs pointwise computations over the replicated output. Finally, the first group sends the result of computations to the corresponding rank in the second group using point-to-point (P2P) sends. (Line 2 in Figure <ref type="figure" target="#fig_8">8a</ref> shows these computations but are omitted in Figure <ref type="figure" target="#fig_6">7</ref> for simplicity). Since the output of AllReduce in Figure <ref type="figure" target="#fig_6">7a</ref> is replicated, redundant data is sent using P2P. We can avoid this redundant communication by splitting the AllReduce to ReduceScatter and AllGather and reordering the P2Ps with the AllGather. Hence, the inter-group communication is reduced by the group size. We can further optimize by overlapping all communication operations. Figure <ref type="figure" target="#fig_6">7b</ref> shows that if the buffers are split into multiple tiles (T0-T2 in the figure), intra-group and inter-group communications can be overlapped.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adam in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Point-to-Point Communication in Pipeline Parallelism</head><formula xml:id="formula_0">: Fig- ure</formula><p>Figure <ref type="figure" target="#fig_8">8a</ref> is the original program, while Figure <ref type="figure" target="#fig_8">8b</ref> optimizes it by applying transformations. Line 1 fuses the P2P send with computations. Line 2 splits the AllReduce and reorders the returned AllGather with the fused P2P send at Line 4. Hence, P2P send and computations are performed on only a slice of data on the next group where the AllGather is also performed. Finally, all three new operations get overlapped in Line 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">THE COCONET CODE GENERATOR</head><p>CoCoNet generates CUDA kernels for computation and communication operations for running on a distributed system with NVIDIA GPUs. For each operation, CoCoNet either generates (i) a call to a collective communication operation, (ii) a CUDA kernel for fused computations, (iii) a CUDA kernel for fused-collective communications (Section 5.2), or (iv) CUDA kernels for overlapping of communication and computation operations (Section 5.3). Moreover, CoCoNet generates code for performing operations on multiple non-contiguous tensors (Section 5.4). After generating CUDA kernels, CoCoNet traverses the program's DFG to generate kernel calls. CoCoNet wraps generated programs as custom operators and integrates them into PyTorch, so that, applications like Megatron-LM can invoke them directly (Section 5.5). We now discuss how Co-CoNet adapts NVIDIA Collective Communication Library (NCCL), a widely-used hand-optimized high performance communication library, into a runtime to execute above CUDA kernels.</p><p>1 Var sum = AllReduce ( " + " , in ); 2 Var send = Dropout ( recv +b ,0.1) + r ; 3 Var output = Send ( send , 4 GroupRank ( GROUP +1 , RANK ));  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">NCCL Architecture</head><p>NCCL communicates data stored in the global memory of one GPU to a memory location on another GPU using CUDA kernels. NCCL's CUDA kernels perform communication by directly copying data from memory of one GPU to another GPU using GPUDirect Remote Data Memory Access [5]. NCCL's architecture defines four key properties: (i) topology, (ii) protocols, (iii) channels, and (iv) threads in a thread block of the CUDA kernel. NCCL automatically sets key configuration values for these properties based on the size of the input buffer, network architecture, and the size of WORLD. To ensure good performance, CoCoNet's code generation must carefully reconfigure these properties when extending NCCL to custom communication and computation. We now provide a high level overview of these properties.</p><p>Topology NCCL creates logical topologies, such as ring and tree, over the underlying interconnect network.</p><p>Channels NCCL maps copies of a logical topology on the underlying interconnect network. Each copy is called a channel and is assigned to one CUDA thread block.</p><p>Protocols NCCL sends data using one of the three protocols: LL, LL128, and Simple. These protocols make different tradeoffs between latency and bandwidth based on the type of inter-node synchronization used: LL has the lowest latency and Simple provides the highest bandwidth.</p><p>Number of Threads NCCL sets a fixed number of threads for each channel (and thread block). NCCL's kernels have high register usage, which limits the number of thread blocks per SM to one.</p><p>NCCL Workflow After determining the topology, protocol, number of channels, and number of threads, NCCL calls its CUDA kernel for communication. Each collective communication has three levels of tiling to fully utilize the massive parallelism of GPUs. Data is first divided into buffer tiles equal to the size of the communication buffer. Each buffer tile is further divided among all ranks and channels to obtain chunks. Each channel communicates a chunk of data at a time. The threads in channels copy elements in and out of the buffers and apply reduction operations (sum, min, max) if needed. We now present details about CoCoNet's code generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Fused Collective Communications</head><p>Fused Collective Communication extends NCCL's existing kernels to enable arbitrary pointwise computations and reductions (i.e., beyond min, max, and sum). We inspected more than 10K lines of code in NCCL to identify where computations can be added to pass intermediate values from communication to fused computations directly through registers. CoCoNet supports fusion of both pointwise operations and reductions into NCCL collectives.</p><p>Each NCCL protocol utilizes a different mechanism for communication and CoCoNet generates code for all of them. The important features of a protocol are the pack type (64-bit for LL, 128-bit for LL128 and Simple) and the load/store access pattern (shared memory for LL128, global memory for LL and Simple). CoCoNet generates template code for all element types in NCCL, and dispatches accordingly at runtime. There are some subtleties in the code generation worth discussing:</p><p>Mixed Precision When the element types of computations and the input tensors are different, CoCoNet finds the largest element type and based on the pack type of the protocol calculates how many elements can be loaded at once. All code will then be generated to operate on these many elements.</p><p>Sliced Tensor When a sliced tensor is used by a fused collective communication, all memory accesses performed need to be mapped to elements of the sliced tensor. CoCoNet generates code that produces this mapping. To perform an AllGather on sliced tensors, the inverse of this mapping is produced.</p><p>Tensor Reduction To reduce a sliced tensor, each rank reduces locally and do an AllReduce. This AllReduce reuses already established connections among ranks in the surrounding communication kernel to avoid extra startup latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Overlapping of Communication and Computation</head><p>Overlapping of computation and communication has been studied in the context of executing stencil computations in a distributed system <ref type="bibr">[14-16, 19, 20, 33, 36, 37, 44, 50, 51]</ref>. These works use nonblocking MPI operations to communicate data and simultaneously perform computations on CPUs. A similar approach for overlapping of computation and communication operations for a GPU workload would involve dividing all operations into sub-operations and ensuring dependency between sub-operations using CUDA streams. However, this approach would provide sub-optimal performance because each sub-operation is performed on only a part of data, which leads to in-efficient computation and under-utilization of communication bandwidth. Figure <ref type="figure" target="#fig_11">9</ref> shows how the fine-grained overlapping of CoCoNet addresses this issue using the example of a MatMul followed by a ring AllReduce. First, it schedules the MatMul kernel (based on CUTLASS <ref type="bibr" target="#b0">[4]</ref>) to produce chunks in the same order as the AllReduce consumes them. Here, the 𝑛 th rank sends chunks in the order starting from the 𝑛 th chunk. Hence, the MatMul kernel on 𝑛 th rank produces chunks in the same order. Second, CoCoNet invokes both kernels only once on different streams and synchronizes the AllReduce with the MatMul using an efficient fine-grained spin-lock on a memory buffer to ensure that the AllReduce wakes up as soon as the MatMul produces a chunk. Third, to provide opportunities to tune the 2-D tile sizes of the MatMul kernel, CoCoNet generates a 2-D AllReduce kernel that communicates 2-D chunks, while NCCL AllReduce only supports 1-D continuous chunk.</p><p>The example in Figure <ref type="figure" target="#fig_11">9</ref> works as follows. At T = 1 , all ranks invoke MatMul and AllReduce kernels. On rank 0, after computing chunk 0, the MatMul kernel wakes the AllReduce kernel at T = 2 , which starts communicating chunk 0. While on rank 1, at T = 2 the MatMul kernel wakes the AllReduce kernel to communicate chunk 1. Concurrently, both MatMul kernels compute their corresponding next chunk. At T = 3 , MatMul kernels finished computing chunk 1 on rank 0 and chunk 2 on rank 1 and wakes up corresponding AllReduce kernels to communicate these chunks. This process continues until all chunks are processed.</p><p>This process allows the MatMul kernel and AllReduce to be overlapped in a fine-grained manner, which reduces the startup latency of AllReduce. Since AllReduce communicates on the same chunk sizes, it achieves maximum communication bandwidth. Furthermore, the MatMul kernel achieves maximum efficiency because the kernel is invoked on the full matrix size. Figure <ref type="figure" target="#fig_0">1</ref> shows that this overlapping provides up to 1.36× better performance and hides more than 80% of the MatMul time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Operations on Scattered Tensors</head><p>In data parallelism, communication and computation occur on different layers of widely different sizes. Since machine learning frameworks allocate parameters and gradients of layers in noncontiguous buffers, gradients are copied to a large buffer to avoid launching multiple AllReduce operations.</p><p>CoCoNet supports generating a single kernel for both computation and communication operations acting on non-contiguous tensors. In this section, we show how CoCoNet modifies NCCL to generate a single communication kernel for scattered tensors. This code generation is non-trivial because NCCL has several design decisions based on the assumption that it is communicating a single contiguous buffer. For example, each thread of a NCCL channel copies only a few elements in each iteration, and hence indexing the correct tensor at a particular offset requires a linear search through all non-contiguous tensors, which can lead to significant overhead. CoCoNet solves this problem by first dividing each tensor into buckets of size at most 2 10 elements and then assigning buckets to warps in a round-robin manner. This mechanism allows each thread to quickly find the offset in a tensor, since a warp can directly index in its assigned bucket. CoCoNet pre-calculates the number of buckets that belong to the same contiguous buffer and calculates the offset for all of them once.</p><p>The process of breaking each tensor to buckets has computation overhead and extra memory requirements. Since this bucketing is done only once on the CPU and training tasks run for thousands </p><formula xml:id="formula_1">0 R 1 R 2 R 3 R 4 R 5 R 6 R 7 C 0 C 0 C 0 R 0 C 0 R 1 R 2 R 3</formula><p>(a) Workflow of overlap on rank 0. Rank 0 starts with chunk 0. </p><formula xml:id="formula_2">B 1 B 2 B 3 B 4 C 0 C 0 C 0 MatMul AllReduce R 1 1 2 B 5 B 6 B 7 B 0 C 0 C 0 C 0 C 0 AllReduce Buffer Tile 1 B 5 B 1 B 2 3 Wait W a k e</formula><formula xml:id="formula_3">0 R 2 R 3 R 4 R 5 R 6 R 7 R 0 C 0 C 0 C 0 R 1 C 0 R 2 R 3 R 4</formula><p>(b) Workflow of overlap on rank 1. Rank 1 starts with chunk 1.  of iterations on the same tensors, the computation overhead is negligible. Each bucket is represented by a pair of 64-bit tensor address and a 32-bit offset into the associated tensor, leading to 12 × 𝑁 2 10 bytes of extra memory for a tensor with 𝑁 elements. However, this memory overhead is negligible for large models. For example, for BERT model with 334M elements, the memory requirement is 0.6%. Table <ref type="table" target="#tab_3">2</ref> shows that the overhead of scattered tensors is insignificant over contiguous tensors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">PyTorch Integration</head><p>We integrated CoCoNet generated code as a function to PyTorch's torch.distributed module. This design allows us to re-use the logic for initializing NCCL and provide compatibility with models already using torch.distributed. We added wrapper functions for calling CoCoNet generated operations. These wrapper functions prepare the arguments for calling CoCoNet's operations, which includes pre-calculating pointers to the buckets for scattered tensors and clearing the spin-lock buffers for overlapping. Machine learning models can invoke CoCoNet functions using PyTorch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EVALUATION</head><p>This section evaluates the effectiveness of CoCoNet through standalone experiments and end-to-end distributed machine learning scenarios of data, model, and pipeline parallelism.</p><p>Our experiments are performed on a cluster of 16 NVIDIA DGX-2 nodes where each node contains dual 24-core Intel Xeon CPUs and 16 NVIDIA Tesla V100 (32GB) GPUs. Each GPU within a node is connected to six NVSwitches with six NVLinks (25 GBps per NVLink). Nodes are connected with 8 non-blocking EDR Infini-Band (100 Gbps) network. All nodes run Ubuntu 20.04, CUDA 11.3, cuDNN 8.2 and PyTorch 1.10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Data Parallel Training</head><p>In data parallelism, communication involves an AllReduce of gradients among all ranks. The output is used by the optimizer to update the model parameters. We evaluate CoCoNet generated code for two widely-used optimizers, Adam and LAMB. All our experiments in this section were performed on all 16 DGX-2 nodes in our cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Standalone Experiments.</head><p>We first perform standalone experiments to explore different CoCoNet schedules over a range of input tensors from 2 10 to 2 30 elements. The autotuner generates and executes implementations with different configurations, including all NCCL protocols and all channels from 2 to 64. For each tensor, the autotuner reports the best average result of 1000 iterations.</p><p>Baselines The baselines perform parameter update by first doing AllReduce over gradients and then call FusedAdam or FusedLAMB from NVIDIA Apex <ref type="bibr" target="#b1">[6]</ref>. Both FusedAdam and FusedLAMB fuses all the parameter update computations.</p><p>CoCoNet Schedules The autotuner generates following three schedules of Adam and LAMB by applying different CoCoNet transformations for each input size and reports the best schedule to the user for each input size:</p><p>(1) AR-Opt (Opt = Adam/LAMB) refer to the traditional parameter update technique, i.e., an AllReduce over gradients and then each GPU individually performs the optimizer computation. These schedules fuse all computations into a single kernel, thereby simulating the baseline implementations of FusedAdam and FusedLAMB. (2) GShard-Eq or RS-Opt-AG (Opt = Adam/LAMB) are generated from AR-Opt by first splitting the AllReduce into ReduceScatter and AllGather, and then reordering AllGather with the fused optimizer computations. Hence, these schedules distribute parameter update across all ranks, similar to GShard <ref type="bibr" target="#b28">[34]</ref> and ZeRO <ref type="bibr" target="#b37">[43]</ref>. Since GShard does not support execution on GPUs, we refer to this schedule as GShard-Eq in our results. (3) fuse(RS-Opt-AG) (Opt = Adam/LAMB) are generated by fusing all operations of RS-Opt-AG into FusedAllReduce.</p><p>2 10 2 12 2 14 2 16 2 18 2 20 2 22 2 24 2 26 2 28 2 30 # of Elements in Tensor 0.8 1.0 1.2 1.4 1.6 Speedup over AllReduce+FusedAdam UB CoCoNet(AR-A) CoCoNet(fuse(RS-A-AG)) GShard-Eq (a) Mixed-precision Adam. AR-Adam(AR-A) runs best till 2 16 . fuse(RS-A-AG) represents fuse(RS-Adam-AG) and runs best after 2 17 . 2 10 2 12 2 14 2 16 2 18 2 20 2 22 2 24 2 26 2 28 2 30 # of Elements in Tensor 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0 Speedup over AllReduce+FusedLAMB UB CoCoNet(AR-L) CoCoNet(fuse(RS-L-AG)) GShard-Eq</p><p>(b) Mixed-precision LAMB. AR-LAMB(AR-L) runs best till 2 16 . fuse(RS-L-AG) represents fuse(RS-LAMB-AG) and runs best after 2 17 .</p><p>Figure <ref type="figure" target="#fig_0">10</ref>: CoCoNet speedup on 256 GPUs. For each size, Co-CoNet chooses the best schedules. UB (upper bound) takes AllReduce-only as max achievable speedup.</p><p>Results. Figure <ref type="figure" target="#fig_0">10</ref> shows the speedup of CoCoNet schedules over the baseline for several tensor sizes. The results are shown for mixed-precision <ref type="bibr" target="#b6">[12]</ref> using Float 16, and the results for Float 32 are qualitatively similar. In these figures, UB represents the cost of AllReduce alone without doing any computation, and thus is the upper bound of possible speedups.</p><p>Even though the AR-Opt schedules emulate the baseline implementations, they are faster on smaller tensors. This is because the baseline implementations perform additional preprocessing to optimize the amount of thread-parallelism and instruction-level parallelism per invocation. While this preprocessing cost hurts smaller tensors, its benefit shows up for larger tensors where AR-Opt performs worse.</p><p>Since GShard-Eq and fuse(RS-Opt-AG) schedules distribute the optimizer computation, they perform better than the baseline for large tensors. The performance of fuse(RS-Opt-AG) shows the advantage of fusing computation and communication kernels as these schedules achieve near optimal speedups for large tensors. These Table <ref type="table">3</ref>: Lines of code of implementation of distributed machine learning workloads in CUDA and CoCoNet, and time taken by the autotuner to find the best schedule. schedules are respectively 13% and 14% faster than GShard-Eq for Adam and LAMB. For smaller tensor sizes, multiple kernel calls are required for GShard-Eq schedules significantly hurt performance. Interestingly, fuse(RS-Opt-AG) schedules are slower than AR-Opt schedules for smaller tensor sizes though they require one less kernel call because the fused kernels have a higher register usage, thereby restricting the thread-level parallelism. This demonstrates that the fusion of communication and computation is not always a good idea.</p><p>Table <ref type="table">3a</ref> shows that the lines of generated code for each schedule are significantly more than the implementation in CoCoNet and the autotuner explored all schedules in 10 seconds. In summary, CoCoNet provides performance improvements over baselines with fewer lines of code. The AR-Opt and the fuse(RS-Opt-AG) reach close to optimal performance for smaller and larger tensors respectively. This amounts to a speedup of 1.2× to 1.7× for Adam and 1.35× to 2.0× for LAMB. There is no schedule that performs best for all sizes, which demonstrates the need for the autotuner. Baselines We consider three baselines for this experiment:</p><p>• NV BERT [7] is the NVIDIA BERT Script. It copies gradients of each layer into a single buffer, calls AllReduce on the buffer, and copy back the results into original gradients. Finally, it calls either FusedAdam or FusedLAMB.</p><p>• PyTorch DDP <ref type="bibr" target="#b29">[35]</ref> stores all gradients in buckets of 25MB and overlaps the AllReduce on each gradient bucket with computations during training. After reducing all gradients it calls FusedAdam or FusedLAMB. • ZeRO <ref type="bibr" target="#b37">[43]</ref> copies gradients into a contiguous buffer and then distributes Adam's computation similar to RS-Opt-AG schedules above. The ZeRO implementation of LAMB does not support distributing optimizer state among GPUs because significant engineering efforts are required to implement reduction over distributed gradients and weights in a distributed LAMB implementation <ref type="bibr" target="#b5">[11]</ref>.</p><p>CoCoNet Integeration We integrated the scattered tensors implementation of fuse(RS-Opt-AG) schedule for both Adam and LAMB in PyTorch. These implementations provide three benefits over the baselines: (i) the scattered tensor implementation avoids copying all gradients to a single buffer and allocating this buffer, (ii) the fused schedule performs best for the tensor sizes used in BERT, and (iii) the fused schedule distributes memory of optimizer state among all GPUs.</p><p>Results Table <ref type="table" target="#tab_7">4</ref> shows the speedup provided by CoCoNet in training three BERT models over baselines. For Adam optimizer, Co-CoNet provides speedup over all baselines in training BERT 336M because CoCoNet's fused schedules perform better than other implementations. CoCoNet provides even higher speedup on larger BERT models because the fused schedules decrease memory usage by distributing Adam's state over all GPUs, which improves the efficiency of matrix multiplication GPU kernels by enabling higher batch size per iteration. For example, for BERT 1.2B CoCoNet provides 1.53× speedup over NV BERT and PyTorchDDP because of the optimized fused schedule and higher batch size enabled by Co-CoNet. On 3.9B parameter model, NV BERT and PyTorch go Out of Memory. ZeRO also supports higher batch size for BERT 1.2B and 3.9B but CoCoNet still gives speedup because of the advantages of scattered tensor implementation of fused schedules. Results for LAMB are similar. CoCoNet provides up to 1.64× speedup over all baselines. For LAMB, the speedup over ZeRO is higher than Adam because ZeRO does not support distributing LAMB optimizer state, and hence, supports smaller batch sizes as compared to CoCoNet.</p><p>In summary, CoCoNet significantly improves data-parallel training time of BERT models. CoCoNet's schedules can be automatically generated and CoCoNet's scattered tensors implementation can support a wide range of optimizers. Not only does the fusion of computation and communication lead to performance improvement over the baselines of PyTorch DDP and ZeRO, it also decreases the memory usage, which helps in increasing the batch size to train models faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Model Parallelism</head><p>Megatron-LM <ref type="bibr" target="#b41">[47]</ref> uses a model parallel approach for inference and training of transformer models, such as BERT <ref type="bibr" target="#b15">[21]</ref> and GPT-2 <ref type="bibr" target="#b35">[41]</ref>. A transformer layer contains a self-attention block and a multi-layer perceptron (MLP) block. Last few operations of a selfattention block are the same computations as shown in Figure <ref type="figure" target="#fig_1">3</ref>. An MLP block's last operations are similar to Figure <ref type="figure" target="#fig_1">3</ref>  </p><formula xml:id="formula_4">M -A R -C M M -A R -C M M -A R -C M M -A R -C G S h a r d -E q G S h a r d -E q G S h a r d -E q G S h a r d -E q C o C o N e t C o</formula><p>C o N e t C o C o N e t C o C o N e t 0.2 0.4 0.6 0.8 1.0 1.2 Times normalized to MegatronLM 1.06× 1.07× 1.05× 1.05× 1.21× 1.16× 1.15× 1.29× 1.46× 1.42× 1.54× 1.70× [B, S, H/16] x [H/16, H] [B, S, 4*H/16] x [4*H/16, H] AR MM C RS AG Overlap+Fuse 𝐻 are batch size, sequence length, and hidden size, respectively). Since model parallelism is applied within one node, all experiments in this section are performed on a single NVIDIA DGX-2 node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Standalone Experiments.</head><p>We first perform standalone experiments to evaluate different schedules generated by the autotuner. We compare following schedules for model parallel self-attention code of Figure <ref type="figure" target="#fig_1">3</ref> and similar operations of multi-layer perceptron:</p><p>(1) Megatron-LM is the baseline implementation of Figure <ref type="figure" target="#fig_1">3</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We evaluate these schedules with sizes of GPT-2 8.3 Billion parameter model (i.e., 𝑆 = 1024, 𝐻 = 3072) for 8 and 16 batch sizes.</p><p>Figure <ref type="figure" target="#fig_15">11</ref> shows the times of all schedules normalized to the time of implementation in Megatron-LM. MM-AR-C schedule provides speedup over Megatron-LM's implementation because this schedule fuses all pointwise computations in a single GPU kernel. GShard-Eq (MM-RS-C-AG) provides 1.15× to 1.29× speedup over Megatron-LM by distributing computations on all ranks. CoCoNet's best schedule (ol(MM,fuse(RS-C-AG))) provides 1.42× to 1.70× speedup over Megatron-LM and 1.21× to 1.34× over GShard-Eq because it overlaps FusedAllReduce with the matrix multiplication. Table <ref type="table">3b</ref> shows that the lines of generated CUDA code for each schedule are significantly more than the implementation in CoCoNet and the autotuner explored all schedules in 12 seconds. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Pipeline Parallelism</head><p>CoCoNet can decrease inference times in pipeline parallelism by fusing computation and communication and overlapping multiple communication operations. We evaluate CoCoNet on computations of model and pipeline parallelism in Megatron-LM for GPT-2 8.3B and GPT-3 175B parameter models. A transformer layer contains several operations but the operations of interest for this experiment are presented in Figure <ref type="figure" target="#fig_8">8a</ref>. All experiments in this section are performed on all 16 NVIDIA DGX-2 nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">Standalone Experiments.</head><p>We first perform standalone experiments to evaluate different schedules generated by the autotuner. We compare the following schedules for pipeline parallelism code of Figure <ref type="figure" target="#fig_8">8a:</ref> (1) Megatron-LM is the implementation of Figure <ref type="figure" target="#fig_8">8a</ref> in Megatron-LM and serves as a baseline for this experiment. (2) AR-C-P2P-AG is generated by slicing the output of AllReduce to perform sliced P2P sends and computations, and finally an AllGather to collect the output of computations. This schedule improves over Megatron-LM by slicing the P2P sends and fusing all the computations. (3) GShard-Eq or RS-C-P2P-AG is generated from the previous schedule by splitting the AllReduce into a ReduceScatter and an AllGather, then reordering the AllGather with P2P send and computations. Since this schedule is similar to GShard, it represents GShard-Eq in our results. (4) ol(RS,fuse(C-P2P),AG) is generated from previous schedule by fusing computations with P2P sends, and overlapping all three communication operations (Figure <ref type="figure" target="#fig_6">7b</ref>). This schedule is returned by the autotuner as the best schedule and hence, represents CoCoNet in our results.</p><p>Results Figure <ref type="figure" target="#fig_13">12</ref> shows the breakdown of each operation with one transformer layer assigned to each node.</p><p>The sequence length (𝑆 = 2048) and the hidden size (𝐻 = 12288) are of GPT-3 175B model. CoCoNet's best schedule ol(RS,fuse(C-P2P),AG) is 11.75×-12.21× 0.9 1.0 B=2 B=4 B=6 B=8 M e g a t r o n L M M e g a t r o n L M M e g a t r o n L M M e g a t r o n L</p><formula xml:id="formula_5">M A R -C -P 2 P -A G A R -C -P 2 P -A G A R -C -P 2 P -A G A R -C -P 2 P -A G G S h a r d -E q G S h a r d -E q G S h a r d -E q G S h a r d -E q C o C o N e t C o</formula><p>C o N e t C o C o N e t C o C o N e t 0.0 0.1 0.2 0.3 Times normalized to MegatronLM 4.29× 4.49× 4.23× 4.16× 7.10× 7.19× 7.14× 7.06× 12.21× 12.20× 11.85× 11.75× AR P2P C AG RS Overlap+Fuse faster than Megatron-LM's implementation, 2.84× faster than AR-C-P2P-AG, and 1.66×-1.72× faster than GShard (RS-C-P2P-AG). The speedups are because: (i) sliced P2P reduces cross node communication volume, (ii) fusing communication and computation operations improves memory bandwidth utilization, and (iii) overlapping communication using different connections (NVLink within node and InfiniBand across nodes) improves network bandwidth utilization, while other schedules utilize only one stack at a time. Table <ref type="table">3c</ref> shows that the lines of generated CUDA code for each schedule are significantly more than the implementation in CoCoNet and the autotuner explored all schedules in 11 seconds.</p><p>6.3.2 Integration with Megatron-LM. We evaluated the inference throughput of GPT-2 8.3B and GPT-3 175B parameter models by integrating CoCoNet's ol(RS,fuse(C-P2P),AG) schedule in Megatron-LM. Table <ref type="table">5</ref> shows the speedups achieved by CoCoNet. CoCoNet significantly improves inference throughput of GPT-3 and GPT-2 due to its fusion and fine-grained overlapping of multiple communication operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RELATED WORK</head><p>Distributed Machine Learning Abstractions Existing machine learning frameworks [1, <ref type="bibr" target="#b7">13,</ref><ref type="bibr" target="#b23">29,</ref><ref type="bibr" target="#b34">40,</ref><ref type="bibr" target="#b39">45]</ref> and DSLs <ref type="bibr" target="#b12">[18,</ref><ref type="bibr" target="#b14">20]</ref> provide abstractions for writing distributed machine learning workloads. Similar to CoCoNet, in these abstractions, a distributed machine learning program takes input tensors, performs operations on tensors, and returns tensors as the output. However, unlike these Table <ref type="table">5</ref>: Speedup in inference by CoCoNet's implementation of pipeline parallelism for GPT-2 and GPT-3. Layers per node were obtained by equally distributing layers on all nodes. To evenly distribute layers of GPT-2, number of layers were increased to the nearest multiple of 16, i.e., 80.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Layers per node Maximum Micro Batch Size Speedup GPT-2 8.3B 5 16 1.77× GPT-3 175B 6 2 1.33× abstractions, CoCoNet preserves the layout information for each tensor. The layout information enables CoCoNet to perform static type checking of each operation, and automatically perform transformations on the program, which is not possible with existing abstractions. Distributed Neural Network Training Several works have improved data-, model-, and pipeline-parallel techniques for both training and inference. Mesh-Tensorflow <ref type="bibr" target="#b40">[46]</ref> and GShard <ref type="bibr" target="#b28">[34]</ref> create shards of weights and model state that can be split among ranks. Horovod <ref type="bibr" target="#b39">[45]</ref> introduced the Tensor Fusion optimization that copies all gradients to a single buffer of 64MB, calls AllReduce on the buffer, and then copies the updated value to original gradients. ZeRO <ref type="bibr" target="#b37">[43]</ref> splits weights and model state among ranks and uses ReduceScatter and AllGather to distribute computation. FlexFlow <ref type="bibr" target="#b24">[30]</ref> performs operator splitting as a way to represent both data-parallelism and model-parallelism, but does not optimize computation with communication. CoCoNet provides several optimizations over these works that are possible only by breaking the abstraction: (i) scattered tensors that remove extra storage and memory copy operations, (ii) fusion communication collectives, and (iii) novel communication and computation overlapping techniques. PyTorch's DDP <ref type="bibr" target="#b29">[35]</ref> overlaps AllReduce of gradients with the forward and backward pass. However, unlike CoCoNet, PyTorch's DDP requires extra memory for overlapping, which can increase training time for very large models <ref type="bibr" target="#b3">[9]</ref> and do not support slicing of optimizer parameter update that significantly decrease memory usage. GPipe <ref type="bibr" target="#b20">[26]</ref>, Pipedream <ref type="bibr" target="#b32">[38]</ref>, and Narayanan et al. <ref type="bibr" target="#b33">[39]</ref> proposed pipeline training to improve model parallelism, by dividing the forward and backward pass into several mini-batches, which are then pipelined across devices. vPipe <ref type="bibr" target="#b47">[53]</ref> improves these works by providing higher GPU utilization. CoCoNet improves on these works by overlapping inter and intra-node communication operations. BytePS <ref type="bibr" target="#b25">[31]</ref> utilizes CPU in heterogenous clusters to improve training, which is complementary to CoCoNet.</p><p>Optimizing Stencil Computations Prior works have proposed several DSLs and optimizations for data-parallel stencil computations on CPUs, GPUs, and other accelerators. Halide <ref type="bibr" target="#b36">[42]</ref> and Fireiron <ref type="bibr" target="#b18">[24]</ref> separate the algorithm and schedule, which describes the optimizations like fusion, and loop tiling. TVM <ref type="bibr" target="#b12">[18]</ref> extends Halide for generating optimized compute kernels. Lift <ref type="bibr" target="#b19">[25,</ref><ref type="bibr" target="#b42">48]</ref> and PolyMage <ref type="bibr" target="#b21">[27]</ref> automatically optimize stencil computations for a single GPU. Distributed-Halide <ref type="bibr" target="#b14">[20]</ref> extends Halide with scheduling primitives that allow distributing parallelizable dimensions of loops. CoCoNet extends these works to reason about and compose collective communication with computation, which is crucial for distributed machine learning scenarios.</p><p>Overlapping Computation and Communication State-of-theart works on overlapping <ref type="bibr" target="#b8">[14,</ref><ref type="bibr" target="#b27">33,</ref><ref type="bibr" target="#b30">36,</ref><ref type="bibr" target="#b31">37,</ref><ref type="bibr" target="#b44">50]</ref> use either pipelined execution to overlap communication and computation or non-blocking MPI operations. Pencil <ref type="bibr" target="#b45">[51]</ref> improves upon these works by performing pipelining within a process and supports computations in multiple connected iteration spaces. Several techniques distribute tiles and automatically generate communication <ref type="bibr" target="#b10">[16,</ref><ref type="bibr" target="#b14">20,</ref><ref type="bibr" target="#b38">44]</ref>. Basu et. al. <ref type="bibr" target="#b9">[15]</ref> uses overlapped tiling in each process to remove communication between processes. Denis and Trahay <ref type="bibr" target="#b13">[19]</ref> studied the efficiency of overlap. dCUDA <ref type="bibr" target="#b17">[23]</ref> provides hardware supported overlap. These works for MPI+OpenMP are valid for CPU based stencil computations that require sends and receives to share the halo regions. However, unlike CoCoNet, these works do not support overlapping between collectives communication and complex computations like convolutions and matrix multiplications. Co-CoNet supports overlapping multiple computation and communication operations on GPUs without an accelerator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>This paper introduced CoCoNet, a language to describe distributed machine learning workloads and optimize them across computation and communication boundary. We show that CoCoNet generated code significantly improves several training and inference times of large language models. In the future we plan to automate the optimizations through smart search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">DATA AVAILABILITY STATEMENT</head><p>The artifact for this paper <ref type="bibr" target="#b22">[28]</ref> contains the source code of our implementation of CoCoNet and the benchmarking infrastructure to reproduce all the results in Section 6.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Speedup of co-optimized overlapping over sequential MatMul and AllReduce (for model parallel GPT-2 Model input matrix of [B×1024, 768] and weights of [768, 3072]) on 16 Tesla V100 GPUs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An example program in CoCoNet. (B: batch size, S: sequence length, H: hidden dimension size)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: CoCoNet programs produced by performing transformations on the program of Figure 3. Each schedule can be represented as a standalone program. Lines in red highlights changes at a step. (AR: AllReduce, AG: AllGather, and RS: ReduceScatter)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>8</head><figDesc>Data Parallel Training: Figure 6a shows the traditional implementation of parameter update using Adam. First, all ranks 1 Var avg = AllReduce ( " + " , g ); 2 Var m_ = Update (m , ( m * beta1 +(1 -beta1 )* avg )); 3 Var v_ = Update (v , ( v * beta2 +(1 -beta1 )* avg * avg )); 4 Var m1 = m_ /(1 -Pow ( beta1 , t )); 5 Var v1 = v_ /(1 -Pow ( beta2 , t )); 6 Var p_ = Update (p , ( p -lr * m1 /( Sqrt ( v1 )))); 7 Execute adam ({ g ,p ,v ,m , lr } , { p_ }); (a) Traditional implementation where tensors g is local to each rank and p,m, and v are replicated on all ranks. 1 comps = fuse ( m_ , v_ , m1 , v1 , p_ , 2 ComputationFuse ); 3 ( rsG , agG ) = split ( avg , ARSplitRSAG ); 4 ( scComp , agP , agM , agV ) = reorder ( agG , comps , 5 AGReorder ); 6 asSlice ( m ); asSlice ( v ); dead ( agM ); dead ( agV ); 7 fuseAR = fuse ( rsG , scComp , agP , AllReduceFuse ); (b) An Optimized Schedule. Tensors g is local, p is replicated on all ranks, while m and v are sliced among all ranks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Optimizing parameter update using Adam in Co-CoNet. The implementation takes four input tensors: parameters (p), gradients (g), momentum (m), and velocity (v).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><figDesc>In Megatron-LM each GPU sends redundant data.Communication operations can be overlapped at the granularity of each communication buffer tile of data in single kernel call.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Two different schedules of pipeline parallelism.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>5 6</head><label>6</label><figDesc>Execute transformer ({ in } , { output });(a) Traditional implementation. Each rank of a group sends same data to next group. 1 fuseSend = fuse ( send , output , SendFuse ); 2 ( rsSum , agSum ) = split ( sum , ARSplitRSAG ); 3 ( scSend , agOut ) = reorder ( fuseSend , agSum , 4 AGReorder ); 5 overlapOut = overlap ( rsSum , scSend , agOut ); (b) An Optimized Schedule. Each rank sends only a slice of data to ranks in next group and all operations are overlapped.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Optimizing pipeline parallelism of Megatron-LM. Input tensors: layer output in, bias b, and residual r.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Workflow of CoCoNet's overlapping of MatMul with AllReduce for a Float 16 matrix [8192, 3072] on 8 ranks (R 0 to R 7 ) with 1 channel (C 0 ) and 16 MB buffer size. Size of each 2-D chunk (B 0 to B 15 ) is [1024, 1024]. CoCoNet's AllReduce and MatMul enables overlapping without decreasing the communication bandwidth and the efficiency of computation kernels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>14 (</head><label>14</label><figDesc>MM-RS-C-AG140 13 ol(MM,fuse(RS-C-AG))≈ 2k</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>6. 1 . 2</head><label>12</label><figDesc>Integeration with BERT. We use CoCoNet generated optimizers to train three large BERT models from NVIDIA[7]. We use mixed precision training with both Adam with 8192 global batch size and LAMB with 65536 global batch size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><figDesc>with the input tensor and weight sizes as [𝐵, 𝑆, 4 × 𝐻 ] and [4 × 𝐻, 𝐻 ] (𝐵, 𝑆, and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Times of CoCoNet's schedules of model parallel self-attention and multi-layer perceptron of GPT-2 normalized to corresponding Megatron-LM's implementation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><figDesc>in Megatron-LM. (2) MM-AR-C improves the Megatron-LM implementation by fusing all pointwise computations into one kernel. (3) GShard-Eq or MM-RS-C-AG uses the same techniques as GShard. It is generated from MM-AR-C by splitting the AllReduce into a ReduceScatter and an AllGather, and reorders All-Gather with computations. This schedule represents GShard because GShard is not available for GPUs. (4) ol(MM,fuse(RS-C-AG) is generated from the previous schedule by fusing the ReduceScatter, computation, and AllGather into a FusedAllReduce and then overlapping it with the Mat-Mul. The autotuner returned this as the best schedule and hence represents CoCoNet in our results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Times of three schedules for GPT-3 175B in Co-CoNet for pipeline and model parallelism normalized to Megatron-LM's corresponding implementation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Operations supported by CoCoNet includes all common communication and computation operations.</figDesc><table><row><cell cols="2">Communication AllReduce, AllGather, ReduceScatter,</cell></row><row><cell>Operations</cell><cell>Reduce, Broadcast, P2P Send-Recv</cell></row><row><cell>Layers</cell><cell>Matrix Multiplication, Convolution</cell></row><row><cell>Activations</cell><cell>Dropout, tanh, ReLU</cell></row><row><cell>Tensor</cell><cell>+, -,  * , ÷, Norm, ReduceTensor,</cell></row><row><cell>Operations</cell><cell>Sqrt, Pow, Update</cell></row><row><cell cols="2">two sliced tensors produces a local tensor, layer represents the</cell></row><row><cell cols="2">partial result with local layout. At line 9, AllReduce computes the</cell></row><row><cell cols="2">sum of layer of all ranks and returns a replicated tensor with the</cell></row><row><cell cols="2">same values on each rank. The computations at lines 11-13 add the</cell></row><row><cell cols="2">bias, use dropout as an activation, and add the residual. At line 11,</cell></row><row><cell cols="2">the addition of sum and b follows PyTorch's broadcast semantics 2</cell></row><row><cell cols="2">by replicating b in all dimensions of sum. Thus, the shape and lay-</cell></row><row><cell cols="2">out of output of these operations are same as sum. Finally, Execute</cell></row><row><cell cols="2">defines the name, inputs, and outputs of the program.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Time to perform parameter update of all 360 tensors of BERT using Adam/LAMB on 256 Tesla V100 GPUs with scattered tensors implementation and a single contiguous tensor of size equal to the sum of size of all tensors.</figDesc><table><row><cell cols="3">Optimizer Scattered Tensor Single Tensor</cell></row><row><cell>Adam</cell><cell>33.89 ms</cell><cell>33.21 ms</cell></row><row><cell>LAMB</cell><cell>37.04 ms</cell><cell>36.71 ms</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Maximum Micro Batch Size supported by all implementations and speedup of CoCoNet over the baselines when training BERT with three parameter configurations using Adam and LAMB optimizer. OOM represents Out of Memory.</figDesc><table><row><cell cols="2">Optimizer # of Parameters</cell><cell></cell><cell cols="2">Maximum Micro Batch Size</cell><cell></cell><cell cols="3">Speedup of CoCoNet over</cell></row><row><cell></cell><cell></cell><cell cols="7">NV BERT PyTorch DDP ZeRO CoCoNet NV BERT PyTorch DDP ZeRO</cell></row><row><cell></cell><cell>336 M</cell><cell>32</cell><cell>32</cell><cell>32</cell><cell>32</cell><cell>1.18×</cell><cell>1.22×</cell><cell>1.10×</cell></row><row><cell>Adam</cell><cell>1.2 B</cell><cell>8</cell><cell>8</cell><cell>32</cell><cell>32</cell><cell>1.53×</cell><cell>1.52×</cell><cell>1.10×</cell></row><row><cell></cell><cell>3.9 B</cell><cell>OOM</cell><cell>OOM</cell><cell>8</cell><cell>8</cell><cell>-</cell><cell>-</cell><cell>1.22×</cell></row><row><cell></cell><cell>336M</cell><cell>64</cell><cell>64</cell><cell>64</cell><cell>128</cell><cell>1.20×</cell><cell>1.20×</cell><cell>1.15×</cell></row><row><cell>LAMB</cell><cell>1.2B</cell><cell>8</cell><cell>8</cell><cell>8</cell><cell>64</cell><cell>1.67×</cell><cell>1.68×</cell><cell>1.64×</cell></row><row><cell></cell><cell>3.9B</cell><cell>OOM</cell><cell>OOM</cell><cell>OOM</cell><cell>8</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="4">6.2.2 Integration with Megatron-LM. After integrating CoCoNet's</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">overlap schedule in Megatron-LM, we found that CoCoNet im-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">proved inference times of BERT 3.9B parameter model by 1.51× and</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">GPT-2 8.3B parameter model by 1.48×. Hence, overlapping matrix</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">multiplication with fused collective communication significantly</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>improves inference times.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://pytorch.org/docs/stable/notes/broadcasting.html</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>We thank the reviewers and our shepherd, <rs type="person">Tyler Sorensen</rs>, for their constructive feedback. This work was partially supported by the <rs type="funder">National Science Foundation</rs> grant <rs type="grantNumber">CCF-2052696</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_XYc8XWX">
					<idno type="grant-number">CCF-2052696</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A ARTIFACT APPENDIX A.1 Abstract</head><p>This artifact appendix describes how to reproduce results for standalone experiments in Figure <ref type="figure">10</ref>, 11, and 12 and integration results in Section 6.1.2, 6.2.2, and 6.3.2. This artifact includes the CoCoNet DSL and compiler, and CoCoNet's generated code integrated with PyTorch, Megatron-LM, and NVIDIA Bert. To reproduce the results, the experiments should be executed on a system similar to our experimental system. However, all experiments can be executed on a system with more than one NVIDIA GPUs.</p><p>A.2 Artifact Check-list (Meta-information) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Description</head><p>A.3.1 How to Access. The CoCoNet implementation and the benchmarking infrastructure used in our evaluation are publicly available as the artifact <ref type="bibr" target="#b22">[28]</ref>. This artifact contains a zip file with two directories: (i) coconet, which is the implementation of CoCoNet, and (ii) coconet-experiments, which is the benchmarking infrastructure. Latest versions of these directories are available at <ref type="url" target="https://github.com/parasailteam/coconet">https://github.com/parasailteam/coconet</ref> and <ref type="url" target="https://github.com/parasailteam/coconet-experiments">https://github.com/ parasailteam/coconet-experiments</ref>.</p><p>A.3.2 Hardware Dependencies. All benchmarks can be executed on a distributed system with two or more NVIDIA GPUs. However, our results will be reproducible on the evaluation system described in Section 6.</p><p>A.3.3 Software Dependencies. Our experiments require a system running Ubuntu 20.04 with Python 3.8+ and CUDA 11.0+. Prerequisites and their installation procedure is described in README.md files of coconet and coconet-experiments directories.</p><p>A.3.4 Data Sets. The standalone benchmarks (Figure <ref type="figure">10</ref>, 11, and 12) do not require any dataset. Datasets required for executing experiments in Section 6.1.2, 6.2.2, and 6.3.2 can be obtained by following Dataset section of README.md in coconet-experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Installation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Experiment Workflow</head><p>A.5.1 Standalone Experiments. This section describe how to execute standalone experiments of Section 6 and produce results for Figure <ref type="figure">10</ref>, Figure <ref type="figure">11</ref>, and Figure <ref type="figure">12</ref>. All of these experiments will take 1 hour combined.</p><p>(1) Install all CoCoNet prerequisites in coconet/README.md.</p><p>(2) The experiments/ directory contains all scripts for standalone experiments.</p><p>$ cd c o c o n e t / e x p e r i m e n t s /</p><p>(3) Since all our experiments uses MPI to run the executable on all GPUs, set the environment variable NPROC to the number of GPUs in the system. In our experiments, we set NPROC to 256 as follows:</p><p>$ export NPROC=256</p><p>Note: Setting NPROC to a value more than the number of GPUs in a system can lead to failed experiments. (4) If the experiments are performed on a system with multiple nodes then additional arguments to mpirun can be passed by setting the MPI_ARGS environment variable.</p><p>Data-Parallel Experiments.</p><p>(1) To execute standalone data parallel experiments execute data-parallel-exp.py. This script takes a directory to store the results as an argument. Additionally, the script requires MASTER_ADDR and MASTER_PORT to be passed as MPI_RUN_ARGS.</p><p>If the experiments are done on a single system, then it is common to set MASTER_ADDR=127.0.0.1 and MASTER_PORT=10000.</p><p>$ export MPI_ARGS= " -x ␣ MASTER_ADDR = 1 2 7 . 0 . 0 . 1 " $ export MPI_ARGS= " $MPI_ARGS ␣ -x ␣ MASTER_PORT= 1 0 0 0 0 " $ python d a t a -p a r a l l e l -exp . py r e s u l t s /</p><p>The above execution of script will execute all data parallel executables and store the results in the results directory. (2) Generate both graphs of Figure <ref type="figure">10</ref> by executing the script gen-data-parallel-graphs.py. This script takes the directory with results generated in the previous step as an argument.</p><p>$ python gen -d a t a -p a r a l l e l -g r a p h s . py r e s u l t s / Graphs are stored in two files of experiments directory: results-adam-fp16.pdf and results-lamb-fp16.pdf.</p><p>Model-Parallel Experiments.</p><p>(1) To execute standalone model-parallel experiments execute model-parallel-exp.py. Similar to the previous script, this script also takes a directory to store results as its argument.</p><p>$ python model -p a r a l l e l -exp . py r e s u l t s /</p><p>The script will execute all model parallel executables and stores the results in the results directory. (2) Generate Figure <ref type="figure">11</ref> by executing following script. This script will take above results directory as its argument.</p><p>$ python gen -model -p a r a l l e l -g r a p h s . py r e s u l t s / Graph is stored as results-model-parallel.pdf.</p><p>Pipeline-Parallel Experiments.</p><p>(1) To execute standalone pipeline-parallel experiments execute pipeline-parallel-exp.py. This script also requires a directory to store results as its command line argument.</p><p>$ python p i p e l i n e -p a r a l l e l -exp . py r e s u l t s / Above execution of the script will execute all pipeline parallel executables and store the results in results directory. (2) To generate Figure <ref type="figure">12</ref> execute the script gen-pipeline-parallel-graphs.py. This script takes the directory containing above results as its argument.</p><p>$ python gen -p i p e l i n e -p a r a l l e l -g r a p h s . py r e s u l t s /</p><p>The graph is stored in results-model-parallel.pdf.</p><p>A.5.2 Integration Experiments. In this section, we will execute the integration experiments of Section 6.1.2, 6.2.2, and 6.3.2.</p><p>Prerequisites. Install prerequisites and obtain dataset by following the steps in coconet-experiments/README.md.</p><p>Data-Parallel Training. Go to Nvidia-Bert directory and execute coconet-experiments.py.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<idno>Accessed: 2022-01-12. CUTLASS</idno>
		<ptr target="https://docs.nvidia.com/cuda/gpudirect-rdma/index.html" />
	</analytic>
	<monogr>
		<title level="j">GPUDirect RDMA</title>
		<imprint>
			<biblScope unit="page" from="2022" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
		<idno>Accessed: 2022-01-12</idno>
		<ptr target="https://github.com/NVIDIA/apex" />
	</analytic>
	<monogr>
		<title level="j">NVIDIA Apex</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<idno>Accessed: 2022-01-12</idno>
		<ptr target="https://github.com/NVIDIA/nccl" />
		<title level="m">NVIDIA Collective Communication Library</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
		<idno>Accessed: 2022-01-12</idno>
		<ptr target="https://github.com/NVIDIA/Megatron-LM/" />
	</analytic>
	<monogr>
		<title level="j">NVIDIA Megatron-LM</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">OpenAI&apos;s GPT-3 Language Model: A Technical Overview</title>
		<idno>Accessed: 2022-01-12</idno>
		<ptr target="https://lambdalabs.com/blog/demystifying-gpt-3/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<idno>Accessed: 2022-01-12</idno>
		<ptr target="https://github.com/microsoft/DeepSpeed/issues/490" />
		<title level="m">Parameter fusion in optimizer partition makes LAMB behaves differently</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<idno>Accessed: 2022-01-12</idno>
		<ptr target="https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html" />
		<title level="m">Training with Mixed Precision</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">TensorFlow: A System for Large-Scale Machine Learning</title>
		<author>
			<persName><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pete</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th USENIX Symposium on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Maximizing Communication-Computation Overlap Through Automatic Parallelization and Run-time Tuning of Non-blocking Collective Operations</title>
		<author>
			<persName><forename type="first">Youcef</forename><surname>Barigou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edgar</forename><surname>Gabriel</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10766-016-0477-7</idno>
		<ptr target="https://doi.org/10.1007/s10766-016-0477-7" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Parallel Programming</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Compiler generation and autotuning of communication-avoiding operators for geometric multigrid</title>
		<author>
			<persName><forename type="first">P</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Venkat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Straalen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Oliker</surname></persName>
		</author>
		<idno type="DOI">10.1109/HiPC.2013.6799131</idno>
		<ptr target="https://doi.org/10.1109/HiPC.2013.6799131" />
	</analytic>
	<monogr>
		<title level="m">20th Annual International Conference on High Performance Computing</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Compiling Affine Loop Nests for Distributed-Memory Parallel Architectures</title>
		<author>
			<persName><forename type="first">Uday</forename><surname>Bondhugula</surname></persName>
		</author>
		<idno type="DOI">10.1145/2503210.2503289</idno>
		<ptr target="https://doi.org/10.1145/2503210.2503289" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis</title>
		<meeting>the International Conference on High Performance Computing, Networking, Storage and Analysis</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Language Models are Few-Shot Learners</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">TVM: An Automated End-to-End Optimizing Compiler for Deep Learning</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eddie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meghan</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haichen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Krishnamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th USENIX Conference on Operating Systems Design and Implementation</title>
		<meeting>the 13th USENIX Conference on Operating Systems Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">MPI Overlap: Benchmark and Analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Trahay</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICPP.2016.37</idno>
		<ptr target="https://doi.org/10.1109/ICPP.2016.37" />
	</analytic>
	<monogr>
		<title level="m">2016 45th International Conference on Parallel Processing (ICPP)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distributed Halide</title>
		<author>
			<persName><forename type="first">Shoaib</forename><surname>Tyler Denniston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saman</forename><surname>Kamil</surname></persName>
		</author>
		<author>
			<persName><surname>Amarasinghe</surname></persName>
		</author>
		<idno type="DOI">10.1145/2851141.2851157</idno>
		<ptr target="https://doi.org/10.1145/2851141.2851157" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming</title>
		<meeting>the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1423</idno>
		<ptr target="https://doi.org/10.18653/v1/n19-1423" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">MPI: A Message-Passing Interface Standard Version</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>Message Passing Interface Forum</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">dCUDA: Hardware Supported Overlap of Computation and Communication</title>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Gysi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremia</forename><surname>Bär</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torsten</forename><surname>Hoefler</surname></persName>
		</author>
		<idno type="DOI">10.1109/SC.2016.51</idno>
		<ptr target="https://doi.org/10.1109/SC.2016.51" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<meeting>the International Conference for High Performance Computing, Networking, Storage and Analysis</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fireiron: A Data-Movement-Aware Scheduling Language for GPUs</title>
		<author>
			<persName><forename type="first">Bastian</forename><surname>Hagedorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Archibald</forename><surname>Samuel Elliott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrik</forename><surname>Barthels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rastislav</forename><surname>Bodik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinod</forename><surname>Grover</surname></persName>
		</author>
		<idno type="DOI">10.1145/3410463.3414632</idno>
		<ptr target="https://doi.org/10.1145/3410463.3414632" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Parallel Architectures and Compilation Techniques</title>
		<meeting>the ACM International Conference on Parallel Architectures and Compilation Techniques</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">High Performance Stencil Code Generation with Lift</title>
		<author>
			<persName><forename type="first">Bastian</forename><surname>Hagedorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larisa</forename><surname>Stoltzfus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Steuwer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergei</forename><surname>Gorlatch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Dubach</surname></persName>
		</author>
		<idno type="DOI">10.1145/3168824</idno>
		<ptr target="https://doi.org/10.1145/3168824" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 International Symposium on Code Generation and Optimization</title>
		<meeting>the 2018 International Symposium on Code Generation and Optimization</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism</title>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youlong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Model-Based Warp Overlapped Tiling for Image Processing Programs on GPUs</title>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Jangda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arjun</forename><surname>Guha</surname></persName>
		</author>
		<idno type="DOI">10.1145/3410463.3414649</idno>
		<ptr target="https://doi.org/10.1145/3410463.3414649" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Parallel Architectures and Compilation Techniques</title>
		<meeting>the ACM International Conference on Parallel Architectures and Compilation Techniques</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">CoCoNet: Co-Optimize Computation and Communication for Distributed Neural Networks</title>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Jangda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Hosseing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nodehi</forename><surname>Sabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saeed</forename><surname>Maleki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youshan</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madanlal</forename><surname>Musuvathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Mytkowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olli</forename><surname>Saarikivi</surname></persName>
		</author>
		<idno type="DOI">10.6084/m9.figshare.18480953.v3</idno>
		<ptr target="https://doi.org/10.6084/m9.figshare.18480953.v3" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional Architecture for Fast Feature Embedding</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Beyond Data and Model Parallelism for Deep Neural Networks</title>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Aiken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
		<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A Unified Architecture for Accelerating Distributed DNN Training in Heterogeneous GPU/CPU Clusters</title>
		<author>
			<persName><forename type="first">Yimin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yibo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bairen</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanxiong</forename><surname>Guo</surname></persName>
		</author>
		<ptr target="https://www.usenix.org/system/files/osdi20-jiang.pdf" />
	</analytic>
	<monogr>
		<title level="m">14th USENIX Symposium on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6980" />
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR 2015</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A pipelined schedule to minimize completion time for loop tiling with computation and communication overlapping</title>
		<author>
			<persName><forename type="first">N</forename><surname>Koziris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sotiropoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Goumas</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0743-7315(03)00102-3</idno>
		<ptr target="https://doi.org/10.1016/S0743-7315" />
	</analytic>
	<monogr>
		<title level="j">J. Parallel and Distrib. Comput</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">03</biblScope>
			<biblScope unit="page" from="102" to="103" />
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding</title>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">PyTorch Distributed: Experiences on Accelerating Data Parallel Training</title>
		<author>
			<persName><forename type="first">Shen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omkar</forename><surname>Salpekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Vaughan</surname></persName>
		</author>
		<idno type="DOI">10.14778/3415478.3415530</idno>
		<ptr target="https://doi.org/10.14778/3415478.3415530" />
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow</title>
		<meeting>VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note>Pritam Damania, and Soumith Chintala</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">MPI+ULT: Overlapping Communication and Computation with User-Level Threads</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Balaji</surname></persName>
		</author>
		<idno type="DOI">10.1109/HPCC-CSS-ICESS.2015.82</idno>
		<ptr target="https://doi.org/10.1109/HPCC-CSS-ICESS.2015.82" />
	</analytic>
	<monogr>
		<title level="m">IEEE 7th International Symposium on Cyberspace Safety and Security, and 2015 IEEE 12th International Conference on Embedded Software and Systems</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note>2015 IEEE 17th International Conference on High Performance Computing and Communications</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Overlapping Communication and Computation by Using a Hybrid MPI/SMPSs Approach</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Marjanović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesús</forename><surname>Labarta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Ayguadé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateo</forename><surname>Valero</surname></persName>
		</author>
		<idno type="DOI">10.1145/1810085.1810091</idno>
		<ptr target="https://doi.org/10.1145/1810085.1810091" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM International Conference on Supercomputing</title>
		<meeting>the 24th ACM International Conference on Supercomputing</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">PipeDream: Generalized Pipeline Parallelism for DNN Training</title>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Harlap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amar</forename><surname>Phanishayee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Seshadri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><forename type="middle">R</forename><surname>Devanur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">R</forename><surname>Ganger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><forename type="middle">B</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<idno type="DOI">10.1145/3341301.3359646</idno>
		<ptr target="https://doi.org/10.1145/3341301.3359646" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM Symposium on Operating Systems Principles</title>
		<meeting>the 27th ACM Symposium on Operating Systems Principles</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM</title>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Korthikanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitri</forename><surname>Vainbrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prethvi</forename><surname>Kashinkunti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julie</forename><surname>Bernauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amar</forename><surname>Phanishayee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<idno type="DOI">10.1145/3458817.3476209</idno>
		<ptr target="https://doi.org/10.1145/3458817.3476209" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<meeting>the International Conference for High Performance Computing, Networking, Storage and Analysis</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">PyTorch: An Imperative Style, High-Performance Deep Learning Library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Language Models are Unsupervised Multitask Learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Halide: A Language and Compiler for Optimizing Parallelism, Locality, and Recomputation in Image Processing Pipelines</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ragan-Kelley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frédo</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saman</forename><surname>Amarasinghe</surname></persName>
		</author>
		<idno type="DOI">10.1145/2491956.2462176</idno>
		<ptr target="https://doi.org/10.1145/2491956.2462176" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation</title>
		<meeting>the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">ZeRO: Memory Optimizations toward Training Trillion Parameter Models</title>
		<author>
			<persName><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Rasley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<meeting>the International Conference for High Performance Computing, Networking, Storage and Analysis</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Olatunji Ruwase, and Yuxiong He</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Loop Tiling in Large-Scale Stencil Codes at Run-Time with OPS</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">Z</forename><surname>Reguly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Mudalige</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Giles</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPDS.2017.2778161</idno>
		<ptr target="https://doi.org/10.1109/TPDS.2017.2778161" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Parallel and Distributed Systems</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Horovod: fast and easy distributed deep learning in TensorFlow</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Sergeev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Del</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balso</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05799[cs.LG]</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Mesh-TensorFlow: Deep Learning for Supercomputers</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youlong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Penporn</forename><surname>Koanantakool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingsheng</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blake</forename><surname>Hechtman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08053[cs.CL]</idno>
		<title level="m">Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">LIFT: A functional data-parallel IR for high-performance GPU code generation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Steuwer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Remmelg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dubach</surname></persName>
		</author>
		<idno type="DOI">10.1109/CGO.2017.7863730</idno>
		<ptr target="https://doi.org/10.1109/CGO.2017.7863730" />
	</analytic>
	<monogr>
		<title level="m">2017 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananya</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02243[cs.CL]</idno>
		<title level="m">Energy and Policy Considerations for Deep Learning in NLP</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Designing Dynamic and Adaptive MPI Point-to-Point Communication Protocols for Efficient Overlap of Computation and Communication</title>
		<author>
			<persName><forename type="first">Hari</forename><surname>Subramoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sourav</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhabaleswar</forename><forename type="middle">K</forename><surname>Panda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High Performance Computing</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Pencil: A Pipelined Algorithm for Distributed Stencils</title>
		<author>
			<persName><forename type="first">Hengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aparna</forename><surname>Chandramowlishwaran</surname></persName>
		</author>
		<idno type="DOI">10.1109/SC41405.2020.00089</idno>
		<ptr target="https://doi.org/10.1109/SC41405.2020.00089" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<meeting>the International Conference for High Performance Computing, Networking, Storage and Analysis</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Large Batch Optimization for Deep Learning: Training BERT in 76 minutes</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sashank</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Hseu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srinadh</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Syx4wnEtvH" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">vPipe: A Virtualized Acceleration System for Achieving Efficient and Scalable Pipeline Parallel DNN Training</title>
		<author>
			<persName><forename type="first">Shixiong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fanxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xusheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuxian</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heming</forename><surname>Cui</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPDS.2021.3094364</idno>
		<ptr target="https://doi.org/10.1109/TPDS.2021.3094364" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Parallel and Distributed Systems</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
