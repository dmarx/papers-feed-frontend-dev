- Decision to use ensemble models for initial training
- Choice of distillation technique for knowledge transfer
- Selection of softmax temperature for distillation
- Use of logits instead of probabilities for training the distilled model
- Decision to include both soft targets and hard targets in the training objective
- Choice of training set for the distilled model (original vs. separate transfer set)
- Implementation of regularization techniques (e.g., dropout)
- Design of specialist models for fine-grained classification
- Decision to use high-entropy soft targets for improved training efficiency
- Choice of architecture for the cumbersome model
- Decision to zero-mean logits for gradient calculation
- Selection of hyperparameters for the distilled model (e.g., layer sizes, learning rates)
- Decision to experiment with different temperatures during distillation
- Choice of evaluation metrics for model performance
- Decision to omit certain classes from the training set for the distilled model
- Use of bias adjustment for improved classification accuracy
- Decision to apply data augmentation techniques (e.g., image jittering)
- Choice of framework or library for model implementation
- Decision to conduct preliminary experiments on MNIST before ASR tasks
- Choice of acoustic model architecture for speech recognition experiments