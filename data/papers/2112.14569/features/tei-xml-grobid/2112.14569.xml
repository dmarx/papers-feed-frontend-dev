<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fine-Tuning Transformers: Vocabulary Transfer</title>
				<funder>
					<orgName type="full">Yandex Cloud platform</orgName>
				</funder>
				<funder ref="#_n596pkQ">
					<orgName type="full">HSE University</orgName>
				</funder>
				<funder ref="#_8csU6Pg">
					<orgName type="full">Analytical Center for the Government of the Russian Federation</orgName>
					<orgName type="abbreviated">ACRF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2022-12-14">December 14, 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Vladislav</forename><surname>Mosin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Igor</forename><surname>Samenko</surname></persName>
							<email>i.samenko@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Borislav</forename><surname>Kozlovskii</surname></persName>
							<email>kborislav@fb.com</email>
						</author>
						<author>
							<persName><forename type="first">Alexey</forename><surname>Tikhonov</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ivan</forename><forename type="middle">P</forename><surname>Yamshchikov</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">LEYA Lab, Yandex and Higher School of Economics</orgName>
								<address>
									<addrLine>3A Kantemirovskaya Street</addrLine>
									<settlement>St. Petersburg</settlement>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Facebook UK</orgName>
								<address>
									<addrLine>1 Rathbone Square</addrLine>
									<postCode>W1T 1HQ</postCode>
									<settlement>Fitzrovia</settlement>
									<region>London, UK</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Independent researcher</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Max Planck Institute for Mathematics in the Sciences</orgName>
								<orgName type="institution">CEMAPRE</orgName>
								<address>
									<settlement>Leipzig</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">University of Lisbon</orgName>
								<address>
									<settlement>Lisbon</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fine-Tuning Transformers: Vocabulary Transfer</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-12-14">December 14, 2022</date>
						</imprint>
					</monogr>
					<idno type="MD5">A54BD4EE7AC8EFBA606384EF2A8C5010</idno>
					<idno type="arXiv">arXiv:2112.14569v2[cs.CL]</idno>
					<note type="submission">Preprint submitted to Artificial Intelligence</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>vocabulary transfer</term>
					<term>transformers</term>
					<term>vocabulary tokenization 2010 MSC: 68T50</term>
					<term>91F20</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformers are responsible for the vast majority of recent advances in natural language processing. The majority of practical natural language processing applications of these models are typically enabled through transfer learning.</p><p>This paper studies if corpus-specific tokenization used for fine-tuning improves the resulting performance of the model. Through a series of experiments, we demonstrate that such tokenization combined with the initialization and finetuning strategy for the vocabulary tokens speeds up the transfer and boosts the performance of the fine-tuned model. We call this aspect of transfer facilitation vocabulary transfer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The transformer first introduced in <ref type="bibr" target="#b0">[1]</ref> is an architecture that consists of encoder and decoder stacks with stacked self-attention and point-wise, fully connected layers for both the encoder and decoder. The transformer gave rise to such models as GPT <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> or BERT <ref type="bibr" target="#b3">[4]</ref>. These architectures are shown to beat state of the art for various Natural Language Processing tasks. The performance of such models improves with the size, and training of such architectures from scratch requires a lot of computational power and huge datasets. These obstacles hinder the broader adoption of these architectures and limit most successful applications to transfer learning: a huge pretrained model is fine-tuned on a smaller dataset collected for a specific downstream task. This stimulates a growing interest to transfer learning procedures and gives rise to various approaches and practices aimed at raising the effectiveness of the transfer. For a review of transfer learning methodology for transformers, see <ref type="bibr" target="#b4">[5]</ref>.</p><p>Typical tokenization used for transformer pretraining includes several thousand tokens. These tokens include smaller chunks of words (down to the size of a single letter) and representations with longer tokens that directly correspond to certain words. One can speculate that the model uses shorter tokens to adopt grammatical information and deal with longer, rarely observed words. In contrast, the representations with longer tokens could be useful for semantically intensive problems. These longer, semantically charged tokens may vary significantly on various downstream tasks. Therefore, adopting new downstreamspecific tokenization might be beneficial for the performance of the resulting model. Indeed, various researchers have shown that corpus-specific tokenization could be beneficial for an NLP task. For example, <ref type="bibr" target="#b5">[6]</ref> show that optimal vocabulary is dependent on the frequencies of the words in the target corpus. <ref type="bibr" target="#b6">[7]</ref> show that tokenization on language model pretraining has a direct impact on the resulting performance, yet do not discuss the implications of this result for transfer learning. <ref type="bibr" target="#b7">[8]</ref> introduce BPE-dropout that stochastically corrupts the segmentation procedure of Byte Pair Encoding (BPE) <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9]</ref>, which leads to producing multiple segmentations within the same fixed BPE framework.</p><p>Using BPE-dropout in the pretraining is shown to improve the downstream performance. <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> and <ref type="bibr" target="#b11">[12]</ref> discuss the tokenization in the setting of crosslanguage transfer. <ref type="bibr" target="#b12">[13]</ref> demonstrate that replacing the embedding layers of the neural machine translation (NMT) model by projecting general word embeddings induced from monolingual data in a target domain onto a source-domain embedding space is beneficial for task performance. <ref type="bibr" target="#b13">[14]</ref> extension of the input and output embedding layer to account for the new vocabulary items improves NMT performance. Outside the NMT setting, transformer-based models are routinely fine-tuned on the same tokenization they inherit from the initial corpus. However, many NLP tasks are not cross-lingual. For example, pre-trained transformers are standardly used for text classification after fine-tuning on a task-specific corpus. Such an approach could be suboptimal since the vocabulary and frequencies of the words in a new corpus could differ significantly. This paper investigates whether new tokenization tailored for the fine-tuning corpus could improve the resulting performance of the model and speed up the transfer, and formalizes such problem as a new natural language processing task.</p><p>If one wants a new, corpus-specific vocabulary for fine-tuning the model, one can no longer use the embedding matrix obtained in the pretraining phase.</p><p>One has either learn it from scratch or come up with some fine-tuning procedures that could partially preserve the information acquired by the model in the pretraining phase. We suggest a new type of transfer learning task that we call vocabulary transfer. We define this task as finding optimal tokenization for a specific downstream task and developing such information preserving fine-tuning strategy. In this paper, we demonstrate that vocabulary transfer facilitates transfer learning in terms of downstream task quality and the speed of the transfer. To our knowledge, this is the first work that addresses the adoption of data-specific tokenization in the context of transfer-learning for transformers.</p><p>The contribution of this paper is threefold:</p><p>• we test several ways in which one can effectively leverage a model that was pretrained with different vocabulary tokenization;</p><p>• we conduct a series of experiments that show that adoption of new vocabulary can indeed boost the performance of the model on the downstream tasks;</p><p>• we thus build a case to broaden the scope of transfer learning to include a problem of fine-tuning the model on new vocabulary tokenization; we call the task addressing the effective transfer of information from an old vocabulary to a new one a vocabulary transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>There are different attempts to facilitate transfer learning through some enhancement or preprocessing of the new training data. For example, <ref type="bibr" target="#b14">[15]</ref> proposes to inject phrasal paraphrase relations into BERT to generate suitable representations for semantic equivalence assessment instead of increasing the model's size.</p><p>In this work, instead of enhancing the dataset with additional information, we try to find out if it is possible to organize transfer learning when new vocabulary tokenization is created for a fine-tuning dataset. We also want to see if there are specific ways to initialize and fine-tune new vocabulary-dependent embeddings to facilitate transfer learning. Particular works address the differences between the original vocabulary used in pretraining and the vocabulary of the dataset used for fine-tuning. For example, <ref type="bibr" target="#b15">[16]</ref> propose an algorithm that allows adapting general-purpose models to changing word distributions. <ref type="bibr" target="#b5">[6]</ref>  It is important to note that though several subword tokenization methods are used in pretrained language models, there are only a few attempts to see how tokenization affects performance. In particular, <ref type="bibr" target="#b17">[18]</ref> mention that WordPiece <ref type="bibr" target="#b18">[19]</ref> has a small advantage over BPE. <ref type="bibr" target="#b16">[17]</ref> introduces the unigram language model tokenization method but finds it comparable in performance to BPE. <ref type="bibr" target="#b6">[7]</ref> demonstrate that unigram-based language models such as <ref type="bibr" target="#b19">[20]</ref> consistently match or outperform BPE. Based on this result, in this paper, we mostly experiment with sentencepiece<ref type="foot" target="#foot_0">foot_0</ref> implementation of unigram language model tokenization developed in <ref type="bibr" target="#b19">[20]</ref>. To ensure that the observed effects of vocabulary transfer could be reproduced with other tokenization procedures, we also provide results with BPE and BPE-dropout tokenizations in Section 4.4.</p><p>Further, we propose a method that allows us to partially inherit some knowledge from the pretrained vocabulary and pass it to new tokenization specifically tailored for downstream data. Conducting a series of experiments with several open English datasets, we demonstrate that the proposed vocabulary initialization procedure facilitates transfer learning and boosts the performance of resulting models on downstream tasks. We report our experiments with English datasets and BERT but have no reason to believe that vocabulary transfer would not be useful for other languages and models. In particular, we saw a downstream performance boost and faster transfer with GPT-2 and BERT on proprietary datasets in a more morphologically rich language. To facilitate further research on vocabulary transfer, we publish the code used for our experiments<ref type="foot" target="#foot_1">foot_1</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Vocabulary Transfer</head><p>This paper, to our knowledge, is the first to introduce the concept of vocabulary transfer. We do it as follows:</p><p>// Randomly initialize Θ , the matrix of embeddings for the new vocabulary V Θ ← random_init() // Copying information from old embeddings Θ for the old vocabulary V for each new_token in V do if new_token exists in V // if there is the same token in the old vocabulary, take its embedding Table 1: VIPI: Vocabulary Initialization with Partial Inheritance.</p><formula xml:id="formula_0">Θ[new_token] ← Θ[new_token]</formula><p>• in this section, we methodologically describe vocabulary transfer as a general problem that is open to future research; • we propose an example of a possible solution for the problem of vocabulary transfer;</p><p>• we demonstrate that such solution improves the performance of the resulting model but do not claim that such a solution is optimal;</p><p>• we discuss the obtained results to stimulate further interest in the problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Let us introduce the following notation. The vocabulary of tokens</head><formula xml:id="formula_1">V = {t k , v k } M</formula><p>0 is obtained as a result of a pretraining phase. Here t k stands for some chunk of text that forms a token, and v k is an embedding that corresponds to it. The new vocabulary</p><formula xml:id="formula_2">V = { t k , v k } N</formula><p>0 is used for the fine-tuning. Vocabulary transfer conceptually is a process of finding such dataset-specific tokenization V , its initialization, and a fine-tuning procedure for it that would result in the superior performance of a given NLP model. In the following Sections, we showcase one possible solution to the vocabulary transfer problem and run a series of ablation studies to prove that the boost in the performance is due to a specific initialization procedure and fine-tuning of the new embedding matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Example of Vocabulary Transfer: VIPI</head><p>To transfer pretrained knowledge about old tokens to the new, corpusspecific tokens, one could have some heuristic procedure of token matching.</p><p>We propose to organize such pairing in the way described in pseudo-code in Table <ref type="table">1</ref>. The basic intuition behind VIPI is the attempt to preserve as much information from the old tokenization as possible in the most straightforward way. It simply searches for the strings of old tokens that form the new ones and initializes them with some average over the pretrained embeddings of old tokens.</p><p>First, if a token in the new vocabulary coincides with some token in the old one, we can assign its old embedding to it. In our experiments, from 50% to 60% of tokens in new, dataset-specific tokenization could be found among original tokens. At the same time, up to 30% of new tokens could be split into a partition of several tokens from the original tokenization. For every such token in a new vocabulary, we build all possible partitions that consist of the old vocabulary tokens. Out of these partitions, we choose ones with a minimal number of tokens. If there is more than one partition with the same amount of tokens, we choose the one that includes the longest token. We iterate over these partitions if there is still more than one partition in line with these rules. We initialize the corresponding token from the new vocabulary with the old vocabulary embeddings averaged over the chosen partition for every chosen partition. Then we average these initializations across all partitions.</p><p>Using this heuristic that we call VIPI or Vocabulary Initialization with Partial Inheritance, we attempt to transfer information learned on an old vocabulary to a new one. VIPI is one of the examples of vocabulary transfer that, as we show further, happens to be effective in facilitating the transfer. We hope that further research could provide better vocabulary transfer strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Data</head><p>All experiments reported in the paper were conducted on four open datasets.</p><p>We run the experiment on the BERT model that we pre-train on English Wikipedia 7 that has approximately sixteen gigabytes of raw text. We then use three different downstream datasets to experiment with fine-tuning: Quora Insincere Questions Detection Dataset 8 , Twitter sentiment analysis 9 and SemEval-19 Hyperpartisan News Detection Dataset 10 . The parameters of the datasets are shown in Table <ref type="table">2</ref>.</p><p>All datasets were tokenized with sentencepiece 11 implementation of Unigram Language Modelling <ref type="bibr" target="#b16">[17]</ref>, since according to <ref type="bibr" target="#b6">[7]</ref> it is superior to BPE. We used default out-of-the-box sentencepiece tokenization for every dataset and only varied the number of tokens. We run our experiments with eight, sixteen, and</p><p>7 <ref type="url" target="https://dumps.wikimedia.org/">https://dumps.wikimedia.org/</ref> 8 <ref type="url" target="https://www.kaggle.com/c/quora-insincere-questions-classification/data">https://www.kaggle.com/c/quora-insincere-questions-classification/data</ref> 9 <ref type="url" target="http://help.sentiment140.com/">http://help.sentiment140.com/</ref> 10 <ref type="url" target="https://pan.webis.de/semeval19/semeval19-web/">https://pan.webis.de/semeval19/semeval19-web/</ref> 11 <ref type="url" target="https://github.com/google/sentencepiece">https://github.com/google/sentencepiece</ref> Dataset Size Documents Unique words Quora 150 Mb 1 306 122 201 122 Sentiment140 300 Mb 1 600 000 1 350 544 Hyper.News 2.2 Gb 735 915 3 688 358</p><p>Table 2: Parameters of three datasets used in the experiments. Quora stands for Quora Insincere Questions Detection Dataset; Sentimen140 is a dataset for twitter sentiment analysis; Hyper.News stands for SemEval-19 Hyperpartisan News Detection Dataset. thirty-two thousand tokens. The downstream datasets were randomized and split into 50% train, 25% dev, and 25% test subsets, preserving the balance of labels in each subset. All results reported below are the results for the test subset unless noted otherwise.</p><p>First, BERT Masked Language Model (MLM) was trained on Wikipedia data. Then for each of the three downstream tasks, we carried out a series of experiments with various embedding initialization strategies that had the following structure:</p><p>• build a new vocabulary with sentencepiece;</p><p>• tokenize the downstream dataset with this new vocabulary;</p><p>• initialize embeddings matrix of the BERT model for the new vocabulary;</p><p>• for every vocabulary initialization procedure run one epoch of MLM finetuning with new tokenization on the corresponding dataset that we transfer to;</p><p>• for every vocabulary initialization train the resulting BERT model classifier on a downstream dataset.</p><p>In Section 4.2, we show that vocabulary transfer could be directly done without the intermediary MLM step, but the resulting performance is significantly better with it, so we stick to it in other reported experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Experiments</head><p>To see if vocabulary transfer is beneficial for transfer learning and if it can improve the resulting quality of the model, say, in terms of the downstream classifier accuracy, we compare VIPI with several different baselines. The first is the original tokenization that is fine-tuned on the downstream datasets: original tokenization, pretrained body, and embeddings. This is a standard transfer learning setup for BERT without any vocabulary transfer. The second baseline is BERT trained on the downstream dataset from scratch with the new tokenization: random body, new tokenization, and randomly initialized embeddings. The third is the model where the new embedding matrix is randomly initialized, but the body is taken from the pretrained model.</p><p>Table <ref type="table">3</ref> shows the resulting relative performances of all three baselines and VIPI when fine-tuning over Quora insincere questions detection, Sentiment140, and hyperpartisan news detection datasets for various numbers of tokens in the vocabulary.</p><p>First, Table <ref type="table">3</ref> clearly shows that new tokenization combined with VIPI outperforms all three other baselines across three different tokenization sizes. Second, Figure <ref type="figure" target="#fig_2">1</ref> demonstrates that new tokenization combined with VIPI speeds up the transfer consistently across all three tokenization sizes. Finally, the Quora dataset is two times smaller than Sentiment140 and seven times smaller than the dataset of hyperpartisan news. Quora dataset has approximately two hundred thousand unique words; see Table <ref type="table">2</ref>. One could advocate that the impact of vocabulary transfer becomes more significant as the number of unique words in the dataset grows. Comparing Figure <ref type="figure" target="#fig_3">2</ref> and Figure <ref type="figure" target="#fig_2">1</ref>, one could see that benefits of vocabulary transfer in terms of the transfer speed becomes more evident on a bigger dataset of hyperpartisan news. We address this reasoning in greater detail in Section 5.</p><p>In the next subsections, we run several ablation studies and investigate the impact of various aspects of vocabulary transfer on the resulting performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Number of tokens in vocabulary Original Tokenization pretrained Body and Embeddings New Tokenization Random Body Pretrained Body Random Embeddings VIPI Quora, 150 Mb. 8 000 95.64 95.82 96.01 96.03 16 000 95.91 95.92 96.03 96.11 32 000 95.70 95.97 95.83 96.11 Sentiment 140 300 Mb. 8 000 85.65 85.62 85.71 85.73 16 000 85.64 85.71 85.67 85.86 32 000 84.53 85.23 85.78 85.80 Hyperpartisan News, 2.2 Gb. 8 000 88.66 88.72 88.66 89.05 16 000 86.24 86.51 88.03 88.58 32 000 86.39 86.95 89.17 89.74</p><p>Table 3: Accuracy of downstream classifiers for Quora insincere questions detection, Sen-timen140 twitter sentiment classification, and Hyperpartisan news datasets. Usage of the corpus-specific tokenization combined with the pretrained body and VIPI improves the re-</p><p>sulting performance for all datasets across all vocabulary sizes. One could see that the effect of vocabulary transfer becomes more noticeable as the size of the downstream dataset grows.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Aspects of Vocabulary Transfer</head><p>As we have stated in Section 3 vocabulary transfer conceptually is a process of finding dataset-specific tokenization, its initialization, and fine-tuning procedure for it that would result in the superior performance of a given NLP model.</p><p>We test different popular tokenizations and investigate embeddings initialization and fine-tuning procedures further in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Embeddings Initialization</head><p>Let us look into the initialization of the BERT's embeddings matrix in detail.</p><p>First, one can claim that the tokens that coincide in both vocabularies are solely responsible for boosting the performance. To address that, we perform a series of experiments with partial initialization of the embeddings. Namely, we only initialize coinciding tokens with their pretrained values but do not complete the rest of the VIPI and initialize every other token randomly. Table <ref type="table">4</ref> shows that, though such initialization of matching old tokens in the vocabularies does improve the model's performance, it is inferior to VIPI. We also test two other initialization procedures.</p><p>The first heuristic is denoted in Table <ref type="table">4</ref> as Matching plus Shuffle. It could be regarded as an intermediary step between matching tokens and VIPI. Here we match the tokens that coincide with the tokens present in the old vocabulary.</p><p>If a new token could be split into some partition with old tokens, we randomly sample an equal amount of tokens from the old vocabulary and average across them.</p><p>The second initialization is a version of VIPI. Instead of averaging all possible partitions, we take only one partition randomly. In Table <ref type="table">4</ref>, this series of experiments is denoted as VIPI over one random partition.</p><p>Table 4 clearly illustrates several ideas. First, even such a naive idea as matching the tokens that coincide assists the transfer. Second, VIPI improves the results even further. At the same time, matching the tokens that coincide and averaging across some additional shuffled old tokens reduces the performance of the simple matching. The quality of vocabulary transfer depends on Dataset Number of tokens Random Init. Match Old Tokens VIPI over Random Partition VIPI Quora 8 000 95.64 96.08 96.09 96.03 16 000 95.91 96.09 96.10 96.11 32 000 95.70 96.13 96.09 96.11 Sentiment 140 8 000 85.65 85.67 85.77 85.72 16 000 85.64 85.70 85.82 85.86 32 000 84.53 85.82 85.77 85.80 Hyperpartisan News 8 000 88.66 88.93 89.05 89.05 16 000 88.03 88.73 88.97 88.58 32 000 89.17 89.67 88.99 89.74 Table 4: Accuracy of downstream classifiers for Quora insincere questions detection, Sen-timent140 and Hyperpartisan news datasets with corpus-specific embeddings, various embedding initialization procedures, and intermediary MLM fine-tuning. VIPI improves the resulting performance for all datasets and all vocabulary sizes.</p><p>the initialization of the embeddings. In our experiments that we report further, there were approximately 30% of tokens that exactly match the tokens from the initial tokenization. This fact in itself contributes to better-resulting performance. Indeed, the model performs better if it keeps the tokens it has already learned. This could be seen in the column "Match old tokens." However, these are frequent tokens that are relatively easy to learn. Thus, direct matching is sub-optimal. A more elaborate procedure for vocabulary transfer can help with the less frequent tokens. This becomes especially important on bigger datasets that include a large percentage of rarely used tokens (e.g., scientific text processing). The second case is discussed in more detail in Section 4.5 below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of tokens</head><p>Tokenization Classification Accuracy Classification Accuracy after MLM 8000 New, VIPI 86.52 89.05 New, Random 85.22 88.66 Old Tokens 84.97 85.68 16000 New, VIPI 88.45 90.70 New, Random 87.24 90.09 Old Tokens 86.30 87.62 32000 New, VIPI 87.34 89.74 New, Random 87.24 89.17 Old Tokens 85.84 86.39 Table 5: Accuracy of downstream classifiers for Hyperpartisan news dataset. Fine-tuning BERT with new, corpus-specific tokenization improves the performance with and without the intermediary MLM step. Pre-training BERT as an MLM before training of actual classifier improves the performance of all tokenization and initialization strategies. VIPI improves the resulting performance for both fine-tuning scenarios and all vocabulary sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Fine-tuning Embeddings</head><p>As we demonstrate above, embedding initialization plays a crucial role in vocabulary transfer, but one should also pay attention to the procedure of finetuning. In all experiments reported above, we initialized new embeddings with one of the heuristics, then fine-tuned BERT as an MLM, and only after this MLM step trained the downstream classifier. In this section, we look into the role of this intermediary step.</p><p>Table <ref type="table">5</ref> clearly shows that adopting the new, corpus-specific tokenization benefits transfer across the board. However, the intermediary MLM step that helps to fine-tune the initialized embeddings is equally essential. Figure <ref type="figure" target="#fig_4">3</ref> illustrates that the only case when the MLM step does not seem to be beneficial for the resulting performance is a situation when we train a classifier from scratch and do not use the pretrained body. Summing up, we can highlight three aspects of vocabulary transfer:</p><p>• new vocabulary tokenization -the size of such vocabulary and the procedure to obtain it are out of the scope of this paper but could be topics for further research;</p><p>• embeddings initialization procedure that could partially inherit information from the pretrained model -we propose several such initialization procedures and demonstrate that all of them outperform random initialization, and one strategy, namely, VIPI tends to be superior to others;</p><p>• fine-tuning procedure for the initialized embeddings -we illustrate this aspect demonstrating that the intermediary MLM step significantly improves the adoption of new embeddings, further investigation of this aspect also looks promising.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experimenting with the Size of Vocabulary</head><p>The sizes of vocabulary in current transformer-based language models usually do not have any quantitative justification. Thus we choose three vocabulary sizes with 16 000 tokens as a "default" size and two other vocabularies that are two times bigger and two times smaller than this arbitrary default. It is reasonable to believe that the behavior observed in these vocabularies would be, to some extent, observed in the vocabularies of another size.</p><p>It makes sense to expect that the optimal vocabulary size might depend on the task in question and the data structure available for fine-tuning. While the optimal size of the vocabulary is a valid research question, it remains out of the scope of this paper. However, it might be reasonable to find out if vocabulary transfer could not only be performed over the same sized vocabulary but be used in the setup where one wants to change the size of vocabulary used for fine-tuning. In this subsection, we briefly explore this question</p><p>In order to explore possibilities of vocabulary transfer with changing vocabulary size, the applied VIPI to Quora Insincere Questions dataset using an initial vocabulary size of 16 000 tokens and fine-tuning the classifier over the vocabulary of 8 000 and 32 000 correspondingly.</p><p>Looking at 6, one could see that vocabulary transfer could improve the resulting performance even when the size of the target vocabulary differs from the size of the initial one when compared with standard fine-tuning of the model with the fixed vocabulary size. It also stands to reason that injecting a larger vocabulary into the vocabulary of a smaller size is easier, and an additional MLM step hardly improves the resulting performance. However, an intermediary MLM step significantly improves the downstream performance on the dataset with a larger vocabulary.</p><p>Vocabulary sizes Change of accuracy Change of accuracy for classifier + VIPI for classifier after MLM + VIPI 16000 → 8000 +1.7% +1.7% 16000 → 32000 +1.3% +2.3%</p><p>Table 6: Relative change in accuracy of downstream classifiers for Quora Insincere Questions dataset in comparison with standard fine-tuning baseline with 16 000 tokens. Fine-tuning BERT with new, corpus-specific tokenization improves the performance with and without the intermediary MLM step, even if the size of the vocabulary is changed. The effect is more pronounced when vocabulary transfer is performed from a smaller vocabulary to a bigger one. Pre-training BERT as an MLM before training of actual classifier improves the performance on larger vocabulary yet has no benefits when we transfer to a vocabulary of a smaller size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Experiments with different tokenizations</head><p>Experiments in the main part of this paper were processed with the most common tokenization technique -Unigram Language Modelling <ref type="bibr" target="#b16">[17]</ref>, with results presented in Table <ref type="table">4</ref>. To prove that vocabulary transfer is not an attribute of a specific tokenization we provide results on Quora and Sentiment 140 with two more popular tokenizations: Byte-Pair Encoding (BPE) <ref type="bibr" target="#b5">[6]</ref> and BPE-Dropout <ref type="bibr" target="#b7">[8]</ref>, see Table <ref type="table">7</ref> and Table <ref type="table" target="#tab_12">8</ref> respectively. Experiments show that performance is similar to the results in the previous sections. Thus vocabulary transfer positive effect may be detected for different tokenizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Vocabulary Transfer for Scientific Text Processing</head><p>As we have already mentioned, vocabulary transfer seems to be especially relevant when the dataset used for fine-tuning has rarely used words. In this case, better transfer of the existing knowledge could be especially relevant. For example, "entomophobia" is a specific phobia characterized by an excessive or unrealistic fear of one or more classes of insects. Though it is reasonably rare in generic texts, its' proper disambiguation might be relevant for medical purposes.</p><p>Let us check if vocabulary transfer improves the performance of the model on such datasets that significantly differ from generic natural language. For example, we can use an annotated dataset of medical texts presented in <ref type="bibr" target="#b20">[21]</ref>.</p><p>Table 9 Dataset Number of tokens Random Init. Match Old Tokens VIPI over Random Partition VIPI Quora 8 000 96.02 96.05 96.02 96.06 16 000 95.86 96.08 96.10 96.06 32 000 95.68 96.03 96.08 96.10 Sentiment 140 8 000 85.56 85.56 85.65 85.71 16 000 85.56 85.77 85.73 85.67 32 000 85.72 85.67 85.70 85.91 Table 7: Accuracy of downstream classifiers for Quora insincere questions detection, Senti-ment140 and Hyperpartisan news datasets with corpus-specific embeddings, various embedding initialization procedures, and intermediary MLM fine-tuning with BPE tokenization.</p><p>summarizes the performance of BERT with a vocabulary of 16 000 tokens with and without VIPI on the task of medical diagnosis prediction. Indeed, the effect of vocabulary transfer on medical text seems even more pronounced. Thus, we expect that vocabulary transfer would be especially relevant for scientific text processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>Though fine-tuning BERT over new tokenization seems beneficial across various vocabulary sizes and speeds up the transfer for both downstream datasets, one has to address the discrepancy between the benefits of vocabulary transfer for hyperpartisan news detection or Sentiment140, and Quora insincere questions datasets. Indeed, Table <ref type="table">3</ref> shows that vocabulary transfer is much more useful for hyperpartisan news than for Quora. Let us briefly discuss this difference since it provides an illustrative example of applicability for vocabulary transfer.</p><p>Let us return to Table 9: Accuracy of downstream classifiers for MeDAL. Fine-tuning BERT with new, corpusspecific tokenization improves the performance with and without the intermediary MLM step. Pre-training BERT as an MLM before training of actual classifier improves the performance further.</p><p>cantly: the Quora dataset is seven times smaller than hyperpartisan news and two times smaller than Sentiment140. At the same time, it has twice as many documents as hyperpartisan news. This stands to reason: questions tend to be shorter than news articles. Finally, hyperpartisan news contains fifteen times more unique words. These differences might be important for dataset-specific tokenization. One would intuitively expect that such tokenization would have a higher impact on the hyperpartisan news dataset. Figure <ref type="figure" target="#fig_5">4</ref> proves this intuition.</p><p>To estimate the impact of dataset-specific tokenization on the data representation, we propose a very naive heuristic. Take a part of the downstream  <ref type="table">3</ref> and information compression in Figure <ref type="figure" target="#fig_5">4</ref> one could see that this simple heuristic allows estimating the applicability of vocabulary transfer. If corpusspecific tokenization compresses the downstream dataset more effectively than original tokenization, one could expect that vocabulary transfer would have an impact on the downstream performance.</p><p>We hope that further research into vocabulary transfer to facilitate transfer learning could establish its other properties. In particular, it is interesting to find out if the behavior shown in this paper is a consequence of specific aspects related to transformer architecture or if a general property of NLP problems leads to such behavior. Another line that we encourage is further research into how various tokenization procedures could affect downstream performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This paper studies the effect of dataset-specific tokenization on the finetuning of a transformer-based architecture. We carry out experiments that demonstrate that a dataset-specific vocabulary paired with procedures for the initialization and fine-tuning of the embeddings facilitates transfer learning. We call this phenomenon vocabulary transfer.</p><p>We discuss three aspects of vocabulary transfer: tokenization, initialization, and fine-tuning. We demonstrate that dataset-specific tokenization is helpful for different datasets and fine-tuning procedures. We propose several embedding initialization procedures and show that one of them, Vocabulary Initialization with Partial Inheritance (VIPI), is superior to the others. Through a series of experiments, we demonstrate that a fine-tuning scenario for the model with new tokenization has a comparable influence on the performance as the initialization strategy. To our knowledge, these are the first results of such type. Finally, we outline the further direction of research around vocabulary transfer.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc>made the neural machine translation (NMT) model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. [17] address the same problem of open-vocabulary translation. The authors suggest training the model with multiple subword segmentations probabilistically sampled during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><figDesc>else // Select some partition for the new token // Calculate all tokenization of new token with tokens from old vocabulary V partitions ← tokenize(new_token, V ) // Take only partitions with the smallest number of tokens partitions ← filter(len(partition) == min(map(len,partitions)),partitions) // Take only partitions with the maximal length of the longest token longest_token ← max_length(each token in partitions) partitions ← filter(max(map(len,partition)) == longest_token,partitions) if len(partitions) &gt; 0 // If any multiple partitions are left after filtering, average over them for each partition in partitions do // Initialize the new token's embedding with average of old tokens' embeddings Θ[new_token, partition]← average(Θ[old_token] for each old_token in partition) // Average resulting embedding over all available partitions Θ[new_token] ← average(Θ[new_token,partition] for each partition in partitions)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Quora insincere questions classifier epoch train loss. Fine-tuning the pretrained BERT with tokenization of various sizes. Corpus-specific tokenization combined with VIPI and a pretrained body speeds up learning. Models are ranked on the figure according to the resulting loss.</figDesc><graphic coords="12,133.77,216.88,177.90,241.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Hyperpartisan news classifier epoch train loss. Fine-tuning the pretrained BERT with tokenization of various sizes. The model with corpus-specific tokenizations and pretrained body learns faster than the model with original tokenization. Models are ranked on the figure according to the resulting loss.</figDesc><graphic coords="12,310.61,216.88,174.64,240.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Fine-tuning a BERT classifier over hyperpartisan news detection data for 32 000 tokens. The first plot corresponds to the original tokenization. The second plot shows the relative performance of a classifier trained from scratch with new tokenization. The intermediary MLM step hardly plays any role when one trains a model with new tokenization from scratch. The two last plots demonstrate that pre-training an MLM model with dataset-specific tokenization boosts the performance of the resulting classifier. The classifier without MLM is represented on every plot with a dashed line.</figDesc><graphic coords="16,193.72,124.80,223.80,242.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Relative information compression with data-specific tokenization for Quora insincere questions and hyperpartisan news datasets. The ratio between information estimators for dataset-specific tokenization and the same information estimator for original tokenization. Dataset-specific tokenization compresses hyperpartisan news datasets across various vocabulary sizes. Dataset-specific tokenization for Quora does not compress data for larger vocabularies and generally is weaker than for other datasets.</figDesc><graphic coords="21,155.68,124.80,299.88,145.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Table2and look at it carefully. There are several differences between the three downstream datasets. First of all, dataset sizes differ signifi-Accuracy of downstream classifiers for Quora insincere questions detection, Senti-</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>VIPI</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Match</cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell>Number</cell><cell>Random</cell><cell>Old</cell><cell>over</cell><cell>VIPI</cell></row><row><cell></cell><cell>of tokens</cell><cell>Init.</cell><cell></cell><cell>Random</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Tokens</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Partition</cell><cell></cell></row><row><cell></cell><cell>8 000</cell><cell>95.95</cell><cell>95.95</cell><cell>95.94</cell><cell>95.96</cell></row><row><cell>Quora</cell><cell>16 000</cell><cell>95.95</cell><cell>95.94</cell><cell>96.03</cell><cell>96.00</cell></row><row><cell></cell><cell>32 000</cell><cell>96.01</cell><cell>95.94</cell><cell>96.05</cell><cell>95.98</cell></row><row><cell></cell><cell>8 000</cell><cell>85.26</cell><cell>85.31</cell><cell>85.42</cell><cell>85.40</cell></row><row><cell>Sentiment 140</cell><cell>16 000</cell><cell>85.52</cell><cell>85.48</cell><cell>85.45</cell><cell>85.58</cell></row><row><cell></cell><cell>32 000</cell><cell>85.41</cell><cell>85.56</cell><cell>85.69</cell><cell>85.65</cell></row><row><cell cols="6">Fine-tuning Classifier Accuracy MLM + Classifier Accuracy</cell></row><row><cell>Standard</cell><cell></cell><cell>77.4</cell><cell></cell><cell></cell><cell>80.4</cell></row><row><cell>VIPI</cell><cell></cell><cell>81.9</cell><cell></cell><cell></cell><cell>83.1</cell></row></table><note><p>ment140 and Hyperpartisan news datasets with corpus-specific embeddings, various embedding initialization procedures, and intermediary MLM fine-tuning with BPE-Dropout tokenization.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_0"><p>https://github.com/google/sentencepiece</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_1"><p>https://github.com/LEYADEV/Vocabulary-Transfer</p></note>
		</body>
		<back>

			<div type="funding">
<div><p>The publication was supported by the grant for research centers in the field of AI provided by the <rs type="funder">Analytical Center for the Government of the Russian Federation (ACRF)</rs> in accordance with the agreement on the provision of subsidies (identifier of the agreement <rs type="grantNumber">000000D730321P5Q0002</rs>) and the agreement with <rs type="funder">HSE University</rs> No. <rs type="grantNumber">70-2021-00139</rs>. The article was prepared with the support of the Yandex DataSphere service from the <rs type="funder">Yandex Cloud platform</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_8csU6Pg">
					<idno type="grant-number">000000D730321P5Q0002</idno>
				</org>
				<org type="funding" xml:id="_n596pkQ">
					<idno type="grant-number">70-2021-00139</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pretraining</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://s3-us-west-2.amazonaws.com/openai-assets/researchcovers/languageunsupervised/languageunderstandingpaper.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bert</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1162</idno>
		<ptr target="https://www.aclweb.org/anthology/P16-1162" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
	<note>Long Papers), Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Byte pair encoding is suboptimal for language model pretraining</title>
		<author>
			<persName><forename type="first">K</forename><surname>Bostrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Durrett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4617" to="4624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Provilkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Emelianenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Voita</surname></persName>
		</author>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1882" to="1892" />
		</imprint>
	</monogr>
	<note>Bpe-dropout: Simple and effective subword regularization</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A new algorithm for data compression</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gage</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">C Users Journal</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="23" to="38" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lakew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gangi</surname></persName>
		</author>
		<author>
			<persName><surname>Federico</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10408</idno>
		<title level="m">Controlling the output length of neural machine translation</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">In neural machine translation, what does transfer learning transfer?</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Aji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bogoychev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7701" to="7710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-view subword regularization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="473" to="482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Vocabulary adaptation for domain adaptation in neural machine translation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sakuma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yoshinaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Toyoda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kitsuregawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4269" to="4279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The lmu munich system for the wmt 2020 unsupervised machine translation shared task</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chronopoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stojanovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Hangya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fraser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.13192</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Transfer fine-tuning: A bert case study</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Arase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP-IJCNLP</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5396" to="5407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Updating pretrained word vectors and text classifiers using monolingual alignment</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Celebi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.06241</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Subword regularization: Improving neural network translation models with multiple subword candidates</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kudo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="66" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberta</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Japanese and korean voice search</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nakajima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="5149" to="5152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="66" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Medal: Medical abbreviation disambiguation dataset for natural language understanding pretraining</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reddy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>CLINI-CALNLP</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
