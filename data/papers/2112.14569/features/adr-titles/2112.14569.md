- Decision on the choice of transformer architecture (e.g., BERT vs. GPT)
- Decision to focus on vocabulary transfer as a new transfer learning task
- Decision to use corpus-specific tokenization for fine-tuning
- Decision on the initialization strategy for new vocabulary embeddings
- Decision to implement VIPI (Vocabulary Initialization with Partial Inheritance) for embedding transfer
- Decision to conduct experiments on specific open datasets (e.g., Quora, English Wikipedia)
- Decision on the evaluation metrics for measuring model performance
- Decision to compare different tokenization methods (e.g., BPE, SentencePiece)
- Decision to explore the impact of tokenization on downstream task performance
- Decision to publish the code for reproducibility and further research
- Decision on the scope of the research (e.g., focusing on English datasets)
- Decision to include ablation studies to validate findings
- Decision to investigate the relationship between token length and model performance
- Decision to analyze the effects of vocabulary size on transfer learning efficiency
- Decision to explore the implications of vocabulary transfer for other languages and models