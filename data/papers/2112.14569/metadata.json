{
  "arxivId": "2112.14569",
  "title": "Fine-Tuning Transformers: Vocabulary Transfer",
  "authors": "Vladislav Mosin, Igor Samenko, Alexey Tikhonov, Borislav Kozlovskii, Ivan P. Yamshchikov",
  "abstract": "Transformers are responsible for the vast majority of recent advances in\nnatural language processing. The majority of practical natural language\nprocessing applications of these models are typically enabled through transfer\nlearning. This paper studies if corpus-specific tokenization used for\nfine-tuning improves the resulting performance of the model. Through a series\nof experiments, we demonstrate that such tokenization combined with the\ninitialization and fine-tuning strategy for the vocabulary tokens speeds up the\ntransfer and boosts the performance of the fine-tuned model. We call this\naspect of transfer facilitation vocabulary transfer.",
  "url": "https://arxiv.org/abs/2112.14569",
  "issue_number": 878,
  "issue_url": "https://github.com/dmarx/papers-feed/issues/878",
  "created_at": "2025-01-10T01:58:58.948611",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 28,
  "last_read": "2025-01-10T02:01:59.118581",
  "last_visited": "2025-01-10T01:59:25.704Z",
  "main_tex_file": null,
  "published_date": "2021-12-29T14:22:42Z",
  "arxiv_tags": [
    "cs.CL",
    "cs.AI",
    "cs.LG",
    "68T50, 91F20",
    "I.2.7"
  ]
}