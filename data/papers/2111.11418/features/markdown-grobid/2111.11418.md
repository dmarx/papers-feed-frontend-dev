# MetaFormer Is Actually What You Need for Vision

## Abstract

## 

MetaFormer (General Arch.) Transformer (e.g. DeiT) MLP-like model (e.g. ResMLP) PoolFormer (Ours) (a) 0 5 10 15 20 25 MACs (G) 76 78 80 82 84 ImageNet Top-1 Accuracy (%) 20M 40M 80M Model Size RSB-ResNet PoolFormer DeiT ResMLP Accuracy vs. MACs vs. Model Size (b) Figure 1. MetaFormer and performance of MetaFormer-based models on ImageNet-1K validation set. As shown in (a), we present MetaFormer as a general architecture abstracted from Transformers [56] by not specifying the token mixer. When using attention/spatial MLP as the token mixer, MetaFormer is instantiated as Transformer/MLP-like models. We argue that the competence of Transformer/MLPlike models primarily stems from the general architecture MetaFormer instead of the equipped specific token mixers. To demonstrate this, we exploit an embarrassingly simple non-parametric operator, pooling, to conduct extremely basic token mixing. Surprisingly, the resulted model PoolFormer consistently outperforms the well-tuned vision Transformer [17] baseline (DeiT [53]) and MLP-like [51] baseline (ResMLP [52]) as shown in (b), which well supports that MetaFormer is actually what we need to achieve competitive performance. RSB-ResNet in (b) means the results are from "ResNet Strikes Back" [59] where ResNet [24] are trained with improved training procedure for 300 epochs.

## Introduction

Transformers have gained much interest and success in the computer vision field [[3,](#b2)[8,](#b7)[44,](#b44)[55]](#b55). Since the seminal work of Vision Transformer (ViT) [[17]](#b16) that adapts pure Transformers to image classification tasks, many follow-up models are developed to make further improvements and achieve promising performance in various computer vision tasks [[36,](#b35)[53,](#b53)[63]](#b62).

The Transformer encoder, as shown in Figure [1](#)(a), consists of two components. One is the attention module for mixing information among tokens and we term it as token mixer. The other component contains the remaining modules, such as channel MLPs and residual connections. By regarding the attention module as a specific token mixer, we further abstract the overall Transformer into a general architecture MetaFormer where the token mixer is not specified, as shown in Figure [1(a)](#).

The success of Transformers has been long attributed to the attention-based token mixer [[56]](#b56). Based on this common belief, many variants of the attention modules [[13,](#b12)[22,](#b21)[57,](#b57)[68]](#b66) have been developed to improve the Vision Transformer. However, a very recent work [[51]](#b51) replaces the attention module completely with spatial MLPs as token mixers, and finds the derived MLP-like model can readily attain competitive performance on image classification benchmarks. The follow-up works [[26,](#b25)[35,](#b34)[52]](#b52) further improve MLP-like models by data-efficient training and specific MLP module design, gradually narrowing the performance gap to ViT and challenging the dominance of attention as token mixers.

Some recent approaches [[32,](#b31)[39,](#b38)[40,](#b39)[45]](#b45) explore other types of token mixers within the MetaFormer architecture, and have demonstrated encouraging performance. For example, [[32]](#b31) replaces attention with Fourier Transform and still achieves around 97% of the accuracy of vanilla Transformers. Taking all these results together, it seems as long as a model adopts MetaFormer as the general architecture, promising results could be attained. We thus hypothesize compared with specific token mixers, MetaFormer is more essential for the model to achieve competitive performance.

To verify this hypothesis, we apply an extremely simple non-parametric operator, pooling, as the token mixer to conduct only basic token mixing. Astonishingly, this derived model, termed PoolFormer, achieves competitive performance, and even consistently outperforms welltuned Transformer and MLP-like models, including DeiT [[53]](#b53) and ResMLP [[52]](#b52), as shown in Figure [1(b)](#). More specifically, PoolFormer-M36 achieves 82.1% top-1 accuracy on ImageNet-1K classification benchmark, surpassing well-tuned vision Transformer/MLP-like baselines DeiT-B/ResMLP-B24 by 0.3%/1.1% accuracy with 35%/52% fewer parameters and 50%/62% fewer MACs. These results demonstrate that MetaFormer, even with a naive token mixer, can still deliver promising performance. We thus argue that MetaFormer is our de facto need for vision models which is more essential to achieve competitive performance rather than specific token mixers. Note that it does not mean the token mixer is insignificant. MetaFormer still has this abstracted component. It means token mixer is not limited to a specific type, e.g. attention.

The contributions of our paper are two-fold. Firstly, we abstract Transformers into a general architecture MetaFormer, and empirically demonstrate that the success of Transformer/MLP-like models is largely attributed to the MetaFormer architecture. Specifically, by only employing a simple non-parametric operator, pooling, as an extremely weak token mixer for MetaFormer, we build a simple model named PoolFormer and find it can still achieve highly competitive performance. We hope our findings inspire more future research dedicated to improving MetaFormer instead of focusing on the token mixer modules. Secondly, we evaluate the proposed PoolFormer on multiple vision tasks including image classification [[14]](#b13), object detection [[34]](#b33), instance segmentation [[34]](#b33), and semantic segmentation [[67]](#b65), and find it achieves competitive performance compared with the SOTA models using sophistic design of token mixers. The PoolFormer can readily serve as a good starting baseline for future MetaFormer architecture design.

## Related work

Transformers are first proposed by [[56]](#b56) for translation tasks and then rapidly become popular in various NLP tasks. In language pre-training tasks, Transformers are trained on large-scale unlabeled text corpus and achieve amazing performance [[2,](#b1)[15]](#b14). Inspired by the success of Transformers in NLP, many researchers apply attention mechanism and Transformers to vision tasks [[3,](#b2)[8,](#b7)[44,](#b44)[55]](#b55). Notably, Chen et al. introduce iGPT [[6]](#b5) where the Transformer is trained to auto-regressively predict pixels on images for self-supervised learning. Dosovitskiy et al. propose Vision Transformer (ViT) with hard patch embedding as input [[17]](#b16). They show that on supervised image classification tasks, a ViT pre-trained on a large propriety dataset (JFT dataset with 300 million images) can achieve excellent performance. DeiT [[53]](#b53) and T2T-ViT [[63]](#b62) further demonstrate that the ViT pre-trained on only ImageNet-1K (∼ 1.3 million images) from scratch can achieve promising performance. A lot of works have been focusing on improving the token mixing approach of Transformers by shifted windows [[36]](#b35), relative position encoding [[61]](#b60), refining attention map [[68]](#b66), or incorporating convolution [[12,](#b11)[21,](#b20)[60]](#b59), etc. In addition to attention-like token mixers, [[51,](#b51)[52]](#b52) surprisingly find that merely adopting MLPs as token mixers can still achieve competitive performance. This discovery challenges the dominance of attention-based token mixers and triggers a heated discussion in the research community

Algorithm 1 Pooling for PoolFormer, PyTorch-like Code import torch.nn as nn class Pooling(nn.Module): def __init__(self, pool_size=3): super().__init__() self.pool = nn.AvgPool2d( pool_size, stride=1, padding=pool_size//2, count_include_pad=False, ) def forward(self, x): """ [B, C, H, W] = x.shape Subtraction of the input itself is added since the block already has a residual connection. """ return self.pool(x) -x

about which token mixer is better [[7,](#b6)[26]](#b25). However, the target of this work is neither to be engaged in this debate nor to design new complicated token mixers to achieve new state of the art. Instead, we examine a fundamental question: What is truly responsible for the success of the Transformers and their variants? Our answer is the general architecture i.e., MetaFormer. We simply utilize pooling as basic token mixers to probe the power of MetaFormer. Contemporarily, some works contribute to answering the same question. Dong et al. prove that without residual connections or MLPs, the output converges doubly exponentially to a rank one matrix [[16]](#b15). Raghu et al. [[43]](#b43) compare the feature difference between ViT and CNNs, finding that self-attention allows early gathering of global information while residual connections greatly propagate features from lower layers to higher ones. Park et al. [[42]](#b42) shows that multi-head self-attentions improve accuracy and generalization by flattening the loss landscapes. Unfortunately, they do not abstract Transformers into a general architecture and study them from the aspect of general framework.

## Method

## MetaFormer

We present the core concept "MetaFormer" for this work at first. As shown in Figure [1](#), abstracted from Transformers [[56]](#b56), MetaFormer is a general architecture where the token mixer is not specified while the other components are kept the same as Transformers. The input I is first processed by input embedding, such as patch embedding for ViTs [[17]](#b16),

$X = InputEmb(I),(1)$where X ∈ R N ×C denotes the embedding tokens with sequence length N and embedding dimension C. Then, embedding tokens are fed to repeated MetaFormer blocks, each of which includes two residual sub-blocks. Specifically, the first sub-block mainly contains a token mixer to communicate information among tokens and this sub-block can be expressed as

$Y = TokenMixer(Norm(X)) + X,(2)$where Norm(•) denotes the normalization such as Layer Normalization [[1]](#b0) or Batch Normalization [[28]](#b27); TokenMixer(•) means a module mainly working for mixing token information. It is implemented by various attention mechanism in recent vision Transformer models [[17,](#b16)[63,](#b62)[68]](#b66) or spatial MLP in MLP-like models [[51,](#b51)[52]](#b52). Note that the main function of the token mixer is to propagate token information although some token mixers can also mix channels, like attention.

The second sub-block primarily consists of a twolayered MLP with non-linear activation,

$Z = σ(Norm(Y )W 1 )W 2 + Y,(3)$where W 1 ∈ R C×rC and W 2 ∈ R rC×C are learnable parameters with MLP expansion ratio r; σ(•) is a non-linear activation function, such as GELU [[25]](#b24) or ReLU [[41]](#b41).

Instantiations of MetaFormer. MetaFormer describes a general architecture with which different models can be obtained immediately by specifying the concrete design of the token mixers. As shown in Figure [1](#)(a), if the token mixer is specified as attention or spatial MLP, MetaFormer then becomes a Transformer or MLP-like model respectively.

## PoolFormer

From the introduction of Transformers [[56]](#b56), lots of works attach much importance to the attention and focus on designing various attention-based token mixer components. In contrast, these works pay little attention to the general architecture, i.e., the MetaFormer.

In this work, we argue that this MetaFormer general architecture contributes mostly to the success of the recent Transformer and MLP-like models. To demonstrate it, we deliberately employ an embarrassingly simple operator, pooling, as the token mixer. This operator has no learnable parameters and it just makes each token averagely aggregate its nearby token features.

Since this work is targeted at vision tasks, we assume the input is in channel-first data format, i.e., T ∈ R C×H×W . The pooling operator can be expressed as

$T ′ :,i,j = 1 K × K K p,q=1 T :,i+p-K+1 2 ,i+q-K+1 2 -T :,i,j , (4$$)$where K is the pooling size. Since the MetaFormer block already has a residual connection, subtraction of the input itself is added in Equation ( [4](#formula_3)). The PyTorch-like code of the pooling is shown in Algorithm 1. As well known, self-attention and spatial MLP have computational complexity quadratic to the number of tokens to mix. Even worse, spatial MLPs bring much more parameters when handling longer sequences. As a result, self-attention and spatial MLPs usually can only process hundreds of tokens. In contrast, the pooling needs a computational complexity linear to the sequence length without any learnable parameters. Thus, we take advantage of pooling by adopting a hierarchical structure similar to traditional CNNs [[24,](#b23)[31,](#b30)[49]](#b49) and recent hierarchical Transformer variants [[36,](#b35)[57]](#b57). Figure [2](#fig_0) shows the overall framework of Pool-Former. Specifically, PoolFormer has 4 stages with H 4 × W 4 , H 8 × W 8 , H 16 × W 16 , and H 32 × W 32 tokens respectively, where H and W represent the width and height of the input image.

There are two groups of embedding size: 1) small-sized models with embedding dimensions of 64, 128, 320, and 512 responding to the four stages; 2) medium-sized models with embedding dimensions 96, 192, 384, and 768. Assuming there are L PoolFormer blocks in total, stages 1, 2, 3, and 4 will contain L/6, L/6, L/2, and L/6 PoolFormer blocks respectively. The MLP expansion ratio is set as 4. According to the above simple model scaling rule, we obtain 5 different model sizes of PoolFormer and their hyperparameters are shown in Table [1](#tab_2).

## Experiments

## Image classification

Setup. ImageNet-1K [[14]](#b13) is one of the most widely used datasets in computer vision. It contains about 1.3M training images and 50K validation images, covering common 1K classes. Our training scheme mainly follows [[53]](#b53) and [[54]](#b54). Specifically, MixUp [65], CutMix [[64]](#b63), CutOut [[66]](#b64) and RandAugment [[11]](#b10) are used for data augmentation. The models are trained for 300 epochs using AdamW opti- mizer [[29,](#b28)[37]](#b36) with weight decay 0.05 and peak learning rate lr = 1e -3 • batch size/1024 (batch size 4096 and learning rate 4e -3 are used in this paper). The number of warmup epochs is 5 and cosine schedule is used to decay the learning rate. Label Smoothing [[50]](#b50) is set as 0.1. Dropout is disabled but stochastic depth [[27]](#b26) and LayerScale [[54]](#b54)  where ResNet [[24]](#b23) is trained with improved training procedure for 300 epochs.

used to help train deep models. We modified Layer Normalization [[1]](#b0) to compute the mean and variance along token and channel dimensions compared to only channel dimension in vanilla Layer Normalization.

Modified Layer Normalization (MLN) can be implemented for channel-first data format with GroupNorm API in PyTorch by specifying the group number as 1. MLN is preferred by PoolFormer as shown in Section 4.4. See the appendix for more details on hyper-parameters. Our implementation is based on the Timm codebase [58] and the experiments are run on TPUs. Results. Table 2 shows the performance of PoolFormers on ImageNet classification. Qualitative results are shown in the appendix. Surprisingly, despite the simple pooling token mixer, PoolFormers can still achieve highly competitive performance compared with CNNs and other MetaFormer-like models. For example, PoolFormer-S24 reaches the top-1 accuracy of more than 80 while only requiring 21M parameters and 3.4G MACs. Comparatively, the wellestablished ViT baseline DeiT-S [53], attains slightly worse accuracy of 79.8 and requires 35% more MACs (4.6G). To obtain similar accuracy, MLP-like model ResMLP-S24 [52] needs 43% more parameters (30M) as well as 76% more computation (6.0G) while only 79.4 accuracy is attained. Even compared with more improved ViT and MLPlike variants [35, 57], PoolFormer still shows better performance. Specifically, the pyramid Transformer PVT-Medium obtains 81.2 top-1 accuracy with 44M parameters and 6.7G MACs while PoolFormer-S36 reaches 81.4 with 30% fewer parameters (31M) and 25% fewer MACs (5.0G) than those of PVT-Medium. Besides, compared with RSB-ResNet ("ResNet Strikes Back") [[59]](#b58) where ResNet [[24]](#b23) is trained with improved training procedure for the same 300 epochs, PoolFormer still performs better. With ∼ 22M parameters/3.7G MACs, RSB-ResNet-34 [[59]](#b58) gets 75.5 accuracy while PoolFormer-S24 can obtain 80.3. Since the local spatial modeling ability of the pooling layer is much worse than the neural convolution layer, the competitive performance of PoolFormer can only be attributed to its general architecture MetaFormer.

With the pooling operator, each token evenly aggregates the features from its nearby tokens. Thus it is an extremely basic token mixing operation. However, the experiment results show that even with this embarrassingly simple token mixer, MetaFormer still obtains highly competitive performance. Figure [3](#) clearly shows that PoolFormer surpasses other models with fewer MACs and parameters. This finding conveys that the general architecture MetaFormer is actually what we need when designing vision models. By adopting MetaFormer, it is guaranteed that the derived models would have the potential to achieve reasonable performance.

## Object detection and instance segmentation

Setup.

We evaluate PoolFormer on the challenging COCO benchmark [[34]](#b33) that includes 118K training images (train2017) and 5K validation images (val2017). The models are trained on training set and the performance on validation set is reported. PoolFormer is employed as the backbone for two standard detectors, i.e., RetinaNet [[33]](#b32) and Mask R-CNN [[23]](#b22). ImageNet pre-trained weights are utilized to initialize the backbones and Xavier [[20]](#b19) to initialize the added layers. AdamW [[29,](#b28)[37]](#b36) is adopted for training with an initial learning rate of 1×10 -4 and batch size of 16. Following [[23,](#b22)[33]](#b32), we employ 1× training schedule, i.e., training the detection models for 12 epochs. The training images are resized into shorter side of 800 pixels and longer side of no more than 1,333 pixels. For testing, the shorter side of the images is also resized to 800 pixels. The imple-mentation is based on the mmdetection [[4]](#b3) codebase and the experiments are run on 8 NVIDIA A100 GPUs.

## Results. Equipped with RetinaNet for object detection, PoolFormer-based models consistently outperform their comparable ResNet counterparts as shown in

Table 3. For instance, PoolFormer-S12 achieves 36.2 AP, largely surpassing that of ResNet-18 (31.8 AP). Similar results are observed for those models based on Mask R-CNN on object detection and instance segmentation. For example, PoolFormer-S12 largely surpasses ResNet-18 (bounding box AP 37.3 vs. 34.0, and mask AP 34.6 vs. 31.2). Overall, for COCO object detection and instance segmentation, PoolForemrs achieve competitive performance, consistently outperforming those counterparts of ResNet.

## Semantic segmentation

Setup. ADE20K [[67]](#b65), a challenging scene parsing benchmark, is selected to evaluate the models for semantic segmentation. The dataset includes 20K and 2K images in the training and validation set, respectively, covering 150 finegrained semantic categories. PoolFormers are evaluated as backbones equipped with Semantic FPN [[30]](#b29). ImageNet-1K trained checkpoints are used to initialize the backbones while Xavier [[20]](#b19) is utilized to initialize other newly added layers. Common practices [[5,](#b4)[30]](#b29) train models for 80K iterations with a batch size of 16. To speed up training, we double the batch size to 32 and decrease the iteration number to 40K. The AdamW [[29,](#b28)[37]](#b36) is employed with an initial learning rate of 2 × 10 -4 that will decay in the polynomial decay schedule with a power of 0.9. Images are resized and cropped into 512 × 512 for training and are resized to shorter side of 512 pixels for testing. Our implementation is based on the mmsegmentation [[10]](#b9) codebase and the experiments are conducted on 8 NVIDIA A100 GPUs.

Results. Table [4](#) shows the ADE20K semantic segmentation performance of different backbones using FPN [[30]](#b29). PoolFormer-based models consistently outperform the models with backbones of CNN-based ResNet [[24]](#b23) and ResNeXt [[62]](#b61) as well as Transformer-based PVT. For instance, PoolFormer-12 achieves mIoU of 37.1, 4.3 and 1.5 better than ResNet-18 and PVT-Tiny, respectively.

These results demonstrate that our PoorFormer which serves as backbone can attain competitive performance on semantic segmentation although it only utilizes pooling for basically communicating information among tokens. This further indicates the great potential of MetaFormer and supports our claim that MetaFormer is actually what we need.

## Ablation studies

The experiments of ablation studies are conducted on ImageNet-1K [[14]](#b13). Table [5](#tab_8) reports the ablation study of PoolFormer. We discuss the ablation below according to the following aspects. Table 4.

Performance of Semantic segmentation on ADE20K [[67]](#b65) validation set. All models are equipped with Semantic FPN [[30]](#b29).

Token mixers. Compared with Transformers, the main change made by PoolFormer is using simple pooling as a token mixer. We first conduct ablation for this operator by directly replacing pooling with identity mapping. Surprisingly, MetaFormer with identity mapping can still achieve 74.3% top-1 accuracy, supporting the claim that MetaFormer is actually what we need to guarantee reasonable performance.

Then the pooling is replaced with global random matrix W R ∈ R N×N for each block. The matrix is initialized with random values from a uniform distribution on the interval [0, 1), and then Softmax is utilized to normalize each row. After random initialization, the matrix parameters are frozen and it conducts token mixing by X ′ = W R X where X ∈ R N×C are the input token features with the token length of N and channel dimension of C. The token mixer of random matrix introduces extra 21M frozen parameters for the S12 model since the token lengths are extremely large at the first stage. Even with such random token mixing method, the model can still achieve reasonable performance of 75.8% accuracy, 1.5% higher than that of identity mapping. It shows that MetaFormer can still work well even with random token mixing, not to say with other well-designed token mixers.

Further, pooling is replaced with Depthwise Convolution [[9,](#b8)[38]](#b37) that has learnable parameters for spatial modeling. Not surprisingly, the derived model still achieve highly competitive performance with top-1 accuracy of 78.1%, 0.9% higher than PoolFormer-S12 due to its better local spatial modeling ability. Until now, we have specified multiple token mixers in Metaformer, and all resulted models keep promising results, well supporting the claim that MetaFormer is the key to guaranteeing models' competitiveness. Due to the simplicity of pooling, it is mainly utilized as a tool to demonstrate MetaFormer.

We test the effects of pooling size on PoolFormer. We observe similar performance when pooling sizes are 3, 5, and 7. However, when the pooling size increases to 9, there is an obvious performance drop of 0.5%. Thus, we adopt the default pooing size of 3 for PoolFormer.

Normalization. We modify Layer Normalization [[1]](#b0) into Modified Layer Normalization (MLN) that computes the mean and variance along token and channel dimensions compared with only channel dimension in vanilla Layer Normalization. The shape of learnable affine parameters of MLN keeps the same as that of Layer Normalization, i.e., R C . MLN can be implemented with GroupNorm API in Py-Torch by setting the group number as 1. See the appendix for details. We find PoolFormer prefers MLN with 0.7% or 0.8% higher than Layer Normalization or Batch Normalization. Thus, MLN is set as default for PoolFormer. When removing normalization, the model can not be trained to converge well, and its performance dramatically drops to only 46.1%.

Activation. We change GELU [[25]](#b24) to ReLU [[41]](#b41) or SiLU [[18]](#b17). When ReLU is adopted for activation, an obvious performance drop of 0.8% is observed. For SiLU, its performance is almost the same as that of GELU. Thus, we still adopt GELU as default activation.

Other components. Besides token mixer and normalization discussed above, residual connection [[24]](#b23) and channel MLP [[46,](#b46)[47]](#b47)   the model cannot converge and only achieves the accuracy of 0.1%/5.7%, proving the indispensability of these parts.

Hybrid stages. Among token mixers based on pooling, attention, and spatial MLP, the pooling-based one can handle much longer input sequences while attention and spatial MLP are good at capturing global information. Therefore, it is intuitive to stack MetaFormers with pooling in the bottom stages to handle long sequences and use attention or spatial MLP-based mixer in the top stages, considering the sequences have been largely shortened. Thus, we replace the token mixer pooling with attention or spatial FC 1 in the top one or two stages in PoolFormer. From Table 5, the hybrid models perform quite well. The variant with pooling in the bottom two stages and attention in the top two stages delivers highly competitive performance. It achieves 81.0% accuracy with only 16.5M parameters and 2.5G MACs. As a comparison, ResMLP-B24 needs 7.0× parameters (116M) and 9.2× MACs (23.0G) to achieve the same accuracy. These results indicate that combining pooling with other token mixers for MetaFormer may be a promising direction to further improve the performance.

## Conclusion and future work

In this work, we abstracted the attention in Transformers as a token mixer, and the overall Transformer as a general 1 Following [[52]](#b52), we use only one spatial fully connected layer as a token mixer, so we call it FC. architecture termed MetaFormer where the token mixer is not specified. Instead of focusing on specific token mixers, we point out that MetaFormer is actually what we need to guarantee achieving reasonable performance. To verify this, we deliberately specify token mixer as extremely simple pooling for MetaFormer. It is found that the derived Pool-Former model can achieve competitive performance on different vision tasks, which well supports that "MetaFormer is actually what you need for vision".

In the future, we will further evaluate PoolFormer under more different learning settings, such as self-supervised learning and transfer learning. Moreover, it is interesting to see whether PoolFormer still works on NLP tasks to further support the claim "MetaFormer is actually what you need" in the NLP domain. We hope that this work can inspire more future research devoted to improving the fundamental architecture MetaFormer instead of paying too much attention to the token mixer modules.

![Figure 2. (a) The overall framework of PoolFormer. Similar to [24, 36, 57], PoolFormer adopts hierarchical architecture with 4 stages. For a model with L PoolFormer blocks, stage [1, 2, 3, 4] have [L/6, L/6, L/2, L/6] blocks, respectively. The feature dimension Di of stage i is shown in the figure. (b) The architecture of PoolFormer block. Compared with Transformer block, it replaces attention with extremely simple non-parametric operator, pooling, to conduct only basic token mixing.]()

![(parameters are frozen after random initialization) to conduct token mixing by X ′ = WRX where X ∈ R N ×C are input tokens with the token length of N and channel dimension of C. † Modified Layer Normalization (MLN) computes the mean and variance along token and channel dimensions compared with vanilla Layer Normalization only along channel dimension. MLN can be implemented with GroupNorm API in PyTorch by specifying the group number equal to 1. The numbers of MACs are counted by fvcore[19] library.]()

![Configurations of different PoolFormer models.There are two groups of embedding dimensions, i.e., small size with[64, 128, 320, 512] dimensions and medium size with[96, 196, 384, 768]. Notation "S24" means the model is in small size of embedding dimensions with 24 PoolFormer blocks in total. The numbers of MACs are counted by fvcore[19] library.]()

![arePerformance of different types of models on ImageNet-1K classification. All these models are only trained on the ImageNet-1K training set and the accuracy on the validation set is reported. RSB-ResNet means the results are from "ResNet Strikes Back"[59] where ResNet[24] is trained with improved training procedure for 300 epochs.]()

![Performance of object detection using RetinaNet, and object detection and instance segmentation using Mask R-CNN on COCO val2017[34]. 1× training schedule (i.e.12 epochs) is used for training detection models. AP b and AP m represent bounding box AP and mask AP, respectively.]()

![are two other important components in MetaFormer. Without residual connection or channel MLP, Ablation for PoolFormer on ImageNet-1K classification benchmark. PoolFormer-S12 is utilized as the baseline to conduct ablation study. The top-1 accuracy on the validation set is reported.]()

