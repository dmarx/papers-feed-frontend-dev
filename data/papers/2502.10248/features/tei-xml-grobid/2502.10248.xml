<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-02-17">17 Feb 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-02-17">17 Feb 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">E26F205A7BED5C37593A80991C556640</idno>
					<idno type="arXiv">arXiv:2502.10248v2[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>The Practice</term>
					<term>Challenges</term>
					<term>and Future of Video Foundation Model Step-Video Team StepFun</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>We present</head><p>Step-Video-T2V, a state-of-the-art text-to-video pre-trained model with 30B parameters and the ability to generate videos up to 204 frames in length. A deep compression Variational Autoencoder, Video-VAE, is designed for video generation tasks, achieving 16x16 spatial and 8x temporal compression ratios, while maintaining exceptional video reconstruction quality. User prompts are encoded using two bilingual text encoders to handle both English and Chinese. A DiT with 3D full attention is trained using Flow Matching and is employed to denoise input noise into latent frames. A video-based DPO approach, Video-DPO, is applied to reduce artifacts and improve the visual quality of the generated videos. We also detail our training strategies and share key observations and insights. Step-Video-T2V's performance is evaluated on a novel video generation benchmark, Step-Video-T2V-Eval, demonstrating its state-of-the-art text-to-video quality when compared with both open-source and commercial engines. Additionally, we discuss the limitations of current diffusion-based model paradigm and outline future directions for video foundation models. We make both Step-Video-T2V and Step-Video-T2V-Eval available at <ref type="url" target="https://github.com/stepfun-ai/Step-Video-T2V">https://github.com/stepfun-ai/Step-Video-T2V</ref>. The online version can be accessed from <ref type="url" target="https://yuewen.cn/videos">https://yuewen.cn/videos</ref> as well. Our goal is to accelerate the innovation of video foundation models and empower video content creators.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Preface</head><p>A video foundation model is a model pre-trained on large video datasets that can generate videos in response to text, visual, or multimodal inputs from users. It can be applied to a wide range of downstream video-related tasks, such as text/image/video-to-video generation, video understanding and editing, as well as video-based conversion, question answering, and task completion.</p><p>Based on our understanding, we define two levels towards building video foundation models. Level-1: translational video foundation model. A model at this level functions as a cross-modal translation system, capable of generating videos from text, visual, or multimodal context. Level-2: predictable video foundation model. A model at this level acts as a prediction system, similar to large language models (LLMs), that can forecast future events based on text, visual, or multimodal context and handle more advanced tasks, such as reasoning with multimodal data or simulating real-world scenarios.</p><p>Current diffusion-based text-to-video models, such as Sora OpenAI <ref type="bibr">[2024]</ref>, Veo DeepMind <ref type="bibr">[2024]</ref>, Kling <ref type="bibr">Kuaishou [2024</ref><ref type="bibr" target="#b2">], Hailuo MiniMax [2024]</ref>, and Step-Video (as described in this report), belong to Level-1. These models can generate high-quality videos from text prompts, lowering the barrier for creators to produce video content. However, they often fail to generate videos that require complex action sequences (such as a gymnastic performance) or adherence to the laws of physics (such as a basketball bouncing on the floor), let alone performing causal or logical tasks like LLMs. Such limitations arise because these models learn only the mappings between text prompts and corresponding videos, without explicitly modeling the underlying causal relationships within videos. Autoregression-based text-to-video models introduce the causal modeling mechanism by predicting</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Introduction</head><p>Large language models (LLMs), as part of Artificial General Intelligence (AGI), has made impressive progress in recent years. These models are capable of understanding human instructions and generating coherent, fluent responses in natural language. However, language is a symbolic abstraction of thought, using words and concepts to represent the world. This abstraction often falls short in capturing the complexity and richness of reality, particularly when it comes to dynamic processes like object motion or the intricate spatial and temporal relationships between entities. As a result, video generation has emerged as an important frontier in the pursuit of AGI, offering a pathway toward bridging these cognitive gaps. Moreover, video content is now the dominant form of communication and entertainment online. Developing video generation systems capable of producing high-quality content can significantly reduce barriers for creators and democratize video production. This empowers everyone, from amateurs to professionals, to effortlessly create compelling videos.</p><p>In this technical report, we present Step-Video-T2V, a state-of-the-art video foundation model with 30B parameters, capable of generating high-quality videos from text, featuring strong motion dynamics, high aesthetics, and consistent content. Like most commercial video generation engines,</p><p>Step-Video-T2V is a diffusion Transformer (DiT)-based model trained using Flow Matching. A specially designed deep compression Variational Auto-encoder (VAE) achieves 16x16 spatial and 8x temporal compression ratios, significantly reducing the computational complexity of large-scale video generation training. Two bilingual text encoders enable Step-Video-T2V to directly understand Chinese or English prompts. A cascaded training pipeline, including text-to-image pre-training, text-to-video pre-training, supervised fine-tuning (SFT), and direct preference optimization (DPO), is introduced to accelerate model convergence and fully leverage video datasets of varying quality. A new benchmark dataset called Step-Video-T2V-Eval is created for text-to-video generation, which includes 128 diverse prompts across 11 categories, alongside video generation results from several top text-to-video open-source and commercial engines for comparison.</p><p>Insights are gained throughout the entire development of Step-Video-T2V, spanning data, model, training, and inference. First, text-to-image pre-training is essential for the video generation model to acquire rich visual knowledge, including concepts, scenes, and their spatial relationships, providing a solid foundation for the subsequent text-to-video pre-training stages. Second, text-to-video pretraining at low resolution is critical for the model to learn motion dynamics. The more stable the model is trained during this stage, using as much diverse training data as possible, the easier it becomes to refine and scale the model to higher resolutions and more complex video generation tasks. Third, using high-quality videos with accurate captions and desired styles in SFT is crucial to the stability of the model and the style of the generated videos. Fourth, video-based DPO can further enhance the visual quality by reducing artifacts, ensuring smoother and more realistic video outputs.</p><p>Challenges remain in state-of-the-art video foundation models. For example, current video captioning models still face hallucination issues, leading to unstable training and poor instruction-following performance. Composing multiple concepts with low occurrence in the training data (e.g., an elephant and a penguin) within a single generated video is still a difficult task. Additionally, training and generating long-duration, high-resolution videos still face significant computational cost hurdles. Furthermore, even a DiT-based model like Step-Video-T2V with 30B parameters struggles to generalize well when generating videos involving complex action sequences or requiring adherence to the laws of physics. By open-sourcing Step-Video-T2V, we aim to provide researchers and engineers with a strong baseline, helping them better understand these challenges and accelerate innovations in the development and application of video foundation models.</p><p>The key contributions of this technical report are as follows:</p><p>• We present and open-source Step-Video-T2V, a state-of-the-art text-to-video pre-trained model with 30B parameters, capable of understanding both Chinese and English prompts, generating</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head><p>Video generation technology has seen significant progress over the past year, with advancements from Sora OpenAI <ref type="bibr">[2024]</ref> to <ref type="bibr">Gen-3 RunwayML [2024]</ref>, Kling <ref type="bibr">Kuaishou [2024]</ref>, Hailuo MiniMax <ref type="bibr">[2024]</ref>, Veo DeepMind <ref type="bibr">[2024]</ref>, and others.</p><p>Commercial video generation engines (e.g., Sora, Gen-3, Kling, and Hailuo) offer text-to-video generation capabilities, as well as extended applications like image-to-video generation or specialized video effect generation. Compared to these closed-source engines, which often involve longer and more complex video generation pipelines with extensive pre-and post-processing, Step-Video-T2V delivers comparable performance for general text prompts and even surpasses them in specific domains, such as generating videos with high motion dynamics or text content.</p><p>Open-source video generation models, such as HunyuanVideo <ref type="bibr" target="#b3">Kong et al. [2025]</ref>, CogVideoX <ref type="bibr">Yang et al. [2024a]</ref>, Open-Sora <ref type="bibr" target="#b5">Zheng et al. [2024]</ref>, and Open-Sora-Plan <ref type="bibr" target="#b6">Lin et al. [2024]</ref>, offer greater transparency in their implementations, making them more accessible to researchers and content creators. Both HunyuanVideo and CogVideoX are based on <ref type="bibr">MMDiT Esser et al. [2024]</ref>, a variation of the full attention Transformer architecture. Open-Sora and Open-Sora-Plan are built on DiT <ref type="bibr" target="#b8">Peebles and Xie [2023]</ref>, with the former using spatial-temporal attention and the latter employing full attention. Compared to these open-source models, the key contributions of Step-Video-T2V include being the largest open-source model to date, utilizing a high-compression VAE for videos, supporting bilingual text prompts in both English and Chinese, implementing a video-based DPO approach to further reduce artifacts and enhance visual quality, and providing comprehensive training and inference documentation, as outlined in this report.</p><p>Movie Gen Video <ref type="bibr" target="#b9">Polyak et al. [2024]</ref> is another video generation model from Meta, featuring a similar architecture and model size. Compared to Movie Gen Video, Step-Video-T2V stands out with four unique features. First, it incorporates a more powerful high compression VAE for large-scale video generation training. Second, it supports bilingual text prompt understanding in both English and Chinese. Third, it adds an additional DPO stage to the training process, reducing artifacts and improving the visual quality of the generated videos. Fourth, it is open-source and provides state-of-the-art video generation quality comparing with both open-source and commercial engines.</p><p>Videos encompass both spatial and temporal information, leading to significantly larger data volumes compared to images. Addressing the computational challenge of modeling video data efficiently is therefore a fundamental problem. Various methods have been proposed to reduce the complexity of video modeling. These include approaches such as 3D Causal Convolution <ref type="bibr" target="#b10">[Yu et al., 2024</ref><ref type="bibr">, Yang et al., 2024a</ref><ref type="bibr" target="#b3">, Kong et al., 2025</ref><ref type="bibr">, Zheng et al., 2024]</ref>, wavelet transform <ref type="bibr" target="#b11">[Nvidia, 2025</ref><ref type="bibr">, Li et al., 2024a]</ref>, and Residual Autoencoding in images <ref type="bibr" target="#b13">[Chen et al., 2025]</ref>. While these methods show promise in terms of either reconstruction quality or compression ratio, achieving a balance between high quality and effective compression remains difficult. Our work addresses this challenge, providing a solution that opens new possibilities in video generation, such as extending the context length or scaling up the DiT model more aggressively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Model</head><p>The overall architecture of Step-Video-T2V is given in Figure <ref type="figure" target="#fig_0">1</ref>. Videos are represented by a highcompression Video-VAE, achieving 16x16 spatial and 8x temporal compression ratios. User prompts are encoded using two bilingual pre-trained text encoders to handle both English and Chinese. A DiT with 3D full attention is trained using Flow Matching <ref type="bibr" target="#b14">[Lipman et al., 2023]</ref> and is employed to denoise input noise into latent frames, with text embeddings and timesteps serving as conditioning factors. To further enhance the visual quality of the generated videos, a video-based DPO approach is applied, which effectively reduces artifacts and ensures smoother, more realistic video outputs.</p><p>Next, we will introduce the implementation details of Video-VAE, bilingual text encoders, DiT with 3D full attention, and Video-DPO, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Video-VAE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Latent Space Compression in Video Generation</head><p>State-of-the-art video models, such as HunyuanVideo <ref type="bibr" target="#b3">[Kong et al., 2025]</ref>, CogVideoX <ref type="bibr">[Yang et al., 2024a]</ref>, and Meta Movie Gen <ref type="bibr" target="#b9">[Polyak et al., 2024]</ref>, leverage Variational Autoencoders (VAEs) with spatial-temporal downscaling factors of 4×8×8 or 8×8×8. These VAEs map 3-channel RGB inputs to 16-channel latent representations, achieving compression ratios as high as 1:96. To further reduce the number of tokens, these systems typically employ patchifiers that group 2×2×1 latent patches into individual tokens.</p><p>While this two-stage process of compression and tokenization is effective, it introduces architectural complexity and can potentially degrade the performance of the subsequent diffusion stages. The efficiency of text-to-video diffusion-transformer models is fundamentally dependent on their ability to operate within compressed latent spaces. Given that computational costs scale quadratically with the number of tokens due to attention operations, it is crucial to mitigate spatial-temporal redundancy through effective compression. This not only accelerates training and inference but also aligns with the diffusion process's inherent preference for condensed representations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Advancing Compression through New Architecture</head><p>Our Video-VAE introduces a novel dual-path architecture at the later stage of the encoder and the early stage of the decoder, featuring unified spatial-temporal compression. This design achieves 8×16×16 downscaling through the synergistic use of 3D convolutions and optimized pixel unshuffling operations. For an input video tensor X ∈ R B×C×T ×H×W , the encoder E produces latent representation Z = E(X) ∈ R B×Cz×⌈T /8⌉×⌈H/16⌉×⌈W/16⌉ through:</p><p>Causal 3D Convolutional Modules The early stage of the encoder consists of three stages, each featuring two Causal Res3DBlock and corresponding downsample layers. Following this, a MidBlock combines convolutional layers with attention mechanisms to further refine the compressed representations. To enable joint image and video modeling, we employ temporal causal 3D convolution. Our architecture implements temporal causality through:</p><formula xml:id="formula_0">C 3D (X) t = Conv3D([0, ..., X t ], Θ) t = 0 Conv3D([X t-k , ..., X t ], Θ) t &gt; 0 (1)</formula><p>where k is the temporal kernel size, ensuring frame t only depends on previous frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dual-Path Latent Fusion</head><p>The primary motivation for Dual-Path Latent Fusion is to maintain high-frequency details through convolutional processing while preserving low-frequency structure via channel averaging. Notably, <ref type="bibr" target="#b13">Chen et al. [2025]</ref> identify similar mechanisms within the realm of image VAE modeling. Our approach, however, introduces a unified structure adept at handling both image and video data. This approach allows the network to use its parameters more efficiently, thereby overcoming the blurring artifacts typically associated with traditional VAEs.</p><p>1. Conv Path: Combines causal 3D convolutions with pixel unshuffling,</p><formula xml:id="formula_1">H conv = U (3) s (C 3D (X))<label>(2)</label></formula><p>where</p><formula xml:id="formula_2">U (3) s : R B×C×T ×H×W → R B×C•s 3 × T s t × H ss × W</formula><p>ss with spatial stride s s = 2, temporal stride s t = 2, and C 3D denoting our causal 3D convolution. 2. Shortcut Path: Preserves structural semantics through grouped channel averaging,</p><formula xml:id="formula_3">H avg = 1 s 3 s 3 -1 k=0 U (3) s (X) [...,kCz:(k+1)Cz]<label>(3)</label></formula><p>where U</p><p>(3) s implements 3D pixel unshuffle with spatial-temporal blocking, C z is the latent dim of next stage.</p><p>The output of fusion combines both paths through residual summation:</p><formula xml:id="formula_4">Z = H conv ⊕ H avg (4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Decoder Architecture</head><p>The early stage of the decoder consists of two symmetric Dual Path architectures. In these architectures, the 3D pixel unshuffle operation U is replaced by 3D pixel shuffle operator P, the grouped channel averaging path is replaced by a grouped channel repeating operation, which efficiently unfolds the compressed information into spatial-temporal dimensions. In ResNet backbone, we replace all groupnorm with spatial groupnorm to avoid temporal flickering between different chunks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Training Details</head><p>Our VAE training process is meticulously designed in multiple stages, which is the key reason for achieving our final goal of efficient and high-quality video data modeling.</p><p>In the first stage, we train a VAE with a 4x8x8 compression ratio without employing a dual-path structure. This initial training is conducted jointly on images and videos of varying frame counts, adhering to a preset ratio. In this stage, we set a lower compress goal for model to sufficiently learn low level representations.</p><p>In the second stage, we enhance the model by incorporating two dual-path modules in both the encoder and decoder, replacing the latter part after the mid-block. During this phase, we gradually unfreeze the dual-path modules, the mid-block, and the ResNet backbone, allowing for a more refined and flexible training process.</p><p>Throughout the training, we utilize a combination of L1 reconstruction loss, Video-LPIPS, and KL-divergence constrain to guide the model. Once these losses have converged, we introduce GAN loss to further refine the model's performance. This staged approach ensures a robust and high-quality VAE capable of handling complex video data efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Bilingual Text Encoder</head><p>The text encoder plays a crucial role in text-to-video generation by guiding the model in the latent space. In Step-Video-T2V, we use two bilingual text encoders to process user text prompts: Hunyuan-CLIP and Step-LLM.</p><p>Hunyuan-CLIP is the bidirectional text encoder of an open-source bilingual CLIP model <ref type="bibr">Li et al. [2024b]</ref>. Due to the training mechanism of the CLIP model, Hunyuan-CLIP can produce text representations well-aligned with the visual space. However, because its maximum input length is limited to 77 tokens, Hunyuan-CLIP faces challenges when processing longer user prompts.</p><p>Step-LLM, on the other hand, is an in-house, unidirectional bilingual text encoder pre-trained using the next-token prediction task. It incorporates a redesigned Alibi-Positional Embedding <ref type="bibr" target="#b16">Press et al. [2022]</ref> to improve both efficiency and accuracy in sequence processing. Unlike Hunyuan-CLIP,</p><p>Step-LLM has no input length restriction, making it particularly effective for handling lengthy and complex text sequences.</p><p>By combining these two text encoders, Step-Video-T2V is able to handle user prompts of varying lengths, generating robust text representations that effectively guide the model in the latent space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">DiT w/ 3D Full Attention</head><p>Layers Attention Heads Head Dim FFN Dim Cross-Attn Dim Activation Function Normalization 48 48 128 24,576 (6,144, 1,024) GELU-approx RMSNorm Table 1: Hyper-parameters used in Step-Video-T2V.</p><p>Step-Video-T2V is built on the DiT <ref type="bibr" target="#b8">Peebles and Xie [2023]</ref> architecture, which consists of 30B parameters and contains 48 layers. Each layer contains 48 attention heads, with each head's dimension set to 128. The setting of hyper-parameters used in Step-Video-T2V is outlined in Table <ref type="table">1</ref>. 3D Full-Attention: We employ the 3D full-attention in Step-Video-T2V instead of the spatial-temporal attention, which is more computationally efficient. This choice is driven by its theoretical upper bound for modeling both spatial and temporal information in videos, as well as its superiority in generating videos with smooth and consistent motion observed from large-scale experiments.</p><p>Cross-Attention for Text Prompt: We introduce a cross-attention layer between the self-attention and feed-forward network (FFN) in each transformer block to incorporate text prompts. This layer enables the model to attend to textual information while processing visual features. The prompt is embedded using two distinct bilingual text encoders, Hunyuan-CLIP and Step-LLM, as described in §4.2. The outputs from these two encoders are concatenated along the sequence dimension, creating the final text embedding sequence. This combined embedding is then injected into the cross-attention layer, allowing the model to generate videos conditioned on the input prompt.</p><p>Adaptive Layer Normalization (AdaLN) with Optimized Computation: In standard DiT, each block includes an adaptive layer normalization (AdaLN) operation to embed timestep and class label information. Since the text-to-video task does not require class labels, we remove class labels from AdaLN. Furthermore, we follow <ref type="bibr" target="#b17">Chen et al. [2023]</ref> and adopt the AdaLN-Single structure to reduce the computational overhead of traditional AdaLN operations and improve overall model efficiency. In the first layer of the model, AdaLN uses an MLP block to embed timestep information. In subsequent layers, a learnable parameter is initialized to summarize the timestep embeddings, which are then used as parameters for the adaptive normalization in each block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RoPE-3D:</head><p>We use RoPE-3D, an extension of the traditional Rotation-based Positional Encoding (RoPE) <ref type="bibr" target="#b18">Su et al. [2023]</ref>, specifically designed to handle video data by accounting for temporal (frame) and spatial (height and width) dimensions. The original RoPE-1D applies a rotational transformation to positional encodings to enable flexible and continuous representation of positions in sequences of varying lengths. The rotational transformation is applied by rotating the positional encoding E i at position i by an angle θ i = 2πi τ , where τ is a period controlling the rotation rate, and the resulting encoding P i = Rot(E i , θ i ) is obtained. To extend this to video data, we introduce RoPE-3D. This method splits the query and key tensors along the channel dimension, applying RoPE-1D independently to each tensor for the temporal (frame) and spatial (height and width) dimensions. The resulting encodings are then concatenated. This approach enables the model to handle video inputs with varying lengths and resolutions effectively. RoPE-3D offers several advantages, such as the ability to process videos with different frame counts and resolutions without being restricted by fixed positional encoding lengths. It improves generalization across diverse video data and effectively captures both spatial and temporal relationships within the video. By providing a continuous and flexible encoding for three-dimensional video data, RoPE-3D enhances the model's capacity to process and generate high-quality video content.</p><p>QK-Norm: We use Query-Key Normalization (QK-Norm) to stabilize the self-attention mechanism. QK-Norm normalizes the dot product between the query (Q) and key (K) vectors, addressing numerical instability caused by large dot products that can lead to vanishing gradients or overly concentrated attention. This normalization ensures stable attention during training, accelerates convergence, and improves efficiency, allowing the model to focus on learning meaningful patterns. Additionally, QK-Norm helps maintain a balanced distribution of attention weights, enhancing the model's ability to capture relationships within the input sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Training Objective for Video and Image Generation</head><p>We use Flow Matching in the training of Step-Video-T2V. At each training step, we begin by sampling a Gaussian noise, X 0 ∼ N (0, 1), and a random timestep t ∈ [0, 1]. We then construct the model input X t as a linear interpolation between X 0 and X 1 , where X 1 is the target sample corresponding to the noise-free input. Specifically, we define X t as:</p><formula xml:id="formula_5">X t = (1 -t) • X 0 + t • X 1 .</formula><p>The ground truth velocity V t , which represents the rate of change of X t with respect to the timestep t, is defined as:</p><formula xml:id="formula_6">V t = dX t dt = X 1 -X 0 .<label>(5)</label></formula><p>In other words, V t captures the direction and magnitude of change from the initial noise X 0 to the target data X 1 . The model is then trained by minimizing the mean squared error (MSE) loss between the predicted velocity u(X t , y, t; θ) and the true velocity V t . Here, u(X t , y, t; θ) denotes the model's predicted velocity at timestep t, given input X t and an optional conditioning input y (e.g., a bilingual sentence). The training loss is given by: loss = E t,X0,X1,y ∥u(X t , y, t; θ) -V t ∥ 2 , (6) where the expectation is taken over all training samples, with t being the random timestep, and X 0 , X 1 , and y drawn from the dataset. The term θ represents model parameters. This approach ensures that the model learns to predict the instantaneous rate of change of the noisy sample X t with respect to t, which can later be used to reverse the diffusion process and recover data samples from noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Inference</head><p>During inference, we begin by sampling random noise X 0 ∼ N (0, 1). The goal is to recover the denoised sample X 1 by iteratively refining the noise through an ODE-based method. For simplicity, we adopt a Gaussian solver and define a sequence of timesteps {t 0 , t 1 , . . . , t n }, where t 0 = 0, t n = 1, and t 0 &lt; t 1 &lt; • • • &lt; t n . The denoising process is then carried out by integrating over these timesteps. Specifically, the denoised sample X 1 can be expressed as:</p><formula xml:id="formula_7">X 1 = n-1 i=0 u(X ti , y, t i ; θ) • (t i+1 -t i ),<label>(7)</label></formula><p>where u(X ti , y, t i ; θ) represents the predicted velocity at timestep t i , given the noisy sample X ti and an optional conditioning input y. The integral is computed over the timesteps from t 0 to t n , with each term u(X ti , y, t i ; θ) multiplied by the corresponding timestep difference (t i+1 -t i ). This iterative process allows the model to gradually denoise the input sample, starting from the noise X 0 and progressing toward the target sample X 1 over the defined timesteps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Video-DPO</head><p>The integration of human feedback has been widely validated in the domain of LLMs, particularly through methods such as Reinforcement Learning with Human Feedback (RLHF) Ouyang et al.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prompt Pool</head><p>Training data</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Handcrafted by annotators</head><p>Step-Video-T2V</p><p>"A ballet dancer practicing in the dance studio" reward model Human Annotation Feedback [2022], Christiano et al. <ref type="bibr">[2017]</ref>, where models adjust their generated content based on human feedback. Recently, this practice has also been applied in image and video generation, yielding significant advancements. To improve the visual quality of Step-Video-T2V, we design a pipeline to introduce human feedback. The overall pipeline is shown in Figure <ref type="figure" target="#fig_3">4</ref>, and details are discussed in the following.</p><p>In Step-Video-T2V, we select Direct Preference Optimization (DPO) <ref type="bibr" target="#b21">Rafailov et al. [2024]</ref> as the method for incorporating human feedback. It has been proven effective across a variety of generation tasks <ref type="bibr" target="#b22">Wallace et al. [2024]</ref>, <ref type="bibr">Yang et al. [2024b]</ref>, and the essence of the method is simple, making it both intuitive and easy to implement. Intuitively, given human preference data and non-preference data under the same conditions, the goal is to adjust the current policy (i.e., the model) to be more aligned with the generation of preferred data, while avoiding the generation of non-preferred data. To stabilize training, the reference policy (i.e., the reference model) is introduced to prevent the current policy from deviating too far from the reference policy. The policy objectvie can be formulated as:</p><formula xml:id="formula_8">L DPO = -E (y,xw,x l )∼D log σ β log π θ (x w |y) π ref (x w |y) -log π θ (x l |y) π ref (x l |y)<label>(8)</label></formula><p>where π θ and π ref refers to current policy and reference policy, respectively, x w and x l are the preferred sample and non-preferred sample, and y denotes the condition.</p><p>To collect these samples (x w , x l given y) for training, we construct a diverse prompt set. First, we randomly select a subset of prompts from the training data to ensure prompt diversity. Second, we invite human annotators to synthesize prompts based on a carefully designed guideline that mirrors real-world user interaction patterns. And then, for each prompt, Step-Video-T2V generates multiple videos using different seeds. Human annotators rate the preference of these samples. The annotation process is monitored by quality control personnel to ensure accuracy and consistency. This process results in a set of preference and non-preference data, which serves as the foundation for model training. Two labeled examples are shown in Figure <ref type="figure">5</ref>.</p><p>At each training step, we select a prompt and its corresponding positive and negative sample pairs described above. Each sample is generated by the model itself, ensuring smooth updates and improving overall training stability. In addition, to maintain consistency in the training data, we align the positive and negative samples by fixing the initial noise and timestep, which contributes to a more stable training process. Our training objective in Eqn. 8 is based on the DiffusionDPO method <ref type="bibr" target="#b22">Wallace et al. [2024]</ref> and <ref type="bibr">DPO Rafailov et al. [2024]</ref> but with slight modifications, extending it to the Flow Matching framework. By denoting the policy-related terms in Eqn. 8 as inside term z, it can be derived that:</p><formula xml:id="formula_9">∂L DPO ∂θ ∝ -β(1 -σ(βz)) • ∂z ∂θ ,<label>(9)</label></formula><p>which indicates large β (e.g., 5,000 in DiffusionDPO) may cause gradient explode when z &lt; 0, as it amplifies gradients by β times. As a result, gradient clipping and an extreme low learning rate (e.g., 1e-8 in DiffusionDPO) are required to ensure stable training, leading to slow convergence.To address this, we reduce β and increase the learning rate, results much faster convergence.</p><p>Human feedback effectively improves visual quality. However, we observe that the improvements saturate when the model can easily distinguish between positive and negative samples. This phenomenon may stem from the following reason: the training data used in Video-DPO is generated </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Distillation for Step-Video-T2V Turbo</head><p>Diffusion models for video generation typically require substantial computational resources during inference, often necessitating more than 50 steps of ODE integration to produce a video. Reducing the number of function evaluations (NFE) is crucial for improving inference efficiency. We demonstrate that a large-scale trained Video DiT can reduce NFE to as few as 8 steps with negligible performance degradation. This is achieved through self-distillation with a rectified flows objective and a specifically designed inference strategy.</p><p>Our base model is trained using rectified flow, and the distillation objective aims to train a 2-rectifiedflow model <ref type="bibr" target="#b24">[Liu et al., 2022]</ref>, which facilitates more direct ODE paths during inference. As discussed by <ref type="bibr" target="#b25">Lee et al. [2024]</ref>, the loss function for the 2-rectified flow can be formulated as follows:</p><formula xml:id="formula_10">L(θ, t) := 1 t 2 E[∥v -u θ (x t , t)∥ 2 2 ] = 1 t 2 E[∥x -E[x|x t ]∥ 2 2 ] + L(θ, t). (<label>10</label></formula><formula xml:id="formula_11">)</formula><p>Since all training samples are generated by the base 1-rectified model, the irreducible loss (first term) is relatively small. The reducible error (second term) can be efficiently optimized by assigning more weight to timesteps that are more challenging. Specifically, the training loss of 2-rectified flow is large at each end of the interval t ∈ [0, 1] and small in the middle.</p><p>We sampled approximately 95,000 data samples using a curated distribution of SFT data prompts with 50 NFE and carefully designed positive and negative prompts to formulate a distillation dataset. We modified the timestep sampling strategy to a U-shaped distribution, specifically p t (u) ∝ exp(au) + exp(-au) on u ∈ [0, 1], with a larger a = 5 as the time shift required by the video model is higher.</p><p>During inference, we observed that as the training progresses, the model requires more significant sampling time shifts and a lower classifier-free guidance (CFG) scale. By combining this with a linear diminishing CFG schedule as described in Eqn. 11, our model can achieve comparable sample quality with up to 10 times fewer steps. Figure <ref type="figure">6</ref>, shows generated samples with 204 frames from our turbo model with 10 NFE.</p><formula xml:id="formula_12">cfg t = max(cfg max -9t(cfg max -1), 1) for 0 ≤ t ≤ 1 (11)</formula><p>Figure <ref type="figure">6</ref>: Generated samples with Step-Video-T2V Turbo with 10 NFE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">System</head><p>This section describes our infrastructure that facilitates the efficient and robust training of Step-Video-T2V at scale. The discussion starts with a comprehensive system overview, providing a holistic perspective of the workflow, followed by an in-depth examination of each constituent component. Furthermore, we present our insights and practical experiences gained from our training platform implementation and routine operational management. To enable systematic monitoring and analysis during large-scale training, we implement a dual-layer monitoring approach through StepTelemetry ( §6.4). This system collects detailed data statistics from inference clusters while simultaneously collecting iteration-level, fine-grained performance metrics from training clusters. The resulting telemetry data provides multidimensional insights into the training system, enabling precise identification of algorithmic patterns and systematic detection of potential performance bottlenecks across the entire infrastructure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Overview</head><p>We have constructed a datacenter comprising thousands of NVIDIA H800 GPUs interconnected by a rail-optimized RoCEv2 fabric (1.6Tbps bandwidth per node). Nodes of the datacenter can be dynamically assigned to inference clusters or training clusters according to GPU resource requirements. To support a single large-scale training job with thousands of GPUs spanning multiple GPU clusters concurrently, we have gained valuable insights from addressing challenges related to the training platform (StepMind) and its operational complexities. A detailed examination of these findings,</p><p>Text Encoder Server VAE VAE VAE Server Video DiT Video DiT Video DiT Inference Clusters Training Clusters StepRPC Model Config Training Emulator Resource Config Resource Plan Parallelism Strategy optimal training parallelism Image/Video Latent Text Embed optimal GPU alloc ratio Step Emulator StepTelemetry Data Statistics Fine-grained perf monitor Training Platform StepMind Figure 7: The workflow of Step-Video-T2V training system. including specific implementation strategies and best practices, will be presented in §6.5. Through comprehensive improvements to infrastructure reliability, we have achieved 99% effective GPU training time over more than one month. 6.2 Training Framework Optimizations 6.2.1 Step Emulator The large model size and extended context length of video require partitioning both the model parameters and activations/gradients across devices using multiple parallelism strategies during training, such as Tensor-parallelism (TP), Sequence-parallelism (SP), Context-parallelism (CP), Pipeline-parallelism (PP) and Virtual Pipeline-parallelism (VPP) Narayanan et al. [2021], Korthikanti et al. [2023], Liu et al. [2023], Jacobs et al. [2023]. However, the large scale of GPU cluster required for DiT training poses significant challenges in tuning and validating architecture designs and optimizations. To address this, we developed Step Emulator (SEMU), a highly accurate and efficient simulator designed to estimate resource consumption and end-to-end performance during training, under various model architecture and parallelism configurations. Specifically, to accommodate the dynamic and mixed input data for DiT training, SEMU allows customization of input data with varying frames and resolutions. SEMU helps to design model parameters, architecture and the associated optimal parallelism strategies. It also determines the resource allocation of inference (i.e., text-encoder and VAE) and training (i.e., video DiT) clusters before the training actually starts. 6.2.2 Distributed Training Parallelism Strategy Table 2 outlines the MFU of different configurations for 540P video pretraining obtained by SEMU. As shown in the table, simply applying PP on top of TP does not achieve a high MFU. This is because PP only reduces memory usage for model parameters and gradients by about 20GB after 8-way TP, and it can only disable a small portion of activation checkpointing, given the 120GB of activation memory. While CP directly reduces activation memory, its communication cost through the NIC is comparable to the TP cost via NVLink. To reduce CP overhead, we apply head-wise CP Jacobs et al. [2023] to the self-attention block, leveraging the MHA in the DiT model, and sequence-wise CP Liu et al. [2023] to the cross-attention block, due to the relatively short sequence length of k and v from the prompts. Despite these optimizations, the CP cost remains non-negligible, and relying solely on CP does not lead to a high MFU.</p><p>As a result, the optimal MFU is always achieved by combining TP, CP, PP, and VPP. However, for large-scale GPU cluster training, it is crucial to keep the backend framework as simple as possible for robustness and easy identification of stragglers during training. This hinders us from adopting PP since it generally lacks the necessary flexibility. As a trade-off, we adopt an 8-way tensor parallelism (TP) strategy combined with sequence parallelism (SP) and Zero1 <ref type="bibr" target="#b30">Rajbhandari et al. [2020]</ref>. This configuration results in a MFU that is marginally lower (-0.88%) than the theoretical optimum. In practice, the actual training MFU reaches 32%, which is slightly below the estimated value due to metric collection overhead and minor delays caused by stragglers. Multiprocessors (SMs). This design enables simultaneous execution of StepCCL operations and GEMM computations on the same GPU, achieving true concurrency without mutual performance interference, thereby maximizing hardware utilization and computational throughput. More details can be found at Section 7 of <ref type="bibr" target="#b31">Zhang et al. [2024]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TP overlap</head><p>DP overlap In the first two stages (i.e. Image and 192P video pre-training), the context length is below 10K and activation memory does not pose a limiting factor. The primary memory usage stems from model parameters, which are handled via 8-way TP. While the performance bottleneck arises from gradient reduce-scatter and parameter all-gather operations introduced by DP, which can take up more than 30% of the training time. To mitigate this, we developed DP overlap, where the parameter all-gather is performed during the forward pass of the first micro-batch, while the gradient reduce-scatter overlaps with the backward pass of the last micro-batch. Note that in DiT training, the activation norm is typically a key metric in the training process, which registers forward hooks for monitoring. These forward hooks can slow down the kernel launch in forward process, further rendering the forward overlap of DP communication ineffective. Therefore, the effectiveness of forward overlap may vary depending on the scenario, and the decision to enable it should be made carefully on a case-by-case basis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.3">Tailored Computation and Communication Modules</head><p>VAE Computation To accelerate the convolution op (the most compute-intensive) in VAE, we employ the channel-last principle that is more GPU-friendly PyTorch [2023] than naive PyTorch implementation. Specifically, a raw PyTorch tensor uses the NCHW memory format by default, while GPU tensorcores only support NHWC format essentially, causing additional format transformation overhead that slows down overall speed. We solve this by performing format permutation at the beginning, putting the channel to the last dimension physically. We modify each op (e.g., Conv, GroupNorm) along the computation graph to adapt to channel-last format. Overall, we achieve up to 7x VAE encode throughput with this optimization.</p><p>Multi-GPUs VAE To further reduce VAE latency and also support long, high resolution videos, using multiple GPUs is necessary to reduce computation and memory footprint on a single device.</p><p>We support both Temporal and Spatial Parallel for the convolution op. As an example, for Temporal Parallel, we divide the video latent along the frame dimension and let each GPU hold only a subset of video frames. If a downstream Conv op requires cross-frame computation, we transfer the overlapped frames using all-to-all communication. The overhead is typically small (&lt; 1%) compared to the computation time.</p><p>DiT The plain RoPE implementation is inefficient due to the numerous time-consuming slice and concat operations required for building the embedding table and indexing. We developed a custom RoPE-3D kernel that replaces these indexing operations with efficient embedding computation, significantly improving performance. The timestep modulation in the DiT model results in high activation memory usage, as the timestep is repeated across the sequence length, which is redundant since it remains the same within a single video clip. We implement a memory-efficient modulation operation where the timestep is repeated only during the forward process, and the non-repeated timestep is saved for the backward process. To further reduce memory costs, we fuse the LayerNorm op and the downstream timestep modulation op, eliminating the need for saving an intermediate output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.4">DP Load Balance</head><p>A critical challenge in large-scale video generation arises when processing mixed-resolution videos and images within the same global iteration. Conventional approaches that segregate different resolutions into separate batches lead to significant FLOPs disparities across model instances, resulting in GPU under-utilization due to load imbalance. To address this issue, we propose a hybrid-grained load balancing strategy that operates through two complementary stages, as illustrated in Figure <ref type="figure" target="#fig_6">8</ref>. In the first stage, we perform coarse-grained FLOPs alignment by adjusting batch sizes of videos with different resolutions. For each resolution r, we estimate its FLOPs per sample F r and compute optimal batch sizes B r through:</p><formula xml:id="formula_13">B r = F target αF r<label>(12)</label></formula><p>where F target represents the target FLOPs per batch (typically the batch of the highest resolution videos) and α is a normalization factor to ensure the consistency of global batch size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FLOPs Batchs</head><p>Video Samples Pipe Image Samples Pipe The second stage addresses residual FLOPs variations through fine-grained image padding. Our system caches N video batches and calculates required image supplements based on a predetermined video-to-image ratio β. Using a greedy heuristic, we iteratively allocate images to the batch with the smallest current FLOPs until all supplements are distributed.</p><p>The hybrid-grained approach effectively balances computational loads while maintaining practical deployability. Our solution requires only superficial awareness of data distribution, needing merely batch size adjustments and supplemental image padding rather than deep architectural changes. This minimal intervention preserves the original training pipeline's integrity while introducing small memory overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">StepRPC</head><p>To facilitate data transfer, we developed StepRPC, a high-performance communication framework.</p><p>StepRPC leverages distributed named pipes as the core programming abstraction, enabling a large number of servers to communicate seamlessly by declaring pipes with the identical name.</p><p>The spraying mode distributes data evenly across training servers. Compared to existing communication frameworks Moritz et al. [2018], Damania et al. [2023], Qin et al. [2024], StepRPC incorporates the following essential engineering optimizations. Tensor-Native Communication over RDMA and TCP In existing frameworks, tensor transfer typically entails heavy-weight serialization and deserialization overheads, amounting to tens of milliseconds. To address the inefficiency, StepRPC implements tensor-native communication that directly transfers bits within tensor memory, thereby eliminating the overheads associated with serialization and deserialization. StepRPC harnesses the power of RDMA to implement direct transfer of both GPU and CPU tensors. When RDMA-capable networks are not available, StepRPC can be seamlessly configured to utilize TCP transports. Note that TCP only supports CPU tensors. Consequently, transferring GPU tensors over TCP introduces additional overheads due to the necessity of copying memory between GPUs and CPUs. To mitigate the overheads, StepRPC proposes to overlap CudaMemcpy with TCP send and recv operations. Such optimization hides the latency of memory copying, thereby improving overall communication performance in non-RDMA environments. Flexible Workload Patterns with High Resilience To optimize GPU utilization, we leverage the same inference servers to generate data for large-scale pre-training experiments and small-scale ablation experiments simultaneously. StepRPC facilitates this via a combination of broadcasting and spraying communications. First, StepRPC broadcasts the data from inference servers to all training jobs. This ensures that each job receives the necessary data without redundant computations. Second, within an individual job, StepRPC sprays data to each training server. Though ingesting data from same inference servers, training jobs can operate independently and elastically with the help of StepRPC, meaning that jobs can begin, terminate or scale as needed without affecting the others. Meanwhile, StepRPC isolates failures across jobs, preventing cascading effects that could destabilize the entire system. Enhanced Visbility for Real-Time Failure Detection and Resource Optimization StepRPC offers comprehensive performance metrics for deep insights into the communication process. The metrics encompass critical aspects such as data counts, queuing latency and transmission cost. The enhanced visibility serves multiple purposes, empowering both operators and researchers to optimize performance and resource utilization effectively. Firstly, by monitoring the counts of produced and consumed data, StepRPC enables real-time failure detection. Discrepancies between these counts can indicate potential issues such as data loss, communication failures, or bottlenecks. This proactive approach allows operators to promptly identify and address failures. Next, researchers can leverage the metrics like queuing latency and API invoking latency to assess whether inference or training processes constitute the overall performance bottleneck. Furthermore, armed with the metrics like rates of producing and consuming data, researchers can make informed decisions regarding GPU resource allocation for inference and training jobs. 6.4 StepTelemetry The lack of the observability of training framework makes analyzing it's inner state and debugging job failure difficult. Thus StepTelemetry, an observability suite for training frameworks, is introduced. This suite's goal is not only to enhance anomaly detection capabilities but also to establish a reusable pipeline for collecting, post-processing, and analyzing any training-related data. StepTelemetry employs a simple and asynchronous data collection pipeline. It offers a Python SDK for easy integration with the training framework, supporting both batch and streaming data writes to files on local disk. An additional consumer process is responsible for collecting, transforming, and writing data into various remote databases. StepTelemetry benefits Step-Video-T2V training in the following aspects. Anomaly Detection Common profiling tools like PyTorch Profiler and Megatron-LM Timer introduce approximately 10% to 15% overhead, and struggle to support collaborative analysis among multiple ranks. Instead, StepTelemetry adopts a CUDA event-based approach without any unnecessary synchronizations. This enables continuously collecting timer data of all ranks during training with almost zero overhead. By providing various data visualizations and supporting data drill-down, StepTelemetry helps pinpointing root cause in case of hardware and software failure. As an example, during one training session, the training efficiency fell below expectations, yet no hardware alerts were triggered. Upon analyzing the collected data, we identified that the backward propagation time for certain ranks was abnormally prolonged. Since the backward process primarily involves tensor parallelism (TP) group communication and computation, it is highly probable that the machines hosting these ranks were underperforming. After removing these machines from the training cluster, the training efficiency returned to the expected level. Data Statistics During video training, it is vital to monitor data consumption. Instead of just counting tokens, it is required to record consumed videos' metadata. The legacy approach was to dump metadata to files on local disk, and then use scripts to parse them offline, which is particularly inefficient and inconvenient. By instrumenting dataloader with StepTelemetry, the metadata is written to database, thus OLAP is enabled. Visualizations such as duplicated data filtering and data distribution monitoring based on source url is provided to researchers, which help evaluating the model.</p><p>Performance Optimization StepTelemetry provides insight for performance optimization. By visualizing the time consumption of each stage within an iteration, it provides developers with a comprehensive overview, enabling them to identify and optimize performance bottlenecks in critical paths. Additionally, dataloader statistics reveal the actual throughput of the training process. Although image and video data are supplied in a mixed manner, the iteration time remained unchanged after addressing the data parallelism (DP) imbalance issue. Nevertheless, the observed increase in data throughput demonstrates a significant improvement in system efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">StepMind</head><p>To ensure high availability of computing resources for large-scale Step-Video-T2V training tasks, we have invested substantial efforts into developing StepMind, a distributed training platform designed for large-scale machine learning workloads. StepMind has successfully achieved an effective GPU utilization rate exceeding 99.0% for Step-Video-T2V training, primarily through the implementation of the following key techniques.</p><p>Fine Grained Monitoring at Full Coverage To maximize distributed training efficiency, we developed a fine-grained monitoring system at full coverage that rapidly identifies faulty nodes. The monitoring system collects metrics at seconds-granularity across hardware, e.g., CPU/GPU/memory/PCIe/network/storage/power/fans, and software, e.g., OS stack, enabling rapid and full coverage fault detection. Based on our operation experiences, faulty nodes can be generally classified into two categories: a) Nodes with Fatal Errors (about 86.2% of failures). These nodes can interrupt the training process immediately. Upon detection of these nodes, we will replace them with healthy nodes and restart the job. In order to avoid incorrect restarts due to false alarms, we develop a multi-signal approach to ascertain whether a job requires restarting. The signals incorporated in this approach encompass RoCEv2 traffic disruption, low GPU power usage, and the cessation of updates in job training logs. Once being identified as failed, the job will be restarted immediately, thereby reducing the time cost of unavailability resulting from node malfunctions. b) Nodes with Non-Fatal Errors (about 13.8% of failures). Although these nodes do not immediately disrupt the training task, they can degrade training efficiency. Detecting such nodes is challenging, and we have developed specialized methods to identify them. These nodes are scheduled for replacement during planned maintenance, typically after a checkpoint is saved, to minimize the wasting time of computational resource. To speed up GPU repairing, we've created an automated system that quickly reboots servers based on the identified failure type. By integrating this reboot system with follow-up health checks and stress tests, we ensure servers can be brought online rapidly and with assured quality. • Comprehensive health checks via extensive diagnostic scripts. We encode human expertise into reusable scripts to conduct comprehensive checks on the hardware and software configurations of GPU nodes. These checks include GPU, NIC, software driver, and firmware configurations, ensuring that servers in operation have uniform and correct hardware and software setups. In our experience, this practice prevents nodes with abnormal configurations from running training jobs, thereby reducing the likelihood of job interruptions. Video Quality Assessment To construct a refined dataset optimized for model training on highquality, we systematically evaluated and filtered video clips by assigning multiple Quality Assessment tags based on specific criteria. We uniformly sampled eight frames from each clip to compute these tags, providing a consistent and comprehensive assessment of each video.</p><p>• Aesthetic Score: We used the public LAION CLIP-based aesthetic predictor <ref type="bibr" target="#b39">Schuhmann et al. [2022]</ref> to predict the aesthetic scores of eight frames from each clip and calculated their average.</p><p>• NSFW Score: We employed the public LAION CLIP-based NSFW detector <ref type="bibr" target="#b40">LAION [2021]</ref>, a lightweight two-class classifier using CLIP ViT-L/14 embeddings, to identify content inappropriate for safe work environments.</p><p>• Watermark Detection: Employing an EfficientNet image classification model <ref type="bibr" target="#b41">Tan and Le [2019]</ref>, we detected the presence of watermarks within the videos.</p><p>• Subtitle Detection: Utilizing PaddleOCR <ref type="bibr" target="#b42">Contributors [2023]</ref>, we recognized and localized text within video frames, identifying clips with excessive on-screen text or captions.</p><p>• Saturation Score: We assessed color saturation by converting video frames from BGR to HSV color space and extracting the saturation channel, using OpenCV OpenCV Developers <ref type="bibr">[2021]</ref>.</p><p>We computed statistical measures-including mean, maximum, and minimum saturation values-across the frames.</p><p>• Blur Score: We detect blurriness by applying the variance of the Laplacian method Pech-Pacheco et al. <ref type="bibr">[2000]</ref> to measure the sharpness of each frame. Low variance values indicate blurriness caused by camera shake or lack of clarity.</p><p>• Black Border Detection: We use FFmpeg to detect black borders in frames and record their dimensions to facilitate cropping, ensuring that the model trains on content free of distracting edges.</p><p>Video Motion Assessment Recognizing that motion content is crucial for representing dynamic scenes and ensuring effective model training, we calculate the motion score by averaging the mean magnitudes of the optical flow OpenCV <ref type="bibr" target="#b43">Developers [2021]</ref> between pairs of resized grayscale frames, using the Farneback algorithm. We introduced three evaluative tags centered around motion scores:</p><p>• Motion_Mean: The average motion magnitude across all frames in the clip, indicating the general level of motion. This score helps us identify clips with appropriate motion; clips with extremely low Motion_Mean values suggest static or slow motion scenes that may not effectively contribute to training models focused on dynamic content.</p><p>• Motion_Max: The maximum motion magnitude observed in the clip, highlighting instances of extreme motion or motion distortion. High Motion_Max values may indicate the presence of frames with excessive or jittery motion.</p><p>• Motion_Min: The minimum motion magnitude in the clip, identifying clips with minimal motion.</p><p>Clips with very low Motion_Min may contain idle frames or abrupt pauses, which could be undesirable for training purposes.</p><p>Video Captioning Recent studies <ref type="bibr" target="#b0">[OpenAI, 2024</ref><ref type="bibr" target="#b45">, Betker et al., 2023]</ref> have highlighted that both precision and richness of captions are crucial in enhancing the prompt-following ability and output quality of generative models.</p><p>Motivated by this, we introduced three types of caption labeling into our video captioning process by employing an in-house Vision Language Model (VLM) designed to generate both short and dense captions for video clips.</p><p>• Short Caption: The short caption provides a concise description, focusing solely on the main subject and action, closely mirroring real user prompts.</p><p>• Dense Caption: The dense caption integrates key elements, emphasizing the main subject, events, environmental and visual aspects, video type and style, as well as camera shots and movements.</p><p>To refine camera movements, we manually collected annotated data and performed SFT on our in-house VLM, incorporating common camera movements and shooting angles.</p><p>• Original Title: We also included a variety of caption styles by incorporating a portion of the original titles from the raw videos, adding diversity to the captions.</p><p>Video Concept Balancing To address category imbalances and facilitate deduplication in our dataset, we computed embeddings for all video clips using an internal VideoCLIP model and applied K-means clustering <ref type="bibr" target="#b46">MacQueen [1967]</ref> to group them into over 120,000 clusters, each representing a specific concept or category. By leveraging the cluster size and the distance to centroid tags, we balanced the dataset by filtering out clips that were outliers within their respective clusters. As part of this process, we added two new tags to each clip:</p><p>• Cluster_Cnt: The total number of clips in the cluster to which the clip belongs.</p><p>• Center_Sim: The cosine distance between the clip's embedding and the cluster center.</p><p>Video-Text Alignment Recognizing that accurate alignment between video content and textual descriptions is essential to generate high-quality output and effective data filtering, we compute a CLIP Score to measure video-text alignment. This score assesses how well the captions align with the visual content of the video clips.</p><p>• CLIP Score: We begin by uniformly sampling eight frames from the given video clip. Using the CLIP model <ref type="bibr" target="#b47">Yang et al. [2022]</ref>, we then extract image embeddings for these frames and a text embedding for the video caption. The CLIP Score is computed by averaging the cosine similarities between each frame embedding and the caption embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Post-training Data</head><p>For SFT in post-training, we curate a high-quality video dataset that captures good motion, realism, aesthetics, a broad range of concepts, and accurate captions. Inspired by <ref type="bibr" target="#b48">Dai et al. [2023]</ref>, <ref type="bibr" target="#b9">Polyak et al. [2024]</ref>, <ref type="bibr" target="#b3">Kong et al. [2025]</ref>, we utilize both automated and manual filtering techniques:</p><p>• Filtering by Video Assessment Scores: Using video assessment scores and heuristic rules, we filter the entire dataset to a subset of 30M videos, significantly improving its overall quality.</p><p>• Filtering by Video Categories: For videos within the same cluster, we use the "Distance to Centroid" values to remove those whose distance from the centroid exceeds a predefined threshold. This ensures that the resulting video subset contains a sufficient number of videos for each cluster while maintaining diversity within the subset.</p><p>• Labeling by Human Annotators: In the final stage, human evaluators assess each video for clarity, aesthetics, appropriate motion, smooth scene transitions, and the absence of watermarks or subtitles.</p><p>Captions are also manually refined to ensure accuracy and include essential details such as camera movements, subjects, actions, backgrounds, and lighting.</p><p>8 Training Strategy training stage dataset bs/node learning rate #iters #seen samples Step-1: T2I Pre-training (256px) O(1)B images 40 1e-4 53k 0.8B O(1)B images 40 1e-4 200k 3B Total 253k 3.8B Step-2: T2VI Pre-training (192px) O(1)B video clips 4 6e-5 171k 256M O(100)M video clips 4 6e-5 101k 151M O(100)M video clips 4 6e-5 158k 237M Total 430k 644M Step-2: T2VI Pre-training (540px) O(100)M video clips 2 2e-5 23k 17.3M O(10)M video clips 2 1e-5 17k 8.5M O(1)M video clips 1 1e-5 6k 1.5M Total 46k 27.3M</p><p>Table 6: Pre-training details of Step-Video-T2V. 256px, 192px, and 540px denote resolutions of 256x256, 192x320, and 544x992, respectively. A cascaded training strategy is employed in Step-Video-T2V, which mainly includes four steps: textto-image (T2I) pre-training, text-to-video/image (T2VI) pre-training, text-to-video (T2V) fine-tuning, and direct preference optimization (DPO) training. The pre-training recipe is summarized in Table 6. Step-1: T2I Pre-training In the initial step, we begin by training Step-Video-T2V with a T2I pre-training approach from scratch. We intentionally avoid starting with T2V pre-training directly, as doing so will significantly slow down model convergence. This conclusion stems from our early experiments with the T2V pre-training from scratch on the 4B model, where we observed that the model struggled to learn new concepts and was much slower to converge. By first focusing on T2I, the model can establish a solid foundation in understanding visual concepts, which can later be expanded to handle temporal dynamics in the T2V phase.</p><p>Step-2: T2VI Pre-training After acquiring spatial knowledge from T2I pre-training in Step-1,</p><p>Step-Video-T2V progresses to a T2VI joint training stage, where both T2I and T2V are incorporated. This step is further divided into two stages. In the first stage, we pre-train Step-Video-T2V using low-resolution (192x320, 192P) videos, allowing the model to primarily focus on learning motionrelated knowledge rather than fine details. In the second stage, we increase the video resolution to 544x992 (540P) and continue pre-training to enable the model to learn more intricate details. We observed that during the first stage, the model concentrates on learning motion, while in the second stage, it shifts its focus more toward learning fine details. Based on these observations, we allocate more computational resources to the first stage in Step-2 to better capture motion knowledge.</p><p>Step-3: T2V Fine-tuning Due to the diversity in pre-training video data across different domains and qualities, using a pre-trained checkpoint usually introduces artifacts and varying styles in the generated videos. To mitigate these issues, we continue the training pipeline with a T2V fine-tuning step. In this stage, we use a small number of text-video pairs and remove T2I, allowing the model to fine-tune and adapt specifically to text-to-video generation.</p><p>Similar to Movie Gen Video, we found that averaging models fine-tuned with different SFT datasets improves the quality and stability of the generated videos, outperforming the Exponential Moving Average (EMA) method. Even averaging checkpoints from the same data source enhances stability and reduces distortions. Additionally, we select model checkpoints based on the period after the gradient norm peaks, ensuring both the gradient norm and loss have decreased for improved stability.</p><p>Step-4: DPO Training As described in §4.4, video-based DPO training is employed to enhance the visual quality of the generated videos and ensure better alignment with user prompts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hierarchical Data Filtering</head><p>We apply a series of filters to the data, progressively increasing their thresholds to create six pre-training subsets for Step-2: T2VI Pre-training, as shown in Table <ref type="table">6</ref>. The final SFT dataset is then constructed through manual filtering. Figure <ref type="figure" target="#fig_9">11</ref> illustrates the key filters applied at each stage, with gray bars representing the data removed by each filter, and colored bars indicating the remaining data at each stage.</p><p>Observations from Pre-training Curve During pre-training, we observe a notable reduction in loss, which correlates with the improved quality of the training data, as illustrated in Figure <ref type="figure" target="#fig_8">10</ref>. Additionally, a sudden drop in loss occurs as the quality of the training dataset improves. This improvement is not directly driven by supervision through a loss function during model training, but rather follows human intuition (e.g., filtering via CLIP scores, aesthetic scores, etc.). While the flow matching algorithm does not impose strict requirements on the distribution of the model's input data, adjusting the training data to reflect what is considered higher-quality by humans results in a significant, stepwise reduction in training loss. This suggests that, to some extent, the model's learning process may emulate human cognitive patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bucketization for Variable Duration and Size</head><p>To accommodate varying video lengths and aspect ratios during training, we employed variable-length and variable-resolution strategies Chen et al.</p><p>[2023], <ref type="bibr" target="#b5">Zheng et al. [2024]</ref>. We defined four length buckets (1, 68, 136, and 204 frames) and dynamically adjusted the number of latent frames based on the video length. Additionally, we grouped videos into three aspect ratio buckets-landscape, portrait, and square-according to the closest height-to-width ratio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1">Benchmark and Metric</head><p>We build Step-Video-T2V-Eval, a new benchmark for assessing the quality of text-to-video models. This benchmark consists of 128 Chinese prompts sourced from real users and is designed to evaluate the quality of generated videos across 11 categories, including Sports, Food, Scenery, Animals, Festivals, Combined Concepts, Surreal, People, 3D Animation, Cinematography, and Style.</p><p>Additionally, we propose two human evaluation metrics based on Step-Video-T2V-Eval, which can be used to compare the performance of Step-Video-T2V with that of a target model:</p><p>• Metric-1 compares Step-Video-T2V with a target model by having each human annotator assign a Win/Tie/Loss label to each generated video pair from the two models for the same prompt, with the model names masked. A "Win" means Step-Video-T2V performs better than the target model, a "Loss" means it performs worse, and a "Tie" indicates the models have similar quality. • Metric-2 assigns four scores to each generated video to measure its quality across the following 4 dimensions: (1) instruction following, (2) motion smoothness, (3) physical plausibility, and (4) aesthetic appeal. The two models are then compared based on their labeled scores.</p><p>The criteria for scoring each dimension in Metric-2 are outlined below:</p><p>• Instruction Following. Score=5: The video is fully consistent with the prompt, with all elements and details generated accurately, and the expression of complex scenarios is flawless. Score=4:</p><p>The video is generally consistent with the prompt, but there are slight discrepancies in some minor details. Score=3: The video mostly meets the prompt's requirements, but there are noticeable deviations in several details or core content. Score=2: The video is clearly inconsistent with the prompt, with significant detail omissions or overall deviations. Score=1: The video is completely inconsistent with the prompt, with major scenes or subjects completely incorrect.</p><p>• Motion Smoothness. Score=5: The motion is smooth and natural, with all movements and transitions flowing seamlessly. Score=4: The motion is generally smooth, but there are occasional slight unnatural movements in certain scenes. Score=3: The motion has slight unnatural or stuttering elements, but it doesn't affect overall understanding. Score=2: The motion is unnatural or disconnected, with noticeable stuttering. Score=1: The motion is very unnatural, with frequent stuttering, making it difficult to understand.</p><p>• Physical Plausibility. Score=5: All object interactions and movements adhere to real-world physical laws, with accurate lighting, shadow, and collision effects, and smooth motion. Score=4: Most of the physical behavior is reasonable, with occasional minor unnatural collisions or lighting issues, but they don't affect the overall effect. Score=3: Several instances of object motion, lighting, or interactions conflict with physical logic, but the main actions still have a degree of coherence. Score=2: The physical behavior is unrealistic, with lighting or object interactions violating physical laws, making the scene appear unnatural. Score=1: The physical behavior is completely incorrect, with severe distortion in object interactions or lighting, making the scene difficult to understand.</p><p>Aesthetic Appeal. Score=5: Highly captivating, deeply moving, with significant artistic value and visual appeal. Score=4: Pleasant and engaging, effectively capturing the audience's attention with good visual value. Score=3: Somewhat appealing, but overall performance is mediocre and doesn't leave a lasting impression. Score=2: Average, lacking in appeal, and may cause the audience to lose interest. Score=1: Unpleasant, lacking in appeal, and the overall effect is disappointing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">Comparisons to Open-source Model</head><p>We first compare Step-Video-T2V with HunyuanVideo on Step-Video-T2V-Eval.</p><p>Step  Table <ref type="table">8</ref>: Comparison with HunyuanVideo using Metric-2. We invited three human annotators to evaluate each video. For each category and evaluation dimension, we aggregated the scores given by all annotators across all prompts within the category for that dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.3">Comparisons to Commercial Model</head><p>We then compare Step-Video-T2V with two leading text-to-video engines in China, T2VTopA (2025-02-10 version) and T2VTopB (2025-02-10 version), on Step-Video-T2V-Eval.</p><p>Step Table <ref type="table">9</ref>: Comparison with T2VTopA using Metric-1. A total of 126 prompts were evaluated, rather than 128, as T2VTopA rejected 2 prompts.</p><p>From Table <ref type="table">9</ref>, Table <ref type="table" target="#tab_12">10</ref>, and Table <ref type="table">11</ref> we got three observations.</p><p>First, the overall ranking of the three models in Table <ref type="table">9</ref> and Table <ref type="table" target="#tab_12">10</ref> is as follows: T2VTopA &gt;</p><p>Step-Video-T2V &gt; T2VTopB. We analyzed categories such as Scenery, Animals, People, and Style, where Step-Video-T2V performs worse than the other two models, and found that the primary reason lies in their generally higher aesthetic appeal. We believe this advantage mainly stems from the higher resolutions of the generated videos (720P in T2VTopA, 1080P in T2VTopB, and 540P in</p><p>Step-Video-T2V) and the high-quality aesthetic data used during their post-training stages. Table <ref type="table">11</ref> also shows that 4 out of 6 annotators rate T2VTopA and T2VTopB as having higher aesthetic appeal.</p><p>Second, Step-Video-T2V consistently outperforms T2VTopA and T2VTopB in the Sports category in Table <ref type="table">9</ref> and Table <ref type="table" target="#tab_12">10</ref>, demonstrating its strong capability in modeling and generating videos with high-motion dynamics.</p><p>Table 11 also highlights Step-Video-T2V's superiority in Motion Smoothness and Physical Plausibility.</p><p>Third, we observed that T2VTopA has better instruction-following capability, which contributes to its superior performance in categories such as Combined Concepts, Surreal, and Cinematography. We believe the key reasons for this are better video captioning model and the greater human effort involved in labeling the post-training data used by T2VTopA.</p><p>Step-Video-T2V vs. T2VTopB (Win-Tie-Loss)</p><p>Annotator  Table <ref type="table">11</ref>: Comparison with T2VTopA and T2VTopB using Metric-2. We invited six human annotators to evaluate each video. For each evaluation dimension, we aggregated the scores given by each annotator across all prompts for that dimension. Prompts that were rejected by any model were excluded from the analysis for all models.</p><p>Note that Step-Video-T2V still lacks sufficient training in the final stage of pre-training with 540P videos, having only seen 25.3M samples (as shown in Table <ref type="table">6</ref>). Additionally, compared to these two commercial engines, we are using significantly less high-quality data in the post-training phase, which will be continuously improved in the future. Finally, the video length is 204 frames, nearly twice the length of T2VTopA and T2VTopB, making our training more challenging. We assert that Step-Video-T2V has already achieved the strongest motion dynamics modeling and generation capabilities among all commercial engines. Given comparable training resources and high-quality data, we believe it can achieve state-of-the-art results in general domains as well.</p><p>We also compare Step-Video-T2V with the international commercial text-to-video engine, Runway Gen-3 Alpha, and present the results in Table <ref type="table" target="#tab_13">12</ref> and <ref type="table" target="#tab_5">Table 13</ref>. Since Gen-3 Alpha has a stronger understanding of English, we translate the Chinese prompts into English before generating results.</p><p>As shown in Table <ref type="table" target="#tab_13">12</ref> and <ref type="table" target="#tab_5">Table 13,</ref> Step-Video-T2V outperforms Gen-3 Alpha overall, while Gen-3 Alpha excels in generating videos within the Cinematography domain.</p><p>Step-Video-T2V vs. Gen-3 Alpha (Win-Tie-Loss)</p><p>Annotator-1 Annotator-2 Annotator-3</p><p>Overall 68-3-38 60-27-25 54-36-22 Sports 10-0-2 11-1-0 6-5-1 Food 10-0-1 7-2-2 5-3-3 Scenery 7-2-3 7-2-3 7-1-4 Animals 7-1-4 7-3-2 4-7-1 Festivals 7-0-4 6-5-0 2-9-0 Combined Concepts 6-1-5 4-4-4 9-0-3 Surreal 1-1-4 2-1-3 6-0-0 People 5-1-6 5-3-4 7-3-2 3D Animation 1-0-0 1-0-0 0-1-0 Cinematography 4-0-7 2-3-6 3-3-5 Style 10-0-2 8-3-1 5-4-3 Table <ref type="table" target="#tab_5">13</ref>: Comparison with Runway Gen-3 Alpha using Metric-2. We invited six human annotators to evaluate each video. For each evaluation dimension, we aggregated the scores given by each annotator across all prompts for that dimension. Prompts that were rejected by Gen-3 Alpha were excluded from the analysis for all models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.4">Evaluation on Movie Gen Video Bench</head><p>Movie Gen Video Bench <ref type="bibr" target="#b9">Polyak et al. [2024]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.5">Generating Text Content in Videos</head><p>We also compare Step-Video-T2V with open-source and commercial engines on a list of prompts such as "a squirrel holding a sign that says 'hello'.", where the model is required to generate videos that include text content as well.</p><p>Our observations show that Step-Video-T2V outperforms all other models in generating basic English text. We attribute this capability to the T2I pre-training stage, where a portion of the images contained text, and the captions explicitly described it. However, the accuracy of text generation remains far from ideal. Furthermore, due to the complexity of Chinese characters, Step-Video-T2V is currently able to generate only a limited number of them. Enhancing text generation capabilities for both English and Chinese will be a focus of our future work.</p><p>Figure <ref type="figure" target="#fig_1">12</ref>: Four frames sampled from the video generated based on the prompt "In the video, a Chinese girl is dressed in an exquisite traditional outfit, smiling with a confident and graceful expression. She holds a piece of paper with the words "we will open source" clearly written on it.</p><p>The background features an ancient and elegant setting, complementing the girl's demeanor. The entire scene is clear and has a realistic style.".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.6">VAE Video Reconstruction</head><p>Model Downsample Factor SSIM↑ PSNR↑ rFVD↓ OpenSora-1.2 (Zheng et al. [2024]) 4 × 8 × 8 0.9126 31.41 20.42 CogvideoX-1.5 (Yang et al. [2024a]) 4 × 8 × 8 0.9373 38.10 16.33 HunyuanVideo (Kong et al. [2025]) 4 × 8 × 8 0.9710 39.56 4.17 Cosmos-VAE (Nvidia [2025]) 4 × 8 × 8 0.9315 37.66 9.10 Cosmos-VAE (Nvidia [2025]) 8 × 16 × 16 0.8862 34.82 40.33 Video-VAE (Ours) 8 × 16 × 16 0.9776 39.37 3.61</p><p>Table 15: Comparison of reconstruction metrics.</p><p>We compare Video-VAE with several open-source baselines using 1,000 test videos from various domains, each with dimensions of 50(frames)×480(height)×768(width). As shown in Table <ref type="table">15</ref>, despite having a compression ratio 8 times larger than most baselines, our reconstruction quality still maintains state-of-the-art performance. While Cosmos-VAE also offers a high-compression version with a factor of 8×16×16, its reconstruction quality falls significantly behind our method.</p><p>Figure <ref type="figure" target="#fig_2">13</ref> illustrates typical challenge cases in video reconstruction, including high-motion (first row), text (second row), texture (third row), high-motion combined with text (fourth row), and high-motion combined with texture (fifth row). Our models significantly outperform other baselines, even with higher compression ratios.</p><p>Figure <ref type="figure" target="#fig_2">13</ref>: Video reconstruction results compared with public available models, in scenarios including high-motion (first row), text (second row), texture (third row), high-motion combined with text (fourth row), and high-motion combined with texture (fifth row).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.7">DPO</head><p>To assess the effectiveness of the proposed Video-DPO algorithm, we conduct inference on 300 diverse prompts. The evaluation involves two models: the baseline model and the baseline model with the Video-DPO enhancement (baseline w/. DPO). Both models are sampled under identical initial noise conditions to control for extraneous variables and ensure a fair comparison. For each generated video, three independent annotators are tasked with evaluating their preference between the two models, with an option to select "no preference". The evaluation protocol is as follows:</p><p>• • If an annotator prefers the video generated by "baseline w/. DPO", the model receives 1 point.</p><p>• If an annotator prefers the "baseline" video, the baseline model receives 1 point.</p><p>• If an annotator indicates "no preference," both models receive 0.5 points.</p><p>Upon aggregating the scores, we find that the baseline model with DPO (baseline w/. DPO) achieves a preference score of 55%, outperforming the baseline model (45%). This result demonstrates the efficacy of Video-DPO in generating videos more aligned with user preferences. The visual comparison is shown in Figure <ref type="figure" target="#fig_14">14</ref>, demonstrates that human feedback enhances the plausibility and consistency of generated videos. Additionally, we observe that the DPO baseline enhances the alignment with the given prompts, resulting in more accurate and relevant video generation.</p><p>While Video-DPO demonstrates effectiveness, several issues remain.</p><p>(1) The trajectory from initial noise to timestep-specific latents acts as implicit dynamic conditions beyond text prompts -yet this dimension remains underutilized due to computational limitations. (2) A tradeoff exists between sparse and imprecise feedback, especially in video diffusion models. For instance, in videos with over 100 million pixels, only a few pixels may be problematic, yet feedback often comes as a single scalar or lacks precision.</p><p>(3) Unlike LLMs, which use token-level softmax to create competition between tokens, diffusion models rely on regression, which may result in less efficient preference optimization. We hope these discussions provide insights and inspire further algorithmic advancements in incorporating human feedback.  Unlike DiT, which relies on a modulation mechanism to condition the network on the text prompt, MMDiT integrates the text prompt directly into the Transformer, separates the weights for text and video, and uses a shared attention mechanism to merge the latent representations of both modalities. We compared the training curves of DiT and MMDiT in the early stages and found both architectures exhibited similar performance. Given these comparable results, along with DiT's ability to disentangle text and video and its natural extension to pure video prediction models without text, we ultimately selected DiT as the model architecture for Step-Video-T2V. Due to computational cost constraints, we did not train MMDiT-based model for an extended period to assess its upper performance limit.</p><p>We also compared spatial-temporal attention and 3D full attention mechanisms within DiT. In the spatial-temporal attention mechanism, the model captures spatial information among tokens with the same temporal index within spatial Transformer blocks, and temporal information across time steps in temporal Transformer blocks. In contrast, 3D full attention mechanism combines both spatial and temporal information in a unified attention process, offering higher performance potential but at the cost of increased computational demands. We trained two DiT models-one using spatialtemporal attention and the other using 3D full attention-both in a 4B setting. Upon comparing their performances, we found that 3D full attention-based model outperforms spatial-temporal attentionbased model, particularly in generating videos with high motion dynamics. Given its superior quality, we ultimately selected the 3D full attention setting.</p><p>In addition, 3D full attention is known for its high training and inference cost, so we are still actively investigating more efficient way to reduce the computation overhead, while preserving the same model quality <ref type="bibr" target="#b49">Tan et al. [2025]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.2">Instruction Following</head><p>Based on the evaluation results, we found that even a DiT-based model like Step-Video-T2V, with 30B parameters, struggles to generate videos involving complex action sequences. Additionally, generating videos that incorporate multiple concepts with low occurrence in the training data (e.g., an elephant and a penguin) remains challenging in Step-Video-T2V and other leading text-to-video generation models. Both of these challenges can be viewed as instruction-following problems.</p><p>We examine the instruction-following capability of Step-Video-T2V, focusing on how it interprets instructions involving various objects, actions, and other details. Our analysis reveals that the distribution of cross-attention scores is occasionally highly concentrated, with a strong focus on specific objects or actions. This pronounced attention can result in missing objects, wrong details, or incomplete action sequences in the generated videos.</p><p>By heuristically repeating the missing objects in the prompt, some of the problematic cases can be significantly improved. This demonstrates the importance of ensuring that all elements in the prompt receive appropriate attention. We leave the task of balancing this attention for future work, aiming to refine the model's ability to better attend and follow all elements in the prompt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.3">Laws of Physics Following</head><p>We analyzed a number of videos generated by leading text-to-video models, including Sora, Hailuo, Kling, and Step-Video-T2V, and found that all of these models struggle to accurately simulate the real world and generate videos that adhere to the laws of physics-such as a ball bouncing on the floor or a drop of water falling into a cup. Some text-to-video engines can produce good results for certain prompts, but these successes are often due to the model over-fitting to specific annotations, and cannot generalize well.</p><p>This finding highlights a key limitation of diffusion-based models in text-to-video generation. To address this challenge, we plan to develop more advanced model paradigms in future work, such as combining autoregressive and diffusion models within a unified framework <ref type="bibr">(Chen et al. [2024b]</ref>, <ref type="bibr" target="#b51">HaCohen et al. [2024]</ref>, <ref type="bibr" target="#b52">Zhou et al. [2025]</ref>), to better adhere to the laws of physics and more accurately simulate realistic interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.4">High-quality Labeled Data for Post-training</head><p>By applying a small amount of high-quality human-labeled data in SFT, Step-Video-T2V achieves significant improvements in the overall video quality, demonstrating that the quality and diversity of the data outweigh its sheer scale. We also observed that certain characteristics of these curated high-quality datasets, such as video style and the degree of motion dynamics, generalize well across a broader range of prompts. This further underscores the importance of high-quality, small-scale, and diverse datasets for post-training.</p><p>Curating such datasets is both expensive and time-consuming, involving tasks such as selecting high-quality videos from a large pool, labeling them with accurate captions, and ensuring the dataset covers a diverse range of objects, actions, styles, and domains. We plan to build a comprehensive video knowledge base with structured labels as part of our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.5">RL-based Optimization Mechanism for Post-training</head><p>We employed a simple yet effective DPO-based model for video generation and also explored training a reward model to automate the entire post-training process. However, the proposed method still requires human labeling efforts in the early stages and is time-consuming when extending it to general domains. On the other hand, RL-based approaches have achieved great success in LLMs, such as OpenAI-O1 and DeepSeek-R1 <ref type="bibr" target="#b53">DeepSeek-AI et al. [2025]</ref>. However, unlike RL-focused natural language tasks, such as solving math problems or generating code, which have well-defined problems with clear answers, it remains challenging to define similar tasks in the video generation domain. We consider this a key challenge for future research exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11">Conclusion and Future Work</head><p>This technical report introduces and open-sources Step-Video-T2V, a state-of-the-art pre-trained video generation model from text, featuring 30B parameters, a deep compression Video-VAE, a DPO approach for video generation, and the ability to generate videos up to 204 frames in length. We provide a comprehensive overview of our pre-training and post-training strategies and introduce</p><p>Step-Video-T2V-Eval as a new benchmark for evaluating text-to-video generation models.</p><p>We highlight challenges faced by current text-to-video models. First, high-quality labeled data remains a significant hurdle. Existing video captioning models often struggle with hallucination issues, and human annotations are expensive and difficult to scale. Second, instruction-following requires more attention, as it encompasses a wide range of scenarios, from generating videos based on detailed descriptions to handling complex action sequences and combinations of multiple concepts. Third, current models still face difficulties in generating videos that obey the laws of physics, an issue stemming from the inherent limitations of diffusion models. Lastly, RL-based optimization mechanisms are areas worth exploring for post-training improvements in video generation models.</p><p>Looking ahead, we plan to launch a series of open-source projects focused on the development of video foundation models, starting with Step-Video-T2V. We hope these efforts will drive innovation in video foundation models and empower video content creators.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Architecture overview of Step-Video-T2V. Videos are represented by a high-compression Video-VAE, achieving 16x16 spatial and 8x temporal compression ratios. User prompts are encoded using two bilingual pre-trained text encoders to handle both English and Chinese. A DiT with 3D full attention is trained using Flow Matching and is employed to denoise input noise into latent frames, with text embeddings and timesteps serving as conditioning factors. To further enhance the visual quality of the generated videos, a video-based DPO approach is applied, which effectively reduces artifacts and ensures smoother, more realistic video outputs.</figDesc><graphic coords="4,108.00,72.00,396.02,215.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Architecture overview of Video-VAE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The model architecture of our bilingual text encoder and DiT with 3D Attention.</figDesc><graphic coords="7,108.00,71.99,395.91,320.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Overall pipeline of incorporating human feedback.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><figDesc>Figure 5: We generate different samples with same prompt ("A ballet dancer practicing in the dance studio" in this case), and annotate these samples as non-preferred (a) or preferred (b).</figDesc><graphic coords="10,49.23,169.00,128.69,80.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7</head><label>7</label><figDesc>Figure7shows the overall workflow of Step-Video-T2V training system. The workflow comprises several stages. The offline stage, based on our in-house training emulator (Step Emulator, §6.2), is specifically designed to estimate and determine optimal resource allocation and training parallelism strategies. This determination is achieved through systematic analysis of model architectures and resource configurations provided as input parameters. Next, with the theoretical optimal resource allocation plan, we deploy the training job with GPUs allocated in the training and inference clusters, respectively. The training clusters, responsible for training video DiT, uses the parallelization strategy recommended by the emulator, which has been specifically optimized to maximize the Model Flops Utilization (MFU). The other side with VAE and Text Encoder, runs on the inference clusters, and constantly provides the processed input data (pairs of image/video latent and text embedding) needed for DiT training. Data transmission between clusters is facilitated by StepRPC ( §6.3), our highperformance RPC framework that seamlessly integrates both TCP and RDMA protocols, enabling efficient cross-cluster communication with robust fault tolerance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Load balancing with hybrid granularity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: The pipeline of Step-Video-T2V data process.</figDesc><graphic coords="18,48.60,72.00,514.79,201.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Training curve of different training stages, where s i denotes the i th dataset used in the corresponding stage.</figDesc><graphic coords="21,207.00,72.00,198.00,148.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Hierarchical data filtering for pre-training and post-training.</figDesc><graphic coords="22,48.60,72.00,514.79,174.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><figDesc>a close-up of two chameleons under the night sky in a realistic style. (a) Videos generated with the DPO baseline show greater realism and improved consistency. (b) DPO baseline videos align better.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Visual comparison of video generation with and without the DPO baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Estimated MFU from SEMU of different parallelism strategies under 540P video pre-training stage.</figDesc><table><row><cell cols="6">TP CP PP VPP Checkpointing (%) MFU</cell></row><row><cell>4</cell><cell>1</cell><cell>2 4</cell><cell>24 24</cell><cell>93.75 93.75</cell><cell>35.90 35.71</cell></row><row><cell></cell><cell></cell><cell>1</cell><cell>1</cell><cell>83.33</cell><cell>35.59</cell></row><row><cell></cell><cell>1</cell><cell>2</cell><cell>24</cell><cell>72.91</cell><cell>36.06</cell></row><row><cell></cell><cell></cell><cell>4</cell><cell>12</cell><cell>72.91</cell><cell>35.76</cell></row><row><cell>8</cell><cell>2</cell><cell>1 4</cell><cell>1 12</cell><cell>62.50 31.25</cell><cell>31.79 35.11</cell></row><row><cell></cell><cell>3</cell><cell>1 4</cell><cell>1 12</cell><cell>31.25 11.53</cell><cell>33.41 36.47</cell></row></table><note><p>To minimize TP overhead, we have developed StepCCL, a proprietary collective communication library that implements advanced communication-computation overlap techniques.</p><p>StepCCL directly utilizes the DMA engine for data transmission, completely bypassing the Stream</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Table 3 outlines the FLOPs per sample of different resolutions. FLOPs per sample of different resolutions.</figDesc><table><row><cell cols="2">Resolution (F, H, W) TFLOPs per sample</cell></row><row><cell>204 × 256 × 256</cell><cell>1,717.20</cell></row><row><cell>204 × 192 × 320</cell><cell>1,592.61</cell></row><row><cell>136 × 256 × 256</cell><cell>1,079.85</cell></row><row><cell>136 × 192 × 320</cell><cell>1,004.89</cell></row><row><cell>68 × 256 × 256</cell><cell>509.31</cell></row><row><cell>68 × 192 × 320</cell><cell>475.87</cell></row><row><cell>1 × 256 × 256</cell><cell>44.99</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Table4shows more detailed statistics.Over a month of Step-Video-T2V training, fatal hardware failures occurred only 7 times.GPU Machine High Quality EnsuranceTraining GPU nodes exhibit significant quality variations, i.e., their failure probabilities differ substantially. Some servers have much higher failure risks than others, necessitating the selection of the most reliable servers for large-scale training tasks to minimize the job interruptions. We developed an innovative node quality assessment framework that systematically integrates historical alert patterns, maintenance logs, stress test results, and load test durations to generate comprehensive quality scores. When node failures occur within production resource pools, replacement units are selectively deployed from a dedicated buffer pool following</figDesc><table><row><cell>Fault</cell><cell>Category</cell><cell>Count</cell></row><row><cell>GPU_DBE_ERROR</cell><cell>GPU</cell><cell>3</cell></row><row><cell>GPU_LOCKED</cell><cell>GPU</cell><cell>1</cell></row><row><cell>LINK_DOWN</cell><cell>Network</cell><cell>1</cell></row><row><cell>NODE_SHUTDOWN</cell><cell>Host</cell><cell>2</cell></row><row><cell>SOFTWARE_FAULT</cell><cell>Software</cell><cell>11</cell></row><row><cell>CUDA_OOM</cell><cell>Software</cell><cell>7</cell></row><row><cell>NON_FATAL</cell><cell>Hardware</cell><cell>4</cell></row></table><note><p>a prioritized matching rule: buffer machines' quality scores must meet or exceed the operational requirements of the target resource pool's priority tier. This methodology has achieved a statistically significant reduction in failure rates for critical resource pools (i.e., video pool) from an original monthly average of 7.0% to 0.9%. Correspondingly, the daily restart rate per 1,000 GPUs caused by hardware issues decreased to approximately 1/11 of that reported in LLaMA3.1[LlamaTeam, 2024]</p><p>.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Comparison with HunyuanVideo using Metric-1.From Table7and Table8we got three observations. First, Step-Video-T2V demonstrates state-of-the-art performance as the strongest open-source text-tovideo generation model to date. This success is attributed to multiple factors, including the model's structural design and its pre-training and post-training strategies. Second, in some categories like Animals, Step-Video-T2V performs worse than HunyuanVideo, as shown in Table7. This is primarily due to aesthetic issues, as verified by the Aesthetic Appeal score in Table8. Third, Video-VAE achieves compression ratios of 16x16 spatial and 8x temporal, compared to HunyuanVideo's 8x8 spatial and 4x temporal compression. This higher compression rate enables Step-Video-T2V to generate videos up to 204 frames, nearly double the 129-frame maximum of HunyuanVideo.</figDesc><table><row><cell>Step-Video-T2V vs. HunyuanVideo</cell><cell>Instruction Following</cell><cell>Motion Smoothness</cell><cell>Physical Plausibility</cell><cell>Aesthetic Appeal</cell></row><row><cell>Overall</cell><cell>1,273-1,221</cell><cell>1,407-1,327</cell><cell>1,417-1,238</cell><cell>1,312-1,238</cell></row><row><cell>Sports</cell><cell>130-111</cell><cell>120-104</cell><cell>113-99</cell><cell>110-98</cell></row><row><cell>Food</cell><cell>85-92</cell><cell>110-97</cell><cell>107-93</cell><cell>111-90</cell></row><row><cell>Scenery</cell><cell>130-129</cell><cell>139-126</cell><cell>134-120</cell><cell>125-122</cell></row><row><cell>Animals</cell><cell>104-106</cell><cell>123-114</cell><cell>110-107</cell><cell>99-108</cell></row><row><cell>Festivals</cell><cell>102-91</cell><cell>110-102</cell><cell>97-90</cell><cell>103-94</cell></row><row><cell>Combined Concepts</cell><cell>132-115</cell><cell>139-136</cell><cell>139-135</cell><cell>118-115</cell></row><row><cell>Surreal</cell><cell>99-101</cell><cell>138-139</cell><cell>135-134</cell><cell>125-126</cell></row><row><cell>People</cell><cell>115-117</cell><cell>129-129</cell><cell>148-150</cell><cell>115-112</cell></row><row><cell>3D Animation</cell><cell>113-109</cell><cell>137-133</cell><cell>149-146</cell><cell>139-135</cell></row><row><cell>Cinematography</cell><cell>121-117</cell><cell>121-122</cell><cell>132-133</cell><cell>116-115</cell></row><row><cell>Style</cell><cell>142-133</cell><cell>141-125</cell><cell>153-134</cell><cell>151-123</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 :</head><label>10</label><figDesc>Comparison with T2VTopB using Metric-1. A total of 122 prompts were evaluated, rather than 128, as T2VTopB rejected 6 prompts.</figDesc><table><row><cell></cell><cell>Model</cell><cell>Instruction Following</cell><cell>Motion Smoothness</cell><cell>Physical Plausibility</cell><cell>Aesthetic Appeal</cell></row><row><cell></cell><cell>Step-Video-T2V</cell><cell>204</cell><cell>210</cell><cell>203</cell><cell>187</cell></row><row><cell>Annotator-1</cell><cell>T2VTopA</cell><cell>211</cell><cell>200</cell><cell>198</cell><cell>196</cell></row><row><cell></cell><cell>T2VTopB</cell><cell>185</cell><cell>184</cell><cell>178</cell><cell>175</cell></row><row><cell></cell><cell>Step-Video-T2V</cell><cell>211</cell><cell>243</cell><cell>256</cell><cell>217</cell></row><row><cell>Annotator-2</cell><cell>T2VTopA</cell><cell>241</cell><cell>243</cell><cell>242</cell><cell>228</cell></row><row><cell></cell><cell>T2VTopB</cell><cell>234</cell><cell>236</cell><cell>229</cell><cell>204</cell></row><row><cell></cell><cell>Step-Video-T2V</cell><cell>170</cell><cell>197</cell><cell>172</cell><cell>178</cell></row><row><cell>Annotator-3</cell><cell>T2VTopA</cell><cell>177</cell><cell>177</cell><cell>153</cell><cell>171</cell></row><row><cell></cell><cell>T2VTopB</cell><cell>164</cell><cell>163</cell><cell>139</cell><cell>148</cell></row><row><cell></cell><cell>Step-Video-T2V</cell><cell>199</cell><cell>232</cell><cell>230</cell><cell>225</cell></row><row><cell>Annotator-4</cell><cell>T2VTopA</cell><cell>217</cell><cell>221</cell><cell>201</cell><cell>199</cell></row><row><cell></cell><cell>T2VTopB</cell><cell>194</cell><cell>219</cell><cell>194</cell><cell>194</cell></row><row><cell></cell><cell>Step-Video-T2V</cell><cell>218</cell><cell>225</cell><cell>213</cell><cell>211</cell></row><row><cell>Annotator-5</cell><cell>T2VTopA</cell><cell>221</cell><cell>220</cell><cell>213</cell><cell>212</cell></row><row><cell></cell><cell>T2VTopB</cell><cell>209</cell><cell>217</cell><cell>202</cell><cell>196</cell></row><row><cell></cell><cell>Step-Video-T2V</cell><cell>187</cell><cell>213</cell><cell>251</cell><cell>211</cell></row><row><cell>Annotator-6</cell><cell>T2VTopA</cell><cell>193</cell><cell>201</cell><cell>259</cell><cell>197</cell></row><row><cell></cell><cell>T2VTopB</cell><cell>201</cell><cell>224</cell><cell>271</cell><cell>227</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 12 :</head><label>12</label><figDesc>Comparison with Runway Gen-3 Alpha using Metric-1, excluding prompts that were rejected by Gen-3 Alpha.</figDesc><table><row><cell></cell><cell>Model</cell><cell>Instruction Following</cell><cell>Motion Smoothness</cell><cell>Physical Plausibility</cell><cell>Aesthetic Appeal</cell></row><row><cell>Annotator-1</cell><cell>Step-Video-T2V Gen-3 Alpha</cell><cell>214 178</cell><cell>221 180</cell><cell>214 150</cell><cell>198 169</cell></row><row><cell>Annotator-2</cell><cell>Step-Video-T2V Gen-3 Alpha</cell><cell>183 185</cell><cell>200 173</cell><cell>210 177</cell><cell>173 176</cell></row><row><cell>Annotator-3</cell><cell>Step-Video-T2V Gen-3 Alpha</cell><cell>174 179</cell><cell>202 180</cell><cell>176 158</cell><cell>184 194</cell></row><row><cell>Annotator-4</cell><cell>Step-Video-T2V Gen-3 Alpha</cell><cell>162 147</cell><cell>186 165</cell><cell>189 133</cell><cell>183 160</cell></row><row><cell>Annotator-5</cell><cell>Step-Video-T2V Gen-3 Alpha</cell><cell>228 200</cell><cell>237 189</cell><cell>225 149</cell><cell>211 166</cell></row><row><cell>Annotator-6</cell><cell>Step-Video-T2V Gen-3 Alpha</cell><cell>160 178</cell><cell>178 161</cell><cell>207 182</cell><cell>171 153</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 14 :</head><label>14</label><figDesc>is another existing benchmark for the text-to-video generation task. It includes 1,003 prompts across multiple categories, covering human activities, animals, nature and scenery, physics, as well as unusual subjects and activities. Although Movie Gen Video has not been open-sourced, its generated results on the Movie Gen Video Bench are publicly available (https://github.com/facebookresearch/MovieGenBench). Therefore, we also compare Step-Video-T2V with Movie Gen Video and HunyuanVideo in Table14using this benchmark.Compared to Movie Gen Video, Step-Video-T2V achieves a comparable performance. We got several observations from this comparison. First, the pre-training of Step-Video-T2V remains insufficient. While Movie Gen Video was trained on 73.8M videos during its high-resolution pre-training phase,Step-Video-T2V was trained on only 27.3M videos-about one-third of the number used by Movie Gen Video. Additionally, we observed that the training curves for all pre-training stages in Step-Video-T2V continue to show a downward trend. Due to resource limitations, we plan to conduct more extensive pre-training as part of our future work. Second, the Movie Gen Video paper highlights the significant human effort involved in labeling the high-quality SFT dataset. However, due to limited human resources, we lack enough high-quality labeled data at this stage to effectively refine the visual style and quality of the generated results. Third, Movie Gen Video can generate 720P videos, which are visually more appealing than the 540P resolution produced by Step-Video-T2V. Feedback from human annotators suggests that high resolution can often be a key factor in determining which model Comparison of Movie Gen Video and HunyuanVideo using the Movie Gen Video Bench. The total number of evaluations (1,289) is greater than 1,003 due to some prompts having multiple category tags. This evaluation involved six human annotators. performs better. Compared to HunyuanVideo, Step-Video-T2V achieves significant improvements across all categories, solidifying its position as the state-of-the-art open-source text-to-video model.</figDesc><table><row><cell>Category</cell><cell>Step-Video-T2V vs. Movie Gen Video (Win-Tie-Loss)</cell><cell>Step-Video-T2V vs. HunyuanVideo (Win-Tie-Loss)</cell><cell># of Prompts</cell></row><row><cell>Overall</cell><cell>485-315-489</cell><cell>615-313-361</cell><cell>1,289</cell></row><row><cell>human</cell><cell>123-58-160</cell><cell>181-64-96</cell><cell>341</cell></row><row><cell>physics</cell><cell>61-54-64</cell><cell>87-47-45</cell><cell>179</cell></row><row><cell>unusual activity &amp; subject</cell><cell>110-74-108</cell><cell>136-75-81</cell><cell>292</cell></row><row><cell>animal</cell><cell>39-37-42</cell><cell>47-30-41</cell><cell>118</cell></row><row><cell>scene</cell><cell>84-53-63</cell><cell>91-58-51</cell><cell>200</cell></row><row><cell>sequential motion</cell><cell>9-2-2</cell><cell>6-2-5</cell><cell>13</cell></row><row><cell>camera motion</cell><cell>59-37-50</cell><cell>67-37-42</cell><cell>146</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributors and Acknowledgments</head><p>We designate core contributors as those who have been involved in the development of Step-Video-T2V throughout its entire process, while contributors are those who worked on the early versions or contributed part-time. All contributors are listed in alphabetical order by first name. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Video generation models as world simulators</title>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<ptr target="https://openai.com/index/video-generation-models-as-world-simulators" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><surname>Deepmind</surname></persName>
		</author>
		<author>
			<persName><surname>Veo</surname></persName>
		</author>
		<ptr target="https://klingai.kuaishou.com" />
		<imprint>
			<date type="published" when="2024-02">2. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><surname>Minimax</surname></persName>
		</author>
		<author>
			<persName><surname>Hailuo</surname></persName>
		</author>
		<ptr target="https://hailuoai.com/video" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Hunyuanvideo: A systematic framework for large video generative models</title>
		<author>
			<persName><forename type="first">Weijie</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rox</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuozhuo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangfeng</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathrina</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junkun</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanxin</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aladdin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changlin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duojun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongmei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianbing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinbao</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joey</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinchi</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutao</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanbo</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhentao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zixiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zunnan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangyu</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinglin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dax</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongfa</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caesar</forename><surname>Zhong</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2412.03603" />
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Cogvideox: Text-to-video diffusion models with an expert transformer</title>
		<author>
			<persName><forename type="first">Zhuoyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayan</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wendi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiazheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyi</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanyu</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.06072</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Open-sora: Democratizing efficient video production for all</title>
		<author>
			<persName><forename type="first">Zangwei</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianji</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenhui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenggui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<ptr target="https://github.com/hpcaitech/Open-Sora" />
		<imprint>
			<date type="published" when="2024-03">March 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Open-sora plan: Open-source large video generation model</title>
		<author>
			<persName><forename type="first">Bin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunyang</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinhua</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongjian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaodong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianyi</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenghai</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liuhan</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.00131</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Scaling rectified flow transformers for high-resolution image synthesis</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumith</forename><surname>Kulal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahim</forename><surname>Entezari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harry</forename><surname>Saini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yam</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Axel</forename><surname>Sauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederic</forename><surname>Boesel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Podell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dockhorn</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2403.03206" />
		<imprint>
			<date type="published" when="2024">2024</date>
			<pubPlace>Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Scalable diffusion models with transformers</title>
		<author>
			<persName><forename type="first">William</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2212.09748" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Adam</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Zohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andros</forename><surname>Tjandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Animesh</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ann</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chih-Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ching-Yao</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingkang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geet</forename><surname>Sethi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guan</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiran</forename><surname>Jagadeesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luxin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitesh</forename><forename type="middle">Kumar</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Duval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roshan</forename><surname>Sumbaly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sai</forename><surname>Saketh Rambhatla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samaneh</forename><surname>Azadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samyak</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharadh</forename><surname>Ramaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shelly</forename><surname>Sheynin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simran</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingbo</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Ning</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaniv</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaqiao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yen-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Chiao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Kirstain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zecheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Pumarola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Artsiom</forename><surname>Sanakoyeu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baishan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Araya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Breena</forename><surname>Kerr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carleigh</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitry</forename><surname>Vengertsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edgar</forename><surname>Schonfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elliot</forename><surname>Blanchard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Juefei-Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fraylie</forename><surname>Nord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Kohler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaolin</forename><surname>Fire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Sivakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luya</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markos</forename><surname>Georgopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rashel</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><forename type="middle">K</forename><surname>Sampson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shikai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Parmeggiani</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2410.13720" />
		<imprint>
			<date type="published" when="2024">2024</date>
			<publisher>Vladan Petrovic</publisher>
			<pubPlace>Steve Fine, Tara Fowler</pubPlace>
		</imprint>
	</monogr>
	<note>and Yuming Du. Movie gen: A cast of media foundation models</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Language model beats diffusion -tokenizer is key to visual generation</title>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Lezama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitesh</forename><surname>Bharadwaj Gundavarapu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Versari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Minnen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuye</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boqing</forename><surname>Alexander G Hauptmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irfan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><surname>Jiang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=gzqrANCF4g" />
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Cosmos world foundation model platform for physical ai</title>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2501.03575</idno>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Wf-vae: Enhancing video vae by wavelet-driven energy flow for latent video diffusion model</title>
		<author>
			<persName><forename type="first">Zongjian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liuhan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinhua</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenghai</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2411.17459</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep compression autoencoder for efficient high-resolution diffusion models</title>
		<author>
			<persName><forename type="first">Junyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junsong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=wH8XXUOUZU" />
	</analytic>
	<monogr>
		<title level="m">The Thirteenth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Flow matching for generative modeling</title>
		<author>
			<persName><forename type="first">Yaron</forename><surname>Lipman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricky</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heli</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Le</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2210.02747" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Hunyuan-dit: A powerful multi-resolution diffusion transformer with fine-grained chinese understanding</title>
		<author>
			<persName><forename type="first">Zhimin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangfeng</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanxin</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinchi</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingfang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingchao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minbin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zedong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dayou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyue</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongwei</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxiang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiabin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxiao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jihong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinbao</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangyu</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sihuan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingtao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinglin</forename><surname>Lu</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2405.08748" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Train short, test long: Attention with linear biases enables input length extrapolation</title>
		<author>
			<persName><forename type="first">Ofir</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2108.12409" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis</title>
		<author>
			<persName><forename type="first">Junsong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jincheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongjian</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongdao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2310.00426" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Roformer: Enhanced transformer with rotary position embedding</title>
		<author>
			<persName><forename type="first">Jianlin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengfeng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Murtadha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfeng</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2104.09864" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Training language models to follow instructions with human feedback</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carroll</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarina</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="27730" to="27744" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning from human preferences</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Paul F Christiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miljan</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Martic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Direct preference optimization: Your language model is secretly a reward model</title>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Rafailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Archit</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization</title>
		<author>
			<persName><forename type="first">Bram</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meihua</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Rafailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Senthil</forename><surname>Purushwalkam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="8228" to="8238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Using human feedback to fine-tune diffusion models without any reward model</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiafei</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunjiang</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiu</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="8941" to="8951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Flow straight and fast: Learning to generate and transfer data with rectified flow</title>
		<author>
			<persName><forename type="first">Xingchao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengyue</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<idno>ArXiv, abs/2209.03003</idno>
		<ptr target="https://api.semanticscholar.org/CorpusID" />
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">252111177</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Improving the training of rectified flows</title>
		<author>
			<persName><forename type="first">Sangyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zinan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giulia</forename><surname>Fanti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.20320</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient large-scale language model training on gpu clusters using megatron-lm</title>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Korthikanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitri</forename><surname>Vainbrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prethvi</forename><surname>Kashinkunti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julie</forename><surname>Bernauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<meeting>the International Conference for High Performance Computing, Networking, Storage and Analysis</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Reducing activation recomputation in large transformer models</title>
		<author>
			<persName><forename type="first">Anand</forename><surname>Vijay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Korthikanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sangkug</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Lym</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mcafee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Andersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
		<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="341" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Ring attention with blockwise transformers for near-infinite context</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.01889</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Deepspeed ulysses: System optimizations for enabling training of extreme long sequence transformer models</title>
		<author>
			<persName><forename type="first">Ade</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masahiro</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengming</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjia</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuaiwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leon</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.14509</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Zero: Memory optimizations toward training trillion parameter models</title>
		<author>
			<persName><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Rasley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olatunji</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC20: International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Disttrain: Addressing model and data heterogeneity with disaggregated training for multimodal large language models</title>
		<author>
			<persName><forename type="first">Zili</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinmin</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ranchen</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanpeng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianjian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yibo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.04275</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Channels Last Memory Format in PyTorch</title>
		<author>
			<persName><surname>Pytorch</surname></persName>
		</author>
		<ptr target="https://pytorch.org/tutorials/intermediate/memory_format_tutorial.html" />
	</analytic>
	<monogr>
		<title level="j">PyTorch</title>
		<imprint>
			<date type="published" when="2023-10-04">2023. Oct 4, 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Ray: A distributed framework for emerging AI applications</title>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Nishihara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Tumanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Liaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melih</forename><surname>Elibol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)</title>
		<meeting><address><addrLine>Carlsbad, CA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2018-10">October 2018</date>
			<biblScope unit="page" from="561" to="577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pytorch rpc: Distributed deep learning built on tensor-optimized remote procedure calls</title>
		<author>
			<persName><forename type="first">Pritam</forename><surname>Damania</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alisson</forename><surname>Azzolini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Vaughan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Guoqiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Howard</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
		<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="219" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Mooncake: A kvcache-centric disaggregated architecture for llm serving</title>
		<author>
			<persName><forename type="first">Ruoyu</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weimin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinran</forename><surname>Xu</surname></persName>
		</author>
		<ptr target="https://ai.meta.com/research/publications/the-llama-3-herd-of-models/" />
		<imprint>
			<date type="published" when="2024-04">2024. April 2024</date>
		</imprint>
	</monogr>
	<note>The llama 3 herd of models</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title/>
		<ptr target="https://www.scenedetect.com/" />
	</analytic>
	<monogr>
		<title level="j">PySceneDetect Developers. PySceneDetect. PySceneDetect</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title/>
		<ptr target="https://ffmpeg.org/" />
	</analytic>
	<monogr>
		<title level="j">FFmpeg Developers. FFmpeg. FFmpeg</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Panda-70m: Captioning 70m videos with multiple cross-modality teachers</title>
		<author>
			<persName><forename type="first">Aliaksandr</forename><surname>Tsai-Shien Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Willi</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekaterina</forename><surname>Menapace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiang-Wei</forename><surname>Deyneka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byung</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Eun Jeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsin-Ying</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="13320" to="13331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Laion-5b: An open large-scale dataset for training next generation image-text models</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Schuhmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romain</forename><surname>Beaumont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Vencu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cade</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Cherti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theo</forename><surname>Coombes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aarush</forename><surname>Katta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clayton</forename><surname>Mullis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="25278" to="25294" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Clip-based nsfw detector</title>
		<author>
			<persName><surname>Laion</surname></persName>
		</author>
		<ptr target="https://github.com/LAION-AI/CLIP-based-NSFW-Detector" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Insert Access Date</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Efficientnet</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Paddleocr</forename><surname>Contributors</surname></persName>
		</author>
		<author>
			<persName><surname>Paddleocr</surname></persName>
		</author>
		<ptr target="https://github.com/PaddlePaddle/PaddleOCR" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>Insert Access Date</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName><surname>Opencv Developers</surname></persName>
		</author>
		<author>
			<persName><surname>Opencv</surname></persName>
		</author>
		<author>
			<persName><surname>Opencv</surname></persName>
		</author>
		<ptr target="https://opencv.org/" />
		<imprint>
			<date type="published" when="2021-08-01">2021. August 1, 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Diatom autofocusing in brightfield microscopy: a comparative study</title>
		<author>
			<persName><forename type="first">José</forename><surname>Luis Pech-Pacheco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Cristóbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesús</forename><surname>Chamorro-Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joaquín</forename><surname>Fernández-Valdivia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 15th International Conference on Pattern Recognition. ICPR-2000</title>
		<meeting>15th International Conference on Pattern Recognition. ICPR-2000</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="314" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Improving image generation with better captions</title>
		<author>
			<persName><forename type="first">James</forename><surname>Betker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juntang</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joyce</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Guo</surname></persName>
		</author>
		<ptr target="https://cdn.openai.com/papers/dall-e-3.pdf" />
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Some methods for classification and analysis of multivariate observations</title>
		<author>
			<persName><surname>Macqueen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 5-th Berkeley Symposium on Mathematical Statistics and Probability</title>
		<meeting>5-th Berkeley Symposium on Mathematical Statistics and Probability</meeting>
		<imprint>
			<publisher>University of California Press</publisher>
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Chinese clip: Contrastive visionlanguage pretraining in chinese</title>
		<author>
			<persName><forename type="first">Xuejin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingsheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyong</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.01335</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Emu: Enhancing image generation models using photogenic needles in a haystack</title>
		<author>
			<persName><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chih-Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhimanyu</forename><surname>Dubey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.15807</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Xin</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuetao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yimin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yibo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><surname>Dsv</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2502.07590</idno>
		<title level="m">Exploiting Dynamic Sparsity to Accelerate Large-Scale Video DiT Training</title>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Diffusion forcing: Next-token prediction meets full-sequence diffusion</title>
		<author>
			<persName><forename type="first">Boyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Marti Monso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yilun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Simchowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><surname>Tedrake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Sitzmann</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2407.01392" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Ltx-video: Realtime video latent diffusion</title>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Hacohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nisan</forename><surname>Chiprut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benny</forename><surname>Brazowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Shalem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dudu</forename><surname>Moshe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eitan</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eran</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Shiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nir</forename><surname>Zabari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ori</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Poriya</forename><surname>Panet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sapir</forename><surname>Weissbuch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Kulikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaki</forename><surname>Bitterman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeev</forename><surname>Melumian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ofir</forename><surname>Bibi</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2501.00103" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Taming teacher forcing for masked autoregressive video generation</title>
		<author>
			<persName><forename type="first">Deyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runpei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duomin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lionel</forename><forename type="middle">M</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2501.12389" />
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Deepseek-Ai</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Daya</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dejian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haowei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junxiao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runxin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qihao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peiyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaokang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingkai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhibin</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoshu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aixin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bochao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengda</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenggang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damai</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deli</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongjie</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erhang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangyun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fucong</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangbo</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guowei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haocheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honghui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajian</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huazuo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianzhong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingchang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingyang</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junlong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaige</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kexin</forename><surname>Kang Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuai</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lean</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lecong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Litong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingchuan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minghua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minghui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miaojun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panpan</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiancheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiushi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruisong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runji</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruyi</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shangyan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanhuang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengfeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shunfeng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Shuting Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengfeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wangding</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanjia</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenfeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaokang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaotao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingchao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuecheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Q</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyue</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaosha</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxiang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinnan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyi</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianzu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinxia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">K</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanhong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaofeng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaohui</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiliang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yishi</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yisong</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiyang</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiyuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongqiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuduan</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuheng</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanhong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaohui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunxian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuting</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zehui</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangli</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhean</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenda</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhigang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyu</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijia</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zilin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Xie</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2501.12948" />
		<imprint>
			<date type="published" when="2025">2025</date>
			<pubPlace>Ziyang Song, Zizheng Pan</pubPlace>
		</imprint>
	</monogr>
	<note>Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
