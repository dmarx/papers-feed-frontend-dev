- Decision to use a 30B parameter model for Step-Video-T2V
- Choice of deep compression Variational Autoencoder (Video-VAE) for video generation
- Implementation of bilingual text encoders for English and Chinese prompts
- Adoption of a DiT with 3D full attention for denoising latent frames
- Use of Flow Matching for training the DiT
- Application of video-based DPO to enhance visual quality
- Strategy for text-to-image pre-training before text-to-video pre-training
- Selection of a cascaded training pipeline (SFT and DPO)
- Creation of the Step-Video-T2V-Eval benchmark dataset
- Decision to open-source Step-Video-T2V and Step-Video-T2V-Eval
- Approach to handle complex action sequences in video generation
- Design choices for spatial and temporal compression ratios in Video-VAE
- Methodology for reducing artifacts in generated videos
- Insights gained from training on diverse video datasets
- Challenges identified in current video generation models
- Future directions outlined for video foundation models