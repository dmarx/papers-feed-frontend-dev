# Agnosticism About Artificial Consciousness

## Abstract

## 

Could an AI have conscious experiences? Any answer to this question should conform to Evidentialism -that is, it should be based not on intuition, dogma or speculation but on solid scientific evidence. I argue that such evidence is hard to come by and that the only justifiable stance on the prospects of artificial consciousness is agnosticism. In the current debate, the main division is between biological views that are sceptical of artificial consciousness and functional views that are sympathetic to it. I argue that both camps make the same mistake of over-estimating what the evidence tells us. Scientific insights into consciousness have been achieved through the study of conscious organisms. Although this has enabled cautious assessments of consciousness in various creatures, extending this to AI faces serious obstacles. AI thus presents consciousness researchers with a dilemma: either reach a verdict on artificial consciousness but violate Evidentialism; or respect Evidentialism but offer no verdict on the prospects of artificial consciousness. The dominant trend in the literature has been to take the first option while purporting to follow the scientific evidence. I argue that if we truly follow the evidence, we must take the second option and adopt agnosticism.

## Artificial Consciousness in Context

Could an AI have conscious experiences? Not long ago, this was widely regarded as a theoretical question about an obscure science fiction scenario. Now, though, it is generally taken to be a serious question of immediate concern. The interdisciplinary literature on AI is replete with proposals on the prospects of artificial consciousness (AC). Government bodies are starting to take seriously the possibility that measures might be required to prevent, or at least regulate, the development of conscious AI. [1](#foot_0) And the public at large, fed by considerable media coverage, increasingly see it as a question that must be addressed. This escalation of interest is animated by the ethical implications of conscious AI. Schneider captures the central ethical worry: 'The question of whether AIs could have experience will be key to how we value their existence. Consciousness is the philosophical cornerstone of our moral systems, being central to our judgment of whether someone or something is a self or person rather than a mere automaton. And if an AI is a conscious being, forcing it to serve us would be akin to slavery.' [(Schneider 2019, pp. 3-4)](#) The question of AC has thus become not just a question that we'd like to answer but a question we have a moral imperative to answer. This flurry of interest in AC takes place against the backdrop of progress in both AI and consciousness science.

In the context of AI research, we've witnessed various breakthroughs that have greatly enhanced the performance of AI. In particular, the advancement of Large Language Models (LLMs) has given us AI that can sometimes give the appearance of consciousness [(Colombatto & Fleming 2024)](#b9). In a notable incident, the Google engineer Blake Lemoine claimed that the chatbot LaMDA had achieved consciousness. Although very few people -experts or otherwise -agreed with Lemoine's claim, the incident contributed to a growing feeling that robust tests for artificial consciousness are needed. Beyond LLMs, there has been progress in other forms of AI that raise serious questions about artificial consciousness. Although whole-brain emulations are a long way from emulating a human brain, they can emulate the neural connectome of more simple organisms such as C. elegans and, more recently, the larval fruit fly [(Bentley et al 2016;](#b1)[Winding et al 2023)](#b30). If we had reason to believe these organisms are conscious, should we infer that their AI emulations are also conscious? Other programs are designed not to emulate whole brains but just the mechanisms thought to be responsible for consciousness. Would such emulations be conscious? In computer simulations of evolution, AI evolves in a manner comparable to that of organisms. Given that our consciousness emerged through an evolutionary process, might artificial evolution also give rise to conscious entities?

In the context of consciousness science, there have been considerable developments in the scientific tools used to examine consciousness and a major expansion in the range of theories available (see [Kuhn 2024)](#b14). This has contributed to a new-found confidence that questions about the distribution of consciousness can (and should) be answered scientifically. Consider the question of octopus consciousness. The science of consciousness is not in a position to prove whether an octopus has conscious experiences. But it is in a position to deliver an assessment of the likelihood of octopus consciousness based on solid empirical findings rather than intuition, speculation or dogma [(Birch et al 2021)](#b3).

So when it comes to the question of conscious AI, the dominant view is that the question can (and should) be answered scientifically. For instance, a recent major report led by Butlin and Long starts from the claim that '…the assessment of consciousness in AI is scientifically tractable because consciousness can be studied scientifically and findings from this research are applicable to AI' [(Butlin & Long et al, 2023 p. 4)](#). We can summarise this outlook in the following principle:

Evidentialism: positive or negative attributions of consciousness to AI should be based exclusively on scientific evidence.

Crucially, this principle isn't just meant to guide how we make judgements of artificial consciousness in research contexts. It is meant to guide real-world decisions about AI, such as how people should treat AI, how governments should regulate the development of AI and perhaps even how the law should protect AI entities. Birch captures this important practical dimension of Evidentialism:

'Evidence-free speculations may still have their own space elsewhere -the pub or café, or even the seminar room, book group, or lab meeting -but, for the purpose of making important, sober decisions affecting real lives, we need to create a space in which they are left at the door. [' (2024, p. 50)](#) So given that our verdicts ought to be evidence-based, what should we conclude about the prospects of AC? Different authors reach different conclusions: AC-Advocates say that there is enough evidence to conclude that the right kind of AI would be conscious while AC-Deniers say that there is enough evidence to conclude that AI, on its current trajectory, is unlikely to be conscious. Although most of these authors reach their conclusions with caution, my aim is to show that even cautious conclusions are unwarranted. The evidence we have doesn't tell us either way whether AI could be conscious and won't be able to tell us any time soon. So following Evidentialism, the only stance that is warranted is agnosticism about artificial consciousness.

In the next section I'll make the case for agnosticism about artificial consciousness. My argument starts from the fact that what we know about consciousness we know from human organisms. This enables us to make some warranted inferences about consciousness in non-human organisms, but when we try to extend these inferences to sophisticated AI we hit an epistemic wall. So although the ideal of a science-based measure of artificial consciousness is a good one, the reality is that it leaves us in epistemic limbo.

In Section 3 I pit agnosticism against different approaches to the assessment of AC: the theory-heavy approach and the theory-light approach. I argue that both approaches violate Evidentialism by taking a 'leap of faith' that theories or markers developed with regards to organisms are also applicable to AI. I also consider the possibility that future progress in the science of consciousness might overcome this epistemic problem and argue that such optimism is unfounded. In Section 4 I address some potential objections to agnosticism: that it sets epistemic standards unnaturally high; that it is detached from the reality of how we attribute consciousness to entities; and that it relies on problematic metaphysical assumptions about consciousness. In each case I show that the objections don't stick. In Section 5 I move on to considering the ethical consequences of agnosticism. Not reaching a verdict on whether sophisticated AI would be conscious leaves us with a serious dilemma about whether to develop such AI and how to treat it if we did. I argue that the key moral difference-maker is not consciousness as such but sentience (i.e. valenced consciousness) and that we can get enough of an epistemic grip on artificial sentience to guide our decision-making without having to abandon agnosticism about artificial consciousness.

My main aim in this paper is to make a case for agnosticism about artificial consciousness. My more modest aim is to show that agnosticism is at least a serious option that deserves consideration. At present, the main division-point in the literature is between AC-Advocates who think sophisticated AI probably will be consciousness and AC-Deniers who think that it probably won't. I seek to highlight another key division point, this time between AC-Gnostics who believe that we can reach evidencebased verdicts on artificial consciousness and AC-Agnostics who believe that we cannot. The epistemic problems presented by artificial consciousness are different in kind to those raised by, say, octopus consciousness and the ethical problems are similarly distinctive. Both of these special features must be kept in mind as we continue to grapple with the prospects of artificial consciousness.

## The Case for Agnosticism

## AI Candidates for Consciousness

Before making the case for agnosticism, I should specify what kind of AI is under consideration. For three reasons, I'm not making a case for agnosticism about current AI being conscious. The first is that even those favourable to AC are doubtful that current AI is conscious. For instance, Butlin & Long et al argue that '...no current AI systems are conscious, but… there are no obvious technical barriers to building AI systems which satisfy these indicators ['. (2023, p. 1)](#) This kind of future-orientated view is common among AC-Advocates, so an agnostic argument against them should also be targeted at future AI. The second reason is that these doubts about current AI being conscious are well-founded. As we will see, being agnostic about whether current AI is conscious would be unnecessarily cautious. The third reason is that ethical worries about AC tend to be focussed less on the AI we currently have and more on the AI we might develop (e.g. the 'Run-Ahead Principle' in [Birch 2024, p. 324](#)). Whether we can know if future AI will be conscious is the more serious ethical question.

I will thus make a case for agnosticism about consciousness in hypothetical sophisticated AIs i.e. the kind of AI to which AC-Advocates would attribute consciousness. My case for agnosticism is based on the problem of applying theories and markers developed in the context of organic consciousness to non-organic cases. To capture this problem, it will be helpful to focus on AIs with features that would constitute strong evidence of consciousness if displayed by an organism. I will call such hypothetical cases 'Challenger-AIs'. They are challengers in the sense that they seem to be serious contenders for consciousness but also challengers in the sense that they present us with a challenging epistemic conundrum. Different approaches suggest different indicators of consciousness. Table of indicator properties from [Butlin & Long et al (2023, p. 5)](#) We can stipulate that Challenger-AI has all of the features listed. The argument for agnosticism will show that even if we develop AI with all the markers of consciousness proposed by AC-Advocates, we should still be agnostic.

That said, it will also be helpful to place some limits on the kind of AI under consideration. There might one day be AI that works on completely different principles to current AI and is much more akin to an organism. AC-Deniers who regard consciousness as a biological phenomenon tend to qualify their sceptical conclusions by limiting them to conventional AI. Seth, for instance, suggests that AC might '…only be possible if we create machines that are also in some relevant sense alive' (2024 p. 21).[foot_1](#foot_1) So we can stipulate that Challenger-AI is AI that works on the same principles as current AI and that displays potential markers of consciousness that would be considered strong indicators if displayed by an organism. I argue that if we were presented with such a Challenger-AI, we should be agnostic about its consciousness.[foot_2](#foot_2)

## The Argument for Agnosticism

The overall argument for agnosticism is simple: 1) We do not have a deep explanation of consciousness.

2) If we do not have a deep explanation of consciousness, then we cannot justify a verdict on whether Challenger-AI is conscious. 3) Therefore, we cannot justify a verdict on whether Challenger-AI is conscious.

In this section I will make a preliminary case for the two premises of the argument and unpack its conclusion. A fuller defence of the argument, including an assessment of whether the epistemic problem is temporary or permanent, will unfold in following sections.

Starting with the first premise, why think that we don't have a deep explanation of consciousness? A deep explanation is one that tells us why a cognitive episode occurs consciously rather unconsciously. Put another way, it explains why there is something it's like to be in a given state rather than nothing it's like. However, attempts to offer such an explanation run into the 'hard problem' [(Chalmers 1995)](#b7). Whenever we identify some physical or functional state associated with consciousness, it remains a mystery why that state should constitute a conscious experience rather than obtaining unconsciously. Consider a Global Workspace Theory (GWT), such as the Dehaene-Changeux Model [(Dehaene & Changeux 2005)](#b10), according to which a state is conscious when it is made globally available to a range of brain processes. What is it about a state being globally distributed that makes it a conscious state? Why aren't we zombies that have a global workspace but lack subjective experience? Nothing in theory tells us why there should be something it's like to be in a globally distributed cognitive state. Different theories make different claims about what kind of state suffices for consciousness, but none explains why it would be sufficient. This is not to say that existing theories of consciousness have no explanatory value. Imagine we want to know why someone undergoing inattentional blindness is conscious of a basketball player in their visual field but not, say, of the gorilla.[foot_3](#foot_3) A Global Workspace Theory would answer (roughly) that a visual representation of the player has made it into the neuronal network that broadcasts information to other brain areas whereas a visual representation of the gorilla has not. In a shallow sense, this tells us why the subject is conscious of the player: assuming that global distribution constitutes consciousness, the visual representation of the player is conscious because it's globally distributed. But there is a deeper question left unanswered: why would the global distribution of this visual information constitute a visual experience? The same is true for every other theory on the table: each theory can informatively answer various questions about consciousness but leaves the hard question unanswered.

Let's move on to the second premise of the argument: if we do not have a deep explanation of consciousness, then we cannot justify a verdict on whether Challenger-AI is conscious. Were we to discover a deep explanation of consciousness that solves the hard problem, we would have no trouble determining whether a Challenger-AI is conscious. We'd understand the necessary and sufficient conditions of consciousness and could just check whether the AI meets the conditions. But in the absence of a deep explanation we are left only with more shallow explanations, and such shallow explanations are unsuited to the task.

Again, I will defend this claim further in due course but for now I will illustrate the point using GWT. Studies of consciousness in humans, such as inattentional blindness studies, yield a body of evidence taken to support GWT. This theory can then give us a verdict on various difficult cases of human consciousness. Based on what we know from ordinary human cases, we can reach conclusions about whether a vegetative state patient is likely to be conscious, for example. We can even go a step further and reach conclusions about consciousness in non-human animals by looking at whether they have a neuronal global workspace like we do. And all such conclusions would enjoy indirect empirical support from the evidence that motivated GWT in the first place. The theory gives us what you might call inferred conditions of consciousness: there's nothing about the theory that explains why global distribution would be necessary and sufficient for consciousness, but the evidence suggests that it is necessary and sufficient in typical human subjects and we can cautiously infer that the same is true for atypical humans and non-human animals.

When it comes to AI, however, a deeper problem emerges. Even if there is good evidence that the mental states of organisms are conscious when broadcast in a global workspace, that is not enough to tell us that a comparable broadcasting of information in an AI would also be conscious. The evidence we have says nothing about whether non-biological global workspaces also give rise to phenomenal consciousness. When we take evidence from organic human consciousness and try to apply it to AI, our evidence hits an epistemic wall. The conditions of consciousness proposed by GWT are inferred, but the inferences are only warranted if kept within an appropriate scope. AI is beyond that scope.

A good way of highlighting this epistemic wall is to consider the debate between computational functionalist and biological accounts of consciousness. According to computational functionalism, consciousness is a matter of instantiating the right kind of information processes. GWT, for instance, attempts to describe the software of consciousness. This software happens to be running on the hardware (or wetware) of the brain, but if that same software were to run on silicon chips you would still get consciousness. It is substrate independent. On the biological account, consciousness is a matter of having the right kind of biological state. Consciousness is an organic phenomenon that depends on how processes are physically realised. Although we might be able to simulate that phenomenon on silicon, the result would merely be a model of consciousness and not itself conscious. Now, how can we decide between these two accounts? The putative evidence for GWT is evidence that globally distributed information is conscious in organisms. But when it comes to whether the same would also be true in AI, GWT hits the evidence wall. The evidence does not distinguish between two live possibilities: i) the global workspace in itself being sufficient for consciousness; ii) the global workspace being integral to how consciousness is achieved in organisms but, in itself, insufficient for consciousness. If GWT offered a deep explanation of consciousness that captured why global distribution would constitute consciousness then we could rule out the second possibility. But the evidence yields at best a shallow explanation that leaves both possibilities wide open.

The example relies on a distinction between function and substrate that has rightly come under criticism. Many have argued that we can't neatly divide the mind into levels, and that the functional structure of the mind is deeply intertwined with the neural structures that realise it [(Cao 2022](#b6)[, Godfrey Smith 2023](#b12)[, Seth 2024)](#). But this is just one of many ways of illustrating the epistemic wall. Consider Godfrey Smith's (2016) suggestion that consciousness requires a subject and that subjecthood requires an embodied organism. This presents theories like GWT with another pair of possibilities that their evidence cannot select between: i) that a global workspace is sufficient for consciousness; ii) that a global workspace yields consciousness only when embedded in an organic subject. And if we look beyond the debate between functional and biological accounts of consciousness and consider other accounts -such as dualism, panpsychism or information integration theory -we will find yet more options that the evidence is unable to decide between.

Having made a preliminary case for the two premises, we can now move on to the agnostic conclusion. Presented with a Challenger-AI, the evidence available cannot discriminate between two possibilities: i) that the AI has these markers because it is genuinely conscious; ii) that the AI has these markers because it mirrors conscious processes without having subjective experience. The conclusion is not that the evidence of consciousness is mixed, with a dead heat between evidence in favour and evidence against. Rather, the conclusion is that there is no real evidence to be had. The AI has features that would constitute evidence were they found in an organism. But the shallow explanations available to us do not justify taking these features as evidence in non-biological cases. The only stance available to us is thus agnosticism. The term 'agnostic', introduced by T.H. Huxley, seems apposite for several reasons. Huxley explains: 'I invented the word "Agnostic" to denote people who, like myself, confess themselves to be hopelessly ignorant concerning a variety of matters, about which metaphysicians and theologians, both orthodox and heterodox, dogmatise with the utmost confidence.' [(Huxley, 1884)](#) This passage reflects the spirit of my proposed view. It captures the idea that, when presented with a Challenger-AI, we would be ignorant as to whether it is conscious. It also captures the idea that we should oppose any view that reaches a verdict with any confidence. It's not about picking between orthodox and heterodox views of AC, but rather criticising both on the grounds that their confidence is unwarranted. Huxley goes on to elaborate his outlook: 'Agnosticism is of the essence of science, whether ancient or modern. It simply means that a man shall not say he knows or believes that which he has no scientific grounds for professing to know or believe. Consequently Agnosticism puts aside not only the greater part of popular theology, but also the greater part of anti-theology. On the whole, the "bosh" of heterodoxy is more offensive to me than that of orthodoxy, because heterodoxy professes to be guided by reason and science, and orthodoxy does not.' [(Huxley, 1884)](#) Again, this reflects the spirit of my proposal. What is at issue is whether assessments of artificial consciousness can be grounded in scientific evidence. The first half of this quotation captures the Evidentialist principle that many in the AC literature purport to defend. The second half captures the problem that Gnostics not only draw conclusions that are insufficiently supported by the evidence, but do so whilst purporting to be led by the science. It is exactly this mismatch that my agnostic proposal is intended to highlight.

It is important here to distinguish agnosticism about artificial consciousness from mere uncertainty about artificial consciousness. Expressions of uncertainty are ubiquitous in the literature.

'…we can never be certain that AI is conscious, even if we could study it up close.' [(Schneider 2016, p. 198)](#) 'A definitive answer to this question is not currently possible, given the lack of a consensus view about the minimally sufficient conditions for consciousness.' [(Seth 2024, p. 1)](#) 'At this stage, we know a lot more that is relevant than we used to know. But giving a definite answer to the main questions…is not something that can be done with a lot of confidence.' (Godfrey Smith 2023) 'The motto of my approach is 'no magic tricks'. We start in a position of horrible, disorienting, apparently inescapable uncertainty about other minds, and then…the uncertainty is still there at the end.' [(Birch 2024, p. 17)](#) Such expressions of uncertainty might appear quite agnostic. However, there is a world of difference between taking a cautious stance on whether Challenger-AI is conscious and taking no stance. The authors above all offer an opinion one way or the other: Godfrey Smith and Seth (cautiously) deny that Challenger-AI would be conscious while [Schneider and Birch (cautiously)](#) claim that it would be. The rest of the quoted passage from Birch illustrates this: 'I am not in the business of selling magical escape routes from uncertainty. My aim is to construct a framework that allows us to reach collective decisions despite our uncertainty: decisions that command our confidence.' [(Birch 2024, p. 17, my italics)](#) If the argument for agnosticism is sound, conceding uncertainty in our conclusions is not enough. The only level of confidence warranted by the evidence is no level of confidence either way. For a genuine expression of such agnosticism, we can turn to Massimo Pigliucci: ['](#)The only examples we have of consciousness are biological. That doesn't mean that, in principle, it is not possible to build artificial consciousness, but we have no idea how to do it. And we don't know whether, in fact, it is even possible. This truly is an open question where I am entirely agnostic.' (Pigliucci, quoted Kuhn 2024 p. 147)

## Agnosticism vs. Gnostic Approaches

The previous section offered a general argument for agnosticism about artificial consciousness. This section reinforces that argument by pitting agnosticism against specific approaches to artificial consciousness. The approaches available can be divided according to two cross-cutting distinctions: the theory-heavy and theory-light distinction [(Birch 2022)](#b2), and the contemporary optimism vs. future optimism distinction. What unites the different views is that they are all Gnostic -that is, they claim that some verdict can be reached on the consciousness of Challenger-AI that conforms to Evidentialism.

## Agnosticism vs. the Theory-Heavy Approach

To adopt a theory-heavy approach to consciousness is to select a particular theory of consciousness then assess whether a given entity is conscious in terms of that theory. Heavy-theorists can be AC-Advocates or AC-Deniers. AC-Advocates support artificial consciousness on the grounds that their preferred theory entails that a Challenger-AI would be conscious. AC-Deniers reject artificial consciousness on the grounds that their preferred theory would say that a Challenger-AI isn't conscious. An initial problem here is that there are too many theories to choose from. Without even an approximate consensus, we don't know which theory to adopt and are left with a stalemate between contradictory proposals. The deeper problem, though, is that a theory cannot get the kind of evidential support needed to yield warranted conclusions about AC.

As discussed, any theory that gets its empirical support from human consciousness hits an epistemic wall when it tries to generalise to artificial consciousness. For a theory-heavy approach to work, you need an empirically well-supported theory that yields a verdict (one way or the other) on whether a Challenger-AI is conscious. But here heavy-theorists face what I call the Evidentialist Dilemma. On the one hand, they can adopt a modest version of their preferred theory that respects the epistemic wall and avoids unwarranted extrapolations to non-organic cases. But such modest theories would yield no verdict on whether a Challenger-AI is conscious, leading to agnosticism. On the other hand, they can adopt a bold version of their preferred theory that specifies the necessary and sufficient conditions of consciousness as such. This theory would yield a verdict on whether a Challenger-AI is conscious, but at the expense of Evidentialism. It is this second horn of the Evidentialist Dilemma on which heavytheorists tend to fall.

Let's start with how AC-Advocate heavy-theorists compromise Evidentialism. An assumption that tends to do a lot of heavy lifting (pun very much intended) is computational functionalism. Often this assumption is tacit but sometimes it is presented explicitly. For instance, Butlin & Long et al state: 'we adopt computational functionalism, the thesis that performing computations of the right kind is necessary and sufficient for consciousness, as a working hypothesis' (2023, p. 17). Computational functionalism is the auxiliary thesis needed to get from evidence about organic consciousness to conclusions about artificial consciousness. But what justifies the assumption of computational functionalism? No empirical evidence is offered in favour of it. In fact, it's not even clear what kind of empirical evidence could speak in favour of it. This suggests that computational functionalism isn't a hypothesis arrived it by careful by reflection on the evidence but rather an article of faith. Here the AC-Advocate comes up against the epistemic wall then takes a leap of faith over it: a leap of faith that the right computational structure suffices for consciousness. But, as Seth puts it, '…the fact that computational functionalism is so frequently assumed does not mean it is true. [' (2024, p. 6)](#) Computational functionalists might respond that they don't need empirical evidence for the claim as it enjoys a priori support from broader philosophical considerations. Chalmers's famous neural replacement thought experiment offers an argument of this kind. The idea is that the neurons in your brain are gradually replaced by silicon chips with the same functionality until eventually the whole brain is artificial. During this process, would you cease to be conscious? Chalmers and company say no and infer that computational structures suffice for consciousness. But critics respond that it simply begs the question. If computational functionalism is false then at some point the subject would cease to be conscious but, due to the preservation of function, would continue to report being conscious [(Udell & Schwitzgebel 2021)](#b29). This is representative of the wider philosophical debate around computational functionalism: a priori considerations can't settle the matter any better than empirical evidence.

What about heavy-theorists who deny artificial consciousness? AC-Deniers agree with the foregoing objections to computational functionalism but make their own unwarranted claims. This is because biological theories of consciousness run into the hard problem too. Just as there's nothing about computational processes that explains why there would be something it's like to undergo some of those processes, nor is there anything about biological processes that explains why there would be something it's like to undergo them. Consider the following passage from Godfrey Smith: '…if you ask…why there should be "something it's like" to be one of these organisms to have this going on -why there should be something it feels like to have an endogenous large-scale pattern of nervous system activity, modulated by events and also put into service into the service of action from a point of view -then to me, there's not much of a gap here anymore. Once such systems exist, it should feel like something to be them.' (Godfrey Smith 2023)

Here Godfrey Smith seems to take a leap of faith comparable to that of the computational functionalists. He establishes some biological features associated with consciousness and concludes that these features suffice for consciousness. Yet nothing about the biological processes explains why they should occur phenomenally. Furthermore, none of the empirical evidence warrants inferring that they are sufficient for consciousness. Crucially for the discussion of AC, the considerations offered also fail to warrant the conclusion that such features are necessary for consciousness.

The lesson here us that offering an alternative to computational functionalism isn't the same as showing that computational functionalism is false. A biological theory-heavy approach faces the same Evidentialist Dilemma as its opponent. They can adopt a modest biological theory that infers how consciousness is achieved in organisms but that stays neutral on whether it might be achieved in some other way for AI. Or they can adopt a bold theory that says consciousness is essentially biological, yielding a clear verdict on Challenger-AI but at the expense of Evidentialism. The problems with computational functionalism shouldn't push us towards denying artificial consciousness. They should push us towards agnosticism. The evidence doesn't tell us whether to go down the functional path or the biological path, so the evidence doesn't tell us whether a Challenger-AI would be conscious.

## Agnosticism vs. the Theory-Light Approach

Might a theory-light approach to conscious fare better? This approach was developed by Griffin and Speck in the context of animal consciousness: '…although no single piece of evidence provides absolute proof of consciousness, [the] accumulation of strongly suggestive evidence increases significantly the likelihood that some animals experience at least simple conscious thoughts and feelings' [(Griffin & Speck, 2004)](#b13). Without adopting any particular theory of consciousness, we can take a meta-theoretical approach that identifies markers of consciousness that draw on a range of theories. These markers might then be used to assess artificial consciousness (see [Birch 2024)](#b2). This approach has some initial advantages over the theory-heavy approach. The hard problem might stop us from arriving at a single explanation of consciousness, but this approach suggests we don't need to explain consciousness to establish viable markers.

However, the fundamental problem with this approach is much the same. Why should we believe that any given is indicative of consciousness? Something is taken as a marker of consciousness when a theory of consciousness says that it is. And that theory should be given credence (even if not believed outright) because of the evidence in favour of it. But that evidence hits just the same epistemic wall as the theory-heavy approach. The evidence shows, at best, that these properties are markers of consciousness in organisms. So applying them to AI is unwarranted. This problem is captured by Shevlin:

'…the theory-light approach works for non-human animals insofar as it relies on the assumption that a given cluster of abilities that are consciousness-dependent in humans would be similarly consciousness-dependent if found in a non-human animal. This assumption is broadly plausible to the extent that there are broad homologies between human and animal cognitive architectures. Yet such homologies or even structural similarities are unlikely to apply when considering non-biological systems, since it seems prima facie unlikely (though, of course, possible) that abilities like trace-conditioning and reversal learning if present in a machine would be underwritten by consciousness in the same way as biological systems.' (Shevlin 2024, p. 4)[foot_4](#foot_4)

So the theory-light approach faces the now-familiar Evidentialist Dilemma. They can adopt a modest set of markers that are well-supported by the empirical evidence but only applicable in the domain where the evidence was gathered viz. organisms. Or they can adopt a bold set of markers applicable to biological and non-biological cases but which are no longer warranted by the empirical evidence. Once more a leap of faith is taken. This time it's not a leap of faith that a given theory is true. Instead, it is a leap of faith that the available theories taken as a whole somehow overcome the epistemic wall when none of them could do so individually. The wall is less salient here because the markers are arrived at on the basis of numerous theories and each theory hits the wall in a different way. But hit it they do. So a theory-light approach is similarly unable to reach a verdict on artificial consciousness without compromising Evidentialism.

## Agnosticism vs. Future optimism

So far the approaches discussed have offered a verdict on artificial consciousness based on the current state of consciousness science. A possible response to the agnostic arguments above is to concede that although we're in a bad epistemic position today, future research will allow us to reach empiricallyinformed verdicts about artificial consciousness. For the heavy-theorist, that would mean establishing the correct theory of consciousness. For the light-theorist, that would mean establishing a more robust set of markers. This gives us the following set of options: Fig. [2](#) Four approaches to artificial consciousness Expressions of future optimism are widespread. Seth, for example, suggests that '…as we become better able to explain, predict, and control properties of consciousness in terms of underlying mechanisms, the sense of mystery about how consciousness relates to matter will gradually dissolve and may eventually disappear [(2024, p. 20)](#).

I suggest that such optimism is unwarranted. The problems for Gnostic views is that efforts to understand artificial consciousness face an epistemic wall. Why should we think that science is on a trajectory to overcome this wall? Consider the sense of mystery that motivated Huxley's agnosticism about the underpinnings of consciousness: 'How it is that anything so remarkable as a state of consciousness comes about as a result of irritating nervous tissue, is just as unaccountable as the appearance of the djinn when Aladdin rubbed his lamp in the story.' (Huxley 1886) One might argue that the ensuing 150 years of scientific progress have lessened the mystery and, if allowed to continue, will overcome it entirely. There is a clear sense in which we have made considerable progress in the science of consciousness. But when it comes to the hard problem we have made no progress at all. We still have nothing that begins to explain why some states would give rise to phenomenal consciousness. The problem is neatly captured by the following cartoon: 

## Theory Heavy Approach

We have a sufficiently wellsupported theory of consciousness that can deliver verdicts on Challenger-AI

We will have a sufficiently wellsupported theory of consciousness that can deliver verdicts on Challenger-AI

## Theory Light Approach

We have a sufficiently wellsupported set of cross-theoretical markers that can deliver verdicts on Challenger-AI

We will have a sufficiently wellsupported set of cross-theoretical markers that can deliver verdicts on Challenger-AI New theories simply relocate the apparent miracle of consciousness without managing to make it any less miraculous. And without progress on this matter, our efforts to understand artificial consciousness will keep hitting the epistemic wall between organic evidence and artificial applications. So optimism that we will be able to reach a verdict on AC in the future is unwarranted on inductive grounds. Of course, one could just choose to be optimistic and not worry about whether it is warranted. But that would be its own violation of Evidentialism. Once more we find Gnostics taking a leap of faith -this time faith that the scientific method will overcome all obstacles in its path.

In philosophy of religion we find a distinction between soft agnosticism, which says we don't know whether God exists, and hard agnosticism, which says we can't know whether God exists [(Moreland & Lane Craig 2017)](#). Does my case for agnosticism about artificial consciousness lead us to hard agnosticism about AC? I think hard agnosticism is difficult to justify. Kuhn advocates such a view: '…after super-strong AI exceeds some threshold, science could never distinguish, not even in principle, actual inner awareness from apparent inner awareness.' (2024, p. 152) [6](#foot_5) The problem with 'in principle' problems is that they sometimes turn out to be an artefact of our ignorance [(Stoljar 2006;](#b27)[McClelland 2019)](#b18). Lots of scientific explanations seemed impossible in principle right up until they were explained. So we shouldn't be so confident that the problems highlighted are insurmountable. But that doesn't mean we should be optimistic either. Faced with obstacles that we don't know how to overcome, and that we make no progress in overcoming, we must take seriously the possibility that the knowledge we desire is unattainable. The appropriate stance to take is then a hard-ish agnosticism that acknowledges serious obstacles to knowledge that may transpire be surmountable but might not be. The theme of this paper is humility, and it might look like the most epistemically humble view is strong agnosticism. But strong agnosticism requires us to know that the problems highlighted are insurmountable. Given our position of uncertainty, the most appropriate position is the hard-ish agnosticism described. It might be that a revolution in consciousness science will overcome the epistemic wall, but there is no good reason to think that such a revolution is on its way.

## Objections to Agnosticism

I now move on to consider, and rebut, three likely objections to agnosticism about artificial consciousness.

## The Untenable Standards Objection

The first objection is that agnosticism sets the standards of justification too high. One way to show that the standards are too high is to show that they over-generate i.e. that they lead to agnosticism in cases where agnosticism is clearly inappropriate. There are two ways in which an agnostic argument might over-generate: it might entail agnosticism in cases where we ought to attribute consciousness or entail agnosticism in cases where we ought to deny consciousness.

The first kind of error might occur in the context of non-human animals. If not having a deep explanation of consciousness entails that I can't know that a Challenger-AI is conscious, doesn't it also entail that I can't know that a chimp is conscious? After all, without a deep explanation of consciousness we can't rule out the possibility of chimps being zombies that display the biological and behavioural markers of consciousness without having subjective experience. Yet there seem to be very good empirical reasons for attributing consciousness to chimps. So if the argument for agnosticism entails that we shouldn't attribute consciousness to chimps, we have reason to doubt the argument.

An example of the second kind of error is consciousness in LLMs. Without a deep explanation of consciousness, we don't know how consciousness might be achieved. LLMs might be very different in functional structure and physical constitution to organisms that we take to be consciousness. But the evidence available cannot rule out the possibility that LLMs are conscious. Yet it seems there are good reasons to deny that LLMs are conscious. Aru et al convincingly argue that '…it is extremely unlikely that LLMs, in their current form, have the capacity for phenomenal consciousness. Rather, they mimic signatures of consciousness that are implicitly embedded within the language that people use to describe the richness of their conscious experience.' (2023, p. 1015) So if the argument for agnosticism entails that we shouldn't deny that LLMs are conscious, we have reason to doubt the argument.

In response to this line of objection, I would maintain that my argument for agnosticism about Challenger-AI does not entail agnosticism about other cases. There are agnostic positions that do entail agnosticism in other cases. Kenneth Steiglitz, for example, endorses agnosticism about AC on the basis of the much broader agnostic view that 'it is simply not possible to test for consciousness' (quoted in Kuhn 2024, p. 56) But this is not the kind of agnosticism entailed by my arguments. Without a deep explanation of consciousness we can use what we know about human consciousness to infer markers of consciousness in other organisms. The problem is that when we try to extend this to non-organic cases we hit an epistemic wall. Claims about chimp consciousness are on the right side of this wall, so my arguments are quite consistent with attributions of chimp consciousness being well-supported.

My arguments are also consistent with denying LLM consciousness. We can't prove that LLMs lack consciousness but that's a low bar. After all, the difficulties with disproving consciousness are what allow panpsychists to speculate that everything is conscious. The point is that the putative evidence in favour of LLM consciousness can be debunked. For example, linguistic statements that would be indicative of consciousness in humans prompt us to consider whether they are manifestations of consciousness in LLMs. But once we understand the process responsible for those reports -a process completely unlike that responsible for human reports of consciousness -we know not to trust the marker. Of course, you could still maintain that the LLM is conscious but such a claim would be speculative rather than evidence-based. Denying LLM consciousness is thus appropriate. This is quite different to the epistemic situation with Challenger-AI. When such an AI makes linguistic reports indicative of consciousness, we cannot debunk the evidence as such. After all, the process underlying those reports has salient similarities to the process in humans and salient differences. But thanks to the epistemic wall, we don't know whether those differences make a difference. So in difficult cases like this the appropriate response is agnosticism.

## The Ivory Tower Objection

The next objection is that any reservations we might have about Challenger-AI being conscious will disappear when actually faced with such an AI. After all, the essence of a Challenger-AIs is that it will show all outward signs of being conscious. It will claim to be conscious in just the way humans do. As Graziano puts it, '…when a robot acts like it's conscious and can talk about its own awareness, and when we interact with it, we will inevitably have that social perception, that gut feeling, that the robot is conscious' (Graziano quoted in [Kuhn 2024, p. 150)](#). Even with today's more rudimentary AI, we are already seeing people attribute consciousness to programs like Replika and robots like Sophia (Shevlin 2024). So it is very likely that more advanced AI will invite even stronger attributions of consciousness.

A philosopher might sit in their ivory tower and argue for agnosticism but their epistemic worries would be detached from the reality of how we actually attribute consciousness.

There are two ways of reading this objection. On the first reading, it is a claim about evidence. The idea is that when we actually interact with such an AI we will have strong evidence of its consciousness. Tye advocates such a view in the context of the fictional android Commander Data:

'…we have evidence that Data has experiences. Specifically, we have evidence that he feels anger, fear, and pain; that he has various perceptual experiences; that he feels moods such as depression and happiness. So we have (defeasible) reason at least to prefer the hypothesis that Data has the experiences and feelings he appears to have to the hypothesis that he does not.' [(Tye 2016, p. 176)](#) Against Tye, we have good reason not to treat such appearances as evidence. Data acts in ways that would be indicative of consciousness in organisms, but if we follow the arguments for agnosticism then we should not take them to be indicative of consciousness in AI. Putting too much weight on the intuitive appearance of consciousness risks compromising Evidentialism.[foot_6](#foot_6)

Another way of reading the objection is not as a claim about evidence but as a claim about human psychology. It's not about whether we'd be justified in attributing consciousness to Data. It's about the fact that we almost inevitably will attribute consciousness to Data (Shevlin 2024). If that's the case, then agnosticism will find itself out of step with the general population. I think this psychological prediction could well be true. I do not, however, regard it as an objection to agnosticism. Sometimes good arguments have counter-intuitive conclusions. And the lesson from the arguments offered is that if we ever come face-to-face with Challenger-AI, our intuitions should not be trusted.

## The Implausible Metaphysics Objection

Another likely objection is that agnosticism makes dubious metaphysical assumptions. In the context of general agnosticism about the scientific study of consciousness, Birch suggest that agnosticism:

'…often flows from a certain type of background philosophical picture. That picture is epiphenomenalism, according to which conscious experiences have no physical effects, and so leave no measurable imprint on the physical world. Conscious experience literally does nothing, so we cannot study it scientifically.' [(Birch 2024, p. 51)](#) But epiphenomenalism is implausible for many reasons. Birch rehearses William James's influential argument that consciousness is an evolved trait and that it is unlikely we would have evolved a complex trait that has no causal power and so confers no evolutionary advantage. If agnosticism rests on epiphenomenalism, then any objection that undermines epiphenomenalism will also undermine agnosticism.

In response to this objection, I concede that the epiphenomenalist route to agnosticism is a bad one. This is the route that some, including Huxley himself, have taken. But it is not the route taken in this paper. The epistemic claims in my argument are consistent with a wide variety of metaphysical views.

For instance, it could well be that conscious states are nothing more than certain physical states. Those physical states are causally efficacious, therefore conscious states are causally efficacious. The problem is with knowing which physical states are conscious and, more specifically, knowing whether states of an AI could be conscious. The case offered for agnosticism about artificial consciousness avoids any problematic metaphysical commitments.

## The Ethical Implications of Agnosticism

My final task is to establish the ethical implications of agnosticism. There are two incentives for determining these implications. The first is that they are important. The possibility of conscious AI presents us with a unique moral quandary and it is important to work out what agnosticism tells us about how to navigate that quandary. The second is that doing so allows us to confront another potential objection to agnosticism. Deciding how to regulate the development of more advanced AI, and how to treat it when it arrives, seems to require us to make a judgement about how likely such AI is to be conscious. Agnosticism tells us we can't make such judgements, but the practical reality may be that such agnosticism is unfeasible. As Birch puts it: 'It is tempting to throw our hands aloft and say 'Maybe we'll never know!'. But practical and legal contexts force a choice. [' (2024, p. 11](#)). I will call this the Forced Choice Objection'. My aim in this section is thus to make a positive recommendation about how to manage the possibility of AC responsibly and, in doing so, to rebut the Forced Choice Objection. In 5.1 I will explain the ethical problem faced by agnosticism and in 5.2 I will overcome that problem by sketching a concrete proposal for the responsible development of AI.

## The Ethical Problem for Agnosticism

I started the paper by saying that consciousness is regarded as ethically salient. It would be more precise to say that a particular kind of consciousness is regarded as ethically salient. To be sentient is to have valanced consciousness i.e. conscious states that are good or bad to be in from the perspective of the conscious subject. Sentientism is the view that all (and only) sentient beings warrant our ethical consideration. Beings that are unconscious, and beings that have consciousness devoid of valence, need not feature in our moral deliberations. Singer suggests that 'the limit of sentience…is the only defensible boundary of concern for the interests of others.' (2016) This might sound like a broadly utilitarian ethos, but we actually find similar claims in deontological ethics. Godfrey Smith (2023) points out that in Korsgaard's ethics being sentient is what makes an entity a Kantian 'end in itself' and in Nussbaum's ethics it is a pre-requisite of 'significant striving'.

Sentience is thus a plausible ticket into the 'moral circle'. But there is a great deal of uncertainty about the boundaries of sentience. Knowing for sure that an octopus, say, is sentient would be an unrealistic expectation. What we can do, however, is assess how likely something is to be sentient then take proportionate measures to avoid actions that would cause it to suffer were it sentient. This is the precautionary principle championed by [Metzinger (2021)](#b19), [Birch (2024)](#b2) and others. When it comes to AI, then, we don't need to know for sure whether a given type of AI is sentient. Instead, we determine whether there is a risk that it is sentient. And if there is a sufficient risk, we then take proportionate precautionary measures. This leaves open what the measures should be. It might be that when we create AI that could be conscious, we should give it the same rights as a sentient human agent. Or it might be that we should just avoid creating AI that would require such precautions should be taken. This generates an interesting debate, but the debate only arises if we first judge there to be a significant chance of artificial sentience.

But now we get to the problem for agnosticism. My argument says that we cannot take a view either way on the chances of a Challenger-AI being conscious. But if we can't make a judgement on how likely it is to be conscious, we don't know what level of precaution would be proportionate. The agnostic acknowledgement that there's a possibility of consciousness is not enough. Without an assessment of how likely it is be conscious, there is no way to proportion the precautions to the risk. Birch considers situations where there is '…a total or near-total lack of evidence one way or the other, making it impossible to mount a credible, evidence-based case either for or against sentience.' (2024, p. 125) He thinks this is the situation we are in for nematode worms and insect larvae, but my argument suggests that this is the situation we would be in with Challenger-AI. What is to be done? One option Birch proposes is to make such cases targets of further research when'…further investigation could plausibly lead to the recognition of S as a sentience candidate' (2024, p. 126)

However, as we saw in Section 3, there is no reason to be optimistic that scientific research in the immediate future will enable to us to overcome the epistemic wall that blocks scientific assessments of artificial consciousness. And without a scientific assessment of artificial consciousness, there can be no scientific assessment of artificial sentience. It seems, then, that agnosticism leaves us in a moral limbo that is both unhelpful and unsustainable.

## A Solution to the Ethical Problem

In order to solve this problem we must reflect more on the link between consciousness and sentience. Consciousness is a necessary condition of sentience, so if we cannot acquire positive evidence of artificial consciousness then we cannot acquire positive evidence of artificial sentience. However, we could well reach conclusions about what kind of experience an AI would have were it conscious [(McClelland 2017)](#b17). Chrisley suggests the following approach to the scientific study of artificial consciousness '…do not attempt to explain why something is conscious, but instead explain why it is in this conscious state rather than that one.' (2008 p. 132) Sidestepping the vexed question of whether a self-driving car is conscious, for example, we might be able to demonstrate that if it is conscious then the contents of its consciousness would include perceptions of the road but not thoughts about the destination.

This opens up an interesting possibility regarding sentience Perhaps we can reach verdicts of the following form: if the Challenger-AI is conscious then the contents of its consciousness would have positive/negative/no valence. Working out this kind of conditional conclusion wouldn't be easy and would require further research [(Butlin & Long et al (2023)](#b5) already recommend prioritising research on computational theories of valence). Crucially though, this wouldn't require us to solve the hard problem. We can determine what the contents (or likely contents) of a cognitive state are without knowing what further features are needed to make that cognitive state phenomenally conscious.

This kind of conditional claim is enough to ground responsible decision-making. Consider a Challenger-AI with computational states such that if those states were conscious then they would have no valence.

There would be no need to take precautions over the development or treatment of such AI because we can be confident of its insentience. Whilst remaining agnostic about whether it is conscious, we can make an evidence-based argument that even if it were conscious it wouldn't be sentient. Now consider a Challenger-AI with computational states such that if those states were conscious then they would have negative valence. Such an AI would present us with a moral dilemma. Taking precautions must be proportionate to the likelihood of sentience but given agnosticism we lack the evidence needed to assess that likelihood. Yet taking no precautions presents a real (if unquantifiable) possibility of doing great harm to a sentient entity. Here a different type of precaution enables us avoid the moral dilemma: just don't create AI that would be sentient were it conscious! [Schwitzgebel & Garza (2020)](#b23) argue that if an AI is such that we don't know whether it is conscious then we should avoid the moral dilemma it would present us with by not developing that AI . I propose a subtly different version of this principle: if an AI is such that we don't know whether it is sentient we should avoid the moral dilemma it would present us with by not developing that AI. Crucially, this escapes the moral dilemma without placing undue restrictions on the development of useful AI. If the advancement of AI requires us to create AI that might be conscious then so be it. We only have to ensure that its consciousness wouldn't be valenced. This constraint would only be problematic if progress in AI somehow required us to engineer AI with states that would be valenced were it conscious. But in the absence of good reasons to think that this requirement obtains, the dilemma can be avoided.

Far from leaving us in moral limbo, agnosticism is consistent with us reaching evidence-based decisions about ethical problems. And against the Forced Choice Objection, it can do so whilst staying neutral on whether Challenger-AI is conscious.

## Conclusion

In the future we may develop AI with features that would be considered markers of consciousness were they found in an organism. Without a deep explanation of consciousness, efforts to assess the likelihood of artificial consciousness hit an epistemic wall. The dominant approaches to artificial consciousness, whether they be in favour of AC or sceptical of it, leap over this epistemic wall and thereby compromise the Evidentialist principle that they purport to defend. Widespread claims to offer science-based tests for artificial consciousness are thus overstated. Although further progress in the science of consciousness might one day overcome this wall, there is little reason to be optimistic that it will do so any time soon. The resulting agnosticism makes ethical decisions about AI hard to reach. However, thanks to the distinction between consciousness and sentience it should be possible to make an evidence-based argument that AI is insentient without taking a stand on whether it is conscious. If the right precautions are taken, this is enough to overcome the ethical worries that animate the debate. My primary aim is to have made a strong case for agnosticism about artificial consciousness. My secondary aim is to have at least shown that agnosticism is an important option that alters the landscape of the debate and warrants further consideration.

![Fig. 1: Table of indicator properties fromButlin & Long et al (2023, p. 5)]()

![Fig 3: 'Then a miracle occurs' by Sidney Harris (This cartoon has previously been used to illustrate the problem of consciousness by Bill Seager.)]()

Consider the European Parliament report by[Brundage, Metzinger, Bentley and Häggström (2018)](#b4) which includes a proposed moratorium on 'synthetic phenomenology'.

Also see[Aru et al (2023)](#b0).

I put aside here questions about levels of consciousness (Godfrey Smith 2023). This would take us into contentious territory without making much of a difference to the arguments offered.

For the sake of the illustration, I put aside the possibility that the subject has a perception of the gorilla that is phenomenally conscious but not access conscious.

Megan Peters makes the related point in terms of Bayesian priors. The priors for creatures similar to us (like chimps) being conscious are high but become lower the more distant an entity is from a human (interview quoted in[LeDoux & Birch 2023)](#b16). Although this is very close to the claim I want to make, Peters suggests a difference of degree in how difficult it is to justify attributions of consciousness to AI compared to animals. The idea of the epistemic wall, however, is that there is a difference of kind.

Also see[Kurzweil (2007)](#b15)

Tye himself advocates a theory-free approach to consciousness, but this isn't an option for the views under discussion that advocate a scientific verdict on artificial consciousness.

