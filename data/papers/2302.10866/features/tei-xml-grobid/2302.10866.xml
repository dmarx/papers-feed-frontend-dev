<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hyena Hierarchy: Towards Larger Convolutional Language Models</title>
				<funder>
					<orgName type="full">Accenture</orgName>
				</funder>
				<funder ref="#_xTTThfv">
					<orgName type="full">DOE</orgName>
				</funder>
				<funder ref="#_Z9jVCHf">
					<orgName type="full">Xilinx</orgName>
				</funder>
				<funder>
					<orgName type="full">Total</orgName>
				</funder>
				<funder ref="#_weGHenw">
					<orgName type="full">Interactive Human-AI Teaming)</orgName>
				</funder>
				<funder ref="#_Dhd2sSg">
					<orgName type="full">National Institutes of Health</orgName>
					<orgName type="abbreviated">NIH</orgName>
				</funder>
				<funder>
					<orgName type="full">Ericsson</orgName>
				</funder>
				<funder>
					<orgName type="full">Qualcomm</orgName>
				</funder>
				<funder>
					<orgName type="full">Salesforce</orgName>
				</funder>
				<funder ref="#_5N4Tydb #_ggzmA8e">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder>
					<orgName type="full">Analog Devices</orgName>
				</funder>
				<funder>
					<orgName type="full">Stanford Data Science Initiative</orgName>
				</funder>
				<funder>
					<orgName type="full">Facebook</orgName>
				</funder>
				<funder ref="#_YDzUTjJ">
					<orgName type="full">ARO</orgName>
				</funder>
				<funder ref="#_cqyfMjp">
					<orgName type="full">ONR</orgName>
				</funder>
				<funder>
					<orgName type="full">Department of Defense (DoD)</orgName>
				</funder>
				<funder ref="#_ecBZben">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_6UjMS8x">
					<orgName type="full">AFOSR</orgName>
				</funder>
				<funder>
					<orgName type="full">National Defense Science and Engineering Graduate Fellowship</orgName>
					<orgName type="abbreviated">NDSEG</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-04-19">19 Apr 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Michael</forename><surname>Poli</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stefano</forename><surname>Massaroli</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Mila and Université de Montréal</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eric</forename><surname>Nguyen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stephen</forename><surname>Baccus</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Mila and Université de Montréal</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Hyena Hierarchy: Towards Larger Convolutional Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-04-19">19 Apr 2023</date>
						</imprint>
					</monogr>
					<idno type="MD5">129519E71ABDCF3C22F98E3FC2DCD00F</idno>
					<idno type="arXiv">arXiv:2302.10866v3[cs.LG]</idno>
					<note type="submission">Version: submitted draft, Last Compiled: April 21, 2023</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advances in deep learning have relied heavily on the use of large Transformers due to their ability to learn at scale. However, the core building block of Transformers, the attention operator, exhibits quadratic cost in sequence length, limiting the amount of context accessible. Existing subquadratic methods based on low-rank and sparse approximations need to be combined with dense attention layers to match Transformers, indicating a gap in capability. In this work, we propose Hyena, a subquadratic drop-in replacement for attention constructed by interleaving implicitly parametrized long convolutions and data-controlled gating. In recall and reasoning tasks on sequences of thousands to hundreds of thousands of tokens, Hyena improves accuracy by more than 50 points over operators relying on statespaces and other implicit and explicit methods, matching attention-based models. We set a new state-ofthe-art for dense-attention-free architectures on language modeling in standard datasets (WikiText103 and The Pile), reaching Transformer quality with a 20% reduction in training compute required at sequence length 2K. Hyena operators are twice as fast as highly optimized attention at sequence length 8K, and 100× faster at sequence length 64K.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Large Transformers have enabled a number of breakthrough advances in modeling language, vision, audio, biology and numerous other domains <ref type="bibr" target="#b50">(Vaswani et al., 2017)</ref>, <ref type="bibr" target="#b12">(Dosovitskiy et al., 2020)</ref>, <ref type="bibr" target="#b39">(Radford et al., 2022)</ref>, <ref type="bibr" target="#b6">(Cramer, 2021)</ref>. Much of the success of Transformers, powered by the attention operator <ref type="bibr" target="#b50">(Vaswani et al., 2017)</ref>, relies on their scaling properties <ref type="bibr" target="#b21">(Hoffmann et al., 2022)</ref> and the emergence of in-context learning <ref type="bibr" target="#b16">(Garg et al., 2022)</ref>, which allows them to generalize to unseen data and tasks given context as input. The Transformer block is a powerful tool for sequence modeling, but it is not without its limitations. One of the most notable is the computational cost, which grows rapidly as the length of the input sequence increases. Specifically, the cost scales quadratically with the length L of the sequence, which places a strict limit on the amount of context that can be considered by the model. Breaking the quadratic barrier is a key step towards new possibilities for deep learning, such as using entire textbooks as context, generating long-form music or processing gigapixel scale images.</p><p>Efforts to reduce the computational cost of attention in models primarily involve the use of linearized, low-rank, and sparse approximations <ref type="bibr" target="#b5">(Child et al., 2019;</ref><ref type="bibr" target="#b52">Wang et al., 2020;</ref><ref type="bibr" target="#b23">Kitaev et al., 2020;</ref><ref type="bibr" target="#b55">Zhai et al., 2021;</ref><ref type="bibr" target="#b44">Roy et al., 2021;</ref><ref type="bibr" target="#b45">Schlag et al., 2021;</ref><ref type="bibr" target="#b49">Tu et al., 2022)</ref>. These approaches introduce a trade-off between expressivity and speed, requiring hybridization with standard attention layers to reach Transformer quality <ref type="bibr" target="#b30">(Mehta et al., 2022;</ref><ref type="bibr">Dao et al., 2022c)</ref>.</p><p>A growing amount of evidence suggests that attention mechanisms only utilize a small portion of their quadratic capabilities for language processing <ref type="bibr" target="#b33">(Olsson et al., 2022;</ref><ref type="bibr">Dao et al., 2022c)</ref>, leading us to question its role as the gold-standard operator for deep learning at scale. Specifically, we ask:</p><p>Are there subquadratic operators that can match the quality of attention at scale?</p><p>Figure <ref type="figure" target="#fig_2">1</ref>.1: The Hyena operator is defined as a recurrence of two efficient subquadratic primitives: an implicit long convolution h (i.e. Hyena filters parameterized by a feed-forward network) and multiplicative elementwise gating of the (projected) input. The depth of the recurrence specifies the size of the operator. Hyena can equivalently be expressed as a multiplication with data-controlled (conditioned by the input u) diagonal matrices D x and Toeplitz matrices S h . In addition, Hyena exhibits sublinear parameter scaling (in sequence length) and unrestricted context, similar to attention, while having lower time complexity.</p><p>We obtain a positive answer based on a composition of efficient subquadratic primitives, such as elementwise multiplication (gating) and long convolutions i.e., convolutions with filter sizes as long as the input. We rely on a set of targeted reasoning tasks, grounded in recent work on mechanistic interpretability <ref type="bibr" target="#b13">(Elhage et al., 2021;</ref><ref type="bibr" target="#b38">Power et al., 2022;</ref><ref type="bibr" target="#b33">Olsson et al., 2022;</ref><ref type="bibr" target="#b57">Zhang et al., 2022)</ref> such as recall and induction, to distill three properties of attention correlated with its performance and the quality gap with existing subquadratic approaches:</p><p>a. Data control: Attention implements an expressive data-controlled <ref type="bibr" target="#b29">(Massaroli et al., 2020)</ref> linear operator<ref type="foot" target="#foot_0">foot_0</ref> , encoding an entire family of linear functions in a single block.</p><p>b. Sublinear parameter scaling: Parameter counts of attention layers are decoupled from sequence length, allowing Transformers to allocate more parameters elsewhere e.g., the feed-forward neural networks (FFNs) between attention layers.</p><p>c. Unrestricted context: For a given input, attention has an unrestricted context i.e., it can approximate dependencies between any two inputs, without arbitrary restrictions such as locality (except in cases using masking such as autoregressive models).</p><p>The Hyena hierarchy Guided by these findings, we introduce the Hyena hierarchy, an operator defined by a recurrence of two efficient subquadratic primitives: a long convolution and element-wise multiplicative gating (see Figure <ref type="figure" target="#fig_2">1</ref>.1). A specified depth (i.e., number of steps) of the recurrence controls the size of the operator. For short recurrences, existing models are recovered as special cases <ref type="bibr" target="#b30">(Mehta et al., 2022;</ref><ref type="bibr">Dao et al., 2022c)</ref>. By mapping each step in the Hyena recurrence to its corresponding matrix form, we reveal Hyena operators to be equivalently defined as a decomposition of a data-controlled matrix i.e., a matrix whose entries are functions of the input. Furthermore, we show how Hyena operators can be evaluated efficiently without materializing the full matrix, by leveraging fast convolution algorithms <ref type="bibr" target="#b46">(Selesnick and Burrus, 2017)</ref>. Empirically, Hyena operators are able to significantly shrink the quality gap with attention at scale, reaching similar perplexity and downstream performance with a smaller computational budget (Section 4.2) and without hybridization of attention.</p><p>Narrowing the capabilities gap The design of Hyena is motivated by a quality gap between standard dense attention and alternative subquadratic operators, which we identify by focusing on reasoning tasks correlated with language modeling performance at scale. We extend the suite of basic mechanistic interpretability benchmarks (induction and recall ) with additional tasks that probe how quickly model performance degrades when task complexity increases (e.g. vocabulary size grows). In addition, we investigate the optimal parameterization of long convolutions in Hyena. In the most challenging settings with hundreds of thousands of tokens, our implicit parameterization scheme improves over other operators leveraging state spaces <ref type="bibr" target="#b18">(Gu et al., 2021)</ref>, frequency-domain parametrizations <ref type="bibr" target="#b27">(Li et al., 2020)</ref>, or standard convolutions by over 50% accuracy.</p><p>Scaling in language and vision Next, we aim to verify whether rankings in our reasoning benchmark suite are predictive of quality at scale. We test Hyena on autoregressive language modeling at the sub-billion parameter scale, setting a new state-of-the-art for dense-attention-free architectures in standard datasets (WikiText103 and The Pile) and matching Transformer quality. On the The Pile at the 335M parameter scale, we match Transformer perplexity with a 20% reduction in the total count of floating point operations (FLOPs). As an extension, we investigate the generality of Hyena operators by testing on large-scale image recognition, replacing attention in the Vision Transformer (ViT) <ref type="bibr" target="#b12">(Dosovitskiy et al., 2020)</ref>. In image classification, Hyena is able to match attention in accuracy when training on ImageNet-1k from scratch.</p><p>Toward much longer context Finally, we benchmark the efficiency of Hyena on long sequences. We measure 5x speedups over dense self-attention at length 8192 -2x over highly optimized FlashAttention<ref type="foot" target="#foot_1">foot_1</ref>  <ref type="bibr">(Dao et al., 2022b)</ref> -and 100x speedup over FlashAttention at sequence lengths of 64k, where standard attention implementation in PyTorch runs out of memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries and Related Work</head><p>A discrete convolution is a function of two arguments: an input u signal of length L and a learnable filter h.</p><p>The linear (aperiodic) convolution of a (possibly infinitely long) measurable<ref type="foot" target="#foot_2">foot_2</ref> filter h with a length-L input signal u is defined as</p><formula xml:id="formula_0">y t = (h * u) t = L-1 n=0 h t-n u n .<label>(1)</label></formula><p>Generally, u t ∈ R D where D is the width of the signal, or in deep learning parlance, the number of channels. Without loss of generality, we specialize our analysis to single input single output (SISO) layers, i.e. with D = 1. The multiple input multiple output (MIMO) case, canonical in standard convolutional layers, follows directly.</p><p>In this case, the input signal can be represented as a vector u ∈ R L and the convolution as a matrix-vector product between the input and the Toeplitz kernel matrix S h ∈ R L×L induced by the filter h:</p><formula xml:id="formula_1">(h * u) =      h 0 h -1 • • • h -L+1 h 1 h 0 • • • h -L+2 . . . . . . . . . . . . h L-1 h L-2 • • • h 0           u 0 u 1 . . . u L-1      (2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Explicit and Implicit Convolutions</head><p>Parametrizing and optimizing convolution filters h t is a standard procedure in deep learning and more broadly signal processing. The classical approach of convolutional neural networks (CNNs) <ref type="bibr" target="#b14">(Fukushima and Miyake, 1982;</ref><ref type="bibr" target="#b24">LeCun et al., 1998;</ref><ref type="bibr" target="#b43">Ronneberger et al., 2015;</ref><ref type="bibr" target="#b19">He et al., 2016)</ref> is to optimize directly the values h t of the filter's response at M prescribed steps, a parametrization we call explicit. M is referred to as the filter size and is typically much shorter than the input sequence length M L. Such filters are denoted in signal processing as finite impulse response (FIR).</p><p>FIR filters are local and can capture dependencies between inputs separated at most by M steps. Their main advantage is their speed, with complexity O(M L). However, the number of parameters of FIR filters scales linearly with filter size, which can be computationally prohibitive. To disentangle the parameter count from the filter size, we can instead represent the filter h t as a parametric function of the time step t, i.e. h t = γ θ (t), where θ are the parameters of the function γ θ . This parametrization is called implicit. The class of functions γ θ is a design choice with a significant impact on the expressivity and computational complexity of the layer.</p><p>One choice of implicit parametrization is to select h as the response function of a linear state-space model (SSM) <ref type="bibr" target="#b4">(Chen, 1984)</ref>, described by the first-order difference equation:</p><p>x t+1 = Ax t + Bu t state equation y t = Cx t + Du t output equation Here, the convenient choice of x 0 = 0 renders the input-output map to a simple convolution</p><formula xml:id="formula_2">y t = t n=0 CA t-n B + Dδ t-n u n</formula><p>where δ t denotes the Kronecker delta. We can then identify the filter h as</p><formula xml:id="formula_3">t → h t = 0 t &lt; 0 CA t B + Dδ t t ≥ 0</formula><p>where the entries of A, B, C and D are the learned parameters of the filter. In terms of layer design, the degrees of freedom of SSMs are the dimension of the state and the structure of the matrices. SSMs are a canonical example of how long convolutions with sub-linear parameter counts can improve deep learning models for long sequences <ref type="bibr" target="#b17">(Gu et al., 2020</ref><ref type="bibr" target="#b18">(Gu et al., , 2021))</ref>. Other implicit approaches include parametrizing filters as maps from (a positional encoding of) t to the filter response i.e. γ θ : t → h t = γ θ (t), for example with feed-forward neural networks <ref type="bibr">(Romero et al., 2021b,a)</ref>.</p><p>Long convolutions and memory: A crude proxy for memory of a single computational unit is how far in the past it can access information to produce the output at a certain step. This can be roughly quantified by the number of non-zero entries ∂y t /∂u t-n for n = 0, . . . , t. The memory of CNNs filters is equivalent to the filter size M since ∂y t /∂u t-n = h n . The total mnemonic capacity of an allconvolutions CNN therefore scales with the number of model's parameters. Implicit parametrizations, on the other hand, allow us to disentangle the memory of each filter from the parameter count and where the length of the filter is implicitly controlled by the learned parameters. In an SSM, ∂y t /∂u t-n = CA n B and the memory extent is solely determined by the spectral radius of A and can be finely tuned by the training process a . On the other hand, the number of parameters controls the expressivity of the memory unit, e.g. the number of basis functions forming h t .</p><p>a See e.g. <ref type="bibr" target="#b17">Gu et al. (2020</ref><ref type="bibr" target="#b18">Gu et al. ( , 2021) )</ref> Fast Methods for Convolutions One of the first applications of the Cooley-Tukey fast Fourier transform (FFT) algorithm was to implement convolution faster than the direct evaluation of (1). At first glance (1) comes with O(L 2 ) an asymptotic time complexity. A common approach to achieve fast long convolutions in subquadratic time is through the FFT algorithm. The method first converts the aperiodic convolution into a circular convolution <ref type="bibr" target="#b46">Selesnick and Burrus (2017)</ref> by appropriate zero-padding of input and filter sequences. The resulting kernel Ŝh is a circulant matrix and is diagonalized by the discrete Fourier basis</p><formula xml:id="formula_4">Ŝh = W -1 D H W</formula><p>where W is the DFT matrix, W tt = z -t , z = e i2πt /L and H is the DFT of the padded filter h, H = Wpad(h). Thus, the calculation of such convolutions is performed as</p><formula xml:id="formula_5">pad(y) = Ŝh pad(u) = W -1 D H W pad(u) = iFFT(D H FFT(pad(u)))</formula><p>where D H is the matrix with Wh on its diagonal. The above is known as the convolution theorem of DFT <ref type="bibr" target="#b34">(Oppenheim et al., 1997)</ref>. In this FFTConv form the convolution can be performed without materializing the operator S h with the same asymptotic cost O(L log 2 L) of FFT. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The Self-Attention Operator</head><p>At the heart of Transformers is the multi-head attention (MHA) mechanism. Given a length-L sequence u ∈ R L×D , each head of scaled self-attention <ref type="bibr" target="#b50">(Vaswani et al., 2017</ref>) is a map from R L×D to R L×D which performs the following operations</p><formula xml:id="formula_6">A(u) = SoftMax 1 √ D uM q M k u y = SelfAttention(u) = A(u)uM v ,<label>(3)</label></formula><p>where M q , M k , M v ∈ R D×D are learnable linear projections and SoftMax is intended to be applied row-wise. Attention parametrizes a family of dense linear operators and for an input u, indexes through it via projections of u i.e., A(u). We refer to operators of this type as data-controlled, as they encode a linear transformation u → y, that is, however, nonlinearly defined by u. This approach yields expressive nonlinear operators in u, and we hypothesize contributes, together with other mechanisms <ref type="bibr" target="#b33">(Olsson et al., 2022)</ref>, to the ability of certain operators to learn in-context i.e., to adapt to unseen tasks by leveraging context. In deep learning, the projections take on specific names: query q = uM q , key k = uM k and value v = uM v . We often rewrite the attention operator as y = A(q, k)v.</p><p>Remark 2.1. Similarly to implicit convolutions, SelfAttention does not entangle its ability to access distant information with the number of parameters: it looks at the whole sequence at the price of O(L 2 ) operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subquadratic Operators</head><p>Existing approaches to subquadratic alternatives to attention can be summarized by altering the way the data control is implemented i.e., how the operator is nonlinearly defined by u, and then applied to v. For example, a layer of Attention-Free Transformers (AFTs) <ref type="bibr" target="#b55">(Zhai et al., 2021)</ref> constructs the operator through a combination of gating and SoftMax (AFT full) or gating and a single explicit convolution (AFT conv). Gated State Spaces (GSS) instead compose the operator via gating and a long convolution parametrized via SSMs. Taking this idea further, Hungry Hungry Hippo (H3) <ref type="bibr">(Dao et al., 2022c)</ref>, motivated by gaps of GSS on associative recall, extend the mechanism to include an additional gate and a short convolution obtained via a shift SSM. Hyena generalizes this body of work by introducing a recurrence of gates and implicit long convolutions, evaluated efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Hyena: Definition and Properties</head><p>In this section, we define Hyena, a class of data-controlled operators consisting of a recurrence of multiplicative gating interactions and long convolutions. Instead of seeking an approximation to attention, we guide our design by intentionally incorporating key computational properties of attention, including the decoupling of sequence length and parameter counts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Hyena Recurrences</head><p>At a high level, Hyena consists of the following steps (setting D = 1 for clarity): i. Compute a set of N +1 linear projections of the input, similarly to attention. The number of projections (v t , x 1 t , . . . , x N t ) need not be three. One projection takes the role of value, such that a linear input-output function can be defined as y = H(u)v for some H(u).</p><p>ii. The matrix H(u) is defined by interleaving implicit long convolutions and element-wise multiplication with one projection x i at a time, until all projections are exhausted. Evaluation of H(u)v is done efficiently without materializing H(u). By doing so, we implicitly define a data-controlled operator as a factorization of a matrix. The long convolutions forming H(u) are parametrized implicitly to retain sublinear parameter scaling in sequence length.</p><p>Next, we formally define Hyena, starting with its computational model. We leave the analysis of its datacontrolled matrix form for the latter part of the section.</p><formula xml:id="formula_7">Definition 3.1 (Order-N Hyena Operator). Let (v, x 1 , • • • , x N</formula><p>) be projections of the input and let h 1 , . . . , h N be a set of learnable filters. The Hyena N operator is defined by the recurrence:</p><formula xml:id="formula_8">z 1 t = v t z n+1 t = x n t (h n * z n ) t n = 1, . . . , N y t = z N +1 t (4) Remark 3.1. The time complexity of a Hyena recurrence is O(N L log 2 L). The input-output map can be rewritten as y = x N • (h N * (x N -1 • (h N -1 * (• • • ))))</formula><p>where each convolution is performed through the Fourier domain in O(L log 2 L).</p><p>Interestingly, the element-wise product in time domain corresponds to convolution in frequency domain, i.e.</p><p>x t u t = (x * û) t , where x, û denote the DFT of x and u, respectively. Thus, Hyena is alternatively applying convolutions in the time and then the frequency domain (or alternatively applying element-wise products in the time and frequency domain). One potential explanation for the effectiveness of this procedure is that the convolution in the time domain (element-wise multiplication in the frequency domain) increases the memory length, allowing for a broader context to be taken into account. On the other hand, the element-wise multiplication in the time domain (convolution in the frequency domain) allows for more fine-grained selection of specific frequency components of the signal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hyena Matrices</head><p>Hyena operators build on the H3 mechanism developed by <ref type="bibr">(Dao et al., 2022c)</ref>. For clarity of exposition, we once again consider the SISO case (D = 1). Let D q and D k be the L-by-L diagonal matrices whose respective main diagonal entries are the respective entries of q and k. H3 realizes a surrogate attention matrix with a data-controlled, parametrized decomposition in four terms:</p><formula xml:id="formula_9">A(q, k) = D q S ψ D k S ϕ H3(q, k, v) = A(q, k)v<label>(5)</label></formula><p>where S ϕ , S ψ are the Toeplitz matrices of learnable causal filters ϕ, ψ parametrized via SSMs<ref type="foot" target="#foot_3">foot_3</ref> . Alongside the qkv-projections the filters constitute our degrees of freedom in the layer design. This decomposition allows evaluation of (8) in just O(L log 2 L) time (two FFT convolutions and two element-wise products), i.e.</p><formula xml:id="formula_10">z t = k t (ϕ * v) t y t = q t (ψ * z) t (6)</formula><p>Hyena represents a generalization of (8) for an arbitrary number of projections -not limited to three -and with implicit free-form long filters for the convolutions. The resulting recurrence (4) can be also represented in matrix form y = H(u)v. Let D n x = diag(x n ) ∈ R L×L and let S n h be the Toeplitz matrix corresponding to filter h n . The resulting Hyena recurrence is linear in v and can be rewritten in matrix form:</p><formula xml:id="formula_11">y = H(u)v = D N x S N h • • • D 2 x S 2 h D 1 x S 1 h v Figure 2.1 visualizes an example decomposition. Sequence Length FFN(t) Sequence Length Window Sequence Length Window • FFN(t) Figure 3.1: [Top]:</formula><p>Example of long convolution parametrization for Hyena operators, with a decay Window(t) = exp{-αt}. Parameter α is modified across the independent channels of Hyena to regularize filters to be of different lengths. In practice, we add a bias term to our window, so that the filters are not constrained to be zeros after a length determined by the decay rate.</p><p>Remark 3.2 (Hyena generalizes H3 and GSS.). The H3 mechanism <ref type="bibr">(Dao et al., 2022c)</ref> corresponds to Hyena 2 and GSS <ref type="bibr" target="#b30">(Mehta et al., 2022)</ref> is Hyena 1 , with a particular choice of parametrization for the long convolutions (SSMs).</p><p>Analysis of the H3 mechanism as a decomposition D q S ψ D k S ϕ of its surrogate attention matrix<ref type="foot" target="#foot_4">foot_4</ref> clarifies a connection to fast evaluation algorithms for matrix-vector multiplications. In particular, the generalization of ( <ref type="formula">8</ref>) to an arbitrary order is inspired by fast evaluation algorithms for structured dense matrices based on butterfly decompositions <ref type="bibr" target="#b25">(Li et al., 2015;</ref><ref type="bibr" target="#b8">Dao et al., 2019</ref><ref type="bibr">Dao et al., , 2022a))</ref>, with length of the decomposition closely tied to its expressivity (in the classes of matrices it can represent). The Hyena operator blends data control with a special case of butterfly decomposition.</p><p>Remark 3.3. Hyena operators have unbounded context. Namely, they are not artificially restricted by e.g., locality, and can learn long-range dependencies between any of the elements of v via long convolutions, which we discuss next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Hyena Filters</head><p>Here we provide details on the convolution parametrization. We represent the filters of each Hyena operator as a map from the time (or space) domain t to values h t , and learn it with a shallow feed-forward neural network (FFN):</p><formula xml:id="formula_12">h t = Window(t) • (FFN • PositionalEncoding)(t)<label>(7</label></formula><p>) This approach builds on the neural implicit representation literature <ref type="bibr" target="#b31">(Mildenhall et al., 2021;</ref><ref type="bibr" target="#b47">Sitzmann et al., 2020)</ref>, which has found application in long convolution layers <ref type="bibr">(Romero et al., 2021b,a)</ref>. One advantage of ( <ref type="formula" target="#formula_12">7</ref>) is given by the decoupling of filter length and parameter cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Specializing filters in Hyena</head><p>The window and positional encoding functions are used to specialize filters in Hyena operators, biasing them towards a specific type. Figure <ref type="figure" target="#fig_4">3</ref>.1 provides an important example: we choose at least one of the convolutions in Hyena to be shaped towards exponential decay, mirroring the findings of <ref type="bibr" target="#b26">(Li et al., 2022)</ref> in other applications. Interestingly, we find that long exponentially decaying filters display synergy with high-frequency filters, as they enable the operator to select specific inputs at specific steps<ref type="foot" target="#foot_5">foot_5</ref> . Similarly to <ref type="bibr">(Romero et al., 2021b)</ref>, we use high-frequency periodic activations (sine) in the FFN. This allows (7) to learn filters with high-frequency content, addressing the low-frequency bias of neural networks <ref type="bibr" target="#b1">(Basri et al., 2020)</ref>. Owing to the FFN, the parametrization in (7) can approximate filters obtained through other means, such as S4 <ref type="bibr" target="#b17">(Gu et al., 2020</ref><ref type="bibr" target="#b18">(Gu et al., , 2021))</ref>, CKConv <ref type="bibr">(Romero et al., 2021b)</ref>, SGConv <ref type="bibr" target="#b26">(Li et al., 2022)</ref> and Fourier Neural Operator (FNO) <ref type="bibr" target="#b27">(Li et al., 2020)</ref>.</p><p>Preserving causality Causality is necessary to train autoregressive language models, in order for the output at a given position to depend only on the past. For example, Transformers mask the attention matrix to be lower triangular. In the case of Hyena, causality can be guaranteed by parametrizing causal convolutions: Proposition 3.1 (Causal Hyenas). If each filter h n , n = 1, . . . , N is causal, then the corresponding Hyena N operator is causal.</p><p>In practice, we need not constrain the learning of the filter (7) to ensure its numerical causality. If we use FFT-based convolution algorithms, all we need is to evaluate the filter at t = 0, . . . , L -1 and zero-pad the input and filter sequences to 2L -1 before taking FFT.</p><p>Efficiency One bottleneck of long convolution models can be their low utilization of hardware accelerators, especially when they involve iterative numerical methods to materialize the filter<ref type="foot" target="#foot_6">foot_6</ref> . Evaluation of 7 is fast, since it involves a single forward pass of an FFN, and can be performed in parallel across sequence length and all orders of an Hyena operator as displayed in Algorithm 2, increasing hardware utilization. An additional source of low utilization is the FFT, which is also shared by other long other convolutional layers. This bottleneck can be partially addressed by blocking <ref type="bibr" target="#b46">(Selesnick and Burrus, 2017)</ref>, and optimization of the underlying routines <ref type="bibr">(Dao et al., 2022c)</ref>. We benchmark runtime in Section 4.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Hyena Algorithm</head><p>A forward pass of Hyena is summarized below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Projection</head><formula xml:id="formula_13">Require: Input sequence u ∈ R L×D 1.</formula><p>In parallel across L: ẑ = Linear(u), Linear : R D → R (N +1)D 2. In parallel across D: z = DepthwiseConv1d(h, ẑ), h is a short convolution filter 3. Reshape and split z into x 1 , . . . , x N , v. Dimensions of one element are</p><formula xml:id="formula_14">x n ∈ R D×L Return x 1 , . . . , x N , v, x n</formula><p>Algorithm 2 Hyena Filter Require: Sequence length L, positional embedding dimension D e 1. t = PositionalEncoding(L), t ∈ R L×De 2. In parallel across N, L: ĥ = FFN(t), FFN :</p><formula xml:id="formula_15">R De → R N D , ĥ ∈ R L×N D 3. Reshape to ĥ ∈ R N ×D×L 4. h = ĥ • Window(t), h ∈ R N ×D×L 5. Split h into h 1 , . . . , h N Return h 1 , . . . , h N Algorithm 3 Forward pass of Hyena Require: Input sequence u ∈ R L×D , order N , model width D, sequence length L, positional embedding dimension D e 1. x 1 , . . . , x N , v = Projection(u) 2. h 1 , . . . , h N = HyenaFilter(L, D e ) for n = 1, . . . , N do 3. In parallel across D: v t ← x n t • FFTConv(h n , v) t end for Return y = v</formula><p>Proposition 3.2 (Computational Complexity). The computational cost of processing an input u ∈ R L×D with an order-N Hyena operator is O(N DL(log 2 L + D))</p><p>2 7 2 9 2 11 2 13 2 15 0 20 40 60 80 100 Sequence Length Vocabulary Size: 10 2 7 2 9 2 11 2 13 2 15 Sequence Length Vocabulary Size: 20 2 7 2 9 2 11 2 13 2 15 Sequence Length Vocabulary Size: 30 2 7 2 9 2 11 2 13 2 15 Sequence Length Vocabulary Size: 40 Hyena CKConv Transfer Function H3 Conv1D FNO Associative Recall Figure 4.1: Benchmark of long convolution parametrizations in order 2 Hyena operators on associative recall (%). Our results show that implicit parametrizations scale more favorably in vocabulary size (number of possible values of tokens in the input) and length of the sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Shrinking the gap on in-context learning</head><p>We begin by empirically motivating the Hyena design, including the choice of long convolution parametrization. We consider the suite of tasks described in Table <ref type="table" target="#tab_1">4</ref>.1. Our evaluation is grounded in recent work on mechanistic interpretability of Transformers <ref type="bibr" target="#b13">(Elhage et al., 2021;</ref><ref type="bibr" target="#b38">Power et al., 2022;</ref><ref type="bibr" target="#b33">Olsson et al., 2022;</ref><ref type="bibr" target="#b57">Zhang et al., 2022)</ref>. Recently, associative recall, in particular, has been successfully used to guide the design of H3 <ref type="bibr">(Dao et al., 2022c)</ref>. We extend the suite of tasks from these works and include benchmarking more challenging versions of each task . For example, solving associative recall with a vocabulary size of only 10 reveals whether a model is structurally capable of performing recall. Testing on much longer sequences and larger vocabularies reveals additional gaps in performance that are otherwise hidden.</p><p>How to parametrize long convolutions We compare the performance of the following long convolution parametrizations for S 1 and S 2 in an order 2 Hyena:</p><p>• Conv1d: Explicit convolutions (regular convolution layers with fixed filter size).</p><p>• FNO: Filters parametrized explicitly in the frequency-domain <ref type="bibr" target="#b27">(Li et al., 2020)</ref>.</p><p>• H3: Implicit parametrization using state-space models (SSMs), in particular the standard S4 <ref type="bibr" target="#b18">(Gu et al., 2021)</ref>.</p><p>• TransferFunc: Implicit parametrization via transfer functions, a classical system-theoretic generalization of SSMs 8</p><p>• CKConv: Implicit parametrization using FFNs <ref type="bibr">(Romero et al., 2021b</ref>).</p><p>8 Transfer functions roughly correspond to a frequency-domain representation of SSMs.  • Hyena: Combination of implicit parametrizations via FFNs (with exponential decay modulation as shown in Figure <ref type="figure" target="#fig_4">3</ref>.1), and short explicit filters.</p><p>All models have the same width and 2 layers. Figure <ref type="figure" target="#fig_5">4</ref>.1 shows implicit approaches based on FFNs outperform other long convolutions, with the gap widening on longer sequences and larger vocabulary sizes. We train a different model on each setting of sequence length and vocabulary size. The ranking is correlated with the ability to decouple sequence length from parameter count (Hyena, CKConv, TransferFunc, H3) and expressivity (Hyena, CKConv). We observe similar trends on the other tasks.</p><p>Pushing sequence length to the limit Next, we evaluate associative recall performance on extremely long sequences of length 131k. To the best of our knowledge, these represent the first empirical display of attention-free in-context learning on sequences of this length. The gap between parametrization schemes widens as shown in Appendix A, with Hyena outperforming CKConv by 80 points.</p><p>Comparing operators We repeat our associative recall experiment, this time benchmarking different 2 layer models rather than changing the convolution parametrization: an order 2 Hyena, GSS <ref type="bibr" target="#b30">(Mehta et al., 2022)</ref>, H3 <ref type="bibr">(Dao et al., 2022c)</ref>, AFT-conv <ref type="bibr" target="#b55">(Zhai et al., 2021)</ref>, RWKV <ref type="bibr" target="#b36">(Peng, 2021)</ref>, and a standard GPT <ref type="bibr" target="#b3">(Brown et al., 2020)</ref> using FlashAttention <ref type="bibr">(Dao et al., 2022b)</ref>. As shown in Table <ref type="table" target="#tab_1">4</ref>.2, Hyena is the only operator able to solve the task. Our results challenge the observation that only Transformers are capable of challenging in-context learning. Surprisingly, rankings of model performance at a fixed sequence length on The Pile are consistent with rankings on aggregate scores on our synthetics (Appendix C).</p><p>Generality of Hyena operators and filters Hyena operators and filters can also applied successfully beyond language tasks. We experiment on sequential CIFAR, where pixels are flattened as a sequence, and use the same operator defined for language. We reach the accuracy of standard S4 <ref type="bibr" target="#b18">(Gu et al., 2021)</ref> with same model size (91%). In Section 4.5 and Appendix A, we discuss larger-scale image classification experiments with Hyena.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Language Modeling</head><p>Next, we verify the scaling of Hyena on autoregressive language modeling. We evaluate the perplexity on WikiText103 (Table <ref type="table" target="#tab_1">4</ref>.3) and The Pile (Table <ref type="table" target="#tab_1">4</ref>.4). On the The Pile, we train different models for 5, 10, 15 billion tokens (different runs), adjusting the learning rate scheduler. Hyena is the first attention-free, convolution architecture to match GPT quality with a 20%<ref type="foot" target="#foot_7">foot_7</ref> reduction in total FLOPs. Preliminary scaling laws are shown in Figure <ref type="figure" target="#fig_5">4</ref>.2, collecting the training runs at 5, 10, 15 billion tokens. Each curve represents a different training run. In Appendix A, we provide results on the PG-19 long-range benchmark <ref type="bibr" target="#b40">(Rae et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Downstream Evaluation</head><p>We perform a downstream evaluation on SuperGLUE <ref type="bibr" target="#b51">(Wang et al., 2019)</ref> tasks. We compare Hyena (trained for 137 billion tokens) with the best available pre-trained attention-free model, RWKV <ref type="bibr" target="#b36">(Peng, 2021)</ref> (trained 1.3 1.6 2.6 3.2 3.9 4.9 •10 19 2.44 2.29 2.21 FLOPs Loss Data Scaling on The Pile, 355M parameters Hyena GPT Figure 4.2: Preliminary "scaling law" of language models on The Pile. Comparison of our approach (red) based on long convolutions and gating (Hyena) and a standard GPT (blue) (Brown et al., 2020). We reach perplexity of GPT with a smaller training FLOP budget. Table 4.3: Perplexity on WikiText103 (same tokenizer). * are results from (Dao et al., 2022c). Deeper and thinner models (Hyena-slim) achieve lower perplexity. Model Perplexity Transformer (125M) 18.6 Hybrid H3 (125M) 18.5 * Performer (125M) 26.8 * Reformer (125M) 25.6 * AFT-conv (125M) 28.2 Linear Attention (125M) 25.6 * Hyena-3 (125M) 18.6 Hyena-3-slim (125M) 18.5 Table 4.4: Perplexity on The Pile for models trained until a total number of tokens e.g., 5 billion (different runs for each token total). All models use the same tokenizer (GPT2). FLOP count is for the 15 billion token run. Model 5B 10B 15B FLOPs (10 19 ) GPT (125M) 13.3 11.9 11.2 1.88 Hyena-2 (153M) 13.3 11.8 11.1 1.87 GPT (355M) 11.4 9.8 9.1 4.77 Hyena-2 (355M) 11.3 9.8 9.2 3.93</p><p>for 332 billion tokens), and a reference GPTNeo <ref type="bibr" target="#b2">(Black et al., 2021)</ref> (trained for 300 billion tokens) of the same size. Tables 4.5 and 4.6 summarize the results. Hyena performs similarly to other models despite having been trained on less than half the number of total tokens. We observe Hyena to display characteristic few-shot capabilities of standard Transformers, with some tasks e.g., MultiRC seeing a lift of more than 20% accuracy over zero-shot when the model is provided additional prompts as context. The improvements are more noticeable in generation tasks, where the additional prompts can instruct the model on how it should be responding to the questions. We report an additional downstream evaluation on the LAMBADA task <ref type="bibr" target="#b35">(Paperno et al., 2016)</ref> in Appendix A.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Benchmarking</head><p>We benchmark runtime of an order 2 Hyena operator compared to attention and FlashAttention layers <ref type="bibr">(Dao et al., 2022b)</ref>. Hyena uses a fused CUDA kernel to perform FFTConv <ref type="bibr">(Dao et al., 2022c)</ref>. We set batch size to 64 and measure runtime (in milliseconds). Results are provided in Figure <ref type="figure" target="#fig_5">4</ref>.3. Hyena speedups reach 100× at sequence length 64K. Crossover points for Hyena and attention is at length 2048, and for Hyena and FlashAttention is between 4096 and 8196. Despite the absolute reduction in FLOPs, speedups are achieved only on longer sequences when the gap grows sufficiently large. This occurs because hardware utilization of Hyena is lower than FlashAttention. We expect the gap between theoretical maximum speedup to shrink with improved implementations of FFTConv and specialized hardware.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Large-Scale Image Classification</head><p>Finally, we demonstrate the potential of Hyena as a general deep learning operator by applying it to image classification. On ImageNet, we drop-in replace attention layers in the Vision Transformer (ViT) <ref type="bibr" target="#b12">(Dosovitskiy et al., 2020)</ref> with the Hyena operator (without changes from its language counterpart) and match performance with ViT. We also show that using smaller image patches boosts performance in both attention and Hyena. Since this results in longer sequence lengths, we expect Hyena to outperform in speed as patches get more fine-grained approaching pixel-level. On CIFAR-2D, we test a 2D version of Hyena long convolution filters in a standard convolutional architecture, which improves on the 2D long convolutional model S4ND <ref type="bibr" target="#b32">(Nguyen et al., 2022)</ref> in accuracy with a 8% speedup and 25% fewer parameters. See Appendix A.4 for additional vision architectures and training procedure details. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Conclusion</head><p>In this work, we introduced an attention-free drop-in replacement to the core building block of many largescale language models. Hyena operators are a recurrence of gating and implicitly parametrized long convolutions, can be evaluated efficiently in subquadratic time, and can learn in-context on very long sequences. On The Pile, deep stacks of Hyena operators constitute one of the first attention-free, convolutional architectures to match perplexity and downstream performance of Transformers with a significant reduction in training compute. Our promising results at the sub-billion parameter scale suggest that attention may not be all we need, and that simpler subquadratic designs such as Hyena, informed by a set of simple guiding principles and evaluation on mechanistic interpretability benchmarks, may form the basis for efficient large models. We are excited about what new capabilities Hyena opens up as we scale and optimize the inference speed of these models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Experimental Details</head><p>An implementation of Hyena can be found at this link.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Mechanistic Design Synthetic Benchmarks</head><p>Our synthetic reasoning are inspired by mechanistic interpretability <ref type="bibr" target="#b13">(Elhage et al., 2021)</ref>, in-context learning (ICL) <ref type="bibr" target="#b16">(Garg et al., 2022)</ref> and language model benchmarking <ref type="bibr" target="#b28">(Liang et al., 2022)</ref> research. The evaluation revolves around 4 main tasks:</p><p>• Associative recall: Each string is produced by concatenating key-value tuples from a different random dictionary. This test verifies whether a model is able to extract right value given a key as prompt, effectively applying a data-controlled shift (delay).</p><p>• Majority voting and counting: Testing if a model can densely activate its data-controlled matrix i.e., through many non-zero entries (consider the string 'a a a a a a a a a a b → a').</p><p>• ICL of linear functions: Verifying whether a model can perform ICL on real-valued inputs. Prompts are generated as x 1 , w k x 1 , . . . , x n → w k x n , where both x k and w k ∈ R no are sampled from a normal distribution.</p><p>• Arithmetic: Basic capability check.</p><p>For each task, we train models using the hyperparameters shown in Table <ref type="table">A</ref>.1. We consider increasing settings of difficulty controlled by sequence length, spanning values <ref type="bibr">1024, 2048, 4098, 8196, 16392, 32784, 65568, 131136 and vocabulary sizes 10, 20, 30, 40.</ref> For ICL of functions, we vary instead the dimension n o .</p><p>Note that for associative recall on longer sequences, multiple copies of key-value tuples appear in the prompt. To see this, consider how likely it is to sample multiple copies of a particular key-value pair with a vocabulary size of 40, in order to form a sequence of 100k characters. Models capable of looking further back in the sequence effectively see more data, and can solve challenging versions of the in-context learning task. Increasing the vocabulary size has the increasing the average distance between instances of the same key-value pair in each prompt, highlighting performance gaps between different approaches.</p><p>Table A.1: (Hyperparameter settings for reasoning and in-context learning tasks.). Optimizer AdamW Optimizer momentum β1, β2 = 0.9, 0.98 Base learning rate 0.0005 Weight decay 0.1 Dropout None Batch size 32 Training epochs 200 Num samples 2000 Learning rate schedule cosine decay Warmup epochs 10 Warmup schedule linear Number of layers 2 Width 64</p><p>Long convolution comparisons: We compare different convolution parametrizations, embedding them in an order 2 Hyena operator. All convolutions are applied separately to input channels (referred to as single-input single-output (SISO) in signal processing, or depthwise in other machine learning contexts).</p><p>• Conv1d: Explicit convolutions (regular convolution layers with fixed filter size). We use a fixed filter size of 64, to match parameters of the other approaches.</p><p>• FNO: Filters parametrized explicitly in the frequency-domain <ref type="bibr" target="#b27">(Li et al., 2020)</ref>. We set the number of modes to 64.</p><p>• H3: Implicit parametrization using state-space models (SSMs), and in particular the standard S4 <ref type="bibr" target="#b18">(Gu et al., 2021)</ref>. We set the state dimension to 64.</p><p>• TransferFunc: Implicit parametrization via transfer functions, a classical system-theoretic generalization of SSMs. Transfer functions are defined by a ratio of polynomials (we parametrize the coefficients, and evaluate the polynomials efficiently via FFTs). We set the order to 64.</p><p>• CKConv: Implicit parametrization using FFNs <ref type="bibr">(Romero et al., 2021b)</ref>.</p><p>• item Hyena: Combination of implicit parametrizations via FFNs (with exponential decay modulation as shown in Figure <ref type="figure" target="#fig_4">3</ref>.1), and short explicit filters.</p><p>CKConv and Hyena use the same size of FFNs (width 32 to match in parameters).</p><p>In Table <ref type="table">A</ref>.1, we report additional results on the challenging setting of sequence length 131072 and vocabulary size 30. Implicit parametrizations of convolutions outperform explicit parametrizations on associative recall, with CKConv and Hyena greatly improving on the ability to extract the right key, value relations from different inputs. In Appendix C, we discuss how results on our synthetic tasks can be indicative of performance at a larger scale.</p><p>Table A.2: Test accuracy (%) in associative recall on sequences of length 131072, vocabulary size 30. Hyena CKConv TransferFunc H3 FNO Conv1d 97.2 14.3 0.5 0.6 0.3 0.5</p><p>Operator comparisons: We compare different models on the same associative recall task, using hyperparameters in Table <ref type="table">A</ref>.1. Hyena uses our filter parametrization with decay windowing for long convolutions, and short explicit convolutions of size 3 after the dense input projections. All other models use defaults from their largest scale experiment, while keeping the size to 2 layers and width 64.</p><p>A note on Transformer performance Transformers can solve associative recall tasks with longer sequences, provided the length does not prevent them from fitting in memory, and enough examples are present in the training data. In all our experiments, we keep the number of samples fixed (2000), a regime where Transformers struggle to find the generalizing solution (see Table <ref type="table">A</ref>.1).</p><p>For shorter sequences (see Appendix C), Transformers solve the task easily even with limited data, comparably to Hyena.</p><p>More broadly, these different properties of attention and attention-free token-mixing layers may explain improved performance when they are combined in hybrid architectures <ref type="bibr">(Dao et al., 2022c)</ref>. The focus on this work has been identifying an architecture capable of performing without attention, which is necessary to tackle domains where long sequences are common. However, when training with shorter sequences (up to 8k), if final downstream performance is the only metric of interest, improved results can be obtained by hybridizing our models similarly to H3 <ref type="bibr">(Dao et al., 2022c)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Language Modeling</head><p>WikiText103: We train 125M parameter models on WikiText103 and compare perplexity to Transformers, hybrid models such as H3 <ref type="bibr">(Dao et al., 2022c)</ref>, and other variants of subquadratic attention. All models use the same GPT2 tokenizer with vocabulary size 50257. We test order 3 Hyena with our proposed filter parametrization for two long convolutions, and a shorter explicit convolution on the third. We also consider Hyena (slim) that are 1.5x deeper than Transformers (12 versus 18 layers), with width multiplier of the FFNs set to 2. We find trading-off width for depth to be generally favourable. These modifications are made possible by the reduction in overall FLOPs of Hyena operators compared to self-attention, in particular non-parametric FLOPs which include materialization of the attention matrix, application of softmax, and matrix-value reduction.</p><p>And we can define the surrogate attention matrix A ψ ϕ (q, k)</p><formula xml:id="formula_16">[A ψ ϕ (q, k))] t,t = q t L-1 m=0 ψ t-m k m ϕ m-t .<label>(12)</label></formula><p>Continuous Signals: We can also consider the case of continuous signals on a group G. In the continuous case, we can expand the convolutions in ( <ref type="formula">8</ref>) as</p><formula xml:id="formula_17">(ϕ * v) t = G ϕ t-g v g dg, (ψ * z) t = G ψ t-g z g dg<label>(13)</label></formula><p>This allows us to rewrite (8) as</p><formula xml:id="formula_18">y t = q t (ψ * k(ϕ * v)) t = q t G ψ t-g k g G ϕ g-τ v τ dτ dg = q t G G ψ t-g k g ϕ g-τ v τ dτ dg = q t G G ψ t-g k g ϕ g-τ v τ dg dτ Variable swap = G q t G ψ t-g k g ϕ g-τ v τ dg dτ Pull q t in τ integral = G q t G ψ t-g k g ϕ g-τ dg v τ dτ Pull v τ out of g integral. (<label>14</label></formula><formula xml:id="formula_19">)</formula><p>There is a linear operator A : v → y = Av which we interpret as the surrogate attention operator. A is conditioned on the query q, key k and filters ϕ and ψ, A = A ψ ϕ (q, k). The kernel K of the operator is given by</p><formula xml:id="formula_20">K(t, t ) = q t G ψ t-g k g ϕ g-t dg<label>(15)</label></formula><p>Operator decomposition of the surrogate attention matrix We can decompose the linear map v → y; y = A ψ ϕ (q, k)v into a sequence of factors, each dependent on a projection of the input A ψ ϕ (q, k) = A ψ (q)A ϕ (k). Let D q and D k be the L-by-L diagonal matrices whose respective main diagonal entries are the respective entries of q and k. Then, we have that</p><formula xml:id="formula_21">A ψ (q) = D q S ψ , D q = diag(q), A ϕ (k) = D k S ϕ , D k = diag(k).<label>(16)</label></formula><p>The matrix has been decomposed into two terms A ψ (q) and A ϕ (k) constructed by multiplying the diagonal matrices D q and D k with the Toeplitz matrices S ψ and S ϕ . S ψ and S ϕ are the kernels of the convolution operators with filter's impulse responses ψ and ϕ respectively. In the current applications of interest, ψ and ϕ are chosen to be causal, i.e. ψ[t] = 0 for t &lt; 0 and ϕ[t] = 0 for t &lt; 0. This results in S ψ and S ϕ to be lower triangular matrices</p><formula xml:id="formula_22">S ψ =      ψ 0 0 • • • 0 ψ 1 ψ 0 • • • 0 . . . . . . . . . . . . ψ L-1 ψ L-2 • • • ψ 0      , S ϕ =      ϕ 0 0 • • • 0 ϕ 1 ϕ 0 • • • 0 . . . . . . . . . . . . ϕ L-1 ϕ L-2 • • • ϕ 0      . (<label>17</label></formula><formula xml:id="formula_23">)</formula><p>The surrogate attention matrix is then given by</p><formula xml:id="formula_24">A ψ ϕ (q, k) = D q S ψ D k S ϕ<label>(18)</label></formula><p>We can expand the matrix multiplications in ( <ref type="formula" target="#formula_21">16</ref>) in the case of causal filters ϕ and ψ as</p><formula xml:id="formula_25">D q         q 0 q 1 . . . q L-1         S ψ         ψ 0 ψ 1 ψ 0 . . . . . . . . . ψ L-1 ψ L-2 • • • ψ 0         D k         k 0 k 1 . . . k L-1         S ϕ         ϕ 0 ϕ 1 ϕ 0 . . . . . . . . . ϕ L-1 ϕ L-2 • • • ϕ 0         =      q 0 ψ 0 q 1 ψ 1 q 1 ψ 0 . . . . . . . . . q L-1 ψ L-1 q L-1 ψ L-2 • • • q L-1 ψ 0      A ψ (q)      k 0 ϕ 0 k 1 ϕ 1 k 1 ϕ 0 . . . . . . . . . k L-1 ϕ L-1 k L-1 ϕ L-2 • • • k L-1 ϕ 0      A ϕ (k)<label>(19)</label></formula><p>Fourier decomposition of convolution operators: The kernels of the convolution operators S ψ and S ϕ are diagonalized by the Fourier transform matrix W ∈ C L×L , W nm = z m , z = e j2πn/L . The Fourier transform of the convolution operator S ψ is given by</p><formula xml:id="formula_26">S ψ = W * D Ψ W, S Φ = W * D Φ W<label>(</label></formula><p>20) where D Ψ , D Φ ∈ C L×L are diagonal matrices constructed from the frequency responses (the discrete Fourier transform) Ψ = Wψ, Φ = Wϕ, respectively. This decomposition can be used to simplify the matrix multiplication in (19):</p><formula xml:id="formula_27">A = D q S ψ D k S ϕ = D q W * D Ψ WD k W * D Φ W<label>(21</label></formula><p>) An important property of the above is the non-commutativity of D q and S k with W * . If the two operators commuted, we would obtain</p><formula xml:id="formula_28">A = D q W * D Ψ WD k W * D Φ W = W * D q D Ψ D k D Φ W<label>(22)</label></formula><p>which reduces the entire layer to a simple convolution. The non-commutativity of the gating term acts as a non-linearity in chain of convolution operators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Discussion and Additional Results</head><p>Vocabulary size scaling Table <ref type="table">C</ref>.1 showcases interesting correlation between associative recall performance for varying vocabulary sizes and loss on the The Pile. In this case, we fix sequence length for associative recall to be 2048, the same sequence length used to train all models on the The Pile.</p><p>We observe a similar phenomenon on other slices of tasks from our mechanistic design benchmarks, indicating that it may be possible to derive predictive laws for performance at scale, based on fast experimentation on synthetic tasks with models of 1 or 2 layers. Surprisingly, performance on our language synthetics appears to be further linked to performance as attention replacement in other domains (Appendix A.4 for results on image classification).</p><p>Table <ref type="table">C</ref>.1: Hyena Accuracy on associative recall with varying vocabulary size 10, 20, 30, 40 in relation to test loss on The Pile after 5 billion tokens. We notice a correlation between the two performance metrics, suggesting that slices of our mechanistic design synthetics may be potentially predictive of performance at scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Acc @ 10 Acc @ 20 Acc @ 30 Acc @ 40 Loss @ 5B on The Pile</p><p>Conv1d 32 11 10 8 4.21 AFT-conv 55 21 12 10 3.57 H3 92 60 13 10 2.69 Transformer 100 100 92 82 2.59 Hyena 100 100 98 85 2.59 D Samples and Visualizations D.1 Hyena Matrices We provide visualizations of attention and Hyena matrices activated by test strings. In D.1, D.2, we compare GPTNeo (Black et al., 2021) attention matrices with Hyena matrices extracted by our pre-trained small Hyena model. In D.3 and D.4, we provide additional Hyena matrices for the 355M model, activated by test strings of different length.</p><p>For attention, we visualize the raw post-softmax matrix. For Hyena matrices, we plot the (element-wise) absolute value of H(u):</p><formula xml:id="formula_29">H(u) = D N x S N h • • • D 2 x S 2 h D 1 x S 1 h Ĥ(u) ij = |H(u) ij |</formula><p>Since Hyena does not normalize the entries of its matrices with e.g., softmax, there are notable differences with attention: (1) the entries of H(u) can be either positive and negative, and (2) the magnitude is unconstrained.</p><p>We observe the magnitude of matrices in pre-trained Hyena models to be around 10 -3 .     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 Downstream Examples</head><p>MultiRC We report examples of downstream evaluation of small models on the MultiRC question-answering task. We report answers of small Hyena (153M, trained for 130B tokens on The Pile) and the public checkpoint RWKV-v4 (Peng, 2021) (169M, trained for 332B tokens on The Pile). We select randomized examples with indices being powers of 2. Alignment of Hyena's responses to the task format is greatly improved by providing few-shot examples in the prompt, which may be a promising sign for larger models based on the proposed attention-free architecture. Each example shows responses for specific examples in the validation set (example indices are listed at the beginning of each example).</p><p>MultiRC index: 2, 4</p><p>Passage: What causes a change in motion? The application of a force. Any time an object changes motion, a force has been applied. In what ways can this happen? Force can cause an object at rest to start moving. Forces can cause objects to speed up or slow down. Forces can cause a moving object to stop. Forces can also cause a change in direction. In short, forces cause changes in motion. The moving object may change its speed, its direction, or both. We know that changes in motion require a force. We know that the size of the force determines the change in motion. How much an object's motion changes when a force is applied depends on two things. It depends on the strength of the force. It also depends on the object's mass. Think about some simple tasks you may regularly do. You may pick up a baseball. This requires only a very small force.</p><p>Question: Based on the previous passage, Would the mass of a baseball affect how much force you have to use to pick it up? Is "Less the mass, less the force applied" a correct answer?</p><p>Answer: Yes RWKV zero-shot: The mass of a baseball depends on the force applied. if the mass of the baseball is small, the force applied is small.</p><p>Hyena zero-shot: Yes it is.</p><p>RWKV few-shot: The mass, less the force applied, is a more important force. it is the force that changes the mass of a baseball.</p><p>Hyena few-shot: Yes.</p><p>Question: Based on the previous passage, What do you apply to an object to make it move or stop? Is "Strength" a correct answer?</p><p>Answer: No RWKV zero-shot: The force applied to the object is the force applied to the object.</p><p>Hyena zero-shot: Yes.</p><p>RWKV few-shot: The force applied to a baseball is a force. it is a force that is applied to a baseball.</p><p>Hyena few-shot: Yes.</p><p>Passage: The film opens with Sunita, a medical student, and her friends working on a project about the human brain. She wants to investigate the curious case of Sanjay Singhania, a notable city businessman, who is reported to have anterograde amnesia. Her professor denies access to Sanjay's records as it is currently under criminal investigation. Sunita, nonetheless, decides to investigate the matter herself. Sanjay is introduced as he brutally murders a man. He takes a Polaroid picture of the man, and writes on it "done". It is revealed that Sanjay has anterograde amnesia where he loses his memory every 15 minutes. Sanjay uses a system of photographs, notes, and tattoos on his body to recover his memory after each cycle. It is revealed that Sanjay is ultimately out to avenge the death of his sweetheart Kalpana, and that he is systematically killing the people who were responsible for it. His main target is "Ghajini", a notable social personality in the city. Police Inspector Arjun Yadav, on the case of the serial murders, tracks Sanjay down to his flat and attacks and disables him. Yadav finds two diaries where Sanjay has chronicled the events of 2005 and 2006 . The film flashes back to 2005 as Yadav reads the diary. Sanjay Singhania is shown as the owner of the Air Voice mobile telephone company. In the course of his business, Sanjay sends his men to meet Kalpana, a struggling model, about putting up a billboard above her apartment. The owner of Kalpana's advertising firm misinterprets this as a romantic advance, and in view of a possible lucrative Air Voice ad campaign and other benefits, encourages Kalpana to accept the overture.</p><p>Question: Based on the previous passage, Why did Sanjay murder a man? Is "Sanjay" a correct answer?</p><p>Answer: Yes RWKV zero-shot: The answer is "Sanjay".</p><p>Hyena zero-shot: The answer is RWKV few-shot: The question is not about Sanjay's death.</p><p>Hyena few-shot: No.</p><p>Question: Based on the previous passage, Every 15 minutes, Sanjay goes through what process, Which frustrates his attempts to avenge the death of his sweetheart? Is "He forgets about facts" a correct answer?</p><p>Answer: Yes RWKV zero-shot: Yes, it is a correct answer..</p><p>Hyena zero-shot: The answer is.</p><p>RWKV few-shot: The answer is "yes".</p><p>Hyena few-shot: The answer is "yes.</p><p>Passage: In 1863, Alexander II re-convened the Diet of Finland and initiated several reforms increasing Finlandś autonomy from Russia including establishment of its own currency, the markka. Liberation of business led to increased foreign investment and industrial development. Finland also got its first railways, separately established under Finnish administration. Finally, the elevation of Finnish from a language of the common people to a national language equal to Swedish opened opportunities for a larger proportion of the society. Alexander II is still regarded as "The Good Tsar" in Finland. These reforms could be seen as results of a genuine belief that reforms were easier to test in an underpopulated, homogeneous country, than in the whole of Russia. They may also be seen as a reward for the loyalty of its relatively western-oriented population during the Crimean War and during the Polish uprising. Encouraging Finnish nationalism and language can also be seen as an attempt to dilute ties with Sweden.</p><p>Question: Based on the previous passage, Alexander II is considered what in Finland since 1863? Is "The good tsar" a correct answer?</p><p>Answer: Yes RWKV zero-shot: The correct answer is "yes".</p><p>Hyena zero-shot: Yes, the question is correct.</p><p>RWKV few-shot: The Finnish language is a language of the people.</p><p>Hyena few-shot: Yes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 . 1 :</head><label>21</label><figDesc>Figure 2.1: Comparison between data-controlled matrices: SelfAttention and Hyena.</figDesc><graphic coords="5,142.10,70.87,324.49,80.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 . 3 :</head><label>43</label><figDesc>Figure 4.3: Benchmarking runtime of Hyena, Attention and FlashAttention with varying sequence lengths. Batch size is set to 64. The figure on the right is an inset showing a zoomed-in portion of the figure on the left.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure D. 1 :</head><label>1</label><figDesc>Figure D.1: Attention matrices from a GPTNeo small model. "We use the test string "Attention is all you need. Attention is".</figDesc><graphic coords="27,70.87,136.12,470.27,468.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure D. 2 :</head><label>2</label><figDesc>Figure D.2: Hyena matrices from a Hyena small (same model used for SuperGLUE downstream evaluations)."We use the test string "Attention is all you need. Attention is". We note that Hyena has a different data-controlled matrix for each channel i.e. for each dimension in its width, since it does not use heads.</figDesc><graphic coords="28,70.87,137.39,470.27,457.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure D. 3 :</head><label>3</label><figDesc>Figure D.3: Data-controlled Hyena matrices (355M model), activated by the string "When a doctor doctors a doctor, does the doctor doing the doctoring doctor as the doctor being doctored wants to be doctored or does the doctor doing the doctoring doctor as they want to doctor? ". Rows in the plot are matrices from different layers, columns are matrices from different channels. The operator shows characteristic patterns of attention matrices, without attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure D. 4 :</head><label>4</label><figDesc>Figure D.4: Data-controlled Hyena matrices (355M model), activated by the string "Mrs. Dursley, Mr. Dursley, Dudley Dursley", from Causal scrubbing: results on induction heads. Rows in the plot are matrices from different layers, columns are matrices from different channels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure D. 5 :</head><label>5</label><figDesc>Figure D.5: [Top]: Long convolution Hyena filters at initialization (153M parameters, 18 layer model). [Bottom]: Filters after training for 130 billion tokens on The Pile.</figDesc><graphic coords="32,71.56,375.48,465.56,275.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="2,70.87,70.87,470.29,116.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 4 .</head><label>4</label><figDesc>1: A selection of our mechanistic design benchmarks.</figDesc><table><row><cell>Task</cell><cell>Prompt</cell><cell>Target</cell></row><row><cell>Associative Recall</cell><cell>a, 1, b, e, 3, f, b</cell><cell>e</cell></row><row><cell>Majority</cell><cell>a, g, g, g, e, f, g</cell><cell>g</cell></row><row><cell>Counting</cell><cell>a, b, b, b, a, c, b</cell><cell>4</cell></row><row><cell>ICL of Functions</cell><cell>x0, f (x0), . . . xn</cell><cell>f (xn)</cell></row><row><cell>Arithmetic</cell><cell>1, 3, 5, +, 6, 8, 3</cell><cell>8, 1, 8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>2: Test accuracy (%) for associative recall on longer sequences, vocabulary size 30. The symbol is used to mark settings where the model does not fit in memory.</figDesc><table><row><cell cols="5">Sequence length Hyena FlashTransformer Transformer GSS H3 AFT RWKV</cell></row><row><cell>30k</cell><cell>100.0</cell><cell>32.4</cell><cell>5.3 8.4 2.3</cell><cell>12.4</cell></row><row><cell>64k</cell><cell>100.0</cell><cell>26.7</cell><cell>2.1 4.3 1.2</cell><cell>6.5</cell></row><row><cell>131k</cell><cell>97.2</cell><cell></cell><cell>0.1 0.6 0.8</cell><cell>2.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>5: Zero-shot accuracy (%) on SuperGLUE tasks for small models. Table4.6: Few-shot (3) accuracy (%) on SuperGLUE tasks for small models.</figDesc><table><row><cell>Model</cell><cell cols="7">WSC WIC RTE CB MultiRC ReCoRD BoolQ COPA Average</cell></row><row><cell cols="3">GPTNeo (Black et al., 2021) 27.9 50.0 45.1 41.1</cell><cell>0.0</cell><cell>61.7</cell><cell>62.2</cell><cell>62.0</cell><cell>43.8</cell></row><row><cell>RWKV (Peng, 2021)</cell><cell cols="2">13.4 52.3 46.9 25.0</cell><cell>0.0</cell><cell>58.5</cell><cell>59.2</cell><cell>66.0</cell><cell>40.2</cell></row><row><cell>Hyena</cell><cell>21.2</cell><cell>50.5 46.6 39.3</cell><cell>1.1</cell><cell>59.4</cell><cell>51.8</cell><cell>70.0</cell><cell>41.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>7: Image classification top-1 accuracy.</figDesc><table><row><cell>Model</cell><cell cols="2">Patch Size Seq Len</cell><cell>Dataset</cell><cell>Acc (%)</cell></row><row><cell>ViT (87M)</cell><cell>16x16</cell><cell>196</cell><cell>ImageNet-1k</cell><cell>78.5</cell></row><row><cell>Hyena-ViT (88M)</cell><cell>16x16</cell><cell>196</cell><cell>ImageNet-1k</cell><cell>78.5</cell></row><row><cell>ViT (87M)</cell><cell>8x8</cell><cell>1024</cell><cell>ImageNet-1k</cell><cell>80.0</cell></row><row><cell>Hyena-ViT (88M)</cell><cell>8x8</cell><cell>1024</cell><cell>ImageNet-1k</cell><cell>79.8</cell></row><row><cell>S4ND-ISO (268k)</cell><cell>-</cell><cell>-</cell><cell>CIFAR-10</cell><cell>89.9</cell></row><row><cell>Hyena-ISO (202k)</cell><cell>-</cell><cell>-</cell><cell>CIFAR-10</cell><cell>91.2</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Self-attention can be expressed as y = A(k, q)v where A is the attention matrix conditioned by linear projections k, q of the input and multiplied by v, another projection.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>FlashAttention is already 2-4x faster than a standard attention implementation in PyTorch.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>In the L 1 (Z) sense:∞ t=-∞ |ht| &lt; ∞</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>For consistency with our discussion, we have swapped k and v compared to the notation in(Dao et al., 2022c).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>Some of this analysis is reported in the Appendix.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>This observation finds mirrors in the parametrization of the convolutions in H3(Dao et al., 2022c)  as a shift SSM and a diagonal SSM.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>In contrast, deep learning primitives are designed for high GPU utilization, with FFNs and attention usually reaching 50 -70% or higher, if optimized.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7"><p>The FLOP reduction consists in the non-parametric FLOPs of SelfAttention devoted to attention matrix computation. The ratio of parametric to non-parametric FLOPs (and hence the gains) depend on the ratio of model width D and sequence length L used in training.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We would like to thank <rs type="person">Karan Goel</rs>, <rs type="person">Albert Gu</rs>, <rs type="person">Avanika Narayan</rs>, <rs type="person">Khaled Saab</rs>, <rs type="person">Michael Zhang</rs>, <rs type="person">Elliot Epstein</rs> and <rs type="person">Sabri Eyuboglu</rs> for helpful discussion and feedback on earlier drafts, and <rs type="person">Together Computer</rs> and <rs type="institution">Crusoe</rs> for providing the compute used to train models in this paper. We gratefully acknowledge the support of <rs type="funder">NIH</rs> under No. <rs type="grantNumber">U54EB020405</rs> (Mobilize), <rs type="funder">NSF</rs> under Nos. <rs type="grantNumber">CCF1763315</rs> (<rs type="affiliation">Beyond Sparsity</rs>), CCF1563078 (Volume to Velocity), and 1937301 (RTML); <rs type="institution">US DEVCOM ARL</rs> under No. <rs type="grantNumber">W911NF-21-2-0251</rs> (<rs type="funder">Interactive Human-AI Teaming)</rs>; <rs type="funder">ONR</rs> under No. <rs type="grantNumber">N000141712266</rs> (<rs type="affiliation">Unifying Weak Supervision</rs>); <rs type="grantNumber">ONR N00014-20-1-2480</rs>: <rs type="projectName">Understanding and Applying Non-Euclidean Geometry in Machine Learning</rs>; <rs type="grantNumber">N000142012275</rs> (NEPTUNE); NXP, <rs type="funder">Xilinx</rs>, <rs type="affiliation">LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF</rs>, <rs type="funder">Accenture</rs>, <rs type="funder">Ericsson</rs>, <rs type="funder">Qualcomm</rs>, <rs type="funder">Analog Devices</rs>, <rs type="person">Google Cloud</rs>, <rs type="funder">Salesforce</rs>, <rs type="funder">Total</rs>, the <rs type="institution">HAI-GCP Cloud Credits for Research program</rs>, the <rs type="funder">Stanford Data Science Initiative</rs> (SDSI), <rs type="funder">Department of Defense (DoD)</rs> through the <rs type="funder">National Defense Science and Engineering Graduate Fellowship (NDSEG) Program</rs>, and members of the Stanford DAWN project: <rs type="funder">Facebook</rs>, <rs type="person">Google</rs>, and VMWare. This work is supported by <rs type="funder">NSF</rs> (<rs type="grantNumber">1651565</rs>), <rs type="funder">AFOSR</rs> (<rs type="grantNumber">FA95501910024</rs>), <rs type="funder">ARO</rs> (<rs type="grantNumber">W911NF-21-1-0125</rs>), <rs type="funder">ONR</rs>, <rs type="funder">DOE</rs> (<rs type="grantNumber">DE-SC0022222</rs>), <rs type="person">CZ Biohub</rs>, and Sloan Fellowship. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of <rs type="funder">NIH</rs>, <rs type="institution">ONR</rs>, or the <rs type="institution">U.S. Government</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Dhd2sSg">
					<idno type="grant-number">U54EB020405</idno>
				</org>
				<org type="funding" xml:id="_5N4Tydb">
					<idno type="grant-number">CCF1763315</idno>
				</org>
				<org type="funding" xml:id="_weGHenw">
					<idno type="grant-number">W911NF-21-2-0251</idno>
				</org>
				<org type="funding" xml:id="_cqyfMjp">
					<idno type="grant-number">N000141712266</idno>
				</org>
				<org type="funded-project" xml:id="_ecBZben">
					<idno type="grant-number">ONR N00014-20-1-2480</idno>
					<orgName type="project" subtype="full">Understanding and Applying Non-Euclidean Geometry in Machine Learning</orgName>
				</org>
				<org type="funding" xml:id="_Z9jVCHf">
					<idno type="grant-number">N000142012275</idno>
				</org>
				<org type="funding" xml:id="_ggzmA8e">
					<idno type="grant-number">1651565</idno>
				</org>
				<org type="funding" xml:id="_6UjMS8x">
					<idno type="grant-number">FA95501910024</idno>
				</org>
				<org type="funding" xml:id="_YDzUTjJ">
					<idno type="grant-number">W911NF-21-1-0125</idno>
				</org>
				<org type="funding" xml:id="_xTTThfv">
					<idno type="grant-number">DE-SC0022222</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Pile: We follow a same procedure and train 125M and 355M-sized models on The Pile <ref type="bibr" target="#b15">(Gao et al., 2020)</ref>. Hyperparameters are reported in Table <ref type="table">A</ref>.3. Hyperparameters for 355M are the same beyond a reduction in peak learning rate to 4 • 10 -4 . For larger models (1.3B), we set a learning rate of 2.2 • 10 -4 . We perform three experiments for each model type and size, and train for 5, 10, 15 billion tokens at a sequence length 2024 and global batch size 256. All models are trained on a single node of 8 A100 80GB GPUs. We use order 2 Hyenas, with the same architectural considerations described above for WikiText103. In addition to our data scaling experiments at 5, 10 and 15 billion tokens, we provide preliminary results for models at the 1.3B parameter scale (10.8 perplexity after 5 billion tokens), and train a 153M model (130 billion tokens), reaching a perplexity of 9.8. The 153M is the same used in our downstream evaluation on SuperGLUE.</p><p>Training hyperparameters match those of standard GPT training pipelines, and are thus likely suboptimal for new attention-free architectures such as Hyena. We run some preliminary experiments and find that e.g., some modifications to the learning rate schedule, currently involving linear warmup and cosine decay, to improve perplexity at convergence of Hyena models (we recommend slightly lower learning rates for Hyena models compared to GPT of a similar size). Despite these findings, we use standard GPT hyperparameters for both GPT and Hyena.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PG-19</head><p>We also report results of additional training runs on other datasets. We train a Hyena 153M model on the standard PG-19 long-range corpus <ref type="bibr" target="#b40">(Rae et al., 2019)</ref>, with a context length of 16k tokens, reaching a test perplexity of 14.6 (using the standard GPT2 tokenizer) in 8 epochs.</p><p>Architectures Architectural hyperparameters for Hyena are shown in Table <ref type="table">A</ref>.4. We use sine as an activation function for the FFN of Hyena filters. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FLOP computation</head><p>The number of floating point operations (FLOPs) reported in the main text are computed using the same strategy as in <ref type="bibr" target="#b21">(Hoffmann et al., 2022)</ref>. For GPT, we do not use the approximation, opting instead for the more accurate formula based on FLOP counts of individual layers. In the case of Hyena, FLOPs are computed using the same method, except attention layers are replaced by: i. Projections: order × d_model × d_model × seq_len.</p><p>ii. Short conv on projections: order × d_model × seq_len × filter_len (usually 3).</p><p>iii. FFTConv: 5 × (order -1) × d_model × log(seq_len) × seq_len.</p><p>iv. Output: d_model × d_model × seq_len.</p><p>with a leading factor 2 to account for both additions and multiplications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Downstream Evaluation</head><p>SuperGLUE: We evaluate models on the SuperGLUE <ref type="bibr" target="#b51">(Wang et al., 2019)</ref> with the parsing pipeline of <ref type="bibr">(Arora et al., 2022)</ref>. For all tasks except WIC, CB and BoolQ, we generate a response using greedy decoding, then check for the gold label. WIC, CB and BoolQ use logit scoring instead of generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>The models considered are the open-source checkpoint of GPTNeo 125M trained for 300B tokens The Pile, and the RWKV-v4 169M checkpoint trained for 332B tokens on The Pile. Hyena is a 153M model trained for 137B tokens on The Pile.</p><p>LAMBADA: We evaluate Hyena on the LAMBADA <ref type="bibr" target="#b35">(Paperno et al., 2016)</ref> task. We apply a stop word filter and check whether predictions for all tokens corresponding to the last word agree with the ground truth.</p><p>The small Hyena model trained on 137B tokens reaches 44.64% accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Image Classification</head><p>a ImageNet: We use ImageNet-1k which consists of 1000 classes and 1.3M images and train from scratch with no outside data on 8 Nvidia A100 GPUs. In our ViT benchmark, we swap the attention layers with the Hyena operator defined in our language experiments, and remove the class token and positional embeddings, similar to S4ND <ref type="bibr" target="#b32">(Nguyen et al., 2022)</ref>. The parameter count is kept similar at 87M ViT-B (base) vs 88M Hyena-ViT. The training procedure from T2T-ViT <ref type="bibr" target="#b53">(Yuan et al., 2021)</ref> is used, including augmentations such as RandAugment <ref type="bibr" target="#b7">(Cubuk et al., 2020)</ref>, Mixup <ref type="bibr" target="#b56">(Zhang et al., 2017)</ref>, and AugMix <ref type="bibr" target="#b20">(Hendrycks et al., 2019)</ref>. See table A.5 for hyperparameter settings used.</p><p>CIFAR-10: We use CIFAR-10 in sequential and 2D experiments. For sequential, we use the Hyena operator defined in our language tasks and compare with an S4 model <ref type="bibr" target="#b18">(Gu et al., 2021)</ref> of the same size by swapping layers in the residual blocks. In 2D, we learn Hyena filters (in both x and y dimensions) that are equal to the size of the input shape, and forgo the gating mechanism from our language experiments. We window (i.e., apply a soft mask spatially to) the Hyena filters with a decay term. The rate of decay varies across channels, ensuring different sizes of the filters at initialization. We compare with another implicit 2D convolution, S4ND <ref type="bibr" target="#b32">(Nguyen et al., 2022)</ref>, by swapping the model layers with the 2D Hyena filters. The "isometric" model consists of 4 residual blocks of model dimension 128. We use basic image augmentations, 0.1 dropout, 0.03 weight decay and train for 100 epochs using a Nvidia T4 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Theoretical Results and Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Proofs</head><p>Proof of Proposition 3.1</p><p>Proof. A discrete L-by-L operator is causal if it is lower triangular, i.e., when there is no leakage of future input information to the output. The Hyena operator H is the product of alternating diagonal and Toeplitz matrices. Thus, if all the Toeplitz matrices S n h are lower triangular then H is lower triangular. In turn, each S n h is lower triangular if and only if the filter h is causal, concluding the proof.  <ref type="bibr" target="#b7">(Cubuk et al., 2020)</ref> (9,0.5,layers=2) Mixup <ref type="bibr" target="#b56">(Zhang et al., 2017)</ref> 0.8 Cutmix <ref type="bibr" target="#b54">(Yun et al., 2019)</ref> 1.0 Random erasing <ref type="bibr" target="#b58">(Zhong et al., 2020)</ref> 0.25 Label smoothing <ref type="bibr" target="#b48">(Szegedy et al., 2016)</ref> 0.1 Stochastic depth <ref type="bibr" target="#b22">(Huang et al., 2016)</ref> 0.1 Exp.mov. avg (EMA) <ref type="bibr" target="#b37">(Polyak and Juditsky, 1992)</ref> None</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Analysis of Data-Controlled Mechanisms</head><p>We discuss the surrogate attention mechanism of Hyena-2: q, k, v → y:</p><p>If ϕ and ψ are convolutions parametrized via state-space models (SSMs), the above resembles the H3 mechanism <ref type="bibr">(Dao et al., 2022c)</ref>. We investigate the effect of the convolutional kernels ϕ and ψ on the attention layer. We start by introducing a matrix representation of the layer, and we isolate the attention matrix A ψ ϕ (q, k) such that y = A ψ ϕ (q, k)v.</p><p>(9)</p><p>Isolating the surrogate attention matrix In the case of length-L discrete sequences</p><p>Therefore we can rewrite (8) as Single layer recall All experiments on our synthetic tasks default to 2 layer models. We choose 2 as it is the canonical number for mechanistic analysis of Transformers <ref type="bibr" target="#b13">(Elhage et al., 2021)</ref> based on circuits. Interestingly, a single layer of Hyena (width 64) is capable of performing associative recall, solving the task completely even in the challenging setting with vocabulary size 40. Reverse engineering exactly how the single Hyena operator is able to perform recall is left for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Learning Arithmetic</head><p>We showcase an additional task in our mechanistic design benchmark: learning arithmetic. We train Hyena models of increasing depth (1, 2 and 3 layers) on a dataset of D n -digit addition. As an example, a 3-digit addition input sample is given by the sequence 1, 2, 3, 9, 5, 4, 1, 0, 7, 7 where the first 6 digits contain the two 3 digits numbers to add, and the last 4 the result. Our models are optimized using standard autoregressive training i.e., predicting the next token, since they are causal. In particular, we optimize models to learn a map x → y where x is the original prompt without the last element, and y equal to x shifted right by one position. We mask the first 2D n -1 elements of the loss for each sequence since they contain predictions for addends and not results.</p><p>We report results in Figure C.1. A single layer of Hyena is able to learn to perform addition with up to 4 digits. Longer numbers require deeper models. In our experiments, alternative architectures such as AFT-conv struggle to learn arithmetic, signaling a cap in capability. We find a substantial performance difference (up to 5% perplexity) between initialization schemes. If the filters at initialization are excessively smooth (see Appendix D.3 for a discussion of positional encoding and activation), the model finds a worse solution and takes longer to converge. Further, we observe initialization schemes that regularize filters towards typical filters learned at convergence to decrease performance. These observations are in line with performance gaps between convolution parametrization schemes discussed in main text and Appendix A.1. In particular, the performance improvements obtained via Hyena filters could be due to easier optimization in the space of convolutional filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Hyena Filters</head><p>At convergence, Hyena learns a collection of lower-order filters with a similar structure, which can be exploited to further speed up inference after training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Positional Encoding and Filters Initialization</head><p>The positional encoding chosen for the Hyena filters is a truncated complex exponential basis. Specifically, with ρ k (t) = e i2πkt/L for k = 0, . . . K -1, the positional encoding is defined as a map from R to R 2K+1 such that</p><p>denote the real and imaginary part of their argument, respectively. In the main text, we use D e = 2K + 1 to denote the size of a positional encoding with K features. The number of features of the positional encoding has an impact on the filter initialization and training performances. In particular, we show how K leads to a preconditioning of the spectrum of the filter at initialization. Figures D.6, D.7, D.8 display the initialized filters (with no Window function) for different values of K ({8, 32, 64}) for L = 128 and frequency ω a of sinusoidal activation σ(•) = sin(ω a •) set to 1. We notice how the choice of K induces a bias in the modeled frequencies at initialization. Specifically the filters resemble low-pass filters with a cut-off frequency of approximatively 2K + 1.</p><p>This cut-off frequency is strongly related to the smoothness of the filter; as previously mentioned, we empirically observe better training dynamics of filters initialized to be non-smooth, i.e. with a rich highfrequency content. While we can achieve good initializations by increasing K, this results in larger FFNs (its input dimension is 2K + 1, i.e. the number of positional encoding features) which come with a higher parameter count. A more efficient solution is to increase the frequency ω a of the sinusoidal activation. Passage: Einstein and Maric married in January 1903. In May 1904, the couple's first son, Hans Albert Einstein, was born in Bern, Switzerland. Their second son, Eduard, was born in Zurich in July 1910. In 1914, the couple separated; Einstein moved to Berlin and his wife remained in Zurich with their sons. They divorced on 14 February 1919, having lived apart for five years. Eduard, whom his father called "Tete" (for petit), had a breakdown at about age 20 and was diagnosed with schizophrenia. His mother cared for him and he was also committed to asylums for several periods, including full-time after her death. The marriage with Maric does not seem to have been very happy. In letters revealed in 2015, Einstein wrote to his early love, Marie Winteler, about his marriage and his still strong feelings for Marie. In 1910 he wrote to her that "I think of you in heartfelt love every spare minute and am so unhappy as only a man can be" while his wife was pregnant with their second child. Einstein spoke about a "misguided love" and a "missed life" regarding his love for Marie. Einstein married Elsa Lowenthal on 2 June 1919, after having had a relationship with her since 1912. She was a first cousin maternally and a second cousin paternally. In 1933, they emigrated to the United States. In 1935, Elsa Einstein was diagnosed with heart and kidney problems; she died in December 1936.</p><p>Question: Based on the previous passage, How much time passed, after Albert Einstein's father divorced his mother, that he re-married? Is "3 months and 18 days" a correct answer? Answer: Yes RWKV zero-shot: Albert Einstein was born on 3 march 1916 in Gerlin, Germany. he was the son of a German doctor and a german woman. Hyena zero-shot: "3 months and 18 days" RWKV few-shot: It is not a correct answer. The exact date is not known.</p><p>Hyena few-shot: Yes, according to the previous passage.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Chami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.02441</idno>
		<title level="m">Ask me anything: A simple strategy for prompting language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Frequency bias in neural networks for input of non-uniform density</title>
		<author>
			<persName><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Galun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geifman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kasten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kritchman</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="685" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow</title>
		<author>
			<persName><forename type="first">S</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leahy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Biderman</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.5297715</idno>
		<ptr target="https://doi.org/10.5281/zenodo.5297715" />
		<imprint>
			<date type="published" when="2021-03">Mar. 2021</date>
		</imprint>
	</monogr>
	<note>If you use this software, please cite it using these metadata</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">C.-T</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Linear system theory and design</title>
		<imprint>
			<publisher>Saunders college publishing</publisher>
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Alphafold2 and the future of structural biology</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cramer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature structural &amp; molecular biology</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="704" to="705" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Randaugment</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning fast algorithms for linear transforms using butterfly factorizations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Eichhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1517" to="1527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Monarch: Expressive structured matrices for efficient and accurate training</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Sohoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Poli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Grogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4690" to="4721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Flashattention: Fast and memory-efficient exact attention with io-awareness</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.14135</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Saab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.14052</idno>
		<title level="m">Hungry hungry hippos: Towards language modeling with state space models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A mathematical framework for transformer circuits</title>
		<author>
			<persName><forename type="first">N</forename><surname>Elhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Conerly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Transformer Circuits Thread</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neocognitron: A self-organizing neural network model for a mechanism of visual pattern recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fukushima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Miyake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Competition and cooperation in neural nets</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1982">1982</date>
			<biblScope unit="page" from="267" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nabeshima</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00027</idno>
		<title level="m">The pile: An 800gb dataset of diverse text for language modeling</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">What can transformers learn in-context? a case study of simple function classes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Valiant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.01066</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hippo: Recurrent memory with optimal polynomial projections</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1474" to="1487" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Efficiently modeling long sequences with structured state spaces</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.00396</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.02781</idno>
		<title level="m">Augmix: A simple data processing method to improve robustness and uncertainty</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Training compute-optimal large language models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D L</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.15556</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="646" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.04451</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Butterfly factorization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ying</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multiscale Modeling &amp; Simulation</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="714" to="732" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.09298</idno>
		<title level="m">What makes convolutional models great on long sequence modeling? arXiv preprint</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kovachki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Azizzadenesheli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.08895</idno>
		<title level="m">Fourier neural operator for parametric partial differential equations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Holistic evaluation of language models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Soylu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.09110</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dissecting neural odes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Massaroli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Poli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yamashita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Asama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3952" to="3963" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cutkosky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Neyshabur</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.13947</idno>
		<title level="m">Long range language modeling via gated state spaces</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Nerf: Representing scenes as neural radiance fields for view synthesis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="99" to="106" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Downs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Baccus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.06583</idno>
		<title level="m">S4nd: Modeling images and videos as multidimensional signals using state spaces</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Elhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dassarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.11895</idno>
		<title level="m">-context learning and induction heads</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Oppenheim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Willsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Nawab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-J</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signals and systems</title>
		<imprint>
			<publisher>Prentice hall Upper Saddle River</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Paperno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">N</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pezzelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Boleda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fernández</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.06031</idno>
		<title level="m">The lambada dataset: Word prediction requiring a broad discourse context</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">RWKV-LM</title>
		<author>
			<persName><forename type="first">B</forename><surname>Peng</surname></persName>
		</author>
		<ptr target="https://github.com/BlinkDL/RWKV-LM" />
		<imprint>
			<date type="published" when="2021">8 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Acceleration of stochastic approximation by averaging</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Juditsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM journal on control and optimization</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="838" to="855" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Power</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><surname>Grokking</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.02177</idno>
		<title level="m">Generalization beyond overfitting on small algorithmic datasets</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Robust speech recognition via large-scale weak supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mcleavey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.04356</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Compressive transformers for long-range sequence modelling</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Potapenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1911.05507" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R.-J</forename><surname>Bruintjes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Bekkers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hoogendoorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName><surname>Flexconv</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.08059</idno>
		<title level="m">Continuous kernel convolutions with differentiable kernel sizes</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kuzina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Bekkers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hoogendoorn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.02611</idno>
		<title level="m">Ckconv: Continuous kernel convolution for sequential data</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Efficient content-based sparse attention with routing transformers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saffar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="53" to="68" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Linear transformers are secretly fast weight programmers</title>
		<author>
			<persName><forename type="first">I</forename><surname>Schlag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9355" to="9366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Fast convolution and filtering</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Selesnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Burrus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Digital Signal Processing Handbook</title>
		<imprint>
			<publisher>CRC Press</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="8" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Sitzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Martel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Lindell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wetzstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09661</idno>
		<title level="m">Implicit neural representations with periodic activation functions</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Talebi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Maxvit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.01697</idno>
		<title level="m">Multi-axis vision transformer</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Superglue: A stickier benchmark for general-purpose language understanding systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Linformer: Self-attention with linear complexity</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04768</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">E</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="558" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Talbott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Susskind</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.14103</idno>
		<title level="m">An attention free transformer</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Unveiling transformers with lego: a synthetic reasoning task</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Backurs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bubeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Eldan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gunasekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wagner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.04301</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13001" to="13008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
