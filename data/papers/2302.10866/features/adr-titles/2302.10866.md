- Decision to replace traditional attention with Hyena operator
- Choice of using long convolutions in the Hyena architecture
- Implementation of data-controlled gating mechanism
- Selection of implicit parameterization for convolution filters
- Design decision on the depth of the Hyena recurrence
- Trade-off considerations between expressivity and computational efficiency
- Benchmarking against existing attention mechanisms
- Choice of datasets for performance evaluation (WikiText103, The Pile, ImageNet-1k)
- Decision to focus on reasoning tasks for model evaluation
- Strategy for scaling the model to handle longer sequences
- Choice of optimization techniques for fast convolutions
- Decision to evaluate performance in both language and vision tasks
- Assumptions regarding the limitations of traditional attention mechanisms
- Design of the mechanistic interpretability benchmarks
- Decision to not hybridize with standard attention layers
- Choice of empirical methods for measuring performance improvements