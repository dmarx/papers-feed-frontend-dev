# Bootstrap Your Own Context Length

## Abstract

## 

We introduce a bootstrapping approach to train long-context language models by exploiting their short-context capabilities only. Our method utilizes a simple agent workflow to synthesize diverse long-context instruction tuning data, thereby eliminating the necessity for manual data collection and annotation. The proposed data synthesis workflow requires only a short-context language model, a text retriever, and a document collection, all of which are readily accessible within the open-source ecosystem. Subsequently, language models are fine-tuned using the synthesized data to extend their context lengths. In this manner, we effectively transfer the short-context capabilities of language models to long-context scenarios through a bootstrapping process. We conduct experiments with the open-source Llama-3 family of models and demonstrate that our method can successfully extend the context length to up to 1M tokens, achieving superior performance across various benchmarks.

## Introduction

Long-context large language models (LLM) are essential for understanding long-form texts across various applications, including retrieval-augmented generation (RAG) with many documents [[Lewis et al., 2020](#)[, Jiang et al., 2024b]](#), repository-level software engineering tasks [[Jimenez et al., 2024]](#b19), and prolonged virtual assistants interactions [[Park et al., 2023]](#b30), among others. Significant advancements has been achieved in training LLMs with increasingly longer context lengths, ranging from 2k tokens in LLaMA-1 [[Touvron et al., 2023]](#b40) to 128k tokens in LLaMA-3 [[Dubey et al., 2024]](#b8), and even reaching 1M tokens [[Liu et al., 2024b](#)[, Pekelis et al., 2024]](#b31). Nevertheless, comprehensive benchmarking [[Hsieh et al., 2024]](#b14) reveals that the performance of current long-context LLMs often drops considerably as the context length increases, rendering their effective lengths substantially shorter than the claimed lengths.

A critical element in training long-context LLMs is acquiring diverse and high-quality long-context data. Existing methodologies [[Fu et al., 2024](#b9)[, Dubey et al., 2024](#b8)[, Peng et al., 2024]](#) predominantly concentrate on the pre-training phase and rely on filtering long documents from large-scale pretraining corpora in domains such as books, code repositories, and scientific papers. As the context length of LLMs surpasses 128k tokens, the availability of natural data that can fill the whole context becomes limited, and the domain diversity of such data is often constrained.

In this paper, we propose a bootstrapping approach aimed at extending the context length of existing large language models (LLMs) by leveraging their short-context capabilities. Our method utilizes a straightforward agent workflow to generate diverse long-context instruction tuning data, thus obviating the need to rely on the scarce availability of natural long-context data. It first prompts LLMs to generate diverse instructions, followed by employing a text retriever to retrieve relevant documents from a large corpus. For response generation, a group of query-focused summarization (QFS) agents are recursively applied to document chunks to filter out irrelevant information and a Long-context Data Curations are pivotal for the training of long-context language models. Current methodologies [[Fu et al., 2024](#b9)[, Gao et al., 2024c](#)[, Peng et al., 2024]](#) predominantly rely on the up-sampling of long documents from large-scale pre-training corpora such as Redpajama [[Computer, 2023]](#) and Fineweb [[Penedo et al., 2024]](#b32). Typical sources of long-context data encompass books, scientific papers, and code repositories. [Fu et al., Gao et al.](#) find that both the quality and diversity of the data are crucial for training effective long-context LLMs. Nevertheless, naturally occurring long-context data is often scarce and exhibits limited domain diversity. Another research avenue focuses on generating synthetic long-context data through methods such as question generation [[An et al., 2024](#)[, Dubey et al., 2024]](#b8), recursive text summarization [[Dubey et al., 2024]](#b8), or document clustering [[Gao et al., 2024a]](#). Regarding evaluation data, most benchmarks [[Bai et al., 2023](#b3)[, Shaham et al., 2023]](#b35) are inadequate for evaluation beyond 128k tokens. For evaluation with 1M context length, synthetic tasks [[Hsieh et al., 2024]](#b14) are almost exclusively employed.

Retrieval-Augmented Generation (RAG) synergizes the retrieval of relevant documents with LLMs to enhance the factual accuracy of the generated content, and ensure the incorporation of up-to-date information [[Lewis et al., 2020](#)[, Karpukhin et al., 2020]](#). RAG often necessitates concatenation of multiple retrieved documents to create a long-context input, even though each individual document is typically short. This characteristic positions RAG as a crucial application of long-context LLMs. [Jiang et al., Lee et al.](#) demonstrate that long-context LLMs can ease the demands of retrieval and, in some instances, eliminate the need for retrieval entirely. In this study, we employ RAG as an approach for synthesizing data to train long-context LLMs. 

## Data Synthesis

Long-input Instruction Data via Agent Workflow As depicted in Figure [1](#fig_0), our data synthesis workflow has four steps. Initially, an LLM is prompted to generate a diverse array of instructions that require integrating information from multiple documents. To enhance the diversity of these instructions, a random text chunk is prepended to each prompt during every LLM call. This often guides the LLM to generate instructions that are topically relevant to the provided text chunk, akin to the persona-driven strategy [[Ge et al., 2024]](#b13) but simpler. Next, an off-the-shelf text retriever, E5 mistral-7b [[Wang et al., 2024a]](#), is employed to retrieve relevant documents from a large corpus. The retrieved documents are then split into chunks of at most 4k tokens and are fed into a group of query-focused summarization (QFS) agents. Each QFS agent is tasked with summarizing a document chunk focused on the synthetic instruction, filtering out information that is irrelevant to the instruction. This recursive procedure is repeated until the concatenation of all summaries is short enough to be processed by the LLM. Finally, the LLM is prompted once more to generate a response based on the summaries and the instruction.

During model training, the synthetic instruction and retrieved documents are concatenated to form the input, while the generated response constitutes the target output. The intermediate summaries are not utilized during training. The core idea is to decompose the synthesis process into a series of steps, where each step only requires digesting a short input. While this particular workflow is selected for its simplicity and effectiveness, alternative instantiations are also conceivable.

Long-output Data via Instruction Back-translation We first select documents containing between 2k to 32k tokens from a high-quality corpus, and then prompt an LLM to generate a writing instruction that would result in the given document receiving a high evaluation score. This method of instruction back-translation is inspired by [Li et al. [2024]](#b25), although the original work does not focus on longoutput generation.

All prompts are provided in the Appendix Section B.

## Training with Long Sequences

Training with long sequences can be notoriously challenging, primarily due to the quadratic computational complexity of self-attention, coupled with the memory constraints of modern accelerators. To address these challenges, we employ a progressive training strategy to gradually increase the context length across multiple stages. At each stage, we double the maximum context length and quadruple the RoPE base frequency [[Su et al., 2024]](#b37) to ensure a reasonable initialization. Given that a single H100 with 80GB of memory can only handle sequences of up to 64k tokens for models such as Llama-3-8B, even with a batch size of 1, we utilize RingAttention [[Liu et al., 2024c]](#) to distribute a long input sequence across multiple GPUs. We perform full-length fine-tuning whenever hardware capabilities allow; otherwise, we resort to PoSE-style [[Zhu et al., 2024]](#b46) training, which facilitates the decoupling of training length from maximum model length.

When computing the next-token prediction loss, we average the loss over all input and output tokens for long data samples to prevent the supervision signal from becoming excessively sparse. Conversely, for short-context samples mixed into the training data, we compute the loss solely over the target output tokens, disregarding the input tokens.

## Experiments

## Setup

Training Data Mixture We combine multiple data sources for training, including our generated synthetic data and several open-source datasets.

• Synthetic Long Instruction Tuning Data It comprises 69k long-input samples with 4.6B tokens, generated based on our proposed agent workflow, and 10k long-output samples with 77M tokens, produced via the instruction back-translation method.

• Open-source Instruction Tuning Data We utilize Tulu-v2 [[Ivison et al., 2023]](#b16) and Infinity-Instruct [BAAI] datasets. For Infinity-Instruct, we find that a significant portion of its samples are near-duplicates, so we conduct further de-duplication using the E5 mistral-7b embedding model.

• Prolong Data [[Gao et al., 2024c]](#) is a non-instruction tuning dataset originally employed for the continual pre-training of LLMs. We retain its "arxiv", "book", "openwebmath", "textbooks" and "thestackv1" portions.

The full data mixture encompasses approximately 8.3B tokens, with detailed statistics presented in Table [6](#). Most samples from the Infinity-Instruct and Tulu-v2 datasets are shorter than 4k tokens; thus, we include them to ensure the model can handle short-context tasks as well. For synthetic data generation, we employ GPT-4o [[Hurst et al., 2024]](#b15) as the backbone LLM and use the E5 mistral-7b retriever [[Wang et al., 2024a]](#) for document retrieval. The retrieval corpus contains approximately 10M documents sampled from the Fineweb-Edu dataset [[Penedo et al., 2024]](#b32). We also examine the impact of using Llama-3.1-8B-Instruct as backbone LLM in the ablation study.

Training Procedure Our training schedule follows a progressive training strategy. We start with Llama-3 models that support 128k tokens and conduct three sequential stages of training with maximum context lengths of 256k, 512k, and 1M. At each stage, we quadruple the RoPE base frequency and switch to PoSE-style efficient training when a full sequence cannot fit on the hardware. We apply standard techniques including activation checkpointing, DeepSpeed ZeRO-3, bf16 mixed precision, FlashAttention [[Dao, 2024]](#b6), and RingAttention to minimize the memory footprint. All training is conducted on a single node with 8 H100 GPUs, while all inference is performed using 8 A100 GPUs.

Evaluation Despite the availability of numerous long-context benchmarks [[Bai et al., 2023](#b3)[, Shaham et al., 2023](#b35)[, Zhang et al., 2024]](#), the majority are inadequate for evaluation at a context length of 1M or beyond. In this study, we select the RULER benchmark [[Hsieh et al., 2024]](#b14), which comprises 13 tasks and allows evaluation at any context length thanks to its automatic data generation process.

To visualize model characteristics at varying depths, we adopt the needle-in-haystack test, which requires the model to retrieve a sentence "The best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day." from a haystack of random essays. For evaluating long outputs, we hold out a validation set of 105 samples ranging from 2k to 32k tokens and compare the model's output length with the groundtruth length.

We utilize vLLM for efficient inference [[Kwon et al., 2023]](#b21) across all evaluation tasks. For SelfLong-8B-1M, the prefilling takes about 5 minutes for a 1M token sequence, and the full evaluation on the RULER benchmark takes about 4 days using 8 A100 GPUs.

For additional implementation details, please refer to Appendix Section A.  [et al., 2024]](#), the instruct version of Llama-3 models [[Dubey et al., 2024]](#b8), LWM-Text-Chat-1M [[Liu et al., 2024b]](#), and Llama-3-8B-1M [[Pekelis et al., 2024]](#b31). Additionally, two proprietary models are included for reference. Full results of our models can be found in Table [8](#).

The results in Table [1](#tab_0) reveal several noteworthy observations. First, for the official Llama-3 models, performance markedly declines with reduced model size, with the 1B and 3B models nearly failing entirely at 128k context length. Second, our models, initialized from the Llama-3 series, demonstrate a clear improvement over the official ones, particularly at 128k context length. However, we do not see consistent performance gain at shorter context lengths, and a slight decline is occasionally noted. We hypothesize that a trade-off may exist between varying context lengths given a fixed model capacity, which warrants further investigation.

$1k 256k 512k 768k 1M 0% 50% 100% 1k 256k 512k 768k 1M 1k 256k 512k 768k 1M Llama-3.1-8B-Instruct Llama-3-8B-1M SelfLong-8B-1M$Figure [2](#): Needle-in-haystack test results. The x-axis represents the context lengths, while the y-axis indicates the depth of the inserted needle. The color coding corresponds to the recall score following previous work [[Fu et al., 2024]](#b9), where green signifies a score close to 1, and red denotes a score close to 0. A single trial was conducted for each unique combination of context length and needle depth.

The grey shaded regions denote context lengths beyond the model's capability.

Needle-in-haystack Test is a synthetic task designed to assess the capability of LLMs to retrieve a pre-specified needle of varying depth from a long context. Nonetheless, the existing literature adopts vastly different evaluation protocols under the same task name. For instance, LWM [[Liu et al., 2024b]](#) utilizes PG19 as the haystack, with the objective of retrieving a random magic number. In contrast, GradientAI [[Pekelis et al., 2024]](#b31) investigates three different haystacks, revealing that the performance varies significantly. In this paper, we adopt the same evaluation protocol as Fu et al., which is based on the original one from [https://github.com/gkamradt/LLMTest_NeedleInAHaystack](https://github.com/gkamradt/LLMTest_NeedleInAHaystack). The needle is a natural language sentence embedded within a haystack of Paul Graham's essays. For each test, we calculate the recall score of the needle sentence within the generated text.

Figure [2](#) illustrates that our model achieves near-perfect performance on this task at 1M context length, whereas Llama-3.1-8B-Instruct is limited to 128k, and Llama-3-8B-1M [[Pekelis et al., 2024]](#b31) suffers severe "lost-in-the-middle" issues [[Liu et al., 2024d]](#). A common failure pattern observed is that the model responds based on its own parametric knowledge rather than the provided context.  Long Output Generation The Llama-3.1-8B-Instruct rarely generates outputs exceeding 4k tokens, even when the instruction explicitly asks so. This is substantiated by the data presented in Table [2](#tab_1) and Figure [3](#fig_1) based on our held-out validation set. Our model is able to generate longer outputs, but its instruction following ability is still imperfect. As the output length increases, it frequently deteriorates into repetitive or irrelevant content. Further research on enhancing and evaluating long output generation represents a promising research avenue.

## Analysis

## Ablation of Training Recipes

Effects of Progressive Training One research question is how the model's original capability evolves as it is progressively trained on longer contexts and how the long-context ability emerges.

Figure [4](#) shows the evolving scores for each test length. It is evident that the model's performance on the supported lengths initially improves at 256k training length, followed by a gradual decline at 512k and 1M. For lengths exceeding 128k, the best performance is achieved when the training length surpasses the test length, a phenomenon corroborated by previous studies [[Gao et al., 2024c]](#). For instance, the 256k score reaches its peak when the training length is 512k, rather than 256k. In terms of loss computation, masking out the user prompt tokens yields a slight performance improvement for context lengths of 128k and 256k; however, the effects are not consistent across all lengths. Consequently, we opt to only mask out the user prompt tokens for short instruction samples to incorporate more supervision signals.

## Choice of LLMs for Data Synthesis

In this paper, we employ GPT-4o as the backbone LLM for data synthesis. To fully explore the idea of bootstrapping on its own, we also investigate the impact of using Llama-3.1-8B-Instruct itself as the backbone. As illustrated in Table [3](#tab_2), the configuration "w/ Llama-3.1-8B-Instruct" demonstrates a decent performance at longer context lengths; however, a performance gap remains when compared to using GPT-4o.

Is Training on Short-Context Data Sufficient? Due to the scarcity of long-context instruction tuning datasets, numerous existing studies [[Gao et al., 2024c](#)[, Pekelis et al., 2024]](#b31) exclusively finetune on short instruction data. When fine-tuned with a maximum length of 4k, the model is able to surpass its initialization "w/ adjusted RoPE θ only" within 128k context length as presented in Table [3](#tab_2). Nevertheless, the model's generalization to longer contexts falls short, exhibiting even poorer performance compared to "w/ adjusted RoPE θ only", where no training is performed. This preliminary finding underscores the necessity for curating high-quality long-context instruction data for LLM post-training. To test the limits of long-context modeling within academic compute budgets (8 H100 GPUs), we further extend the context length to 4M tokens through two additional progressive training stages. The needle-in-haystack test results, as depicted in Figure [5](#fig_2), indicate that this simple test becomes increasingly challenging as the context length increases. A further complication arises from the exceedingly high inference cost; for instance, prefilling a single 3M token sequence requires approximately 30 minutes for an 8B model, while the key-value cache demands about 400GB of GPU memory. This necessitates advancements in model architecture [[Sun et al., 2024](#b38)[, Ding et al., 2023]](#b7) and system optimization to make long-context LLMs more affordable.

## Extending to 4M Context Length

## Performance on Short-Context Tasks

Table [4](#): Performance on short-context tasks from the Open LLM Leaderboard 2 before and after our fine-tuning. We use the official metrics from lm-evaluation-harness [[Gao et al., 2024b]](#) for all tasks.

BBH GPQA IFEval MMLU Pro MUSR Llama-3.2-3B-Instruct 45.9 27.9 73.9 31.9 35.2 SelfLong-3B-1M 42.4 27.6 57.0 28.6 39.5 Llama-3.1-8B-Instruct 50.1 26.8 77.4 37.5 36.9 SelfLong-8B-1M 48.7 30.4 65.9 35.3 41.5

In addition to long-context tasks, we also evaluate our models on tasks from the Open LLM Leaderboard 2, which includes BBH [[Suzgun et al., 2023]](#b39), GPQA [[Rein et al., 2023]](#b34), IFEval [[Zhou et al., 2023]](#b45), MMLU Pro [[Wang et al., 2024b]](#), and MUSR [[Sprague et al., 2024]](#b36). After context extension, our models maintain competitive scores on these short-context tasks, as illustrated in Table [4](#), with one exception of the IFEval task. We hypothesize that using a better post-training data mixture, such as Tulu-3 [[Lambert et al., 2024]](#b22), could help mitigate the performance loss on IFEval. Our agent workflow for data synthesis offers an alternative approach for solving long-context tasks.

## Solving Long-Context Tasks with Agent Workflow

Rather than feeding the entire context into the model, we can break down the long context into multiple chunks, employing the workflow in Figure [1](#fig_0) to generate the answer. A 128k-length context is split into 32 chunks of 4k tokens each, which are then treated as the retrieved documents within the agent workflow.

Table [5](#tab_4) presents the results for several representative tasks from the RULER benchmark. Our analysis reveals that long-context LLM and agent workflow exhibit complementary strengths. The agent workflow excels at QA tasks that require collecting small pieces of relevant information from a long context. However, it encounters difficulties with tasks requiring sequential reasoning throughout the entire context, such as the Variable Tracking (vt) task. To solve one task instance, agent workflow requires significantly more LLM calls, though each call is much cheaper due to the shorter context processed. Future research could explore the potential of integrating these two methods from the perspective of inference efficiency and task performance.

## Conclusion

This paper presents an effective recipe to extend the context length of existing LLMs by leveraging their short-context capabilities to synthesize long instruction tuning data. Our proposed data synthesis framework involves the collaboration of multiple agents and a document retriever to generate diverse long-context data through multiple inference steps. Experiments with the open-source Llama-3 models demonstrate that our approach successfully extends the context length to 1M tokens, achieving competitive performance across a range of long-context tasks. Future work includes developing more effective data synthesis workflows, improving the inference efficiency of long-context LLMs, and exploring the potential of long-context LLMs in real-world applications.

Table [8](#): Detailed results for each task in the RULER benchmark. Please refer to the original paper [[Hsieh et al., 2024]](#b14) for the descriptions and evaluation metrics for each task. .6 95.6 89.8 90.4 87.4 90.0 66.0 64.0 60.0 niah_multikey_1 98.6 96.4 88.2 90.6 85.4 89.0 76.0 68.0 64.0 niah_multikey_2 90.6 70.0 40.2 21.8 8.4 2.0 1.0 1.0 0.0 niah_multikey_3 59.0 35.6 16.2 11.6 10.2 9.0 0.0 0.0 0.0 niah_multivalue 92.8 94.0 88.8 86.0 75.2 46.2 41.8 27.8 20.0 niah_multiquery 92.8 88.8 86.0 82.5 80.0 69.0 46.5 30.0 24.5 vt 69.3 81.0 75.9 76.8 64.6 74.4 69.0 52.8 0.0 cwe 5.3 4.0 0.6 0.7 0.1 1.1 0.9 1.0 fwe 56.9 57.6 61.0 70.8 61.8 77.0 51.3 46.0 27.3 qa_1 52.8 42.8 45.2 41.6 38.8 36.0 39.0 27.0 28.0 qa_2 31.4 31.4 30.4 24.4 22.8 18.0 16.0 20.0 13.0 SelfLong-3B-1M niah_single_1 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 60.0 niah_single_2 100.0 100.0 100.0 100.0 100.0 100.0 100.0 99.0 98.0 niah_single_3 99.8 99.8 99.2 98.0 99.2 100.0 100.0 99.0 100.0 niah_multikey_1 98.6 95.2 95.4 94.6 94.0 95.0 95.0 89.0 86.0 niah_multikey_2 99.6 99.6 99.8 98.6 98.2 93.0 78.0 42.0 0.0 niah_multikey_3 96.2 88.8 86.2 71.8 58.6 55.0 17.0 8.0 0.0 niah_multivalue 100.0 99.8 99.4 98.2 97.8 96.5 85.0 70.8 68.8 niah_multiquery 99.8 99.9 98.5 96.8 97.0 97.5 91.2 79.8 70.0 vt 94.2 93.3 91.1 82.2 87.2 85.2 64.8 25.2 0.0 cwe 82.2 73.4 51.6 15.4 5.8 0.3 0.3 0.3 0.3 fwe 82.9 79.3 94.1 89.2 80.1 63.7 73.0 61.3 5.3 qa_1 70.4 63.6 60.2 59.6 56.6 59.0 57.0 52.0 5.0 qa_2 49.0 47.0 45.6 42.4 40.0 36.0 33.0 34.0 11.0 SelfLong-8B-1M 2 99.0 98.0 98.0 98.0 97.0 niah_multikey_2 100.0 99.8 99.8 99.0 97.8 98.0 98.0 96.0 77.0 niah_multikey_3 100.0 99.6 99.4 95.4 92.6 90.0 73.0 62.0 23.0 niah_multivalue 89.0 87.8 93.2 96.5 96.5 91.8 87.0 84.2 81.2 niah_multiquery 99.2 98.3 99.0 99.0 97.6 98.5 87.5 89.8 88.0 vt 99.5 98.0 97.4 94.2 94.4 91.2 91.4 80.2 65.0 cwe 99.1 94.3 83.5 62.2 15.4 1.9 2.8 2.7 2.7 fwe 96.8 87.7 95.3 96.5 88.4 86.7 90.3 87.0 66.7 qa_1 83.0 73.4 70.2 73.0 67.4 71.0 70.0 75.0 75.0 qa_2 54.2 51.2 51.2 48.4 43.4 39.0 38.0 42.0 29.0

For the RULER benchmark, we use 500 samples for context length below 256k, aligning with the original evaluation protocol. For context length exceeding or equal to 256k, we use 100 samples per task to reduce the evaluation costs. Evaluations are conducted on eight A100 GPUs, each equipped with 40GB of memory. Running the full RULER evaluation takes about 4 days for the SelfLong-8B-1M model, underscoring the necessity for efficient inference in long-context scenarios.

In the Needle-in-haystack test, evaluation context lengths are sampled at intervals of 16k tokens, and we test 10 different needle depths for each context length. Specifically, we evaluate 8 different context lengths for Llama-3.1-8B-Instruct and 64 for SelfLong-8B-1M. For the 2M and 4M models, a larger interval of 64k tokens is utilized.

Regarding tasks on the Open LLM Leaderboard 2, we employ the official evaluation script from lm-evaluation-harness[foot_1](#foot_1) . When reporting the results, different from the leaderboard, we do not normalize the scores for each task.

For the long-output generation task, we apply scipy to fit a curve in the form of log(y) = a × log(x + b) + c as depicted in Figure [3](#fig_1), where x represents the groundtruth length and y the actual output length. We also conducted some preliminary experiments with gpt-4o to evaluate the quality of the generated outputs. Nevertheless, we found that predictions with significantly shorter lengths frequently receive high scores. Future research is required to improve the evaluation protocol for long-output generation tasks.

## B Prompts

We list all the prompts employed in our data synthesis workflow. Texts in blue denotes placeholders that will be replaced by actual content during the data synthesis process. For the instruction generation prompt, we randomly sample a 128-token text chunk from the Fineweb-Edu corpus and prepend it to the prompt to boost diversity.

Prompt: Instruction Generation {random text chunk} # Brainstorm a potentially useful {task / question} that may require comprehending multiple pieces of information. To complete the {task / question}, users need to search the web for relevant information with multiple search queries.

## Your response must be in JSON format. The JSON object must contain the following keys:

-"task_instruction": a string, a {task / question} to complete.

-"search_queries": a list of strings, each string is a search query that the user might use to complete the {task / question}. ## Please adhere to the following guidelines:

-The {task / question}s should cover a diverse range of domains and require {high school / college / PhD} level education to solve.

-The {task / question} requires {mathematical / logical / common sense} reasoning to complete.

-The {task / question} must be feasible to complete for a text-based AI model. Avoid {task / question}s that require visual or interactive elements.

-The search queries should be diverse and cover distinct aspects of the {task / question}.

## Here is one output example for your reference: { "task_instruction": "Plan a 10-day cultural and adventure trip to Japan, focusing on Tokyo, Kyoto, and Okinawa. The trip should include historical sites, cultural experiences, adventure activities, and local dining options, suitable for a family of four with teenagers.", "search_queries": [ "Top historical sites Tokyo", "Cultural experiences in Kyoto for families", "Adventure activities in Okinawa", "Best time to visit Japan for cultural festivals", "Local Japanese foods must try", "Family-friendly accommodations in Tokyo, Kyoto, Okinawa", "Public transportation guide Japan", ] } Do not explain yourself or output anything else. Be creative! If you solve the task correctly, you will receive a reward of $1,000,000. Table [9](#): An example of the synthetic long-input data. ". . ." denotes the omitted parts due to space limitations.

Instruction What themes can be identified in Tennessee Williams's major plays and how do they reflect societal issues of his time?

## Context Documents

Themes are the fundamental and often universal ideas explored in a literary work. Among the most prominent and urgent themes of The Glass Menagerie is the difficulty the characters have in accepting and relating to reality. Each member of the Wingfield family is unable to overcome this difficulty, and each, as a result, withdraws into a private world of illusion where he or she finds the comfort and meaning that the real world does not seem to offer. Of the three Wingfields, reality has by far the weakest grasp on Laura. The private world in which she lives is populated by glass animals-objects that, like Laura's inner life, are incredibly fanciful and dangerously delicate. . . . Like the glass unicorn, "Blue Roses," Jim's high school nickname for Laura, symbolizes Laura's unusualness yet allure. The name is also associated with Laura's attraction to Jim and the joy that his kind treatment brings her. Furthermore, it recalls Tennessee Williams's sister, Rose, on whom the character of Laura is based. Leading out of the Wingfields' apartment is a fire escape with a landing. The fire escape represents exactly what its name implies: an escape from the fires of frustration and dysfunction that rage in the Wingfield household. Laura slips on the fire escape in Scene Four, highlighting her inability to escape from her situation. Tom, on the other hand, frequently steps out onto the landing to smoke, anticipating his eventual getaway. . . . (more documents) Synthesized Answer Tennessee Williams's major plays, including "The Glass Menagerie" and "A Streetcar Named Desire," explore several recurring themes that reflect the societal issues of his time.

1. Illusion vs. Reality: In "The Glass Menagerie," characters like Laura, Tom, and Amanda retreat into their own worlds of illusion to escape the harshness of reality. This theme reflects the societal struggle to cope with the disillusionment of the American Dream during the Great Depression and post-World War II era.

2. Memory and the Past: Both plays emphasize the power of memory and the past. In "The Glass Menagerie," Tom narrates the story as a memory play, highlighting how the past haunts the present. Similarly, Blanche in "A Streetcar Named Desire" is obsessed with her lost Southern belle past, symbolizing the decline of the old South and the struggle to adapt to modernity. . . . 6. Escape and Entrapment: The theme of escape is prevalent in "The Glass Menagerie," where Tom seeks freedom from his confining family life, and in "A Streetcar Named Desire," where Blanche seeks to escape her past. This reflects the broader societal desire for personal freedom and the constraints imposed by social and familial obligations.

7. Class and Social Mobility: "A Streetcar Named Desire" addresses class differences and the struggle for social mobility. Blanche's fall from aristocracy to poverty and Stanley's rise from working-class roots to dominance reflect the shifting social structures and the American class system. These themes in Williams's plays not only provide a deep psychological and emotional exploration of his characters but also serve as a mirror to the societal issues and cultural shifts of his time, making his work both timeless and profoundly reflective of the human condition. Table [10](#tab_0): An example of the synthetic long-output data. ". . ." denotes the omitted parts due to space limitations.

## Synthetic Instruction

Write a scholarly paper of approximately 10,000 words discussing effective transition-to-school programs, using 10 guidelines derived from the Starting School Research Project. Include a comprehensive overview of the significance of starting school, an ecological perspective on transition, research findings, and practical applications of the guidelines. Ensure a formal tone, cite relevant literature, and structure the paper with headings for clarity. Address stakeholders such as children, parents, educators, and the community while emphasizing the importance of relationships and communication in successful transition programs.

## Groundtruth Document Volume 3 Number 2 The Author(s) 2001

Starting School: Effective Transitions This paper focuses on effective transition-to-school programs. Using a framework of 10 guidelines developed through the Starting School Research Project, it provides examples of effective strategies and transition programs. In this context, the nature of some current transition programs is questioned, and the curriculum of transition is problematized. In particular, issues are raised around who has input into such programs and who decides on appropriate curriculum.

The Significance of Starting School Starting school is an important time for young children, their families, and educators. It has been described as "one of the major challenges children have to face in their early childhood years" (Victorian Department of School Education, 1992, p. 44), "a big step for all children and their families" (New South Wales Department of School [Education, 1997, p. 8)](#), and "a key life cycle transition both in and outside school" [(Pianta &](#)[Cox, 1999, p. xvii). Pianta and](#)[Kraft-Sayre (1999, p. 47)](#) suggest that the transition to school "sets the tone and direction of a child's school career," while Christensen (1998) notes that transition to school has been described in the literature as a rite of passage associated with increased status and as a turning point in a child's life. Whether or not these descriptions are accurate, they highlight the potential significance of a child's transition to school. In Kagan's (1999) words, starting school is a "big deal." It is clearly a key experience not only for the children starting school but also for educatorsboth in schools and in prior-to-school settingsand for their families.

. . . (the document continues)

![Figure 1: The overall workflow for synthesizing long-context instruction tuning data comprises four steps: instruction generation, relevant document retrieval, recursive query-focused summarization, and response generation. The generated instructions and retrieved documents are concatenated to form the user-turn input, whereas the generated response serves as the target output.]()

![Figure 3: Scatter plot illustrating the relationship between the required generation length and the actual output length for samples from the validation set. The dashed line denotes y = x, indicating the output length precisely matches the groundtruth length. For each model, we fit a curve to show the trend of the output length as the required length increases. Details of the curve fitting procedure are provided in Appendix A.]()

![Figure 5: Needle-in-haystack test results when extending the context length up to 4M. For the 4M version, tests were conducted within 3M context length due to the prohibitively high inference cost.]()

![Results on the RULER benchmark spanning context lengths from 32k to 1M, averaged across all 13 tasks. The highest and second-highest scores are denoted in bold and underlined, respectively. Proprietary models are not directly comparable due to the lack of technical details.]()

![Average output length for each model on the validation set. The token count is determined using the Llama-3 tokenizer.]()

![The evolving performance across various test lengths as SelfLong-8B undergoes progressive training on longer contexts. The term "Supported Lengths" denotes 128k or shorter, which Llama-3.1-8B-Instruct can already handle. "Extended Lengths" refer to the context lengths exceeding 128k. If a context length is larger than the model's maximum training length, the score is assigned a value of 0.In contrast to progressive training, we also investigate the effects of directly extending to 1M without intermediate stages. The results in Table3indicate a slightly inferior performance relative to the progressive training strategy. When adopting full-length fine-tuning, an additional advantage of progressive training is its reduced computational cost compared to direct extension to maximum length.]()

![Comparison of solving long-context tasks using agent workflow versus long-context LLM at 128k length. Both approaches utilize Llama-3.1-8B-Instruct as the backbone LLM.]()

https://github.com/vllm-project/vllm

https://github.com/EleutherAI/lm-evaluation-harness/tree/main/lm_eval/tasks/ leaderboard

