{
  "arxivId": "2412.18860",
  "title": "Bootstrap Your Own Context Length",
  "authors": "Liang Wang, Nan Yang, Xingxing Zhang, Xiaolong Huang, Furu Wei",
  "abstract": "We introduce a bootstrapping approach to train long-context language models\nby exploiting their short-context capabilities only. Our method utilizes a\nsimple agent workflow to synthesize diverse long-context instruction tuning\ndata, thereby eliminating the necessity for manual data collection and\nannotation. The proposed data synthesis workflow requires only a short-context\nlanguage model, a text retriever, and a document collection, all of which are\nreadily accessible within the open-source ecosystem. Subsequently, language\nmodels are fine-tuned using the synthesized data to extend their context\nlengths. In this manner, we effectively transfer the short-context capabilities\nof language models to long-context scenarios through a bootstrapping process.\nWe conduct experiments with the open-source Llama-3 family of models and\ndemonstrate that our method can successfully extend the context length to up to\n1M tokens, achieving superior performance across various benchmarks.",
  "url": "https://arxiv.org/abs/2412.18860",
  "issue_number": 539,
  "issue_url": "https://github.com/dmarx/papers-feed/issues/539",
  "created_at": "2025-01-04T14:48:48.243492",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 44,
  "last_read": "2025-01-04T14:48:54.232387",
  "last_visited": "2024-12-30T04:37:40.310Z",
  "main_tex_file": null,
  "published_date": "2024-12-25T10:08:54Z",
  "arxiv_tags": [
    "cs.CL",
    "cs.IR"
  ]
}