# Mechanisms of Projective Composition of Diffusion Models

## Abstract

## 

We study the theoretical foundations of composition in diffusion models, with a particular focus on out-of-distribution extrapolation and lengthgeneralization. Prior work has shown that composing distributions via linear score combination can achieve promising results, including lengthgeneralization in some cases [(Du et al., 2023;](#b5)[Liu et al., 2022)](#b20). However, our theoretical understanding of how and why such compositions work remains incomplete. In fact, it is not even entirely clear what it means for composition to "work". This paper starts to address these fundamental gaps. We begin by precisely defining one possible desired result of composition, which we call projective composition. Then, we investigate: (1) when linear score combinations provably achieve projective composition, (2) whether reverse-diffusion sampling can generate the desired composition, and (3) the conditions under which composition fails. Finally, we connect our theoretical analysis to prior empirical observations where composition has either worked or failed, for reasons that were unclear at the time.

## Introduction

The possibility of composing different concepts represented by pretrained models has been of both theoretical and practical interest for some time [(Jacobs et al., 1991;](#b13)[Hinton, 2002;](#b8)[Du & Kaelbling, 2024)](#), with diverse applications including image and video synthesis [(Du et al., 2023;](#b5)[2020;](#)[Liu et al., 2022;](#b20)[2021;](#)[Nie et al., 2021;](#b24)[Yang et al., 2023a;](#)[Wang et al., 2024)](#b41), planning [(Ajay et al., 2024;](#b0)[Janner et al., 2022)](#b14), constraint satisfaction [(Yang et al., 2023b)](#), parameter-efficient training [(Hu et al., 2021;](#b11)[Ilharco et al., 2022)](#b12), and many others [(Wu et al., 2024;](#b43)[Su et al., 2024;](#b38)[Urain et al., 2023;](#b39)[Anonymous, 2024)](#). One central goal in this field is to build Figure [1](#): Composing diffusion models via score combination. Given two diffusion models, it is sometimes possible to sample in a way that composes content from one model (e.g. your dog) with style of another model (e.g. oil paintings). We aim to theoretically understand this empirical behavior.  novel compositions at inference time using only the outputs of pretrained models (either entirely separate models, or different conditionings of a single model), to create generations that are potentially more complex than any model could produce individually. As a concrete example to keep in mind, suppose we have two diffusion models, one trained on your personal photos of your dog and another trained on a collection of oil paintings, and we want to somehow combine these to generate oil paintings of your dog. Note that in order to achieve this goal, compositions must be able to generate images that are out-of-distribution (OOD) with respect to each of the individual models, since for example, there was no oil painting of your dog in either model's training set. Prior empirical work has shown that this ambitious vision is at least partially achievable in practice. However, the theoretical foundations of how and why composition works in practice, as well as its limitations, are still incomplete.

The goal of this work is to advance our theoretical understanding of composition-we will take a specific family of methods used for composing diffusion models, and we will analyze conditions under which this method provably generates the "correct" composition. Specifically, are there sufficient properties of the distributions we are composing that can guarantee that composition will work "correctly"? And what does correctness even mean, formally?

We focus our study on composing diffusion models by linearly combining their scores, a method introduced by [Du et al. (2023)](#b5); [Liu et al. (2022)](#b20) (though many other interesting constructions are possible, see Section 2). Concretely, suppose we have three separate diffusion models, one for the distribution of dog images p dog , another for oil-paintings p oil , and another unconditional model for generic images p u . Then, we can use the individual score estimates ∇ x log p(x) given by the models to construct a composite score: ∇ x log p(x) :=

(1) ∇ x log p dog (x) + ∇ x log p oil (x) -∇ x log p u (x). This implicitly defines a distribution which we will call a "product composition": p(x) ∝ p dog (x)p oil (x)/p u (x). Finally, we can try to sample from p by using these scores with a generic score-based sampler, or even reverse-diffusion. This method of composition often achieves good results in practice, yielding e.g. oil paintings of dogs, but it is unclear why it works theoretically.

We are particularly interested in the OOD generalization capabilities of this style of composition. By this we mean the compositional method's ability to generate OOD with respect to each of the individual models being composedwhich may be possible even if none of the individual models are themselves capable of OOD generation. A specific desiderata is length-generalization, understood as the ability to compose arbitrarily many concepts. For example, consider the CLEVR [(Johnson et al., 2017)](#b15) setting shown in Figure [2](#fig_1). Given conditional models trained on images each containing a single object and conditioned on its location, we want to generate images containing k > 1 objects composed in the same scene. How could such length-generalizing composition be possible? Here is one illustrative toy exampleconsider the following construction, inspired by but slightly different from [Du et al. (2023)](#b5); [Liu et al. (2022)](#b20). Suppose p b is a distribution of empty background images, and each p i a distribution of images with a single object at location i, on an otherwise empty background. Assume all locations we wish to compose are non-overlapping. Then, performing reversediffusion sampling using the following score-composition will work -meaning will produce images with k objects at appropriate locations:

$∇ x log p t b (x) + k i=1 ∇ x log p t i (x) -∇ x log p t b (x) score delta δi ∈ R n . (2)$Above, the notation p t i denotes the distribution p i after time t in the forward diffusion process (see Appendix D). Intuitively this works because during the reverse-diffusion process, the update performed by model i modifies only pixels in the vicinity of location i, and otherwise leaves them identical to the background. Thus the different models do not interact, and the sampler acts as if each model individually "pastes" an object onto an empty background. Formally, sampling works because the score delta vectors δ i are mutually orthogonal, and in fact have disjoint supports. Notably, we can sample from this composition with a standard diffusion sampler, in contrast to [Du et al. (2023)](#b5)'s observations that more sophisticated samplers are necessary. This construction would not be guaranteed to work, however, if the "background" p b was chosen to be the unconditional distribution p u (as in Equation [1](#)), a common choice in many prior works [(Du et al., 2023;](#b5)[Liu et al., 2022)](#b20).

The remainder of this paper is devoted to trying to generalize this example as far as possible, and understand both its explanatory power and its limitations. It turns out the core mechanism can be generalized surprisingly far, and does not depend on "orthogonality" as strongly as the above example may suggest. We will encounter some subtle aspects along the way, starting from formal definitions of what it means for composition to succeed -a definition that can capture both composing objects (as in Figure [2](#fig_1)), and composing other attributes (such as style + content, in Figure [1](#)).

## Contributions and Organization

In this work we introduce a theoretical framework to help understand the empirical success of certain methods of composing diffusion models, with emphasis on understanding how compositions can sometimes length-generalize. We start by discussing the limitations of several prior definitions of composition in Section 3. In Section 4 we offer a formal definition of "what we want composition to do", given precise information about which aspects we want to compose, which we call Projective Composition (Definition 4.1). (Note that there are many other valid notions of composition; we are merely formalizing one particular goal.) Then, we study how projective composition can be achieved. In Section 5 we introduce formal criteria called Factorized Conditionals (Definition 5.2), which is a type Figure [3](#): Attempted compositional length-generalization up to 9 objects. We attempt to compose via linear score combination the distributions p 1 through p 9 shown on the far left, where each p i is conditioned on a specific object location as described below. Settings (A) and (C) approximately satisfy the conditions of our theory of projective composition, and thus are expected to length-generalize at least somewhat, while setting (B) does not even approximately satisfy our conditions and indeed fails to length-generalize. Experiment (A): In this experiment, the distributions p i each contain a single object at a fixed location, and the background p b is empty. In this case any successful composition of more than one object represents length-generalization. We find that composition succeeds up to several objects, but then degrades as number of objects increases (see Section 5.3 for details). Experiment (B): Here the distributions p i are identical to (A), but the background p b is chosen as the unconditional distribution (i.e. a single object at a random location)-this the "Bayes composition" (Section 3). This composition entirely fails-remarkably, trying to compose many objects often produces no objects! Experiment (C): Here each distribution p i contains an object at a fixed location i, and 0 -4 other objects (sampled uniformly) in random locations; see samples at far left. The background distribution p b is a distribution of 1 -5 objects (sampled uniformly) in random locations. In this case length-generalization means composition of more than 5 objects. This composition can length-generalize, but artifacts appear for large numbers of objects. See Section 5.3 for a full discussion.

of independence criteria along both distributions and coordinates. We prove that when this criteria holds, projective composition can be achieved by linearly combining scores (as in Equation [2](#)), and can be sampled via standard reversediffusion samplers. In Section 6 we show that parts of this result can be extended much further to apply even in nonlinear feature spaces; but interestingly, even when projective composition is achievable, it may be difficult to sample. We find that in many important cases existing constructions approximately satisfy our conditions, but the theory also helps characterize and explain certain limitations. Finally in Section 8 we discuss how our results can help explain existing experimental results in the literature where composition worked or failed, for reasons that were unclear at the time.

## Related Work

Single vs. Multiple Model Composition. First, we distinguish the kind of composition we study in this paper from approaches that rely a single model but with OOD condition-ers; for example, passing OOD text prompts to text-to-image models [(Nichol et al., 2021;](#b23)[Podell et al., 2023)](#b28), or works like [Okawa et al. (2024)](#b25); [Park et al. (2024)](#b27). In contrast, we study compositions which recombine the outputs of multiple separate models at inference time, where each model only sees in-distribution conditionings. Among compositions involving multiple models, many different variants have been explored. Some are inspired by logical operators like AND and OR, which are typically implemented as product p 0 (x)p 1 (x) and sum p 0 (x)+p 1 (x) [(Du et al., 2023;](#b5)[Du & Kaelbling, 2024;](#)[Liu et al., 2022)](#b20). Some composition methods are based on diffusion models, while others use energy-based models [(Du et al., 2020;](#b4)[2023;](#)[Liu et al., 2021)](#b19) or densities [(Skreta et al., 2024)](#b32). In this work, we focus specifically on product-style compositions implemented with diffusion models via a linear combinations of scores as in [Du et al. (2023)](#b5); [Liu et al. (2022)](#b20). Our goal is not to propose a new method of composition but to improve theoretical understanding of existing methods.

Learning and Generalization. In this work we focus only on mathematical aspects of composition, and we do not consider any learning-theoretic aspects such as inductive bias or sample complexity. Our work is thus complementary to [Kamb & Ganguli (2024)](#b16), which studies how a type of compositional generalization can arise from inductive bias in the learning procedure. Additional related works are discussed in Appendix A.

## Prior Definitions of Composition

In this section we will describe why two popular mathematical definitions of composition are insufficient for our purposes: the "simple product" definition, and the Bayes composition. Specifically, neither of these notions can describe the outcome of the CLEVR length-generalization experiment from Figure [2](#fig_1). Our observations here will thus motivate us to propose a new definition of composition, in the following section. As a running example, we will consider a subset of the CLEVR experiment from Figure [2](#fig_1). Suppose we are trying to compose two distributions p 1 , p 2 of images each containing a single object in an otherwise empty scene, where the object is in the lower-left corner under p 1 , and the upper-right corner under p 2 . We would like the composed distribution p to place objects in at least the lower-left and upper-right, simultaneously.

## The Simple Product

The simple product is perhaps the most familiar type of composition: Given two distributions p 1 and p 2 over R n , the simple product is defined[foot_0](#foot_0) as p(x) ∝ p 1 (x)p 2 (x). The simple product can represent some interesting types of composition, but it has a key limitation: the composed distribution can never be truly "out-of-distribution" w.r.t. p 1 or p 2 , since p(x) = 0 whenever p 1 (x) = 0 or p 2 (x) = 0. This presents a problem for our CLEVR experiment. Using the simple product definition, we must have p(x) = 0 for any image x with two objects, since neither p 1 nor p 2 was supported on images with two objects. Therefore, the simple product definition cannot represent our desired composition.

## The Bayes Composition

Another candidate definition for composition, which we will call the "Bayes composition", was introduced and studied by [Du et al. (2023)](#b5); [Liu et al. (2022)](#b20). The Bayes composition is theoretically justified when the desired composed distribution is formally a conditional distribution of the model's training distribution. However, it is not formally capable of generating truly out-of-distribution samples, as our example below will illustrate.

Let us attempt to apply the Bayes composition methodology to our CLEVR example. We interpret our two distributions p 1 , p 2 as conditional distributions, conditioned on an object appearing in the lower-left or upper-right, respectively. Thus we write p(x|c 1 ) ≡ p 1 (x), where c 1 is the event that an object appears in the lower-left of image x, and c 2 the event an object appears in the upper-right. Now, since we want both objects simultaneously, we define the composition as p(x) := p(x|c 1 , c 2 ). Because the two events c 1 and c 2 are conditionally independent given x (since they are deterministic functions of x), we can compute p in terms of the individual conditionals:

$p(x) := p(x|c 1 , c 2 ) ∝ p(x|c 1 )p(x|c 2 )/p(x).$(3)

Equivalently in terms of scores: ∇ x log pt (x) := ∇ x log p(x|c 1 ) + ∇ x log p(x|c 2 ) -∇ x log p(x). Line (3) thus serves as our definition of the Bayes composition p, in terms of the conditional distributions p(x|c 1 ) and p(x|c 2 ), and the unconditional p(x).

The definition of composition above seems natural: we want both objects to appear simultaneously, so let us simply condition on both these events. However, there is an obvious error in the conclusion: p(x) must be 0 whenever p(x|c 1 ) or p(x|c 2 ) is zero (by Line 3). Since neither conditional distribution have support on images with two objects, the composition p cannot contain images of two objects either.

Where did this go wrong? The issue is: p(x|c 1 , c 2 ) is not well-defined in our case. We intuitively imagine some unconditional distribution p(x) which allows both objects simultaneously, but no such distribution has been defined, or encountered by the models during training. Thus, the definition of p in Line (3) does not actually correspond to our intuitive notion of "conditioning on both objects at once." More generally, this example illustrates how the Bayes composition cannot produce truly out-of-distribution samples, with respect to the distributions being composed. [2](#foot_1)Figure [3b](#) shows that the Bayes composition does not always work experimentally either: for diffusion models trained in a CLEVR setting similar to Figure [2](#fig_1), the Bayes composition of k > 1 locations typically fails to produce k objects (and sometimes produces zero). The difficulties discussed lead us to propose a precise definition of what we actually "want" composition to do in this case.

## Our Proposal: Projective-Composition

We now present our formal definition of what it means to "correctly compose" distributions. Our main insight here is, a realistic definition of composition should not purely be a function of distributions {p 1 , p 2 , . . . }, in the way the simple product p(x) = p 1 (x)p 2 (x) is purely a function of p 1 , p 2 . We must also somehow specify which aspects of each distribution we care about preserving in the composition. For example, informally, we may want a composition that mimics the style of p 1 and the content of p 2 . Our definition below of projective composition allows us this flexibility.

Roughly speaking, our definition requires specifying a "feature extractor" Π i : R n → R k associated with every distribution p i . These functions can be arbitrary, but we usually imagine them as projections[foot_2](#foot_2) in some feature-space, e.g, Π 1 (x) may be a transform of x which extracts only its style, and Π 2 (x) a transform which extracts only its content. Then, a projective composition is any distribution p which "looks like" distribution p i when both are viewed through Π i (see Figure [4](#fig_2)). Formally:

Definition 4.1 (Projective Composition). Given a collection of distributions {p i } along with associated "projection" functions

${Π i : R n → R k }, we call a distribution p a projective composition if 4 ∀i : Π i ♯p = Π i ♯p i .(4)$That is, when p is projected by each Π i , it yields marginals identical to those of p i .

There are a few aspects of this definition worth emphasizing, which are conceptually different from many prior notions of composition. First, our definition above does not construct a composed distribution; it merely specifies what properties the composition must have. For a given set of {(p i , Π i )}, there may be many possible distributions p which are projective compositions; or in other cases, a projective composition may not even exist. Separately, the definition of projective composition does not posit any sort of "true" underlying distribution, nor does it require that the distributions p i are conditionals of an underlying joint distribution. In particular, projective compositions can be truly "out of distribution" with respect to the p i : p can be supported on samples x where none of the p i are supported.

Examples. We have already discussed the style+content composition of Figure [1](#) as an instance of projective composition. Another even simpler example to keep in mind is the following coordinate-projection case. Suppose we take Π i : R n → R to be the projection onto the i-th coordinate. Then, a projective composition of distributions {p i } with these associated functions {Π i } means: a distribution where the first coordinate is marginally distributed identically to the first coordinate of p 1 , the second coordinate is marginally distributed as p 2 , and so on. (Note, we do not require any independence between coordinates). This notion of composition would be meaningful if, for example, we are already working in some disentangled feature space, where the first coordinate controls the style of the image the second coordinate controls the texture, and so on. The CLEVR length-generalization example from Figure [2](#fig_1) can also be described as a projective composition in almost an identical way, by letting Π i : R n → R k be a restriction onto the set of pixels neighboring location i. We describe this explicitly later in Section 5.3.

## Simple Construction of Projective Compositions

It is not clear apriori that projective compositional distributions satisfying Definition 4.1 ever exist, much less that there is any straightforward way to sample from them. To explore this, we first restrict attention to perhaps the simplest setting, where the projection functions {Π i } are just coordinate restrictions. This setting is meant to generalize the intuition we had in the CLEVR example of Figure [2](#fig_1), where different objects were composed in disjoint regions of the image. We first define the construction of the composed distribution, and then establish its theoretical properties.

## Defining the Construction

Formally, suppose we have a set of distributions (p 1 , p 2 , . . . , p k ) that we wish to compose; in our running CLEVR example, each p i is the distribution of images with a single object at position i. Suppose also we have some reference distribution p b , which can be arbitrary, but should be thought of as a "common background" to the p i s. Then, one popular way to construct a composed distribution is via the compositional operator defined below. (A special case of this construction is used in [Du et al. (2023)](#b5), for example).

Definition 5.1 (Composition Operator). Define the composition operator C acting on an arbitrary set of distributions (p b , p 1 , p 2 , . . .) by

$C[⃗ p] := C[p b , p 1 , p 2 , . . . ](x) := 1 Z p b (x) i p i (x) p b (x) , (5$$)$where Z is the appropriate normalization constant. We name C[⃗ p] the composed distribution, and the score of C[⃗ p] the compositional score:

$∇ x log C[⃗ p](x) (6) = ∇ x log p b (x) + i (∇ x log p i (x) -∇ x log p b (x)) .$Notice that if p b is taken to be the unconditional distribution then this is exactly the Bayes-composition.

## When does the Composition Operator Work?

We can always apply the composition operator to any set of distributions, but when does this actually yield a "correct" composition (according to Definition 4.1)? One special case is when each distribution p i is "active" on a different, nonoverlapping set of coordinates. We formalize this property below as Factorized Conditionals (Definition 5.2). The idea is, each distribution p i must have a particular set of "mask" coordinates M i ⊆ [n] which it samples in a characteristic way, while independently sampling all other coordinates from a common background distribution. If a set of distributions (p b , p 1 , p 2 , . . .) has this Factorized Conditional structure, then the composition operator will produce a projective composition (as we will prove below).

Definition 5.2 (Factorized-Conditionals). We say a set of distributions (p b , p 1 , p 2 , . . . p k ) over R n are Factorized Conditionals if there exists a partition of coordinates

$[n] into disjoint subsets M b , M 1 , . . . M k such that: 1. (x| Mi , x| M c i ) are independent under p i . 2. (x| M b , x| M1 , x| M2 , . . . , x| M k ) are mutually indepen- dent under p b . 3. p i (x| M c i ) = p b (x| M c i ).$Equivalently, if we have:

$p i (x) = p i (x| Mi )p b (x| M c i ), and(7)$$p b (x) = p b (x| M b ) i∈[k] p b (x| Mi ).$Equation ( [7](#formula_7)) means that each p i can be sampled by first sampling x ∼ p b , and then overwriting the coordinates of M i according to some other distribution (which can be specific to distribution i). For instance, the experiment of Figure [2](#fig_1) intuitively satisfies this property, since each of the conditional distributions could essentially be sampled by first sampling an empty background image (p b ), then "pasting" a random object in the appropriate location (corresponding to pixels M i ). If a set of distributions obey this Factorized Conditional structure, then we can prove that the composition operator C yields a correct projective composition, and reverse-diffusion correctly samples from it. Below, let N t denote the noise operator of the diffusion process[foot_4](#foot_4) at time t.

Theorem 5.3 (Correctness of Composition). Suppose a set of distributions (p b , p 1 , p 2 , . . . p k ) satisfy Definition 5.2, with corresponding masks {M i } i . Consider running the reverse-diffusion SDE using the following compositional scores at each time t:

$s t (x t ) := ∇ x log C[p t b , p t 1 , p t 2 , . . .](x t ),(8)$where p t i := N t [p i ] are the noisy distributions. Then, the distribution of the generated sample x 0 at time t = 0 is:

$p(x) := p b (x| M b ) i p i (x| Mi ).(9)$In particular, p(x| Mi ) = p i (x| Mi ) for all i, and so p is a projective composition with respect to projections {Π i (x) := x| Mi } i , per Definition 4.1.

Unpacking this, Line 9 says that the final generated distribution p(x) can be sampled by first sampling the coordinates M b according to p b (marginally), then independently sampling coordinates M i according to p i (marginally) for each i. Similarly, by assumption, p i (x) can be sampled by first sampling the coordinates M i in some specific way, and then independently sampling the remaining coordinates according to p b . Therefore Theorem 5.3 says that p(x) samples the coordinates M i exactly as they would be sampled by p i , for each i we wish to compose.

Proof. (Sketch) Since ⃗ p satisfies Definition 5.2, we have

$C[⃗ p](x) := p b (x) i pi(x) p b (x) = p b (x) i p b (xt| M c i )pi(x|M i ) p b (x| M c i )p b (x|M i ) = p b (x) i pi(x|M i ) p b (x|M i ) = p b (x|M b ) i pi(xt|M i ) := p(x).$The sampling guarantee follows from the commutativity of composition with the diffusion noising process, i.e. C[

$⃗ p t ] = Nt[C[⃗ p]]. The complete proof is in Appendix G.$Remark 5.4. In fact, Theorem 5.3 still holds under any orthogonal transformation of the variables, because the diffusion noise process commutes with orthogonal transforms. We formalize this as Lemma 7.1. Remark 5.5. Compositionality is often thought of in terms of orthogonality between scores. Definition 5.2 implies orthogonality between the score differences that appear in the composed score (6): ∇ x log p t i (x t ) -∇ x log p t b (x t ), but the former condition is strictly stronger (c.f. Appendix F). Remark 5.6. Notice that the composition operator C can be applied to a set of Factorized Conditional distributions without knowing the coordinate partition {M i }. That is, we can compose distributions and compute scores without knowing apriori exactly "how" these distributions are supposed to compose (i.e. which coordinates p i is active on). This is already somewhat remarkable, and we will see a much stronger version of this property in the next section.

Importance of background. Our derivations highlight the crucial role of the background distribution p b for the composition operator (Definition 5.1). While prior works have taken p b to be an unconditional distribution and the p i 's its associated conditionals, our results suggest this is not always the optimal choice -in particular, it may not satisfy a Factorized Conditional structure (Definition 5.2). Figure [3](#) demonstrates this empirically: settings (a) and (b) attempt to compose the same distributions using different backgrounds -empty (a) or unconditional (b) -with very different results.

## Approximate Factorized Conditionals in CLEVR.

In Figure [3](#) we explore compositional length-generalization (or lack thereof) in three different setting, two of which (Figure [3a](#) and [3c](#)) approximately satisfy Definition 5.2. In this section we explicitly describe how our definition of Factorized Conditionals approximately captures the CLEVR settings of Figures [3a](#) and [3c](#). The setting of 3b does not satisfy our conditions, as discussed in Section 3.

Single object distributions with empty background. This is the setting of both Figure [2](#fig_1) and Figure [3a](#). The background distribution p b over n pixels is images of an empty scene with no objects. For each i ∈ {1, . . . , L} (where L = 4 in Figure [2](#fig_1) and L = 9 in Figure [3a](#)), define the set M i ⊂ [n] as the set of pixel indices surrounding location i. (M i should be thought of as a "mask" that that masks out objects at location i). Let M b := (∪ i M i ) c be the remaining pixels in the image. Then, we claim the distributions (p b , p 1 , . . . , p L ) form approximately Factorized Conditionals, with corresponding coordinate partition {M i }. This is essentially because each distribution p i matches the background p b on all pixels except those surrounding location i (further detail in Appendix B.2). Note, however, that the conditions of Definition 5.2 do not exactly hold in the experiment of Figure [2](#fig_1) -there is still some dependence between the masks M i , since objects can cast shadows or even occlude each other. Empirically, these deviations have greater impact when composing many objects, as seen in Figure [3a](#).  [2022](#)) where the images contain many objects (1-5) and the conditions label the location of one randomlychosen object. It turns out the unconditional together with the conditionals can approximately act as Factorized Conditionals in "cluttered" settings like this one. The intuition is that if the conditional distributions each contain one specific object plus many independently sampled random objects ("clutter"), then the unconditional distribution almost looks like independently sampled random objects, which together with the conditionals would satisfy Definition 5.2 (further discussion in Appendix B.2 and E). This helps to explain the length-generalization observed in [Liu et al. (2022)](#b20) and verified in our experiments (Figure [3c](#)).

## Projective Composition in Feature Space

So far we have focused on the setting where the projection functions Π i are simply projections onto coordinate subsets M i in the native space (e.g. pixel space). This covers simple examples like Figure [2](#fig_1) but does not include more realistic situations such as Figure [1](#), where the properties to be composed are more abstract. For example a property like "oil painting" does not correspond to projection onto a specific subset of pixels in an image. However, we may hope that there exists some conceptual feature space in which "oil painting" does correspond to a particular subset of variables. In this section, we extend our results to the case where Figure [6](#): A commutative diagram illustrating Theorem 6.1. Performing composition in pixel space is equivalent to encoding into a feature space (A), composing there, and decoding back to pixel space (A -1 ).

the composition occurs in some conceptual feature space, and each distribution to be composed corresponds to some particular subset of features.

Our main result is a featurized analogue of Theorem 5.3: if there exists any invertible transform A mapping into a feature space where Definition 5.2 holds, then the composition operator (Definition 5.1) yields a projective composition in this feature space, as shown in Figure [6](#). 

$A♯p(z) ≡ (A♯p b (z))| M b k i=1 (A♯p i (z))| Mi . (10) Therefore, p is a projective composition of ⃗ p w.r.t. projection functions {Π i (x) := A(x)| Mi }.$This theorem is remarkable because it means we can compose distributions (p b , p 1 , p 2 , . . . ) in the base space, and this composition will "work correctly" in the feature space automatically (Equation [10](#)), without us ever needing to compute or even know the feature transform A explicitly. Theorem 6.1 may apriori seem too strong to be true, since it somehow holds for all feature spaces A simultaneously. The key observation underlying Theorem 6.1 is that the composition operator C behaves well under reparameterization. Lemma 6.2 (Reparameterization Equivariance). The composition operator of Definition 5.1 is reparameterizationequivariant. That is, for all diffeomorphisms A : R n → R n and all tuples of distributions ⃗ p = (p b , p 1 , p 2 , . . . , p k ),

$C[A♯⃗ p] = A♯C[⃗ p]. (11$$)$This lemma is potentially of independent interest: equivariance distinguishes the composition operator from many other common operators (e.g. the simple product). Lemma 6.2 and Theorem 6.1 are proved in Appendix H.

## Sampling from Compositions.

The feature-space Theorem 6.1 is weaker than Theorem 5.3 in one important way: it does not provide a sampling algorithm. That is, Theorem 6.1 guarantees that p := C[⃗ p] is a projective composition, but does not guarantee that reverse-diffusion is a valid sampling method.

There is one special case where diffusion sampling is guaranteed to work, namely, for orthogonal transforms (which can seen as a straightforward extension of the coordinatealigned case of Theorem 5.3):

Lemma 7.1 (Orthogonal transform enables diffusion sampling). If the assumptions of Lemma 6.1 hold for A(x) = Ax, where A is an orthogonal matrix, then running a reverse diffusion sampler with scores

$s t = ∇ x log C[⃗ p t ] generates the composed distribution p = C[⃗ p] satisfying (10).$The proof is given in Appendix I.

However, for general invertible transforms, we have no such sampling guarantees. Part of this is inherent: in the featurespace setting, the diffusion noise operator N t no longer commutes with the composition operator C in general, so scores of the noisy composed distribution N t [C[⃗ p]] cannot be computed from scores of the noisy base distributions N t [⃗ p].

Nevertheless, one may hope to sample from the distribution p using other samplers besides diffusion, such as annealed Langevin Dynamics or Predictor-Corrector methods [(Song et al., 2020)](#b36). We find that the situation is surprisingly subtle: composition C produces distributions which are in some cases easy to sample (e.g. with diffusion), yet in other cases apparently hard to sample. For example, in the setting of Figure [5](#fig_3), our Theorem 6.1 implies that all pairs of colors should compose equally well at time t = 0, since there exist diffeomorphisms (indeed, linear transforms) between different colors. However, as we saw, the diffusion sampler fails to sample from compositions of non-orthogonal colors-and empirically, even more sophisticated samplers such as Predictor-Correctors also fail in this setting. At first glance, it may seem odd that composed distributions are so hard to sample, when their constituent distributions are relatively easy to sample. One possible reason for this below is that the composition operator has extremely poor Lipchitz constant, so it is possible for a set of distributions ⃗ p to "vary smoothly" (e.g. diffusing over time) while their Figure [7](#): Composing Entangled Concepts. The left image composes the text-conditions "photo of a dog" with "photo of a horse", which both control the subject of the image, and produces unexpected results. In contrast, the right image composes "photo of a dog" with "photo, with red hat," which intuitively correspond to disentangled features. Both samples from SDXL using score-composition with an unconditional background; details in Appendix C.

composition C[⃗ p] changes abruptly. We formalize this in Lemma 7.2 (further discussion and proof in Appendix J).

Lemma 7.2 (Composition Non-Smoothness). For any set of distributions {p b , p 1 , p 2 , . . . , p k }, and any noise scale t := σ, define the noisy distributions p t i := N t [p i ], and let q t denote the composed distribution at time t: q

$t := C[⃗ p t ].$Then, for any choice of τ > 0, there exist distributions {p b , p 1 , . . . p k } over R n such that 1. For all i, the annealing path of p i is O(1)-Lipshitz: ∀t, t ′ : W 2 (p t i , p t ′ i ) ≤ O(1)|t -t ′ |. 2. The annealing path of q has Lipshitz constant at least Ω(τ -1 ): ∃t, t ′ : W 2 (q t , q t ′ ) ≥ |t-t ′ | 2τ .

## Connections with Prior Observations

We have presented a mathematical theory of composition.

Although this theoretical model is a simplification of reality (we do not claim its assumptions hold exactly in practice) we believe the spirit of our results carries over to practical settings, and can help understand empirical observations from prior work. We now discuss some of these connections.

Independence Assumptions and Disentangled Features.

Our theory relies on a type of independence between distributions, related to orthogonality between scores, which we formalize as Factorized Conditionals. While such conditional structure typically does not exist in pixel-space, it is plausible that a factorized structure exists in an appropriate feature space, as permitted by our theory (Section 6). In particular, a feature space and distribution with perfectly "disentangled" features would satisfy our assumptions. Conversely, if distributions are not appropriately disentangled, our theory predicts that linear score combinations will fail to compose correctly. This effect is well-known; see Figure [7](#) for an example; similar failure cases are highlighted in [Liu et al. (2022)](#b20) as well (such as "A bird" failing to compose with "A flower"). Regarding successful cases, style and content compositions consistently work well in practice, and are often taken to be disentangled features (e.g. Karras (2019); [Gatys et al. (2016)](#b6); [Zhu et al. (2017)](#b48)). Finally, similar in spirit to our theory, methods for successful composition in practice such as LoRA task arithmetic [(Zhang et al., 2023a;](#)[Ilharco et al., 2022)](#b12), typically require some type of approximate "concept-space orthogonality".

Text conditioning with location information. Conditioning on location is a straightforward way to achieve factorized conditionals (provided the objects in different locations are approximately independent) since the required disjointness already holds in pixel-space. Many successful text-to-image compositions in Liu et al. ( [2022](#)) use location information in the prompts, either explicitly (e.g. "A blue bird on a tree" + "A red car behind the tree") or implicitly ("A horse" + "A yellow flower field"; since horses are typically in the foreground and fields in the background).

Unconditional Backgrounds. Most prior works on diffusion composition use the Bayes composition, with substantial practical success. As discussed in Section 5.3, Bayes composition may be approximately projective in "cluttered" settings, helping to explain its practical success in text-toimage settings, where images often contain many different possible objects and concepts.

## Conclusion

In this work, we have developed a theory of one possible mechanism of composition in diffusion models. We study how composition can be defined, and sufficient conditions for it to be achieved. Our theory can help understand a range of diverse compositional phenomena in both synthetic and practical settings, and we hope it will inspire further work on foundations of composition.

## A. Additional Related Works

Structured compositional generative models. Structured generative models leverage architectural inductive biases in an encoder-decoder framework, such as recurrent attention mechanisms [(Gregor et al., 2015)](#b7) or slot-attention [(Wang et al., 2023)](#b40). These models decompose scenes into background and parts-based representations in an unsupervised manner guided by modeling priors. While these approaches can flexibly generate scenes with single or multiple objects, they are not explicitly controllable, and require specific model pre-training on datasets containing compositions of interest.

Controllable generation. Composition at inference-time is one potential mechanism for exerting control over the generation process. Another way to modify compositions of style and/or content attributes is through spatial conditioning a pre-trained diffusion model on a structural attribute (e.g., pose or depth) as in [Zhang et al. (2023b)](#), or on multiple attributes of style and/or content as in [Stracke et al. (2024)](#b37). Another option is control through resampling, as in [Liu et al. (2024)](#b21). These methods are complementary to single or multiple model conditioning mechanisms based on score composition that we study in the current work.

Single model conditioning. We distinguish the kind of composition we study in this paper from approaches that rely on a single model but use OOD conditioners to achieve novel combinations of concepts never seen together during training; for example, passing OOD text prompts to text-to-image models [(Nichol et al., 2021;](#b23)[Podell et al., 2023)](#b28), or works like [Okawa et al. (2024)](#b25); [Park et al. (2024)](#b27) where a single model conditions simultaneously on multiple attributes like shape and color, with some combinations held out during training. In contrast, the compositions we study recombine the outputs of multiple separate models at inference time. Though less powerful, this can still be surprisingly effective, and is more amenable to theoretical study since it disentangles the potential role of conditional embeddings.

Multiple model composition. Among compositions involving multiple separate models, many different variants have been explored with different goals and applications. Some definitions of composition are inspired by logical operators like AND and OR, usually taken to mean that the composed distribution should have high probability under all of the conditional distributions to be composed, or at least one of them, respectively. Given two conditional probabilities p 0 (x), p 1 (x), AND is typically implemented as the product p 0 (x)p 1 (x) and OR as sum p 0 (x) + p 1 (x) (though these only loosely correspond to the logical operators and other implementations are also possible). Some composition methods are based on diffusion models and use the learned scores (mainly for product compositions), others use energy-based models (which allows for OR-inspired sum compositions, as well as more sophisticated samplers, in particular sampling at t = 0 [(Du et al., 2020;](#b4)[2023;](#)[Liu et al., 2021)](#b19), and still others work directly with the densities [(Skreta et al., 2024)](#b32) (enabling an even greater variety of compositions, including a different style of AND, taken to mean p 0 (x) = p 1 (x)). [McAllister et al. (2025)](#b22) explore another type of OR composition. [(Wiedemer et al., 2024](#b42)) take a different approach of taking the final rendered images generated by separate diffusion models and "adding them up" in pixel-space, as part of a study on generalization of data-generating processes. Task-arithmetic [(Zhang et al., 2023a;](#)[Ilharco et al., 2022)](#b12), often using LoRAs [(Hu et al., 2021)](#b11), is a kind of composition in weight-space that has had significant practical impact.

Product compositions. In this work, we focus specifically on product compositions (broadly defined to allow for a "background" distribution, i.e. compositions of the form p(x) = p b (x) i pi(x) p b (x) ) implemented with diffusion models, which allows the composition to be implemented via a linear combinations of scores as in [Du et al. (2023)](#b5); [Liu et al. (2022)](#b20). Our goal is not to propose a wholly new method of composition but rather to improve theoretical understanding of existing methods.

Learning and Generalization. Recently, [Kamb & Ganguli (2024)](#b16) demonstrated how a type of compositional generalization arises from inductive bias in the learning procedure (equivariance and locality). Their findings are relevant to our broader motivation, but complementary to the focus of this work. Specifically, we focus only on mathematical aspects of defining and sampling from compositional distributions, and we do not consider any learning-theoretic aspects such as inductive bias or sample complexity. This allows us to study the behavior of compositional sampling methods even assuming perfect knowledge of the underlying distributions.

1. In each distribution p i , the pixels inside the mask M i are approximately independent from the pixels outside the mask, since the outside pixels always describe an empty scene.

2. In the background p b , the set of masks {M i } specify approximately mutually-independent sets of pixels, since all pixels are roughly constant.

3. The distribution of p i and p b approximately agree along all pixels outside mask M i , since they both describe an empty scene outside this mask.

Thus, the set of distributions approximately form Factorized Conditionals. However the conditions of Definition 5.2 do not exactly hold, since objects can cast shadows on each other and may even occlude each other. Empirically, this can significantly affect the results when composing many objects, as explored in Figure [3](#)(a).

## B.2.2. CLUTTERED DISTRIBUTED WITH UNCONDITIONAL BACKGROUND

Figure [8](#): Samples from unconditional model trained on images containing 1-5 objects. The sampled images sometimes contain 6 objects (circled in orange).

Next, we discuss the setting of Figure [3c](#), which is a Bayes composition based on an unconditional distribution where each scene contains 1-5 objects (with the number of objects sampled uniformly). The locations and all other attributes of the objects are sampled independently. The conditions label the location of one randomly-chosen object. Just as in the previous case, for each i ∈ {1, 2, . . . , L} (L = 9 in Figure [3c](#)), we define the set M i ∈ [n] as the set of pixel indices surrounding location i, and let M b := (∪ i M i ) c be the remaining pixels in the image, excluding all the masks. Again, we claim that the distributions (p b , p 1 , . . . , p L ) are approximately Factorized Conditionals, with corresponding coordinate partition (M b , M 1 , . . . , M L ). We examine the criteria in Definition 5.2:

1. In each distribution p i , the pixels inside the mask M i are approximately independent from the pixels outside the mask, since the outside pixels approximately describe a distribution containing 0-4 objects, and the locations and other attributes of all objects are independent.

2. In the unconditional background distribution p b , we argue that in practice, the set of masks {M i } are approximately mutually-independent. By assumption, the locations and other attributes of all shapes are all independent, and the masks M i are chosen in these experiment to minimize interaction/overlap. The main difficulty is the restriction to 1-5 total objects, which we discuss below.

3. The distribution of p i and p b approximately agree along all pixels outside mask M i , since

$p i | M c i contains 0-4 objects, while p b | M c i contains 0-5 objects (since one object could be 'hidden' in M c i ).$There are, however, two important caveats to the points above. First, overlap or other interaction (shadows, etc.) between objects can clearly violate all three criteria. In our experiment, this is mitigated by the fact that the masks M i are chosen to minimize interaction/overlap (though interactions start to occur as we compose more objects, leading to some image degradation). Second, since the number of objects is sampled uniformly from 1-5, the presence of one object affects the probability that another will be present. Thus, the masks {M i } are not perfectly independent under the background distribution, nor do p i and p b perfectly agree on M c i . Ideally, each p i would place an object in mask M i and independently follow p b on M c i , and p b would be such that the probability that an object appears in mask M i is independently Bernoulli (c.f. Appendix E.2). In particular, this would imply that the distribution of the total number of objects is Binomial (which allows the total object-count to range from zero to the total-number-of-locations, as well as placing specific probabilities on each object-count), which clearly differs from the uniform distribution over 1-5 objects. However, a few factors mitigate this discrepancy:

• A Binomial with sufficiently small probability-of-success places very little probability on large k. For example, under Binomial(9, 0.3), P(k = 0 : 5) = 0.04, 0.156, 0.27, 0.27, 0.17, 0.07 and P(k > 5) = 0.026.

• Empirically, the learned unconditional distribution does not actually enforce k < 5; we sometimes see samples with k = 6 for example, as seen in Figure [8](#).

Intuitively, the train distribution is "close to Bernoulli" and the learned distribution seems to be even closer.

With these considerations in mind, we see that the set of distributions approximately -though imperfectly -form Factorized Conditionals. One advantage of this setting compared to the single-object setting is that the models can learn how multiple objects should interact and even overlap correctly, potentially making it easier compose nearby locations. We explore the length-generalization of this composition empirically in Figure [3c](#) (note, however, that only compositions of more than 5 objects are actually OOD w.r.t. the distributions p i in this case).

## B.3. Additional CLEVR samples

In this section we provide additional non-cherrypicked samples of the experiments shown in the main text.  distributions p i that set a single index i to one and all other indices to zero, a zero-background distribution p b , and an unconditional distribution formed from the conditionals by assuming p(c = i) is uniform. That is:

$p t i (x t ) = N (x t ; e i , σ 2 t ) ∝ exp - ∥x t -e i ∥ 2 2σ 2 t p t b (x t ) = N (x t ; 0, σ 2 t ) ∝ exp - ∥x t ∥ 2 2σ 2 t p t u (x t ) = 1 n n i=1 p i (x t )(16)$Suppose we want to compose all n distributions p i , that is, we want to activate all indices. It is enough to consider x t of the special form x t = (α, . . . , α) since there is no reason to favor any condition over any another. Making this restriction,

$x t = (α, . . . , α) =⇒ p t i (x t ) ∝ exp - (n -1)α 2 + (1 -α) 2 2σ 2 t = exp - nα 2 -2α + 1 2σ 2 t , ∀i p t u (x t ) = exp - nα 2 -2α + 1 2σ 2 t p t b (x t ) ∝ exp - nα 2 2σ 2 t$Let us find the value of α that maximizes the probability under the Bayes composition of all condition:

$x t = (α, . . . , α) =⇒ p t i (x t ) p t u (x t ) = 1 =⇒ p t u (x t ) n i=1 p t i (x t ) p t u (x t ) ∝ p t u (x t ) ∝ exp - nα 2 -2α + 1 2σ 2 t = exp - n(α -1 n ) 2 + const 2σ 2 t =⇒ α ⋆ = 1 n ,$so the optimum is α ⋆ = 1 n . That is, under the Bayes composition the most likely configuration places value 1 n at each index we wished to activate, rather than the desired value 1.

On the other hand, if we instead use p b in the linear score combination and optimize, we find that:

$x t = (α, . . . , α) =⇒ =⇒ p t i (x t ) p t b (x t ) ∝ exp - 1 -2α 2σ 2 t =⇒ p t b (x t ) n i=1 p t i (x t ) p t b (x t ) ∝ exp - nα 2 2σ 2 t exp - n(1 -2α) 2σ 2 t ∝ exp - n(α 2 -2α + 1) 2σ 2 t ∝ exp - n(α -1) 2 2σ 2 t =⇒ α ⋆ = 1 so the optimum is α ⋆ = 1.$That is, the most likely configuration places the desired value 1 at each index we wished to activate, achieving projective composition, and in particular, length-generalizing correctly.

## E.2. Cluttered Distributions

In certain "cluttered" settings, the Bayes composition may be approximately projective. We explore this in the following simplified setting, corresponding to the experiment in Figure [11](#) (right). Suppose that x is binary-valued, M i = {i}, ∀i, the x i are independently Bernoulli with parameter q under the background, and the projected conditional distribution p i (x| i ) just guarantees that x i = 1:

$p b (x| i c ) ∼ Bern q (x| i c ), i.i.d. ∀i, p i (x| i ) = 1 x|i=1 ,(17)$The distributions (p b , p 1 , p 2 , . . .) then clearly satisfy Definition 5.2 and hence guarantee projective composition. In this case, the unconditional distribution used in the Bayes composition is similar to the background distribution if number of conditions is large. Intuitively, each conditional looks very similar to the Bernoulli background except for a single index that is guaranteed to be equal to 1, and the unconditional distribution is just a weighted sum of conditionals. Therefore, we expect the Bayes composition to be approximately projective.

More precisely, we will show that the unconditional distribution converges to the background in the limit as n → ∞, where n is both the data dimension and number of conditions, in the following sense:

$E x∼p b p u (x) -p b (x) p b (x) 2 → 0 as n → ∞.$We define the conditional and background distributions by:

$x ∈ R n , M i = {i} p b (x| i ) ∼ Bern q (x| i ), i.i.d. for i = 1, . . . , n p i (x| i ) = 1 x|i=1 , for all i = 1, . . . , n =⇒ p b (x) = q nnz(x) (1 -q) n-nnz(x) p i (x) = 1 x|i=1 p b (x| i c ) = 1 x|i=1 q nnz(x| i c ) (1 -q) n-1-nnz(x| i c )$We construct the unconditional distribution with assuming uniform probabibility over all labels: p u (x) := 1 n i p i (x). The number-of-nonzeros (nnz) in all of these distributions follow Binomial distributions:

$x ∼ p b =⇒ p b (nnz(x) = k) ∼ Binom(k; n, q) x ∼ p i =⇒ p i (nnz(x) = k) = p b (nnz(x| i c ) = k -1) ∼ Binom(k -1; n -1, q) if k > 0 else 0 x ∼ p u =⇒ p u (nnz(x) = k) = 1 n p i (nnz(x) = k) ∼ Binom(k -1; n -1, q) if k > 0 else 0$The basic intuition is that for large k and n, p b ∼ Binom(k; n, q) and p u ∼ Binom(k -1; n -1, q) are similar. More precisely, we can calculate:

$E x∼p b p u (x) -p b (x) p b (x) 2 = E x∼p b nnz(x) qn -1 2 , since B(k -1; n -1, q) B(k; n, q) = k qn = E k∼Binom(n,q) k qn -1 2 = 1 (nq) 2 E k∼Binom(n,q) (k -nq) 2 = 1 (nq) 2 Var(k), k ∼ Binom(n, q) = 1 (nq) 2 nq(1 -q) = 1 -q nq → 0 as n → ∞.$
## F. Factorized conditionals vs. orthogonal score differences

To see that Definition 5.2 implies orthogonality between the score differences, we note that

$v t i (x) := ∇ x log p t i (x t ) -∇ x log p t b (x t ) = ∇ x log p t i (x) p t b (x) = ∇ x log p t i (x| Mi )p t b (x| M c i x) p t b (x| Mi )p t b (x| M c i ) = ∇ x log p t i (x| Mi ) p t b (x| Mi ) =⇒ v t i (x)[k] = 0, ∀k / ∈ M i =⇒ v t i (x) T v t j (x) = 0, ∀i ̸ = j, since M i ∩ M j = ∅,$where in the second-to-last line we used the fact that the gradient of a function depending only on a subset of variables has zero entries in the coordinates outside that subset.

In fact, the same argument implies that {v t i (x) : x ∈ R n } ⊂ M i ; in other words, {v t i (x) : x ∈ R n } and {v t j (x) : x ∈ R n } occupy mutually-orthogonal subspaces. But even this latter condition does not imply the stronger condition of Definition 5.2. To find an equivalent definition in terms of scores we must also capture the independence of the subsets under p b . Specifically:

$   p t i (x) = p t i (x| Mi x)p t b (x| M c i x) p t b (x) = p t b (x| M x) i p t b (x| Mi ) ⇐⇒    ∇ x log p t i (x) = ∇ x log p t i (x| Mi x) + ∇ x log p t b (x| M c i x) ∇ x log p t b (x) = ∇ x log p t b (x| M x) + i ∇ x log p t b (x| Mi ) ⇐⇒        ∇ x log p t i (x) -∇ x log p t b (x) = ∇ x log p t i (x| Mi x) p t b (x| Mi x) ∇ x log p t b (x) = ∇ x log p t b (x| M x) + i ∇ x log p t b (x| Mi )$So an equivalent definition in terms of scores could be:

$Definition F.1. The distributions (p b , p 1 , p 2 , . . .) form factored conditionals if the score-deltas v t i := ∇ x log p t i (x) - ∇ x log p t b (x) satisfy {v t i (x) : x ∈ R n } ⊂ M i$, where the M i are mutually-orthogonal subsets, and furthermore the score of the background distribution decomposes over these subsets as follows:

$∇ x log p t b (x) = ∇ x log p t b (x| M x) + i ∇ x log p t b (x| Mi ).$(Note: this is actually equivalent to a slightly more general version of Definition 5.2 that allows for orthogonal transformations, which is the most general assumption under diffusion sampling generates a projective composition, per Lemmas 6.1 and 7.1.)

G. Proof of Theorem 5.3

Proof. (Theorem 5.3) For any set of distributions ⃗ q = (q b , q 1 , q 2 , . . .) satisfying Definition 5.2, we have

$C[⃗ q](x) := q b (x) i q i (x) q b (x) = q b (x) i q b (x t | M c i )q i (x| Mi ) q b (x| M c i )q b (x| Mi ) = q b (x) i q i (x| Mi ) q b (x| Mi ) = q b (x| M b ) i q i (x t | Mi )(18)$(where we used (7) in the second equality). Since (p b , p 1 , p 2 , . . .) satisfy Definition 5.2 by assumption, applying (18) gives

$C[⃗ p](x) = p b (x| M b ) i p i (x| Mi ) := p(x),$so the composition at t = 0 is projective, as desired.

Now to show that reverse-diffusion sampling with the compositional scores generates C[⃗ p], we need to show that C[ ⃗ p t ] = N t [C[⃗ p]], where p t := N t [p] denotes the t-noisy version of distribution p under the forward diffusion process. First, notice that if ⃗ p satisfies Definition 5.2, then ⃗ p t does as well. This is because the diffusion process adds Gaussian noise independently to each coordinate, and thus preserves independence between sets of coordinates. Therefore by (18), we have C[ ⃗ p t ](x) = p t b (x| M ) i p t i (x t | Mi ). Now we apply the same argument (that diffusion preserves independent sets of coordinates) once again, to see that C[ ⃗ p t ] = N t [C[⃗ p]], as desired.

H. Parameterization-Independent Compositions and Proof of Lemma 6.1

The proof of Lemma 6.1 relies on certain general fact about parametrization-independence of certain operators, which we develop here.

Suppose we have an operator that takes as input two probability distributions (p, q) over the same space X , and outputs a distribution over X . That is, F : ∆(X ) × ∆(X ) → ∆(X ). We can think of such operators as performing some kind of "composition" of p, q.

Certain operators are independent of parameterization, meaning for any reparameterization of the base space A : X → Y, we have

$F (p, q) = A -1 ♯(F (A♯p, A♯q))$or equivalently:

$F (A♯p, A♯q) = A♯F (p, q),$where ♯ is the pushforward:

$(A♯p)(z) := 1 |∇A| p(A -1 (z)).$This means that reparameterization commutes with the operator: it does not matter if we first reparameterize, then compose, or first compose, then reparamterize. A few examples:

1. The pointwise-geometric median, F (p, q)(x) := p(x)q(x), is independent of reparameterization:

2. Squaring a distribution, F (p, q)(x) := p(x) 2 , is NOT independent of reparameterization:

3. The "CFG composition" [(Ho & Salimans, 2022)](#b9), F (p, q)(x) := p(x) γ q(x) 1-γ , is independent of reparameterization:

We can analogously define parametrization-independence for operators on more than 2 distributions. Notably, given a tuple of distributions ⃗ p = (p b , p 1 , p 2 , . . . , p k ), our composition operator C of Definition 5.1, x) is independent of parameterization.

$C[⃗ p] ∝ p b (x) i pi(x) p b ($Lemma H.1 (Parametrization-independence of 1-homogeneous operators). If an operator F is 1-homogeneous, i.e. F (tp, tq, . . .) = tF (p, q, . . .) and operates pointwise, then it is independence of parametrization.

Proof.

$F (A♯p, A♯q, . . .)(z) = F (A♯p(z), A♯q(z), . . .), pointwise = F 1 |∇A| p(A -1 (z)), 1 |∇A| q(A -1 (z)), . . . = 1 |∇A| F p(A -1 (z)), q(A -1 (z)), . . . , 1-homogeneous = A♯F (p, q, . . .)(z)$Corollary H.2 (Parametrization-invariance of composition). The composition operator C given by Definition 5.1 is independent of parametrization.

Proof. The composition operator given by Definition 5.1 is 1-homogeneous:

$C(tp b , tp 1 , tp 2 , . . .)(x) = tp b (x) i tp i (x) tp b (x) = tp b (x) i p i (x) p b (x) = tC(p b , p 1 , p 2 , . . . )(x)$and so the result follows from Lemma H.1. Alternatively, a direct proof is:

$C(p b , p 1 , p 2 , . . .)(x) := p b (x) i p i (x) p b (x) C(A♯p b , A♯p 1 , A♯p 2 , . . .)(z) = (A♯p b )(z) i (A♯p i )(z) (A♯p b )(z) = 1 |∇A| p b (A -1 (z)) i p i (A -1 (z)) p b (A -1 (z)) = A♯C(p b , p 1 , p 2 , . . .)(z).$Theorem 6.1 follows from Corollary H.2:

Proof. (Theorem 6.1) Let (q b , q 1 , q 2 , . . . , q k ) := (A♯p b , A♯p 1 , . . . A♯p k ), for which Definition 5.2 holds by assumption.

Applying an intermediate result from the proof of Theorem 5.3 gives:

$C[⃗ q](z) := q b (z) i q i (z) q b (z) = q b (z| M ) i q i (z| Mi ).$By Corollary H.2, C is independent of parametrization, hence

$A♯p := A♯(C[⃗ p]) = C[ ⃗ A♯p] := C(⃗ q).$I. Proof of Lemma 7.1

Figure [12](#fig_8) shows a synthetic experiment illustrating the sampling guarantees of Lemma 7.1 in contrast to the lack-ofguarantees in the non-orthogonal case.

The proof of Lemma 7.1 relies on the fact that diffusion noising commutes with orthogonal transformation, i.e. A♯N t [q] = N t [A♯q] if A is orthogonal, since standard Gaussians are invariant under orthogonal transformation. Now we have to be careful with commuting operators. We know that composition is independent of parametrization, i.e. A♯C[⃗ p] = C[ ⃗ A♯p]. Diffusion noising N t commutes with orthogonal transformation, i.e. A♯N t [q] = N t [A♯q] if A is orthogonal, because a standard Gaussian multiplied by an orthonormal matrix Q remains a standard Gaussian: η ∼ N (0, I) =⇒ Qη ∼ N (0, QQ T ) = N (0, I) (this is false for non-orthogonal transforms, however). Therefore, in the orthogonal case, we can rewrite:

$A♯C[N t [p]] = A♯N t [C[p]],$which implies the desired result since A is invertible.  [13](#). We show samples from the individual conditional distributions p 0 , p 1 using DDIM, samples from the desired exact composition C[p b , p 0 , p 1 ] at t = 0 (obtained by sampling from A♯C[⃗ p] with DDIM and transforming by A -1 ), and samples from the composition C[p b , p 0 , p 1 ] using DDIM with exact scores. We take τ = 0.02, and set σ min = 0.02 in the diffusion schedule. In the top row we take a = 1 ("very non-orthogonal") as in the proof, and compare this to a = 0.3 ("mildly non-orthogonal") in the bottom row. With a = 1, as in the proof we see that DDIM barely samples two of the clusters. With a = 0.3, DDIM still slightly undersamples the "hard" clusters but the effect is much less pronounced.

there exists a (linear, but not orthogonal) A such that the distribution of z = Ax is axis-aligned (A♯p 0 )(z) ≈ 1 2 δ e0 (x) + 1 2 δ e2 (x), (A♯p 1 )(z) ≈ 1 2 δ e1 (x) + 1 2 δ e3 (x), and thus does satisfy Definition 5.2 at t = 0, which guarantees correct composition of p at t = 0 under Lemma 6.1. The correct composition should sample uniformly from {(1+a)e 0 +e 1 , e 0 +ae 2 +e 3 , ae 0 +e 2 +e 1 , (1+a)e 2 +e 3 }. What goes wrong is that as soon as we add Gaussian noise to the distribution p(x) at time t > 0 of the diffusion forward process, the relationship z = Ax breaks and so we are no longer guaranteed correct composition of p t (x). In fact, the distribution is still a GMM but places nearly all its weight on only two of the four clusters, namely: {(1 + a)e 0 + e 1 , (1 + a)e 2 + e 3 }.

Intuitively, let us focus on the mode ae 0 + e 1 of p 1 and consider how it interacts with the two modes e 0 , e 2 of p 0 , at some time t > 0 when we have isotropic Gaussians centered at each mode. Since ae 0 + e 1 is further away from e 2 (distance √ a 2 + 2) than it is from e 0 (distance √ a 2 -2a + 2), it is much less likely under N (e 2 , σ t ) than N (e 0 , σ t ), leading to a lower weight. This intuition is shown graphically in a 2D projection in Figure [13](#) (left).

For the detailed proof, we actually want to ensure that p has full support even at t = 0 so we add a little bit of noise to it, but choose the covariance such that z = Ax still holds at t = 0. 1 -a 0 0 0 1 0 0 0 0 1 -a 0 0 0 1

$    , 1 2 µ T 2 Σ -1 µ 2 + 1 2 (µ 1 + µ 2 ) T Σ -1 (µ 1 + µ 2 ) = exp(µ T 1 Σ -1 µ 2 )$Noting that

$Σt := σ 2 t I + τ 2 (A T A) -1 = σ 2 t I + τ 2     1 + a 2 a 0 0 a 1 0 0 0 0 1 + a 2 a 0 0 a 1     Σt -1 = 1 (a 2 + 2)$σ 2 t τ 2 + σ 4 t + τ 4     σ 2 t + τ 2 -aτ 2 0 0 -aτ 2 (a 2 + 1)τ 2 + σ 2 t 0 0 0 0 σ 2 t + τ 2 -aτ 2 0 0 -aτ 2 (a 2 + 1)τ 2 + σ 2 t     ,

![Figure 1: Composing diffusion models via score combination. Given two diffusion models, it is sometimes possible to sample in a way that composes content from one model (e.g. your dog) with style of another model (e.g. oil paintings). We aim to theoretically understand this empirical behavior. Figure generated via score composition with SDXL finetuned on the author's dog; details in Appendix C.]()

![Figure 2: Length-generalization, another capability of composition enabled by our framework. Diffusion models trained to generate a single object conditioned on location (left) can be composed at inference-time to generate images of multiple objects at specified locations (right). Notably, such images are strictly out-of-distribution for the individual models being composed. (Additional samples in Figure 9.)]()

![Figure 4: Distribution p is a projective composition of p 1 and p 2 w.r.t. projection functions (Π 1 , Π 2 ), because p has the same marginals as p 1 when both are post-processed by Π 1 , and analogously for p 2 .]()

![Figure 5: Composing yellow objects with objects of other colors. Yellow objects successfully compose with blue, cyan and magenta objects but not with brown, gray, green, or red objects. Per the histograms (left), in RGB-colorspace yellow has R, G distributed like the background (gray) while B has a distinct distribution peaked closer to zero. Taking M yellow ≈ {B}, Theorem 5.3 predicts that standard diffusion can sample from compositions of yellow with any color where the B channel is distributed like the background: namely, blue, cyan, magenta per the histograms. (Other colors may theoretically compose per Theorem 6.1, but be difficult to sample.) (Additional samples in Figure 10.)]()

![Bayes composition with cluttered distributions. In Figure 3c we replicate CLEVR experiments in Du et al. (2023); Liu et al. (]()

![Theorem 6.1 (Feature-space Composition). Given distributions ⃗ p := (p b , p 1 , p 2 , . . . p k ), suppose there exists a diffeomorphism A : R n → R n such that (A♯p b , A♯p 1 , . . . A♯p k ) satisfy Definition 5.2, with corresponding partition M i ⊆ [n]. Then, the composition p := C[⃗ p] satisfies:]()

![Figure 9: Additional non-cherrypicked samples for CLEVR experiment of Figure 2.]()

![Figure 10: Additional non-cherrypicked samples for CLEVR experiment of Figure 5. Top left grid shows conditional samples for each color. Top right grid shows compositions of red-colored objects (p 6 ) with objects of other colors (8 samples of each), which only succeeds for cyan-colored objects. Bottom grid shows compositions of yellow-colored objects (p 7 ) with objects of other colors (16 samples of each): these are additional samples of the exact experiment shown in Figure 5.]()

![Figure12: Synthetic composition experiment illustrating the sampling guarantees of Lemma 7.1 in contrast to the lackof-guarantees in the non-orthogonal case. We compare a coordinate-aligned case (which satisfies Definition 5.2 in the native space) (top), an orthogonal-transform case (middle) (which satisfies the assumptions of Lemma 7.1), and a nonorthogonal-transform case (bottom) (which satisfies the assumptions of Theorem 6.1 but not of Lemma 7.1). In the first two cases the correct composition can be sampled using either diffusion (DDIM) or Langevin dynamics (LD) at t = 0, while in the final case DDIM sampling is unsuccessful although LD at t = 0 still works. The distributions are 4-dimensional and we show 8 samples (rows) for each. We show samples from the individual conditional distributions p 0 , p 1 using DDIM, samples from the desired exact composition C[p b , p 0 , p 1 ] at t = 0 (obtained by sampling from A♯C[⃗ p] with DDIM and transforming by A -1 ), samples from the composition C[p b , p 0 , p 1 ] using DDIM with exact scores, and samples from the composition C[p b , p 0 , p 1 ] using Langevin dynamics (LD) with exact scores at time t = 0 in the diffusion schedule (σ min = 0.02). The noiseless distributions p 0 and p 1 are each 4-dimensional 2-cluster Gaussian mixtures with means as noted in the figure, equal weights, and standard deviation τ = 0.02. For example, in the non-orthogonal-transform case, p 0 has means [1, 0, 0, 0] and [0, 0, 1, 0], and p 1 has means [1, 1, 0, 0] and [0, 0, 1, 1], (which can be transformed to satisfy Definition 5.2 via a non-orthogonal linear transform).]()

![Figure 14: Composition experiments for the setting in the proof of Lemma 7.2. Left pane shows 8 samples (rows) of each distribution in the native 4d representation; right pane shows 1000 samples under the 2D projection used in Figure13. We show samples from the individual conditional distributions p 0 , p 1 using DDIM, samples from the desired exact composition C[p b , p 0 , p 1 ] at t = 0 (obtained by sampling from A♯C[⃗ p] with DDIM and transforming by A -1 ), and samples from the composition C[p b , p 0 , p 1 ] using DDIM with exact scores. We take τ = 0.02, and set σ min = 0.02 in the diffusion schedule.In the top row we take a = 1 ("very non-orthogonal") as in the proof, and compare this to a = 0.3 ("mildly non-orthogonal") in the bottom row. With a = 1, as in the proof we see that DDIM barely samples two of the clusters. With a = 0.3, DDIM still slightly undersamples the "hard" clusters but the effect is much less pronounced.]()

![ae 0 + e 1 , τ 2 (A T A) -1 ) + 1 2 N (x; ae 2 + e 3 , τ 2 (A T A) -1 ) p 0 b (x) = N (x; 0, τ 2 (A T A) -1 ), where A :=    ]()

![]()

The geometric mean p1(x)p2(x) is also often used; our discussion applies equally to this as well.

Although[Du et al. (2023)](#b5) use the Bayes composition to achieve a kind of length-generalization, our discussion shows that the Bayes justification does not explain the experimental results.

We use the term "projection" informally here, to convey intuition; these functions Πi are not necessarily coordinate projections, although this is an important special case (Section 5).

The notation ♯ refers to push-forward of a probability measure.

Our results are agnostic to the specific diffusion noiseschedule and scaling used.

https://github.com/facebookresearch/clevr-dataset-gen

