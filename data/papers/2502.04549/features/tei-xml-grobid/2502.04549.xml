<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mechanisms of Projective Composition of Diffusion Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-02-06">6 Feb 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Arwen</forename><surname>Bradley</surname></persName>
							<email>bradley@apple.com&gt;</email>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution 1 Apple</orgName>
								<address>
									<settlement>Cupertino</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution 1 Apple</orgName>
								<address>
									<settlement>Cupertino</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Preetum</forename><surname>Nakkiran</surname></persName>
							<email>nakkiran@apple.com&gt;.</email>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution 1 Apple</orgName>
								<address>
									<settlement>Cupertino</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Berthelot</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution 1 Apple</orgName>
								<address>
									<settlement>Cupertino</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">James</forename><surname>Thornton</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution 1 Apple</orgName>
								<address>
									<settlement>Cupertino</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Joshua</forename><forename type="middle">M</forename><surname>Susskind</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution 1 Apple</orgName>
								<address>
									<settlement>Cupertino</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Mechanisms of Projective Composition of Diffusion Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-02-06">6 Feb 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">96072C7AC7BA4B03ACA0571EB9FE54B1</idno>
					<idno type="arXiv">arXiv:2502.04549v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study the theoretical foundations of composition in diffusion models, with a particular focus on out-of-distribution extrapolation and lengthgeneralization. Prior work has shown that composing distributions via linear score combination can achieve promising results, including lengthgeneralization in some cases <ref type="bibr" target="#b5">(Du et al., 2023;</ref><ref type="bibr" target="#b20">Liu et al., 2022)</ref>. However, our theoretical understanding of how and why such compositions work remains incomplete. In fact, it is not even entirely clear what it means for composition to "work". This paper starts to address these fundamental gaps. We begin by precisely defining one possible desired result of composition, which we call projective composition. Then, we investigate: (1) when linear score combinations provably achieve projective composition, (2) whether reverse-diffusion sampling can generate the desired composition, and (3) the conditions under which composition fails. Finally, we connect our theoretical analysis to prior empirical observations where composition has either worked or failed, for reasons that were unclear at the time.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The possibility of composing different concepts represented by pretrained models has been of both theoretical and practical interest for some time <ref type="bibr" target="#b13">(Jacobs et al., 1991;</ref><ref type="bibr" target="#b8">Hinton, 2002;</ref><ref type="bibr">Du &amp; Kaelbling, 2024)</ref>, with diverse applications including image and video synthesis <ref type="bibr" target="#b5">(Du et al., 2023;</ref><ref type="bibr">2020;</ref><ref type="bibr" target="#b20">Liu et al., 2022;</ref><ref type="bibr">2021;</ref><ref type="bibr" target="#b24">Nie et al., 2021;</ref><ref type="bibr">Yang et al., 2023a;</ref><ref type="bibr" target="#b41">Wang et al., 2024)</ref>, planning <ref type="bibr" target="#b0">(Ajay et al., 2024;</ref><ref type="bibr" target="#b14">Janner et al., 2022)</ref>, constraint satisfaction <ref type="bibr">(Yang et al., 2023b)</ref>, parameter-efficient training <ref type="bibr" target="#b11">(Hu et al., 2021;</ref><ref type="bibr" target="#b12">Ilharco et al., 2022)</ref>, and many others <ref type="bibr" target="#b43">(Wu et al., 2024;</ref><ref type="bibr" target="#b38">Su et al., 2024;</ref><ref type="bibr" target="#b39">Urain et al., 2023;</ref><ref type="bibr">Anonymous, 2024)</ref>. One central goal in this field is to build Figure <ref type="figure">1</ref>: Composing diffusion models via score combination. Given two diffusion models, it is sometimes possible to sample in a way that composes content from one model (e.g. your dog) with style of another model (e.g. oil paintings). We aim to theoretically understand this empirical behavior.  novel compositions at inference time using only the outputs of pretrained models (either entirely separate models, or different conditionings of a single model), to create generations that are potentially more complex than any model could produce individually. As a concrete example to keep in mind, suppose we have two diffusion models, one trained on your personal photos of your dog and another trained on a collection of oil paintings, and we want to somehow combine these to generate oil paintings of your dog. Note that in order to achieve this goal, compositions must be able to generate images that are out-of-distribution (OOD) with respect to each of the individual models, since for example, there was no oil painting of your dog in either model's training set. Prior empirical work has shown that this ambitious vision is at least partially achievable in practice. However, the theoretical foundations of how and why composition works in practice, as well as its limitations, are still incomplete.</p><p>The goal of this work is to advance our theoretical understanding of composition-we will take a specific family of methods used for composing diffusion models, and we will analyze conditions under which this method provably generates the "correct" composition. Specifically, are there sufficient properties of the distributions we are composing that can guarantee that composition will work "correctly"? And what does correctness even mean, formally?</p><p>We focus our study on composing diffusion models by linearly combining their scores, a method introduced by <ref type="bibr" target="#b5">Du et al. (2023)</ref>; <ref type="bibr" target="#b20">Liu et al. (2022)</ref> (though many other interesting constructions are possible, see Section 2). Concretely, suppose we have three separate diffusion models, one for the distribution of dog images p dog , another for oil-paintings p oil , and another unconditional model for generic images p u . Then, we can use the individual score estimates ∇ x log p(x) given by the models to construct a composite score: ∇ x log p(x) :=</p><p>(1) ∇ x log p dog (x) + ∇ x log p oil (x) -∇ x log p u (x). This implicitly defines a distribution which we will call a "product composition": p(x) ∝ p dog (x)p oil (x)/p u (x). Finally, we can try to sample from p by using these scores with a generic score-based sampler, or even reverse-diffusion. This method of composition often achieves good results in practice, yielding e.g. oil paintings of dogs, but it is unclear why it works theoretically.</p><p>We are particularly interested in the OOD generalization capabilities of this style of composition. By this we mean the compositional method's ability to generate OOD with respect to each of the individual models being composedwhich may be possible even if none of the individual models are themselves capable of OOD generation. A specific desiderata is length-generalization, understood as the ability to compose arbitrarily many concepts. For example, consider the CLEVR <ref type="bibr" target="#b15">(Johnson et al., 2017)</ref> setting shown in Figure <ref type="figure" target="#fig_1">2</ref>. Given conditional models trained on images each containing a single object and conditioned on its location, we want to generate images containing k &gt; 1 objects composed in the same scene. How could such length-generalizing composition be possible? Here is one illustrative toy exampleconsider the following construction, inspired by but slightly different from <ref type="bibr" target="#b5">Du et al. (2023)</ref>; <ref type="bibr" target="#b20">Liu et al. (2022)</ref>. Suppose p b is a distribution of empty background images, and each p i a distribution of images with a single object at location i, on an otherwise empty background. Assume all locations we wish to compose are non-overlapping. Then, performing reversediffusion sampling using the following score-composition will work -meaning will produce images with k objects at appropriate locations:</p><formula xml:id="formula_0">∇ x log p t b (x) + k i=1 ∇ x log p t i (x) -∇ x log p t b (x) score delta δi ∈ R n . (2)</formula><p>Above, the notation p t i denotes the distribution p i after time t in the forward diffusion process (see Appendix D). Intuitively this works because during the reverse-diffusion process, the update performed by model i modifies only pixels in the vicinity of location i, and otherwise leaves them identical to the background. Thus the different models do not interact, and the sampler acts as if each model individually "pastes" an object onto an empty background. Formally, sampling works because the score delta vectors δ i are mutually orthogonal, and in fact have disjoint supports. Notably, we can sample from this composition with a standard diffusion sampler, in contrast to <ref type="bibr" target="#b5">Du et al. (2023)</ref>'s observations that more sophisticated samplers are necessary. This construction would not be guaranteed to work, however, if the "background" p b was chosen to be the unconditional distribution p u (as in Equation <ref type="formula">1</ref>), a common choice in many prior works <ref type="bibr" target="#b5">(Du et al., 2023;</ref><ref type="bibr" target="#b20">Liu et al., 2022)</ref>.</p><p>The remainder of this paper is devoted to trying to generalize this example as far as possible, and understand both its explanatory power and its limitations. It turns out the core mechanism can be generalized surprisingly far, and does not depend on "orthogonality" as strongly as the above example may suggest. We will encounter some subtle aspects along the way, starting from formal definitions of what it means for composition to succeed -a definition that can capture both composing objects (as in Figure <ref type="figure" target="#fig_1">2</ref>), and composing other attributes (such as style + content, in Figure <ref type="figure">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Contributions and Organization</head><p>In this work we introduce a theoretical framework to help understand the empirical success of certain methods of composing diffusion models, with emphasis on understanding how compositions can sometimes length-generalize. We start by discussing the limitations of several prior definitions of composition in Section 3. In Section 4 we offer a formal definition of "what we want composition to do", given precise information about which aspects we want to compose, which we call Projective Composition (Definition 4.1). (Note that there are many other valid notions of composition; we are merely formalizing one particular goal.) Then, we study how projective composition can be achieved. In Section 5 we introduce formal criteria called Factorized Conditionals (Definition 5.2), which is a type Figure <ref type="figure">3</ref>: Attempted compositional length-generalization up to 9 objects. We attempt to compose via linear score combination the distributions p 1 through p 9 shown on the far left, where each p i is conditioned on a specific object location as described below. Settings (A) and (C) approximately satisfy the conditions of our theory of projective composition, and thus are expected to length-generalize at least somewhat, while setting (B) does not even approximately satisfy our conditions and indeed fails to length-generalize. Experiment (A): In this experiment, the distributions p i each contain a single object at a fixed location, and the background p b is empty. In this case any successful composition of more than one object represents length-generalization. We find that composition succeeds up to several objects, but then degrades as number of objects increases (see Section 5.3 for details). Experiment (B): Here the distributions p i are identical to (A), but the background p b is chosen as the unconditional distribution (i.e. a single object at a random location)-this the "Bayes composition" (Section 3). This composition entirely fails-remarkably, trying to compose many objects often produces no objects! Experiment (C): Here each distribution p i contains an object at a fixed location i, and 0 -4 other objects (sampled uniformly) in random locations; see samples at far left. The background distribution p b is a distribution of 1 -5 objects (sampled uniformly) in random locations. In this case length-generalization means composition of more than 5 objects. This composition can length-generalize, but artifacts appear for large numbers of objects. See Section 5.3 for a full discussion.</p><p>of independence criteria along both distributions and coordinates. We prove that when this criteria holds, projective composition can be achieved by linearly combining scores (as in Equation <ref type="formula">2</ref>), and can be sampled via standard reversediffusion samplers. In Section 6 we show that parts of this result can be extended much further to apply even in nonlinear feature spaces; but interestingly, even when projective composition is achievable, it may be difficult to sample. We find that in many important cases existing constructions approximately satisfy our conditions, but the theory also helps characterize and explain certain limitations. Finally in Section 8 we discuss how our results can help explain existing experimental results in the literature where composition worked or failed, for reasons that were unclear at the time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Single vs. Multiple Model Composition. First, we distinguish the kind of composition we study in this paper from approaches that rely a single model but with OOD condition-ers; for example, passing OOD text prompts to text-to-image models <ref type="bibr" target="#b23">(Nichol et al., 2021;</ref><ref type="bibr" target="#b28">Podell et al., 2023)</ref>, or works like <ref type="bibr" target="#b25">Okawa et al. (2024)</ref>; <ref type="bibr" target="#b27">Park et al. (2024)</ref>. In contrast, we study compositions which recombine the outputs of multiple separate models at inference time, where each model only sees in-distribution conditionings. Among compositions involving multiple models, many different variants have been explored. Some are inspired by logical operators like AND and OR, which are typically implemented as product p 0 (x)p 1 (x) and sum p 0 (x)+p 1 (x) <ref type="bibr" target="#b5">(Du et al., 2023;</ref><ref type="bibr">Du &amp; Kaelbling, 2024;</ref><ref type="bibr" target="#b20">Liu et al., 2022)</ref>. Some composition methods are based on diffusion models, while others use energy-based models <ref type="bibr" target="#b4">(Du et al., 2020;</ref><ref type="bibr">2023;</ref><ref type="bibr" target="#b19">Liu et al., 2021)</ref> or densities <ref type="bibr" target="#b32">(Skreta et al., 2024)</ref>. In this work, we focus specifically on product-style compositions implemented with diffusion models via a linear combinations of scores as in <ref type="bibr" target="#b5">Du et al. (2023)</ref>; <ref type="bibr" target="#b20">Liu et al. (2022)</ref>. Our goal is not to propose a new method of composition but to improve theoretical understanding of existing methods.</p><p>Learning and Generalization. In this work we focus only on mathematical aspects of composition, and we do not consider any learning-theoretic aspects such as inductive bias or sample complexity. Our work is thus complementary to <ref type="bibr" target="#b16">Kamb &amp; Ganguli (2024)</ref>, which studies how a type of compositional generalization can arise from inductive bias in the learning procedure. Additional related works are discussed in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Prior Definitions of Composition</head><p>In this section we will describe why two popular mathematical definitions of composition are insufficient for our purposes: the "simple product" definition, and the Bayes composition. Specifically, neither of these notions can describe the outcome of the CLEVR length-generalization experiment from Figure <ref type="figure" target="#fig_1">2</ref>. Our observations here will thus motivate us to propose a new definition of composition, in the following section. As a running example, we will consider a subset of the CLEVR experiment from Figure <ref type="figure" target="#fig_1">2</ref>. Suppose we are trying to compose two distributions p 1 , p 2 of images each containing a single object in an otherwise empty scene, where the object is in the lower-left corner under p 1 , and the upper-right corner under p 2 . We would like the composed distribution p to place objects in at least the lower-left and upper-right, simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The Simple Product</head><p>The simple product is perhaps the most familiar type of composition: Given two distributions p 1 and p 2 over R n , the simple product is defined<ref type="foot" target="#foot_0">foot_0</ref> as p(x) ∝ p 1 (x)p 2 (x). The simple product can represent some interesting types of composition, but it has a key limitation: the composed distribution can never be truly "out-of-distribution" w.r.t. p 1 or p 2 , since p(x) = 0 whenever p 1 (x) = 0 or p 2 (x) = 0. This presents a problem for our CLEVR experiment. Using the simple product definition, we must have p(x) = 0 for any image x with two objects, since neither p 1 nor p 2 was supported on images with two objects. Therefore, the simple product definition cannot represent our desired composition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The Bayes Composition</head><p>Another candidate definition for composition, which we will call the "Bayes composition", was introduced and studied by <ref type="bibr" target="#b5">Du et al. (2023)</ref>; <ref type="bibr" target="#b20">Liu et al. (2022)</ref>. The Bayes composition is theoretically justified when the desired composed distribution is formally a conditional distribution of the model's training distribution. However, it is not formally capable of generating truly out-of-distribution samples, as our example below will illustrate.</p><p>Let us attempt to apply the Bayes composition methodology to our CLEVR example. We interpret our two distributions p 1 , p 2 as conditional distributions, conditioned on an object appearing in the lower-left or upper-right, respectively. Thus we write p(x|c 1 ) ≡ p 1 (x), where c 1 is the event that an object appears in the lower-left of image x, and c 2 the event an object appears in the upper-right. Now, since we want both objects simultaneously, we define the composition as p(x) := p(x|c 1 , c 2 ). Because the two events c 1 and c 2 are conditionally independent given x (since they are deterministic functions of x), we can compute p in terms of the individual conditionals:</p><formula xml:id="formula_1">p(x) := p(x|c 1 , c 2 ) ∝ p(x|c 1 )p(x|c 2 )/p(x).</formula><p>(3)</p><p>Equivalently in terms of scores: ∇ x log pt (x) := ∇ x log p(x|c 1 ) + ∇ x log p(x|c 2 ) -∇ x log p(x). Line (3) thus serves as our definition of the Bayes composition p, in terms of the conditional distributions p(x|c 1 ) and p(x|c 2 ), and the unconditional p(x).</p><p>The definition of composition above seems natural: we want both objects to appear simultaneously, so let us simply condition on both these events. However, there is an obvious error in the conclusion: p(x) must be 0 whenever p(x|c 1 ) or p(x|c 2 ) is zero (by Line 3). Since neither conditional distribution have support on images with two objects, the composition p cannot contain images of two objects either.</p><p>Where did this go wrong? The issue is: p(x|c 1 , c 2 ) is not well-defined in our case. We intuitively imagine some unconditional distribution p(x) which allows both objects simultaneously, but no such distribution has been defined, or encountered by the models during training. Thus, the definition of p in Line (3) does not actually correspond to our intuitive notion of "conditioning on both objects at once." More generally, this example illustrates how the Bayes composition cannot produce truly out-of-distribution samples, with respect to the distributions being composed. <ref type="foot" target="#foot_1">2</ref>Figure <ref type="figure">3b</ref> shows that the Bayes composition does not always work experimentally either: for diffusion models trained in a CLEVR setting similar to Figure <ref type="figure" target="#fig_1">2</ref>, the Bayes composition of k &gt; 1 locations typically fails to produce k objects (and sometimes produces zero). The difficulties discussed lead us to propose a precise definition of what we actually "want" composition to do in this case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Our Proposal: Projective-Composition</head><p>We now present our formal definition of what it means to "correctly compose" distributions. Our main insight here is, a realistic definition of composition should not purely be a function of distributions {p 1 , p 2 , . . . }, in the way the simple product p(x) = p 1 (x)p 2 (x) is purely a function of p 1 , p 2 . We must also somehow specify which aspects of each distribution we care about preserving in the composition. For example, informally, we may want a composition that mimics the style of p 1 and the content of p 2 . Our definition below of projective composition allows us this flexibility.</p><p>Roughly speaking, our definition requires specifying a "feature extractor" Π i : R n → R k associated with every distribution p i . These functions can be arbitrary, but we usually imagine them as projections<ref type="foot" target="#foot_2">foot_2</ref> in some feature-space, e.g, Π 1 (x) may be a transform of x which extracts only its style, and Π 2 (x) a transform which extracts only its content. Then, a projective composition is any distribution p which "looks like" distribution p i when both are viewed through Π i (see Figure <ref type="figure" target="#fig_2">4</ref>). Formally:</p><p>Definition 4.1 (Projective Composition). Given a collection of distributions {p i } along with associated "projection" functions</p><formula xml:id="formula_2">{Π i : R n → R k }, we call a distribution p a projective composition if 4 ∀i : Π i ♯p = Π i ♯p i .<label>(4)</label></formula><p>That is, when p is projected by each Π i , it yields marginals identical to those of p i .</p><p>There are a few aspects of this definition worth emphasizing, which are conceptually different from many prior notions of composition. First, our definition above does not construct a composed distribution; it merely specifies what properties the composition must have. For a given set of {(p i , Π i )}, there may be many possible distributions p which are projective compositions; or in other cases, a projective composition may not even exist. Separately, the definition of projective composition does not posit any sort of "true" underlying distribution, nor does it require that the distributions p i are conditionals of an underlying joint distribution. In particular, projective compositions can be truly "out of distribution" with respect to the p i : p can be supported on samples x where none of the p i are supported.</p><p>Examples. We have already discussed the style+content composition of Figure <ref type="figure">1</ref> as an instance of projective composition. Another even simpler example to keep in mind is the following coordinate-projection case. Suppose we take Π i : R n → R to be the projection onto the i-th coordinate. Then, a projective composition of distributions {p i } with these associated functions {Π i } means: a distribution where the first coordinate is marginally distributed identically to the first coordinate of p 1 , the second coordinate is marginally distributed as p 2 , and so on. (Note, we do not require any independence between coordinates). This notion of composition would be meaningful if, for example, we are already working in some disentangled feature space, where the first coordinate controls the style of the image the second coordinate controls the texture, and so on. The CLEVR length-generalization example from Figure <ref type="figure" target="#fig_1">2</ref> can also be described as a projective composition in almost an identical way, by letting Π i : R n → R k be a restriction onto the set of pixels neighboring location i. We describe this explicitly later in Section 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Simple Construction of Projective Compositions</head><p>It is not clear apriori that projective compositional distributions satisfying Definition 4.1 ever exist, much less that there is any straightforward way to sample from them. To explore this, we first restrict attention to perhaps the simplest setting, where the projection functions {Π i } are just coordinate restrictions. This setting is meant to generalize the intuition we had in the CLEVR example of Figure <ref type="figure" target="#fig_1">2</ref>, where different objects were composed in disjoint regions of the image. We first define the construction of the composed distribution, and then establish its theoretical properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Defining the Construction</head><p>Formally, suppose we have a set of distributions (p 1 , p 2 , . . . , p k ) that we wish to compose; in our running CLEVR example, each p i is the distribution of images with a single object at position i. Suppose also we have some reference distribution p b , which can be arbitrary, but should be thought of as a "common background" to the p i s. Then, one popular way to construct a composed distribution is via the compositional operator defined below. (A special case of this construction is used in <ref type="bibr" target="#b5">Du et al. (2023)</ref>, for example).</p><p>Definition 5.1 (Composition Operator). Define the composition operator C acting on an arbitrary set of distributions (p b , p 1 , p 2 , . . .) by</p><formula xml:id="formula_3">C[⃗ p] := C[p b , p 1 , p 2 , . . . ](x) := 1 Z p b (x) i p i (x) p b (x) , (<label>5</label></formula><formula xml:id="formula_4">)</formula><p>where Z is the appropriate normalization constant. We name C[⃗ p] the composed distribution, and the score of C[⃗ p] the compositional score:</p><formula xml:id="formula_5">∇ x log C[⃗ p](x) (6) = ∇ x log p b (x) + i (∇ x log p i (x) -∇ x log p b (x)) .</formula><p>Notice that if p b is taken to be the unconditional distribution then this is exactly the Bayes-composition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">When does the Composition Operator Work?</head><p>We can always apply the composition operator to any set of distributions, but when does this actually yield a "correct" composition (according to Definition 4.1)? One special case is when each distribution p i is "active" on a different, nonoverlapping set of coordinates. We formalize this property below as Factorized Conditionals (Definition 5.2). The idea is, each distribution p i must have a particular set of "mask" coordinates M i ⊆ [n] which it samples in a characteristic way, while independently sampling all other coordinates from a common background distribution. If a set of distributions (p b , p 1 , p 2 , . . .) has this Factorized Conditional structure, then the composition operator will produce a projective composition (as we will prove below).</p><p>Definition 5.2 (Factorized-Conditionals). We say a set of distributions (p b , p 1 , p 2 , . . . p k ) over R n are Factorized Conditionals if there exists a partition of coordinates</p><formula xml:id="formula_6">[n] into disjoint subsets M b , M 1 , . . . M k such that: 1. (x| Mi , x| M c i ) are independent under p i . 2. (x| M b , x| M1 , x| M2 , . . . , x| M k ) are mutually indepen- dent under p b . 3. p i (x| M c i ) = p b (x| M c i ).</formula><p>Equivalently, if we have:</p><formula xml:id="formula_7">p i (x) = p i (x| Mi )p b (x| M c i ), and<label>(7)</label></formula><formula xml:id="formula_8">p b (x) = p b (x| M b ) i∈[k] p b (x| Mi ).</formula><p>Equation ( <ref type="formula" target="#formula_7">7</ref>) means that each p i can be sampled by first sampling x ∼ p b , and then overwriting the coordinates of M i according to some other distribution (which can be specific to distribution i). For instance, the experiment of Figure <ref type="figure" target="#fig_1">2</ref> intuitively satisfies this property, since each of the conditional distributions could essentially be sampled by first sampling an empty background image (p b ), then "pasting" a random object in the appropriate location (corresponding to pixels M i ). If a set of distributions obey this Factorized Conditional structure, then we can prove that the composition operator C yields a correct projective composition, and reverse-diffusion correctly samples from it. Below, let N t denote the noise operator of the diffusion process<ref type="foot" target="#foot_4">foot_4</ref> at time t.</p><p>Theorem 5.3 (Correctness of Composition). Suppose a set of distributions (p b , p 1 , p 2 , . . . p k ) satisfy Definition 5.2, with corresponding masks {M i } i . Consider running the reverse-diffusion SDE using the following compositional scores at each time t:</p><formula xml:id="formula_9">s t (x t ) := ∇ x log C[p t b , p t 1 , p t 2 , . . .](x t ),<label>(8)</label></formula><p>where p t i := N t [p i ] are the noisy distributions. Then, the distribution of the generated sample x 0 at time t = 0 is:</p><formula xml:id="formula_10">p(x) := p b (x| M b ) i p i (x| Mi ).<label>(9)</label></formula><p>In particular, p(x| Mi ) = p i (x| Mi ) for all i, and so p is a projective composition with respect to projections {Π i (x) := x| Mi } i , per Definition 4.1.</p><p>Unpacking this, Line 9 says that the final generated distribution p(x) can be sampled by first sampling the coordinates M b according to p b (marginally), then independently sampling coordinates M i according to p i (marginally) for each i. Similarly, by assumption, p i (x) can be sampled by first sampling the coordinates M i in some specific way, and then independently sampling the remaining coordinates according to p b . Therefore Theorem 5.3 says that p(x) samples the coordinates M i exactly as they would be sampled by p i , for each i we wish to compose.</p><p>Proof. (Sketch) Since ⃗ p satisfies Definition 5.2, we have</p><formula xml:id="formula_11">C[⃗ p](x) := p b (x) i pi(x) p b (x) = p b (x) i p b (xt| M c i )pi(x|M i ) p b (x| M c i )p b (x|M i ) = p b (x) i pi(x|M i ) p b (x|M i ) = p b (x|M b ) i pi(xt|M i ) := p(x).</formula><p>The sampling guarantee follows from the commutativity of composition with the diffusion noising process, i.e. C[</p><formula xml:id="formula_12">⃗ p t ] = Nt[C[⃗ p]]. The complete proof is in Appendix G.</formula><p>Remark 5.4. In fact, Theorem 5.3 still holds under any orthogonal transformation of the variables, because the diffusion noise process commutes with orthogonal transforms. We formalize this as Lemma 7.1. Remark 5.5. Compositionality is often thought of in terms of orthogonality between scores. Definition 5.2 implies orthogonality between the score differences that appear in the composed score (6): ∇ x log p t i (x t ) -∇ x log p t b (x t ), but the former condition is strictly stronger (c.f. Appendix F). Remark 5.6. Notice that the composition operator C can be applied to a set of Factorized Conditional distributions without knowing the coordinate partition {M i }. That is, we can compose distributions and compute scores without knowing apriori exactly "how" these distributions are supposed to compose (i.e. which coordinates p i is active on). This is already somewhat remarkable, and we will see a much stronger version of this property in the next section.</p><p>Importance of background. Our derivations highlight the crucial role of the background distribution p b for the composition operator (Definition 5.1). While prior works have taken p b to be an unconditional distribution and the p i 's its associated conditionals, our results suggest this is not always the optimal choice -in particular, it may not satisfy a Factorized Conditional structure (Definition 5.2). Figure <ref type="figure">3</ref> demonstrates this empirically: settings (a) and (b) attempt to compose the same distributions using different backgrounds -empty (a) or unconditional (b) -with very different results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Approximate Factorized Conditionals in CLEVR.</head><p>In Figure <ref type="figure">3</ref> we explore compositional length-generalization (or lack thereof) in three different setting, two of which (Figure <ref type="figure">3a</ref> and <ref type="figure">3c</ref>) approximately satisfy Definition 5.2. In this section we explicitly describe how our definition of Factorized Conditionals approximately captures the CLEVR settings of Figures <ref type="figure">3a</ref> and <ref type="figure">3c</ref>. The setting of 3b does not satisfy our conditions, as discussed in Section 3.</p><p>Single object distributions with empty background. This is the setting of both Figure <ref type="figure" target="#fig_1">2</ref> and Figure <ref type="figure">3a</ref>. The background distribution p b over n pixels is images of an empty scene with no objects. For each i ∈ {1, . . . , L} (where L = 4 in Figure <ref type="figure" target="#fig_1">2</ref> and L = 9 in Figure <ref type="figure">3a</ref>), define the set M i ⊂ [n] as the set of pixel indices surrounding location i. (M i should be thought of as a "mask" that that masks out objects at location i). Let M b := (∪ i M i ) c be the remaining pixels in the image. Then, we claim the distributions (p b , p 1 , . . . , p L ) form approximately Factorized Conditionals, with corresponding coordinate partition {M i }. This is essentially because each distribution p i matches the background p b on all pixels except those surrounding location i (further detail in Appendix B.2). Note, however, that the conditions of Definition 5.2 do not exactly hold in the experiment of Figure <ref type="figure" target="#fig_1">2</ref> -there is still some dependence between the masks M i , since objects can cast shadows or even occlude each other. Empirically, these deviations have greater impact when composing many objects, as seen in Figure <ref type="figure">3a</ref>.  <ref type="formula">2022</ref>) where the images contain many objects (1-5) and the conditions label the location of one randomlychosen object. It turns out the unconditional together with the conditionals can approximately act as Factorized Conditionals in "cluttered" settings like this one. The intuition is that if the conditional distributions each contain one specific object plus many independently sampled random objects ("clutter"), then the unconditional distribution almost looks like independently sampled random objects, which together with the conditionals would satisfy Definition 5.2 (further discussion in Appendix B.2 and E). This helps to explain the length-generalization observed in <ref type="bibr" target="#b20">Liu et al. (2022)</ref> and verified in our experiments (Figure <ref type="figure">3c</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Projective Composition in Feature Space</head><p>So far we have focused on the setting where the projection functions Π i are simply projections onto coordinate subsets M i in the native space (e.g. pixel space). This covers simple examples like Figure <ref type="figure" target="#fig_1">2</ref> but does not include more realistic situations such as Figure <ref type="figure">1</ref>, where the properties to be composed are more abstract. For example a property like "oil painting" does not correspond to projection onto a specific subset of pixels in an image. However, we may hope that there exists some conceptual feature space in which "oil painting" does correspond to a particular subset of variables. In this section, we extend our results to the case where Figure <ref type="figure">6</ref>: A commutative diagram illustrating Theorem 6.1. Performing composition in pixel space is equivalent to encoding into a feature space (A), composing there, and decoding back to pixel space (A -1 ).</p><p>the composition occurs in some conceptual feature space, and each distribution to be composed corresponds to some particular subset of features.</p><p>Our main result is a featurized analogue of Theorem 5.3: if there exists any invertible transform A mapping into a feature space where Definition 5.2 holds, then the composition operator (Definition 5.1) yields a projective composition in this feature space, as shown in Figure <ref type="figure">6</ref>. </p><formula xml:id="formula_13">A♯p(z) ≡ (A♯p b (z))| M b k i=1 (A♯p i (z))| Mi . (10) Therefore, p is a projective composition of ⃗ p w.r.t. projection functions {Π i (x) := A(x)| Mi }.</formula><p>This theorem is remarkable because it means we can compose distributions (p b , p 1 , p 2 , . . . ) in the base space, and this composition will "work correctly" in the feature space automatically (Equation <ref type="formula">10</ref>), without us ever needing to compute or even know the feature transform A explicitly. Theorem 6.1 may apriori seem too strong to be true, since it somehow holds for all feature spaces A simultaneously. The key observation underlying Theorem 6.1 is that the composition operator C behaves well under reparameterization. Lemma 6.2 (Reparameterization Equivariance). The composition operator of Definition 5.1 is reparameterizationequivariant. That is, for all diffeomorphisms A : R n → R n and all tuples of distributions ⃗ p = (p b , p 1 , p 2 , . . . , p k ),</p><formula xml:id="formula_14">C[A♯⃗ p] = A♯C[⃗ p]. (<label>11</label></formula><formula xml:id="formula_15">)</formula><p>This lemma is potentially of independent interest: equivariance distinguishes the composition operator from many other common operators (e.g. the simple product). Lemma 6.2 and Theorem 6.1 are proved in Appendix H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Sampling from Compositions.</head><p>The feature-space Theorem 6.1 is weaker than Theorem 5.3 in one important way: it does not provide a sampling algorithm. That is, Theorem 6.1 guarantees that p := C[⃗ p] is a projective composition, but does not guarantee that reverse-diffusion is a valid sampling method.</p><p>There is one special case where diffusion sampling is guaranteed to work, namely, for orthogonal transforms (which can seen as a straightforward extension of the coordinatealigned case of Theorem 5.3):</p><p>Lemma 7.1 (Orthogonal transform enables diffusion sampling). If the assumptions of Lemma 6.1 hold for A(x) = Ax, where A is an orthogonal matrix, then running a reverse diffusion sampler with scores</p><formula xml:id="formula_16">s t = ∇ x log C[⃗ p t ] generates the composed distribution p = C[⃗ p] satisfying (10).</formula><p>The proof is given in Appendix I.</p><p>However, for general invertible transforms, we have no such sampling guarantees. Part of this is inherent: in the featurespace setting, the diffusion noise operator N t no longer commutes with the composition operator C in general, so scores of the noisy composed distribution N t [C[⃗ p]] cannot be computed from scores of the noisy base distributions N t [⃗ p].</p><p>Nevertheless, one may hope to sample from the distribution p using other samplers besides diffusion, such as annealed Langevin Dynamics or Predictor-Corrector methods <ref type="bibr" target="#b36">(Song et al., 2020)</ref>. We find that the situation is surprisingly subtle: composition C produces distributions which are in some cases easy to sample (e.g. with diffusion), yet in other cases apparently hard to sample. For example, in the setting of Figure <ref type="figure" target="#fig_3">5</ref>, our Theorem 6.1 implies that all pairs of colors should compose equally well at time t = 0, since there exist diffeomorphisms (indeed, linear transforms) between different colors. However, as we saw, the diffusion sampler fails to sample from compositions of non-orthogonal colors-and empirically, even more sophisticated samplers such as Predictor-Correctors also fail in this setting. At first glance, it may seem odd that composed distributions are so hard to sample, when their constituent distributions are relatively easy to sample. One possible reason for this below is that the composition operator has extremely poor Lipchitz constant, so it is possible for a set of distributions ⃗ p to "vary smoothly" (e.g. diffusing over time) while their Figure <ref type="figure">7</ref>: Composing Entangled Concepts. The left image composes the text-conditions "photo of a dog" with "photo of a horse", which both control the subject of the image, and produces unexpected results. In contrast, the right image composes "photo of a dog" with "photo, with red hat," which intuitively correspond to disentangled features. Both samples from SDXL using score-composition with an unconditional background; details in Appendix C.</p><p>composition C[⃗ p] changes abruptly. We formalize this in Lemma 7.2 (further discussion and proof in Appendix J).</p><p>Lemma 7.2 (Composition Non-Smoothness). For any set of distributions {p b , p 1 , p 2 , . . . , p k }, and any noise scale t := σ, define the noisy distributions p t i := N t [p i ], and let q t denote the composed distribution at time t: q</p><formula xml:id="formula_17">t := C[⃗ p t ].</formula><p>Then, for any choice of τ &gt; 0, there exist distributions {p b , p 1 , . . . p k } over R n such that 1. For all i, the annealing path of p i is O(1)-Lipshitz: ∀t, t ′ : W 2 (p t i , p t ′ i ) ≤ O(1)|t -t ′ |. 2. The annealing path of q has Lipshitz constant at least Ω(τ -1 ): ∃t, t ′ : W 2 (q t , q t ′ ) ≥ |t-t ′ | 2τ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Connections with Prior Observations</head><p>We have presented a mathematical theory of composition.</p><p>Although this theoretical model is a simplification of reality (we do not claim its assumptions hold exactly in practice) we believe the spirit of our results carries over to practical settings, and can help understand empirical observations from prior work. We now discuss some of these connections.</p><p>Independence Assumptions and Disentangled Features.</p><p>Our theory relies on a type of independence between distributions, related to orthogonality between scores, which we formalize as Factorized Conditionals. While such conditional structure typically does not exist in pixel-space, it is plausible that a factorized structure exists in an appropriate feature space, as permitted by our theory (Section 6). In particular, a feature space and distribution with perfectly "disentangled" features would satisfy our assumptions. Conversely, if distributions are not appropriately disentangled, our theory predicts that linear score combinations will fail to compose correctly. This effect is well-known; see Figure <ref type="figure">7</ref> for an example; similar failure cases are highlighted in <ref type="bibr" target="#b20">Liu et al. (2022)</ref> as well (such as "A bird" failing to compose with "A flower"). Regarding successful cases, style and content compositions consistently work well in practice, and are often taken to be disentangled features (e.g. Karras (2019); <ref type="bibr" target="#b6">Gatys et al. (2016)</ref>; <ref type="bibr" target="#b48">Zhu et al. (2017)</ref>). Finally, similar in spirit to our theory, methods for successful composition in practice such as LoRA task arithmetic <ref type="bibr">(Zhang et al., 2023a;</ref><ref type="bibr" target="#b12">Ilharco et al., 2022)</ref>, typically require some type of approximate "concept-space orthogonality".</p><p>Text conditioning with location information. Conditioning on location is a straightforward way to achieve factorized conditionals (provided the objects in different locations are approximately independent) since the required disjointness already holds in pixel-space. Many successful text-to-image compositions in Liu et al. ( <ref type="formula">2022</ref>) use location information in the prompts, either explicitly (e.g. "A blue bird on a tree" + "A red car behind the tree") or implicitly ("A horse" + "A yellow flower field"; since horses are typically in the foreground and fields in the background).</p><p>Unconditional Backgrounds. Most prior works on diffusion composition use the Bayes composition, with substantial practical success. As discussed in Section 5.3, Bayes composition may be approximately projective in "cluttered" settings, helping to explain its practical success in text-toimage settings, where images often contain many different possible objects and concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Conclusion</head><p>In this work, we have developed a theory of one possible mechanism of composition in diffusion models. We study how composition can be defined, and sufficient conditions for it to be achieved. Our theory can help understand a range of diverse compositional phenomena in both synthetic and practical settings, and we hope it will inspire further work on foundations of composition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional Related Works</head><p>Structured compositional generative models. Structured generative models leverage architectural inductive biases in an encoder-decoder framework, such as recurrent attention mechanisms <ref type="bibr" target="#b7">(Gregor et al., 2015)</ref> or slot-attention <ref type="bibr" target="#b40">(Wang et al., 2023)</ref>. These models decompose scenes into background and parts-based representations in an unsupervised manner guided by modeling priors. While these approaches can flexibly generate scenes with single or multiple objects, they are not explicitly controllable, and require specific model pre-training on datasets containing compositions of interest.</p><p>Controllable generation. Composition at inference-time is one potential mechanism for exerting control over the generation process. Another way to modify compositions of style and/or content attributes is through spatial conditioning a pre-trained diffusion model on a structural attribute (e.g., pose or depth) as in <ref type="bibr">Zhang et al. (2023b)</ref>, or on multiple attributes of style and/or content as in <ref type="bibr" target="#b37">Stracke et al. (2024)</ref>. Another option is control through resampling, as in <ref type="bibr" target="#b21">Liu et al. (2024)</ref>. These methods are complementary to single or multiple model conditioning mechanisms based on score composition that we study in the current work.</p><p>Single model conditioning. We distinguish the kind of composition we study in this paper from approaches that rely on a single model but use OOD conditioners to achieve novel combinations of concepts never seen together during training; for example, passing OOD text prompts to text-to-image models <ref type="bibr" target="#b23">(Nichol et al., 2021;</ref><ref type="bibr" target="#b28">Podell et al., 2023)</ref>, or works like <ref type="bibr" target="#b25">Okawa et al. (2024)</ref>; <ref type="bibr" target="#b27">Park et al. (2024)</ref> where a single model conditions simultaneously on multiple attributes like shape and color, with some combinations held out during training. In contrast, the compositions we study recombine the outputs of multiple separate models at inference time. Though less powerful, this can still be surprisingly effective, and is more amenable to theoretical study since it disentangles the potential role of conditional embeddings.</p><p>Multiple model composition. Among compositions involving multiple separate models, many different variants have been explored with different goals and applications. Some definitions of composition are inspired by logical operators like AND and OR, usually taken to mean that the composed distribution should have high probability under all of the conditional distributions to be composed, or at least one of them, respectively. Given two conditional probabilities p 0 (x), p 1 (x), AND is typically implemented as the product p 0 (x)p 1 (x) and OR as sum p 0 (x) + p 1 (x) (though these only loosely correspond to the logical operators and other implementations are also possible). Some composition methods are based on diffusion models and use the learned scores (mainly for product compositions), others use energy-based models (which allows for OR-inspired sum compositions, as well as more sophisticated samplers, in particular sampling at t = 0 <ref type="bibr" target="#b4">(Du et al., 2020;</ref><ref type="bibr">2023;</ref><ref type="bibr" target="#b19">Liu et al., 2021)</ref>, and still others work directly with the densities <ref type="bibr" target="#b32">(Skreta et al., 2024)</ref> (enabling an even greater variety of compositions, including a different style of AND, taken to mean p 0 (x) = p 1 (x)). <ref type="bibr" target="#b22">McAllister et al. (2025)</ref> explore another type of OR composition. <ref type="bibr" target="#b42">(Wiedemer et al., 2024</ref>) take a different approach of taking the final rendered images generated by separate diffusion models and "adding them up" in pixel-space, as part of a study on generalization of data-generating processes. Task-arithmetic <ref type="bibr">(Zhang et al., 2023a;</ref><ref type="bibr" target="#b12">Ilharco et al., 2022)</ref>, often using LoRAs <ref type="bibr" target="#b11">(Hu et al., 2021)</ref>, is a kind of composition in weight-space that has had significant practical impact.</p><p>Product compositions. In this work, we focus specifically on product compositions (broadly defined to allow for a "background" distribution, i.e. compositions of the form p(x) = p b (x) i pi(x) p b (x) ) implemented with diffusion models, which allows the composition to be implemented via a linear combinations of scores as in <ref type="bibr" target="#b5">Du et al. (2023)</ref>; <ref type="bibr" target="#b20">Liu et al. (2022)</ref>. Our goal is not to propose a wholly new method of composition but rather to improve theoretical understanding of existing methods.</p><p>Learning and Generalization. Recently, <ref type="bibr" target="#b16">Kamb &amp; Ganguli (2024)</ref> demonstrated how a type of compositional generalization arises from inductive bias in the learning procedure (equivariance and locality). Their findings are relevant to our broader motivation, but complementary to the focus of this work. Specifically, we focus only on mathematical aspects of defining and sampling from compositional distributions, and we do not consider any learning-theoretic aspects such as inductive bias or sample complexity. This allows us to study the behavior of compositional sampling methods even assuming perfect knowledge of the underlying distributions.</p><p>1. In each distribution p i , the pixels inside the mask M i are approximately independent from the pixels outside the mask, since the outside pixels always describe an empty scene.</p><p>2. In the background p b , the set of masks {M i } specify approximately mutually-independent sets of pixels, since all pixels are roughly constant.</p><p>3. The distribution of p i and p b approximately agree along all pixels outside mask M i , since they both describe an empty scene outside this mask.</p><p>Thus, the set of distributions approximately form Factorized Conditionals. However the conditions of Definition 5.2 do not exactly hold, since objects can cast shadows on each other and may even occlude each other. Empirically, this can significantly affect the results when composing many objects, as explored in Figure <ref type="figure">3</ref>(a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.2. CLUTTERED DISTRIBUTED WITH UNCONDITIONAL BACKGROUND</head><p>Figure <ref type="figure">8</ref>: Samples from unconditional model trained on images containing 1-5 objects. The sampled images sometimes contain 6 objects (circled in orange).</p><p>Next, we discuss the setting of Figure <ref type="figure">3c</ref>, which is a Bayes composition based on an unconditional distribution where each scene contains 1-5 objects (with the number of objects sampled uniformly). The locations and all other attributes of the objects are sampled independently. The conditions label the location of one randomly-chosen object. Just as in the previous case, for each i ∈ {1, 2, . . . , L} (L = 9 in Figure <ref type="figure">3c</ref>), we define the set M i ∈ [n] as the set of pixel indices surrounding location i, and let M b := (∪ i M i ) c be the remaining pixels in the image, excluding all the masks. Again, we claim that the distributions (p b , p 1 , . . . , p L ) are approximately Factorized Conditionals, with corresponding coordinate partition (M b , M 1 , . . . , M L ). We examine the criteria in Definition 5.2:</p><p>1. In each distribution p i , the pixels inside the mask M i are approximately independent from the pixels outside the mask, since the outside pixels approximately describe a distribution containing 0-4 objects, and the locations and other attributes of all objects are independent.</p><p>2. In the unconditional background distribution p b , we argue that in practice, the set of masks {M i } are approximately mutually-independent. By assumption, the locations and other attributes of all shapes are all independent, and the masks M i are chosen in these experiment to minimize interaction/overlap. The main difficulty is the restriction to 1-5 total objects, which we discuss below.</p><p>3. The distribution of p i and p b approximately agree along all pixels outside mask M i , since</p><formula xml:id="formula_18">p i | M c i contains 0-4 objects, while p b | M c i contains 0-5 objects (since one object could be 'hidden' in M c i ).</formula><p>There are, however, two important caveats to the points above. First, overlap or other interaction (shadows, etc.) between objects can clearly violate all three criteria. In our experiment, this is mitigated by the fact that the masks M i are chosen to minimize interaction/overlap (though interactions start to occur as we compose more objects, leading to some image degradation). Second, since the number of objects is sampled uniformly from 1-5, the presence of one object affects the probability that another will be present. Thus, the masks {M i } are not perfectly independent under the background distribution, nor do p i and p b perfectly agree on M c i . Ideally, each p i would place an object in mask M i and independently follow p b on M c i , and p b would be such that the probability that an object appears in mask M i is independently Bernoulli (c.f. Appendix E.2). In particular, this would imply that the distribution of the total number of objects is Binomial (which allows the total object-count to range from zero to the total-number-of-locations, as well as placing specific probabilities on each object-count), which clearly differs from the uniform distribution over 1-5 objects. However, a few factors mitigate this discrepancy:</p><p>• A Binomial with sufficiently small probability-of-success places very little probability on large k. For example, under Binomial(9, 0.3), P(k = 0 : 5) = 0.04, 0.156, 0.27, 0.27, 0.17, 0.07 and P(k &gt; 5) = 0.026.</p><p>• Empirically, the learned unconditional distribution does not actually enforce k &lt; 5; we sometimes see samples with k = 6 for example, as seen in Figure <ref type="figure">8</ref>.</p><p>Intuitively, the train distribution is "close to Bernoulli" and the learned distribution seems to be even closer.</p><p>With these considerations in mind, we see that the set of distributions approximately -though imperfectly -form Factorized Conditionals. One advantage of this setting compared to the single-object setting is that the models can learn how multiple objects should interact and even overlap correctly, potentially making it easier compose nearby locations. We explore the length-generalization of this composition empirically in Figure <ref type="figure">3c</ref> (note, however, that only compositions of more than 5 objects are actually OOD w.r.t. the distributions p i in this case).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Additional CLEVR samples</head><p>In this section we provide additional non-cherrypicked samples of the experiments shown in the main text.  distributions p i that set a single index i to one and all other indices to zero, a zero-background distribution p b , and an unconditional distribution formed from the conditionals by assuming p(c = i) is uniform. That is:</p><formula xml:id="formula_19">p t i (x t ) = N (x t ; e i , σ 2 t ) ∝ exp - ∥x t -e i ∥ 2 2σ 2 t p t b (x t ) = N (x t ; 0, σ 2 t ) ∝ exp - ∥x t ∥ 2 2σ 2 t p t u (x t ) = 1 n n i=1 p i (x t )<label>(16)</label></formula><p>Suppose we want to compose all n distributions p i , that is, we want to activate all indices. It is enough to consider x t of the special form x t = (α, . . . , α) since there is no reason to favor any condition over any another. Making this restriction,</p><formula xml:id="formula_20">x t = (α, . . . , α) =⇒ p t i (x t ) ∝ exp - (n -1)α 2 + (1 -α) 2 2σ 2 t = exp - nα 2 -2α + 1 2σ 2 t , ∀i p t u (x t ) = exp - nα 2 -2α + 1 2σ 2 t p t b (x t ) ∝ exp - nα 2 2σ 2 t</formula><p>Let us find the value of α that maximizes the probability under the Bayes composition of all condition:</p><formula xml:id="formula_21">x t = (α, . . . , α) =⇒ p t i (x t ) p t u (x t ) = 1 =⇒ p t u (x t ) n i=1 p t i (x t ) p t u (x t ) ∝ p t u (x t ) ∝ exp - nα 2 -2α + 1 2σ 2 t = exp - n(α -1 n ) 2 + const 2σ 2 t =⇒ α ⋆ = 1 n ,</formula><p>so the optimum is α ⋆ = 1 n . That is, under the Bayes composition the most likely configuration places value 1 n at each index we wished to activate, rather than the desired value 1.</p><p>On the other hand, if we instead use p b in the linear score combination and optimize, we find that:</p><formula xml:id="formula_22">x t = (α, . . . , α) =⇒ =⇒ p t i (x t ) p t b (x t ) ∝ exp - 1 -2α 2σ 2 t =⇒ p t b (x t ) n i=1 p t i (x t ) p t b (x t ) ∝ exp - nα 2 2σ 2 t exp - n(1 -2α) 2σ 2 t ∝ exp - n(α 2 -2α + 1) 2σ 2 t ∝ exp - n(α -1) 2 2σ 2 t =⇒ α ⋆ = 1 so the optimum is α ⋆ = 1.</formula><p>That is, the most likely configuration places the desired value 1 at each index we wished to activate, achieving projective composition, and in particular, length-generalizing correctly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2. Cluttered Distributions</head><p>In certain "cluttered" settings, the Bayes composition may be approximately projective. We explore this in the following simplified setting, corresponding to the experiment in Figure <ref type="figure">11</ref> (right). Suppose that x is binary-valued, M i = {i}, ∀i, the x i are independently Bernoulli with parameter q under the background, and the projected conditional distribution p i (x| i ) just guarantees that x i = 1:</p><formula xml:id="formula_23">p b (x| i c ) ∼ Bern q (x| i c ), i.i.d. ∀i, p i (x| i ) = 1 x|i=1 ,<label>(17)</label></formula><p>The distributions (p b , p 1 , p 2 , . . .) then clearly satisfy Definition 5.2 and hence guarantee projective composition. In this case, the unconditional distribution used in the Bayes composition is similar to the background distribution if number of conditions is large. Intuitively, each conditional looks very similar to the Bernoulli background except for a single index that is guaranteed to be equal to 1, and the unconditional distribution is just a weighted sum of conditionals. Therefore, we expect the Bayes composition to be approximately projective.</p><p>More precisely, we will show that the unconditional distribution converges to the background in the limit as n → ∞, where n is both the data dimension and number of conditions, in the following sense:</p><formula xml:id="formula_24">E x∼p b p u (x) -p b (x) p b (x) 2 → 0 as n → ∞.</formula><p>We define the conditional and background distributions by:</p><formula xml:id="formula_25">x ∈ R n , M i = {i} p b (x| i ) ∼ Bern q (x| i ), i.i.d. for i = 1, . . . , n p i (x| i ) = 1 x|i=1 , for all i = 1, . . . , n =⇒ p b (x) = q nnz(x) (1 -q) n-nnz(x) p i (x) = 1 x|i=1 p b (x| i c ) = 1 x|i=1 q nnz(x| i c ) (1 -q) n-1-nnz(x| i c )</formula><p>We construct the unconditional distribution with assuming uniform probabibility over all labels: p u (x) := 1 n i p i (x). The number-of-nonzeros (nnz) in all of these distributions follow Binomial distributions:</p><formula xml:id="formula_26">x ∼ p b =⇒ p b (nnz(x) = k) ∼ Binom(k; n, q) x ∼ p i =⇒ p i (nnz(x) = k) = p b (nnz(x| i c ) = k -1) ∼ Binom(k -1; n -1, q) if k &gt; 0 else 0 x ∼ p u =⇒ p u (nnz(x) = k) = 1 n p i (nnz(x) = k) ∼ Binom(k -1; n -1, q) if k &gt; 0 else 0</formula><p>The basic intuition is that for large k and n, p b ∼ Binom(k; n, q) and p u ∼ Binom(k -1; n -1, q) are similar. More precisely, we can calculate:</p><formula xml:id="formula_27">E x∼p b p u (x) -p b (x) p b (x) 2 = E x∼p b nnz(x) qn -1 2 , since B(k -1; n -1, q) B(k; n, q) = k qn = E k∼Binom(n,q) k qn -1 2 = 1 (nq) 2 E k∼Binom(n,q) (k -nq) 2 = 1 (nq) 2 Var(k), k ∼ Binom(n, q) = 1 (nq) 2 nq(1 -q) = 1 -q nq → 0 as n → ∞.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Factorized conditionals vs. orthogonal score differences</head><p>To see that Definition 5.2 implies orthogonality between the score differences, we note that</p><formula xml:id="formula_28">v t i (x) := ∇ x log p t i (x t ) -∇ x log p t b (x t ) = ∇ x log p t i (x) p t b (x) = ∇ x log p t i (x| Mi )p t b (x| M c i x) p t b (x| Mi )p t b (x| M c i ) = ∇ x log p t i (x| Mi ) p t b (x| Mi ) =⇒ v t i (x)[k] = 0, ∀k / ∈ M i =⇒ v t i (x) T v t j (x) = 0, ∀i ̸ = j, since M i ∩ M j = ∅,</formula><p>where in the second-to-last line we used the fact that the gradient of a function depending only on a subset of variables has zero entries in the coordinates outside that subset.</p><p>In fact, the same argument implies that {v t i (x) : x ∈ R n } ⊂ M i ; in other words, {v t i (x) : x ∈ R n } and {v t j (x) : x ∈ R n } occupy mutually-orthogonal subspaces. But even this latter condition does not imply the stronger condition of Definition 5.2. To find an equivalent definition in terms of scores we must also capture the independence of the subsets under p b . Specifically:</p><formula xml:id="formula_29">   p t i (x) = p t i (x| Mi x)p t b (x| M c i x) p t b (x) = p t b (x| M x) i p t b (x| Mi ) ⇐⇒    ∇ x log p t i (x) = ∇ x log p t i (x| Mi x) + ∇ x log p t b (x| M c i x) ∇ x log p t b (x) = ∇ x log p t b (x| M x) + i ∇ x log p t b (x| Mi ) ⇐⇒        ∇ x log p t i (x) -∇ x log p t b (x) = ∇ x log p t i (x| Mi x) p t b (x| Mi x) ∇ x log p t b (x) = ∇ x log p t b (x| M x) + i ∇ x log p t b (x| Mi )</formula><p>So an equivalent definition in terms of scores could be:</p><formula xml:id="formula_30">Definition F.1. The distributions (p b , p 1 , p 2 , . . .) form factored conditionals if the score-deltas v t i := ∇ x log p t i (x) - ∇ x log p t b (x) satisfy {v t i (x) : x ∈ R n } ⊂ M i</formula><p>, where the M i are mutually-orthogonal subsets, and furthermore the score of the background distribution decomposes over these subsets as follows:</p><formula xml:id="formula_31">∇ x log p t b (x) = ∇ x log p t b (x| M x) + i ∇ x log p t b (x| Mi ).</formula><p>(Note: this is actually equivalent to a slightly more general version of Definition 5.2 that allows for orthogonal transformations, which is the most general assumption under diffusion sampling generates a projective composition, per Lemmas 6.1 and 7.1.)</p><p>G. Proof of Theorem 5.3</p><p>Proof. (Theorem 5.3) For any set of distributions ⃗ q = (q b , q 1 , q 2 , . . .) satisfying Definition 5.2, we have</p><formula xml:id="formula_32">C[⃗ q](x) := q b (x) i q i (x) q b (x) = q b (x) i q b (x t | M c i )q i (x| Mi ) q b (x| M c i )q b (x| Mi ) = q b (x) i q i (x| Mi ) q b (x| Mi ) = q b (x| M b ) i q i (x t | Mi )<label>(18)</label></formula><p>(where we used (7) in the second equality). Since (p b , p 1 , p 2 , . . .) satisfy Definition 5.2 by assumption, applying (18) gives</p><formula xml:id="formula_33">C[⃗ p](x) = p b (x| M b ) i p i (x| Mi ) := p(x),</formula><p>so the composition at t = 0 is projective, as desired.</p><p>Now to show that reverse-diffusion sampling with the compositional scores generates C[⃗ p], we need to show that C[ ⃗ p t ] = N t [C[⃗ p]], where p t := N t [p] denotes the t-noisy version of distribution p under the forward diffusion process. First, notice that if ⃗ p satisfies Definition 5.2, then ⃗ p t does as well. This is because the diffusion process adds Gaussian noise independently to each coordinate, and thus preserves independence between sets of coordinates. Therefore by (18), we have C[ ⃗ p t ](x) = p t b (x| M ) i p t i (x t | Mi ). Now we apply the same argument (that diffusion preserves independent sets of coordinates) once again, to see that C[ ⃗ p t ] = N t [C[⃗ p]], as desired.</p><p>H. Parameterization-Independent Compositions and Proof of Lemma 6.1</p><p>The proof of Lemma 6.1 relies on certain general fact about parametrization-independence of certain operators, which we develop here.</p><p>Suppose we have an operator that takes as input two probability distributions (p, q) over the same space X , and outputs a distribution over X . That is, F : ∆(X ) × ∆(X ) → ∆(X ). We can think of such operators as performing some kind of "composition" of p, q.</p><p>Certain operators are independent of parameterization, meaning for any reparameterization of the base space A : X → Y, we have</p><formula xml:id="formula_34">F (p, q) = A -1 ♯(F (A♯p, A♯q))</formula><p>or equivalently:</p><formula xml:id="formula_35">F (A♯p, A♯q) = A♯F (p, q),</formula><p>where ♯ is the pushforward:</p><formula xml:id="formula_36">(A♯p)(z) := 1 |∇A| p(A -1 (z)).</formula><p>This means that reparameterization commutes with the operator: it does not matter if we first reparameterize, then compose, or first compose, then reparamterize. A few examples:</p><p>1. The pointwise-geometric median, F (p, q)(x) := p(x)q(x), is independent of reparameterization:</p><p>2. Squaring a distribution, F (p, q)(x) := p(x) 2 , is NOT independent of reparameterization:</p><p>3. The "CFG composition" <ref type="bibr" target="#b9">(Ho &amp; Salimans, 2022)</ref>, F (p, q)(x) := p(x) γ q(x) 1-γ , is independent of reparameterization:</p><p>We can analogously define parametrization-independence for operators on more than 2 distributions. Notably, given a tuple of distributions ⃗ p = (p b , p 1 , p 2 , . . . , p k ), our composition operator C of Definition 5.1, x) is independent of parameterization.</p><formula xml:id="formula_37">C[⃗ p] ∝ p b (x) i pi(x) p b (</formula><p>Lemma H.1 (Parametrization-independence of 1-homogeneous operators). If an operator F is 1-homogeneous, i.e. F (tp, tq, . . .) = tF (p, q, . . .) and operates pointwise, then it is independence of parametrization.</p><p>Proof.</p><formula xml:id="formula_38">F (A♯p, A♯q, . . .)(z) = F (A♯p(z), A♯q(z), . . .), pointwise = F 1 |∇A| p(A -1 (z)), 1 |∇A| q(A -1 (z)), . . . = 1 |∇A| F p(A -1 (z)), q(A -1 (z)), . . . , 1-homogeneous = A♯F (p, q, . . .)(z)</formula><p>Corollary H.2 (Parametrization-invariance of composition). The composition operator C given by Definition 5.1 is independent of parametrization.</p><p>Proof. The composition operator given by Definition 5.1 is 1-homogeneous:</p><formula xml:id="formula_39">C(tp b , tp 1 , tp 2 , . . .)(x) = tp b (x) i tp i (x) tp b (x) = tp b (x) i p i (x) p b (x) = tC(p b , p 1 , p 2 , . . . )(x)</formula><p>and so the result follows from Lemma H.1. Alternatively, a direct proof is:</p><formula xml:id="formula_40">C(p b , p 1 , p 2 , . . .)(x) := p b (x) i p i (x) p b (x) C(A♯p b , A♯p 1 , A♯p 2 , . . .)(z) = (A♯p b )(z) i (A♯p i )(z) (A♯p b )(z) = 1 |∇A| p b (A -1 (z)) i p i (A -1 (z)) p b (A -1 (z)) = A♯C(p b , p 1 , p 2 , . . .)(z).</formula><p>Theorem 6.1 follows from Corollary H.2:</p><p>Proof. (Theorem 6.1) Let (q b , q 1 , q 2 , . . . , q k ) := (A♯p b , A♯p 1 , . . . A♯p k ), for which Definition 5.2 holds by assumption.</p><p>Applying an intermediate result from the proof of Theorem 5.3 gives:</p><formula xml:id="formula_41">C[⃗ q](z) := q b (z) i q i (z) q b (z) = q b (z| M ) i q i (z| Mi ).</formula><p>By Corollary H.2, C is independent of parametrization, hence</p><formula xml:id="formula_42">A♯p := A♯(C[⃗ p]) = C[ ⃗ A♯p] := C(⃗ q).</formula><p>I. Proof of Lemma 7.1</p><p>Figure <ref type="figure" target="#fig_8">12</ref> shows a synthetic experiment illustrating the sampling guarantees of Lemma 7.1 in contrast to the lack-ofguarantees in the non-orthogonal case.</p><p>The proof of Lemma 7.1 relies on the fact that diffusion noising commutes with orthogonal transformation, i.e. A♯N t [q] = N t [A♯q] if A is orthogonal, since standard Gaussians are invariant under orthogonal transformation. Now we have to be careful with commuting operators. We know that composition is independent of parametrization, i.e. A♯C[⃗ p] = C[ ⃗ A♯p]. Diffusion noising N t commutes with orthogonal transformation, i.e. A♯N t [q] = N t [A♯q] if A is orthogonal, because a standard Gaussian multiplied by an orthonormal matrix Q remains a standard Gaussian: η ∼ N (0, I) =⇒ Qη ∼ N (0, QQ T ) = N (0, I) (this is false for non-orthogonal transforms, however). Therefore, in the orthogonal case, we can rewrite:</p><formula xml:id="formula_43">A♯C[N t [p]] = A♯N t [C[p]],</formula><p>which implies the desired result since A is invertible.  <ref type="figure">13</ref>. We show samples from the individual conditional distributions p 0 , p 1 using DDIM, samples from the desired exact composition C[p b , p 0 , p 1 ] at t = 0 (obtained by sampling from A♯C[⃗ p] with DDIM and transforming by A -1 ), and samples from the composition C[p b , p 0 , p 1 ] using DDIM with exact scores. We take τ = 0.02, and set σ min = 0.02 in the diffusion schedule. In the top row we take a = 1 ("very non-orthogonal") as in the proof, and compare this to a = 0.3 ("mildly non-orthogonal") in the bottom row. With a = 1, as in the proof we see that DDIM barely samples two of the clusters. With a = 0.3, DDIM still slightly undersamples the "hard" clusters but the effect is much less pronounced.</p><p>there exists a (linear, but not orthogonal) A such that the distribution of z = Ax is axis-aligned (A♯p 0 )(z) ≈ 1 2 δ e0 (x) + 1 2 δ e2 (x), (A♯p 1 )(z) ≈ 1 2 δ e1 (x) + 1 2 δ e3 (x), and thus does satisfy Definition 5.2 at t = 0, which guarantees correct composition of p at t = 0 under Lemma 6.1. The correct composition should sample uniformly from {(1+a)e 0 +e 1 , e 0 +ae 2 +e 3 , ae 0 +e 2 +e 1 , (1+a)e 2 +e 3 }. What goes wrong is that as soon as we add Gaussian noise to the distribution p(x) at time t &gt; 0 of the diffusion forward process, the relationship z = Ax breaks and so we are no longer guaranteed correct composition of p t (x). In fact, the distribution is still a GMM but places nearly all its weight on only two of the four clusters, namely: {(1 + a)e 0 + e 1 , (1 + a)e 2 + e 3 }.</p><p>Intuitively, let us focus on the mode ae 0 + e 1 of p 1 and consider how it interacts with the two modes e 0 , e 2 of p 0 , at some time t &gt; 0 when we have isotropic Gaussians centered at each mode. Since ae 0 + e 1 is further away from e 2 (distance √ a 2 + 2) than it is from e 0 (distance √ a 2 -2a + 2), it is much less likely under N (e 2 , σ t ) than N (e 0 , σ t ), leading to a lower weight. This intuition is shown graphically in a 2D projection in Figure <ref type="figure">13</ref> (left).</p><p>For the detailed proof, we actually want to ensure that p has full support even at t = 0 so we add a little bit of noise to it, but choose the covariance such that z = Ax still holds at t = 0. 1 -a 0 0 0 1 0 0 0 0 1 -a 0 0 0 1</p><formula xml:id="formula_44">    , 1 2 µ T 2 Σ -1 µ 2 + 1 2 (µ 1 + µ 2 ) T Σ -1 (µ 1 + µ 2 ) = exp(µ T 1 Σ -1 µ 2 )</formula><p>Noting that</p><formula xml:id="formula_45">Σt := σ 2 t I + τ 2 (A T A) -1 = σ 2 t I + τ 2     1 + a 2 a 0 0 a 1 0 0 0 0 1 + a 2 a 0 0 a 1     Σt -1 = 1 (a 2 + 2)</formula><p>σ 2 t τ 2 + σ 4 t + τ 4     σ 2 t + τ 2 -aτ 2 0 0 -aτ 2 (a 2 + 1)τ 2 + σ 2 t 0 0 0 0 σ 2 t + τ 2 -aτ 2 0 0 -aτ 2 (a 2 + 1)τ 2 + σ 2 t     ,</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc>Figure 1: Composing diffusion models via score combination. Given two diffusion models, it is sometimes possible to sample in a way that composes content from one model (e.g. your dog) with style of another model (e.g. oil paintings). We aim to theoretically understand this empirical behavior. Figure generated via score composition with SDXL finetuned on the author's dog; details in Appendix C.</figDesc><graphic coords="1,307.44,189.67,233.98,84.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Length-generalization, another capability of composition enabled by our framework. Diffusion models trained to generate a single object conditioned on location (left) can be composed at inference-time to generate images of multiple objects at specified locations (right). Notably, such images are strictly out-of-distribution for the individual models being composed. (Additional samples in Figure 9.)</figDesc><graphic coords="1,307.44,386.45,233.99,117.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Distribution p is a projective composition of p 1 and p 2 w.r.t. projection functions (Π 1 , Π 2 ), because p has the same marginals as p 1 when both are post-processed by Π 1 , and analogously for p 2 .</figDesc><graphic coords="5,67.14,67.06,210.60,97.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Composing yellow objects with objects of other colors. Yellow objects successfully compose with blue, cyan and magenta objects but not with brown, gray, green, or red objects. Per the histograms (left), in RGB-colorspace yellow has R, G distributed like the background (gray) while B has a distinct distribution peaked closer to zero. Taking M yellow ≈ {B}, Theorem 5.3 predicts that standard diffusion can sample from compositions of yellow with any color where the B channel is distributed like the background: namely, blue, cyan, magenta per the histograms. (Other colors may theoretically compose per Theorem 6.1, but be difficult to sample.) (Additional samples in Figure 10.)</figDesc><graphic coords="5,307.44,67.06,234.00,172.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><figDesc>Bayes composition with cluttered distributions. In Figure 3c we replicate CLEVR experiments in Du et al. (2023); Liu et al. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><figDesc>Theorem 6.1 (Feature-space Composition). Given distributions ⃗ p := (p b , p 1 , p 2 , . . . p k ), suppose there exists a diffeomorphism A : R n → R n such that (A♯p b , A♯p 1 , . . . A♯p k ) satisfy Definition 5.2, with corresponding partition M i ⊆ [n]. Then, the composition p := C[⃗ p] satisfies:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Additional non-cherrypicked samples for CLEVR experiment of Figure 2.</figDesc><graphic coords="15,55.44,242.97,485.99,243.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Additional non-cherrypicked samples for CLEVR experiment of Figure 5. Top left grid shows conditional samples for each color. Top right grid shows compositions of red-colored objects (p 6 ) with objects of other colors (8 samples of each), which only succeeds for cyan-colored objects. Bottom grid shows compositions of yellow-colored objects (p 7 ) with objects of other colors (16 samples of each): these are additional samples of the exact experiment shown in Figure 5.</figDesc><graphic coords="16,55.44,149.45,486.01,440.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 12 :</head><label>12</label><figDesc>Figure12: Synthetic composition experiment illustrating the sampling guarantees of Lemma 7.1 in contrast to the lackof-guarantees in the non-orthogonal case. We compare a coordinate-aligned case (which satisfies Definition 5.2 in the native space) (top), an orthogonal-transform case (middle) (which satisfies the assumptions of Lemma 7.1), and a nonorthogonal-transform case (bottom) (which satisfies the assumptions of Theorem 6.1 but not of Lemma 7.1). In the first two cases the correct composition can be sampled using either diffusion (DDIM) or Langevin dynamics (LD) at t = 0, while in the final case DDIM sampling is unsuccessful although LD at t = 0 still works. The distributions are 4-dimensional and we show 8 samples (rows) for each. We show samples from the individual conditional distributions p 0 , p 1 using DDIM, samples from the desired exact composition C[p b , p 0 , p 1 ] at t = 0 (obtained by sampling from A♯C[⃗ p] with DDIM and transforming by A -1 ), samples from the composition C[p b , p 0 , p 1 ] using DDIM with exact scores, and samples from the composition C[p b , p 0 , p 1 ] using Langevin dynamics (LD) with exact scores at time t = 0 in the diffusion schedule (σ min = 0.02). The noiseless distributions p 0 and p 1 are each 4-dimensional 2-cluster Gaussian mixtures with means as noted in the figure, equal weights, and standard deviation τ = 0.02. For example, in the non-orthogonal-transform case, p 0 has means [1, 0, 0, 0] and [0, 0, 1, 0], and p 1 has means [1, 1, 0, 0] and [0, 0, 1, 1], (which can be transformed to satisfy Definition 5.2 via a non-orthogonal linear transform).</figDesc><graphic coords="24,176.94,81.46,243.00,200.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Composition experiments for the setting in the proof of Lemma 7.2. Left pane shows 8 samples (rows) of each distribution in the native 4d representation; right pane shows 1000 samples under the 2D projection used in Figure13. We show samples from the individual conditional distributions p 0 , p 1 using DDIM, samples from the desired exact composition C[p b , p 0 , p 1 ] at t = 0 (obtained by sampling from A♯C[⃗ p] with DDIM and transforming by A -1 ), and samples from the composition C[p b , p 0 , p 1 ] using DDIM with exact scores. We take τ = 0.02, and set σ min = 0.02 in the diffusion schedule.In the top row we take a = 1 ("very non-orthogonal") as in the proof, and compare this to a = 0.3 ("mildly non-orthogonal") in the bottom row. With a = 1, as in the proof we see that DDIM barely samples two of the clusters. With a = 0.3, DDIM still slightly undersamples the "hard" clusters but the effect is much less pronounced.</figDesc><graphic coords="26,55.44,81.46,485.99,165.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><figDesc>ae 0 + e 1 , τ 2 (A T A) -1 ) + 1 2 N (x; ae 2 + e 3 , τ 2 (A T A) -1 ) p 0 b (x) = N (x; 0, τ 2 (A T A) -1 ), where A :=    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="3,55.44,81.46,486.00,195.96" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The geometric mean p1(x)p2(x) is also often used; our discussion applies equally to this as well.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Although<ref type="bibr" target="#b5">Du et al. (2023)</ref> use the Bayes composition to achieve a kind of length-generalization, our discussion shows that the Bayes justification does not explain the experimental results.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>We use the term "projection" informally here, to convey intuition; these functions Πi are not necessarily coordinate projections, although this is an important special case (Section 5).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>The notation ♯ refers to push-forward of a probability measure.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>Our results are agnostic to the specific diffusion noiseschedule and scaling used.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>https://github.com/facebookresearch/clevr-dataset-gen</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>Acknowledgements: We thank <rs type="person">Miguel Angel Bautista Martin</rs>, <rs type="person">Etai Littwin</rs>, <rs type="person">Jason Ramapuram</rs>, and <rs type="person">Luca Zappella</rs> for helpful discussions and feedback throughout this work, and <rs type="person">Preetum's dog Papaya</rs> for his contributions to Figure <ref type="figure">1</ref>.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. CLEVR Experimental Details</head><p>All of our CLEVR experiments use raw conditional diffusion scores, without applying any guidance/CFG <ref type="bibr" target="#b9">(Ho &amp; Salimans, 2022)</ref>. Details below.</p><p>B.1. Dataset, models, and training details B.1.1. CLEVR DATASET We used the CLEVR <ref type="bibr" target="#b15">(Johnson et al., 2017)</ref> dataset generation procedure 6 to generate datasets customized to the needs of the present work. All default objects, shapes, sizes, colors were kept unchanged. Images were generated in their original resolution of 320 × 240 and down-sampled to a lower resolution of 128 × 128 to facilitate experimentation and to be more GPU resources friendly. The various datasets we generated from this procedure include:</p><p>• A background dataset (0 objects) with 50,000 samples • Single object dataset with 1,550,000 samples • A dataset having 1 to 5 objects, with 500,000 samples for each object count, for a total of 2,500,000 samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.2. MODEL ARCHITECTURE</head><p>We used our own PyTorch re-implementation of the EDM2 <ref type="bibr" target="#b18">(Karras et al., 2024)</ref> U-net architecture. Our re-implementation is functionally equivalent, and only differs in optimizations introduced to save memory and GPU cycles. We used the smallest model architecture, e.g. edm2-img64-xs from <ref type="url" target="https://github.com/NVlabs/edm2">https://github.com/NVlabs/edm2</ref>. This model has a base channel width of 128, resulting in a total of 124M trainable weights. Two versions of this model were used:</p><p>• An unmodified version for background and class-conditioned experiments.</p><p>• A modified version for (x, y) conditioning in which we simply replaced Fourier embeddings for the class with concatenated Fourier embeddings for x and y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.3. TRAINING AND INFERENCE</head><p>In all experiments, the model is trained with a batch size of 2048 over 128 × 2 20 samples by looping over the dataset as often as needed to reach that number. In practice, training takes around 16 hours to complete on 32 A100 GPUs. We used almost the same training procedure as in EDM2 <ref type="bibr" target="#b18">(Karras et al., 2024)</ref>, which is basically a standard training loop with gradient accumulation. The only difference is that we do weight renormalization after the weights are updated rather than before as the authors originally did.</p><p>For simplicity, we did not use posthoc-EMA to obtain the final weights used in inference. Instead we took the average of weights over the last 4096 training updates. The denoising procedure for inference is exactly the same as in EDM2 <ref type="bibr" target="#b18">(Karras et al., 2024)</ref>, e.g. 65 model calls using a 32-step Heun sampler.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Factorized Conditionals in CLEVR.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.1. SINGLE OBJECT DISTRIBUTIONS WITH EMPTY BACKGROUND</head><p>Let us explicitly describe how our definition of Factorized Conditionals captures the CLEVR setting of Figures <ref type="figure">2</ref> and <ref type="figure">3a</ref>.</p><p>Recall, the background distribution p b over n pixels is images of an empty scene with no objects. For each i ∈ {1, . . . , L} (where L = 4 in Figure <ref type="figure">2</ref> and L = 9 in Figure <ref type="figure">3</ref>(a)) define the set M i ∈ [n] as the set of pixel indices surrounding location i. Each M i should be thought of as a "mask" that that masks out objects at location i. Then, let M b := (∪ i M i ) c be the remaining pixels in the image, excluding all the masks. Now we claim the distributions (p b , p 1 , . . . , p L ) are approximately Factorized Conditionals, with corresponding coordinate partition (M b , M 1 , . . . , M L ). We can confirm each criterion in Definition 5.2 individually:</p><p>C. SDXL experimental details C.1. Figure <ref type="figure">1</ref> The two models composed are</p><p>1. An SDXL model <ref type="bibr" target="#b28">(Podell et al., 2023)</ref> fine-tuned on 30 personal photos of the author's dog (Papaya).</p><p>2. SDXL-base-1.0 <ref type="bibr" target="#b28">(Podell et al., 2023)</ref> conditioned on prompt "an oil painting in the style of van gogh."</p><p>The background score distribution is the unconditional background (i.e. SDXL conditioned on the empty prompt). We use the DDPM sampler <ref type="bibr" target="#b10">(Ho et al., 2020)</ref> with 30 steps, using the composed score, and CFG guidance weight of 2 Ho et al.</p><p>(2020).</p><p>Note that using guidance weight 1 (i.e. no guidance) also performs reasonably in this case, but is lower quality.</p><p>C.2. Figure <ref type="figure">7</ref> Left: The two score models composed are 1. SDXL-base-1.0 <ref type="bibr" target="#b28">(Podell et al., 2023)</ref> conditioned on prompt "photo of a dog" 2. SDXL-base-1.0 <ref type="bibr" target="#b28">(Podell et al., 2023)</ref> conditioned on prompt "photo of a horse"</p><p>The background score distribution is the unconditional background (i.e. SDXL conditioned on the empty prompt).</p><p>For improved sample quality, we use a Predictor-Corrector method <ref type="bibr" target="#b36">(Song et al., 2020)</ref> with the DDPM predictor and the Langevin dynamics corrector, both operating on the composed score. We use 100 predictor denoising steps, and 3 Langevin iterations per step. We do not use any guidance/CFG.</p><p>Right: Identical setting as above, using prompts:</p><p>1. "photo of a dog" 2. "photo, with red hat"</p><p>Note that the DDPM sampler also performed reasonably in this setting, but Predictor-Corrector methods improved quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Reverse Diffusion and other Samplers</head><p>D.1. Diffusion Samplers DDPM <ref type="bibr" target="#b10">(Ho et al., 2020)</ref> and DDIM <ref type="bibr" target="#b34">(Song et al., 2021)</ref> are standard reverse diffusion samplers <ref type="bibr" target="#b33">(Sohl-Dickstein et al., 2015;</ref><ref type="bibr" target="#b35">Song &amp; Ermon, 2019</ref>) that correspond to discretizations of a reverse-SDE and reverse-ODE, respectively (so we will sometimes refer to the reverse-SDE as DDPM and the reverse-ODE as DDIM for short). The forward process, reverse-SDE, and equivalent reverse-ODE <ref type="bibr" target="#b36">(Song et al., 2020)</ref> for the variance-preserving (VP) <ref type="bibr" target="#b10">(Ho et al., 2020)</ref> conditional diffusion are</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Langevin Dynamics</head><p>Langevin dynamics (LD) <ref type="bibr" target="#b31">(Rossky et al., 1978;</ref><ref type="bibr" target="#b26">Parisi, 1981)</ref> an MCMC method for sampling from a desired distribution. It is given by the following SDE <ref type="bibr" target="#b29">(Robert et al., 1999</ref>)</p><p>which converges (under some assumptions) to ρ(x) <ref type="bibr" target="#b30">(Roberts &amp; Tweedie, 1996)</ref>. That is, letting ρ s (x) denote the solution of LD at time s, we have lim s→∞ ρ s (x) = ρ(x).  <ref type="formula">17</ref>), where each conditional p i activates index i on an independently 'cluttered' background. In this case the unconditional is similar to the cluttered background. Again we attempt to compose p 0 , p 2 , p 4 , p 6 , and in this case we find that the composition using p u works similarly well to p b .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Connections with the Bayes composition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1. The Bayes composition and length-generalization</head><p>We give a counterexample for which the Bayes composition fails to length-generalize, while composition using an "empty background" succeeds. The example corresponds to the experiment shown in Figure <ref type="figure">11</ref> (left). Suppose we have conditional J. Proof and further discussion of Lemma 7.2 J.1. Benefits of sampling at t = 0</p><p>Interestingly, <ref type="bibr" target="#b5">(Du et al., 2023)</ref> have observed that sophisticated samplers like Hamiltonian Monte Carlo (HMC) requiring energy-based formulations often outperform standard diffusion sampling for compositional quality. Lemmas 6.1 and 7.2 help explain why this may be the case. In particular, HMC (or any variant of Langevin dynamics) can enable sampling p 0 at time t = 0, even when the path p t used for annealing does not necessarily represent a valid forward diffusion process starting from p 0 (as <ref type="bibr" target="#b5">Du et al. (2023)</ref> note, C[⃗ p t ]] may not be). Lemma 6.1 should gives us hope that approximately-projective composition may often be possible at t = 0, since it allows any invertible transform A to transform into a factorized feature space (which need not be explicitly constructed). However, that does not mean that we can actually sample from this projection at time t = 0. As Lemma 7.2 shows, C[⃗ p t ]] is not necessarily a valid diffusion path unless A is orthogonal, so standard diffusion sampling may not work. This is consistent with Du et al. ( <ref type="formula">2023</ref>)'s observation that non-diffusion samplers that allow sampling at t = 0 may be necessary. Interestingly, Lemma 7.2 further cautions that sometimes C[⃗ p t ]] may not even be an effective annealing path for any kind of sampler (which is consistent with our own experiments but not reported by other works, to our knowledge.)</p><p>J.2. Proof of Lemma 7.2</p><p>Figure <ref type="figure">13</ref>: (Left) A visualization of the intuition behind the proof of Lemma 7.2, under a 2D projection. (Right) An experiment where the colors red, green, and blue all compose projectively, while the colors red and yellow do not. We trained a Unet on images each containing a single square in one of 4 locations (selected randomly) and a certain color, conditioned on the color. We then generate composed distributions by running DDIM on the composed scores. The desired result of composing red and blue is an image containing a red and a blue square, both with randomly-chosen locations (so we occasionally get a purple square when the locations overlap). When we try to compose red and yellow, we only only ever obtain a single yellow square.Note that in pixel space, the colors are represented as red (1, 0, 0), green (0, 1, 0), blue (0, 0, 1), yellow (1, 1, 0), so that red, green and blue are all orthogonal and are expected to work by Lemma 5.3, while red and yellow are not orthogonal, and fail as allowed by Lemma 7.2. In fact this experiment is closely related to the counterexample used to prove Lemma 7.2.</p><p>We will prove Lemma 7.2 using a counterexample, which is inspired by an experiment, shown in Figure <ref type="figure">14</ref> (left), where non-orthogonal conditions fail to compose projectively.</p><p>The basic idea for the counterexample is that given a distribution p(x) with two conditions, c = 0, 1, such at t = 0,</p><p>for some 0 &lt; a ≤ 1, so the conditional distributions do not satisfy the independence assumption of Definition 5.2, However, so that in the transformed space:</p><p>Therefore Lemma 6.1 implies that at time t = 0,</p><p>When we add noise at time t &gt; 0 we get:</p><p>We will start by using this counterexample to prove Part 2 of Lemma 7.2, which is the hard part. Note that pt (x) is made up of terms of the following form:</p><p>after some algebra we find that p0 (x) and pt (x) are both GMMs with same means: ⃗ µ = {(1 + a)e 0 + e 1 , e 0 + ae 2 + e 3 , ae 0 + e 2 + e 1 , (1 + a)e 2 + e 3 }, different variances ( Σ0 = τ 2 (A T A) -1 and Σt for all clusters, respectively), and different weights, as follows:</p><p>The key idea is that if you compare the weight on the mode at (1 + a)e 0 + e 1 (which is proportional to M ) vs the weight on the mode at e 1 + ae 2 + e 3 (proportional to 1) the former is much more likely than the latter as σ t → 0.</p><p>The basic idea for lower-bounding the W 2 distance is that w t has almost no mass on the two of the clusters and so we will need to move a little less than 1/4 probability over to those clusters. For example we need to move 1/4 probability onto cluster e 0 + ae 2 + e 3 from either (1 + a)e 0 + e 1 (L2 distance between means is √ 2a + 2) or (1 + a)e 2 + e 3 (L2 distance √</p><p>2). So overall we will have to move a bit less that 1/2 probability at least √ 2 distance.</p><p>To complete the proof we will exploit the Mixture Wasserstein distance as an intermediate. We need the following facts from <ref type="bibr" target="#b2">Delon &amp; Desolneux (2020)</ref>:</p><p>∥y 0 -y 1 ∥ 2 dγ(y 0 , y 1 ), M W 2 2 (q 0 , q 1 ) = min c∈Π(w0,w1) k,l c k,l W 2 2 (q k 0 , q l 1 ) (Delon Prop. 4), M W 2 (q 0 , q 1 ) ≤ W 2 (q 0 , q 1 ) + 2 i=0,1 Ki k=1</p><p>where Π(q 0 , q 1 ) denotes the set of all joint distributions with marginals q 0 and q 1 , and GMM d (∞) := ∪ K≥0 GMM d (K) denotes the set of all finite GMMs. Plus one more handy fact:</p><p>Above, we noted any c ∈ Π(w 0 , w t ) has to move at least 1 4 -ε probability each away from indices 1 and 2 in w 0 and onto indices either 0 or 3, and for any of these moves the squared L2 distance is at least 2, i.e. ∥µ k -µ l ∥ 2 2 ≥ 2 for k ∈ (1, 2), l ∈ (0, 3). We can use the M W 2 distance to bound the W 2 distance:</p><p>Putting everything together, we have</p><p>Choosing a = 1 allows some further simplification:</p><p>(in the second-to-last line we used the fact that</p><p>32 , and in the last line we made an arbitrary choice).</p><p>We wanted to show that</p><p>Let's use the simple schedule σ t := t.</p><p>For any τ 2 &lt; 1 66 , if we pick t ′ = 0 and t = τ , then we have as desired that W 2 (p 0 , pt ) ≥ 0.5 ≡ 0.5τ -1 |t|.</p><p>For Part 1 of Lemma 7.2, we need to show that the distributions p i satisfy: p i is 1-Lipschitz w.r.  x Σ y Σ 1 2</p><p>x )</p><p>1 2 ) := ∥µ x -µ y ∥ 2 2 + ∥Σ 1 2</p><p>x -Σ </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Compositional foundation models for hierarchical planning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ajay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scaling in-the-wild training for diffusionbased illumination harmonization and editing by imposing consistent light transport</title>
		<ptr target="https://openreview.net/forum" />
	</analytic>
	<monogr>
		<title level="m">Submitted to The Thirteenth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>Anonymous id=u1cQYxRI1H. under review</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A wasserstein-type distance in the space of gaussian mixture models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Delon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Desolneux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Imaging Sciences</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="936" to="970" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Compositional generative modeling: A single model is not all you need</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaelbling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.01103</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Compositional visual generation and inference with energy based models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mordatch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.06030</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Reduce, reuse, recycle: Compositional generation with energy-based diffusion models and mcmc</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Durkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Strudel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Grathwohl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="8489" to="8510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2414" to="2423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Draw: A recurrent neural network for image generation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1502.04623" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Training products of experts by minimizing contrastive divergence</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1771" to="1800" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.12598</idno>
		<title level="m">Classifier-free diffusion guidance</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Lora</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09685</idno>
		<title level="m">Low-rank adaptation of large language models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Editing models with task arithmetic</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.04089</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adaptive mixtures of local experts</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Nowlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="87" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Planning with diffusion for flexible behavior synthesis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Janner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.09991</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Clevr: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lawrence Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2901" to="2910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">An analytic theory of creativity in convolutional diffusion models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kamb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.20292</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04948</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Analyzing and improving the training dynamics of diffusion models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="24174" to="24184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to compose visual relations</title>
		<author>
			<persName><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="23166" to="23178" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Compositional visual generation with composable diffusion models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="423" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Correcting diffusion generation through resampling</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="8713" to="8723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Decentralized diffusion models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2501.05450</idno>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Glide</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.10741</idno>
		<title level="m">Towards photorealistic image generation and editing with text-guided diffusion models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Controllable and compositional generation with latent-space energy-based models</title>
		<author>
			<persName><forename type="first">W</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13497" to="13510" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Compositional abilities emerge multiplicatively: Exploring diffusion models on a synthetic task</title>
		<author>
			<persName><forename type="first">M</forename><surname>Okawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Lubana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tanaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Correlation functions and computer simulations</title>
		<author>
			<persName><forename type="first">G</forename><surname>Parisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nuclear Physics B</title>
		<imprint>
			<biblScope unit="volume">180</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="378" to="384" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Okawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Lubana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tanaka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.19370</idno>
		<title level="m">Emergence of hidden capabilities: Exploring learning dynamics in concept space</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Sdxl: Improving latent diffusion models for high-resolution image synthesis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Podell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>English</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lacey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dockhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Penna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.01952</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Monte Carlo statistical methods</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Casella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Casella</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Exponential convergence of langevin distributions and their discrete approximations</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">O</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Tweedie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Brownian dynamics as smart monte carlo simulation</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Rossky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Doll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Chemical Physics</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4628" to="4633" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">The superposition of diffusion models using the it\ˆo density estimator</title>
		<author>
			<persName><forename type="first">M</forename><surname>Skreta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Atanackovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Neklyudov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.17762</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Deep unsupervised learning using nonequilibrium thermodynamics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
		<idno>CoRR, abs/1503.03585</idno>
		<ptr target="http://arxiv.org/abs/1503.03585" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Denoising diffusion implicit models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=St1giarCHLP" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Generative modeling by estimating gradients of the data distribution</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.13456</idno>
		<ptr target="https://arxiv.org/pdf/2011.13456.pdf" />
		<title level="m">Score-based generative modeling through stochastic differential equations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Ctrloralter: Conditional loradapter for efficient zero-shot control &amp; altering of t2i models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Stracke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Baumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A B</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2405.07913" />
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.19298</idno>
		<title level="m">Compositional image decomposition with diffusion models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Composable energy policies for reactive motion generation and reinforcement learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Urain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>D'eramo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="827" to="858" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Slot-vae: Object-centric scene generation with slot attention</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dauwels</surname></persName>
		</author>
		<idno>ArXiv, abs/2306.06997</idno>
		<ptr target="https://api.semanticscholar.org/CorpusID" />
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">259138897</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Concept algebra for (score-based) text-controlled generative models</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Negrea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Veitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Compositional generalization from first principles</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wiedemer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mayilvahanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Maruyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Iaccarino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.13171</idno>
		<title level="m">Compositional generative inverse design</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Probabilistic adaptation of text-tovideo models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.01872</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lozano-Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.00966</idno>
		<title level="m">Compositional diffusionbased continuous constraint solvers</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Composing parameterefficient modules with arithmetic operation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="12589" to="12610" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Adding conditional control to text-to-image diffusion models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Agrawala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="3836" to="3847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
