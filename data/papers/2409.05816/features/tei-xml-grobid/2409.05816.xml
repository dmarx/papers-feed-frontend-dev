<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IMPROVING PRETRAINING DATA USING PERPLEXITY CORRELATIONS</title>
				<funder>
					<orgName type="full">Sandia National Laboratories</orgName>
				</funder>
				<funder>
					<orgName type="full">Meta</orgName>
				</funder>
				<funder ref="#_h8zmxK5">
					<orgName type="full">Tianqiao and Chrissy Chen Institute</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-09-09">9 Sep 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tristan</forename><surname>Thrush</surname></persName>
							<email>tthrush@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University Stanford</orgName>
								<address>
									<postCode>94305</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
							<email>cgpotts@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University Stanford</orgName>
								<address>
									<postCode>94305</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tatsunori</forename><surname>Hashimoto</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University Stanford</orgName>
								<address>
									<postCode>94305</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">IMPROVING PRETRAINING DATA USING PERPLEXITY CORRELATIONS</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-09-09">9 Sep 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">03571304DDD15FB67C4901E5E2427008</idno>
					<idno type="arXiv">arXiv:2409.05816v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Quality pretraining data is often seen as the key to high-performance language models. However, progress in understanding pretraining data has been slow due to the costly pretraining runs required for data selection experiments. We present a framework that avoids these costs and selects high-quality pretraining data without any LLM training of our own. Our work is based on a simple observation: LLM losses on many pretraining texts are correlated with downstream benchmark performance, and selecting high-correlation documents is an effective pretraining data selection method. We build a new statistical framework for data selection centered around estimates of perplexity-benchmark correlations and perform data selection using a sample of 90 LLMs taken from the Open LLM Leaderboard on texts from tens of thousands of web domains. In controlled pretraining experiments at the 160M parameter scale on 8 benchmarks, our approach outperforms DSIR on every benchmark, while matching the best data selector found in DataComp-LM, a hand-engineered bigram classifier.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Dataset curation is increasingly crucial for training high-quality large language models (LLMs). As pretraining datasets have grown, from under 200B tokens in 2020 <ref type="bibr" target="#b38">(Raffel et al., 2020;</ref><ref type="bibr" target="#b15">Gao et al., 2020)</ref> to 240T tokens today <ref type="bibr" target="#b27">(Li et al., 2024)</ref>, it has become critical to identify subsets of the available data that will lead to the best LLMs, and a wide range of methods have arisen to meet these needs <ref type="bibr" target="#b22">(Ilyas et al., 2022;</ref><ref type="bibr">Xie et al., 2023a;</ref><ref type="bibr">b;</ref><ref type="bibr" target="#b12">Engstrom et al., 2024;</ref><ref type="bibr" target="#b13">Everaert &amp; Potts, 2024;</ref><ref type="bibr" target="#b28">Liu et al., 2024;</ref><ref type="bibr">Llama Team, 2024)</ref>. However, data-driven approaches to data selection typically involve expensive model retraining steps that limit their effectiveness, and no algorithm has been reported to consistently beat or match hand-crafted classifiers for data selection <ref type="bibr" target="#b27">(Li et al., 2024)</ref>.</p><p>Is training new LLMs necessary for data selection? Instead of training our own models, can we use the growing collection of publicly available, high-performance LLMs <ref type="bibr" target="#b50">(Wolf et al., 2019;</ref><ref type="bibr" target="#b3">Beeching et al., 2023)</ref> to perform data valuation and selection? This would have significant benefits: we could leverage the millions of dollars collectively spent on building these LLMs, and we would have coverage over a large, heterogeneous collection of high-performance models varying in size, architectures, and pretraining data distribution. Despite these advantages, using existing models for pretraining data selection is challenging, as the training data for these models are often unknown and heterogeneous. Our key observation is that data selection can be done using two observable features of all public models today: 1) all openweight models produce a causal language modeling loss for a given text, and 2) all of them can be evaluated on benchmarks. Prior work has found systematic relationships between web corpus loss and benchmark performance <ref type="bibr" target="#b48">(Wei et al., 2022)</ref>, which suggests the possibility of using correlations between perplexity and benchmark scores as the basis for a data selection policy.</p><p>In the present paper, we pursue this possibility and find a radically simple approach that is also effective: we select data via perplexity correlations (Figure <ref type="figure" target="#fig_4">1</ref>), where we select data domains (e.g. We want to pretrain on domains where lower loss is generally correlated with higher downstream performance. Our approach does this by taking public, pretrained LLMs and measuring correlations across their log-likelihoods (left, red matrix) and performance on a target benchmark (center, blue vector). We then perform data selection by picking domains with high correlation and training a fastText classifier that distinguishes these domains from others. This approach is on par with the best-known data selection methods in our experiments, despite requiring no human selection of high-quality domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Domains</head><p>wikipedia.org, stackoverflow.com, etc.) for which LLM log-probabilities are highly correlated with downstream benchmark performance. To enable our approach, we complement our algorithm with a statistical framework for correlation-based data selection and derive correlation estimators that perform well over our heterogeneous collection of LLMs.</p><p>We validate our approach over a large collection of pretrained causal LLMs on the Hugging Face Open LLM Leaderboard <ref type="bibr" target="#b3">(Beeching et al., 2023)</ref> and find that perplexity correlations are often predictive of an LLM's benchmark performance. More importantly, we find that these relationships are robust enough to enable reliable data selection that targets downstream benchmarks. In controlled pretraining experiments at the 160M parameter scale on eight benchmarks, our approach strongly outperforms DSIR <ref type="bibr">(Xie et al., 2023b</ref>) (a commonly used training-free data selection approach based on n-gram statistics) while generally matching the performance of the best overall method validated at scale by <ref type="bibr">Li et al.</ref> (the OH-2.5 +ELI5 fastText classifier <ref type="bibr" target="#b23">(Joulin et al., 2016)</ref>) without any parameter tuning or human curation.</p><p>Our manuscript focuses on pretraining experiments at the 160M scale. To assess the sensitivity of our experimental results to the choice of pretraining data pool, scale, and experiment design, we use this paper also as a preregistration for further pretraining experiments using different data sources and evaluation benchmarks at the 1.4B model scale. We commit to updating the arXiv version of our manuscript with both positive and negative results on these preregistered validation experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>To go beyond the status quo of deduplication, perplexity filtering, and hand-curation <ref type="bibr" target="#b26">(Laurençon et al., 2022;</ref><ref type="bibr" target="#b5">BigScience, 2023;</ref><ref type="bibr" target="#b0">Abbas et al., 2023;</ref><ref type="bibr" target="#b18">Groeneveld et al., 2024;</ref><ref type="bibr" target="#b41">Soldaini et al., 2024;</ref><ref type="bibr" target="#b34">Penedo et al., 2024;</ref><ref type="bibr">Llama Team, 2024)</ref>, targeted methods have been proposed to filter pretraining data so that the resulting LLM will achieve higher scores on given benchmarks. There are lightweight approaches that use n-gram overlap <ref type="bibr">(Xie et al., 2023b)</ref> or embedding similarity <ref type="bibr" target="#b13">(Everaert &amp; Potts, 2024)</ref> to select training data that is similar to data from a given benchmark. There are also less-scalable methods that require training proxy LLMs on different data mixtures <ref type="bibr" target="#b22">(Ilyas et al., 2022;</ref><ref type="bibr">Xie et al., 2023a;</ref><ref type="bibr" target="#b12">Engstrom et al., 2024;</ref><ref type="bibr" target="#b28">Liu et al., 2024;</ref><ref type="bibr">Llama Team, 2024)</ref>.</p><p>Given the high costs of proxy-based data selection methods, they have primarily been used to select among human-curated pretraining data mixtures <ref type="bibr">(Llama Team, 2024;</ref><ref type="bibr" target="#b27">Li et al., 2024)</ref> rather than a high dimensional space of mixtures. Our work takes an orthogonal approach and builds upon recent observational studies that have found scaling relationships that hold across collections of uncontrolled and diverse LLMs <ref type="bibr" target="#b30">(Owen, 2024;</ref><ref type="bibr" target="#b39">Ruan et al., 2024)</ref>. While these studies do not examine loss-to-performance relationships or derive useful data selection methods from them, we know that losses and performance are generally highly correlated. Validation losses on samples of text corpora are commonly used as a proxy for downstream performance when comparing LLMs pretrained on the same data distribution <ref type="bibr" target="#b25">(Kaplan et al., 2020;</ref><ref type="bibr" target="#b21">Hoffmann et al., 2022;</ref><ref type="bibr" target="#b48">Wei et al., 2022)</ref>, even if they have different architectures <ref type="bibr" target="#b37">(Poli et al., 2023;</ref><ref type="bibr" target="#b35">Peng et al., 2023;</ref><ref type="bibr" target="#b19">Gu &amp; Dao, 2024)</ref>.</p><p>According to a recent survey of data selection approaches by <ref type="bibr" target="#b27">Li et al. (2024)</ref>, the heavier-weight pretraining data selection methods have not yet shown large gains, and the current state-of-the-art across many tasks is still primitive: a fixed fastText classifier <ref type="bibr" target="#b23">(Joulin et al., 2016)</ref> combined with an English filter as a final layer after extensive deduplication and filtering. Are we missing important information that we can efficiently extract from a diverse collection of already trained models, larger and more diverse than any single organization is likely to produce? We show promising evidence supporting this hypothesis -simple loss-performance correlation coefficients are effective when used for data selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROBLEM SETTING</head><p>Our goal is to build predictive models of how pretraining data distributions affect downstream benchmark performance and use them to build better language models. Unfortunately, this task is challenging and computationally expensive. A standard approach adopted in paradigms such as datamodeling <ref type="bibr" target="#b22">(Ilyas et al., 2022)</ref> is to obtain N different pretraining distributions <ref type="bibr">domains (e.g. arxiv.org, stackoverflow.com, etc.)</ref>, pretrain and measure model errors on a target benchmark y i ∈ [0, 1], and fit a model p → y. This approach requires N LLM training runs, performed at a scale sufficient to obtain non-random performance on y. This can cost tens to hundreds of millions of dollars for hard benchmarks such as MMLU, where even the performance of 1B parameter LLMs often do not exceed random chance <ref type="bibr" target="#b3">(Beeching et al., 2023)</ref>.</p><formula xml:id="formula_0">{p i : i ∈ [N ], p i ∈ R + 0 D } over D ≫ N</formula><p>Instead, our work considers the following observational setting that requires no training. We obtain N pretrained, high-performance LLMs that vary in pretraining data, tokenizer, architecture, and scale (e.g. models on Huggingface's OpenLLM leaderboard). Now, if we could train a predictor p → y on these N models, we could avoid large scale model training. Unfortunately, this is impossible as the training data for these models is often proprietary, and so we have no knowledge of p.</p><p>The key observation of our work is that we can replace p i,j (the unobserved sampling probability of model i's data selection policy on document j) with an observable surrogate x i,j , which is the negative log-likelihood of document j under model i. <ref type="foot" target="#foot_0">1</ref> We can then build a regression model that relates negative log-likelihood x i and benchmark error y i . Using this model, we can select pretraining data from domains j for which decreasing the loss x i,j is predicted to rapidly decrease error y i .</p><p>The perplexity-performance hypothesis. We formulate the task of predicting errors y i from negative log-probabilities x i as a single-index model (SIM),</p><formula xml:id="formula_1">y i = f (⟨θ * , x i ⟩ + ϵ i )<label>(1)</label></formula><p>where f : R → R is some unknown monotonically increasing univariate function, ϵ i is zero-mean noise which is independent of x, and θ * ∈ R D are unknown weights over D domains.</p><p>A single index model is highly flexible (due to the arbitrary, monotone f ) and has the advantage that we do not need to estimate the nonlinear function f if our goal is to optimize model performance. We can see this directly from the monotonicity of f as</p><formula xml:id="formula_2">⟨θ * , x i ⟩ + ϵ i &lt; ⟨θ * , x j ⟩ + ϵ j ⇐⇒ f (⟨θ * , x i ⟩ + ϵ i ) &lt; f (⟨θ * , x j ⟩ + ϵ j ).<label>(2)</label></formula><p>Data selection from perplexity correlations. The weights θ * tell us which domain perplexities are correlated with downstream performance. However, this isn't sufficient for data selection. Even if we know how model likelihoods relate to model performance, we do not know how data selection affects likelihoods. Even worse, this data mixture to likelihood relationship cannot be learned observationally, as we do not know the data mixture of any of our models.</p><p>Despite this, we show that there is a clean approach for optimizing the data mixture. Our core observation is the following: if we find a nonnegative θ * , sampling proportional to θ * is always a good choice. More formally, we see that this sampling distribution defines the pretraining loss such that optimizing the training loss directly optimizes the downstream task via the single index model.</p><p>Proposition 1 Suppose that θ * weights are non-negative. Then, for models with associated likelihoods x ∈ X ⊂ R D , the minimizer of the pretraining loss over the θ * sampling distribution E j∼θ * [x j ] also has the lowest expected downstream error according to the single index model:</p><formula xml:id="formula_3">arg min x∈X E j∼θ * [x j ] = arg min x∈X E[f (⟨θ * , x⟩ + ϵ)].</formula><p>This observation follows directly from the fact that we can normalize any non-negative θ * into a distribution (and shift the normalization constant into f ) which allows us to write the inner product in the single-index model as a monotone function of the expected pretraining loss:</p><formula xml:id="formula_4">y = f (⟨θ * , x⟩ + ϵ) = f (E j∼θ * [x j ] + ϵ).<label>(3)</label></formula><p>Proposition 1 allows us to entirely avoid the task of finding the optimal data mixture for a target likelihood. Instead, we pick sampling distributions that make the pretraining loss a monotone function of the predicted downstream error. Afterward, we can rely on our ability to optimize the loss to optimize downstream performance.</p><p>This view gives us a straightforward roadmap for data selection in the remainder of the paper: estimate a set of domains where loss and downstream benchmark performance is highly correlated, and then constrain our θ * estimates to be a pretraining data sampling distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHODS</head><p>We now describe the details of our approach, starting by presenting the algorithm itself and the intuitions behind it, followed by a more precise and mathematical justification for the various steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">ALGORITHM</head><p>Estimating θ * . The parameter θ * j measures the relationship between log-likelihoods in domain j and downstream performance. Because of this, we might naturally expect θ * j to be related to nonlinear correlation coefficients between x and y. Our work uses a simple correlation measure,</p><formula xml:id="formula_5">γ j = 1≤k,l≤n k̸ =l sign(y k -y l )(rank j (x k,j ) -rank j (x l,j ))</formula><p>where rank j (x) is the rank of x among {x 1,j . . . x N,j }. This formula is intuitive: when model k does better than model l, what percentile is model k's log-likelihood compared to model l's? While this is not the only correlation coefficient that performs well (see Appendix E), this functional form has the additional benefit of being a principled estimate of θ * . In particular, we show in sections below that in expectation, the ranking of domains in γ exactly matches those of θ * (under standard high-dimensional regression assumptions; see Section 4.2 for a complete discussion).</p><p>Selecting pretraining data. Suppose that we have an accurate estimate γ j which is nonnegative. In this case, we could use γ j directly as a data selection procedure and Proposition 1 would ensure that minimizing the population pretraining loss minimizes downstream errors. Unfortunately, γ j can be negative and the finite number of tokens per domain can make it difficult to minimize the population pretraining loss. Thus, we must project γ j onto the set of reasonable pretraining data distributions that are nonnegative and account for the per-domain token counts.</p><p>What is a good way to project a set of domain rankings estimated via γ into a sampling distribution? Intuitively, if wikipedia.org has a γ j = 0.5 and arxiv.org is γ k = 0.9, it would be natural to select tokens in order of γ, preferring tokens from arxiv.org over tokens from wikipedia.org when selecting pretraining data.</p><p>Having established the ordering of domains, the remaining question is how many tokens we take for each domain. We follow recent observations that repeating data degrades performance <ref type="bibr" target="#b0">(Abbas et al., 2023)</ref> to arrive at a simple selection algorithm: select domains in greatest to least γ, taking all the tokens in each domain once, until we exhaust our total pretraining token budget.</p><p>Full algorithm. Together, these steps result in a simple, parameter-free algorithm that calculates our rank correlation coefficient, and selects domains in order from largest to smallest coefficient. We show this process explicitly in Algorithm 1, and additionally show an extra step where we train a fastText <ref type="bibr" target="#b23">(Joulin et al., 2016)</ref> classifier (using standard settings and bigram features from <ref type="bibr" target="#b27">Li et al. (2024)</ref>) which distinguishes our selected documents and domains from the rest of the pool. The fastText classifier allows us to perform data selection at a single-page level, and scale the selection process to larger datasets. We also found the classifier to slightly improve downstream performance over directly selecting the documents. More information on the specifics of the data selection approaches that we tested is given in Appendix D. Output: Target token counts per domain t ∈ N D 0 , a fastText classifier to filter pretraining data.</p><formula xml:id="formula_6">Initialize: γ ← 0 ∈ R D , t ← [0 . . .] ∈ N D 0 , counter ← 0. r 0 , r 1 , . . . , r N ← rank(x 0 , x 1 , . . . , x N ) ▷ 1. Compute the γ correlation coefficient for i, j ∈ 0 to N do γ ← γ + sign(y i -y j ) • (r i -r j ) for i ∈ArgSort(γ, descending=True) do ▷ 2. Select most to least correlated domains t i ← min(a i , b -counter) counter ← counter + a i if counter ≥ b then Break classifier = trainFastText(positive = 1 t&gt;0 , negative = 1 t=0 ) Return t, classifier 4.2 THEORY</formula><p>We now study the approach closely and show that our choices for the correlation coefficient and projection step are extensions of the classic, high-dimensional single index model estimator of <ref type="bibr" target="#b36">Plan et al. (2016)</ref>. We describe the basic single-index model estimators first, describe our extensions, and then conclude with a discussion on how our estimator and results deviate from the theory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">HIGH-DIMENSIONAL ESTIMATION OF SINGLE INDEX MODELS</head><p>For our theory, we consider the standard high-dimensional regression setting of <ref type="bibr" target="#b36">Plan et al. (2016)</ref> and <ref type="bibr" target="#b9">Chen &amp; Banerjee (2017)</ref>. Here, our goal is to estimate the unknown weights θ * in a single-index model y i = f (⟨θ * , x i ⟩ + ϵ i ), with x i ∼ N (0, I) for ∥θ * ∥ 2 = 1 (assumed without loss of generality, as ∥θ * ∥ 2 can be absorbed by f ).</p><p>Our starting point is the classic result of <ref type="bibr" target="#b36">Plan et al. (2016)</ref>, who showed</p><formula xml:id="formula_7">E [y k x k ] = cθ * ,<label>(4)</label></formula><p>for some positive constant c and 1 ≤ k ≤ N . Closely related is the result of Chen &amp; Banerjee (2017) who showed a robust estimator quite similar to ours,</p><formula xml:id="formula_8">E [sign(y k -y l )(x k -x l )] = βθ * (5)</formula><p>for any 1 ≤ k, l ≤ N (where k ̸ = l) and some positive constant β. Both of these results clearly identify that for the high-dimensional single-index model in the Gaussian setting, generalized correlation coefficients provide consistent estimates of the true regression coefficient θ * .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">DERIVING OUR ESTIMATOR</head><p>Both Plan et al. and Chen &amp; Banerjee provide moment-matching style estimators that consistently recover θ * in high-dimensional, sparse settings. However, we found that both estimators directly use the values of x, and this resulted in brittle estimates due to outliers in language model loglikelihoods. While outlier removal is one possibility, we found that a simpler approach was to robustify the estimator of <ref type="bibr" target="#b9">Chen &amp; Banerjee (2017)</ref> to outliers in x.</p><p>Recall that our estimate γ is a U-statistic, defined as pairwise sums of</p><formula xml:id="formula_9">sign(y i -y j )(Φ(x i ) -Φ(x j )),<label>(6)</label></formula><p>for any 1 ≤ i, j ≤ N (where i ̸ = j), where Φ is the empirical CDF of the x values. This estimate is significantly less sensitive to outliers than that of <ref type="bibr" target="#b9">Chen &amp; Banerjee (2017)</ref>, as the empirical CDF is bounded between zero and one, and no single model can make the estimator degenerate.</p><p>We study this estimate theoretically in the Gaussian setting, where we consider the asymptotically equivalent estimator with Φ as the CDF of the standard Gaussian. In this case, we can show that this modified estimator is also consistent in recovering θ * .</p><p>Theorem 1 When ϵ ∼ N (0, σ 2 ), we have:</p><formula xml:id="formula_10">E[sign(y i -y j )(Φ(x i ) -Φ(x j ))] = 2 π sin -1 θ * 2 √ 1 + σ 2 . (<label>7</label></formula><formula xml:id="formula_11">)</formula><p>We provide the proof in Appendix A. Because we assume ||θ * || 2 = 1 and the expected value in Equation 7 must be between -1 and 1, we are always within the domain of sin -1 and able to invert it. After inverting, we get:</p><formula xml:id="formula_12">θ ∝ sin π 2 E [sign(y i -y j )(Φ(x i ) -Φ(x j ))]<label>(8)</label></formula><p>as an estimate for θ * , where the constant 2 √ 1 + σ 2 term due to noise has been dropped.</p><p>Beyond the fact that our estimator is consistent, we can show an even tighter connection to the Chen &amp; Banerjee estimator and show that our estimates agree when running the original estimator on rank-transformed data. More specifically, for two models x i and x j with the estimated model rankings ⟨ θ, x i ⟩ &gt; ⟨ θ, x j ⟩, the expected ranking under rank-transformation (i.e. Φ(x)) match this ranking.</p><p>Corollary 1 Suppose that θ is any vector of fixed weights and x ∼ N (0, I). Then, conditioning on the event ⟨ θ,</p><formula xml:id="formula_13">x i ⟩ &lt; ⟨ θ, x j ⟩, we have ⟨ θ, E[Φ(x i ) | ⟨ θ, x i ⟩ &lt; ⟨ θ, x j ⟩]⟩ &lt; ⟨ θ, E[Φ(x j ) | ⟨ θ, x i ⟩ &lt; ⟨ θ, x j ⟩]⟩.<label>(9)</label></formula><p>with probability 1.</p><p>This proof follows from the same calculations as Theorem 1 and is given in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">SELECTING DATA FOR PRETRAINING</head><p>Recall that our algorithm for data selection is to constrain γ to be a valid sampling distribution (nonnegative, at the very least) and then sample directly from this estimate. For now, we focus on constraining θ, and we will see at the end of this section that we can apply the same constraint to γ directly to get the same result. The theory of constrained estimation for θ is simple and well-understood, with both Plan et al. ( <ref type="formula">2016</ref>) and Chen &amp; Banerjee (2017) extensively studying the problem of estimating θ under a known convex constraint set C. In particular, <ref type="bibr" target="#b36">Plan et al. (2016)</ref> show that performing a L 2 projection via θproj = arg min θ∈C ∥θ -θ∥ 2 provides improved convergence rates that depend on the Gaussian mean width of C rather than the ambient dimension, and Chen &amp; Banerjee (2017) show similar results when maximizing the linear correlation θproj = arg min θ∈C⊆B D -⟨θ, θ⟩.</p><p>We take a similar approach here. We define a convex constraint set C that forces θ to be a reasonable sampling distribution and find the best sampling distribution via the linear correlation approach.</p><p>We define C as the combination of two sets of constraints. First, we must have a valid sampling distribution, so we constrain θ to lie in the simplex. As we noted above, it is well-known that duplicating data harms performance <ref type="bibr" target="#b0">(Abbas et al., 2023)</ref>, and so we constrain θ to avoid data duplication by limiting the maximum weight on domains. Concretely, if want to pretrain on m tokens overall,</p><formula xml:id="formula_14">we enforce θ * i ≤ τ i , ∀i ∈ [1, D]</formula><p>, where τ i is set so τ i m is the number of tokens from the i-th domain that we can access for training.</p><p>The resulting linear program has a simple solution and takes the form of initializing θproj to 0 and then iterating through the values in θ from largest to smallest, setting the value at the corresponding index of θproj to the maximum allowable value, until θproj sums to 1 (see Appendix B for a proof).</p><p>Theorem 2 Suppose we want to solve: θproj = arg min θ∈R D</p><p>-⟨θ, θ⟩, subject to:</p><formula xml:id="formula_15">D i=1 θ i = 1 0 ≤ θ i ≤ τ i , ∀i ∈ [1, D],</formula><p>where τ i &gt; 0 are fixed values. Then, the solution is:</p><formula xml:id="formula_16">θproj k =      τ k if j: rj ( θj )≥r k ( θk ) τ j ≤ 1 1 -j: rj ( θj )&gt;r k ( θk ) τ j if j: rj ( θj )≥r k ( θk ) τ j ≥ 1 ∧ j: rj ( θj )&gt;r k ( θk ) τ j ≤ 1 0 otherwise , (<label>10</label></formula><formula xml:id="formula_17">)</formula><p>where r is some function that breaks all ties between θj and θk for k ̸ = j, and otherwise leaves the ordinal relationships the same.</p><p>We note that while the use of this linear program is in line with the constrained estimators proposed in Chen &amp; Banerjee (2017), the L 2 projection is arguably more natural, and does not require assuming that ∥ θ∥ 2 = 1 for asymptotic recovery conditions. We derive similar closed-form expressions for this quadratic case in Appendix B, but do not use this approach for two separate reasons.</p><p>First, the L 2 projection depends on the L 2 norm of θ, unlike the linear program which only depends on the ranks of the values in θ. The challenge with determining the norm is that the exact recovery result in Equation ( <ref type="formula" target="#formula_10">7</ref>) requires knowledge of the noise level, and the trigonometric functions rely strongly on the Gaussian structure of x. Because of this, we are unlikely to be able to estimate the norm of θ with any accuracy, and the only way to avoid this would be to treat the norm as a hyperparameter, which adds unnecessary complexity. The second reason is empirical (although possibly a consequence of the first) -we found that the linear projection performed better across a wide range of benchmarks and conditions (See Appendix E).</p><p>We conclude by relating our theory to the full algorithm in Section 4.1. The estimation step for γ is the finite sample, U-estimate of the expectation in Equation ( <ref type="formula" target="#formula_12">8</ref>), dropping the nonlinear transform sin and π/2 as these two terms do not change the rankings of the domains. The data selection step directly applies our projection in Equation ( <ref type="formula" target="#formula_16">10</ref>), and we make use of the fact that this projection only relies on rankings among the domains to use γ rather than an exact estimate for θ * .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">ALTERNATIVE METHODS</head><p>Our estimator is far from the only reasonable high-dimensional, single-index model estimator. We briefly discuss some alternatives and the tradeoffs involved before moving to experimental results.</p><p>We could use classic low-dimensional methods regularized for the high-dimensional setting. This includes ordinal regression (Wooldridge, 2010) and the isotron algorithm <ref type="bibr" target="#b24">(Kalai &amp; Sastry, 2009)</ref>.</p><p>We found these methods to underperform correlation-based estimators, and tuning hyperparameters added additional complexity that was not needed in the correlation-based approaches.</p><p>Another class of methods involve scaling laws <ref type="bibr" target="#b25">(Kaplan et al., 2020;</ref><ref type="bibr">Llama Team, 2024;</ref><ref type="bibr" target="#b39">Ruan et al., 2024)</ref>. We could transform the y values via an inverse sigmoid or power law, and fit highdimensional linear regression methods (e.g. ridge, partial least squares, or Lasso). We initially found this approach promising, but the inverse transforms were unstable, and the combination of fitting the nonlinear transform and regularization required significant amounts of tuning.</p><p>Rank-correlation methods, including our robustified version of the estimator from Chen &amp; Banerjee (2017), and even the standard Spearman correlation <ref type="bibr" target="#b42">(Spearman, 1904)</ref> (see Appendix E) performed well. We believe that in general, robust per-feature correlations are likely to perform well as D ≫ N , and extreme levels of regularization are needed to obtain reasonable models. Sparse methods such as the Lasso <ref type="bibr" target="#b44">(Tibshirani, 1996)</ref> are one classic answer, but we cannot necessarily assume that the underlying correlations θ * are sparse, and we did not find these techniques to perform well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS</head><p>We empirically validate our approach to predicting downstream performance and data selection. Our validation consists of three sets of experiments: we first pretrain 160M-parameter LLMs from scratch to study our primary goal of selecting pretraining data to improve downstream performance, followed by analyzing the ability of losses to predict downstream performance, and conclude with an analysis of the loss matrix X. Throughout our experiments, we use the same single-index model that we train using Algorithm 1. As shown in the algorithm, we train the fastText classifier on selected vs unselected domains and use the classifier to filter the pretraining data at the page-level.</p><p>Input data matrix X. To build the input data matrix, X, we collected byte normalized loss values from a sample of 90 Open LLM Leaderboard <ref type="bibr" target="#b3">(Beeching et al., 2023)</ref> LLMs that we could run without errors. Concretely, these values are defined as bits-per-byte L T ℓ L B ln(<ref type="foot" target="#foot_1">foot_1</ref>) where L T is the token count, L B is the number of UTF-8 bytes, and ℓ is the per-token cross-entropy <ref type="bibr" target="#b15">(Gao et al., 2020)</ref>. We collected these values on the smallest sample provided by the RedPajama V2 (RPJv2) dataset (Together Computer, 2023) for all domains with ≥ 25 pages in the sample. Specifically, we used the "sample" subset 2 of RPJv2 resulting in 9,841 domains/features. Specifics of how we computed losses are in Appendix C.</p><p>Target benchmark performance y. We constructed a target vector, y, for LAMBADA <ref type="bibr" target="#b31">(Paperno et al., 2016)</ref>, ARC Easy <ref type="bibr" target="#b10">(Clark et al., 2018)</ref>, PIQA <ref type="bibr" target="#b6">(Bisk et al., 2020)</ref>, and SciQ <ref type="bibr" target="#b49">(Welbl et al., 2017)</ref>. These are all of the tasks reported in the Pythia scaling experiments for which a model in the 160M parameter range could meaningfully perform above chance. We also constructed target vectors for LAMBADA IT , LAMBADA FR , LAMBADA DE , and LAMBADA ES , which are subsets of LAMBADA translated into Italian, French, German, and Spanish by <ref type="bibr" target="#b7">Black (2023)</ref>. These languages match those in RPJv2 where each page is conveniently tagged as one of five languages: English, Spanish, French, German, and Italian. The correspondence between our target benchmark languages and the RPJv2 metadata will be convenient for us, as it will allow us to include language filtering baselines at no additional cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">PRETRAINING</head><p>We begin by validating our algorithm in the end-to-end task of pretraining data selection with controlled experiments at the 160M parameter, 3.2B token scale. The low compute requirements of this setting allowed us to more extensively study replicates and ablations in Appendix E within the timeframe of a few days. We also note that while 160M models are small, this is far from an easy setting for our data selection algorithm. Most of the Open LLM Leaderboard models are 10 to 100× larger than the 160M scale, and our single index model must extrapolate substantially from ≈7B scale models to our small-scale validation setting (see Appendix G for a histogram of model sizes). Finally, the focus on small-scale validation in this manuscript allows us to commit to a clear preregistration-based strategy toward scaling up (Section 6) rather than cherry-picking experiments that succeed at scale.</p><p>Pretraining data and setting. For pretraining, we used the "sample-100B" subset of RPJv2. This is larger than the sample that we used to compute our estimate. We filtered this data so it contains only the domains used for our estimate, and then tokenized the data with the Pythia tokenizer. The vast majority of the domains from our BPB matrix were present in this larger sample of text. However, 42 (out of 9,841) were not, and so we removed them from our estimate. For every data selection method that we tested, the task was to further select 3.2B tokens for pretraining, which is Chinchilla-optimal <ref type="bibr" target="#b21">(Hoffmann et al., 2022)</ref> for the 160M-parameter LLM used in our tests.</p><p>Table <ref type="table">1</ref>: Average rankings of each data selection method (lower is better) across 8 benchmarks shows that correlation-based filtering beats baselines by a wide margin, and matches the current best open data filter from <ref type="bibr" target="#b27">Li et al. (2024)</ref>. Our approach significantly beats the default filter in <ref type="bibr" target="#b27">Li et al. (2024)</ref> with the EN filter and loses slightly after additional manual language filtering that depends on the target task (+ manual Lang Filter).</p><p>Method None Lang Filt DSIR (Xie et al., 2023b) Handcrafted fastText + EN Lang Filter (Li et al., 2024) Handcrafted fastText w/o Lang Filter Handcrafted fastText + manual Lang Filter Perplexity Correlations Avg. Rank 3.750 4.000 4.500 3.750 3.250 1.375 1.750</p><p>Baselines. We compare against several baseline data-selection methods. First, we present the results of uniformly sampling from the available pretraining data. Then we use the language tags present in RPJv2 to filter only for the language matching the target task. In addition to these commonsense baselines, we also run DSIR (Xie et al., 2023b): a lightweight training data selection technique based on n-gram overlaps that <ref type="bibr" target="#b27">Li et al. (2024)</ref> found to be competitive with proxy LLM-based techniques and was also validated at scale <ref type="bibr" target="#b32">(Parmar et al., 2024)</ref>. Finally, we run the state-of-the-art method for pretraining data quality filtering found by Li et al., which is a fastText classifier that beats all of the heavier-weight proxy-LLM methods that were tested. The classifier was trained on a benchmarkagnostic and handcrafted objective, which is to classify data as coming from Common Crawl<ref type="foot" target="#foot_2">foot_2</ref> (low quality) or OH2.5 <ref type="bibr" target="#b43">(Teknium, 2023)</ref> and Reddit ELI5 (Fan et al., 2019) (high quality). It is combined with an English filter in Li et al., and so we present results for this fastText filter with and without the additional English filter. Model and hyperparameters. We use the Pythia 160M LLM configuration from Biderman et al. (2023) and optimize the hyperparameters including learning rate, weight decay, and warmup to minimize loss on the uniform sampling (no selection algorithm) baseline. Training hyperparameters were fixed across all methods. We provide additional training and evaluation details in Appendix D.</p><p>Results. We report average rankings over all benchmarks in Table <ref type="table">1</ref>, and we find that our approach significantly outperforms the basic baselines of random sampling, language filtering, and DSIR. Compared to the existing state of the art from <ref type="bibr" target="#b27">Li et al. (2024)</ref>, our approach beats the performance of the default, English-filtered fastText classifier, but loses slightly once we add in a manual language filtering step to enable better performance on the multilingual LAMBADA datasets. For the maintext comparisons, we use the optional fastText classifier from our algorithm to select pretraining data at the page levels, but we show ablations without the classifier in Appendix E.</p><p>Figure <ref type="figure">2</ref> shows how each data selection method affects benchmark performance in more detail. Each block of rows represents a data selection method, while an individual row represents an LLM within a method that targets a particular benchmark or set of benchmarks. Columns represent benchmarks. We see that language filtering and perplexity correlations both clearly optimize for the target benchmark -within each block, the benchmark column matching each row typically performs best. Surprisingly, the pattern is much less obvious for DSIR -the heatmap looks much more uniform across LLMs with different task targets. We also see that while language filtering has significant impacts on model performance, our performance significantly exceeds the impact of language filtering across all tested benchmarks.</p><p>Figure <ref type="figure">3</ref> shows the distribution of languages in pretraining data selected by our method, targeting each benchmark. Our algorithm provides significant enrichment of the corresponding languages for the multilingual benchmarks (LAMBADA_*), but we also find that it does not exclusively select domains in one language. In contrast, for English benchmarks our approach selects nearly exclusively English data, likely due to the large quantity of high-quality English data in our pretraining data pool. There are significantly fewer tokens in non-English languages in the pretraining data pool and our τ constraint to prevent duplication has a large impact on the weights when the benchmarks are non-English. We provide the same figure when the τ values are made 5× as large in Appendix F.</p><p>Finally, we note that our results are somewhat insensitive to the specifics of the perplexity-correlation procedure we present in Algorithm 1. We show in Appendix E that varying the projection method (linear, L 2 ) and even using Spearman rank correlations <ref type="bibr" target="#b42">(Spearman, 1904)</ref> often work better than the baselines. These results suggest that the performance of our approach is not tightly dependent on Figure <ref type="figure">2</ref>: Pretraining results with different data selection methods. Each row is an LLM, and each column is a task. The number in the upper left indicates the ranking of the method when targeting that benchmark compared to other methods (lower is better). Numbers within the heatmap denote accuracy for all benchmarks except the LAMBADA tasks for which the values are log perplexities (where lower scores are better). We find that our approach appropriately optimizes data mixes for the target language and benchmark, and matches the fastText baseline across most benchmarks.</p><p>the precise form of the estimator that is coupled to our theory results but holds more broadly across general perplexity-correlation relationships. Additionally, our approach performs better with the optional fastText classifier that our algorithm trains, possibly because it can operate at the page-level instead of the domain-level<ref type="foot" target="#foot_3">foot_3</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">PERFORMANCE RANK PREDICTIONS</head><p>We have shown that our approach succeeds at the goal of selecting useful pretraining data, but how good are single index model's predictions? An accurate map between loss to benchmarks would be Figure <ref type="figure">3</ref>: Language distributions of pretraining data selected by perplexity correlations. The default RPJv2 distribution is given in the left column for reference. The English benchmark targets often exclusively select English but the reverse is not the case. In every case, our approach selects more data than the default from the benchmark-matched language (shown as a green box in each column). helpful in selecting among candidate pretraining data mixtures generally, even when not using our pretraining data selection algorithm.</p><p>Comparing model performance rankings predicted by our regression to the ground truth, we find generally accurate predictions. Figure <ref type="figure" target="#fig_2">4</ref> shows 5-fold leave-out plots for PIQA, and LAMBADA FR with the rank predictions given by ⟨ θproj , Φ(x)⟩. Every point in the plot is a held-out point: we estimated θ * five times, holding out a different 20% of the data each time, and plotted the prediction for every point when it was held out.</p><p>We find that our estimator achieves high ordinal prediction performance across all target tasks. We include 5-fold leave-out R 2 scores for all tasks in Figure <ref type="figure">5</ref>. However, we complement these strong results with the additional observation that simply taking the mean loss across all domains is a strong predictor of model performance (bottom row). The surprising effectiveness of average loss over uniformly sampled documents has been discussed extensively <ref type="bibr" target="#b30">(Owen, 2024;</ref><ref type="bibr" target="#b48">Wei et al., 2022;</ref><ref type="bibr" target="#b25">Kaplan et al., 2020)</ref> and our results further suggest that regressions with correlations only slightly above the mean loss baseline still can result in effective data selection methods.</p><p>Finally, we discuss outliers in our prediction of model performance. Our predictions are accurate for LLMs with usual architectures (e.g. Mamba <ref type="bibr" target="#b19">(Gu &amp; Dao, 2024)</ref>), the smallest/largest vocabulary sizes, context sizes, and parameter sizes. However, we also see that LLMs that were trained on unusual data are not as well predicted by our approach (e.g. Phi <ref type="bibr" target="#b20">(Gunasekar et al., 2023)</ref>). We may simply require a bigger or more diverse pretraining data pool and set of models to find estimates that work well for models that expect different styles of text.</p><p>Figure <ref type="figure">5</ref>: Held-out R 2 score of our raw correlation estimate θ, our projected estimate θproj , and the average loss baseline. The 95% bootstrapped confidence intervals are wide enough that no individual comparison is significant. Across benchmarks, θproj has statistically significant gains over the baseline (p=0.035) as it is unlikely that θproj beats mean loss 7 times out of 8 by chance. </p><formula xml:id="formula_18">(a) (b) (c) (d) (e) (f) (g) (h) (i) (j)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">ANALYSIS OF THE MODEL-LOSS MATRIX X</head><p>What information is contained in the matrix of model losses X? Clearly, it must contain semantically meaningful information about the data, such as the language that a piece of text is in. We performed PCA <ref type="bibr" target="#b33">(Pearson, 1901)</ref> and t-SNE (van der Maaten &amp; Hinton, 2008) on X and plotted the first two components for each of our 9,841 domains. As shown in the first row of Figure <ref type="figure" target="#fig_3">6</ref>, we found two components with relatively high singular values. The first component clearly corresponds with the language of a domain. The second component corresponds with the average bits-per-byte or entropy of a domain. The t-SNE components show the same general pattern as well as showing that the language clusters are very well separated. As shown in our plots, there are several salient clusters within the language clusters. Within the English cluster, we found a subcluster for luxury goods, another for legal services and information, another for academic research, and even a cluster for funeral homes.</p><p>The second row of Figure <ref type="figure" target="#fig_3">6</ref> shows plots for the loss matrix when we take the principal components of the other dimension, where points correspond to the 90 LLMs. For PCA, PC1 corresponds to entropy. For both cases, it is less clear what the other PCs are, but when we color the three largest families of models in our data (Pythia <ref type="bibr" target="#b4">(Biderman et al., 2023)</ref>, Qwen <ref type="bibr" target="#b2">(Bai et al., 2023)</ref>, and OpenLlama <ref type="bibr" target="#b17">(Geng &amp; Liu, 2023</ref>)), we see that model families are clustered together in the PC graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ROBUSTNESS CHECKS WITH PRE-REGISTRATION</head><p>In small-scale experiments, our approach is competitive with the leading approach from Li et al.'s survey: a fixed fastText model <ref type="bibr" target="#b23">(Joulin et al., 2016)</ref>, manually augmented with the best language filtering. This leading approach is heuristic and hand-crafted, requiring appropriate language filtering matched to the target benchmark and assumptions about what good pretraining data looks like. Our approach does not make these assumptions and could potentially improve as more public models are released and we have better data to estimate θ * .</p><p>While our results are generally positive, many past data selection methods have reported initially positive results, only to later break: they may fail to scale to larger models or rely on specific details of their experimental setting. Our 160M-scale experiments may also raise such concerns.</p><p>We design a pre-registered scaling experiment that addresses both the concerns of scale and external validity. We use the permanence of arXiv preprints as a mechanism to preregister a series of scaling experiments within the DataComp-LM framework <ref type="bibr" target="#b27">(Li et al., 2024)</ref>, which is a testbed for dataselection techniques released with the recent survey. Pre-registering held-out scaling experiments commits us to reporting negative results, and avoid overfitting to our chosen experimental settings.</p><p>DataComp-LM is ideal for this preregistered scaling experiment, as it standardizes the setting by providing a pool of 240 trillion tokens, pretraining code for 412M to 7B parameter models, and evaluation code for 53 benchmarks, 22 of which are labelled as "core" benchmarks that scale predictably. Importantly, we have not trained any models on DataComp-LM using our methods or baselines, making this a true held-out experiment with known high-performance baselines.</p><p>Preregistered experiment. We will run the best-performing approach from our paper: a fastText filter trained on our correlation estimator. We will define the target benchmark for our estimator as the average of the "core" DataComp-LM benchmarks and run our estimator with perplexities from our set of 90 OpenLM Leaderboard LLMs on a uniform subsample of the DataComp-LM pool of data. We will use the provided DataComp-LM code for training LLMs for the "Filtering 1B-1x" track, where a 1.4B parameter LLM is trained on 28.8B tokens chosen from a 1.64T sample of the DataComp-LM pool of data. In the DataComp-LM paper, they apply their fixed fastText filter as a final step after several complicated deduplication and filtering steps. We will report results where our fastText classifier is used as a direct substitute for this last step alone, as well as another test in which we replace the entire pipeline with one classifier. We will report results where our estimator is trained at the domain-level (following this paper) and where our estimator is trained at the pagelevel (which we have not tried yet). Finally, we will report analogous results where we replace the "core" evaluation score with the average score on all of the non-English LAMBADA translations, and compare the raw fastText classifier from <ref type="bibr" target="#b27">Li et al. (2024)</ref> to our approach, using both of these approaches in place of the full filtering pipeline from 1.64T tokens. We preregister this additional multilingual task because "core" does not include multilingual evaluations.</p><p>We chose the "Filtering 1B-1x" track because it is the most compute-intensive track that we can perform within a couple weeks, given our resources. For these experiments, the compute needs for our data selection procedure are negligible compared to LLM pretraining. Per <ref type="bibr" target="#b27">Li et al. (2024)</ref>, the pretraining for these experiments combined is estimated to take 240 hours on an 8-GPU H100 node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>Does high-performance data selection require careful hand-crafted heuristics or prohibitively expensive model training runs? Our work demonstrates an alternative, viable approach -leveraging existing, public models as a source of information for data selection. Pretraining experiments suggest that a simple, correlation-based approach to selecting data can be effective, but more broadly, we show how to 1) use single-index models as a surrogate for downstream performance and 2) build models that relate losses to downstream performance and use these surrogates effectively in data selection. Finally, we propose pre-registered scaling experiments on held-out data to test external validity of reported results. We hope this type of experiment will help improve the trustworthiness of experimental results for data selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A ESTIMATOR SOLUTION</head><formula xml:id="formula_19">A.1 LEMMA 1 Statement of Lemma 1 Define the PDF of HalfNormal as f (x; σ) = √ 2 σ √ π e -x 2</formula><p>2σ 2 for x &gt; 0 and 0 otherwise. Now, suppose:</p><formula xml:id="formula_20">• β is a vector with ||β|| 2 = 1 • Z 1 , Z 2 are vectors ∼ N (0, I) • ϵ ∼ N (0, σ 2 ) • Z ′ ∼ N (0, 1) • Z + ∼ HalfNormal(1).</formula><p>Then we have:</p><formula xml:id="formula_21">Z 1j |⟨Z 1 -Z 2 , β⟩ + ϵ &gt; 0 d = Z ′ 1 - β 2 j 2 + σ 2 + β j √ 2 + σ 2 Z + ,</formula><p>where Z 1j is the j-th entry of Z 1 .</p><p>Proof: First, note:</p><formula xml:id="formula_22">Z 1j |⟨Z 1 -Z 2 , β⟩+ϵ &gt; 0 d = Z 1j |     Z 1 Z 2 ϵ/σ     ,     β -β σ     &gt; 0 d = Z 1j |     Z 1 Z 2 ϵ/σ     ,     β -β σ     / 2 + σ 2 &gt; 0,</formula><p>where</p><formula xml:id="formula_23">    • • •   </formula><p> denotes the vector-valued result of concatenating vectors and scalars. For readability, we</p><formula xml:id="formula_24">set Z c =     Z 1 Z 2 ϵ/σ     and β c =     β -β σ     / √ 2 + σ 2 .</formula><p>Given that β c is unit-norm (by supposition, β is unit-norm), and every element of Z c is ∼ N (0, 1) (even ϵ/σ), we can easily split a conditional random vector containing Z 1j into a conditionally dependent component and independent component:</p><formula xml:id="formula_25">Z c |⟨Z c , β c ⟩ &gt; 0 d = (I -β c β ⊤ c )Z ′′ + β c Z + .</formula><p>The first term is orthogonal to β c and so it is the part of Z c that is not subject to the condition. In the unconditional case, Z c ∼ N (0, I) and so Z ′′ ∼ N (0, I). The second term is the part of Z c that is in the direction of β c . Z + ∼ HalfNormal(I) because our dot product condition is satisfied for half of the possible non-orthogonal Z c values. Now, we focus on finding Z c |⟨Z c , β c ⟩ &gt; 0 for a single index j. We have (for C defined to be the dimensionality of β c ):</p><formula xml:id="formula_26">((I -β c β ⊤ c )Z ′′ ) j + (β c Z + ) j = Z ′′ j (1 -β c 2 j ) - 1≤i≤C i̸ =j Z ′′ i β cj β ci + β j Z + j = Z ′′ j - C i=1 Z ′′ i β cj β ci + β j Z + j . Now, note that Z ′′ j - C i=1 Z ′′ i β cj β ci</formula><p>is the sum of independent zero-mean Gaussians with variances given by 1 and β c</p><formula xml:id="formula_27">2 j β c 2 i , so it itself is a zero-mean Gaussian Y ∼ N (0, 1 - C i=1 β c 2 j β c 2 i ). We can also use the fact that C i=1 β c 2 i = 1 (recall that β c is unit norm) to get: Y ∼ N (0, 1 -β c 2 j</formula><p>). So we have that the conditional Z 1j is given by:</p><formula xml:id="formula_28">Z ′ 1 -β c 2 j + β cj Z + = Z ′ 1 - β 2 j 2 + σ 2 + β j √ 2 + σ 2 Z + ,</formula><p>for Z ′ ∼ N (0, 1). As a corollary, we can see that Z 2j under the same condition is given by:</p><formula xml:id="formula_29">Z ′ 1 - β 2 j 2 + σ 2 + -β j √ 2 + σ 2 Z + . A.2 LEMMA 2</formula><p>Statement of Lemma 2 Suppose that Φ is the CDF of a standard Gaussian, a and c are constants, and Z ∼ N (0, 1). Then we have:</p><formula xml:id="formula_30">E[Φ(aZ + c)] = Φ c √ 1 + a 2 .</formula><p>Proof: By the definition of the CDF of a standard Gaussian, we have:</p><formula xml:id="formula_31">E[Φ(aZ + c)] = E[P (X ≤ aZ + c)],</formula><p>where X ∼ N (0, 1). Continuing, we have:</p><formula xml:id="formula_32">= E[P (X -aZ -c ≤ 0)].</formula><p>Now, note that X -aZ -c is the sum of independent Gaussian random variables with given mean and variance; it itself is a Gaussian random variable ∼ N (-c, a 2 + 1). To find P (X -aZ -c ≤ 0), we can evaluate its CDF at 0:</p><formula xml:id="formula_33">= E Φ c √ a 2 + 1 = Φ c √ a 2 + 1 . A.3 LEMMA 3</formula><p>Statement of Lemma 3 Suppose Φ is the standard Gaussian CDF, Z + ∼ HalfNormal(1), and b and a are constants. Then we have:</p><formula xml:id="formula_34">E Φ Z + b √ a 2 + 1 = 1 2 + 1 π tan -1 b √ a 2 + 1 .</formula><p>Proof: By the definition of expected value, we can take the following integral where f Z+ is the PDF of Z + . We integrate from 0 instead of -∞ because the PDF of the Standard Half Normal is 0 in the domain below 0:</p><formula xml:id="formula_35">E Φ Z + b √ a 2 + 1 = ∞ 0 Φ zb √ a 2 + 1 f Z+ (z)dz = ∞ 0 Φ zb √ a 2 + 1 √ 2 √ π e -z 2 2 dz = 1 √ 2π ∞ 0 e -z 2 2 dz + ∞ 0 erf zb √ 2 √ a 2 + 1 e -z 2 2 dz ( * ).</formula><p>The second integral is generally non-trivial to solve, but luckily we can solve it by using Equation <ref type="formula" target="#formula_2">2</ref>in Section 4.3 of the integral table from <ref type="bibr" target="#b29">Ng &amp; Geller (1968)</ref>, which states:</p><formula xml:id="formula_36">∞ 0 erf(cx)e -d 2 x 2 dx = √ π 2d - 1 d √ π tan -1 d c</formula><p>Where c and d are real and positive. We split the solution by cases: b &gt; 0, b = 0, and b &lt; 0. We find that in every case, we can manipulate our integral so that the solution is trivial or the constant inside the erf(•) is positive (and so we can use the integral table). In every case, we find that the solution is</p><formula xml:id="formula_37">1 2 + 1 π tan -1 b √ a 2 +1 .</formula><p>Case 1: b &gt; 0. We can use the integral table directly:</p><formula xml:id="formula_38">( * ) = 1 √ 2π √ π √ 2 + √ π √ 2 - √ 2 √ π tan -1 √ a 2 + 1 b = 1 2 + 1 2 - 1 π tan -1 √ a 2 + 1 b .</formula><p>Then, using the identity:</p><formula xml:id="formula_39">tan -1 x + tan -1 1 x = π 2 if x &gt; 0,</formula><p>we find the following:</p><formula xml:id="formula_40">= 1 2 + 1 π tan -1 b √ a 2 + 1 .</formula><p>Case 2: b = 0. Note that erf(0) = 0; we do not have to use the integral table:</p><formula xml:id="formula_41">( * ) = 1 √ 2π √ π √ 2 + 0 = 1 2 .</formula><p>Because tan -1 (0) = 0, we have:</p><formula xml:id="formula_42">= 1 2 + 1 π tan -1 b √ a 2 + 1 .</formula><p>Case 3: b &lt; 0. Because erf(•) is an odd function, we can pull the negative out:</p><formula xml:id="formula_43">( * ) = 1 √ 2π ∞ 0 e -z 2 2 dz - ∞ 0 erf z|b| √ 2 √ a 2 + 1 e -z 2 2 dz .</formula><p>Now we can use the integral table as in the b &gt; 0 case:</p><formula xml:id="formula_44">= 1 √ 2π √ π √ 2 - √ π √ 2 + √ 2 √ π tan -1 √ a 2 + 1 |b| = 1 2 + 1 2 - 1 π tan -1 √ a 2 + 1 |b| .</formula><p>We can then use the same identity again:</p><formula xml:id="formula_45">tan -1 x + tan -1 1 x = π 2 if x &gt; 0 to get: = 1 2 - 1 π tan -1 |b| √ a 2 + 1 .</formula><p>Because tan -1 is an odd function, we can put the negative inside of it:</p><formula xml:id="formula_46">= 1 2 + 1 π tan -1 b √ a 2 + 1 . A.4 FULL PROOF</formula><p>Here, we prove:</p><formula xml:id="formula_47">E[sign(y 1 -y 2 )(Φ(x 1 ) -Φ(x 2 ))] = 2 π sin -1 θ * 4 + 2σ 2 1 + 2σ 2 2</formula><p>with y 1 , y 2 , Φ(x 1 ), Φ(x 2 ), and θ * defined in the main text, for the case where ϵ 1 and ϵ 2 are zeromean Gaussian noise ∼ N (0, σ 2 1 ) and ∼ N (0, σ 2 2 ), respectively. It is easy to see that this is a more general version of the following theorem.</p><p>Theorem 1 When ϵ ∼ N (0, σ 2 ), we have:</p><formula xml:id="formula_48">E[sign(y i -y j )(Φ(x i ) -Φ(x j ))] = 2 π sin -1 θ * 2 √ 1 + σ 2 . (<label>7</label></formula><formula xml:id="formula_49">)</formula><p>Proof: By symmetry, we have:</p><formula xml:id="formula_50">E[sign(y 1 -y 2 )(Φ(x 1 ) -Φ(x 2 ))] = 1 2 E[Φ(x 1 ) -Φ(x 2 )| sign(y 1 -y 2 ) &gt; 0] + 1 2 E[-(Φ(x 1 ) -Φ(x 2 ))| sign(y 1 -y 2 ) &lt; 0].</formula><p>By increasing monotonicity of f , we have sign(y </p><formula xml:id="formula_51">1 -y 2 ) &gt; 0 ⇐⇒ ⟨x 1 -x 2 , θ * ⟩ + ϵ ∆ &gt; 0, for ϵ ∆ = ϵ 1 -ϵ 2 ∼ N (0, σ 2 1 + σ 2 2 ). So: = 1 2 E[Φ(x 1 ) -Φ(x 2 )|⟨x 1 -x 2 , θ * ⟩ + ϵ ∆ &gt; 0] + 1 2 E[-(Φ(x 1 ) -Φ(x 2 ))|⟨x 1 -x 2 , θ * ⟩ + ϵ ∆ &lt; 0].</formula><formula xml:id="formula_52">= E[Φ(x 1 ) -Φ(x 2 )|⟨x 1 -x 2 , θ * ⟩ + ϵ ∆ &gt; 0].</formula><p>By linearity of expectation:</p><formula xml:id="formula_53">= E[Φ(x 1 )|⟨x 1 -x 2 , θ * ⟩ + ϵ ∆ &gt; 0] -E[Φ(x 2 )|⟨x 1 -x 2 , θ * ⟩ + ϵ ∆ &gt; 0].</formula><p>Now, we focus on finding the overall estimate for a single index j. By Lemma 1, we have, for Z ∼ N (0, 1) and Z + ∼ HalfNormal(1):</p><formula xml:id="formula_54">Φ(x 1j )|⟨x 1 -x 2 , θ * ⟩ + ϵ ∆ &gt; 0 d = Φ(Za + Z + b 1 ).</formula><p>Here, a = 1 -</p><formula xml:id="formula_55">(θ * j ) 2 2+σ 2 1 +σ 2 2 and b 1 = θ * j √ 2+σ 2 1 +σ 2 2</formula><p>. As a corollary of Lemma 1, we can see:</p><formula xml:id="formula_56">Φ(x 2j )|⟨x 1 -x 2 , θ * ⟩ + ϵ ∆ &gt; 0 d = Φ(Za + Z + b 2 ).</formula><p>Where</p><formula xml:id="formula_57">b 2 = - θ * j √ 2+σ 2 1 +σ 2 2</formula><p>. So for the index j, our estimate is:</p><formula xml:id="formula_58">E[Φ(Za + Z + b 1 )] -E[Φ(Za + Z + b 2 )] = E[E[Φ(Za + c)|c = Z + b 1 ]] -E[E[Φ(Za + c)|c = Z + b 2 ]].</formula><p>Using Lemma 2, we have:</p><formula xml:id="formula_59">= E Φ Z + b 1 √ a 2 + 1 -E Φ Z + b 2 √ a 2 + 1 .</formula><p>Then, using Lemma 3, we have:</p><formula xml:id="formula_60">= 1 2 + 1 π tan -1 b 1 √ a 2 + 1 - 1 2 - 1 π tan -1 b 2 √ a 2 + 1 = 1 π tan -1 b 1 √ a 2 + 1 - 1 π tan -1 b 2 √ a 2 + 1 .</formula><p>Using the fact that tan -1 is an odd function and b 2 = -b 1 , we get:</p><formula xml:id="formula_61">= 2 π tan -1 b 1 √ a 2 + 1 .</formula><p>Now, we write a and b 1 in terms of θ * j :</p><formula xml:id="formula_62">= 2 π tan -1          </formula><p>.</p><p>Using the identity sin -1 x = tan -1</p><p>x √ 1-x 2 , we have:</p><formula xml:id="formula_63">= 2 π sin -1 θ * j 4 + 2σ 2 1 + 2σ 2 2 .</formula><p>A.5 COROLLARY 1</p><p>Corollary 1 Suppose that θ is any vector of fixed weights and x ∼ N (0, I). Then, conditioning on the event ⟨ θ, x i ⟩ &lt; ⟨ θ, x j ⟩, we have</p><formula xml:id="formula_64">⟨ θ, E[Φ(x i ) | ⟨ θ, x i ⟩ &lt; ⟨ θ, x j ⟩]⟩ &lt; ⟨ θ, E[Φ(x j ) | ⟨ θ, x i ⟩ &lt; ⟨ θ, x j ⟩]⟩.<label>(9)</label></formula><p>with probability 1.</p><p>To see this, we can find:</p><formula xml:id="formula_65">E[Φ(x 1 ) -Φ(x 2 )|⟨ θ, x 1 ⟩ + ϵ 1 &gt; ⟨ θ, x 2 ⟩ + ϵ 2 ] = E[Φ(x 1 ) -Φ(x 2 )|⟨ θ, x 1 -x 2 ⟩ + ϵ ∆ &gt; 0]</formula><p>Note that we have already computed this expected value in the proof above; for an index j, it is:</p><formula xml:id="formula_66">2 π sin -1 θj 4 + 2σ 2 1 + 2σ 2 2 .</formula><p>Because sin -1 is an odd function, the above expression has the same sign as θj . Because the values at every index of E[Φ(x 1 ) -Φ(x 2 )] under our condition and θ are the same sign, we have</p><formula xml:id="formula_67">⟨E[Φ(x 1 ) -Φ(x 2 )], θ⟩ &gt; 0, so ⟨ θ, E[Φ(x 1 )]⟩ &gt; ⟨ θ, E[Φ(x 2 )]⟩.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B OPTIMAL PROJECTED WEIGHTS SOLUTIONS B.1 LINEAR PROJECTION</head><p>Theorem 2 Suppose we want to solve:</p><formula xml:id="formula_68">θproj = arg min θ∈R D -⟨θ, θ⟩, subject to: D i=1 θ i = 1 0 ≤ θ i ≤ τ i , ∀i ∈ [1, D],</formula><p>where τ i &gt; 0 are fixed values. Then, the solution is:</p><formula xml:id="formula_69">θproj k =      τ k if j: rj ( θj )≥r k ( θk ) τ j ≤ 1 1 -j: rj ( θj )&gt;r k ( θk ) τ j if j: rj ( θj )≥r k ( θk ) τ j ≥ 1 ∧ j: rj ( θj )&gt;r k ( θk ) τ j ≤ 1 0 otherwise , (<label>10</label></formula><formula xml:id="formula_70">)</formula><p>where r is some function that breaks all ties between θj and θk for k ̸ = j, and otherwise leaves the ordinal relationships the same.</p><p>Proof: We proceed by considering each of the three cases from Equation <ref type="formula" target="#formula_16">10</ref>.</p><p>Case 1. Suppose for the sake of contradiction that the optimal solution is θproj and yet θproj k &lt; τ k for some θproj k falling under the first case of Equation <ref type="formula" target="#formula_16">10</ref>. Now suppose that we construct a θ ′ also satisfying the projection constraints that is the same as θproj except in these places:</p><formula xml:id="formula_71">θ ′ k = θproj k + ∆ = τ k θ ′ p = θproj p -δ 1 ≥ 0 . . . θ ′ q = θproj q -δ n ≥ 0 for some ∆ = n i=1 δ i &gt; 0 where θp ≥ • • • ≥ θq</formula><p>are all of the θ values which do not fall under the first condition and where the corresponding θproj values are nonzero. We know that there must be some θproj p , • • • , θproj q from which we can subtract δ 1 , • • • , δ n (and so from which we can take the ∆) because j: rj ( θj )≥r k ( θk ) τ j ≤ 1. Now, we have:</p><formula xml:id="formula_72">⟨ θ, θproj ⟩ -⟨ θ, θ ′ ⟩ = θk θproj k + θp θproj p + • • • + θq θproj q -θk θproj k -θk ∆ -θp θproj p -• • • -θq θproj q + θp δ 1 + • • • + θq δ n = -θk ∆ + θp δ 1 + • • • + θq δ n ≤ θp (δ 1 + • • • + δ n ) -θk ∆ = θp ∆ -θk ∆ ≤ 0.</formula><p>At this point, the only way to avoid the contradiction result would be if θk = θp = • • • = θq . Otherwise, the above non-strict inequality would be a strict inequality. If θk = θp = • • • = θq , then we know that θk is the smallest θ value satisfying condition 1 and all of the other greater θ values satisfying condition 1 must be projected to their τ threshold value (otherwise we would get the contradiction result). In this edge case can see above that rearranging the remaining weight among equal θ values does not change the dot product, so all of the solutions that we can get without the contradiction result are equivalently optimal (including the solution from Equation <ref type="formula" target="#formula_16">10</ref>).</p><p>Case 3. This is analogous to case 1. Suppose for the sake of contradiction that the optimal solution is θproj and yet θproj k &gt; 0 for some θproj k falling under the third case of Equation <ref type="formula" target="#formula_16">10</ref>. Now suppose that we construct a θ ′ also satisfying the projection constraints that is the same as θproj except in these places:</p><formula xml:id="formula_73">θ ′ k = θproj k -∆ = 0 θ ′ p = θproj p + δ 1 ≤ τ p . . . θ ′ q = θproj q + δ n ≤ τ q</formula><p>for some ∆ = n i=1 δ i &gt; 0 where θp ≥ • • • ≥ θq are all of the θ values which do not fall under the third condition and where the corresponding θproj values are not at their thresholds. By construction we know that there must be some θproj p , • • • , θproj q to which we can add δ 1 , • • • , δ n . Now, we have:</p><formula xml:id="formula_74">⟨ θ, θproj ⟩ -⟨ θ, θ ′ ⟩ = θk θproj k + θp θproj p + • • • + θq θproj q -θk θproj k + θk ∆ -θp θproj p -• • • -θq θproj q -θp δ 1 -• • • -θq δ n = θk ∆ -θp δ 1 -• • • -θq δ n ≤ -θq (δ 1 + • • • + δ n ) + θk ∆ = -θq ∆ + θk ∆ ≤ 0.</formula><p>At this point, the only way to avoid the contradiction result would be if θk = θp = • • • = θq . Otherwise, the above non-strict inequality would be a strict inequality. If θk = θp = • • • = θq , then we know that θk is the largest θ value satisfying condition 3 and all of the other smaller θ values satisfying condition 3 must be projected to 0 (otherwise we would get the contradiction result). In this edge case, we can see above that rearranging the remaining weight among equal θ values does not change the dot product, so all of the solutions that we can get without the contradiction result are equivalently optimal (including the solution from Equation <ref type="formula" target="#formula_16">10</ref>).</p><p>Case 2. Above, we show that both Case 1 and Case 3 are true. So, the remaining weight must be given to the single value of θproj not covered by either case.</p><p>B.2 QUADRATIC PROJECTION B.2.1 LEMMA 4</p><p>Statement of Lemma 4 Suppose that θproj is the optimal solution to:</p><formula xml:id="formula_75">θproj = arg min θ∈R D || θ -θ|| 2 2 ,</formula><p>subject to:</p><formula xml:id="formula_76">D i=1 θ i = 1 0 ≤ θ i ≤ τ i , ∀i ∈ [1, D],</formula><p>where τ i &gt; 0 are fixed values. Then, θproj s = 0 implies that any j with θs &gt; θj must have θproj j = 0.</p><p>Proof: This is similar to Lemma 2 from <ref type="bibr" target="#b40">Shalev-Shwartz &amp; Singer (2006)</ref>. Assume for the sake of contradiction θproj s = 0 and θs &gt; θj , yet we have θproj j &gt; 0.</p><p>Now we can construct another vector θ ′ that is the same as θproj , except in two places:</p><formula xml:id="formula_77">θ ′ s = θproj s + ∆ θ ′ j = θproj j -∆,</formula><p>for some ∆ satisfying 0 &lt; ∆ &lt; min( θproj j , τ s -θproj s ). This bound on ∆ ensures that θ ′ is still within the thresholds. We know that ∆ can exist because min( θproj j , τ s -θproj s ) &gt; 0 (by supposition, τ s -θproj s = τ s -0 &gt; 0 and θproj j &gt; 0).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Now we can compute</head><formula xml:id="formula_78">: || θ -θproj || 2 2 -|| θ -θ ′ || 2 2 = ( θs -θproj s ) 2 + ( θj -θproj j ) 2 -( θs -( θproj s + ∆)) 2 -( θj -( θproj j -∆)) 2 = 2∆(( θs -θproj s ) -( θj -θproj j ) -∆) &gt; 2∆(( θs -θproj s ) -( θj -θproj j ) -min( θproj j , τ s -θproj s )) ≥ 2∆(( θs -θproj s ) -( θj -θproj j ) -θproj j ) = 2∆( θs -θj ) &gt; 0.</formula><p>So θproj cannot be the optimal solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.2 LEMMA 5</head><p>Statement of Lemma 5 Suppose that θproj is the optimal solution to:</p><formula xml:id="formula_79">θproj = arg min θ∈R D || θ -θ|| 2 2 ,</formula><p>subject to:</p><formula xml:id="formula_80">D i=1 θ i = 1 0 ≤ θ i ≤ τ i , ∀i ∈ [1, D],</formula><p>where τ i &gt; 0 are fixed values. Then, θproj s = τ s implies θproj j = τ j for any θj -τ j &gt; θs -τ s .</p><p>Proof: Again, this is similar to Lemma 2 from <ref type="bibr" target="#b40">Shalev-Shwartz &amp; Singer (2006)</ref>. Assume for the sake of contradiction θproj s = τ s and θj -τ j &gt; θs -τ s , yet we have θproj j &lt; τ j .</p><p>Now we can construct another vector θ ′ that is the same as θproj , except in two places:</p><formula xml:id="formula_81">θ ′ s = θproj s -∆ θ ′ j = θproj j + ∆,</formula><p>for some ∆ satisfying 0 &lt; ∆ &lt; min( θproj s , τ j -θproj j ). This bound on ∆ ensures that θ ′ is still within the thresholds. We know that ∆ can exist because min( θproj s , τ j -θproj j ) &gt; 0 (by supposition, τ j -θproj   <ref type="bibr">et al., 2023b)</ref>, despite its simplicity, requires some tuning. A decision must be made about how to format the bemchmark data into a single piece of text per example so that it can be compared with potential pretraining data in terms of n-gram overlap. The LAMBADA tasks only have one text column per example, so the decision here is trivial. Examples from the other tasks each have a question, possibly a context, and a set of multiple choice answers to choose from. We chose to concatenate all of these columns together with spaces to form one piece of text per example, duplicating the same question as a prefix for each different answer.</p><p>DSIR does not allow the user to specify the exact number of unique tokens desired for pretraining. It only allows the specification of the number of unique pages, which can have wildly varying token counts. For every DSIR job, we set the desired number of pages to 3325589, which we found through binary search to produce slightly more than 3.2B unique tokens for LAMBADA FR . It was expensive to find this number for even one bechmark, because for each iteration of the binary search, we had to run DSIR and then the Pythia tokenizer to know how many tokens resulted from the input page number parameter. We provide the number of unique tokens from DSIR for each task in Table <ref type="table" target="#tab_2">3</ref>. We pretrained on 3.2B tokens for every LLM regardless of whether all of them were unique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 FASTTEXT</head><p>The "SOTA" fastText model from <ref type="bibr" target="#b27">Li et al. (2024)</ref> is available here: <ref type="url" target="https://huggingface.co/mlfoundations/fasttext-oh-eli5">https://huggingface.co/ mlfoundations/fasttext-oh-eli5</ref>. We used this model to filter data by sorting pages by the model's "high quality" score, including the top pages in order until we had either reached or gone slightly over 3.2B unique tokens. This aligns with the data-selection procedure in the original paper, and is also essentially the same as running the linear projection (Equation <ref type="formula" target="#formula_16">10</ref>) at the page-level. We also applied this method when selecting data using our own fastText filter trained by our algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E ADDITIONAL PRETRAINING RESULTS</head><p>In Figure <ref type="figure">7</ref>, we present additional pretraining results for methods in our loss-performance correlation data selection paradigm. We find that using Spearman rank correlation <ref type="bibr" target="#b42">(Spearman, 1904)</ref> in place of our estimator achieves comparable performance. On some tests, it performs even better than our estimator. We also find that using the quadratic projection, while perhaps more intuitive, leads to worse performance than the linear projection.</p><p>F PRETRAINING TOKEN DISTRIBUTION WITH 5 × τ</p><p>Figure <ref type="figure" target="#fig_6">8</ref> shows what the projected estimate in our pretraining experiments would be if we had a pretraining data pool 5× as large. We see here that the estimate does an even better job at selecting pretraining data with the language that matches the target task. We see that our approach selects even more relevant data when the selection pool is larger. The spike around 6.7B parameters is due to a large number of partially trained Pythia <ref type="bibr" target="#b4">(Biderman et al., 2023)</ref> checkpoints from the same training run at that scale. Our algorithm has the hard task of selecting pretraining data for 160M parameter models, which is abnormally small in the set of models used to compute the estimate.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc>Figure1: We want to pretrain on domains where lower loss is generally correlated with higher downstream performance. Our approach does this by taking public, pretrained LLMs and measuring correlations across their log-likelihoods (left, red matrix) and performance on a target benchmark (center, blue vector). We then perform data selection by picking domains with high correlation and training a fastText classifier that distinguishes these domains from others. This approach is on par with the best-known data selection methods in our experiments, despite requiring no human selection of high-quality domains.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 0 N</head><label>10</label><figDesc>Perplexity Correlation Based Data SelectionInput: Benchmark error vector y ∈ [0, 1] N , log-loss matrix normalized as bits-per-byte X ∈ R + ×D , available tokens per domain a ∈ N D , and pretraining token target b ∈ N.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Rank predictions given by ⟨ θproj , Φ(x)⟩ for PIQA and LAMBADA FR. A standard deviation (σ) from the ideal fit is shown in red. 2σ is shown in orange. Many models outside 2σ (named and shown in blue) are trained on atypical data such as heavily multilingual data, code, or GPT-4 (Brown et al., 2020) outputs. Models with atypical architectures (i.e. Mamba (Gu &amp; Dao, 2024)) are named and shown in black. Generally, our estimate tightly predicts ordinal benchmark performance from web corpus losses.</figDesc><graphic coords="11,108.74,232.92,196.01,147.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Analysis of the loss matrix. The first row treats domains as examples to be projected via PCA, while the second row treats models as examples. Panels (a): eigenvalue decay for the eigendecomposition of the D×D covariance matrix resulting from the loss matrix; a few dominant PCs are seen. (b) and (c): domains plotted by the first two PCA components showing separation of language in b and entropy in c. (d,e) show analogous plots in t-SNE with a clearer separation of language. (f): eigenvalue decay analogous to (a). (g,h): models plotted by the first two PCA components showing clustering by model family (clusters show Pythia<ref type="bibr" target="#b4">(Biderman et al., 2023)</ref>, Qwen<ref type="bibr" target="#b2">(Bai et al., 2023)</ref>, and OpenLlama<ref type="bibr" target="#b17">(Geng &amp; Liu, 2023)</ref> derivatives -the three largest clusters in our data), and average model loss. (i,j) show analogous results under t-SNE where (i) is normalized to remove per-model entropy differences.</figDesc><graphic coords="12,108.00,295.88,72.07,60.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Because x 1 d=</head><label>1</label><figDesc>x 2 and ϵ ∆ d = -ϵ ∆ , the two expected values above are the same:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>j&gt; 0</head><label>0</label><figDesc>and θproj s = τ s &gt; 0).Now we can compute:|| θθproj || 2 2 -|| θθ ′ || 2 2 = ( θsθproj s ) 2 + ( θjθproj j ) 2 -( θs -( θproj s -∆)) 2 -( θj -( θproj j + ∆)) 2 = 2∆(( θjθproj j ) -( θsθproj s ) -∆) &gt; 2∆(( θjθproj j ) -( θsθproj s ) -min( θproj s , τ j -θproj j )) ≥ 2∆(( θjθproj j ) -( θsθproj s ) -(τ j -θproj j )) = 2∆(( θj -τ j ) -( θsθproj s )) = 2∆(( θj -τ j ) -( θs -τ s )) &gt; 0.So θproj cannot be the optimal solution.B.2.3 FULL PROOFTheorem 3 Suppose we want to solve:θproj = arg min θ∈R D || θ -θ|| 2 2 ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: This figure is analogous to Figure 3, except the τ thresholds have been multiplied by 5.We see that our approach selects even more relevant data when the selection pool is larger.</figDesc><graphic coords="31,108.00,161.30,396.00,71.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: The parameter-count histogram of the 90 models from the Open LLM Leaderboard (Beeching et al., 2023) that we used to compute our estimate for pretraining data selection. Bar widths are 160M. The smallest model in the sample has ≈33M parameters and the largest has ≈9B.The spike around 6.7B parameters is due to a large number of partially trained Pythia<ref type="bibr" target="#b4">(Biderman et al., 2023)</ref> checkpoints from the same training run at that scale. Our algorithm has the hard task of selecting pretraining data for 160M parameter models, which is abnormally small in the set of models used to compute the estimate.</figDesc><graphic coords="31,108.00,431.75,396.01,132.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="10,108.00,81.86,396.00,386.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Unique pretraining tokens selected per benchmark, from DSIR.</figDesc><table><row><cell>Benchmark</cell><cell>Tokens</cell></row><row><cell>ARC Easy</cell><cell>2,905,206,499</cell></row><row><cell>PIQA</cell><cell>2,910,486,295</cell></row><row><cell>SCIQ</cell><cell>2,920,734,042</cell></row><row><cell>LAMBADA</cell><cell>3,022,219,424</cell></row><row><cell cols="2">LAMBADA DE 3,210,986,137</cell></row><row><cell cols="2">LAMBADA ES 3,396,528,704</cell></row><row><cell cols="2">LAMBADA FR 3,413,930,081</cell></row><row><cell cols="2">LAMBADA IT 3,384,854,845</cell></row><row><cell>D.3 DSIR</cell><cell></cell></row><row><cell>DSIR (Xie</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>To be precise, we use bits-per-byte, which normalizes the sequence negative log-likelihood with the number of UTF-8 bytes. This is defined in terms of the length of the string in tokens LT , the length of the string in UTF-8 bytes LB, and the cross entropy loss ℓ as BPB = L T ℓ L B ln(2)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://huggingface.co/datasets/togethercomputer/RedPajama-Data-V2</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://commoncrawl.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>One might ask if the fastText classifier alone can be an effective data selector, since it appears in both high-performance selection methods. This is not possible -the classifier can only amplify the effectiveness of an existing selector rather than replace it -as training a classifier requires supervision, which is provided by either the perplexity correlation algorithm or human curation (in the case of<ref type="bibr" target="#b27">Li et al. (2024)</ref>).</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>We thank <rs type="person">Jack Spilecki</rs> for conversations on the mathematical aspects of the work. We also thank <rs type="person">Zitong Yang</rs>, <rs type="person">Yangjun Ruan</rs>, and <rs type="person">Lisa Li</rs> for their helpful feedback throughout the project, <rs type="person">Ludwig Schmidt</rs> and <rs type="person">Samir Gadre</rs> for discussions on scaling laws involving benchmark perplexity, <rs type="person">Rohan Pandey</rs> for conversations about scaling laws, <rs type="person">Sung Min Park</rs> for discussions on drafts of this work, and <rs type="person">William Held</rs> for conversations about data selection. This work is supported in part by a grant from <rs type="funder">Sandia National Laboratories</rs>, and gifts from <rs type="person">Open Philanthropy</rs>, <rs type="funder">Meta</rs>, <rs type="person">Panasonic Research</rs>, and the <rs type="funder">Tianqiao and Chrissy Chen Institute</rs>. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of <rs type="institution">Sandia National Laboratories</rs>. <rs type="person">Tristan Thrush</rs> is supported in part by the <rs type="grantName">Stanford Graduate Fellowship</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_h8zmxK5">
					<orgName type="grant-name">Stanford Graduate Fellowship</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>subject to:</p><p>where τ i &gt; 0 are fixed values. Then the solution is:</p><p>where λ is found (through e.g. bisection search) to satisfy: D i=1 min(max( θi -λ, 0), τ i ) = 1.</p><p>Proof: Note that this problem is the same as the simplex projection problem from <ref type="bibr" target="#b40">Shalev-Shwartz &amp; Singer (2006)</ref> and <ref type="bibr" target="#b11">Duchi et al. (2008)</ref>, except here we have additional θ i ≤ τ i constraints. The Lagrangian for this problem is 5 :</p><p>To find the optimality condition with respect single index of θ, we set the derivative to zero:</p><p>The complimentary slackness KKT condition gives us that ζ i = µ i = 0 when 0 &lt; θ i &lt; τ i , so for θ i not at the boundary of our constraints, we get:</p><p>So, we have that for all θ i ∈ (0, τ i ), there is a shared value λ which we subtract from θi to get the value of θ i . How do we know which θ i are 0 and which θ i are τ i , though?</p><p>Assume that we know λ. By Lemma 4, we can characterize the optimal solution as:</p><p>By Lemma 5, we can characterize the optimal solution as:</p><p>for θproj k ̸ = 0. So, we can combine these two forms to get:</p><p>Now recall that we have the following constraint:</p><p>Given this constraint, we can find λ through search (moving the value up or down). We can see this by noticing that D i=1 min(max( θi -λ, 0), τ i ) is a strictly decreasing function of λ between the setting of λ that makes θi -λ &gt; 0 for at least one i, and the setting of λ that makes θi -λ &lt; τ i for at least one i. So in this range, there is only one setting of λ that satisfies this equation. We can only choose a λ outside of this range when D i=1 τ i = 1, and in this case the solution is trivial:</p><p>5 Note that multiplying || θproj -θ|| 2 2 by 1 2 does not change the minimization problem and enables us to get rid of a factor of 2 after taking the derivative of the Lagrangian. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C LOSS MATRIX COMPUTATION SPECIFICS</head><p>For all of our experiments, we computed the loss matrix as follows. For efficiency purposes, we sampled only 25 pages for a domain's bits-per-byte (BPB) computation even if a domain had more than 25 pages. To get an LLM's BPB on a page, we split the page into chunks of text that were 512 tokens according to a reference tokenizer (we used the Llama 2 7B tokenizer; <ref type="bibr" target="#b46">Touvron et al. 2023)</ref>. These text chunks turned out to be small enough to fit in the context of every LLM we tested. We then averaged BPB across chunks for each page and then across pages for each domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D ADDITIONAL DETAILS FOR PRETRAINING EXPERIMENTS</head><p>In this section, we specify hyperparameters and methods used for LLM pretraining and evaluation for our LLM pretraining experiments. We also specify settings used for the data-selection methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 LLM PRETRAINING</head><p>We trained each LLM on 4 NVIDIA A100 GPUs. At 3.2B tokens, each training run took under 3 hours with the Hugging Face Trainer <ref type="bibr" target="#b50">(Wolf et al., 2019)</ref> and appropriate PyTorch <ref type="bibr" target="#b1">(Ansel et al., 2024)</ref> compile flags. We provide pretraining hyperparameters in Table <ref type="table">2</ref>. Given our per-device batch size, we found the learning rate by increasing it by a factor of 2 until we saw instability and then using the highest learning rate where no instability was observed. Refer to the Pythia paper <ref type="bibr" target="#b4">(Biderman et al., 2023)</ref> for more information; we initialized the model from scratch using their 160M model configuration at <ref type="url" target="https://huggingface.co/EleutherAI/pythia-160m">https://huggingface.co/EleutherAI/pythia-160m</ref>. Other hyperparameters can be assumed to be Hugging Face Trainer defaults at the time of this writing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 LLM EVALUATION</head><p>At the end of the pretraining script, we used the Eleuther AI Eval Harness <ref type="bibr" target="#b16">(Gao et al., 2023)</ref>. For efficiency, we set the sample limit to 5000 examples per benchmark. Elsewhere, we used the default settings. On 4 NVIDIA A100s, it took only a few minutes per LLM to compute evaluation results for SciQ, ARC Easy, PIQA, LAMBADA, and all of the translations of LAMBADA.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G PARAMETER COUNT DISTRIBUTION FOR ESTIMATOR LLMS</head><p>In Figure <ref type="figure">9</ref>, we present the parameter-count histogram of the 90 models from the Open LLM Leaderboard <ref type="bibr" target="#b3">(Beeching et al., 2023)</ref> that we used to compute our estimate for pretraining data selection. Only 8 models here are less than 160M parameters. Despite this, our estimate can be used to effectively pretrain 160M parameter LLMs.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semdedup: Dataefficient learning at web-scale through semantic deduplication</title>
		<author>
			<persName><forename type="first">Amro</forename><surname>Abbas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kushal</forename><surname>Tirumala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dániel</forename><surname>Simig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><forename type="middle">S</forename><surname>Morcos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Ansel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Animesh</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Voznesensky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Berard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeni</forename><surname>Burovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geeta</forename><surname>Chauhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anjali</forename><surname>Chourdia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Constable</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elias</forename><surname>Ellison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Gschwind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Hirsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherlock</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kshiteej</forename><surname>Kalambarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Kirsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Lazos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Lezcano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanbo</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinghai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Luk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bert</forename><surname>Maher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunjie</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Reso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Saroufim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcos</forename><forename type="middle">Yukio</forename><surname>Siraichi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helen</forename><surname>Suk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Tillet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eikan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shunting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajit</forename><surname>Mathews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jinze</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfei</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbin</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binyuan</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luo</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dayiheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqiang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keming</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingzhang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuancheng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sinan</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhong</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengguang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shusheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingxuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenru</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. arXiv</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Edward</forename><surname>Beeching</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clémentine</forename><surname>Fourrier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Habib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazneen</forename><surname>Rajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Sanseviero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lewis</forename><surname>Tunstall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<ptr target="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard.OpenLLMLeaderboard" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailey</forename><surname>Schoelkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herbie</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Kyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Brien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Hallahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivanshu</forename><surname>Aflah Khan</surname></persName>
		</author>
		<author>
			<persName><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Usvsn Sai Prashanth</surname></persName>
		</author>
		<author>
			<persName><surname>Raff</surname></persName>
		</author>
		<idno>arXiv</idno>
		<title level="m">Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. Pythia: A suite for analyzing large language models across training and scaling</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BLOOM: A 176b-parameter open-access multilingual language model</title>
		<author>
			<persName><surname>Bigscience</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">PIQA: reasoning about physical commonsense in natural language</title>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<ptr target="https://huggingface.co/datasets/EleutherAI/lambada_openai.MultilingualLAMBADA" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Amodei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Language models are few-shot learners. arXiv</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Robust structured estimation with single-index models</title>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arindam</forename><surname>Banerjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isaac</forename><surname>Cowhey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carissa</forename><surname>Schoenick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<title level="m">Think you have solved question answering? Try ARC, the AI2 reasoning challenge. arXiv</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient projections onto the l1-ball for learning in high dimensions</title>
		<author>
			<persName><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shai</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Chandra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Axel</forename><surname>Feldmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<title level="m">Dsdm: Model-aware dataset selection with datamodels. arXiv</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gio: Gradient information optimization for training dataset selection</title>
		<author>
			<persName><forename type="first">Dante</forename><surname>Everaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ELI5: long form question answering</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Travis</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anish</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noa</forename><surname>Nabeshima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shawn</forename><surname>Presser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Leahy</surname></persName>
		</author>
		<title level="m">The pile: An 800gb dataset of diverse text for language modeling. arXiv</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A framework for few-shot language model evaluation</title>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Tow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Baber Abbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sid</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Dipofi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alain</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haonan</forename><surname>Le Noac'h</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Mcdonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Ociepa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laria</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailey</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aviya</forename><surname>Schoelkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lintang</forename><surname>Skowron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anish</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Zou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>Zenodo</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Openllama: An open reproduction of llama</title>
		<author>
			<persName><forename type="first">Xinyang</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://github.com/openlm-research/open_llama" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Groeneveld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pete</forename><surname>Walsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshita</forename><surname>Bhagia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodney</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananya</forename><surname>Harsh Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamish</forename><surname>Ivison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Magnusson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><surname>Authur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghavi</forename><surname>Khyathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Chandu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanai</forename><surname>Dumas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuling</forename><surname>Elazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Merrill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Morrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Crystal</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentina</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhilasha</forename><surname>Pyatkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Ravichander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emma</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nishant</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Subramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Soldaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><surname>Hajishirzi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>Olmo: Accelerating the science of language models. arXiv</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mamba: Linear-time sequence modeling with selective state spaces</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Suriya</forename><surname>Gunasekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jyoti</forename><surname>Aneja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caio</forename><surname>César</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teodoro</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allie</forename><surname>Del Giorno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sivakanth</forename><surname>Gopi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mojan</forename><surname>Javaheripi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piero</forename><surname>Kauffmann</surname></persName>
		</author>
		<editor>Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, Sébastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li</editor>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>Textbooks are all you need. arXiv</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>De Las</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hennigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bogdan</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelia</forename><surname>Damoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><forename type="middle">W</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Sifre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Training compute-optimal large language models. arXiv</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Datamodels: Predicting predictions from training data</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Logan</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Leclerc</surname></persName>
		</author>
		<author>
			<persName><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The isotron algorithm: High-dimensional isotonic regression</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Tauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalai</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">COLT</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Scaling laws for neural language models</title>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Lucile</forename><surname>Hugo Laurençon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Saulnier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Akiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Villanova Del Moral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leandro</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenghao</forename><surname>Von Werra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduardo</forename><forename type="middle">González</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huu</forename><surname>Ponferrada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jörg</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Frohberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Šaško</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelina</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerard</forename><surname>Mcmillan-Major</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loubna</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><forename type="middle">De</forename><surname>Ben Allal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giada</forename><surname>Toni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Pistilli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somaieh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maraim</forename><surname>Nikpoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Masoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paulo</forename><surname>De La Rosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Thrush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leon</forename><surname>Nagel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Muñoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaid</forename><surname>Van Strien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khalid</forename><surname>Alyafeai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh</forename><forename type="middle">Chien</forename><surname>Almubarak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itziar</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Gonzalez-Dios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Soroa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manan</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><forename type="middle">Ortiz</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Suarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shamik</forename><surname>Gokaslan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Adelani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhas</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenny</forename><surname>Pai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Violette</forename><surname>Chim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suzana</forename><surname>Lepercq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasha</forename><forename type="middle">Alexandra</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Luccioni</surname></persName>
		</author>
		<author>
			<persName><surname>Jernite</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>The bigscience roots corpus: A 1.6tb composite multilingual dataset. NeurIPS Datasets and Benchmarks</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Alon Albalak, Yonatan Bitton</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Smyrnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maor</forename><surname>Ivgi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samir</forename><surname>Gadre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hritik</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etash</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sedrick</forename><surname>Keh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kushal</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reinhard</forename><surname>Heckel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Mercat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mayee</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suchin</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marianna</forename><surname>Nezhurina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amro</forename><surname>Abbas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng-Yu</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruba</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maciej</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanlin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rulin</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Pratt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunny</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giannis</forename><surname>Daras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalyani</forename><surname>Marathe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Gokaslan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khyathi</forename><surname>Chandu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thao</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Vasiljevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sham</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujay</forename><surname>Sanghavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fartash</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewoong</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hadi</forename><surname>Pouransari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Groeneveld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Soldaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pang</forename><surname>Wei Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenia</forename><surname>Jitsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Kollar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><forename type="middle">G</forename><surname>Dimakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yair</forename><surname>Carmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Achal</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>Datacomp-lm: In search of the next generation of training sets for language models. arXiv</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Qian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaosen</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangtao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longxu</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<idno>arXiv</idno>
		<title level="m">Regmix: Data mixture as regression for language model pre-training. arXiv, 2024. Llama Team. The llama 3 herd of models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A table of integrals of the error functions</title>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">W</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murray</forename><surname>Geller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Research of the Natianal Bureau of Standards</title>
		<imprint>
			<date type="published" when="1968">1968</date>
			<publisher>Mathematical Sciences</publisher>
			<pubPlace>Section B</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">David</forename><surname>Owen</surname></persName>
		</author>
		<title level="m">How predictable is language model benchmark performance? arXiv</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Denis</forename><surname>Paperno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ngoc</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raffaella</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandro</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Pezzelle</surname></persName>
		</author>
		<author>
			<persName><surname>Baroni</surname></persName>
		</author>
		<title level="m">Gemma Boleda, and Raquel Fernández. The LAMBADA dataset: Word prediction requiring a broad discourse context. ACL</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Jupinder</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shrimai</forename><surname>Prabhumoye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Jennings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aastha</forename><surname>Jhunjhunwala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno>arXiv</idno>
		<title level="m">Data, data everywhere: A guide for pretraining dataset construction</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">On lines and planes of closest fit to systems of points in space</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Pearson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Magazine</title>
		<imprint>
			<date type="published" when="1901">1901</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Guilherme</forename><surname>Penedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hynek</forename><surname>Kydlíček</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loubna</forename><surname>Ben Allal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Lozhkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leandro</forename><surname>Von Werra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<title level="m">The fineweb datasets: Decanting the web for the finest text data at scale. arXiv</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Bo</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Alcaide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alon</forename><surname>Albalak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Arcadinho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huanqi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Grella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kranthi</forename><surname>Kiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">V</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Xuzheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haowen</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaju</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Przemyslaw</forename><surname>Kazienko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kocon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bartlomiej</forename><surname>Koptyra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hayden</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><surname>Sri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ipsit</forename><surname>Mantri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ferdinand</forename><surname>Mom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atsushi</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangyu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangru</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johan</forename><forename type="middle">S</forename><surname>Wind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislaw</forename><surname>Wozniak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruichong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qihang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui-Jie</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>Rwkv: Reinventing rnns for the transformer era. arXiv</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">High-dimensional estimation with geometric constraints</title>
		<author>
			<persName><forename type="first">Yaniv</forename><surname>Plan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Vershynin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Yudovina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information and Inference: A Journal of the IMA</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Michael</forename><surname>Poli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Massaroli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Baccus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<idno>arXiv</idno>
		<title level="m">Hyena hierarchy: Towards larger convolutional language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Observational scaling laws and the predictability of language model performance</title>
		<author>
			<persName><forename type="first">Yangjun</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori</forename><surname>Hashimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Efficient learning of label ranking by soft projections onto polyhedra</title>
		<author>
			<persName><forename type="first">Shai</forename><surname>Shalev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Shwartz</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Luca</forename><surname>Soldaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodney</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshita</forename><surname>Bhagia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><surname>Authur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Bogin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khyathi</forename><surname>Chandu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Dumas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanai</forename><surname>Elazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananya</forename><surname>Harsh Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sachin</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Lucy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinxi</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Magnusson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Morrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Crystal</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhilasha</forename><surname>Ravichander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zejiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nishant</forename><surname>Subramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pete</forename><surname>Walsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Groeneveld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>Dolma: an open corpus of three trillion tokens for language model pretraining research. arXiv</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The Proof and Measurement of Association between Two Things</title>
		<author>
			<persName><forename type="first">Charles</forename><surname>Spearman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Journal of Psychology</title>
		<imprint>
			<date type="published" when="1904">1904</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Openhermes 2.5: An open dataset of synthetic data for generalist llm assistants</title>
		<author>
			<persName><surname>Teknium</surname></persName>
		</author>
		<ptr target="https://huggingface.co/datasets/teknium/OpenHermes-2.5" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the lasso</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society Series B: Statistical Methodology</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<ptr target="https://github.com/togethercomputer/RedPajama-Data" />
		<title level="m">RedPajama: an Open Dataset for Training Large Language Models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>Together Computer</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasmine</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prajjwal</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Bikel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Blecher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristian Canton</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Esiobu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jude</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyin</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Fuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedanuj</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Hartshorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saghar</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hakan</forename><surname>Inan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Kardas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktor</forename><surname>Kerkez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabel</forename><surname>Kloumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Artem</forename><surname>Korenev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Punit</forename><surname>Singh Koura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenya</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Liskovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinghai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuning</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushkar</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Molybog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Poulton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Reizenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rashi</forename><surname>Rungta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalyan</forename><surname>Saladi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Schelten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruan</forename><surname>Silva</surname></persName>
		</author>
		<editor>Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov</editor>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. arXiv</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Emergent abilities of large language models. TMLR</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Crowdsourcing multiple choice science questions</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nelson</forename><forename type="middle">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>W-NUT</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Brew</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Jeffrey</surname></persName>
		</author>
		<author>
			<persName><surname>Wooldridge</surname></persName>
		</author>
		<title level="m">Econometric Analysis of Cross Section and Panel Data</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Doremi: Optimizing data mixtures speeds up language model pretraining</title>
		<author>
			<persName><forename type="first">Sang</forename><surname>Michael Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Data selection for language models via importance resampling</title>
		<author>
			<persName><forename type="first">Sang</forename><surname>Michael Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
