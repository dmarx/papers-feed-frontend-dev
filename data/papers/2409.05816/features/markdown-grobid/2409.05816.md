# IMPROVING PRETRAINING DATA USING PERPLEXITY CORRELATIONS

## Abstract

## 

Quality pretraining data is often seen as the key to high-performance language models. However, progress in understanding pretraining data has been slow due to the costly pretraining runs required for data selection experiments. We present a framework that avoids these costs and selects high-quality pretraining data without any LLM training of our own. Our work is based on a simple observation: LLM losses on many pretraining texts are correlated with downstream benchmark performance, and selecting high-correlation documents is an effective pretraining data selection method. We build a new statistical framework for data selection centered around estimates of perplexity-benchmark correlations and perform data selection using a sample of 90 LLMs taken from the Open LLM Leaderboard on texts from tens of thousands of web domains. In controlled pretraining experiments at the 160M parameter scale on 8 benchmarks, our approach outperforms DSIR on every benchmark, while matching the best data selector found in DataComp-LM, a hand-engineered bigram classifier.

## INTRODUCTION

Dataset curation is increasingly crucial for training high-quality large language models (LLMs). As pretraining datasets have grown, from under 200B tokens in 2020 [(Raffel et al., 2020;](#b38)[Gao et al., 2020)](#b15) to 240T tokens today [(Li et al., 2024)](#b27), it has become critical to identify subsets of the available data that will lead to the best LLMs, and a wide range of methods have arisen to meet these needs [(Ilyas et al., 2022;](#b22)[Xie et al., 2023a;](#)[b;](#)[Engstrom et al., 2024;](#b12)[Everaert & Potts, 2024;](#b13)[Liu et al., 2024;](#b28)[Llama Team, 2024)](#). However, data-driven approaches to data selection typically involve expensive model retraining steps that limit their effectiveness, and no algorithm has been reported to consistently beat or match hand-crafted classifiers for data selection [(Li et al., 2024)](#b27).

Is training new LLMs necessary for data selection? Instead of training our own models, can we use the growing collection of publicly available, high-performance LLMs [(Wolf et al., 2019;](#b50)[Beeching et al., 2023)](#b3) to perform data valuation and selection? This would have significant benefits: we could leverage the millions of dollars collectively spent on building these LLMs, and we would have coverage over a large, heterogeneous collection of high-performance models varying in size, architectures, and pretraining data distribution. Despite these advantages, using existing models for pretraining data selection is challenging, as the training data for these models are often unknown and heterogeneous. Our key observation is that data selection can be done using two observable features of all public models today: 1) all openweight models produce a causal language modeling loss for a given text, and 2) all of them can be evaluated on benchmarks. Prior work has found systematic relationships between web corpus loss and benchmark performance [(Wei et al., 2022)](#b48), which suggests the possibility of using correlations between perplexity and benchmark scores as the basis for a data selection policy.

In the present paper, we pursue this possibility and find a radically simple approach that is also effective: we select data via perplexity correlations (Figure [1](#fig_4)), where we select data domains (e.g. We want to pretrain on domains where lower loss is generally correlated with higher downstream performance. Our approach does this by taking public, pretrained LLMs and measuring correlations across their log-likelihoods (left, red matrix) and performance on a target benchmark (center, blue vector). We then perform data selection by picking domains with high correlation and training a fastText classifier that distinguishes these domains from others. This approach is on par with the best-known data selection methods in our experiments, despite requiring no human selection of high-quality domains.

## Domains

wikipedia.org, stackoverflow.com, etc.) for which LLM log-probabilities are highly correlated with downstream benchmark performance. To enable our approach, we complement our algorithm with a statistical framework for correlation-based data selection and derive correlation estimators that perform well over our heterogeneous collection of LLMs.

We validate our approach over a large collection of pretrained causal LLMs on the Hugging Face Open LLM Leaderboard [(Beeching et al., 2023)](#b3) and find that perplexity correlations are often predictive of an LLM's benchmark performance. More importantly, we find that these relationships are robust enough to enable reliable data selection that targets downstream benchmarks. In controlled pretraining experiments at the 160M parameter scale on eight benchmarks, our approach strongly outperforms DSIR [(Xie et al., 2023b](#)) (a commonly used training-free data selection approach based on n-gram statistics) while generally matching the performance of the best overall method validated at scale by [Li et al.](#) (the OH-2.5 +ELI5 fastText classifier [(Joulin et al., 2016)](#b23)) without any parameter tuning or human curation.

Our manuscript focuses on pretraining experiments at the 160M scale. To assess the sensitivity of our experimental results to the choice of pretraining data pool, scale, and experiment design, we use this paper also as a preregistration for further pretraining experiments using different data sources and evaluation benchmarks at the 1.4B model scale. We commit to updating the arXiv version of our manuscript with both positive and negative results on these preregistered validation experiments.

## RELATED WORK

To go beyond the status quo of deduplication, perplexity filtering, and hand-curation [(Laurençon et al., 2022;](#b26)[BigScience, 2023;](#b5)[Abbas et al., 2023;](#b0)[Groeneveld et al., 2024;](#b18)[Soldaini et al., 2024;](#b41)[Penedo et al., 2024;](#b34)[Llama Team, 2024)](#), targeted methods have been proposed to filter pretraining data so that the resulting LLM will achieve higher scores on given benchmarks. There are lightweight approaches that use n-gram overlap [(Xie et al., 2023b)](#) or embedding similarity [(Everaert & Potts, 2024)](#b13) to select training data that is similar to data from a given benchmark. There are also less-scalable methods that require training proxy LLMs on different data mixtures [(Ilyas et al., 2022;](#b22)[Xie et al., 2023a;](#)[Engstrom et al., 2024;](#b12)[Liu et al., 2024;](#b28)[Llama Team, 2024)](#).

Given the high costs of proxy-based data selection methods, they have primarily been used to select among human-curated pretraining data mixtures [(Llama Team, 2024;](#)[Li et al., 2024)](#b27) rather than a high dimensional space of mixtures. Our work takes an orthogonal approach and builds upon recent observational studies that have found scaling relationships that hold across collections of uncontrolled and diverse LLMs [(Owen, 2024;](#b30)[Ruan et al., 2024)](#b39). While these studies do not examine loss-to-performance relationships or derive useful data selection methods from them, we know that losses and performance are generally highly correlated. Validation losses on samples of text corpora are commonly used as a proxy for downstream performance when comparing LLMs pretrained on the same data distribution [(Kaplan et al., 2020;](#b25)[Hoffmann et al., 2022;](#b21)[Wei et al., 2022)](#b48), even if they have different architectures [(Poli et al., 2023;](#b37)[Peng et al., 2023;](#b35)[Gu & Dao, 2024)](#b19).

According to a recent survey of data selection approaches by [Li et al. (2024)](#b27), the heavier-weight pretraining data selection methods have not yet shown large gains, and the current state-of-the-art across many tasks is still primitive: a fixed fastText classifier [(Joulin et al., 2016)](#b23) combined with an English filter as a final layer after extensive deduplication and filtering. Are we missing important information that we can efficiently extract from a diverse collection of already trained models, larger and more diverse than any single organization is likely to produce? We show promising evidence supporting this hypothesis -simple loss-performance correlation coefficients are effective when used for data selection.

## PROBLEM SETTING

Our goal is to build predictive models of how pretraining data distributions affect downstream benchmark performance and use them to build better language models. Unfortunately, this task is challenging and computationally expensive. A standard approach adopted in paradigms such as datamodeling [(Ilyas et al., 2022)](#b22) is to obtain N different pretraining distributions [domains (e.g. arxiv.org, stackoverflow.com, etc.)](#), pretrain and measure model errors on a target benchmark y i ∈ [0, 1], and fit a model p → y. This approach requires N LLM training runs, performed at a scale sufficient to obtain non-random performance on y. This can cost tens to hundreds of millions of dollars for hard benchmarks such as MMLU, where even the performance of 1B parameter LLMs often do not exceed random chance [(Beeching et al., 2023)](#b3).

${p i : i ∈ [N ], p i ∈ R + 0 D } over D ≫ N$Instead, our work considers the following observational setting that requires no training. We obtain N pretrained, high-performance LLMs that vary in pretraining data, tokenizer, architecture, and scale (e.g. models on Huggingface's OpenLLM leaderboard). Now, if we could train a predictor p → y on these N models, we could avoid large scale model training. Unfortunately, this is impossible as the training data for these models is often proprietary, and so we have no knowledge of p.

The key observation of our work is that we can replace p i,j (the unobserved sampling probability of model i's data selection policy on document j) with an observable surrogate x i,j , which is the negative log-likelihood of document j under model i. [1](#foot_0) We can then build a regression model that relates negative log-likelihood x i and benchmark error y i . Using this model, we can select pretraining data from domains j for which decreasing the loss x i,j is predicted to rapidly decrease error y i .

The perplexity-performance hypothesis. We formulate the task of predicting errors y i from negative log-probabilities x i as a single-index model (SIM),

$y i = f (⟨θ * , x i ⟩ + ϵ i )(1)$where f : R → R is some unknown monotonically increasing univariate function, ϵ i is zero-mean noise which is independent of x, and θ * ∈ R D are unknown weights over D domains.

A single index model is highly flexible (due to the arbitrary, monotone f ) and has the advantage that we do not need to estimate the nonlinear function f if our goal is to optimize model performance. We can see this directly from the monotonicity of f as

$⟨θ * , x i ⟩ + ϵ i < ⟨θ * , x j ⟩ + ϵ j ⇐⇒ f (⟨θ * , x i ⟩ + ϵ i ) < f (⟨θ * , x j ⟩ + ϵ j ).(2)$Data selection from perplexity correlations. The weights θ * tell us which domain perplexities are correlated with downstream performance. However, this isn't sufficient for data selection. Even if we know how model likelihoods relate to model performance, we do not know how data selection affects likelihoods. Even worse, this data mixture to likelihood relationship cannot be learned observationally, as we do not know the data mixture of any of our models.

Despite this, we show that there is a clean approach for optimizing the data mixture. Our core observation is the following: if we find a nonnegative θ * , sampling proportional to θ * is always a good choice. More formally, we see that this sampling distribution defines the pretraining loss such that optimizing the training loss directly optimizes the downstream task via the single index model.

Proposition 1 Suppose that θ * weights are non-negative. Then, for models with associated likelihoods x ∈ X ⊂ R D , the minimizer of the pretraining loss over the θ * sampling distribution E j∼θ * [x j ] also has the lowest expected downstream error according to the single index model:

$arg min x∈X E j∼θ * [x j ] = arg min x∈X E[f (⟨θ * , x⟩ + ϵ)].$This observation follows directly from the fact that we can normalize any non-negative θ * into a distribution (and shift the normalization constant into f ) which allows us to write the inner product in the single-index model as a monotone function of the expected pretraining loss:

$y = f (⟨θ * , x⟩ + ϵ) = f (E j∼θ * [x j ] + ϵ).(3)$Proposition 1 allows us to entirely avoid the task of finding the optimal data mixture for a target likelihood. Instead, we pick sampling distributions that make the pretraining loss a monotone function of the predicted downstream error. Afterward, we can rely on our ability to optimize the loss to optimize downstream performance.

This view gives us a straightforward roadmap for data selection in the remainder of the paper: estimate a set of domains where loss and downstream benchmark performance is highly correlated, and then constrain our θ * estimates to be a pretraining data sampling distribution.

## METHODS

We now describe the details of our approach, starting by presenting the algorithm itself and the intuitions behind it, followed by a more precise and mathematical justification for the various steps.

## ALGORITHM

Estimating θ * . The parameter θ * j measures the relationship between log-likelihoods in domain j and downstream performance. Because of this, we might naturally expect θ * j to be related to nonlinear correlation coefficients between x and y. Our work uses a simple correlation measure,

$γ j = 1≤k,l≤n k̸ =l sign(y k -y l )(rank j (x k,j ) -rank j (x l,j ))$where rank j (x) is the rank of x among {x 1,j . . . x N,j }. This formula is intuitive: when model k does better than model l, what percentile is model k's log-likelihood compared to model l's? While this is not the only correlation coefficient that performs well (see Appendix E), this functional form has the additional benefit of being a principled estimate of θ * . In particular, we show in sections below that in expectation, the ranking of domains in γ exactly matches those of θ * (under standard high-dimensional regression assumptions; see Section 4.2 for a complete discussion).

Selecting pretraining data. Suppose that we have an accurate estimate γ j which is nonnegative. In this case, we could use γ j directly as a data selection procedure and Proposition 1 would ensure that minimizing the population pretraining loss minimizes downstream errors. Unfortunately, γ j can be negative and the finite number of tokens per domain can make it difficult to minimize the population pretraining loss. Thus, we must project γ j onto the set of reasonable pretraining data distributions that are nonnegative and account for the per-domain token counts.

What is a good way to project a set of domain rankings estimated via γ into a sampling distribution? Intuitively, if wikipedia.org has a γ j = 0.5 and arxiv.org is γ k = 0.9, it would be natural to select tokens in order of γ, preferring tokens from arxiv.org over tokens from wikipedia.org when selecting pretraining data.

Having established the ordering of domains, the remaining question is how many tokens we take for each domain. We follow recent observations that repeating data degrades performance [(Abbas et al., 2023)](#b0) to arrive at a simple selection algorithm: select domains in greatest to least γ, taking all the tokens in each domain once, until we exhaust our total pretraining token budget.

Full algorithm. Together, these steps result in a simple, parameter-free algorithm that calculates our rank correlation coefficient, and selects domains in order from largest to smallest coefficient. We show this process explicitly in Algorithm 1, and additionally show an extra step where we train a fastText [(Joulin et al., 2016)](#b23) classifier (using standard settings and bigram features from [Li et al. (2024)](#b27)) which distinguishes our selected documents and domains from the rest of the pool. The fastText classifier allows us to perform data selection at a single-page level, and scale the selection process to larger datasets. We also found the classifier to slightly improve downstream performance over directly selecting the documents. More information on the specifics of the data selection approaches that we tested is given in Appendix D. Output: Target token counts per domain t ∈ N D 0 , a fastText classifier to filter pretraining data.

$Initialize: γ ← 0 ∈ R D , t ← [0 . . .] ∈ N D 0 , counter ← 0. r 0 , r 1 , . . . , r N ← rank(x 0 , x 1 , . . . , x N ) ▷ 1. Compute the γ correlation coefficient for i, j ∈ 0 to N do γ ← γ + sign(y i -y j ) • (r i -r j ) for i ∈ArgSort(γ, descending=True) do ▷ 2. Select most to least correlated domains t i ← min(a i , b -counter) counter ← counter + a i if counter ≥ b then Break classifier = trainFastText(positive = 1 t>0 , negative = 1 t=0 ) Return t, classifier 4.2 THEORY$We now study the approach closely and show that our choices for the correlation coefficient and projection step are extensions of the classic, high-dimensional single index model estimator of [Plan et al. (2016)](#b36). We describe the basic single-index model estimators first, describe our extensions, and then conclude with a discussion on how our estimator and results deviate from the theory.

## HIGH-DIMENSIONAL ESTIMATION OF SINGLE INDEX MODELS

For our theory, we consider the standard high-dimensional regression setting of [Plan et al. (2016)](#b36) and [Chen & Banerjee (2017)](#b9). Here, our goal is to estimate the unknown weights θ * in a single-index model y i = f (⟨θ * , x i ⟩ + ϵ i ), with x i ∼ N (0, I) for ∥θ * ∥ 2 = 1 (assumed without loss of generality, as ∥θ * ∥ 2 can be absorbed by f ).

Our starting point is the classic result of [Plan et al. (2016)](#b36), who showed

$E [y k x k ] = cθ * ,(4)$for some positive constant c and 1 ≤ k ≤ N . Closely related is the result of Chen & Banerjee (2017) who showed a robust estimator quite similar to ours,

$E [sign(y k -y l )(x k -x l )] = βθ * (5)$for any 1 ≤ k, l ≤ N (where k ̸ = l) and some positive constant β. Both of these results clearly identify that for the high-dimensional single-index model in the Gaussian setting, generalized correlation coefficients provide consistent estimates of the true regression coefficient θ * .

## DERIVING OUR ESTIMATOR

Both Plan et al. and Chen & Banerjee provide moment-matching style estimators that consistently recover θ * in high-dimensional, sparse settings. However, we found that both estimators directly use the values of x, and this resulted in brittle estimates due to outliers in language model loglikelihoods. While outlier removal is one possibility, we found that a simpler approach was to robustify the estimator of [Chen & Banerjee (2017)](#b9) to outliers in x.

Recall that our estimate γ is a U-statistic, defined as pairwise sums of

$sign(y i -y j )(Φ(x i ) -Φ(x j )),(6)$for any 1 ≤ i, j ≤ N (where i ̸ = j), where Φ is the empirical CDF of the x values. This estimate is significantly less sensitive to outliers than that of [Chen & Banerjee (2017)](#b9), as the empirical CDF is bounded between zero and one, and no single model can make the estimator degenerate.

We study this estimate theoretically in the Gaussian setting, where we consider the asymptotically equivalent estimator with Φ as the CDF of the standard Gaussian. In this case, we can show that this modified estimator is also consistent in recovering θ * .

Theorem 1 When ϵ ∼ N (0, σ 2 ), we have:

$E[sign(y i -y j )(Φ(x i ) -Φ(x j ))] = 2 π sin -1 θ * 2 √ 1 + σ 2 . (7$$)$We provide the proof in Appendix A. Because we assume ||θ * || 2 = 1 and the expected value in Equation 7 must be between -1 and 1, we are always within the domain of sin -1 and able to invert it. After inverting, we get:

$θ ∝ sin π 2 E [sign(y i -y j )(Φ(x i ) -Φ(x j ))](8)$as an estimate for θ * , where the constant 2 √ 1 + σ 2 term due to noise has been dropped.

Beyond the fact that our estimator is consistent, we can show an even tighter connection to the Chen & Banerjee estimator and show that our estimates agree when running the original estimator on rank-transformed data. More specifically, for two models x i and x j with the estimated model rankings ⟨ θ, x i ⟩ > ⟨ θ, x j ⟩, the expected ranking under rank-transformation (i.e. Φ(x)) match this ranking.

Corollary 1 Suppose that θ is any vector of fixed weights and x ∼ N (0, I). Then, conditioning on the event ⟨ θ,

$x i ⟩ < ⟨ θ, x j ⟩, we have ⟨ θ, E[Φ(x i ) | ⟨ θ, x i ⟩ < ⟨ θ, x j ⟩]⟩ < ⟨ θ, E[Φ(x j ) | ⟨ θ, x i ⟩ < ⟨ θ, x j ⟩]⟩.(9)$with probability 1.

This proof follows from the same calculations as Theorem 1 and is given in Appendix A.

## SELECTING DATA FOR PRETRAINING

Recall that our algorithm for data selection is to constrain γ to be a valid sampling distribution (nonnegative, at the very least) and then sample directly from this estimate. For now, we focus on constraining θ, and we will see at the end of this section that we can apply the same constraint to γ directly to get the same result. The theory of constrained estimation for θ is simple and well-understood, with both Plan et al. ( [2016](#)) and Chen & Banerjee (2017) extensively studying the problem of estimating θ under a known convex constraint set C. In particular, [Plan et al. (2016)](#b36) show that performing a L 2 projection via θproj = arg min θ∈C ∥θ -θ∥ 2 provides improved convergence rates that depend on the Gaussian mean width of C rather than the ambient dimension, and Chen & Banerjee (2017) show similar results when maximizing the linear correlation θproj = arg min θ∈C⊆B D -⟨θ, θ⟩.

We take a similar approach here. We define a convex constraint set C that forces θ to be a reasonable sampling distribution and find the best sampling distribution via the linear correlation approach.

We define C as the combination of two sets of constraints. First, we must have a valid sampling distribution, so we constrain θ to lie in the simplex. As we noted above, it is well-known that duplicating data harms performance [(Abbas et al., 2023)](#b0), and so we constrain θ to avoid data duplication by limiting the maximum weight on domains. Concretely, if want to pretrain on m tokens overall,

$we enforce θ * i ≤ τ i , ∀i ∈ [1, D]$, where τ i is set so τ i m is the number of tokens from the i-th domain that we can access for training.

The resulting linear program has a simple solution and takes the form of initializing θproj to 0 and then iterating through the values in θ from largest to smallest, setting the value at the corresponding index of θproj to the maximum allowable value, until θproj sums to 1 (see Appendix B for a proof).

Theorem 2 Suppose we want to solve: θproj = arg min θ∈R D

-⟨θ, θ⟩, subject to:

$D i=1 θ i = 1 0 ≤ θ i ≤ τ i , ∀i ∈ [1, D],$where τ i > 0 are fixed values. Then, the solution is:

$θproj k =      τ k if j: rj ( θj )≥r k ( θk ) τ j ≤ 1 1 -j: rj ( θj )>r k ( θk ) τ j if j: rj ( θj )≥r k ( θk ) τ j ≥ 1 ∧ j: rj ( θj )>r k ( θk ) τ j ≤ 1 0 otherwise , (10$$)$where r is some function that breaks all ties between θj and θk for k ̸ = j, and otherwise leaves the ordinal relationships the same.

We note that while the use of this linear program is in line with the constrained estimators proposed in Chen & Banerjee (2017), the L 2 projection is arguably more natural, and does not require assuming that ∥ θ∥ 2 = 1 for asymptotic recovery conditions. We derive similar closed-form expressions for this quadratic case in Appendix B, but do not use this approach for two separate reasons.

First, the L 2 projection depends on the L 2 norm of θ, unlike the linear program which only depends on the ranks of the values in θ. The challenge with determining the norm is that the exact recovery result in Equation ( [7](#formula_10)) requires knowledge of the noise level, and the trigonometric functions rely strongly on the Gaussian structure of x. Because of this, we are unlikely to be able to estimate the norm of θ with any accuracy, and the only way to avoid this would be to treat the norm as a hyperparameter, which adds unnecessary complexity. The second reason is empirical (although possibly a consequence of the first) -we found that the linear projection performed better across a wide range of benchmarks and conditions (See Appendix E).

We conclude by relating our theory to the full algorithm in Section 4.1. The estimation step for γ is the finite sample, U-estimate of the expectation in Equation ( [8](#formula_12)), dropping the nonlinear transform sin and π/2 as these two terms do not change the rankings of the domains. The data selection step directly applies our projection in Equation ( [10](#formula_16)), and we make use of the fact that this projection only relies on rankings among the domains to use γ rather than an exact estimate for θ * .

## ALTERNATIVE METHODS

Our estimator is far from the only reasonable high-dimensional, single-index model estimator. We briefly discuss some alternatives and the tradeoffs involved before moving to experimental results.

We could use classic low-dimensional methods regularized for the high-dimensional setting. This includes ordinal regression (Wooldridge, 2010) and the isotron algorithm [(Kalai & Sastry, 2009)](#b24).

We found these methods to underperform correlation-based estimators, and tuning hyperparameters added additional complexity that was not needed in the correlation-based approaches.

Another class of methods involve scaling laws [(Kaplan et al., 2020;](#b25)[Llama Team, 2024;](#)[Ruan et al., 2024)](#b39). We could transform the y values via an inverse sigmoid or power law, and fit highdimensional linear regression methods (e.g. ridge, partial least squares, or Lasso). We initially found this approach promising, but the inverse transforms were unstable, and the combination of fitting the nonlinear transform and regularization required significant amounts of tuning.

Rank-correlation methods, including our robustified version of the estimator from Chen & Banerjee (2017), and even the standard Spearman correlation [(Spearman, 1904)](#b42) (see Appendix E) performed well. We believe that in general, robust per-feature correlations are likely to perform well as D ≫ N , and extreme levels of regularization are needed to obtain reasonable models. Sparse methods such as the Lasso [(Tibshirani, 1996)](#b44) are one classic answer, but we cannot necessarily assume that the underlying correlations θ * are sparse, and we did not find these techniques to perform well.

## RESULTS

We empirically validate our approach to predicting downstream performance and data selection. Our validation consists of three sets of experiments: we first pretrain 160M-parameter LLMs from scratch to study our primary goal of selecting pretraining data to improve downstream performance, followed by analyzing the ability of losses to predict downstream performance, and conclude with an analysis of the loss matrix X. Throughout our experiments, we use the same single-index model that we train using Algorithm 1. As shown in the algorithm, we train the fastText classifier on selected vs unselected domains and use the classifier to filter the pretraining data at the page-level.

Input data matrix X. To build the input data matrix, X, we collected byte normalized loss values from a sample of 90 Open LLM Leaderboard [(Beeching et al., 2023)](#b3) LLMs that we could run without errors. Concretely, these values are defined as bits-per-byte L T ℓ L B ln([foot_1](#foot_1)) where L T is the token count, L B is the number of UTF-8 bytes, and ℓ is the per-token cross-entropy [(Gao et al., 2020)](#b15). We collected these values on the smallest sample provided by the RedPajama V2 (RPJv2) dataset (Together Computer, 2023) for all domains with ≥ 25 pages in the sample. Specifically, we used the "sample" subset 2 of RPJv2 resulting in 9,841 domains/features. Specifics of how we computed losses are in Appendix C.

Target benchmark performance y. We constructed a target vector, y, for LAMBADA [(Paperno et al., 2016)](#b31), ARC Easy [(Clark et al., 2018)](#b10), PIQA [(Bisk et al., 2020)](#b6), and SciQ [(Welbl et al., 2017)](#b49). These are all of the tasks reported in the Pythia scaling experiments for which a model in the 160M parameter range could meaningfully perform above chance. We also constructed target vectors for LAMBADA IT , LAMBADA FR , LAMBADA DE , and LAMBADA ES , which are subsets of LAMBADA translated into Italian, French, German, and Spanish by [Black (2023)](#b7). These languages match those in RPJv2 where each page is conveniently tagged as one of five languages: English, Spanish, French, German, and Italian. The correspondence between our target benchmark languages and the RPJv2 metadata will be convenient for us, as it will allow us to include language filtering baselines at no additional cost.

## PRETRAINING

We begin by validating our algorithm in the end-to-end task of pretraining data selection with controlled experiments at the 160M parameter, 3.2B token scale. The low compute requirements of this setting allowed us to more extensively study replicates and ablations in Appendix E within the timeframe of a few days. We also note that while 160M models are small, this is far from an easy setting for our data selection algorithm. Most of the Open LLM Leaderboard models are 10 to 100× larger than the 160M scale, and our single index model must extrapolate substantially from ≈7B scale models to our small-scale validation setting (see Appendix G for a histogram of model sizes). Finally, the focus on small-scale validation in this manuscript allows us to commit to a clear preregistration-based strategy toward scaling up (Section 6) rather than cherry-picking experiments that succeed at scale.

Pretraining data and setting. For pretraining, we used the "sample-100B" subset of RPJv2. This is larger than the sample that we used to compute our estimate. We filtered this data so it contains only the domains used for our estimate, and then tokenized the data with the Pythia tokenizer. The vast majority of the domains from our BPB matrix were present in this larger sample of text. However, 42 (out of 9,841) were not, and so we removed them from our estimate. For every data selection method that we tested, the task was to further select 3.2B tokens for pretraining, which is Chinchilla-optimal [(Hoffmann et al., 2022)](#b21) for the 160M-parameter LLM used in our tests.

Table [1](#): Average rankings of each data selection method (lower is better) across 8 benchmarks shows that correlation-based filtering beats baselines by a wide margin, and matches the current best open data filter from [Li et al. (2024)](#b27). Our approach significantly beats the default filter in [Li et al. (2024)](#b27) with the EN filter and loses slightly after additional manual language filtering that depends on the target task (+ manual Lang Filter).

Method None Lang Filt DSIR (Xie et al., 2023b) Handcrafted fastText + EN Lang Filter (Li et al., 2024) Handcrafted fastText w/o Lang Filter Handcrafted fastText + manual Lang Filter Perplexity Correlations Avg. Rank 3.750 4.000 4.500 3.750 3.250 1.375 1.750

Baselines. We compare against several baseline data-selection methods. First, we present the results of uniformly sampling from the available pretraining data. Then we use the language tags present in RPJv2 to filter only for the language matching the target task. In addition to these commonsense baselines, we also run DSIR (Xie et al., 2023b): a lightweight training data selection technique based on n-gram overlaps that [Li et al. (2024)](#b27) found to be competitive with proxy LLM-based techniques and was also validated at scale [(Parmar et al., 2024)](#b32). Finally, we run the state-of-the-art method for pretraining data quality filtering found by Li et al., which is a fastText classifier that beats all of the heavier-weight proxy-LLM methods that were tested. The classifier was trained on a benchmarkagnostic and handcrafted objective, which is to classify data as coming from Common Crawl[foot_2](#foot_2) (low quality) or OH2.5 [(Teknium, 2023)](#b43) and Reddit ELI5 (Fan et al., 2019) (high quality). It is combined with an English filter in Li et al., and so we present results for this fastText filter with and without the additional English filter. Model and hyperparameters. We use the Pythia 160M LLM configuration from Biderman et al. (2023) and optimize the hyperparameters including learning rate, weight decay, and warmup to minimize loss on the uniform sampling (no selection algorithm) baseline. Training hyperparameters were fixed across all methods. We provide additional training and evaluation details in Appendix D.

Results. We report average rankings over all benchmarks in Table [1](#), and we find that our approach significantly outperforms the basic baselines of random sampling, language filtering, and DSIR. Compared to the existing state of the art from [Li et al. (2024)](#b27), our approach beats the performance of the default, English-filtered fastText classifier, but loses slightly once we add in a manual language filtering step to enable better performance on the multilingual LAMBADA datasets. For the maintext comparisons, we use the optional fastText classifier from our algorithm to select pretraining data at the page levels, but we show ablations without the classifier in Appendix E.

Figure [2](#) shows how each data selection method affects benchmark performance in more detail. Each block of rows represents a data selection method, while an individual row represents an LLM within a method that targets a particular benchmark or set of benchmarks. Columns represent benchmarks. We see that language filtering and perplexity correlations both clearly optimize for the target benchmark -within each block, the benchmark column matching each row typically performs best. Surprisingly, the pattern is much less obvious for DSIR -the heatmap looks much more uniform across LLMs with different task targets. We also see that while language filtering has significant impacts on model performance, our performance significantly exceeds the impact of language filtering across all tested benchmarks.

Figure [3](#) shows the distribution of languages in pretraining data selected by our method, targeting each benchmark. Our algorithm provides significant enrichment of the corresponding languages for the multilingual benchmarks (LAMBADA_*), but we also find that it does not exclusively select domains in one language. In contrast, for English benchmarks our approach selects nearly exclusively English data, likely due to the large quantity of high-quality English data in our pretraining data pool. There are significantly fewer tokens in non-English languages in the pretraining data pool and our τ constraint to prevent duplication has a large impact on the weights when the benchmarks are non-English. We provide the same figure when the τ values are made 5× as large in Appendix F.

Finally, we note that our results are somewhat insensitive to the specifics of the perplexity-correlation procedure we present in Algorithm 1. We show in Appendix E that varying the projection method (linear, L 2 ) and even using Spearman rank correlations [(Spearman, 1904)](#b42) often work better than the baselines. These results suggest that the performance of our approach is not tightly dependent on Figure [2](#): Pretraining results with different data selection methods. Each row is an LLM, and each column is a task. The number in the upper left indicates the ranking of the method when targeting that benchmark compared to other methods (lower is better). Numbers within the heatmap denote accuracy for all benchmarks except the LAMBADA tasks for which the values are log perplexities (where lower scores are better). We find that our approach appropriately optimizes data mixes for the target language and benchmark, and matches the fastText baseline across most benchmarks.

the precise form of the estimator that is coupled to our theory results but holds more broadly across general perplexity-correlation relationships. Additionally, our approach performs better with the optional fastText classifier that our algorithm trains, possibly because it can operate at the page-level instead of the domain-level[foot_3](#foot_3) .

## PERFORMANCE RANK PREDICTIONS

We have shown that our approach succeeds at the goal of selecting useful pretraining data, but how good are single index model's predictions? An accurate map between loss to benchmarks would be Figure [3](#): Language distributions of pretraining data selected by perplexity correlations. The default RPJv2 distribution is given in the left column for reference. The English benchmark targets often exclusively select English but the reverse is not the case. In every case, our approach selects more data than the default from the benchmark-matched language (shown as a green box in each column). helpful in selecting among candidate pretraining data mixtures generally, even when not using our pretraining data selection algorithm.

Comparing model performance rankings predicted by our regression to the ground truth, we find generally accurate predictions. Figure [4](#fig_2) shows 5-fold leave-out plots for PIQA, and LAMBADA FR with the rank predictions given by ⟨ θproj , Φ(x)⟩. Every point in the plot is a held-out point: we estimated θ * five times, holding out a different 20% of the data each time, and plotted the prediction for every point when it was held out.

We find that our estimator achieves high ordinal prediction performance across all target tasks. We include 5-fold leave-out R 2 scores for all tasks in Figure [5](#). However, we complement these strong results with the additional observation that simply taking the mean loss across all domains is a strong predictor of model performance (bottom row). The surprising effectiveness of average loss over uniformly sampled documents has been discussed extensively [(Owen, 2024;](#b30)[Wei et al., 2022;](#b48)[Kaplan et al., 2020)](#b25) and our results further suggest that regressions with correlations only slightly above the mean loss baseline still can result in effective data selection methods.

Finally, we discuss outliers in our prediction of model performance. Our predictions are accurate for LLMs with usual architectures (e.g. Mamba [(Gu & Dao, 2024)](#b19)), the smallest/largest vocabulary sizes, context sizes, and parameter sizes. However, we also see that LLMs that were trained on unusual data are not as well predicted by our approach (e.g. Phi [(Gunasekar et al., 2023)](#b20)). We may simply require a bigger or more diverse pretraining data pool and set of models to find estimates that work well for models that expect different styles of text.

Figure [5](#): Held-out R 2 score of our raw correlation estimate θ, our projected estimate θproj , and the average loss baseline. The 95% bootstrapped confidence intervals are wide enough that no individual comparison is significant. Across benchmarks, θproj has statistically significant gains over the baseline (p=0.035) as it is unlikely that θproj beats mean loss 7 times out of 8 by chance. 

$(a) (b) (c) (d) (e) (f) (g) (h) (i) (j)$
## ANALYSIS OF THE MODEL-LOSS MATRIX X

What information is contained in the matrix of model losses X? Clearly, it must contain semantically meaningful information about the data, such as the language that a piece of text is in. We performed PCA [(Pearson, 1901)](#b33) and t-SNE (van der Maaten & Hinton, 2008) on X and plotted the first two components for each of our 9,841 domains. As shown in the first row of Figure [6](#fig_3), we found two components with relatively high singular values. The first component clearly corresponds with the language of a domain. The second component corresponds with the average bits-per-byte or entropy of a domain. The t-SNE components show the same general pattern as well as showing that the language clusters are very well separated. As shown in our plots, there are several salient clusters within the language clusters. Within the English cluster, we found a subcluster for luxury goods, another for legal services and information, another for academic research, and even a cluster for funeral homes.

The second row of Figure [6](#fig_3) shows plots for the loss matrix when we take the principal components of the other dimension, where points correspond to the 90 LLMs. For PCA, PC1 corresponds to entropy. For both cases, it is less clear what the other PCs are, but when we color the three largest families of models in our data (Pythia [(Biderman et al., 2023)](#b4), Qwen [(Bai et al., 2023)](#b2), and OpenLlama [(Geng & Liu, 2023](#b17))), we see that model families are clustered together in the PC graphs.

## ROBUSTNESS CHECKS WITH PRE-REGISTRATION

In small-scale experiments, our approach is competitive with the leading approach from Li et al.'s survey: a fixed fastText model [(Joulin et al., 2016)](#b23), manually augmented with the best language filtering. This leading approach is heuristic and hand-crafted, requiring appropriate language filtering matched to the target benchmark and assumptions about what good pretraining data looks like. Our approach does not make these assumptions and could potentially improve as more public models are released and we have better data to estimate θ * .

While our results are generally positive, many past data selection methods have reported initially positive results, only to later break: they may fail to scale to larger models or rely on specific details of their experimental setting. Our 160M-scale experiments may also raise such concerns.

We design a pre-registered scaling experiment that addresses both the concerns of scale and external validity. We use the permanence of arXiv preprints as a mechanism to preregister a series of scaling experiments within the DataComp-LM framework [(Li et al., 2024)](#b27), which is a testbed for dataselection techniques released with the recent survey. Pre-registering held-out scaling experiments commits us to reporting negative results, and avoid overfitting to our chosen experimental settings.

DataComp-LM is ideal for this preregistered scaling experiment, as it standardizes the setting by providing a pool of 240 trillion tokens, pretraining code for 412M to 7B parameter models, and evaluation code for 53 benchmarks, 22 of which are labelled as "core" benchmarks that scale predictably. Importantly, we have not trained any models on DataComp-LM using our methods or baselines, making this a true held-out experiment with known high-performance baselines.

Preregistered experiment. We will run the best-performing approach from our paper: a fastText filter trained on our correlation estimator. We will define the target benchmark for our estimator as the average of the "core" DataComp-LM benchmarks and run our estimator with perplexities from our set of 90 OpenLM Leaderboard LLMs on a uniform subsample of the DataComp-LM pool of data. We will use the provided DataComp-LM code for training LLMs for the "Filtering 1B-1x" track, where a 1.4B parameter LLM is trained on 28.8B tokens chosen from a 1.64T sample of the DataComp-LM pool of data. In the DataComp-LM paper, they apply their fixed fastText filter as a final step after several complicated deduplication and filtering steps. We will report results where our fastText classifier is used as a direct substitute for this last step alone, as well as another test in which we replace the entire pipeline with one classifier. We will report results where our estimator is trained at the domain-level (following this paper) and where our estimator is trained at the pagelevel (which we have not tried yet). Finally, we will report analogous results where we replace the "core" evaluation score with the average score on all of the non-English LAMBADA translations, and compare the raw fastText classifier from [Li et al. (2024)](#b27) to our approach, using both of these approaches in place of the full filtering pipeline from 1.64T tokens. We preregister this additional multilingual task because "core" does not include multilingual evaluations.

We chose the "Filtering 1B-1x" track because it is the most compute-intensive track that we can perform within a couple weeks, given our resources. For these experiments, the compute needs for our data selection procedure are negligible compared to LLM pretraining. Per [Li et al. (2024)](#b27), the pretraining for these experiments combined is estimated to take 240 hours on an 8-GPU H100 node.

## CONCLUSION

Does high-performance data selection require careful hand-crafted heuristics or prohibitively expensive model training runs? Our work demonstrates an alternative, viable approach -leveraging existing, public models as a source of information for data selection. Pretraining experiments suggest that a simple, correlation-based approach to selecting data can be effective, but more broadly, we show how to 1) use single-index models as a surrogate for downstream performance and 2) build models that relate losses to downstream performance and use these surrogates effectively in data selection. Finally, we propose pre-registered scaling experiments on held-out data to test external validity of reported results. We hope this type of experiment will help improve the trustworthiness of experimental results for data selection.

## A ESTIMATOR SOLUTION

$A.1 LEMMA 1 Statement of Lemma 1 Define the PDF of HalfNormal as f (x; σ) = √ 2 σ √ π e -x 2$2σ 2 for x > 0 and 0 otherwise. Now, suppose:

$• β is a vector with ||β|| 2 = 1 • Z 1 , Z 2 are vectors ∼ N (0, I) • ϵ ∼ N (0, σ 2 ) • Z ′ ∼ N (0, 1) • Z + ∼ HalfNormal(1).$Then we have:

$Z 1j |⟨Z 1 -Z 2 , β⟩ + ϵ > 0 d = Z ′ 1 - β 2 j 2 + σ 2 + β j √ 2 + σ 2 Z + ,$where Z 1j is the j-th entry of Z 1 .

Proof: First, note:

$Z 1j |⟨Z 1 -Z 2 , β⟩+ϵ > 0 d = Z 1j |     Z 1 Z 2 ϵ/σ     ,     β -β σ     > 0 d = Z 1j |     Z 1 Z 2 ϵ/σ     ,     β -β σ     / 2 + σ 2 > 0,$where

$    • • •   $ denotes the vector-valued result of concatenating vectors and scalars. For readability, we

$set Z c =     Z 1 Z 2 ϵ/σ     and β c =     β -β σ     / √ 2 + σ 2 .$Given that β c is unit-norm (by supposition, β is unit-norm), and every element of Z c is ∼ N (0, 1) (even ϵ/σ), we can easily split a conditional random vector containing Z 1j into a conditionally dependent component and independent component:

$Z c |⟨Z c , β c ⟩ > 0 d = (I -β c β ⊤ c )Z ′′ + β c Z + .$The first term is orthogonal to β c and so it is the part of Z c that is not subject to the condition. In the unconditional case, Z c ∼ N (0, I) and so Z ′′ ∼ N (0, I). The second term is the part of Z c that is in the direction of β c . Z + ∼ HalfNormal(I) because our dot product condition is satisfied for half of the possible non-orthogonal Z c values. Now, we focus on finding Z c |⟨Z c , β c ⟩ > 0 for a single index j. We have (for C defined to be the dimensionality of β c ):

$((I -β c β ⊤ c )Z ′′ ) j + (β c Z + ) j = Z ′′ j (1 -β c 2 j ) - 1≤i≤C i̸ =j Z ′′ i β cj β ci + β j Z + j = Z ′′ j - C i=1 Z ′′ i β cj β ci + β j Z + j . Now, note that Z ′′ j - C i=1 Z ′′ i β cj β ci$is the sum of independent zero-mean Gaussians with variances given by 1 and β c

$2 j β c 2 i , so it itself is a zero-mean Gaussian Y ∼ N (0, 1 - C i=1 β c 2 j β c 2 i ). We can also use the fact that C i=1 β c 2 i = 1 (recall that β c is unit norm) to get: Y ∼ N (0, 1 -β c 2 j$). So we have that the conditional Z 1j is given by:

$Z ′ 1 -β c 2 j + β cj Z + = Z ′ 1 - β 2 j 2 + σ 2 + β j √ 2 + σ 2 Z + ,$for Z ′ ∼ N (0, 1). As a corollary, we can see that Z 2j under the same condition is given by:

$Z ′ 1 - β 2 j 2 + σ 2 + -β j √ 2 + σ 2 Z + . A.2 LEMMA 2$Statement of Lemma 2 Suppose that Φ is the CDF of a standard Gaussian, a and c are constants, and Z ∼ N (0, 1). Then we have:

$E[Φ(aZ + c)] = Φ c √ 1 + a 2 .$Proof: By the definition of the CDF of a standard Gaussian, we have:

$E[Φ(aZ + c)] = E[P (X ≤ aZ + c)],$where X ∼ N (0, 1). Continuing, we have:

$= E[P (X -aZ -c ≤ 0)].$Now, note that X -aZ -c is the sum of independent Gaussian random variables with given mean and variance; it itself is a Gaussian random variable ∼ N (-c, a 2 + 1). To find P (X -aZ -c ≤ 0), we can evaluate its CDF at 0:

$= E Φ c √ a 2 + 1 = Φ c √ a 2 + 1 . A.3 LEMMA 3$Statement of Lemma 3 Suppose Φ is the standard Gaussian CDF, Z + ∼ HalfNormal(1), and b and a are constants. Then we have:

$E Φ Z + b √ a 2 + 1 = 1 2 + 1 π tan -1 b √ a 2 + 1 .$Proof: By the definition of expected value, we can take the following integral where f Z+ is the PDF of Z + . We integrate from 0 instead of -∞ because the PDF of the Standard Half Normal is 0 in the domain below 0:

$E Φ Z + b √ a 2 + 1 = ∞ 0 Φ zb √ a 2 + 1 f Z+ (z)dz = ∞ 0 Φ zb √ a 2 + 1 √ 2 √ π e -z 2 2 dz = 1 √ 2π ∞ 0 e -z 2 2 dz + ∞ 0 erf zb √ 2 √ a 2 + 1 e -z 2 2 dz ( * ).$The second integral is generally non-trivial to solve, but luckily we can solve it by using Equation [2](#formula_2)in Section 4.3 of the integral table from [Ng & Geller (1968)](#b29), which states:

$∞ 0 erf(cx)e -d 2 x 2 dx = √ π 2d - 1 d √ π tan -1 d c$Where c and d are real and positive. We split the solution by cases: b > 0, b = 0, and b < 0. We find that in every case, we can manipulate our integral so that the solution is trivial or the constant inside the erf(•) is positive (and so we can use the integral table). In every case, we find that the solution is

$1 2 + 1 π tan -1 b √ a 2 +1 .$Case 1: b > 0. We can use the integral table directly:

$( * ) = 1 √ 2π √ π √ 2 + √ π √ 2 - √ 2 √ π tan -1 √ a 2 + 1 b = 1 2 + 1 2 - 1 π tan -1 √ a 2 + 1 b .$Then, using the identity:

$tan -1 x + tan -1 1 x = π 2 if x > 0,$we find the following:

$= 1 2 + 1 π tan -1 b √ a 2 + 1 .$Case 2: b = 0. Note that erf(0) = 0; we do not have to use the integral table:

$( * ) = 1 √ 2π √ π √ 2 + 0 = 1 2 .$Because tan -1 (0) = 0, we have:

$= 1 2 + 1 π tan -1 b √ a 2 + 1 .$Case 3: b < 0. Because erf(•) is an odd function, we can pull the negative out:

$( * ) = 1 √ 2π ∞ 0 e -z 2 2 dz - ∞ 0 erf z|b| √ 2 √ a 2 + 1 e -z 2 2 dz .$Now we can use the integral table as in the b > 0 case:

$= 1 √ 2π √ π √ 2 - √ π √ 2 + √ 2 √ π tan -1 √ a 2 + 1 |b| = 1 2 + 1 2 - 1 π tan -1 √ a 2 + 1 |b| .$We can then use the same identity again:

$tan -1 x + tan -1 1 x = π 2 if x > 0 to get: = 1 2 - 1 π tan -1 |b| √ a 2 + 1 .$Because tan -1 is an odd function, we can put the negative inside of it:

$= 1 2 + 1 π tan -1 b √ a 2 + 1 . A.4 FULL PROOF$Here, we prove:

$E[sign(y 1 -y 2 )(Φ(x 1 ) -Φ(x 2 ))] = 2 π sin -1 θ * 4 + 2σ 2 1 + 2σ 2 2$with y 1 , y 2 , Φ(x 1 ), Φ(x 2 ), and θ * defined in the main text, for the case where ϵ 1 and ϵ 2 are zeromean Gaussian noise ∼ N (0, σ 2 1 ) and ∼ N (0, σ 2 2 ), respectively. It is easy to see that this is a more general version of the following theorem.

Theorem 1 When ϵ ∼ N (0, σ 2 ), we have:

$E[sign(y i -y j )(Φ(x i ) -Φ(x j ))] = 2 π sin -1 θ * 2 √ 1 + σ 2 . (7$$)$Proof: By symmetry, we have:

$E[sign(y 1 -y 2 )(Φ(x 1 ) -Φ(x 2 ))] = 1 2 E[Φ(x 1 ) -Φ(x 2 )| sign(y 1 -y 2 ) > 0] + 1 2 E[-(Φ(x 1 ) -Φ(x 2 ))| sign(y 1 -y 2 ) < 0].$By increasing monotonicity of f , we have sign(y 

$1 -y 2 ) > 0 ⇐⇒ ⟨x 1 -x 2 , θ * ⟩ + ϵ ∆ > 0, for ϵ ∆ = ϵ 1 -ϵ 2 ∼ N (0, σ 2 1 + σ 2 2 ). So: = 1 2 E[Φ(x 1 ) -Φ(x 2 )|⟨x 1 -x 2 , θ * ⟩ + ϵ ∆ > 0] + 1 2 E[-(Φ(x 1 ) -Φ(x 2 ))|⟨x 1 -x 2 , θ * ⟩ + ϵ ∆ < 0].$$= E[Φ(x 1 ) -Φ(x 2 )|⟨x 1 -x 2 , θ * ⟩ + ϵ ∆ > 0].$By linearity of expectation:

$= E[Φ(x 1 )|⟨x 1 -x 2 , θ * ⟩ + ϵ ∆ > 0] -E[Φ(x 2 )|⟨x 1 -x 2 , θ * ⟩ + ϵ ∆ > 0].$Now, we focus on finding the overall estimate for a single index j. By Lemma 1, we have, for Z ∼ N (0, 1) and Z + ∼ HalfNormal(1):

$Φ(x 1j )|⟨x 1 -x 2 , θ * ⟩ + ϵ ∆ > 0 d = Φ(Za + Z + b 1 ).$Here, a = 1 -

$(θ * j ) 2 2+σ 2 1 +σ 2 2 and b 1 = θ * j √ 2+σ 2 1 +σ 2 2$. As a corollary of Lemma 1, we can see:

$Φ(x 2j )|⟨x 1 -x 2 , θ * ⟩ + ϵ ∆ > 0 d = Φ(Za + Z + b 2 ).$Where

$b 2 = - θ * j √ 2+σ 2 1 +σ 2 2$. So for the index j, our estimate is:

$E[Φ(Za + Z + b 1 )] -E[Φ(Za + Z + b 2 )] = E[E[Φ(Za + c)|c = Z + b 1 ]] -E[E[Φ(Za + c)|c = Z + b 2 ]].$Using Lemma 2, we have:

$= E Φ Z + b 1 √ a 2 + 1 -E Φ Z + b 2 √ a 2 + 1 .$Then, using Lemma 3, we have:

$= 1 2 + 1 π tan -1 b 1 √ a 2 + 1 - 1 2 - 1 π tan -1 b 2 √ a 2 + 1 = 1 π tan -1 b 1 √ a 2 + 1 - 1 π tan -1 b 2 √ a 2 + 1 .$Using the fact that tan -1 is an odd function and b 2 = -b 1 , we get:

$= 2 π tan -1 b 1 √ a 2 + 1 .$Now, we write a and b 1 in terms of θ * j :

$= 2 π tan -1          $.

Using the identity sin -1 x = tan -1

x √ 1-x 2 , we have:

$= 2 π sin -1 θ * j 4 + 2σ 2 1 + 2σ 2 2 .$A.5 COROLLARY 1

Corollary 1 Suppose that θ is any vector of fixed weights and x ∼ N (0, I). Then, conditioning on the event ⟨ θ, x i ⟩ < ⟨ θ, x j ⟩, we have

$⟨ θ, E[Φ(x i ) | ⟨ θ, x i ⟩ < ⟨ θ, x j ⟩]⟩ < ⟨ θ, E[Φ(x j ) | ⟨ θ, x i ⟩ < ⟨ θ, x j ⟩]⟩.(9)$with probability 1.

To see this, we can find:

$E[Φ(x 1 ) -Φ(x 2 )|⟨ θ, x 1 ⟩ + ϵ 1 > ⟨ θ, x 2 ⟩ + ϵ 2 ] = E[Φ(x 1 ) -Φ(x 2 )|⟨ θ, x 1 -x 2 ⟩ + ϵ ∆ > 0]$Note that we have already computed this expected value in the proof above; for an index j, it is:

$2 π sin -1 θj 4 + 2σ 2 1 + 2σ 2 2 .$Because sin -1 is an odd function, the above expression has the same sign as θj . Because the values at every index of E[Φ(x 1 ) -Φ(x 2 )] under our condition and θ are the same sign, we have

$⟨E[Φ(x 1 ) -Φ(x 2 )], θ⟩ > 0, so ⟨ θ, E[Φ(x 1 )]⟩ > ⟨ θ, E[Φ(x 2 )]⟩.$
## B OPTIMAL PROJECTED WEIGHTS SOLUTIONS B.1 LINEAR PROJECTION

Theorem 2 Suppose we want to solve:

$θproj = arg min θ∈R D -⟨θ, θ⟩, subject to: D i=1 θ i = 1 0 ≤ θ i ≤ τ i , ∀i ∈ [1, D],$where τ i > 0 are fixed values. Then, the solution is:

$θproj k =      τ k if j: rj ( θj )≥r k ( θk ) τ j ≤ 1 1 -j: rj ( θj )>r k ( θk ) τ j if j: rj ( θj )≥r k ( θk ) τ j ≥ 1 ∧ j: rj ( θj )>r k ( θk ) τ j ≤ 1 0 otherwise , (10$$)$where r is some function that breaks all ties between θj and θk for k ̸ = j, and otherwise leaves the ordinal relationships the same.

Proof: We proceed by considering each of the three cases from Equation [10](#formula_16).

Case 1. Suppose for the sake of contradiction that the optimal solution is θproj and yet θproj k < τ k for some θproj k falling under the first case of Equation [10](#formula_16). Now suppose that we construct a θ ′ also satisfying the projection constraints that is the same as θproj except in these places:

$θ ′ k = θproj k + ∆ = τ k θ ′ p = θproj p -δ 1 ≥ 0 . . . θ ′ q = θproj q -δ n ≥ 0 for some ∆ = n i=1 δ i > 0 where θp ≥ • • • ≥ θq$are all of the θ values which do not fall under the first condition and where the corresponding θproj values are nonzero. We know that there must be some θproj p , • • • , θproj q from which we can subtract δ 1 , • • • , δ n (and so from which we can take the ∆) because j: rj ( θj )≥r k ( θk ) τ j ≤ 1. Now, we have:

$⟨ θ, θproj ⟩ -⟨ θ, θ ′ ⟩ = θk θproj k + θp θproj p + • • • + θq θproj q -θk θproj k -θk ∆ -θp θproj p -• • • -θq θproj q + θp δ 1 + • • • + θq δ n = -θk ∆ + θp δ 1 + • • • + θq δ n ≤ θp (δ 1 + • • • + δ n ) -θk ∆ = θp ∆ -θk ∆ ≤ 0.$At this point, the only way to avoid the contradiction result would be if θk = θp = • • • = θq . Otherwise, the above non-strict inequality would be a strict inequality. If θk = θp = • • • = θq , then we know that θk is the smallest θ value satisfying condition 1 and all of the other greater θ values satisfying condition 1 must be projected to their τ threshold value (otherwise we would get the contradiction result). In this edge case can see above that rearranging the remaining weight among equal θ values does not change the dot product, so all of the solutions that we can get without the contradiction result are equivalently optimal (including the solution from Equation [10](#formula_16)).

Case 3. This is analogous to case 1. Suppose for the sake of contradiction that the optimal solution is θproj and yet θproj k > 0 for some θproj k falling under the third case of Equation [10](#formula_16). Now suppose that we construct a θ ′ also satisfying the projection constraints that is the same as θproj except in these places:

$θ ′ k = θproj k -∆ = 0 θ ′ p = θproj p + δ 1 ≤ τ p . . . θ ′ q = θproj q + δ n ≤ τ q$for some ∆ = n i=1 δ i > 0 where θp ≥ • • • ≥ θq are all of the θ values which do not fall under the third condition and where the corresponding θproj values are not at their thresholds. By construction we know that there must be some θproj p , • • • , θproj q to which we can add δ 1 , • • • , δ n . Now, we have:

$⟨ θ, θproj ⟩ -⟨ θ, θ ′ ⟩ = θk θproj k + θp θproj p + • • • + θq θproj q -θk θproj k + θk ∆ -θp θproj p -• • • -θq θproj q -θp δ 1 -• • • -θq δ n = θk ∆ -θp δ 1 -• • • -θq δ n ≤ -θq (δ 1 + • • • + δ n ) + θk ∆ = -θq ∆ + θk ∆ ≤ 0.$At this point, the only way to avoid the contradiction result would be if θk = θp = • • • = θq . Otherwise, the above non-strict inequality would be a strict inequality. If θk = θp = • • • = θq , then we know that θk is the largest θ value satisfying condition 3 and all of the other smaller θ values satisfying condition 3 must be projected to 0 (otherwise we would get the contradiction result). In this edge case, we can see above that rearranging the remaining weight among equal θ values does not change the dot product, so all of the solutions that we can get without the contradiction result are equivalently optimal (including the solution from Equation [10](#formula_16)).

Case 2. Above, we show that both Case 1 and Case 3 are true. So, the remaining weight must be given to the single value of θproj not covered by either case.

B.2 QUADRATIC PROJECTION B.2.1 LEMMA 4

Statement of Lemma 4 Suppose that θproj is the optimal solution to:

$θproj = arg min θ∈R D || θ -θ|| 2 2 ,$subject to:

$D i=1 θ i = 1 0 ≤ θ i ≤ τ i , ∀i ∈ [1, D],$where τ i > 0 are fixed values. Then, θproj s = 0 implies that any j with θs > θj must have θproj j = 0.

Proof: This is similar to Lemma 2 from [Shalev-Shwartz & Singer (2006)](#b40). Assume for the sake of contradiction θproj s = 0 and θs > θj , yet we have θproj j > 0.

Now we can construct another vector θ ′ that is the same as θproj , except in two places:

$θ ′ s = θproj s + ∆ θ ′ j = θproj j -∆,$for some ∆ satisfying 0 < ∆ < min( θproj j , τ s -θproj s ). This bound on ∆ ensures that θ ′ is still within the thresholds. We know that ∆ can exist because min( θproj j , τ s -θproj s ) > 0 (by supposition, τ s -θproj s = τ s -0 > 0 and θproj j > 0).

## Now we can compute

$: || θ -θproj || 2 2 -|| θ -θ ′ || 2 2 = ( θs -θproj s ) 2 + ( θj -θproj j ) 2 -( θs -( θproj s + ∆)) 2 -( θj -( θproj j -∆)) 2 = 2∆(( θs -θproj s ) -( θj -θproj j ) -∆) > 2∆(( θs -θproj s ) -( θj -θproj j ) -min( θproj j , τ s -θproj s )) ≥ 2∆(( θs -θproj s ) -( θj -θproj j ) -θproj j ) = 2∆( θs -θj ) > 0.$So θproj cannot be the optimal solution.

## B.2.2 LEMMA 5

Statement of Lemma 5 Suppose that θproj is the optimal solution to:

$θproj = arg min θ∈R D || θ -θ|| 2 2 ,$subject to:

$D i=1 θ i = 1 0 ≤ θ i ≤ τ i , ∀i ∈ [1, D],$where τ i > 0 are fixed values. Then, θproj s = τ s implies θproj j = τ j for any θj -τ j > θs -τ s .

Proof: Again, this is similar to Lemma 2 from [Shalev-Shwartz & Singer (2006)](#b40). Assume for the sake of contradiction θproj s = τ s and θj -τ j > θs -τ s , yet we have θproj j < τ j .

Now we can construct another vector θ ′ that is the same as θproj , except in two places:

$θ ′ s = θproj s -∆ θ ′ j = θproj j + ∆,$for some ∆ satisfying 0 < ∆ < min( θproj s , τ j -θproj j ). This bound on ∆ ensures that θ ′ is still within the thresholds. We know that ∆ can exist because min( θproj s , τ j -θproj j ) > 0 (by supposition, τ j -θproj   [et al., 2023b)](#), despite its simplicity, requires some tuning. A decision must be made about how to format the bemchmark data into a single piece of text per example so that it can be compared with potential pretraining data in terms of n-gram overlap. The LAMBADA tasks only have one text column per example, so the decision here is trivial. Examples from the other tasks each have a question, possibly a context, and a set of multiple choice answers to choose from. We chose to concatenate all of these columns together with spaces to form one piece of text per example, duplicating the same question as a prefix for each different answer.

DSIR does not allow the user to specify the exact number of unique tokens desired for pretraining. It only allows the specification of the number of unique pages, which can have wildly varying token counts. For every DSIR job, we set the desired number of pages to 3325589, which we found through binary search to produce slightly more than 3.2B unique tokens for LAMBADA FR . It was expensive to find this number for even one bechmark, because for each iteration of the binary search, we had to run DSIR and then the Pythia tokenizer to know how many tokens resulted from the input page number parameter. We provide the number of unique tokens from DSIR for each task in Table [3](#tab_2). We pretrained on 3.2B tokens for every LLM regardless of whether all of them were unique.

## D.4 FASTTEXT

The "SOTA" fastText model from [Li et al. (2024)](#b27) is available here: [https://huggingface.co/ mlfoundations/fasttext-oh-eli5](https://huggingface.co/mlfoundations/fasttext-oh-eli5). We used this model to filter data by sorting pages by the model's "high quality" score, including the top pages in order until we had either reached or gone slightly over 3.2B unique tokens. This aligns with the data-selection procedure in the original paper, and is also essentially the same as running the linear projection (Equation [10](#formula_16)) at the page-level. We also applied this method when selecting data using our own fastText filter trained by our algorithm.

## E ADDITIONAL PRETRAINING RESULTS

In Figure [7](#), we present additional pretraining results for methods in our loss-performance correlation data selection paradigm. We find that using Spearman rank correlation [(Spearman, 1904)](#b42) in place of our estimator achieves comparable performance. On some tests, it performs even better than our estimator. We also find that using the quadratic projection, while perhaps more intuitive, leads to worse performance than the linear projection.

F PRETRAINING TOKEN DISTRIBUTION WITH 5 × τ

Figure [8](#fig_6) shows what the projected estimate in our pretraining experiments would be if we had a pretraining data pool 5× as large. We see here that the estimate does an even better job at selecting pretraining data with the language that matches the target task. We see that our approach selects even more relevant data when the selection pool is larger. The spike around 6.7B parameters is due to a large number of partially trained Pythia [(Biderman et al., 2023)](#b4) checkpoints from the same training run at that scale. Our algorithm has the hard task of selecting pretraining data for 160M parameter models, which is abnormally small in the set of models used to compute the estimate.

![Figure1: We want to pretrain on domains where lower loss is generally correlated with higher downstream performance. Our approach does this by taking public, pretrained LLMs and measuring correlations across their log-likelihoods (left, red matrix) and performance on a target benchmark (center, blue vector). We then perform data selection by picking domains with high correlation and training a fastText classifier that distinguishes these domains from others. This approach is on par with the best-known data selection methods in our experiments, despite requiring no human selection of high-quality domains.]()

![Perplexity Correlation Based Data SelectionInput: Benchmark error vector y ∈ [0, 1] N , log-loss matrix normalized as bits-per-byte X ∈ R + ×D , available tokens per domain a ∈ N D , and pretraining token target b ∈ N.]()

![Figure 4: Rank predictions given by ⟨ θproj , Φ(x)⟩ for PIQA and LAMBADA FR. A standard deviation (σ) from the ideal fit is shown in red. 2σ is shown in orange. Many models outside 2σ (named and shown in blue) are trained on atypical data such as heavily multilingual data, code, or GPT-4 (Brown et al., 2020) outputs. Models with atypical architectures (i.e. Mamba (Gu & Dao, 2024)) are named and shown in black. Generally, our estimate tightly predicts ordinal benchmark performance from web corpus losses.]()

![Figure 6: Analysis of the loss matrix. The first row treats domains as examples to be projected via PCA, while the second row treats models as examples. Panels (a): eigenvalue decay for the eigendecomposition of the D×D covariance matrix resulting from the loss matrix; a few dominant PCs are seen. (b) and (c): domains plotted by the first two PCA components showing separation of language in b and entropy in c. (d,e) show analogous plots in t-SNE with a clearer separation of language. (f): eigenvalue decay analogous to (a). (g,h): models plotted by the first two PCA components showing clustering by model family (clusters show Pythia(Biderman et al., 2023), Qwen(Bai et al., 2023), and OpenLlama(Geng & Liu, 2023) derivatives -the three largest clusters in our data), and average model loss. (i,j) show analogous results under t-SNE where (i) is normalized to remove per-model entropy differences.]()

![x 2 and ϵ ∆ d = -ϵ ∆ , the two expected values above are the same:]()

![and θproj s = τ s > 0).Now we can compute:|| θθproj || 2 2 -|| θθ ′ || 2 2 = ( θsθproj s ) 2 + ( θjθproj j ) 2 -( θs -( θproj s -∆)) 2 -( θj -( θproj j + ∆)) 2 = 2∆(( θjθproj j ) -( θsθproj s ) -∆) > 2∆(( θjθproj j ) -( θsθproj s ) -min( θproj s , τ j -θproj j )) ≥ 2∆(( θjθproj j ) -( θsθproj s ) -(τ j -θproj j )) = 2∆(( θj -τ j ) -( θsθproj s )) = 2∆(( θj -τ j ) -( θs -τ s )) > 0.So θproj cannot be the optimal solution.B.2.3 FULL PROOFTheorem 3 Suppose we want to solve:θproj = arg min θ∈R D || θ -θ|| 2 2 ,]()

![Figure 8: This figure is analogous to Figure 3, except the τ thresholds have been multiplied by 5.We see that our approach selects even more relevant data when the selection pool is larger.]()

![Figure 9: The parameter-count histogram of the 90 models from the Open LLM Leaderboard (Beeching et al., 2023) that we used to compute our estimate for pretraining data selection. Bar widths are 160M. The smallest model in the sample has ≈33M parameters and the largest has ≈9B.The spike around 6.7B parameters is due to a large number of partially trained Pythia(Biderman et al., 2023) checkpoints from the same training run at that scale. Our algorithm has the hard task of selecting pretraining data for 160M parameter models, which is abnormally small in the set of models used to compute the estimate.]()

![]()

![Unique pretraining tokens selected per benchmark, from DSIR.]()

To be precise, we use bits-per-byte, which normalizes the sequence negative log-likelihood with the number of UTF-8 bytes. This is defined in terms of the length of the string in tokens LT , the length of the string in UTF-8 bytes LB, and the cross entropy loss ℓ as BPB = L T ℓ L B ln(2)

https://huggingface.co/datasets/togethercomputer/RedPajama-Data-V2

https://commoncrawl.org

One might ask if the fastText classifier alone can be an effective data selector, since it appears in both high-performance selection methods. This is not possible -the classifier can only amplify the effectiveness of an existing selector rather than replace it -as training a classifier requires supervision, which is provided by either the perplexity correlation algorithm or human curation (in the case of[Li et al. (2024)](#b27)).

