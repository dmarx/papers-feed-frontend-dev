{
  "arxivId": "2409.05816",
  "title": "Improving Pretraining Data Using Perplexity Correlations",
  "authors": "Tristan Thrush, Christopher Potts, Tatsunori Hashimoto",
  "abstract": "Quality pretraining data is often seen as the key to high-performance\nlanguage models. However, progress in understanding pretraining data has been\nslow due to the costly pretraining runs required for data selection\nexperiments. We present a framework that avoids these costs and selects\nhigh-quality pretraining data without any LLM training of our own. Our work is\nbased on a simple observation: LLM losses on many pretraining texts are\ncorrelated with downstream benchmark performance, and selecting\nhigh-correlation documents is an effective pretraining data selection method.\nWe build a new statistical framework for data selection centered around\nestimates of perplexity-benchmark correlations and perform data selection using\na sample of 90 LLMs taken from the Open LLM Leaderboard on texts from tens of\nthousands of web domains. In controlled pretraining experiments at the 160M\nparameter scale on 8 benchmarks, our approach outperforms DSIR on every\nbenchmark, while matching the best data selector found in DataComp-LM, a\nhand-engineered bigram classifier.",
  "url": "https://arxiv.org/abs/2409.05816",
  "issue_number": 658,
  "issue_url": "https://github.com/dmarx/papers-feed/issues/658",
  "created_at": "2025-01-04T06:53:00.603803",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 3,
  "last_read": "2025-01-04T06:53:00.604590",
  "last_visited": "2024-12-30T20:20:51.220Z",
  "main_tex_file": null,
  "published_date": "2024-09-09T17:23:29Z",
  "arxiv_tags": [
    "cs.CL",
    "cs.LG",
    "stat.ML"
  ]
}