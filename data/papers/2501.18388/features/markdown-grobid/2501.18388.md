# Improved Replicable Boosting with Majority-of-Majorities

## Abstract

## 

We introduce a new replicable boosting algorithm which significantly improves the sample complexity compared to previous algorithms. The algorithm works by doing two layers of majority voting, using an improved version of the replicable boosting algorithm introduced by Impagliazzo et al. [2022]  in the bottom layer.

## Introduction

Replicability of an algorithm is a property introduced as a reaction to what is called the reproducibility crisis. Multiple Nature articles have pointed out the issue of researchers not being able to replicate findings [[Baker, 2016](#b1)[, Ball, 2023]](#b2). As a supplement to implementing better research practices in order to ensure replicability, [Impagliazzo et al. [2022]](#b10) introduced the concept of replicability as a property of algorithms themselves. Informally, an algorithm is replicable if it, with high probability, outputs the same result when run with different input data drawn from the same distribution.

Definition 1.1 (Replicability [[Impagliazzo et al., 2022]](#b10)). Let A be a randomized algorithm. Then, A is said to be ρ-replicable if there is an n ∈ N such that for all distributions D on some space X , it holds that P S1,S2,r [A(S 1 ; r) = A(S 2 ; r)] ≥ 1 -ρ where S 1 , S 2 ∼ D n are independent, and r denotes the internal randomness used by A.

As is evident from the definition, we require the algorithm to use the same internal randomness r in both runs. This turns out to be crucial -if we remove this requirement, we cannot solve simple tasks such as estimating the mean of a distribution replicably [[Dixon et al., 2024]](#b5). Researchers who use replicable algorithms may then publish the random seed used in their run of the algorithm which lets other researchers use the same seed to replicate the results with high probability, assuming that the data they use comes from the same underlying distribution.

In this work, we consider replicability in the weak-to-strong learning setting. Specifically we improve the best known sample complexity of ρ-replicable boosting algorithms. Let X be an input domain, and let f : X → {-1, 1} be the function we are trying to predict. An algorithm W is said to be a γ-weak learner for γ ∈ (0, 1/2) if there exists an m ∈ N such that for any distribution D on X and any sequence of m labelled samples S = {(x i , f (x i ))} m i=1 drawn i.i.d. from D, it holds that h := W(S) : X → {-1, 1} satisfies P x∼D [h(x) = f (x)] ≤ 1/2 -γ. We call γ the advantage of W and m the sample complexity of W. A strong learner on the other hand, is a learning algorithm such that for any distribution D on X , failure probability δ > 0 and error ε > 0, there is an m = m(ε, δ) ∈ N such that when applied to an i.i.d. sample S ∼ D m , the algorithm outputs a classifier which has error at most ε over D with probability at least 1 -δ. We denote the error of a hypothesis h : X → {-1, 1} with respect to a distribution D by

$Er D (h) = P x∼D [h(x) = f (x)].$Boosting algorithms were originally introduced to answer the following theoretical question posed by [Kearns [1988]](#b12), [Kearns and Valiant [1994]](#b13): Is it possible to combine hypotheses produced by a weak learning algorithm into a strong learner? As shown by [Schapire [1990]](#b14), this turned out to be the case, and one of the most famous algorithms that solves this problem is the A B algorithm [[Freund and Schapire, 1995]](#b9). In short, boosting works by running a number of iterations. In each iteration t, we update a distribution D t on the samples and run the weak learner with this new distribution. After a sufficient number of iterations, we take a weighed majority vote among all the produced weak hypotheses.

## Our contribution

Our main contribution is a replicable boosting algorithm called M B which is inspired by an existing replicable boosting algorithm, B [[Impagliazzo et al., 2022]](#b10) and the S B algorithm [[Servedio, 2001]](#b15). First, fix a distribution D on X and let W denote a replicable weak learner and for ρ ∈ (0, 1) let m W(ρ) be the sample complexity of W when run with replicability parameter ρ. Our main result is the following:

$Theorem 1.2 ( M B$). For any ρ, ε ∈ (0, 1) and Θ(ργ 2 )-replicable weak learner W with advantage γ, M B is ρ-replicable, makes O( ln(1/ε) γ 2 ) calls to W, and with probability at least 1 -ρ outputs a hypothesis H with Er D (H) ≤ ε. Furthermore, its sample complexity is

$O m W( Θ(ργ 2 )) εγ 2 + 1 ρ 2 εγ 3 .$Our algorithm significantly improves on the sample complexity of [Impagliazzo et al. [2022]](#b10) which is

$O m W (Θ(ρεγ 2 )) ε 2 γ 2 + 1 ρ 2 ε 5 γ 6 .$Note that this sample complexity is not what is stated in their paper, but is in fact the correct sample complexity of their algorithm. We improve the first term by a factor 1/ε and also remove a factor ε in the replicability parameter to the weak learner. Since the sample complexity of most replicable algorithms has a quadratic dependence on their replicability parameter, this will amount to an extra 1/ε 2 improvement in this term. In the second term we shave off a factor 1/(ε 4 γ 3 ). All improvements are up to logarithmic factors.

As a secondary contribution, we introduce an algorithm T for performing a replicable threshold check. This algorithm replicably checks if the expected value of a function ϕ is above a certain threshold z, and is used as a subroutine in M B

. We state the guarantees of the algorithm below.

## Lemma 1.3 ( T

). Let z, ρ ∈ (0, 1), δ ∈ (0, ρ/8], let ϕ : X → [0, 1] and let S = (x 1 , . . . , x m ) be samples drawn i.i.d. from distribution D. Then there exists a constant c such that if m ≥ c ln(1/δ) ρ 2 z , T (S, z, ϕ) is ρ-replicable and returns a bit b such that with probability at least 1 -δ:

•

$If E x∼D [ϕ(x)] ≤ z/2, then b = 0. • If E x∼D [ϕ(x)] ≥ 2z, then b = 1.$We believe this algorithm is also of independent interest and can be applied in many scenarios as an alternative to statistical queries which were previously used for such applications. This is because our algorithm achieves a dependence of 1/z in the sample complexity, while using statistical queries for the same purpose comes with a factor 1/z 2 in the sample complexity [(Thm. 2.3, Impagliazzo et al. [2022]](#)). While our approach to threshold checks is not neccesarily novel, it seems to have been overlooked in the context of replicable algorithms.

## Related work

In recent years, replicable algorithms have been developed in a variety of settings. This includes e.g. learning half spaces, clustering, reinforcement learning and online learning [[Kalavasis et al., 2024](#b11)[, Esfandiari et al., 2024](#b8)[, Eaton et al., 2024](#b7)[, Ahmadi et al., 2024]](#b0).

There are also important connections to the field of differential privacy. Intuitively, a replicable algorithm does not depend heavily on the specific sample given to the algorithm. This is similar to the requirement in differential privacy where we demand that when the algorithm is run on two samples differing in only a single point, then the two distributions on the outputs are close in the sense of max divergence. [Bun et al. [2023, Thm. 3.1]](#) show that there is a reduction "without substantial blowup in runtime or sample complexity" from differential privacy to replicability. On the other hand, they also show that no computationally efficient transformation of differentially private algorithms to

We have personally contacted the authors to make them aware, and they have acknowledged this error.

replicable ones can exist under standard cryptographic assumptions. However, if one does not care about computational efficiency, they do give a reduction from differential privacy to replicability with only a quadratic blowup in sample complexity. This means it would be possible to take an existing differentially private boosting algorithm and make it replicable. One example of a differentially private boosting algorithm is B F P [[Dwork et al., 2010](#b6)]. However, using the reduction on this algorithm would incur a 1/γ 8 and 1/ε 2 dependence in the sample complexity.

Moving away from differential privacy, another candidate algorithm to be made replicable is the S B algorithm [[Servedio, 2001]](#b15). This algorithm differs from e.g. the well-known A B [[Freund and Schapire, 1995]](#b9) in that it maintains a smoothness across the distributions D t over the data in every iteration t. Formally, this means that the distribution D t satisfies max x D t (x) ≤ 1/(εm) for some ε > 0 where m is the number of samples. This smoothness property ensures that no single example has too much influence on the distributions which is why smoothness is a desirable property when designing replicable boosting algorithms. In fact, the boosting algorithm by [Impagliazzo et al. [2022]](#b10) can be seen as a translation of S B into the replicable setting. The downside of using S B is that it requires O( 1 εγ 2 ) invocations of the weak learner W. We call this the round complexity of the algorithm. This should be compared to A B which has round complexity O( ln(1/ε) γ 2 ). In the replicable setting, we draw new samples for each invocation of W, so the round complexity directly affects the number of samples used. This motivates looking at smooth boosting algorithms with fewer invocations of W such as the one presented by [Barak et al. [2009]](#b3). This algorithm uses Bregman projections to maintain the smoothness property, and it matches the round complexity of A B

. However, converting the algorithm to the replicable setting would require us to make replicable approximations of these Bregman projections which turns out to use more samples than we obtain in Theorem 1.2.

## High-Level Ideas

We will now explain the very high-level idea behind our new boosting algorithm M B

. The first step towards constructing this improved replicable boosting algorithm is to make slight modifications to the algorithm B of [Impagliazzo et al. [2022]](#b10) to improve its sample complexity. We will refer to this modified version as B * which can be found in Algorithm 1. Remark that the functions g t , µ t are functions over the entire domain X and not just the samples that we see. This means that we cannot afford to update these functions explicitly for every point, so instead we update the description of the functions. To distinguish this from normal assignments in the pseudocode, we use the def = operator for assignments to these functions and the ← operator for normal assignments.

In this algorithm µ t : X → [0, 1] is a function which determines the reweighing of the data distribution D in iteration t. The reweighed distribution is then

$D µt (x) = µ t (x)D(x)/d(µ t ) where d(µ t ) = E x∼D [µ t (x)]$is the normalization factor which we call the density of µ. The subroutine R S then lets us sample from the distribution D µt when given access to µ t and samples from D (see Lemma 2.1 for formal guarantee). We also note without proof that large density of µ t actually implies smoothness of the reweighed distribution D µt with respect to the original distribution D. More precisely, if d(µ t ) ≥ ε, for some ε > 0, then D µt (x) ≤ D(x)/ε for all x ∈ X . These samples from D µt are then given to the replicable weak learner. We will not go into further detail with how or why the original B works but instead refer to [Impagliazzo et al. [2022]](#b10), [Servedio [2001]](#b15). In total, we have made two modifications in B * . The first modification is that we have changed the termination condition in line 14 to use our T algorithm instead of the statistical query algorithm they used. This accomplishes exactly the same thing, but uses a factor 1/ε fewer samples for each call. The second modification is the introduction of the if-statement in line 12. It turns out that this check only makes the algorithm run for a constant factor more iterations. However, this allows us to shave off a factor 1/γ in the number of calls to T . Since the replicability parameter of T needs to be ρ divided by the number of calls to T , this is a great improvement. This is because the sample complexity of T is inversely proportional to the square of its replicability parameter, so it will need a factor 1/γ 2 fewer samples for each invocation of T . Since it is now only called every 1/γ iteration, we shave off a factor 1/γ 3 in total by introducing this check. In Section 3, we will explain in more detail why these modifications preserve correctness, but for now we will just state the guarantees of B * .

Algorithm 1 B * ρ,ε (S, W) Input: Samples S i.i.d. from D, replicable γ-weak learner W, replicability ρ, error ε. Output: Hypothesis H : X → {-1, 1}.

$1: g 0 (x) def = 0 2: µ 1 (x) def = 1 3: t ← 0 4: while true do 5: t ← t + 1 6: D µt (x) def = µ t (x)D(x)/d(µ t ) 7: S 1 ← O(m W(Θ(ρεγ 2 )) /ε) fresh samples from S 8: S W ← R S (S 1 , m W(Θ(ρεγ 2 )) , µ t ) 9:$h t ← Run W(S W ) with replicability Θ(ρεγ 2 ) 10:

$g t (x) def = g t-1 (x) + h t (x)f (x) -γ/(2 + γ) 11: µ t+1 (x) def = 1, if g t (x) ≤ 0 (1 -γ) gt(x)/2 , if g t (x) > 0 12: if ⌊ 1 γ ⌋ divides t then 13: S 2 ← O 1 ρ 2 ε 3 γ 2 fresh$samples from S 14: if T (S 2 , ε/2, µ t ) = 0 then 15: Exit while loop 16: Return: H ← sign( t h t ) Theorem 1.4 ( B * ). For any ρ, ε ∈ (0, 1) and Θ(ρεγ 2 )-replicable weak learner W with advantage γ, B * is ρ-replicable, makes O( 1 εγ 2 ) calls to W, and with probability at least 1 -ρ outputs a hypothesis H with Er D (H) ≤ ε. Furthermore, its sample complexity is

$m B * (ρ, ε) = O ln( 1 ρεγ 2 )m W(Θ(ρεγ 2 )) ε 2 γ 2 + ln( 1 ρεγ ) ρ 2 ε 4 γ 3 = O m W(Θ(ρεγ 2 )) ε 2 γ 2 + 1 ρ 2 ε 4 γ 3 .$Remember that the original version of B had sample complexity O(

$m W (Θ(ρεγ 2 )) ε 2 γ 2 + 1 ρ 2 ε 5 γ 6 ).$The dependence on γ has therefore improved greatly. However, we are still not happy with the dependence on ε. The main idea of our algorithm is therefore to use B * as a subroutine and only call it with constant error parameter ε 0 = 1/16. Our algorithm can be seen as a meta boosting algorithm where in each iteration, we call B * to get a hypothesis with constant advantage. We then perform exponential weight updates similar to A B in order to make our algorithm only run for T = O(ln(1/ε)) iterations. Remark that this entirely removes the problem of B * having a bad dependence on ε, since we only invoke it with a constant error parameter. This is the main insight needed to understand how our algorithm works.

## Our Replicable Boosting Algorithm

In this section, we will present our new ρ-replicable boosting algorithm which can be found in Algorithm 2.

The algorithm runs for T iterations while maintaining functions N t , M t , µ t . In each iteration the algorithm performs rejection sampling to get samples S 2 drawn from distribution D µt . It then gets a hypothesis h t from B * which has constant error of at most ε 0 with respect to D µt . One can interpret N t (x) as a lower bound for counting how many of the first t -1 hypotheses that misclassify element x. However, in order to ensure high density of the updated reweighing function µ t , we check if the points sharing the largest count have a total probability mass of at least ε/16 by using T . If not, we subtract 1 from the largest count which suffices to ensure high density of µ t (see Lemma 2.4). The capped values are stored in M t , and will be used in subsequent iterations. The value c t can be interpreted as a bound for the largest allowed count in iteration t, that is c t ≥ M t (x) for all x ∈ X .

## Algorithm 2 M B

ρ,ε (S, W) Input: Samples S i.i.d. from D, replicable γ-weak learner W, replicability ρ, error ε. Output: Hypothesis H : X → {-1, 1}.

$1: N 1 (x) def = 0 2: M 1 (x) def = 0 3: µ 1 (x) def = 1 4: c 1 ← 0 5: for t = 1 to T do ⊲ T = O(1/ε) 6: D µt (x) def = µ t (x)D(x)/d(µ t ) 7: S 1 ← O(m B * (ρ 0 , ε 0 )/ε) fresh samples from S ⊲ ρ 0 = ρ/(6T ), ε 0 = 1/16 8: S 2 ← R S (S 1 , m B * (ρ 0 , ε 0 ), µ t ) 9: h t ← B * ρ0,ε0 (S 2 , W) 10: N t+1 (x) def = M t (x) + 1{h t (x) = f (x)} 11: S 3 ← O( 1 ρ 2 ε ) fresh samples from S 12: b t+1 ← T (S 3 , ε/16, ϕ) ⊲ ϕ(x) = 1{N t+1 (x) = c t + 1} 13: c t+1 ← c t + b t+1 14: M t+1 (x) def = min(N t+1 (x), c t+1 ) 15: µ t+1 (x) def = exp(M t+1 (x) -c t+1 ) 16: Return H = sign T t=1 h t$Now, before going into the analysis of the algorithm, we will present the guarantees of the R S which we use to draw samples from D µ . The pseudocode and proofs of the below guarantees are described by [Impagliazzo et al. [2022]](#b10), so we will not repeat those here.

Lemma 2.1 (Rejection Sampling [[Impagliazzo et al., 2022]](#b10)). For any ε ∈ (0, 1], if µ has density d(µ) ≥ ε and S ∼ D m where m ≥ 8 ln(1/δ)m target /ε, then R S (S, m target , µ) outputs a sample S out ∼ D mtarget µ with probability at least 1 -δ.

Lemma 2.2 (Composing Replicable Algorithms with Rejection Sampling [[Impagliazzo et al., 2022]](#b10)). Let A(S) be a ρ-replicable algorithm with sample complexity m. Let µ : X → [0, 1]. Then let B be the algorithm that runs A with samples drawn from D µ using rejection sampling. Let q be the failure probability of R S . Then B is (2q + 2ρ)-replicable.

## Analysis of M B

We are now ready to analyze M B . To make it easier to follow the analysis, we will split it into four parts. 1. Correctness, 2. Replicability, 3. Sample complexity, 4. Failure probability. We will start with correctness. However, before going into the formal details, we will give an explanation of the high level ideas in the proof. First, observe that if we did not cap the weights N t , the multiplicative weight updates would be very similar to the updates made in A B

. Recall that for any t ∈ [T ], M t (x) is exactly the number of misclassifications of x minus the amount of times we have capped the weight so far. Hence, if we did not cap the weights by c t each iteration, x would be misclassified by the final hypothesis H only if M T +1 (x) ≥ T /2. We will now take the capping into account. We first show using an argument similar to the standard analysis of A B that the probability of drawing an x from D for which M T +1 (x) ≥ T /4 is at most ε/2. What remains is to argue that the probability of drawing an x which is misclassified but simultaneously satisfies M T +1 (x) < T /4 is small. The only way this can happen is if there were at least T /4 iterations in which we capped down the value of N t+1 (x) when calculating M t+1 (x), since in such iterations we would not increment M t+1 (x) even though h t misclassified x. Observe that due to the threshold check, the total probability mass (w.r.t. D) of points whose value of N t+1 were capped in a single iteration cannot exceed ε/8. Therefore, after T iterations, the total probability mass of points, whose value of N t+1 were capped T /4 times is at most T (ε/8)/(T /4) = ε/2. So in total, the probability mass of all the misclassified points is at most ε. We now prove this formally.

Lemma 2.3 (Correctness). Put T ≥ 8 ln(2/ε) and ε 0 = 1/16. Assuming that all subroutines of the algorithm succeed in every iteration, we achieve an error of at most ε over the distribution D, i.e. Er D (H) ≤ ε.

$Proof. By definition of M T +1 , N T +1 and µ T E[exp(M T +1 (X))] ≤ E[exp(N T +1 (X))] = E[exp(M T (X)) exp(1{h T (X) = f (X)})] = e cT E[µ T (X) exp(1{h T (X) = f (X)})] = e cT E[µ T (X)1{h T (X) = f (X)}] + eE[µ T (X)1{h T (X) = f (X)}] .$(1)

Now, since D µT = µT •D d(µT ) , we can rewrite the above to an expectation involving Y ∼ D µT such that ( [1](#)) is equal to

$e cT d(µ T ) E[1{h T (Y ) = f (Y )}] + eE[1{h T (Y ) = f (Y )}] = e cT d(µ T ) Er Dµ T (h T )(e -1) + 1 ≤ e cT d(µ T ) exp (e -1) Er Dµ T (h T ) ≤ e cT d(µ T ) exp(2ε 0 ),(2)$where the final inequality follows since h t has error at most ε 0 under D µT by Theorem 1.4. Now, note that

$e cT d(µ T ) = e cT E[µ T (X)] = e cT E[exp(M T (X) -c T )] = E[exp(M T (X))].$Plugging this into (2), we recursively get

$E[exp(M T +1 (X))] ≤ E[exp(M T (X))] exp(2ε 0 ) ≤ • • • ≤ exp(2T ε 0 ) Now, define the sets A = {x : M T +1 (x) ≥ T /4} and B = A c ∩ {x : H(x) = f (x)} and note that Er D (H) ≤ P(X ∈ A) + P(X ∈ B).$For bounding P(X ∈ A), we have

$E [exp(M T +1 (X))] ≥ E [exp(M T +1 (X))1 A (X)] ≥ exp(T /4)P(X ∈ A),$and hence

$P(X ∈ A) ≤ exp(-T /4)E [exp(M T +1 (X))] ≤ exp(T (2ε 0 -1/4)) = exp(-T /8) ≤ ε/2.$Bounding P(X ∈ B):

$First, observe that 0 ≤ M t+1 (x) -M t (x) ≤ 1 for all t ∈ [T ], x ∈ X . Now, let x ∈ B.$Since H is a majority classifier, we have

$T /2 ≤ T t=1 1{h t (x) = f (x)} = T t=1 1{N t+1 (x) > c t+1 } + M T +1 (x).$Taking the expectation over the event {X ∈ B} on both ends of the above then yields

$T P(X ∈ B)/2 = T E[1{X ∈ B}]/2 ≤ T t=1 E[1{N t+1 (X) > c t+1 }1{X ∈ B}] + E[M T +1 (X)1{X ∈ B}] ≤ T t=1 P[N t+1 (X) > c t+1 ] + E[M T +1 (X)1{X ∈ B}] ≤ T t=1 P[N t+1 (X) > c t+1 ] + T P(X ∈ B)/4.$Due to the threshold check in line 12 and Lemma 1.3, we know that if b t = 0, then we must have P[N t+1 (X) = c t + 1] ≤ ε/8. Furthermore, in this case c t+1 = c t . Hence,

$P[N t+1 (X) > c t+1 ] = P[N t+1 (X) > c t ] = P[N t+1 (X) = c t + 1] ≤ ε/8. If instead b t = 1, then N t+1 (x) ≤ M t (x) + 1 ≤ c t + 1 = c t+1 , which implies P[N t+1 (X) > c t+1 ] = 0.$Hence, we get the bound

$T P(X ∈ B)/2 ≤ T t=1 P[N t+1 (X) > c t+1 ] + T P(X ∈ B)/4 ≤ T ε/8 + T P(X ∈ B)/4.$Rearranging gives P(X ∈ B) ≤ ε/2, meaning we in total have

$Er D (H) ≤ P(X ∈ A) + P(X ∈ B) = ε/2 + ε/2 = ε.$For the remaining parts, we need the guarantee of Lemma 2.1 that rejection sampling fails with low probability when µ t has large density. Hence, we first show that the density is indeed large.

## Lemma 2.4 (High density of µ t ). Assume that T succeeds in every iteration in M B

. Then for any t ∈ [T ], µ t has density d(µ t ) ≥ ε/32.

Proof. Let X ∼ D. Then using the law of total expectation and the definition of µ t we have

$d(µ t ) = E[µ t (X)] ≥ E[µ t (X)|M t (X) ≥ c t ]P[M t (x) ≥ c t ] = E[exp(M t (X) -c t )|M t (X) ≥ c t ]P[M t (x) ≥ c t ] ≥ P[M t (X) ≥ c t ].$We now show by induction in t that P[M t (X) ≥ c t ] > ε/32. For t = 1, we have M 1 (X) = c 1 = 0, and hence P[M t (X) ≥ c t ] = 1. Now, assume the claim holds for t. We will then show that it holds for t + 1 by case analysis on b t+1 . If b t+1 = 0, we have c t+1 = c t and

$P[M t+1 (X) ≥ c t+1 ] = P[M t+1 (X) ≥ c t ] ≥ P[M t (X) ≥ c t ] > ε/32$using the induction hypothesis and the fact that M t+1 (X) ≥ M t (X). Now assume b t+1 = 1. Then we know by Lemma 1.

$3 that P [N t+1 (X) = c t + 1] > ε/32. Since b t+1 = 1, then c t+1 = c t + 1 which then implies that P[M t+1 (X) ≥ c t+1 ] = P[M t+1 (X) ≥ c t + 1] = P[min(N t+1 (X), c t+1 ) ≥ c t + 1] = P[N t+1 (X) ≥ c t + 1] > ε/32. Lemma 2.5 (Replicability). M B is ρ-replicable.$Proof. Let S 1 , S 2 be two independent samples with distribution D m for some m = O m W ( Θ(ργ 2 )) εγ 2

+ 1 ρ 2 εγ 3 . Assume that in iterations 1, . . . , t -1, the algorithm has produced the same objects, i.e. that the reweighing functions and hypotheses associated with S 1 and S 2 are the same. Then, for iteration t to be replicable, we need the following:

1. B * outputs the same hypothesis for both samples. 2. T outputs the same bit for both samples. When these conditions hold, the rest of the quantities appearing in the algorithm will be the same for both samples and hence ensure replicability. We call B * with replicability parameter ρ 0 = ρ/(6T ) and call R S with at least 8 ln(6T /ρ)/ε samples. Since Lemma 2.4 tells us that the density of µ t satisfies d(µ t ) > ε/32, we can use Lemmas 2.1 and 2.2 to conclude that B * combined with R S is 2ρ/(6T ) + 2ρ/(6T ) = 2ρ/(3T )replicable. Finally, by Lemma 1.3, T is ρ/(3T )-replicable. Hence, by a union bound over the conditions, each iteration is ρ/T -replicable and union bounding over all T iterations, the entire algorithm is ρ-replicable.

## Lemma 2.6 (Sample complexity). M B uses

$m = O m W ( Θ(ργ 2 )) εγ 2 + 1 ρ 2 εγ 3 samples.$Proof. For the sample complexity of a single iteration, we simply add up the sample complexities of all the subroutines: • B * : Since we give it parameters ρ 0 = ρ/(6T ),ε 0 = 1/16, we get from Theorem 1.4 that the sample complexity of B * in a single iteration is

$m B * (ρ 0 , ε 0 ) = O   ln( T ργ 2 )m W(Θ( ργ 2 T )) γ 2 + ln( T ργ )T 2 ρ 2 γ 3  $Remark that the choice of constant ε 0 removes all the dependence on ε.

• R S : To invoke Lemma 2.1 with failure probability ρ/(6T ) the number of samples used in each iteration is

$O ln( T ρ )m B * (ρ 0 , ε 0 ) ε .$• T : To invoke Lemma 1.3 with replicability parameter ρ/(3T ) and failure probability ρ/(24T ) the number of samples used in each iteration is

$O( ln( T ρ )T 2 ρ 2 ε ).$Remembering that the number of iterations is T = O(ln(1/ε)) we get the total sample complexity to be

$O T ln( T ρ ) ln( T ργ 2 )m W(Θ( ργ 2 T )) εγ 2 + ln( T ρ ) ln( T ργ )T 2 ρ 2 εγ 3 + ln( T ρ )T 2 ρ 2 ε = O m W( Θ(ργ 2 )) εγ 2 + 1 ρ 2 εγ 3$Lemma 2.7 (Failure probability). M B fails with probability at most 9ρ/24 ≤ ρ.

Proof. The only sources of failure are the three subroutines. R S fails with probability ρ/(6T ) in each iteration. T fails with probability ρ/(24T ) in each iteration. B * fails with probability at most ρ/(6T ) in each iteration. Hence, the total failure probability of the algorithm is at most 9ρ/24.

## Subroutines

In this section, we will present the replicable subroutines that the boosting algorithm uses. This includes T B * . As mentioned earlier, we will not present R S as we have made no changes to it, so we refer to [Impagliazzo et al. [2022]](#b10) for the description of this subroutine. We now move on to describe the two other subroutines.

## T

In this section, we will describe T in more detail. The pseudocode can be found in Algorithm 3. The purpose of this algorithm is to make a replicable test to see if E[ϕ(X)] > z for some threshold z ∈ (0, 1) and ϕ : X → [0, 1].

## In the original version of B

, this is done by replicably simulating a statistical query for estimating E[ϕ(X)], with an additive error of order z. However, for a threshold check it suffices to have a multiplicative error when estimating E[ϕ(X)], which means we can do a better analysis by using a Chernoff bound.

## Algorithm 3 T

(S, z, ϕ)

$Input: Samples S = (x 1 , . . . , x m ) drawn from D, threshold z, function ϕ : X → [0, 1]. Output: Bit b being a guess, whether E x∼D [ϕ(x)] > z. 1: z 0 ← r [ 3 4 z, 3 2 z] ⊲ Chosen uniformly at random 2: ϕ(S) ← 1 m m i=1 ϕ(x i ) 3: Return: b = 1 ϕ(S) > z 0$We will now prove the guarantee of T

. For convenience, we restate the guarantee here.

Lemma 1.3 Restated. Let z, ρ ∈ (0, 1), δ ∈ (0, ρ/8], let ϕ : X → [0, 1] and let S = (x 1 , . . . , x m ) be samples drawn i.i.d. from distribution D. Then there exists a constant c such that if m ≥ c ln(1/δ) ρ 2 z , T (S, z, ϕ) is ρ-replicable and returns a bit b such that with probability at least 1 -δ:

## B *

We now move on to discuss in more detail why the modifications in B * preserve correctness. The modified version can be seen in Algorithm 1. The only two modifications can be found in line 12 and 14. In line 14 we have substituted a statistical query with our threshold check, and in line 12 we have inserted an if-statement to only do the threshold check every 1/γ iteration. In this algorithm, the threshold check gives the same guarantees as the statistical query, and it will therefore not affect correctness. However, the introduction of the if-statement could lead to two kinds of errors, since we do not check the value of d(µ t ) in every iteration. First, it could be that R S fails, since it needs d(µ t ) to be large. Second, it could be that the number of iterations is increased, since the algorithm does not detect immediately when the density becomes small. To show that these events are not problematic, we first show that the densities do not decrease too much over 1/γ iterations.

Lemma 3.1. Let T 0 denote the number of iterations that B * runs for. Then for all t ∈ [T 0 ] and k ≤ max{⌊1/γ⌋, T 0 -t} that d(µ t+k ) ≥ d(µ t )/2.

Proof. Let x ∈ X . Then by the recursive definition of the g t 's, we have µ t+1 (x) ≥ (1 -γ) 1/2 µ t (x). Inductively, we get

$µ t+k (x) ≥ µ t (x)(1 -γ) k/2 ≥ µ t (1 -γ) ⌊ 1 γ ⌋/2 ≥ µ t (x) 1 -γ⌊ 1 γ ⌋/2 ≥ 1 2 µ t (x),$where we use Bernoulli's inequality which applies since -γ > -1. Taking the expectation with respect to D on both sides yields the desired conclusion.

Theorem 1.4 Restated. For any ρ, ε ∈ (0, 1) and Θ(ρεγ 2 )-replicable weak learner W with advantage γ, B * is ρ-replicable, makes O( 1 εγ 2 ) calls to W, and with probability at least 1 -ρ outputs a hypothesis H with Er D (H) ≤ ε. Furthermore, its sample complexity is

$m B * (ρ, ε) = O ln( 1 ρεγ 2 )m W(Θ(ρεγ 2 )) ε 2 γ 2 + ln( 1 ρεγ ) ρ 2 ε 4 γ 3 = O m W(Θ(ρεγ 2 )) ε 2 γ 2 + 1 ρ 2 ε 4 γ 3 .$Proof. First, we argue that the R S succeeds with high probability. Observe that due to Lemma 3.1, the densities are at most halved in B * compared to the original version of B . Due to Lemma 2.1 we therefore only need to use twice as many samples in the rejection sampler for it to still succeed.

Next, we will argue that the number of iterations remains the same as in B up to constant factors. The number of iterations is bounded in [Servedio [2001]](#b15) by showing that for any κ > 0, there is some t within the first O( 1 κγ 2 ) iterations such that d(µ t ) < κ. This result also applies to B * . However, we will not repeat the proof here. We can then conclude that d(µ t ) will fall below ε/8 within the first O( 1 εγ 2 ) iterations. Since the threshold check in line 14 always realizes when d(µ t ) ≤ ε/4 (see Lemma 1.3), and it takes 1/γ iterations to further decrease the density from ε/4 to ε/8, T will always have terminated the loop before reaching density ε/8. Hence, the number of iterations in B * is still T 0 = O( 1 εγ 2 ). We now argue, that replicability is preserved. Since this argument is almost identical to the proof of Lemma 2.5, we will only describe what differs in this analysis. First, the weak learner is called T 0 times, and therefore it needs replicability parameter ρ/(6T 0 ) = Θ(ρεγ 2 ). Second, T is called γT 0 times and therefore needs replicability parameter ρ/(6γT 0 ) = ρεγ/6, which it achieves due to Lemma 1.3. Thus, our modifications preserve replicability.

Finally, we calculate the sample complexity. The R S uses O(ln( T0 ρ )m W(Θ(ρεγ 2 )) /ε) samples for each call, and it is called T 0 times. Meanwhile, T uses O( ln(γT0/ρ) ρ 2 ε 3 γ 2 ) samples for each call, and is called γT 0 times. Hence, the total sample complexity is

$O T 0 ln( T0 ρ )m W(Θ(ρεγ 2 )) ε + γT 0 ln( γT0 ρ ) ρ 2 ε 3 γ 2 = O ln( 1 ρεγ 2 )m W(Θ(ρεγ 2 )) ε 2 γ 2 + ln( 1 ρεγ ) ρ 2 ε 4 γ 3 = O m W(Θ(ρεγ 2 )) ε 2 γ 2 + 1 ρ 2 ε 4 γ 3 .$