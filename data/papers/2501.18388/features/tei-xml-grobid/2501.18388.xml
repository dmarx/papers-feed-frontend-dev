<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improved Replicable Boosting with Majority-of-Majorities</title>
				<funder>
					<orgName type="full">European Union</orgName>
				</funder>
				<funder>
					<orgName type="full">European Research Council</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-02-04">4 Feb 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kasper</forename><surname>Green Larsen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Aarhus University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Markus</forename><forename type="middle">Engelund</forename><surname>Mathiasen</surname></persName>
							<email>markusm@cs.au.dk</email>
							<affiliation key="aff0">
								<orgName type="institution">Aarhus University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Clement</forename><surname>Svendsen</surname></persName>
							<email>clementks@cs.au.dk</email>
							<affiliation key="aff0">
								<orgName type="institution">Aarhus University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Improved Replicable Boosting with Majority-of-Majorities</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-02-04">4 Feb 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">42F5C7F56E5A32DA429E18B397F7826E</idno>
					<idno type="arXiv">arXiv:2501.18388v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a new replicable boosting algorithm which significantly improves the sample complexity compared to previous algorithms. The algorithm works by doing two layers of majority voting, using an improved version of the replicable boosting algorithm introduced by Impagliazzo et al. [2022]  in the bottom layer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Replicability of an algorithm is a property introduced as a reaction to what is called the reproducibility crisis. Multiple Nature articles have pointed out the issue of researchers not being able to replicate findings <ref type="bibr" target="#b1">[Baker, 2016</ref><ref type="bibr" target="#b2">, Ball, 2023]</ref>. As a supplement to implementing better research practices in order to ensure replicability, <ref type="bibr" target="#b10">Impagliazzo et al. [2022]</ref> introduced the concept of replicability as a property of algorithms themselves. Informally, an algorithm is replicable if it, with high probability, outputs the same result when run with different input data drawn from the same distribution.</p><p>Definition 1.1 (Replicability <ref type="bibr" target="#b10">[Impagliazzo et al., 2022]</ref>). Let A be a randomized algorithm. Then, A is said to be ρ-replicable if there is an n ∈ N such that for all distributions D on some space X , it holds that P S1,S2,r [A(S 1 ; r) = A(S 2 ; r)] ≥ 1 -ρ where S 1 , S 2 ∼ D n are independent, and r denotes the internal randomness used by A.</p><p>As is evident from the definition, we require the algorithm to use the same internal randomness r in both runs. This turns out to be crucial -if we remove this requirement, we cannot solve simple tasks such as estimating the mean of a distribution replicably <ref type="bibr" target="#b5">[Dixon et al., 2024]</ref>. Researchers who use replicable algorithms may then publish the random seed used in their run of the algorithm which lets other researchers use the same seed to replicate the results with high probability, assuming that the data they use comes from the same underlying distribution.</p><p>In this work, we consider replicability in the weak-to-strong learning setting. Specifically we improve the best known sample complexity of ρ-replicable boosting algorithms. Let X be an input domain, and let f : X → {-1, 1} be the function we are trying to predict. An algorithm W is said to be a γ-weak learner for γ ∈ (0, 1/2) if there exists an m ∈ N such that for any distribution D on X and any sequence of m labelled samples S = {(x i , f (x i ))} m i=1 drawn i.i.d. from D, it holds that h := W(S) : X → {-1, 1} satisfies P x∼D [h(x) = f (x)] ≤ 1/2 -γ. We call γ the advantage of W and m the sample complexity of W. A strong learner on the other hand, is a learning algorithm such that for any distribution D on X , failure probability δ &gt; 0 and error ε &gt; 0, there is an m = m(ε, δ) ∈ N such that when applied to an i.i.d. sample S ∼ D m , the algorithm outputs a classifier which has error at most ε over D with probability at least 1 -δ. We denote the error of a hypothesis h : X → {-1, 1} with respect to a distribution D by</p><formula xml:id="formula_0">Er D (h) = P x∼D [h(x) = f (x)].</formula><p>Boosting algorithms were originally introduced to answer the following theoretical question posed by <ref type="bibr" target="#b12">Kearns [1988]</ref>, <ref type="bibr" target="#b13">Kearns and Valiant [1994]</ref>: Is it possible to combine hypotheses produced by a weak learning algorithm into a strong learner? As shown by <ref type="bibr" target="#b14">Schapire [1990]</ref>, this turned out to be the case, and one of the most famous algorithms that solves this problem is the A B algorithm <ref type="bibr" target="#b9">[Freund and Schapire, 1995]</ref>. In short, boosting works by running a number of iterations. In each iteration t, we update a distribution D t on the samples and run the weak learner with this new distribution. After a sufficient number of iterations, we take a weighed majority vote among all the produced weak hypotheses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Our contribution</head><p>Our main contribution is a replicable boosting algorithm called M B which is inspired by an existing replicable boosting algorithm, B <ref type="bibr" target="#b10">[Impagliazzo et al., 2022]</ref> and the S B algorithm <ref type="bibr" target="#b15">[Servedio, 2001]</ref>. First, fix a distribution D on X and let W denote a replicable weak learner and for ρ ∈ (0, 1) let m W(ρ) be the sample complexity of W when run with replicability parameter ρ. Our main result is the following:</p><formula xml:id="formula_1">Theorem 1.2 ( M B</formula><p>). For any ρ, ε ∈ (0, 1) and Θ(ργ 2 )-replicable weak learner W with advantage γ, M B is ρ-replicable, makes O( ln(1/ε) γ 2 ) calls to W, and with probability at least 1 -ρ outputs a hypothesis H with Er D (H) ≤ ε. Furthermore, its sample complexity is</p><formula xml:id="formula_2">O m W( Θ(ργ 2 )) εγ 2 + 1 ρ 2 εγ 3 .</formula><p>Our algorithm significantly improves on the sample complexity of <ref type="bibr" target="#b10">Impagliazzo et al. [2022]</ref> which is</p><formula xml:id="formula_3">O m W (Θ(ρεγ 2 )) ε 2 γ 2 + 1 ρ 2 ε 5 γ 6 .</formula><p>Note that this sample complexity is not what is stated in their paper, but is in fact the correct sample complexity of their algorithm. We improve the first term by a factor 1/ε and also remove a factor ε in the replicability parameter to the weak learner. Since the sample complexity of most replicable algorithms has a quadratic dependence on their replicability parameter, this will amount to an extra 1/ε 2 improvement in this term. In the second term we shave off a factor 1/(ε 4 γ 3 ). All improvements are up to logarithmic factors.</p><p>As a secondary contribution, we introduce an algorithm T for performing a replicable threshold check. This algorithm replicably checks if the expected value of a function ϕ is above a certain threshold z, and is used as a subroutine in M B</p><p>. We state the guarantees of the algorithm below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 1.3 ( T</head><p>). Let z, ρ ∈ (0, 1), δ ∈ (0, ρ/8], let ϕ : X → [0, 1] and let S = (x 1 , . . . , x m ) be samples drawn i.i.d. from distribution D. Then there exists a constant c such that if m ≥ c ln(1/δ) ρ 2 z , T (S, z, ϕ) is ρ-replicable and returns a bit b such that with probability at least 1 -δ:</p><p>•</p><formula xml:id="formula_4">If E x∼D [ϕ(x)] ≤ z/2, then b = 0. • If E x∼D [ϕ(x)] ≥ 2z, then b = 1.</formula><p>We believe this algorithm is also of independent interest and can be applied in many scenarios as an alternative to statistical queries which were previously used for such applications. This is because our algorithm achieves a dependence of 1/z in the sample complexity, while using statistical queries for the same purpose comes with a factor 1/z 2 in the sample complexity <ref type="bibr">(Thm. 2.3, Impagliazzo et al. [2022]</ref>). While our approach to threshold checks is not neccesarily novel, it seems to have been overlooked in the context of replicable algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Related work</head><p>In recent years, replicable algorithms have been developed in a variety of settings. This includes e.g. learning half spaces, clustering, reinforcement learning and online learning <ref type="bibr" target="#b11">[Kalavasis et al., 2024</ref><ref type="bibr" target="#b8">, Esfandiari et al., 2024</ref><ref type="bibr" target="#b7">, Eaton et al., 2024</ref><ref type="bibr" target="#b0">, Ahmadi et al., 2024]</ref>.</p><p>There are also important connections to the field of differential privacy. Intuitively, a replicable algorithm does not depend heavily on the specific sample given to the algorithm. This is similar to the requirement in differential privacy where we demand that when the algorithm is run on two samples differing in only a single point, then the two distributions on the outputs are close in the sense of max divergence. <ref type="bibr">Bun et al. [2023, Thm. 3.1]</ref> show that there is a reduction "without substantial blowup in runtime or sample complexity" from differential privacy to replicability. On the other hand, they also show that no computationally efficient transformation of differentially private algorithms to</p><p>We have personally contacted the authors to make them aware, and they have acknowledged this error.</p><p>replicable ones can exist under standard cryptographic assumptions. However, if one does not care about computational efficiency, they do give a reduction from differential privacy to replicability with only a quadratic blowup in sample complexity. This means it would be possible to take an existing differentially private boosting algorithm and make it replicable. One example of a differentially private boosting algorithm is B F P <ref type="bibr" target="#b6">[Dwork et al., 2010</ref>]. However, using the reduction on this algorithm would incur a 1/γ 8 and 1/ε 2 dependence in the sample complexity.</p><p>Moving away from differential privacy, another candidate algorithm to be made replicable is the S B algorithm <ref type="bibr" target="#b15">[Servedio, 2001]</ref>. This algorithm differs from e.g. the well-known A B <ref type="bibr" target="#b9">[Freund and Schapire, 1995]</ref> in that it maintains a smoothness across the distributions D t over the data in every iteration t. Formally, this means that the distribution D t satisfies max x D t (x) ≤ 1/(εm) for some ε &gt; 0 where m is the number of samples. This smoothness property ensures that no single example has too much influence on the distributions which is why smoothness is a desirable property when designing replicable boosting algorithms. In fact, the boosting algorithm by <ref type="bibr" target="#b10">Impagliazzo et al. [2022]</ref> can be seen as a translation of S B into the replicable setting. The downside of using S B is that it requires O( 1 εγ 2 ) invocations of the weak learner W. We call this the round complexity of the algorithm. This should be compared to A B which has round complexity O( ln(1/ε) γ 2 ). In the replicable setting, we draw new samples for each invocation of W, so the round complexity directly affects the number of samples used. This motivates looking at smooth boosting algorithms with fewer invocations of W such as the one presented by <ref type="bibr" target="#b3">Barak et al. [2009]</ref>. This algorithm uses Bregman projections to maintain the smoothness property, and it matches the round complexity of A B</p><p>. However, converting the algorithm to the replicable setting would require us to make replicable approximations of these Bregman projections which turns out to use more samples than we obtain in Theorem 1.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">High-Level Ideas</head><p>We will now explain the very high-level idea behind our new boosting algorithm M B</p><p>. The first step towards constructing this improved replicable boosting algorithm is to make slight modifications to the algorithm B of <ref type="bibr" target="#b10">Impagliazzo et al. [2022]</ref> to improve its sample complexity. We will refer to this modified version as B * which can be found in Algorithm 1. Remark that the functions g t , µ t are functions over the entire domain X and not just the samples that we see. This means that we cannot afford to update these functions explicitly for every point, so instead we update the description of the functions. To distinguish this from normal assignments in the pseudocode, we use the def = operator for assignments to these functions and the ← operator for normal assignments.</p><p>In this algorithm µ t : X → [0, 1] is a function which determines the reweighing of the data distribution D in iteration t. The reweighed distribution is then</p><formula xml:id="formula_5">D µt (x) = µ t (x)D(x)/d(µ t ) where d(µ t ) = E x∼D [µ t (x)]</formula><p>is the normalization factor which we call the density of µ. The subroutine R S then lets us sample from the distribution D µt when given access to µ t and samples from D (see Lemma 2.1 for formal guarantee). We also note without proof that large density of µ t actually implies smoothness of the reweighed distribution D µt with respect to the original distribution D. More precisely, if d(µ t ) ≥ ε, for some ε &gt; 0, then D µt (x) ≤ D(x)/ε for all x ∈ X . These samples from D µt are then given to the replicable weak learner. We will not go into further detail with how or why the original B works but instead refer to <ref type="bibr" target="#b10">Impagliazzo et al. [2022]</ref>, <ref type="bibr" target="#b15">Servedio [2001]</ref>. In total, we have made two modifications in B * . The first modification is that we have changed the termination condition in line 14 to use our T algorithm instead of the statistical query algorithm they used. This accomplishes exactly the same thing, but uses a factor 1/ε fewer samples for each call. The second modification is the introduction of the if-statement in line 12. It turns out that this check only makes the algorithm run for a constant factor more iterations. However, this allows us to shave off a factor 1/γ in the number of calls to T . Since the replicability parameter of T needs to be ρ divided by the number of calls to T , this is a great improvement. This is because the sample complexity of T is inversely proportional to the square of its replicability parameter, so it will need a factor 1/γ 2 fewer samples for each invocation of T . Since it is now only called every 1/γ iteration, we shave off a factor 1/γ 3 in total by introducing this check. In Section 3, we will explain in more detail why these modifications preserve correctness, but for now we will just state the guarantees of B * .</p><p>Algorithm 1 B * ρ,ε (S, W) Input: Samples S i.i.d. from D, replicable γ-weak learner W, replicability ρ, error ε. Output: Hypothesis H : X → {-1, 1}.</p><formula xml:id="formula_6">1: g 0 (x) def = 0 2: µ 1 (x) def = 1 3: t ← 0 4: while true do 5: t ← t + 1 6: D µt (x) def = µ t (x)D(x)/d(µ t ) 7: S 1 ← O(m W(Θ(ρεγ 2 )) /ε) fresh samples from S 8: S W ← R S (S 1 , m W(Θ(ρεγ 2 )) , µ t ) 9:</formula><p>h t ← Run W(S W ) with replicability Θ(ρεγ 2 ) 10:</p><formula xml:id="formula_7">g t (x) def = g t-1 (x) + h t (x)f (x) -γ/(2 + γ) 11: µ t+1 (x) def = 1, if g t (x) ≤ 0 (1 -γ) gt(x)/2 , if g t (x) &gt; 0 12: if ⌊ 1 γ ⌋ divides t then 13: S 2 ← O 1 ρ 2 ε 3 γ 2 fresh</formula><p>samples from S 14: if T (S 2 , ε/2, µ t ) = 0 then 15: Exit while loop 16: Return: H ← sign( t h t ) Theorem 1.4 ( B * ). For any ρ, ε ∈ (0, 1) and Θ(ρεγ 2 )-replicable weak learner W with advantage γ, B * is ρ-replicable, makes O( 1 εγ 2 ) calls to W, and with probability at least 1 -ρ outputs a hypothesis H with Er D (H) ≤ ε. Furthermore, its sample complexity is</p><formula xml:id="formula_8">m B * (ρ, ε) = O ln( 1 ρεγ 2 )m W(Θ(ρεγ 2 )) ε 2 γ 2 + ln( 1 ρεγ ) ρ 2 ε 4 γ 3 = O m W(Θ(ρεγ 2 )) ε 2 γ 2 + 1 ρ 2 ε 4 γ 3 .</formula><p>Remember that the original version of B had sample complexity O(</p><formula xml:id="formula_9">m W (Θ(ρεγ 2 )) ε 2 γ 2 + 1 ρ 2 ε 5 γ 6 ).</formula><p>The dependence on γ has therefore improved greatly. However, we are still not happy with the dependence on ε. The main idea of our algorithm is therefore to use B * as a subroutine and only call it with constant error parameter ε 0 = 1/16. Our algorithm can be seen as a meta boosting algorithm where in each iteration, we call B * to get a hypothesis with constant advantage. We then perform exponential weight updates similar to A B in order to make our algorithm only run for T = O(ln(1/ε)) iterations. Remark that this entirely removes the problem of B * having a bad dependence on ε, since we only invoke it with a constant error parameter. This is the main insight needed to understand how our algorithm works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Our Replicable Boosting Algorithm</head><p>In this section, we will present our new ρ-replicable boosting algorithm which can be found in Algorithm 2.</p><p>The algorithm runs for T iterations while maintaining functions N t , M t , µ t . In each iteration the algorithm performs rejection sampling to get samples S 2 drawn from distribution D µt . It then gets a hypothesis h t from B * which has constant error of at most ε 0 with respect to D µt . One can interpret N t (x) as a lower bound for counting how many of the first t -1 hypotheses that misclassify element x. However, in order to ensure high density of the updated reweighing function µ t , we check if the points sharing the largest count have a total probability mass of at least ε/16 by using T . If not, we subtract 1 from the largest count which suffices to ensure high density of µ t (see Lemma 2.4). The capped values are stored in M t , and will be used in subsequent iterations. The value c t can be interpreted as a bound for the largest allowed count in iteration t, that is c t ≥ M t (x) for all x ∈ X .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 M B</head><p>ρ,ε (S, W) Input: Samples S i.i.d. from D, replicable γ-weak learner W, replicability ρ, error ε. Output: Hypothesis H : X → {-1, 1}.</p><formula xml:id="formula_10">1: N 1 (x) def = 0 2: M 1 (x) def = 0 3: µ 1 (x) def = 1 4: c 1 ← 0 5: for t = 1 to T do ⊲ T = O(1/ε) 6: D µt (x) def = µ t (x)D(x)/d(µ t ) 7: S 1 ← O(m B * (ρ 0 , ε 0 )/ε) fresh samples from S ⊲ ρ 0 = ρ/(6T ), ε 0 = 1/16 8: S 2 ← R S (S 1 , m B * (ρ 0 , ε 0 ), µ t ) 9: h t ← B * ρ0,ε0 (S 2 , W) 10: N t+1 (x) def = M t (x) + 1{h t (x) = f (x)} 11: S 3 ← O( 1 ρ 2 ε ) fresh samples from S 12: b t+1 ← T (S 3 , ε/16, ϕ) ⊲ ϕ(x) = 1{N t+1 (x) = c t + 1} 13: c t+1 ← c t + b t+1 14: M t+1 (x) def = min(N t+1 (x), c t+1 ) 15: µ t+1 (x) def = exp(M t+1 (x) -c t+1 ) 16: Return H = sign T t=1 h t</formula><p>Now, before going into the analysis of the algorithm, we will present the guarantees of the R S which we use to draw samples from D µ . The pseudocode and proofs of the below guarantees are described by <ref type="bibr" target="#b10">Impagliazzo et al. [2022]</ref>, so we will not repeat those here.</p><p>Lemma 2.1 (Rejection Sampling <ref type="bibr" target="#b10">[Impagliazzo et al., 2022]</ref>). For any ε ∈ (0, 1], if µ has density d(µ) ≥ ε and S ∼ D m where m ≥ 8 ln(1/δ)m target /ε, then R S (S, m target , µ) outputs a sample S out ∼ D mtarget µ with probability at least 1 -δ.</p><p>Lemma 2.2 (Composing Replicable Algorithms with Rejection Sampling <ref type="bibr" target="#b10">[Impagliazzo et al., 2022]</ref>). Let A(S) be a ρ-replicable algorithm with sample complexity m. Let µ : X → [0, 1]. Then let B be the algorithm that runs A with samples drawn from D µ using rejection sampling. Let q be the failure probability of R S . Then B is (2q + 2ρ)-replicable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Analysis of M B</head><p>We are now ready to analyze M B . To make it easier to follow the analysis, we will split it into four parts. 1. Correctness, 2. Replicability, 3. Sample complexity, 4. Failure probability. We will start with correctness. However, before going into the formal details, we will give an explanation of the high level ideas in the proof. First, observe that if we did not cap the weights N t , the multiplicative weight updates would be very similar to the updates made in A B</p><p>. Recall that for any t ∈ [T ], M t (x) is exactly the number of misclassifications of x minus the amount of times we have capped the weight so far. Hence, if we did not cap the weights by c t each iteration, x would be misclassified by the final hypothesis H only if M T +1 (x) ≥ T /2. We will now take the capping into account. We first show using an argument similar to the standard analysis of A B that the probability of drawing an x from D for which M T +1 (x) ≥ T /4 is at most ε/2. What remains is to argue that the probability of drawing an x which is misclassified but simultaneously satisfies M T +1 (x) &lt; T /4 is small. The only way this can happen is if there were at least T /4 iterations in which we capped down the value of N t+1 (x) when calculating M t+1 (x), since in such iterations we would not increment M t+1 (x) even though h t misclassified x. Observe that due to the threshold check, the total probability mass (w.r.t. D) of points whose value of N t+1 were capped in a single iteration cannot exceed ε/8. Therefore, after T iterations, the total probability mass of points, whose value of N t+1 were capped T /4 times is at most T (ε/8)/(T /4) = ε/2. So in total, the probability mass of all the misclassified points is at most ε. We now prove this formally.</p><p>Lemma 2.3 (Correctness). Put T ≥ 8 ln(2/ε) and ε 0 = 1/16. Assuming that all subroutines of the algorithm succeed in every iteration, we achieve an error of at most ε over the distribution D, i.e. Er D (H) ≤ ε.</p><formula xml:id="formula_11">Proof. By definition of M T +1 , N T +1 and µ T E[exp(M T +1 (X))] ≤ E[exp(N T +1 (X))] = E[exp(M T (X)) exp(1{h T (X) = f (X)})] = e cT E[µ T (X) exp(1{h T (X) = f (X)})] = e cT E[µ T (X)1{h T (X) = f (X)}] + eE[µ T (X)1{h T (X) = f (X)}] .</formula><p>(1)</p><p>Now, since D µT = µT •D d(µT ) , we can rewrite the above to an expectation involving Y ∼ D µT such that ( <ref type="formula">1</ref>) is equal to</p><formula xml:id="formula_12">e cT d(µ T ) E[1{h T (Y ) = f (Y )}] + eE[1{h T (Y ) = f (Y )}] = e cT d(µ T ) Er Dµ T (h T )(e -1) + 1 ≤ e cT d(µ T ) exp (e -1) Er Dµ T (h T ) ≤ e cT d(µ T ) exp(2ε 0 ),<label>(2)</label></formula><p>where the final inequality follows since h t has error at most ε 0 under D µT by Theorem 1.4. Now, note that</p><formula xml:id="formula_13">e cT d(µ T ) = e cT E[µ T (X)] = e cT E[exp(M T (X) -c T )] = E[exp(M T (X))].</formula><p>Plugging this into (2), we recursively get</p><formula xml:id="formula_14">E[exp(M T +1 (X))] ≤ E[exp(M T (X))] exp(2ε 0 ) ≤ • • • ≤ exp(2T ε 0 ) Now, define the sets A = {x : M T +1 (x) ≥ T /4} and B = A c ∩ {x : H(x) = f (x)} and note that Er D (H) ≤ P(X ∈ A) + P(X ∈ B).</formula><p>For bounding P(X ∈ A), we have</p><formula xml:id="formula_15">E [exp(M T +1 (X))] ≥ E [exp(M T +1 (X))1 A (X)] ≥ exp(T /4)P(X ∈ A),</formula><p>and hence</p><formula xml:id="formula_16">P(X ∈ A) ≤ exp(-T /4)E [exp(M T +1 (X))] ≤ exp(T (2ε 0 -1/4)) = exp(-T /8) ≤ ε/2.</formula><p>Bounding P(X ∈ B):</p><formula xml:id="formula_17">First, observe that 0 ≤ M t+1 (x) -M t (x) ≤ 1 for all t ∈ [T ], x ∈ X . Now, let x ∈ B.</formula><p>Since H is a majority classifier, we have</p><formula xml:id="formula_18">T /2 ≤ T t=1 1{h t (x) = f (x)} = T t=1 1{N t+1 (x) &gt; c t+1 } + M T +1 (x).</formula><p>Taking the expectation over the event {X ∈ B} on both ends of the above then yields</p><formula xml:id="formula_19">T P(X ∈ B)/2 = T E[1{X ∈ B}]/2 ≤ T t=1 E[1{N t+1 (X) &gt; c t+1 }1{X ∈ B}] + E[M T +1 (X)1{X ∈ B}] ≤ T t=1 P[N t+1 (X) &gt; c t+1 ] + E[M T +1 (X)1{X ∈ B}] ≤ T t=1 P[N t+1 (X) &gt; c t+1 ] + T P(X ∈ B)/4.</formula><p>Due to the threshold check in line 12 and Lemma 1.3, we know that if b t = 0, then we must have P[N t+1 (X) = c t + 1] ≤ ε/8. Furthermore, in this case c t+1 = c t . Hence,</p><formula xml:id="formula_20">P[N t+1 (X) &gt; c t+1 ] = P[N t+1 (X) &gt; c t ] = P[N t+1 (X) = c t + 1] ≤ ε/8. If instead b t = 1, then N t+1 (x) ≤ M t (x) + 1 ≤ c t + 1 = c t+1 , which implies P[N t+1 (X) &gt; c t+1 ] = 0.</formula><p>Hence, we get the bound</p><formula xml:id="formula_21">T P(X ∈ B)/2 ≤ T t=1 P[N t+1 (X) &gt; c t+1 ] + T P(X ∈ B)/4 ≤ T ε/8 + T P(X ∈ B)/4.</formula><p>Rearranging gives P(X ∈ B) ≤ ε/2, meaning we in total have</p><formula xml:id="formula_22">Er D (H) ≤ P(X ∈ A) + P(X ∈ B) = ε/2 + ε/2 = ε.</formula><p>For the remaining parts, we need the guarantee of Lemma 2.1 that rejection sampling fails with low probability when µ t has large density. Hence, we first show that the density is indeed large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 2.4 (High density of µ t ). Assume that T succeeds in every iteration in M B</head><p>. Then for any t ∈ [T ], µ t has density d(µ t ) ≥ ε/32.</p><p>Proof. Let X ∼ D. Then using the law of total expectation and the definition of µ t we have</p><formula xml:id="formula_23">d(µ t ) = E[µ t (X)] ≥ E[µ t (X)|M t (X) ≥ c t ]P[M t (x) ≥ c t ] = E[exp(M t (X) -c t )|M t (X) ≥ c t ]P[M t (x) ≥ c t ] ≥ P[M t (X) ≥ c t ].</formula><p>We now show by induction in t that P[M t (X) ≥ c t ] &gt; ε/32. For t = 1, we have M 1 (X) = c 1 = 0, and hence P[M t (X) ≥ c t ] = 1. Now, assume the claim holds for t. We will then show that it holds for t + 1 by case analysis on b t+1 . If b t+1 = 0, we have c t+1 = c t and</p><formula xml:id="formula_24">P[M t+1 (X) ≥ c t+1 ] = P[M t+1 (X) ≥ c t ] ≥ P[M t (X) ≥ c t ] &gt; ε/32</formula><p>using the induction hypothesis and the fact that M t+1 (X) ≥ M t (X). Now assume b t+1 = 1. Then we know by Lemma 1.</p><formula xml:id="formula_25">3 that P [N t+1 (X) = c t + 1] &gt; ε/32. Since b t+1 = 1, then c t+1 = c t + 1 which then implies that P[M t+1 (X) ≥ c t+1 ] = P[M t+1 (X) ≥ c t + 1] = P[min(N t+1 (X), c t+1 ) ≥ c t + 1] = P[N t+1 (X) ≥ c t + 1] &gt; ε/32. Lemma 2.5 (Replicability). M B is ρ-replicable.</formula><p>Proof. Let S 1 , S 2 be two independent samples with distribution D m for some m = O m W ( Θ(ργ 2 )) εγ 2</p><p>+ 1 ρ 2 εγ 3 . Assume that in iterations 1, . . . , t -1, the algorithm has produced the same objects, i.e. that the reweighing functions and hypotheses associated with S 1 and S 2 are the same. Then, for iteration t to be replicable, we need the following:</p><p>1. B * outputs the same hypothesis for both samples. 2. T outputs the same bit for both samples. When these conditions hold, the rest of the quantities appearing in the algorithm will be the same for both samples and hence ensure replicability. We call B * with replicability parameter ρ 0 = ρ/(6T ) and call R S with at least 8 ln(6T /ρ)/ε samples. Since Lemma 2.4 tells us that the density of µ t satisfies d(µ t ) &gt; ε/32, we can use Lemmas 2.1 and 2.2 to conclude that B * combined with R S is 2ρ/(6T ) + 2ρ/(6T ) = 2ρ/(3T )replicable. Finally, by Lemma 1.3, T is ρ/(3T )-replicable. Hence, by a union bound over the conditions, each iteration is ρ/T -replicable and union bounding over all T iterations, the entire algorithm is ρ-replicable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 2.6 (Sample complexity). M B uses</head><formula xml:id="formula_26">m = O m W ( Θ(ργ 2 )) εγ 2 + 1 ρ 2 εγ 3 samples.</formula><p>Proof. For the sample complexity of a single iteration, we simply add up the sample complexities of all the subroutines: • B * : Since we give it parameters ρ 0 = ρ/(6T ),ε 0 = 1/16, we get from Theorem 1.4 that the sample complexity of B * in a single iteration is</p><formula xml:id="formula_27">m B * (ρ 0 , ε 0 ) = O   ln( T ργ 2 )m W(Θ( ργ 2 T )) γ 2 + ln( T ργ )T 2 ρ 2 γ 3  </formula><p>Remark that the choice of constant ε 0 removes all the dependence on ε.</p><p>• R S : To invoke Lemma 2.1 with failure probability ρ/(6T ) the number of samples used in each iteration is</p><formula xml:id="formula_28">O ln( T ρ )m B * (ρ 0 , ε 0 ) ε .</formula><p>• T : To invoke Lemma 1.3 with replicability parameter ρ/(3T ) and failure probability ρ/(24T ) the number of samples used in each iteration is</p><formula xml:id="formula_29">O( ln( T ρ )T 2 ρ 2 ε ).</formula><p>Remembering that the number of iterations is T = O(ln(1/ε)) we get the total sample complexity to be</p><formula xml:id="formula_30">O T ln( T ρ ) ln( T ργ 2 )m W(Θ( ργ 2 T )) εγ 2 + ln( T ρ ) ln( T ργ )T 2 ρ 2 εγ 3 + ln( T ρ )T 2 ρ 2 ε = O m W( Θ(ργ 2 )) εγ 2 + 1 ρ 2 εγ 3</formula><p>Lemma 2.7 (Failure probability). M B fails with probability at most 9ρ/24 ≤ ρ.</p><p>Proof. The only sources of failure are the three subroutines. R S fails with probability ρ/(6T ) in each iteration. T fails with probability ρ/(24T ) in each iteration. B * fails with probability at most ρ/(6T ) in each iteration. Hence, the total failure probability of the algorithm is at most 9ρ/24.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Subroutines</head><p>In this section, we will present the replicable subroutines that the boosting algorithm uses. This includes T B * . As mentioned earlier, we will not present R S as we have made no changes to it, so we refer to <ref type="bibr" target="#b10">Impagliazzo et al. [2022]</ref> for the description of this subroutine. We now move on to describe the two other subroutines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">T</head><p>In this section, we will describe T in more detail. The pseudocode can be found in Algorithm 3. The purpose of this algorithm is to make a replicable test to see if E[ϕ(X)] &gt; z for some threshold z ∈ (0, 1) and ϕ : X → [0, 1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>In the original version of B</head><p>, this is done by replicably simulating a statistical query for estimating E[ϕ(X)], with an additive error of order z. However, for a threshold check it suffices to have a multiplicative error when estimating E[ϕ(X)], which means we can do a better analysis by using a Chernoff bound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 3 T</head><p>(S, z, ϕ)</p><formula xml:id="formula_31">Input: Samples S = (x 1 , . . . , x m ) drawn from D, threshold z, function ϕ : X → [0, 1]. Output: Bit b being a guess, whether E x∼D [ϕ(x)] &gt; z. 1: z 0 ← r [ 3 4 z, 3 2 z] ⊲ Chosen uniformly at random 2: ϕ(S) ← 1 m m i=1 ϕ(x i ) 3: Return: b = 1 ϕ(S) &gt; z 0</formula><p>We will now prove the guarantee of T</p><p>. For convenience, we restate the guarantee here.</p><p>Lemma 1.3 Restated. Let z, ρ ∈ (0, 1), δ ∈ (0, ρ/8], let ϕ : X → [0, 1] and let S = (x 1 , . . . , x m ) be samples drawn i.i.d. from distribution D. Then there exists a constant c such that if m ≥ c ln(1/δ) ρ 2 z , T (S, z, ϕ) is ρ-replicable and returns a bit b such that with probability at least 1 -δ:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">B *</head><p>We now move on to discuss in more detail why the modifications in B * preserve correctness. The modified version can be seen in Algorithm 1. The only two modifications can be found in line 12 and 14. In line 14 we have substituted a statistical query with our threshold check, and in line 12 we have inserted an if-statement to only do the threshold check every 1/γ iteration. In this algorithm, the threshold check gives the same guarantees as the statistical query, and it will therefore not affect correctness. However, the introduction of the if-statement could lead to two kinds of errors, since we do not check the value of d(µ t ) in every iteration. First, it could be that R S fails, since it needs d(µ t ) to be large. Second, it could be that the number of iterations is increased, since the algorithm does not detect immediately when the density becomes small. To show that these events are not problematic, we first show that the densities do not decrease too much over 1/γ iterations.</p><p>Lemma 3.1. Let T 0 denote the number of iterations that B * runs for. Then for all t ∈ [T 0 ] and k ≤ max{⌊1/γ⌋, T 0 -t} that d(µ t+k ) ≥ d(µ t )/2.</p><p>Proof. Let x ∈ X . Then by the recursive definition of the g t 's, we have µ t+1 (x) ≥ (1 -γ) 1/2 µ t (x). Inductively, we get</p><formula xml:id="formula_32">µ t+k (x) ≥ µ t (x)(1 -γ) k/2 ≥ µ t (1 -γ) ⌊ 1 γ ⌋/2 ≥ µ t (x) 1 -γ⌊ 1 γ ⌋/2 ≥ 1 2 µ t (x),</formula><p>where we use Bernoulli's inequality which applies since -γ &gt; -1. Taking the expectation with respect to D on both sides yields the desired conclusion.</p><p>Theorem 1.4 Restated. For any ρ, ε ∈ (0, 1) and Θ(ρεγ 2 )-replicable weak learner W with advantage γ, B * is ρ-replicable, makes O( 1 εγ 2 ) calls to W, and with probability at least 1 -ρ outputs a hypothesis H with Er D (H) ≤ ε. Furthermore, its sample complexity is</p><formula xml:id="formula_33">m B * (ρ, ε) = O ln( 1 ρεγ 2 )m W(Θ(ρεγ 2 )) ε 2 γ 2 + ln( 1 ρεγ ) ρ 2 ε 4 γ 3 = O m W(Θ(ρεγ 2 )) ε 2 γ 2 + 1 ρ 2 ε 4 γ 3 .</formula><p>Proof. First, we argue that the R S succeeds with high probability. Observe that due to Lemma 3.1, the densities are at most halved in B * compared to the original version of B . Due to Lemma 2.1 we therefore only need to use twice as many samples in the rejection sampler for it to still succeed.</p><p>Next, we will argue that the number of iterations remains the same as in B up to constant factors. The number of iterations is bounded in <ref type="bibr" target="#b15">Servedio [2001]</ref> by showing that for any κ &gt; 0, there is some t within the first O( 1 κγ 2 ) iterations such that d(µ t ) &lt; κ. This result also applies to B * . However, we will not repeat the proof here. We can then conclude that d(µ t ) will fall below ε/8 within the first O( 1 εγ 2 ) iterations. Since the threshold check in line 14 always realizes when d(µ t ) ≤ ε/4 (see Lemma 1.3), and it takes 1/γ iterations to further decrease the density from ε/4 to ε/8, T will always have terminated the loop before reaching density ε/8. Hence, the number of iterations in B * is still T 0 = O( 1 εγ 2 ). We now argue, that replicability is preserved. Since this argument is almost identical to the proof of Lemma 2.5, we will only describe what differs in this analysis. First, the weak learner is called T 0 times, and therefore it needs replicability parameter ρ/(6T 0 ) = Θ(ρεγ 2 ). Second, T is called γT 0 times and therefore needs replicability parameter ρ/(6γT 0 ) = ρεγ/6, which it achieves due to Lemma 1.3. Thus, our modifications preserve replicability.</p><p>Finally, we calculate the sample complexity. The R S uses O(ln( T0 ρ )m W(Θ(ρεγ 2 )) /ε) samples for each call, and it is called T 0 times. Meanwhile, T uses O( ln(γT0/ρ) ρ 2 ε 3 γ 2 ) samples for each call, and is called γT 0 times. Hence, the total sample complexity is</p><formula xml:id="formula_34">O T 0 ln( T0 ρ )m W(Θ(ρεγ 2 )) ε + γT 0 ln( γT0 ρ ) ρ 2 ε 3 γ 2 = O ln( 1 ρεγ 2 )m W(Θ(ρεγ 2 )) ε 2 γ 2 + ln( 1 ρεγ ) ρ 2 ε 4 γ 3 = O m W(Θ(ρεγ 2 )) ε 2 γ 2 + 1 ρ 2 ε 4 γ 3 .</formula></div>		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>All authors are supported by the <rs type="funder">European Union</rs> (ERC, TUCLA, 101125203). Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the <rs type="funder">European Research Council</rs>. Neither the <rs type="funder">European Union</rs> nor the granting.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Proof. It is sufficient to set m ≥ 700 ln(1/δ) zρ 2 . We will first prove the first bullet point. Hence, assume that E x∼D [ϕ(x)] ≤ z/2. We then bound the following probability:</p><p>Since the assumption states that E x∼D [ϕ(x)] ≤ z/2, we can use a Chernoff bound to bound the above probability by</p><p>We move on to the second bullet point. Hence, we now assume E x∼D [ϕ(x)] ≥ 2z and bound the following probability:</p><p>Again, since E x∼D [ϕ(x)] ≥ 2z, we can use a Chernoff bound to bound the above probability by</p><p>We will now show that T is ρ-replicable by considering two different runs of the algorithm with common randomness. Let S 1 , S 2 ∼ D m be the two sequences of samples used in the two runs. Assuming that neither of the runs fail, they will always output the same bit if</p><p>], so we can safely assume that this is not the case. Now, we will bound the probability that ϕ(S i ) deviates too much from E x∼D [ϕ(x)]. So we bound the following probabilities using Chernoff bounds and the assumption that E x∼D [ϕ(x)] ∈ [z/2, 2z]:</p><p>Using a union bound, we can conclude that</p><p>We can therefore conclude that with high probability, the two estimates will be close to each other. That is,</p><p>So assume for now, that the two estimates are within 3zρ/8 of each other. Then the two runs will only give different outputs if the random split z 0 is chosen between them. The probability of this happening is at most the distance between the estimates divided by the total range of z 0 , which is at most 3zρ/8 3z/2 -3z/4 = ρ/2.</p><p>So the probability that the two runs output different bits is at most ρ/2 + ρ/2 = ρ. Therefore, the algorithm is ρ-replicable.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Ahmadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2411.13730</idno>
		<title level="m">Replicable online learning</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Reproducibility crisis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">533</biblScope>
			<biblScope unit="issue">26</biblScope>
			<biblScope unit="page" from="353" to="366" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Is ai leading to a reproducibility crisis in science?</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ball</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">624</biblScope>
			<biblScope unit="issue">7990</biblScope>
			<biblScope unit="page" from="22" to="25" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The uniform hardcore lemma via approximate bregman projections</title>
		<author>
			<persName><forename type="first">B</forename><surname>Barak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twentieth annual ACM-SIAM symposium on Discrete algorithms</title>
		<meeting>the twentieth annual ACM-SIAM symposium on Discrete algorithms</meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1193" to="1200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Stability is stable: Connections between replicability, privacy, and adaptive generalization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gaboardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hopkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Impagliazzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pitassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sivakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sorrell</surname></persName>
		</author>
		<idno type="DOI">10.1145/3564246.3585246</idno>
		<ptr target="https://doi.org/10.1145/3564246.3585246" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual ACM Symposium on Theory of Computing, STOC 2023</title>
		<meeting>the 55th Annual ACM Symposium on Theory of Computing, STOC 2023<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="520" to="527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">List and certificate complexities in replicable learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vander Woude</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vinodchandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Boosting and differential privacy</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">N</forename><surname>Rothblum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vadhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE 51st annual symposium on foundations of computer science</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="51" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Replicable reinforcement learning</title>
		<author>
			<persName><forename type="first">E</forename><surname>Eaton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hussing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sorrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS &apos;23</title>
		<meeting>the 37th International Conference on Neural Information Processing Systems, NIPS &apos;23<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Replicable clustering</title>
		<author>
			<persName><forename type="first">H</forename><surname>Esfandiari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mirrokni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Velegkas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS &apos;23</title>
		<meeting>the 37th International Conference on Neural Information Processing Systems, NIPS &apos;23<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A desicion-theoretic generalization of on-line learning and an application to boosting</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Learning Theory</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Vitányi</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="23" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Reproducibility in learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Impagliazzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pitassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sorrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th annual ACM SIGACT symposium on theory of computing</title>
		<meeting>the 54th annual ACM SIGACT symposium on theory of computing</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="818" to="831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Replicable learning of largemargin halfspaces</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kalavasis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Velegkas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Heller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Weller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Scarlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Berkenkamp</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="https://proceedings.mlr.press/v235/kalavasis24a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st International Conference on Machine Learning</title>
		<meeting>the 41st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2024-07">Jul 2024</date>
			<biblScope unit="volume">235</biblScope>
			<biblScope unit="page" from="21" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning boolean formulae or finite automata is as hard as factoring</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kearns</surname></persName>
		</author>
		<idno>TR-14-88</idno>
		<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
		<respStmt>
			<orgName>Harvard University Aikem Computation Laboratory</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cryptographic limitations on learning boolean formulae and finite automata</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Valiant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="67" to="95" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The strength of weak learnability</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="197" to="227" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Smooth boosting and learning with malicious noise</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Servedio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Learning Theory</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Helmbold</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Williamson</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="473" to="489" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
