{
  "arxivId": "1602.03483",
  "title": "Learning Distributed Representations of Sentences from Unlabelled Data",
  "authors": "Felix Hill, Kyunghyun Cho, Anna Korhonen",
  "abstract": "Unsupervised methods for learning distributed representations of words are\nubiquitous in today's NLP research, but far less is known about the best ways\nto learn distributed phrase or sentence representations from unlabelled data.\nThis paper is a systematic comparison of models that learn such\nrepresentations. We find that the optimal approach depends critically on the\nintended application. Deeper, more complex models are preferable for\nrepresentations to be used in supervised systems, but shallow log-linear models\nwork best for building representation spaces that can be decoded with simple\nspatial distance metrics. We also propose two new unsupervised\nrepresentation-learning objectives designed to optimise the trade-off between\ntraining time, domain portability and performance.",
  "url": "https://arxiv.org/abs/1602.03483",
  "issue_number": 777,
  "issue_url": "https://github.com/dmarx/papers-feed/issues/777",
  "created_at": "2025-01-04T06:51:57.839597",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 24,
  "last_read": "2025-01-04T06:51:57.840814",
  "last_visited": "2025-01-03T20:13:43.540Z",
  "main_tex_file": null,
  "published_date": "2016-02-10T18:49:58Z",
  "arxiv_tags": [
    "cs.CL",
    "cs.LG"
  ]
}