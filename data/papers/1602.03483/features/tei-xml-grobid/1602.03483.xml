<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Distributed Representations of Sentences from Unlabelled Data</title>
				<funder>
					<orgName type="full">Google Faculty Award</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2016-02-10">10 Feb 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
							<email>felix.hill@cl.cam.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Laboratory</orgName>
								<orgName type="department" key="dep2">Courant Institute of Mathematical Sciences &amp; Centre for Data Science</orgName>
								<orgName type="department" key="dep3">Department of Theoretical &amp; Applied Linguistics</orgName>
								<orgName type="institution" key="instit1">University of Cambridge</orgName>
								<orgName type="institution" key="instit2">New York University</orgName>
								<orgName type="institution" key="instit3">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
							<email>kyunghyun.cho@nyu.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Laboratory</orgName>
								<orgName type="department" key="dep2">Courant Institute of Mathematical Sciences &amp; Centre for Data Science</orgName>
								<orgName type="department" key="dep3">Department of Theoretical &amp; Applied Linguistics</orgName>
								<orgName type="institution" key="instit1">University of Cambridge</orgName>
								<orgName type="institution" key="instit2">New York University</orgName>
								<orgName type="institution" key="instit3">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Laboratory</orgName>
								<orgName type="department" key="dep2">Courant Institute of Mathematical Sciences &amp; Centre for Data Science</orgName>
								<orgName type="department" key="dep3">Department of Theoretical &amp; Applied Linguistics</orgName>
								<orgName type="institution" key="instit1">University of Cambridge</orgName>
								<orgName type="institution" key="instit2">New York University</orgName>
								<orgName type="institution" key="instit3">University of Cambridge</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Distributed Representations of Sentences from Unlabelled Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-02-10">10 Feb 2016</date>
						</imprint>
					</monogr>
					<idno type="MD5">D89B3DA9EB0DB230942E7FBE20B24EEA</idno>
					<idno type="arXiv">arXiv:1602.03483v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unsupervised methods for learning distributed representations of words are ubiquitous in today's NLP research, but far less is known about the best ways to learn distributed phrase or sentence representations from unlabelled data. This paper is a systematic comparison of models that learn such representations. We find that the optimal approach depends critically on the intended application. Deeper, more complex models are preferable for representations to be used in supervised systems, but shallow log-linear models work best for building representation spaces that can be decoded with simple spatial distance metrics. We also propose two new unsupervised representation-learning objectives designed to optimise the trade-off between training time, domain portability and performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Distributed representations -dense real-valued vectors that encode the semantics of linguistic units -are ubiquitous in today's NLP research. For single-words or word-like entities, there are established ways to acquire such representations from naturally occurring (unlabelled) training data based on comparatively task-agnostic objectives (such as predicting adjacent words). These methods are well understood empirically <ref type="bibr">(Baroni et al., 2014b)</ref> and theoretically <ref type="bibr">(Levy and Goldberg, 2014)</ref>. The best word representation spaces reflect consistentlyobserved aspects of human conceptual organisation <ref type="bibr">(Hill et al., 2015b)</ref>, and can be added as features to improve the performance of numerous language processing systems <ref type="bibr" target="#b4">(Collobert et al., 2011)</ref>.</p><p>By contrast, there is comparatively little consensus on the best ways to learn distributed representations of phrases or sentences. 1 With the advent of deeper language processing techniques, it is relatively common for models to represent phrases or sentences as continuous-valued vectors. Examples include machine translation <ref type="bibr" target="#b20">(Sutskever et al., 2014)</ref>, image captioning <ref type="bibr" target="#b14">(Mao et al., 2015)</ref> and dialogue systems <ref type="bibr" target="#b18">(Serban et al., 2015)</ref>. While it has been observed informally that the internal sentence representations of such models can reflect semantic intuitions <ref type="bibr" target="#b3">(Cho et al., 2014)</ref>, it is not known which architectures or objectives yield the 'best' or most useful representations. Resolving this question could ultimately have a significant impact on language processing systems. Indeed, it is phrases and sentences, rather than individual words, that encode the human-like general world knowledge (or 'common sense') <ref type="bibr" target="#b17">(Norman, 1972)</ref> that is a critical missing part of most current language understanding systems.</p><p>We address this issue with a systematic comparison of cutting-edge methods for learning distributed representations of sentences. We constrain our comparison to methods that do not require labelled data gathered for the purpose of training models, since such methods are more cost-effective and applicable across languages and domains. We also propose two new phrase or sentence representation learning objectives -Sequential Denoising Autoencoders (SDAEs) and FastSent, a sentence-level log-linear bag-of-words model. We compare all methods on two types of task -supervised and unsupervised evaluations -reflecting different ways in which representations are ultimately to be used. In the former setting, a classifier or regression model is applied to representations and trained with task-specific labelled data, while in the latter, representation spaces are directly queried using cosine distance.</p><p>We observe notable differences in approaches depending on the nature of the evaluation metric. In particular, deeper or more complex models (which require greater time and resources to train) generally perform best in the supervised setting, whereas shallow log-linear models work best on unsupervised benchmarks. Specifically, SkipThought Vectors <ref type="bibr">(Kiros et al., 2015)</ref> perform best on the majority of supervised evaluations, but SDAEs are the top performer on paraphrase identification. In contrast, on the (unsupervised) SICK sentence relatedness benchmark, FastSent, a simple, log-linear variant of the SkipThought objective, performs better than all other models. Interestingly, the method that exhibits strongest performance across both supervised and unsupervised benchmarks is a bag-of-words model trained to compose word embeddings using dictionary definitions <ref type="bibr">(Hill et al., 2015a)</ref>. Taken together, these findings constitute valuable guidelines for the application of phrasal or sentential representationlearning to language understanding systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Distributed Sentence Representations</head><p>To constrain the analysis, we compare neural language models that compute sentence representations from unlabelled, naturally-ocurring data, as with the predominant methods for word representations. 2 Likewise, we do not focus on 'bottom up' models where phrase or sentence representations are built from fixed mathematical operations on word vectors (although we do consider a canonical casesee CBOW below); these were already compared by <ref type="bibr" target="#b15">Milajevs et al. (2014)</ref>. Most space is devoted to our novel approaches, and we refer the reader to the original papers for more details of existing models.</p><p>2 This excludes innovative supervised sentencelevel architectures including (Socher et al., 2011; Kalchbrenner et al., 2014) and many others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Existing Models Trained on Text</head><p>SkipThought Vectors For consecutive sentences S i-1 , S i , S i+1 in some document, the SkipThought model <ref type="bibr">(Kiros et al., 2015)</ref> is trained to predict target sentences S i-1 and S i+1 given source sentence S i . As with all sequence-to-sequence models, in training the source sentence is 'encoded' by a Recurrent Neural Network (RNN) (with Gated Recurrent uUnits <ref type="bibr" target="#b3">(Cho et al., 2014)</ref>) and then 'decoded' into the two target sentences in turn. Importantly, because RNNs employ a single set of update weights at each time-step, both the encoder and decoder are sensitive to the order of words in the source sentence.</p><p>For each position in a target sentence S t , the decoder computes a softmax distribution over the model's vocabulary. The cost of a training example is the sum of the negative log-likelihood of each correct word in the target sentences S i-1 and S i+1 . This cost is backpropagated to train the encoder (and decoder), which, when trained, can map sequences of words to a single vector.</p><p>ParagraphVector <ref type="bibr" target="#b12">Le and Mikolov (2014)</ref> proposed two log-linear models of sentence representation. The DBOW model learns a vector s for every sentence S in the training corpus which, together with word embeddings v w , define a softmax distribution optimised to predict words w ∈ S given S. The v w are shared across all sentences in the corpus. In the DM model, k-grams of consecutive words {w i . . . w i+k ∈ S} are selected and s is combined with {v w i . . . v w i+k } to make a softmax prediction (parameterised by additional weights) of w i+k+1 .</p><p>We used the Gensim implementation,<ref type="foot" target="#foot_0">foot_0</ref> treating each sentence in the training data as a 'paragraph' as suggested by the authors. During training, both DM and DBOW models store representations for every sentence (as well as word) in the training corpus. Even on large servers it was therefore only possible to train models with representation size 200, and DM models whose combination operation was averaging (rather than concatenation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bottom-Up Methods</head><p>We train CBOW and Skip-Gram word embeddings <ref type="bibr">(Mikolov et al., 2013b)</ref> on the Books corpus, and compose by elementwise ad-dition as proposed by <ref type="bibr" target="#b16">Mitchell and Lapata (2010)</ref>. <ref type="foot" target="#foot_1">4</ref>We also compare to C-PHRASE <ref type="bibr" target="#b17">(Pham et al., 2015)</ref>, an approach that exploits a (supervised) parser to infer distributed semantic representations based on a syntactic parse of sentences. C-PHRASE achieves state-of-the-art results for distributed representations on several evaluations used in this study.<ref type="foot" target="#foot_2">foot_2</ref> </p><p>Non-Distributed Baseline We implement a TFIDF BOW model in which the representation of sentence S encodes the count in S of a set of feature-words weighted by their tfidf in C, the corpus. The featurewords are the 200,000 most common words in C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Models Trained on Structured Resources</head><p>The following models rely on (freely-available) data that has more structure than raw text.</p><p>DictRep <ref type="bibr">Hill et al. (2015a)</ref> trained neural language models to map dictionary definitions to pre-trained word embeddings of the words defined by those definitions. They experimented with BOW and RNN (with LSTM) encoding architectures and variants in which the input word embeddings were either learned or pre-trained (+embs.) to match the target word embeddings. We implement their models using the available code and training data.<ref type="foot" target="#foot_3">foot_3</ref> </p><p>CaptionRep Using the same overall architecture, we trained (BOW and RNN) models to map captions in the COCO dataset <ref type="bibr" target="#b2">(Chen et al., 2015)</ref> to pre-trained vector representations of images. The image representations were encoded by a deep convolutional network <ref type="bibr" target="#b21">(Szegedy et al., 2014)</ref>  have identical architecture to SkipThought, but are trained on sentence-aligned translated texts. We used a standard architecture <ref type="bibr" target="#b3">(Cho et al., 2014)</ref> on all available En-Fr and En-De data from the 2015 Workshop on Statistical MT (WMT).<ref type="foot" target="#foot_4">foot_4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Novel Text-Based Models</head><p>We introduce two new approaches designed to address certain limitations with the existing models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sequential (Denoising) Autoencoders</head><p>The SkipThought objective requires training text with a coherent inter-sentence narrative, making it problematic to port to domains such as social media or artificial language generated from symbolic knowledge. To avoid this restriction, we experiment with a representation-learning objective based on denoising autoencoders (DAEs). In a DAE, high-dimensional input data is corrupted according to some noise function, and the model is trained to recover the original data from the corrupted version. As a result of this process, DAEs learn to represent the data in terms of features that explain its important factors of variation <ref type="bibr" target="#b21">(Vincent et al., 2008)</ref>. Transforming data into DAE representations (as a 'pre-training' or initialisation step) gives more robust (supervised) classification performance in deep feedforward networks <ref type="bibr" target="#b22">(Vincent et al., 2010)</ref>.</p><p>The original DAEs were feedforward nets applied to (image) data of fixed size. Here, we adapt the approach to variable-length sentences by means of a noise function N (S|p o , p x ), determined by free parameters p o , p x ∈ [0, 1]. First, for each word w in S, N deletes w with (independent) probability p o . Then, for each non-overlapping bigram w i w i+1 in S, N swaps w i and w i+1 with probability p x . We then train the same LSTM-based encoder-decoder architecture as NMT, but with the denoising objective to predict (as target) the original source sentence S given a corrupted version N (S|p o , p x ) (as source). The trained model can then encode novel word sequences into distributed representations. We call this model the Sequential Denoising Autoencoder (SDAE). Note that, unlike SkipThought, SDAEs can be trained on sets of sentences in arbitrary order.</p><p>We label the case with no noise (i.e. p o = p x = 0 and N ≡ id) SAE. This set-ting matches the method applied to text classification tasks by <ref type="bibr" target="#b5">Dai and Le (2015)</ref>. The 'word dropout' effect when p o ≥ 0 has also been used as a regulariser for deep nets in supervised language tasks <ref type="bibr" target="#b10">(Iyyer et al., 2015)</ref>, and for large p x the objective is similar to word-level 'debagging' <ref type="bibr" target="#b19">(Sutskever et al., 2011)</ref>. For the SDAE, we tuned p o , p x on the validation set (see Section 3.2). <ref type="foot" target="#foot_5">8</ref>We also tried a variant (+embs) in which words are represented by (fixed) pre-trained embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FastSent</head><p>The performance of SkipThought vectors shows that rich sentence semantics can be inferred from the content of adjacent sentences.</p><p>The model could be said to exploit a type of sentence-level Distributional Hypothesis <ref type="bibr" target="#b7">(Harris, 1954;</ref><ref type="bibr">Polajnar et al., 2015)</ref>. Nevertheless, like many deep neural language models, SkipThought is very slow to train (see Table <ref type="table" target="#tab_1">1</ref>). FastSent is a simple additive (log-linear) sentence model designed to exploit the same signal, but at much lower computational expense. Given a BOW representation of some sentence in context, the model simply predicts adjacent sentences (also represented as BOW) .</p><p>More formally, FastSent learns a source u w and target v w embedding for each word in the model vocabulary. For a training example S i-1 , S i , S i+1 of consecutive sentences, S i is represented as the sum of its source embeddings s i = w∈S i u w . The cost of the example is then simply:</p><formula xml:id="formula_0">w∈S i-1 ∪S i+1 φ(s i , v w )<label>(1)</label></formula><p>where φ(v 1 , v 2 ) is the softmax function.</p><p>We also experiment with a variant (+AE) in which the encoded (source) representation must predict its own words as target in addition to those of adjacent sentences. Thus in FastSent+AE, (1) becomes</p><formula xml:id="formula_1">w∈S i-1 ∪S i ∪S i+1 φ(s i , v w ).</formula><p>( </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Training and Model Selection</head><p>Unless stated above, all models were trained on the Toronto Books Corpus,<ref type="foot" target="#foot_6">foot_6</ref> which has the intersentential coherence required for SkipThought and FastSent. The corpus consists of 70m ordered sentences from over 7,000 books. Specifications of the models are shown in Table 1. The log-linear models (SkipGram, CBOW, ParagraphVec and FastSent) were trained for one epoch on one CPU core. The representation dimension d for these models was found after tuning d ∈ {100, 200, 300, 400, 500} on the validation set. <ref type="foot" target="#foot_7">10</ref> All other models were trained on one GPU. The S(D)AE models were trained for one epoch (≈ 8 days). The SkipThought model was trained for two weeks, covering just under one epoch. <ref type="foot" target="#foot_8">11</ref> For CaptionRep and DictRep, performance was monitored on held-out training data and training was stopped after 24 hours after a plateau in cost. The NMT models were trained for 72 hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluating Sentence Representations</head><p>In previous work, distributed representations of language were evaluated either by measuring the effect of adding representations as features in some classification task -supervised evaluation <ref type="bibr" target="#b4">(Collobert et al., 2011;</ref><ref type="bibr">Mikolov et al., 2013a;</ref><ref type="bibr">Kiros et al., 2015)</ref> -or by comparing with human relatedness judgements -unspervised evaluation <ref type="bibr">(Hill et al., 2015a;</ref><ref type="bibr">Baroni et al., 2014b;</ref><ref type="bibr" target="#b13">Levy et al., 2015)</ref>.</p><p>The former setting reflects a scenario in which representations are used to inject general knowledge (sometimes considered as pre-training) into a supervised model. The latter pertains to applications in which the sentence representation space is used for direct comparisons, lookup or retrieval. Here, we apply and compare both evaluation paradigms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Supervised Evaluations</head><p>Representations are applied to 6 sentence classification tasks: paraphrase identification (MSRP) <ref type="bibr" target="#b6">(Dolan et al., 2004)</ref>, movie review sentiment (MR) (Pang and Lee, 2005), product reviews (CR) <ref type="bibr" target="#b9">(Hu and Liu, 2004)</ref>, subjectivity classification (SUBJ) (Pang and Lee, 2004), opinion polarity (MPQA) <ref type="bibr" target="#b24">(Wiebe et al., 2005)</ref> and question type classification (TREC) <ref type="bibr" target="#b23">(Voorhees, 2002)</ref>. We follow the procedure (and code) of <ref type="bibr">Kiros et al. (2015)</ref>: a logistic regression classifier is trained on top of sentence representations, with 10-fold cross-validation used when a train-test split is not pre-defined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Unsupervised Evaluations</head><p>We also measure how well representation spaces reflect human intuitions of the semantic sentence relatedness, by computing the cosine distance between vectors for the two sentences in each test pair, and correlating these distances with gold-standard human judgements. The SICK dataset <ref type="bibr" target="#b14">(Marelli et al., 2014)</ref> consists of 10,000 pairs of sentences and relatedness judgements. The STS 2014 dataset <ref type="bibr" target="#b0">(Agirre et al., 2014)</ref> consists of 3,750 pairs and ratings from six linguistic domains. Example ratings are shown in Table <ref type="table" target="#tab_2">2</ref>. All available pairs are used for testing apart from the 500 SICK 'trial' pairs, which are held-out for tuning hyperparameters (representation size of log-linear models, and noise parameters in SDAE). The optimal settings on this task are then applied to both supervised and unsupervised evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>Performance of the models on the supervised evaluations (grouped according to the data required by their objective) is shown in Table <ref type="table" target="#tab_3">3</ref>. Overall, SkipThought vectors perform best on three of the six evaluations, the BOW DictRep model with pretrained word embeddings performs best on two, and the SDAE on one. SDAEs perform notably well on the paraphrasing task, going beyond SkipThought by three percentage points and approaching stateof-the-art performance of models designed specifically for the task <ref type="bibr" target="#b10">(Ji and Eisenstein, 2013)</ref>. SDAE is also consistently better than SAE, which aligns with other findings that adding noise to AEs produces richer representations <ref type="bibr" target="#b21">(Vincent et al., 2008)</ref>.</p><p>Results on the unsupervised evaluations are shown in Table <ref type="table" target="#tab_5">4</ref>. The same DictRep model performs best on four of the six STS categories (and overall) and is joint-top performer on SICK. Of the models trained on raw text, simply adding CBOW word vectors works best on STS. The best performing raw text model on SICK is FastSent, which achieves almost identical performance to C-PHRASE's state-of-the-art performance for a distributed model <ref type="bibr" target="#b17">(Pham et al., 2015)</ref>. Further, it uses less than a third of the training text and does not require access to (supervised) syntactic representations for training. Together, the results of FastSent on the unsupervised evaluations and SkipThought on the supervised benchmarks provide strong support for the sentence-level distributional hypothesis: the context in which a sentence occurs provides valuable information about its semantics. Across both unsupervised and supervised evaluations, the BOW DictRep with pre-trained word embeddings exhibits by some margin the most consistent performance. Ths robust performance suggests that DictRep representations may be particularly valuable when the ultimate application is nonspecific or unknown, and confirms that dictionary definitions (where available) can be a powerful resource for representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Many additional conclusions can be drawn from the results in Tables <ref type="table" target="#tab_3">3</ref> and <ref type="table" target="#tab_5">4</ref>.</p><p>Different objectives yield different representations It may seem obvious, but the results confirm that different learning methods are preferable for different intended applications (and this variation appears greater than for word representations). For instance, it is perhaps unsurprising that SkipThought performs best on TREC because the labels in this dataset are determined by the language immediately following the represented question (i.e. the answer) <ref type="bibr" target="#b23">(Voorhees, 2002)</ref>. Paraphrase detection, on the other hand, may be better served by a model that focused entirely on the content within a sentence, such as SDAEs. Similar variation can be observed in the unsupervised evaluations. For instance, the (multimodal) representations produced by the CaptionRep model do not perform particularly well apart from on the Image category of STS where they beat all other models, demonstrating a clear effect of the well-studied modality differences in representation learning <ref type="bibr">(Bruni et al., 2014)</ref>.</p><p>The nearest neighbours in Table <ref type="table" target="#tab_7">5</ref> give a more concrete sense of the representation spaces. One notable difference is between (AE-style) models whose semantics come from within-sentence relationships (CBOW, SDAE, DictRep, ParagraphVec) and SkipThought/FastSent, which exploit the context around sentences. In the former case, nearby sentences generally have a high proportion of words in common, whereas for the latter it is the general concepts and/or function of the sentence that is similar, and word overlap is often minimal. Indeed,</p><p>STS 2014 SICK Model News Forum WordNet Twitter Images Headlines All Test + Train SAE 17/.16 .12/.12 .30/.23 .28/.22 .49/.46 .13/.11 .12/.13 .32/.31 SAE+embs. .52/.54 .22/.23 .60/.55 .60/.60 . 64/.64 .41/.41 .42/.43 .47/.49 SDAE .07/.04 .11/.13 .33/.24 .44/.42 .44/.38 .36/.36 .17/.15 .46/.46 SDAE+embs. .51/.54 .29/.29 .56/.50 .57/.58 .59/.59 .43/.44 .37/.38 .46/.46 ParagraphVec DBOW .31/.34 .32/.32 .53/.5 .43/.46 .46/.44 .39/.41 .42/.43 .42/.46 ParagraphVec DM .42/.46 .33/.34 .51/.48 .54/.57 .32/.30 .46/.47 .44/.44 .44/.46 Skipgram .56/.59 .42/.42 .73/.70 .71/.74 .65/.67 .55/.58 .62/.63 .60/.69 CBOW .57/.61 .43/.44 .72/.69 .71/.75 .71/.73 .55/.59 .64/.65 .60/.69 Unigram TFIDF .48/.48 .40/.38 .60/.59 .63/.65 72/.74 .49/.49 .58/.57 .52/.58 SkipThought .44/.45 .14/.15 .39/.34 .42/.43 .55/.60 .43/.44 .27/.29 .57/.60 FastSent .58/.59 .41/.36 .74/.70 .63/.66 .74/.78 .57/.59 .63/.64 .61/.72 FastSent+AE .56/ .59 .41/.40 .69/.64 .70/.74 .63/.65 .58/.60 .62/.62 .60/.65 NMT En to Fr .35/.32 .18/.18 .47/.43 .55/.53 .44/.45 .43/.43 .43/.42 .47/.49 NMT En to De .47/.43 .26/.25 .34/.31 .49/.45 .44/.43 .38/.37 .40/.38 .46/46 CaptionRep BOW .26/.26 .29/.22 .50/.35 .37/.31 .78/.81 .39/.36 .46/.42 .56/.65 CaptionRep RNN .05/.05 .13/.09 .40/.33 .36/.30 .76/.82 .30/.28 .39/.36 .53/.62 DictRep BOW .62/.67 .42/.40 .81/.81 .62/.66 .66/.68 .53/.58 .62/.65 .57/.66 DictRep BOW+embs. .65/.72 .49/.47 .85/.86 .67/.72 .71/.74 .57/.61 .67/.70 .61/.70 DictRep RNN .40/.46 .26/.23 .78/.78 .42/.42 .56/.56 .38/.40 .49/.50 .49/.56 DictRep RNN+embs. .51/.60 .29/.27 .80/.81 .44/.47 .65/.70 .42/.46 .54/.57 .49/.59 CPHRASE .69/.71 .43/.41 .76/.73 .60/.65 .75/.79 .60/.65 .65/.67 .60/.72 Differences between supervised and unsupervised performance Many of the best performing models on the supervised evaluations do not perform well in the unsupervised setting. In the SkipThought, S(D)AE and NMT models, the cost is computed based on a non-linear decoding of the internal sentence representations, so, as also observed by <ref type="bibr" target="#b1">(Almahairi et al., 2015)</ref>, the informative geometry of the representation space may not be reflected in a simple cosine distance. The log-linear models generally perform better in this unsupervised setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Differences in resource requirements</head><p>As shown in Table <ref type="table" target="#tab_1">1</ref>, different models require different resources to train and use. This can limit their possible applications. For instance, while it was easy to make an online demo for fast querying of near neighbours in the CBOW and FastSent spaces, it was not practical for other models owing to memory footprint, encoding time and representation dimension.</p><p>The role of word order is unclear The average scores of models that are sensitive to word order (76.3) and of those that are not (76.6) are approximately the same across supervised evaluations. Across the unsupervised evaluations, however, BOW models score 0.55 on average compared with 0.42 for RNN-based (order sensitive) models. This seems at odds with the widely held view that word order plays an important role in determining the meaning of English sentences. One possibility is that order-critical sentences that cannot be disambiguated by a robust conceptual semantics (that could be encoded in distributed lexical representations) are in fact relatively rare. However, it is also plausible that current available evaluations do not adequately reflect order-dependent aspects of meaning (see below). This latter conjecture is supported by the comparatively strong performance of TFIDF BOW vectors, in which the effective lexical semantics are limited to simple relative frequencies.</p><p>Query If he had a weapon, he could maybe take out An annoying buzz started to ring in my ears, becoming their last imp, and then beat up Errol and Vanessa. louder and louder as my vision began to swim. CBOW Then Rob and I would duke it out, and every Louder. once in a while, he would actually beat me. Skip If he could ram them from behind, send them saling over A weighty pressure landed on my lungs and my vision blurred Thought the far side of the levee, he had a chance of stopping them. at the edges, threatening my consciousness altogether. FastSent Isak's close enough to pick off any one of them, The noise grew louder, the quaking increased as the maybe all of them, if he had his rifle and a mind to. sidewalk beneath my feet began to tremble even more. SDAE He'd even killed some of the most dangerous criminals I smile because I'm familiar with the knock, in the galaxy, but none of those men had gotten to him like Vitktis. pausing to take a deep breath before dashing down the stairs. DictRep Kevin put a gun to the man's head, but even though Then gradually I began to hear a ringing in my ears. (FF+embs.) he cried, he couldn't tell Kevin anything more. Paragraph I take a deep breath and open the doors. They listened as the motorcycle-like roar Vector (DM) of an engine got louder and louder then stopped. Table 6: Internal consistency (Chronbach's α) among evaluations when individual benchmarks are left out of the (supervised or unsupervised) cohorts. Consistency rank within cohort is in parentheses (1 = most consistent with other evaluations).</p><p>The evaluations have limitations The internal consistency (Chronbach's α) of all evaluations considered together is 0.81 (just above 'acceptable'). 12  Table <ref type="table">6</ref> shows that consistency is far higher ('excellent') when considering the supervised or unsupervised tasks as independent cohorts. This indicates that, with respect to common characteristics of sentence representations, the supervised and unsupervised benchmarks do indeed prioritise different properties. It is also interesting that, by this metric, the properties measured by MSRP and imagecaption relatedness are the furthest removed from other evaluations in their respective cohorts.</p><p>While these consistency scores are a promising sign, they could also be symptomatic of a set of evaluations that are all limited in the same way. The inter-rater agreement is only reported for one of the 8 evaluations considered (MPQA, 0.72 <ref type="bibr" target="#b24">(Wiebe et al., 2005)</ref>), and for MR, SUBJ and TREC, each item is only rated by one or two annotators to maximise coverage. Table <ref type="table" target="#tab_2">2</ref> illustrates why this may be an issue for the unsupervised evaluations; the notion of sentential 'relatedness' seems very subjective. It should be emphasised, however, that the tasks considered in this study are all frequently used for evaluation, and, to our knowledge, there are no existing benchmarks that over-12 wikipedia.org/wiki/Cronbach's_alpha come these limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Advances in deep learning algorithms, software and hardware mean that many architectures and objectives for learning distributed sentence representations from unlabelled data are now available to NLP researchers. We have presented the first (to our knowledge) systematic comparison of these methods. We showed notable variation in the performance of approaches across a range of evaluations. Among other conclusions, we found that the optimal approach depends critically on whether representations will be applied in supervised or unsupervised settings -in the latter case, fast, shallow BOW models can still achieve the best performance. Further, we proposed two new objectives, FastSent and Sequential Denoising Autoencoders, which perform particularly well on specific tasks (MSRP and SICK sentence relatedness respectively). <ref type="foot" target="#foot_9">13</ref> If the application is unknown, however, the best all round choice may be DictRep: learning a mapping of pretrained word embeddings from the word-phrase signal in dictionary definitions. While we have focused on models using naturally-occurring training data, in future work we will also consider supervised architectures (including convolutional, recursive and character-level models), potentially training them on multiple supervised tasks as an alternative way to induce the 'general knowledge' needed to give language technology the elusive human touch.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc>trained on the ILSVRC 2014 object recognition task (Russakovsky et al., 2014). Multi-modal distributed representations can be encoded by feeding test sentences forward through the trained model. NMT We consider the sentence representations learned by neural MT models. These models Definitions from the training data matching those in the WordNet STS 2014 evaluation (used in this study) were excluded.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 : Properties of models compared in this study OS: requires</head><label>1</label><figDesc>training corpus of sentences in order. R: requires structured resource for training. WO: encoder sensitive to word order. SD: dimension of sentence representation. WD: dimension of word representation. TR: approximate training time (hours) on the dataset in this paper. * indicates trained on GPU.</figDesc><table><row><cell>2)</cell></row></table><note><p>TE: approximate time (s) taken to encode 0.5m sentences.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Example sentence pairs and 'similarity' ratings from the unsupervised evaluations used in this study.</figDesc><table><row><cell cols="2">Dataset</cell><cell>Sentence 1</cell><cell>Sentence 2</cell><cell>/5</cell></row><row><cell></cell><cell cols="2">News Mexico wishes to guarantee citizens' safety.</cell><cell>Mexico wishes to avoid more violence.</cell><cell>4</cell></row><row><cell></cell><cell cols="2">Forum The problem is simpler than that.</cell><cell>The problem is simple.</cell><cell>3.8</cell></row><row><cell cols="3">STS WordNet A social set or clique of friends.</cell><cell cols="2">An unofficial association of people or groups. 3.6</cell></row><row><cell>2014</cell><cell cols="3">Twitter Taking Aim #Stopgunviolence #Congress #NRA Obama, Gun Policy and the N.R.A.</cell><cell>1.6</cell></row><row><cell></cell><cell cols="2">Images A woman riding a brown horse.</cell><cell>A young girl riding a brown horse.</cell><cell>4.4</cell></row><row><cell></cell><cell cols="2">Headlines Iranians Vote in Presidential Election.</cell><cell>Keita Wins Mali Presidential Election.</cell><cell>0.4</cell></row><row><cell cols="3">SICK (test+train) A lone biker is jumping in the air.</cell><cell>A man is jumping into a full pool.</cell><cell>1.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Performance</figDesc><table><row><cell>Data</cell><cell>Model</cell><cell cols="2">MSRP (Acc / F1) MR</cell><cell>CR</cell><cell cols="3">SUBJ MPQA TREC</cell></row><row><cell></cell><cell>SAE</cell><cell>74.3 / 81.7</cell><cell cols="2">62.6 68.0</cell><cell>86.1</cell><cell>76.8</cell><cell>80.2</cell></row><row><cell></cell><cell>SAE+embs.</cell><cell>70.6 / 77.9</cell><cell cols="2">73.2 75.3</cell><cell>89.8</cell><cell>86.2</cell><cell>80.4</cell></row><row><cell>Unordered</cell><cell>SDAE</cell><cell>76.4 / 83.4</cell><cell cols="2">67.6 74.0</cell><cell>89.3</cell><cell>81.3</cell><cell>77.6</cell></row><row><cell>Sentences</cell><cell>SDAE+embs.</cell><cell>73.7 / 80.7</cell><cell cols="2">74.6 78.0</cell><cell>90.8</cell><cell>86.9</cell><cell>78.4</cell></row><row><cell cols="2">(Toronto Books: ParagraphVec DBOW</cell><cell>72.9 / 81.1</cell><cell cols="2">60.2 66.9</cell><cell>76.3</cell><cell>70.7</cell><cell>59.4</cell></row><row><cell>70m sents,</cell><cell>ParagraphVec DM</cell><cell>73.6 / 81.9</cell><cell cols="2">61.5 68.6</cell><cell>76.4</cell><cell>78.1</cell><cell>55.8</cell></row><row><cell>0.9B words)</cell><cell>Skipgram</cell><cell>69.3 / 77.2</cell><cell cols="2">73.6 77.3</cell><cell>89.2</cell><cell>85.0</cell><cell>82.2</cell></row><row><cell></cell><cell>CBOW</cell><cell>67.6 / 76.1</cell><cell cols="3">73.6 7730 89.1</cell><cell>85.0</cell><cell>82.2</cell></row><row><cell></cell><cell>Unigram TFIDF</cell><cell>73.6 / 81.7</cell><cell cols="2">73.7 79.2</cell><cell>90.3</cell><cell>82.4</cell><cell>85.0</cell></row><row><cell>Ordered</cell><cell>SkipThought</cell><cell>73.0 / 82.0</cell><cell cols="2">76.5 80.1</cell><cell>93.6</cell><cell>87.1</cell><cell>92.2</cell></row><row><cell>Sentences</cell><cell>FastSent</cell><cell>72.2 / 80.3</cell><cell cols="2">70.8 78.4</cell><cell>88.7</cell><cell>80.6</cell><cell>76.8</cell></row><row><cell cols="2">(Toronto Books) FastSent+AE</cell><cell>71.2 / 79.1</cell><cell cols="2">71.8 76.7</cell><cell>88.8</cell><cell>81.5</cell><cell>80.4</cell></row><row><cell></cell><cell>NMT En to Fr</cell><cell>69.1 / 77.1</cell><cell cols="2">64.7 70.1</cell><cell>84.9</cell><cell>81.5</cell><cell>82.8</cell></row><row><cell>Other</cell><cell>NMT En to De</cell><cell>65.2 / 73.3</cell><cell cols="2">61.0 67.6</cell><cell>78.2</cell><cell>72.9</cell><cell>81.6</cell></row><row><cell>structured</cell><cell>CaptionRep BOW</cell><cell>73.6 / 81.9</cell><cell cols="2">61.9 69.3</cell><cell>77.4</cell><cell>70.8</cell><cell>72.2</cell></row><row><cell>data</cell><cell>CaptionRep RNN</cell><cell>72.6 / 81.1</cell><cell cols="2">55.0 64.9</cell><cell>64.9</cell><cell>71.0</cell><cell>62.4</cell></row><row><cell>resource</cell><cell>DictRep BOW</cell><cell>73.7 / 81.6</cell><cell cols="2">71.3 75.6</cell><cell>86.6</cell><cell>82.5</cell><cell>73.8</cell></row><row><cell></cell><cell>DictRep BOW+embs.</cell><cell>68.4 / 76.8</cell><cell cols="2">76.7 78.7</cell><cell>90.7</cell><cell>87.2</cell><cell>81.0</cell></row><row><cell></cell><cell>DictRep RNN</cell><cell>73.2 / 81.6</cell><cell cols="2">67.8 72.7</cell><cell>81.4</cell><cell>82.5</cell><cell>75.8</cell></row><row><cell></cell><cell>DictRep RNN+embs.</cell><cell>66.8 / 76.0</cell><cell cols="2">72.5 73.5</cell><cell>85.6</cell><cell>85.7</cell><cell>72.0</cell></row><row><cell>2.8B words</cell><cell>CPHRASE</cell><cell>72.2 / 79.6</cell><cell cols="2">75.7 78.8</cell><cell>91.1</cell><cell>86.2</cell><cell>78.8</cell></row></table><note><p>of sentence representation models on supervised evaluations (Section 3.1). Bold numbers indicate best performance in class. Underlined indicates best overall.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Performance of sentence representation models (Spearman/Pearson correlations) on unsupervised (relatedness) evaluations (Section 3.2). Models are grouped according to training data as indicated in Table3.</figDesc><table><row><cell>this may be a more important trait of FastSent than</cell></row><row><cell>the marginal improvement on the SICK task. Read-</cell></row><row><cell>ers can compare the CBOW and FastSent spaces at</cell></row><row><cell>http://45.55.60.98/.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Sample nearest neighbour queries selected from a randomly sampled 0.5m sentences of the Toronto Books Corpus.</figDesc><table><row><cell></cell><cell cols="4">Supervised (combined α = 0.90)</cell><cell></cell><cell></cell><cell>Unsupervised (combined α = 0.93)</cell></row><row><cell>MSRP</cell><cell>MR</cell><cell>CR</cell><cell>SUBJ</cell><cell>MPAQ</cell><cell>TREC</cell><cell>News</cell><cell>Forum WordNet Twitter Images Headlines All STS SICK</cell></row><row><cell cols="8">0.94 (6) 0.85 (1) 0.86 (4) 0.85 (1) 0.86 (3) 0.89 (5) 0.92 (4) 0.92 (3) 0.92 (4) 0.93 (6) 0.95 (8) 0.92 (2) 0.91 (1) 0.93 (7)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>https://radimrehurek.com/gensim/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>We also tried multiplication but this gave very poor results.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>Since code for C-PHRASE is not publiclyavailable we use the available pre-trained model (http://clic.cimec.unitn.it/composes/cphrase-vectors.html). Note this model is trained on 3× more text than others in this study.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3"><p>https://www.cl.cam.ac.uk/ ˜fh295/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4"><p>www.statmt.org/wmt15/translation-task.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5"><p>We searched po, px ∈ {0.1, 0.2, 0.3} and observed best results with po = px = 0.1.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_6"><p>http://www.cs.toronto.edu/ ˜mbweb/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_7"><p>For ParagraphVec only d ∈ {100, 200} was possible due to the high memory footprint.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_8"><p>Downloaded from https://github.com/ryankiros/skip-thoughts</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_9"><p>We make all code for training and evaluating these new models publicly available, together with pre-trained models and an online demo of the FastSent sentence space.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was supported by a <rs type="funder">Google Faculty Award</rs> to AK and FH and a Google European Doctoral Fellowship to FH. Thanks also to <rs type="person">Marek Rei</rs>, <rs type="person">Tamara Polajnar</rs>, <rs type="person">Laural Rimell</rs>, <rs type="person">Jamie Ryan Kiros</rs> and <rs type="person">Piotr Bojanowski</rs> for helpful discussion and comments.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semeval-2014 task 10: Multilingual semantic textual similarity</title>
		<author>
			<persName><surname>Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2014">2014. 2014. SemEval 2014</date>
			<biblScope unit="page" from="81" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Baroni et al.2014b] Marco Baroni, Georgiana Dinu, and Germán Kruszewski. 2014b. Don&apos;t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors</title>
		<author>
			<persName><surname>Almahairi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<editor>
			<persName><surname>Bruni</surname></persName>
		</editor>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2015. 2015. 2014. 2014</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1" to="47" />
		</imprint>
	</monogr>
	<note>Baroni et al.2014a Frege in space: A program of compositional distributional semantics Learning distributed representations from reviews for collaborative filtering Proceedings of the 9th ACM Conference on Recommender Systems Multimodal distributional semantics J. Artif. Intell. Res. (JAIR)</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Chen</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00325</idno>
		<title level="m">Microsoft coco captions: Data collection and evaluation server</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Spring Symposium: Quantum Interaction</title>
		<title level="s">Stephen Clark and Stephen Pulman</title>
		<imprint>
			<date type="published" when="2007">2014. 2014. 2007. 2007</date>
			<biblScope unit="page" from="52" to="55" />
		</imprint>
	</monogr>
	<note>Combining symbolic and distributional models of meaning Proceedings of EMNLP</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence learning</title>
		<author>
			<persName><forename type="first">Le</forename><forename type="middle">;</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="3061" to="3069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources</title>
		<author>
			<persName><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th international conference on Computational Linguistics</title>
		<meeting>the 20th international conference on Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="page">350</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Anna Korhonen, and Yoshua Bengio. 2015a. Learning to understand phrases by embedding the dictionary</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zellig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><surname>Cho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1954">1954</date>
		</imprint>
	</monogr>
	<note>Distributional structure Transactions of the Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><surname>Hill</surname></persName>
		</author>
		<title level="m">Felix Hill, Roi Reichart, and Anna Korhonen. 2015b. Simlex-999: Evaluating semantic models with (genuine) similarity estimation</title>
		<imprint>
			<publisher>Computational Linguistics</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName><forename type="first">Liu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="page" from="168" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep unordered composition rivals syntactic methods for text classification</title>
		<author>
			<persName><surname>Iyyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013">2015. 2015. 2013</date>
			<biblScope unit="page" from="891" to="896" />
		</imprint>
	</monogr>
	<note>Ji and Eisenstein2013 Discriminative improvements to distributional sentence similarity EMNLP</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Antonio Torralba, and Sanja Fidler. 2015. Skip-thought vectors</title>
		<author>
			<persName><surname>Kalchbrenner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="3276" to="3284" />
		</imprint>
	</monogr>
	<note>Kiros et al.2015 A convolutional neural network for modelling sentences Proceedings of EMNLP</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName><forename type="first">Le</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Mikolov</forename><forename type="middle">;</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Omer Levy and Yoav Goldberg</publisher>
			<date type="published" when="2014">2014. 2014. 2014. 2014</date>
			<biblScope unit="page" from="2177" to="2185" />
		</imprint>
	</monogr>
	<note>Neural word embedding as implicit matrix factorization Proceedings of ICML</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Improving distributional similarity with lessons learned from word embeddings</title>
		<author>
			<persName><surname>Levy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="211" to="225" />
		</imprint>
	</monogr>
	<note>Omer Levy, Yoav Goldberg, and Ido Dagan Transactions of the</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A sick cure for the evaluation of compositional distributional semantic models</title>
		<author>
			<persName><surname>Mao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
	</analytic>
	<monogr>
		<title level="m">Efficient estimation of word representations in vector space</title>
		<imprint>
			<date type="published" when="2013">2015. 2015. 2014. 2014. 2013. 2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Deep captioning with multimodal recurrent neural networks (m-rnn) Proceedings of LREC Mikolov et al.2013a Proceedings of ICLR Mikolov et al.2013b Distributed representations of words and phrases and their compositionality Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mitchell and Lapata2008] Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition</title>
		<author>
			<persName><surname>Milajevs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="236" to="244" />
		</imprint>
	</monogr>
	<note>Evaluating neural word representations in tensor-based compositional settings ACL</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Composition in distributional models of semantics</title>
		<author>
			<persName><forename type="first">Lapata</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1388" to="1429" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName><forename type="first">Norman</forename><forename type="middle">;</forename><surname>Donald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Olga Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics (LSDSem)</title>
		<editor>
			<persName><forename type="first">Tamara</forename><surname>Polajnar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Laura</forename><surname>Rimell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</editor>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1972">1972. 2004. 2005. 2005. 2015. 2015. 2014</date>
			<biblScope unit="page" from="1" to="42" />
		</imprint>
	</monogr>
	<note>Pang and Lee2004 Memory, knowledge, and the answering of questions Association for Computational Linguistics A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics Pham et al.2015 Jointly optimizing word representations for lexical and sentential tasks with the c-phrase model Proceedings of ALC Polajnar et al.2015 An exploration of discoursebased sentence spaces for compositional distributional semantics Proceedings of the 42nd annual meeting on Association for Computational Linguistics Russakovsky et al.2014 Imagenet large scale visual recognition challenge</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Building end-to-end dialogue systems using generative hierarchical neural network models</title>
		<author>
			<persName><surname>Serban</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.04808</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011">2015. 2015. 2011</date>
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Socher et al.2011 Semi-supervised recursive autoencoders for predicting sentiment distributions</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generating text with recurrent neural networks</title>
		<author>
			<persName><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML-11)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page" from="1017" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.4842</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2014. 2014. 2008</date>
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Vincent et al.2008 Going deeper with convolutions</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Overview of the trec 2001 question answering track</title>
		<author>
			<persName><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIST special publication</title>
		<imprint>
			<biblScope unit="page" from="42" to="51" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Annotating expressions of opinions and emotions in language</title>
		<author>
			<persName><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language resources and evaluation</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="165" to="210" />
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
