- Choice of unsupervised learning methods for sentence representations
- Selection of evaluation metrics for model performance
- Decision to focus on unlabelled data for training
- Comparison of deep vs shallow models for representation learning
- Introduction of Sequential Denoising Autoencoders (SDAEs)
- Development of FastSent model
- Use of bag-of-words vs neural network architectures
- Implementation of noise functions in SDAEs
- Trade-off considerations between training time and performance
- Domain portability of representation learning methods
- Choice of datasets for model training and evaluation
- Decision to exclude supervised sentence-level architectures
- Adoption of cosine distance for unsupervised evaluation
- Use of dictionary definitions for training representations
- Implementation of TFIDF as a non-distributed baseline
- Consideration of computational resources in model selection