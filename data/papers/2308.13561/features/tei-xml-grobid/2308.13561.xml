<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Project Aria: A New Tool for Egocentric Multi-Modal AI Research</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-10-01">1 Oct 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jakob</forename><surname>Engel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kiran</forename><surname>Somasundaram</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Goesele</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Albert</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexander</forename><surname>Gamino</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andrew</forename><surname>Turner</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Arjang</forename><surname>Talattof</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Arnie</forename><surname>Yuan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bilal</forename><surname>Souti</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Brighid</forename><surname>Meredith</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Cheng</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chris</forename><surname>Sweeney</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Cole</forename><surname>Wilson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dan</forename><surname>Barnes</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Detone</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Caruso</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Derek</forename><surname>Valleroy</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dinesh</forename><surname>Ginjupalli</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Duncan</forename><surname>Frost</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Edward</forename><surname>Miller</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Elias</forename><surname>Mueggler</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Evgeniy</forename><surname>Oleinik</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guruprasad</forename><surname>Somasundaram</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gustavo</forename><surname>Solaira</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Harry</forename><surname>Lanaras</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Henry</forename><surname>Howard-Jenkins</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huixuan</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hyo</forename><forename type="middle">Jin</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jaime</forename><surname>Rivera</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ji</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jing</forename><surname>Dong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Julian</forename><surname>Straub</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kevin</forename><surname>Bailey</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kevin</forename><surname>Eckenhoff</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lingni</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Luis</forename><surname>Pesqueira</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mark</forename><surname>Schwesinger</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Maurizio</forename><surname>Monge</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nick</forename><surname>Charron</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nikhil</forename><surname>Raina</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Omkar</forename><surname>Parkhi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peter</forename><surname>Borschowa</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pierre</forename><surname>Moulon</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Prince</forename><surname>Gupta</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Raul</forename><surname>Mur-Artal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Robbie</forename><surname>Pennington</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sachin</forename><surname>Kulkarni</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sagar</forename><surname>Miglani</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Santosh</forename><surname>Gondi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Saransh</forename><surname>Solanki</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sean</forename><surname>Diener</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shangyi</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Simon</forename><surname>Green</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Steve</forename><surname>Saarinen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Suvam</forename><surname>Patra</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tassos</forename><surname>Mourikis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><surname>Whelan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tripti</forename><surname>Singh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vasileios</forename><surname>Balntas</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vijay</forename><surname>Baiyya</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wilson</forename><surname>Dreewes</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaqing</forename><surname>Pan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Lou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yipu</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yusuf</forename><surname>Mansour</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuyang</forename><surname>Zou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhaoyang</forename><surname>Lv</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zijian</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mingfei</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Carl</forename><surname>Ren</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Renzo</forename><surname>De Nardi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Richard</forename><surname>Newcombe</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Project Aria: A New Tool for Egocentric Multi-Modal AI Research</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-10-01">1 Oct 2023</date>
						</imprint>
					</monogr>
					<idno type="MD5">1CAA4696E114D5D3AA6AF4BDB367EDEA</idno>
					<idno type="arXiv">arXiv:2308.13561v3[cs.HC]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Egocentric, multi-modal data as available on future augmented reality (AR) devices provides unique challenges and opportunities for machine perception. These future devices will need to be all-day wearable in a socially acceptable form-factor to support always available, context-aware and personalized AI applications. Our team at Meta Reality Labs Research built the Project Aria device: An egocentric, multi-modal data recording and streaming device with the goal to foster and accelerate research in this area. In this paper, we describe the Project Aria device hardware including its sensor configuration, the corresponding software tools, and the available machine perception functionalities that make it the ideal tool for egocentric machine perception and contextual AI research.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>All-day wearable AR glasses promise to be the next big paradigm shift in computing: They can lift interaction with the digital world from 2D screens into the 3D world around us; blending seamlessly into our lives as opposed to diverting our attention into small 2D rectangles held in our hands. Yet, in order to be more than mere 3D versions of 2D screens, they also require a new compute-and interactionparadigm that is context-aware, highly personalized and natural to interact with. Creating this new paradigm is a significant challenge that still requires a broad range of re- search to be solved. Fortunately, the recent breakthrough of internet-trained Large Language Models such as GPT4 <ref type="bibr" target="#b10">[15]</ref> and llama2 <ref type="bibr" target="#b2">[7]</ref> promise to solve a major part of this challenge, making it a lot more tractable: They demonstrate that modern Transformer architectures combined with sufficient training data enable both long-range reasoning and information retrieval while also adopting seamlessly to new tasks using a local context window and prompt engineering. Taken together, this enables human-level interaction with digital agents that blend into how we humans interact naturally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Project Aria</head><p>However, we believe that this paradigm shift in personalized computing requires another crucial ingredient: the ability for AI Agents to adapt to the unique, personal context and preferences of the individual user. Some of this can be achieved by leveraging a persons digital footprint -the aggregate of interactions with the internet through phones and laptops. It is only a matter of time for such personalized AI Assistants to emerge. Yet, our digital footprint only represents a small fraction of the experiences that matter to us, and rarely contain the most important ones, which play out in the real world, are highly personal and often take place in unpredictable places and at unpredictable times.</p><p>Importantly, this applies not only on an individual level, but also in aggregate: The wealth of digital data accumulated over 25 years in the digital realm only represents a small -and often severely biased -fraction of the sum total of human experience. For example, the overwhelming majority of images available on the web and used to train AI models were consciously captured with handheld devices and curated before upload. Any outtakes, any data that is not deemed interesting is typically deleted or edited although it arguably represents the majority of situations we encounter in our daily life.</p><p>A direct consequence of this is that many state of the art methods in the space of Machine Perception and AI (DALL•E2 <ref type="bibr" target="#b14">[19]</ref>, GPT-4 <ref type="bibr" target="#b10">[15]</ref>, DreamFusion <ref type="bibr" target="#b12">[17]</ref>, SAM <ref type="bibr" target="#b6">[11]</ref>, MaskRCNN <ref type="bibr" target="#b5">[10]</ref>, CLIP <ref type="bibr" target="#b13">[18]</ref>, DINOv2 <ref type="bibr" target="#b11">[16]</ref>) excel when applied to allocentric 2D images and viewpoints, but fare comparatively poorly at tasks that involve egocentric data or require structured reasoning and understanding in 3D/4D space. While this is partially due to the increase in data/compute requirements, it is also a direct result of the aforementioned shortcomings of data typically available through web platforms.</p><p>In order to address this gap and to enable the next big paradigm shift towards context-aware, personalized, and human-oriented AI, we have created Project Aria: At its core, the Project Aria device is a data-capture system in glasses form factor that is sufficiently light and unobtrusive to be worn for long time-spans without inhibiting natural activities and behavior (see Figure <ref type="figure" target="#fig_0">1</ref>), allowing to capture ecologically valid data. The device features a rich multimodal sensor suite that approximates what can be expected in future AR glasses for the purpose of environment-and user-understanding. The onboard battery allows the device to record 1-2 hours of data (with the nominal recording profile). Much longer recordings are possible with an external power bank.</p><p>In this technical report, we introduce the Project Aria device as well as the software and Machine Perception Services that come with it. We make these available to research institutions around the world to foster advancements in the field of egocentric perception towards personalized AI. We also summarize the principles and standards that we have established and adopted to protect the privacy of both wearers and bystanders in accordance with Meta's Responsible Innovation Principles <ref type="bibr" target="#b7">[12]</ref>.</p><p>Since its launch in 2020, Project Aria devices have been</p><p>used by research groups in the USA, UK, Switzerland, India, Canada, Singapore, Colombia and Japan. In 2022, we released the Aria Pilot Dataset [2]. To find out more about Project Aria and how to become a research partner, please visit our website [4].</p><p>This report is organized as follows: Section 2 introduces the Project Aria device and its capabilities. Section 3 introduces the software and tools required for recording and using recordings for research. Section 4 enumerates the set of first-level machine perception capabilities we offer through a web service in order to accelerate and simplify use of Project Aria data. Section 5 details the privacy and responsible innovation principles we have established, and how the Project Aria device implements them. Section 6 gives a set of example research applications that are enabled by Project Aria. We end this paper with a conclusion in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Device</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Sensor Suite</head><p>We built the Project Aria device to emulate future wearable devices catering to machine perception rather than human consumption. To mimic the sensor stack needed for machine perception capabilities on these future devices, we integrate a rich suite of sensors that record egocentric multimodal data (see Figures <ref type="figure" target="#fig_1">2</ref><ref type="figure" target="#fig_2">3</ref><ref type="figure" target="#fig_3">4</ref><ref type="figure">5</ref>).</p><p>Sensor streams are tightly calibrated and time-aligned to make some problems fundamentally easier to solve. This is aligned to future expectations of wearable device hardware, but introduces new challenges -such as the lack of Optical Image Stabilization (OIS) or Auto-Focus (AF).</p><p>To satisfy the strict requirements for capturing representative data in a wearable form-factor constraints, we built the Project Aria device as a data-collection and streaming device. Specifically, it is not designed to handle on-device computation workloads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Form Factor and Fit</head><p>The Project Aria device is designed to contain a rich set of sensors while being light (around 75g) and socially acceptable. The devices provide a good fit for a broad range of the population with two device sizes. They come with adjustable nose pads and temples (i.e. the temple tips can be bent inwards or outwards to improve fit). The glasses temple have a lot more flexibility than a conventional pair of glasses, which means the small size can stretch up to what many glasses might call a medium or large size.</p><p>The sensors on the Project Aria device have been chosen to (1) approximate what we expect to be available on future all-day-wearable AR glasses, and (2) fit into the strict size, weight and power envelope required by the target formfactor to obtain ecologically valid data. We selected a broad range of sensors beyond just cameras as we believe multi- modal egocentric data to be key to solve various Machine Perception and AI tasks -see Section 6 for some examples. The following gives an overview of what sensors are available on a Project Aria device (see also Figure <ref type="figure" target="#fig_5">6</ref>):</p><p>• Mono Scene Cameras: Two monochrome, globalshutter cameras on the left and right side of the glasses. They have a horizontal field of view (HFOV) of 150°a nd are angled outwards to maximize peripheral vision while allowing for some stereo overlap. These cameras are used for supporting machine perception capabilities such as Visual SLAM <ref type="bibr" target="#b1">[6,</ref><ref type="bibr" target="#b8">13,</ref><ref type="bibr" target="#b9">14]</ref>. They have a resolution of 640x480 pixels and use Fisheye (F-Theta) lenses.</p><p>• Point of View (POV) RGB Camera: A single highresolution, rolling-shutter RGB camera on the left side of the glasses with a HFOV of 110°. The RGB camera also uses a F-Theta Fisheye lens, has a maximum resolution of 2880x2880 pixels and faces forward with an approximately 4°bias towards the ground.</p><p>• Eye Tracking Cameras: Two monochrome, globalshutter, inward-facing cameras for eye tracking with a diagonal field of view (DFOV) of 80°. They typically operate at a 320x240 pixels resolution.</p><p>• IMUs: Two inertial measurement units (IMU), one on each side of the glasses. The left IMU is configured to sample at 800 Hz with saturation limits of 4g (accelerometer) and 500°/s (gyroscope). The right IMU samples at 1000 Hz with saturation limits of 8g and 1000°/s. We intentionally chose different IMU models so that their higher-order error behaviors are more likely to be uncorrelated.</p><p>• Microphones: A microphone array comprised of 7 microphones distributed around the glasses (5 front, 1 on each side), allowing to capture spatial audio at 24 bits with a configurable sample rate of up to 48 kHz. • Magnetometer: A magnetometer located on the rim of the glasses to minimize electromagnetic interference, measuring the ambient magnetic field (3-axis) with a resolution of 0.1 µT and a sample rate of 10 Hz.</p><p>• Barometer &amp; Thermometer: A barometer sensor capturing local air pressure and temperature at a resolution of 0.66 Pa and 0.005°C, respectively, with a sample rate of 50 Hz.</p><p>• GNSS receiver: A global navigation satellite system receiver supporting GPS and Galileo constellations, providing pseudo-range measurements as well as lat/long/height solutions with a sample rate of 1 Hz.</p><p>• Wi-Fi &amp; Bluetooth transceiver: A Wi-Fi and Bluetooth radio with the ability to regularly scan and record received signal strengths (RSSI) from Wi-Fi beacon frames (both 2.4G and 5G) and from Bluetooth beacons. Scanning is nominally performed at 0.1 Hz.</p><p>Many of these sensor settings can be configured through recording profiles that are selected at the start of a recording and allow to modify camera frame-rate and resolution, as well as to enable or disable different sensor streams. This is important as recording all sensors at maximum resolution and rate is not feasible due to power and bandwidth limitations. Furthermore, disabling sensor streams or reducing their rate and/or resolution can be desirable from a privacy perspective, and is an effective strategy to prolong maximum recording time. Available recording profiles are listed in the Project Aria documentation site [1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Mounting and Rigidity</head><p>Precise 6DoF alignment -that is, relative positioning and orientation between all sensors -is important for many basic machine perception algorithms, and the Project Aria device has been designed to facilitate this. All sensors are mounted onto a magnesium frame spanning the front of the glasses. With the most non-rigid portion being the nosebridge, there are two primary sensor clusters on the left and right side of the glasses. Sensors within the same cluster have a strong rigid connection.</p><p>We provide extrinsic and intrinsic calibration parameters for each sensor computed at manufacturing time (factory calibration). Through our Machine Perception Services (MPS, see Section 4) we additionally make more accurate online-calibration parameters available that account for the small deformations/changes that might occur while wearing the glasses. Please refer to [1] for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Time and Time-alignment</head><p>The Project Aria device is designed to allow accurate timestamping of all sensor data with respect to a local on-</p><p>200 250 300 350 timestamps (s) 47.6866 47.6868 47.6870 47.6872 47.6874 47.6876 latitude (DMS) 200 250 300 350 timestamps (s) 122.1514 122.1512 122.1510 122.1508 122.1506 122.1504 122.1502 122.1500 longitude (DMS) 200 250 300 350 timestamps (s) device time source. This is essential to combine different modalities in downstream machine perception tasks.</p><p>In addition, Project Aria devices provide the ability to align and convert local timestamps to a time domain shared across multiple devices. Accurately translating to a common time domain is critical when combining or comparing data from different sources and devices. In our sample datasets, we use SMPTE LTC timecode <ref type="bibr" target="#b15">[20]</ref> to provide a common accurate time domain across multiple Project Aria devices (see Aria Pilot Dataset [2]). For situations where lower accuracy is acceptable we have also implemented a methodology for sharing a common time domain over Wi-Fi leveraging the TicSync timing protocol <ref type="bibr" target="#b4">[9]</ref>. For both methodologies, the inner working of the time sharing mechanism is handled at the device level. This means, from the perspective of a user of the data, every sensor reading simply includes a timestamp in the aligned time domain in addition to a timestamp in the local device time. Please refer to the Project Aria documentation [1] for more details on the exact definition and conventions for timestamps of the different sensor modalities.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Recording Tools</head><p>The primary interface to interact with Project Aria devices is a mobile phone companion app. Recordings can be initiated and stopped via the app or the device's capture button. The app is also used to set the sensor configuration by selecting a recording profile. The profile controls which sensors record, at what frequency and resolution, as well as the output format (e.g., storing images in RAW or JPEG format). Multiple recording profiles are available to tailor for different research use cases. Additional profiles are being added as necessary. Once a recording is made on the device, thumbnail previews are available on the companion app for convenient review of the captured data. These functionalities are illustrated in Figure <ref type="figure" target="#fig_6">7</ref>.</p><p>Once the recording is complete, the user can download the recorded data from a Project Aria device via an USB connection to a local machine for further processing. A user can optionally upload their recordings to our Machine Perception Services (MPS), which apply state-of-the-art processing to recover device trajectories, online calibration, a semi-dense point cloud and eye gaze information (see Section 4 for details).</p><p>All device sensors are recorded in VRS file format <ref type="bibr">[22]</ref>. We selected VRS as the data container because it is an open file format designed to record and playback streams of AR sensor data and because it supports very large file sizes. The VRS files contain streams of time-sorted records generated for each sensor, with one set of sensors per stream.</p><p>In order to easily visualize and interact with data, we provide Project Aria tools as part of an open source repository <ref type="bibr" target="#b0">[3]</ref>. This is a set of tools and libraries for accessing, visualizing and manipulating recordings from Aria. The C++/Python toolkit includes VRS Data Provider and Viewer interfaces, enabling researchers to read and visualize Project Aria sequences, to retrieve and interact with device calibration data, and to read and process the output of the MPS. More details about these tools are available from documentation website [1]. The Project Aria tools are available to install from PyPI via pip install projectaria tools (see <ref type="bibr" target="#b0">[3]</ref> for more details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Machine Perception Services</head><p>We provide a range of foundational machine perception capabilities upon which research partners can build their projects. We expect these capabilities to be provided in a similar form on any future AR device.</p><p>These capabilities are exposed as Machine Perception Services (MPS) enabled by a set of proprietary algorithms that are designed for Project Aria devices and provide superior accuracy and robustness on the recorded data compared to current off-the-shelf open source solutions.</p><p>MPS is provided by post-processing VRS recordings on Meta's backend servers. To use the service, Project Aria research partners upload the recording, and, later on, can download the results. Furthermore, we include MPS output in public Aria-based datasets making it available to the broader community.</p><p>Please refer to the Project Aria documentation site [1] for more information about Aria MPS, the output format and their specifications, and an overview over the tooling available to visualize and make use of the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Trajectories</head><p>A highly accurate 6-DoF device trajectory is the foundation to understand the geometric relation of the device and its wearer to the environment. Device trajectories are generated by a state-of-the-art VIO and SLAM system -similar to what can be expected on future AR and VR HMD'sfollowed by offline post-processing and refinement. We use multiple of the available sensors (including cameras, IMUs, GNSS, Wi-Fi, and barometer) to improve accuracy and robustness, and further take advantage of precise knowledge of the sensor models, timing, and rigidity of Project Aria devices. This allows us to robustly localize the device even under the often challenging conditions that occur with realworld data -such as fast motion, low or highly dynamic lighting, partial or temporary occlusion of the cameras, as well as a wide range of static and dynamic environments.</p><p>We provide two types of trajectories as output, open loop and closed loop. The open loop trajectory is a high frequency (1 kHz) odometry estimation, computed strictly causally with a real-time-compatible method. The accumulated translation drift of this open loop trajectory is no more than 0.4% of the distance traveled, and usually significantly less. The closed loop trajectory is a 1 kHz trajectory estimated in post-processing. It is fully optimized and provides poses in a single frame of reference. We also provide </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Online Calibration</head><p>Accurate device calibration is essential to enable high geometric accuracy for downstream 3D perception tasks. Even though Project Aria devices are built to be as rigid as possible, device calibration parameters are not perfectly constant over time due to temperature changes, aging, and external forces applied to the device. Figure <ref type="figure" target="#fig_8">9</ref> shows an example where taking off the device after wearing causes around 25 arcmin instantaneous rotational deformation between the left and right Mono Scene cameras (corresponding to roughly 1.5 pixel shift in the images). To account for this, we estimate the time-varying intrinsic and extrinsic calibrations of cameras and IMUs as part of MPS, and make the result available to researchers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Semi-Dense Point Cloud</head><p>To provide an intuitive understanding of the environment a recording was taken in, we compute semi-dense tracks and point clouds as part of MPS; see Figure <ref type="figure" target="#fig_7">8</ref> for an example. Similar to the odometry trajectory, these tracks are computed causally and provide an accurate -though partial -reconstruction of the static portion of the environment. We also provide the sets of all 2D observations that were used to triangulate each 3D point. Tracks are obtained by continuously spawning new points in images-regions with high gradient, and tracking these over time and across the left/right Mono Scene camera using affine-invariant photoconsistency of local patches. Finally, the 3D point clouds are post-processed and placed into the global frame of reference that is defined by the closed loop trajectories. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Eye Gaze Tracking</head><p>Gaze direction is an important indicator of a wearer's attention, and will likely be one of the crucial inputs to context-aware, personalized AI agents. We compute and provide eye gaze from the Project Aria device eye tracking cameras, estimating a single per-frame 3D ray anchored to the central pupil frame, also called a cyclopean eye frame<ref type="foot" target="#foot_0">foot_0</ref> . We also provide confidence intervals, as eye tracking accuracy can vary by situation and user.</p><p>Furthermore, the Aria companion app described in Section 3 implements the option to capture personalized eye calibration -this allows to improve the accuracy of eye gaze tracking by compensating for user-specific biases. With the current model, we observe a median gaze ray error of 1.5°a fter applying the personalized calibration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Privacy Considerations</head><p>AR glasses and in general egocentric recording devices such as Project Aria devices promise to make technology more accessible, but also pose novel and unique challenges for security and privacy: the more they succeed in being unobtrusive, the more important it becomes to preserve and respect the privacy not only of the wearer, but also that of bystanders and individuals the wearer interacts with.</p><p>A stated goal of Project Aria is to pioneer responsible innovation for research leveraging egocentric data and devices, both by establishing guidelines and principles to preserve privacy of wearers and bystanders as well as by building privacy-facilitating features directly into the device where possible.</p><p>Throughout the development of Project Aria we followed Meta's Responsible Innovation Principles <ref type="bibr" target="#b7">[12]</ref>, which express our commitment to building inclusive, privacy-centric products.</p><p>In concert with our principles we have designed and made available privacy-centric hardware and software features to research partners. The Project Aria device has an LED indicator that signals to bystanders when the device is recording raw data. Furthermore, Project Aria devices have a privacy switch: When activated during a recording session, the device immediately stops and deletes the current recording. This allows a wearer to immediately and easily fulfill a request of a bystander to delete any audio or video recording that might have been captured of them.</p><p>We require all Project Aria partners to follow the Project Aria community guidance [5] and to practice responsible research, protecting the privacy of those who wear the research devices, and most importantly, those who do not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Example Research Applications</head><p>This section provides a brief overview of research tasks that leverage Project Aria's unique features, from lowlevel machine perception functions to high-level user-and environment-understanding. Project Aria is designed to enable and connect research across this spectrum: While the former benefits from well-calibrated and understood sensors and access to raw data, the latter can leverage the multiple modalities available or build upon the machine perception functions provided by Aria Machine Perception Services (MPS). Note that this is neither an exhaustive review of the respective fields nor a complete list of tasks that are enabled by Project Aria. It is meant to provide examples how Project Aria devices or Aria data can be leveraged, with an emphasis on Aria's unique combination of form-factor, sensors and Machine Perception Services.</p><p>Figure <ref type="figure" target="#fig_1">12</ref>. Two NeRF reconstructions obtained from Aria recordings using NerfStudio <ref type="bibr" target="#b16">[21]</ref>. The left shows the result from a carefully curated, hand-held recording that covers the space well and avoids rapid motion. The right shows the result from an egocentric recording during a natural activity. The bottom figures visualize the raw pointcloud and trajectory from MPS. The resulting difference in quality is clearly visible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Life-long Mapping and Re-localization</head><p>Precise 6DoF localization through SLAM or SfM is a common first step across many applications, as well as a base requirement for AR/VR world-locked rendering. It is a comparatively mature field, and Project Aria provides metric 6DoF trajectories as part of Aria MPS (see Sec. 4.1). However, many challenges remain: One of these is to reliably re-localize across strong environment changes that occur in natural environments (typically due to lighting, weather, or human activity), as well as updating maps with such changes over time. Figure <ref type="figure" target="#fig_10">11</ref> shows a map created from 275 Aria recordings, captured over 15 months, in a building that's being built: Using Aria as convenient recording device and building upon the per-recording trajectories from Aria MPS allows to focus on the core problem of longrange re-localization and map-updating under such strong environmental changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Egocentric Scene Reconstruction and Understanding</head><p>Reconstructing the surrounding scene and semantically identifying objects in it is a key problem for AR/VR applications -from creating photo-realistic, virtual memories to identifying affordances of objects in the surroundings as part of a context-aware AI assistant. This becomes particularly challenging when the input data is not neatly curated or intentionally taken for this purpose, but rather stems from a form-factor and power-constrained wearable device undergoing natural, unconstrained human motion.</p><p>Figure <ref type="figure" target="#fig_1">12</ref> shows the results obtained by state-of-the-art methods for NeRF reconstruction <ref type="bibr" target="#b16">[21]</ref> on Aria data, comparing careful "scanning" motion with a natural activity.</p><p>Figure <ref type="figure" target="#fig_2">13</ref>. We use UmeTrack <ref type="bibr" target="#b3">[8]</ref> to track the articulated 3D handpose of the wearer in an Aria recording. Combined with the pointcloud from MPS, this allows to identify when the wearer touches a static object. The figures visualize this by coloring points within 8 cm of the hand in red.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Object Interaction and Manipulation</head><p>Recognizing or tracking objects the user is interacting with, or identifying how the user is interacting with them, is another core egocentric machine perception task. It combines hand-tracking with object tracking, recognition, or scene understanding to connect things with user intent or actions. Figure <ref type="figure" target="#fig_2">13</ref> shows an example application that identifies when the user's hand is near an object in the scene. The approach uses all 3 cameras as well as the trajectory and pointcloud provided by MPS -which can help in particular to resolve the otherwise common scale/depth ambiguity. Figure <ref type="figure" target="#fig_9">10</ref> showed a similar approach using eye gaze to identify what the wearer is looking at.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Activity Recognition and Attention</head><p>Identifying what the wearer is doing or paying attention to is another likely component of any contextual AI assistant. While much information can be derived from egocentric images or videos alone, significant additional signal can be derived from other modalities, including spatial audio, motion, or eye gaze. Figure <ref type="figure" target="#fig_3">14</ref> shows two example situations where these additional signals allow to disambiguate what the wearer is doing in otherwise ambiguous egocentric views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">Summarization and Question Answering</head><p>Summarization and Question Answering goes a step further than activity recognition, aiming to summarize relevant events and activities that occur over longer time periods and allowing to answer questions about them. Note that "relevance" in this context is highly subjective and personal, making signals such as eye gaze or spatial audio key to select the most relevant information for the user.</p><p>Furthermore, "longer time periods" can vary from a few minutes to hours, days, and years -with longer time-spans becoming increasingly important towards personalized AI Figure <ref type="figure" target="#fig_3">14</ref>. Left: two egocentric views of the wearer interacting with a guitar. The audio stream visualized below allows to disambiguate whether the wearer is actively playing or just holding the guitar. Right: the eye gaze (visualized as a heatmap) allows to distinguish whether the wearer is looking at the time or reading a book.</p><p>assistants, but requiring datasets that do not currently exist. We believe that Project Aria's unique combination of formfactor and machine perception capabilities enables more research in this field towards egocentric, longitudinal summarization and question answering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>With Project Aria, we introduce a new tool for the research community that can capture ecologically valid data as we expect it to be captured by future egocentric devices. We also make available a set of spatial AI machine perception technologies as a foundational building block for higher-level contextualized AI applications. The data, and derived machine perception results, can provide the foundation for building novel compute and interaction paradigms needed in order to make AR successful and the rich integrated sensor suite provides unique opportunities to explore novel research, applications and use cases in a wide range of areas towards always-on contextualized AI.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. The Project Aria device.</figDesc><graphic coords="1,308.86,357.32,236.25,89.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Example images from the Project Aria device cameras. Left to right: left Mono Scene camera, POV (RGB) camera, right Mono Scene camera, two Eye-tracking cameras. Output of POV (RGB) and Mono Scene cameras are rotated for visualization.</figDesc><graphic coords="3,50.11,72.00,495.00,151.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Example time-series data from the multi-channel microphone array on the Project Aria device. Audio data is saved in 32-bit format and normalized in the range of [-1, 1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Example data from various motion sensor and location signal data. Top to bottom: accelerometer and gyroscope data provided by the IMUs, and magnetic field measurements provided by the magnetometer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>3 Figure 5 .</head><label>35</label><figDesc>Figure 5. Illustration of the GNSS and Wi-Fi/Bluetooth sensor data. Top to bottom: GNSS signal (individual plots of latitude, longitude, altitude), pressure measurements provided by the barometer, signal strengths from different sources as recorded by the Wi-Fi and Bluetooth receivers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Project Aria device hardware overview of the components, the various sensors, switches, LEDs, battery, etc.</figDesc><graphic coords="5,62.49,74.70,470.25,323.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Basic functionalities of the mobile phone companion app. From left to right, the screenshots illustrate a) the device status information, b) the recording profile that configures the sensors before recording, c) status of the on-going recording, d) thumbnail preview of the recordings visible after a recording finishes</figDesc><graphic coords="5,427.59,434.63,117.48,230.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Closed-loop trajectories and semi-dense point clouds of 18 recordings of Aria Pilot Dataset [2] collected in the same home space.</figDesc><graphic coords="6,310.04,72.50,233.89,156.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Example of rotational deformation between left and right Mono Scene cameras, estimated by MPS. The recording used for this figure contains 3 sections: hand-held motion, no motion, and head-worn motion. The effect of external force applied to the glasses when worn on the head is clearly visible.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Eye gaze computed on a recording in which the user is looking at the food on their plate. Left: The RGB image with semi-dense points that are close to the gaze ray projected as green dots. Right: The Project Aria device pose (shown as RGB camera frustum), semi-dense points, and eye gaze ray. Semi-dense points close to the gaze ray are highlighted in red.</figDesc><graphic coords="7,308.86,72.50,105.00,104.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Top: A 3D map created from 275 Aria recordings (175 hours of data), captured over 15 months in a construction site, showing -from left to right -the state of the map after one, six, and fifteen months. The images at the bottom are samples from the respective points in time, depicting the transformation from an empty lot to an office building.</figDesc><graphic coords="8,309.36,141.76,115.76,71.24" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The central pupil frame origin is defined at the midpoint between the left and right pupils.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>Project Aria was made possible by the contributions of the Project Aria team from <rs type="affiliation">Meta Reality Labs Research</rs>. We are indebted to the complete team and all partners of Project Aria who enabled its inception and continue to develop the platform.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="https://github.com/facebookresearch/projectariatools.6" />
		<title level="m">Project Aria Tools on GitHub</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">LSD-SLAM: Large-scale direct monocular SLAM</title>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Schöps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014: 13th European Conference</title>
		<meeting><address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">September 6-12, 2014. 2014</date>
			<biblScope unit="page" from="834" to="849" />
		</imprint>
	</monogr>
	<note>Proceedings, Part II 13</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Llama 2: Open foundation and fine-tuned chat models</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Et</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Umetrack: Unified multi-view end-to-end hand tracking for VR</title>
		<author>
			<persName><forename type="first">Shangchen</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Po-Chen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yubo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beibei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiguang</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Hodan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Randi</forename><surname>Cabezas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muzaffer</forename><surname>Akbay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsz-Ho</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cem</forename><surname>Keskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH Asia 2022 Conference Papers</title>
		<meeting><address><addrLine>Daegu, Republic of Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">December 6-9, 2022, 2022. 9</date>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">TICSync: Knowing when things happened</title>
		<author>
			<persName><forename type="first">Alastair</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="356" to="363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R-Cnn</forename><surname>Mask</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Mintun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhila</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanzi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chloe</forename><surname>Rolland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Gustafson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Spencer</forename><surname>Whitehead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.02643</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">Segment anything. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<ptr target="https://about.meta.com/metaverse/responsible-innovation/.2,7" />
		<title level="m">Meta Responsible Innovation Principles</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A multi-state constraint Kalman filter for vision-aided inertial navigation</title>
		<author>
			<persName><forename type="first">I</forename><surname>Anastasios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stergios</forename><forename type="middle">I</forename><surname>Mourikis</surname></persName>
		</author>
		<author>
			<persName><surname>Roumeliotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 2007 IEEE International Conference on Robotics and Automation</title>
		<meeting>2007 IEEE International Conference on Robotics and Automation</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="3565" to="3572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ORB-SLAM2: An open-source slam system for monocular, stereo, and RBG-D cameras</title>
		<author>
			<persName><forename type="first">Raul</forename><surname>Mur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Artal</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">D</forename><surname>Tardós</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1255" to="1262" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<idno type="arXiv">arXiv:2303.08774</idno>
		<title level="m">OpenAI. GPT-4 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Maxime</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothée</forename><surname>Darcet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Théo</forename><surname>Moutakanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huy</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Szafraniec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vasil</forename><surname>Khalidov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Haziza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.07193</idno>
		<title level="m">Learning robust visual features without supervision</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<idno>arXiv, 2022. 2</idno>
		<title level="m">DreamFusion: Text-to-3D using 2D diffusion</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<idno>PMLR, 2021. 2</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Hierarchical textconditional image generation with clip latents</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Casey</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.06125</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Linear</forename><surname>Timecode</surname></persName>
		</author>
		<ptr target="https://en.wikipedia.org/wiki/Lineartimecode.4" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Nerfstudio: A modular framework for neural radiance field development</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evonne</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruilong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brent</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Kerr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terrance</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kristoffersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamyar</forename><surname>Salahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhik</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Mcallister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2023 Conference Proceedings, SIGGRAPH &apos;23</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
