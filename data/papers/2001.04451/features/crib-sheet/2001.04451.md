- **Reformer Model Overview**: Introduces two key techniques to enhance Transformer efficiency: locality-sensitive hashing (LSH) for attention and reversible residual layers.
  
- **Complexity Reduction**: 
  - Standard attention complexity: \(O(L^2)\)
  - LSH attention complexity: \(O(L \log L)\)

- **Memory Efficiency**: 
  - Reversible layers allow storing activations once instead of \(N\) times (where \(N\) is the number of layers).
  - Memory usage for activations in a model with \(N\) layers is reduced significantly.

- **Attention Mechanism**:
  - Standard attention: 
    \[
    Attention(Q, K, V) = softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)V
    \]
  - LSH attention focuses on nearest neighbors using hashing to reduce computational load.

- **Locality-Sensitive Hashing (LSH)**:
  - Assigns similar vectors to the same hash bucket with high probability.
  - Uses random projections to create hash functions.

- **Hashing Attention Equation**:
  - For a single query \(i\):
    \[
    o_i = \sum_{j \in P_i} \exp(q_i \cdot k_j - z(i, P_i)) v_j
    \]
  - \(P_i\) is the set of keys attended to by query \(i\).

- **Multi-Round LSH Attention**:
  - Reduces the probability of similar items falling into different buckets by using multiple hash functions:
    \[
    P_i = \bigcup_{r=1}^{n \text{ rounds}} P^{(r)}_i
    \]

- **Causal Masking**: 
  - Prevents future token attention in decoder models, modified to avoid self-attention in shared-QK configurations.

- **Experimental Validation**:
  - Tested on synthetic tasks and real-world tasks (e.g., enwik8, imagenet-64).
  - Results show Reformer matches Transformer performance while being faster and more memory-efficient.

- **Key Findings**:
  - LSH attention has negligible impact on training dynamics.
  - Reversible layers and chunked feed-forward processing significantly reduce memory usage without sacrificing performance.

- **Memory Usage Calculation**:
  - For a sequence length of 64K, the naive attention matrix requires 16GB of memory, impractical for single-GPU setups.

- **Implementation Notes**:
  - Shared Q and K layers do not affect performance.
  - Efficient batching strategies are employed to handle uneven bucket sizes in LSH attention.

- **Figures and Diagrams**:
  - **Figure 1**: Illustrates angular locality-sensitive hashing.
  - **Figure 2**: Compares full attention with hashed attention, showing the clustering of similar items in buckets.