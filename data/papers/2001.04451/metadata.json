{
  "arxivId": "2001.04451",
  "title": "Reformer: The Efficient Transformer",
  "authors": "Nikita Kitaev, \u0141ukasz Kaiser, Anselm Levskaya",
  "abstract": "Large Transformer models routinely achieve state-of-the-art results on a\nnumber of tasks but training these models can be prohibitively costly,\nespecially on long sequences. We introduce two techniques to improve the\nefficiency of Transformers. For one, we replace dot-product attention by one\nthat uses locality-sensitive hashing, changing its complexity from O($L^2$) to\nO($L\\log L$), where $L$ is the length of the sequence. Furthermore, we use\nreversible residual layers instead of the standard residuals, which allows\nstoring activations only once in the training process instead of $N$ times,\nwhere $N$ is the number of layers. The resulting model, the Reformer, performs\non par with Transformer models while being much more memory-efficient and much\nfaster on long sequences.",
  "url": "https://arxiv.org/abs/2001.04451",
  "issue_number": 801,
  "issue_url": "https://github.com/dmarx/papers-feed/issues/801",
  "created_at": "2025-01-05T18:41:03.688230",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 3,
  "last_read": "2025-01-05T20:09:30.079520",
  "last_visited": "2025-01-05T20:08:29.858000+00:00",
  "main_tex_file": null,
  "published_date": "2020-01-13T18:38:28Z",
  "arxiv_tags": [
    "cs.LG",
    "cs.CL",
    "stat.ML"
  ]
}