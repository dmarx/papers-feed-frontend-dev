# TRecViT: A Recurrent Video Transformer

## Abstract

## ViT block

RMSNorm Figure 1. Left: TRecViT architecture. Each video frame is divided into non-overlapping patches that are linearly projected into a token embedding space. We then add a learnt spatial positional encoding. The tokens are passed through gated linear recurrent units (LRUs) that share parameters across space. The outputs of the recurrent blocks are then processed by a ViT block. The recurrent operation followed by ViT is repeated N times. Right: TRecViT block. The input is a batch of videos, each frame with N tokens. We apply recurrent units over temporal tubes to integrate information over time, and self-attention and MLP across tokens within each frame. Note that the recurrent units share parameters, but the information is not mixed across temporal tubes. Similarly, the ViT blocks share parameters, but the information is not mixed across frames.

## Introduction

Video understanding requires low-level scene understanding (e.g. how objects move) and high-level reasoning (e.g. causal relations between events) over a signal that is highdimensional, can be noisy, and contains high correlations and redundancies in both spatial and temporal dimensions. Efficient video modelling needs high-capacity models that can represent the sheer diversity and richness of real-world videos, while having reasonable compute and memory footprint both at training and during inference time. Convolutional neural networks [[8,](#b7)[14]](#b13) have been a successful family of models for video, but their scaling capabilities (in both data and parameters) are limited due to their inductive biases (locality, invariance). Recurrent neural networks, e.g. [[33,](#b32)[38]](#b37) have some desirable properties for video modelling (constant inference cost per timestep independent of the length of the video), but they are slow to train due to their sequential nature and have difficulties in learning over long complex sequences. Transformers [[42]](#b41) have emerged as a very powerful family of models for all modalities, with impressive scaling capabilities. However, they have a significant memory footprint and latency due to the quadratic complexity of the self-attention operation. Recently, a new family of linear recurrent networks [[4,](#b3)[16,](#b15)[17,](#b16)[32]](#b31), referred to as State Space Models (SSMs), has emerged as an answer to the quadratic complexity of self-attention and the slow training of RNNs, with promising results for vision and language [[9,](#b8)[27]](#b26).

In this paper, we propose a hybrid architecture that combines the best of all worlds. It alternates gated linear recurrent units (LRUs) [[9]](#b8) applied over time, with self-attention blocks over space, and MLP over feature channels. As opposed to space and channels, time has a natural order ("arrow-of-time") that LRUs can implicitly and efficiently model with O(N ) complexity in the number of input frames at training time and O(1) complexity at inference time, making it possible to process videos that extend even indefinitely. Space, on the other hand, has a fixed limited dimension, for which the quadratic cost of self-attention is more accessible. From a practical perspective, using selfattention over space allows us to naturally process in parallel all the pixels of a given frame, without having to commit to a particular scanning order [[27]](#b26), making better use of hardware when parallel resources are available.

To further limit the self-attention cost, we use spatial patches as introduced in the successful ViT [[12]](#b11) model. But, compared to existing video transformer models, e.g. ViViT [[1]](#b0), the patches do not have a fixed temporal extent. Instead, the embeddings of the spatial patches are integrated continuously into the hidden state of the gated LRUs, providing persistent memory of the entire temporal sequence up to the current frame. Furthermore, similar to convolutional networks, the parameters of the LRUs are shared over space, preventing the number of parameters from exploding as the resolution of the video increases.

We refer to the resulting model as Temporal Recurrent Video Transformer (TRecViT). TRecViT is highly flexible and can address various video understanding tasks, both sparse (e.g. video classification) and dense (e.g. point tracking), trained in a supervised or self-supervised manner, e.g. using masked auto-encoding. In all our experiments, we use a causal setup that respects the arrow of time, so the model is suitable for any downstream applications, from e.g. video classification where we have offline access to the videos, to e.g. robotics, where online processing is required. Overall, our model is significantly more efficient in both memory footprint and FLOPs compared to vanilla transformers.

Paper structure: We discuss related works in more depth in section 2 and we introduce the proposed model in section 3. We discuss training regimes and analyse efficiency when comparing to baselines in section 4. In section 5, we present extensive experiments for various training regimes, different tasks and datasets. We conclude in section 6 with a discussion of the limitations of the proposed approach and directions for future work.

## Related work

Transformers for Video. Proposed initially as language models, transformers [[42]](#b41) have quickly become the dominant architecture across multiple modalities (images, audio, video). Transformer blocks alternate between a spatial mixing block represented by self-attention and a (feature) channel mixing block, represented by a gated MLP. Given that the self-attention layer treats the input tokens as a set, positional encodings must be used in order to specify the location of each token. It also means that no parsing order is needed, unlike the case with RNNs. Vision transformers (ViT) [[12,](#b11)[29]](#b28) split images into a fixed number of patches that are projected into an embedding space to obtain tokens and these are then processed by a regular transformer. Several works extended ViT to video, e.g. by replacing the regular image patches with spatio-temporal ones. The main challenge with transformers, particularly for video, is the quadratic complexity in the number of input tokens. Multiple approaches have been proposed to address this: e.g. factorisations of the self-attention operation [[1,](#b0)[5]](#b4), iterative attention [[23]](#b22), sparse sampling of the input frames [[34]](#b33), and distributed self-attention operations across different devices [[28]](#b27). Our proposed model uses a novel space-time factorisation, where the temporal dimension is handled by LRUs and the spatial dimension by self-attention.

As these models scale successfully to large number of parameters, their data needs are efficiently met by using self-supervised pre-training like masked autoencoding (MAE) [[40]](#b39) or contrastive learning [[45]](#b44). Due to the factorisation used in our architecture, using such pre-training strategies is straightforward and we include successful experiments with MAE pre-training in Section 5.

SSM, a type of Linear Recurrent Model. While transformers [[42]](#b41) can be efficiently parallelised during training, at inference they need to pay a quadratic cost in the sequence length. On the other hand, recurrent networks [[3,](#b2)[13,](#b12)[20,](#b19)[30,](#b29)[39]](#b38) are compact and efficient at inference but slow at training. State Space Models (SSMs) [[17,](#b16)[18,](#b17)[32]](#b31), a particular type of linear recurrent networks, have recently been proposed as an answer to the scalability problem of RNNs, and have shown strong performance in language and other long-range dependencies tasks [[9,](#b8)[16]](#b15).

SSMs, like S4 [[18]](#b17), S4D [[19]](#b18), or Mamba [[16]](#b15) have been introduced as particular discretizations of a continuous time linear system. On the other hand, the linear recurrent unit (LRU) [[32]](#b31) was designed by identifying the minimal set of changes to a vanilla RNN [[13]](#b12) that allows it to obtain the same key properties as the S4D architecture [[18]](#b17); we discuss the LRU in more detail in Section 3. Improving on the LRU, the gated LRU [[9]](#b8) introduces gating mechanisms similar to LSTM or GRU architectures, to filter the input sequence, while the recurrent gate controls the rate of the information decay. Importantly, different from LSTM or GRU, these gates do not depend on the previous state, which would prevent parallelisation at training time. In our work, we use gated LRUs, but we expect similar results to be obtained when using other gated SSM blocks like Mamba within our factorisation.

SSMs for Video. While SSMs have mostly been explored in language, several architectures like S4 and Mamba have also been adapted to image and video modalities [[44]](#b43).

ViS4mer [[21]](#b20) uses a ViT image encoder to process videos frame by frame, and integrates their representations over time using S4 blocks at the top. TranS4mer [[22]](#b21) uses selfattention over short clips and integrates these with gated S4 blocks. More recently, the Mamba architecture was extended to images and videos by having it process a flattened 1D sequence of image or video patches. This requires defining a processing order for the patches, and different orders have been proposed, e.g. bidirectional and following a column or row order [[27,](#b26)[46]](#b45). As opposed to these Mamba-based architectures, our factorisation naturally uses the arrow-of-time to decide the scanning order, resulting in a causal model. Another important benefit of our hybrid architecture is that we can initialise the ViT blocks with strong existing pre-trained weights. This leads to strong performance even at larger scale, as opposed to VideoMamba [[27]](#b26) where the authors report severe overfitting issues, requiring distillation from smaller models when training in a supervised fashion or distillation from CLIP features [[37]](#b36) for self-supervised training.

## TRecViT Architecture

The proposed architecture, TRecViT, is composed of repeated identical blocks, each performing a sequence of information mixing steps across the different dimensions of the video signal: time, space, and channels; see Figure [1](#).

The mixing over the time dimension is handled by gated linear recurrent units (LRUs), similar to the one introduced in [[9]](#b8) for language. Each spatial token is associated with an LRU that processes the tokens within the same temporal tube over time, without mixing the information across temporal tubes. The LRUs share parameters over space, similar to a convolutional network. When applying this temporal mixing operation, the space dimension is transposed into the batch dimension. The mixing over spatial and channel dimensions is handled by a standard ViT block, which first performs the spatial mixing through a self-attention operation, then the channel mixing by using an MLP. When performing the spatial and channel mixing, the time dimension is transposed into the batch dimension. Empirically, we show that this factorization and choice of building blocks is more efficient for understanding temporal dynamics compared to video transformer approaches (e.g. ViViT [[1]](#b0)) or pure SSM models. By applying selfattention over the spatial dimensions, we allow all tokens to attend to all the other tokens in parallel, without having to commit to a particular order (unlike in VideoMamba). We employ strong transformer blocks from ViTs for this operation, including their Imagenet pre-trained weights. The recurrence of the temporal processing enables efficient frameby-frame inference over long videos, with constant memory footprint and causal operation.

## Background on LRUs

Linear Recurrent Units (LRUs) [[32]](#b31), similar to SSMs, belong to the family of linear recurrent architectures and have been shown to be competitive with Transformers on language tasks [[9,](#b8)[16]](#b15). One potential interpretation of the success of these models, as outlined in [[32]](#b31), is that by sacrificing the nonlinear recurrence typical of a recurrent model, we can improve the scalability and controllability of the system. Specifically, the linearity allows the recurrent matrix to be diagonalised through eigenvalue decomposition and absorbing the (dense) eigenvectors matrix into the neighbouring layers. This gives direct access to the eigenvalues of the Jacobian of the transfer function characterising the system. By initialising these eigenvalues within the unit circle, we have guaranteed stability of the system, bypassing issues like vanishing or exploding gradients. In addition, through the specific initialisation range of the eigenvalues within [0, 1] we can control how quickly the information vanishes, with eigenvalues close to 1 promoting longer-term memory. However, using only linear recurrence can greatly limit the expressivity of the layer. In [[31]](#b30), the authors show that by using these layers within a typical transformer structure that alternates linear recurrences with point-wise nonlinearities (e.g. the MLP block), the overall architecture can be shown to be a universal approximator of finite sequence-tosequence maps.

## Gated LRUs for Video

We adopt the gated variant of the LRU [[9]](#b8) to design our proposed block for video modelling.

Let X ∈ [0, 1] T ×H×W ×3 be an RGB video with T frames and H × W pixels. The video frames are split into N non-overlapping patches p k t of size n × n × 3, with t ∈ {1, T } and k ∈ {1, N }. Let x k t be the tokens obtained after the linear projection of the patches and the addition of the spatial positional encoding, with token size 1 × 1 × d, where d is the token feature dimension. Each LRU operates over a temporal tube {x k t |t = 1, T }, following the equations below (we drop the k spatial index for clarity):

$i t = σ(W x x t + b x ), input gate (1) r t = σ (W λ x t + b λ ) , recurrence gate (2) λ t = σ(λ) C•rt ,(3)$$h t = λ t ⊙ h t-1 + 1 -λ 2 t ⊙ (i t ⊙ x t ). (4$$)$where

$h t ∈ R d is$the state of the LRU, λ t ∈ R d is a vector containing the eigenvalues of the (diagonal) recurrence matrix[foot_0](#foot_0) , i t ∈ R d is the input gate controlling whether x t ∈ R d is integrated within the state h t of the LRU or not, and r t ∈ R d is the recurrence gate. The weights and biases of the LRU (

$W x ∈ R d×d , W λ ∈ R d×d , b x ∈ R d , b λ ∈ R d )$are initialized using LeCun init [[26]](#b25). The (learnable) recurrence weights λ are passed through a sigmoid function to ensure they are between 0 and 1, and are initialised such that σ(λ) is sampled uniformly in [λ min , λ max ]. These recurrent weights are raised to the power C • r t , which effectively acts as a gate controlled by r t given in equation ( [2](#formula_5)). r t is defined as a linear projection, with parameters W λ and b λ , followed by a sigmoid function to ensure again the range [0, 1]. By raising element-wise σ(λ) to r t , the effective recurrence weight at some position j can change between the j-th entry of σ(λ) when the corresponding gate entry is 1 and 1 when the gate entry is 0.

The additional constant coefficient C ∈ R, typically set to 8 as in [[9]](#b8), increases the range to be between σ(λ) C to 1, providing additional flexibility. E.g. if σ(λ) is 0.9 and we set C = 8, we extend the range from [0.9, 1] to [0.43, 1]. More importantly, we change the learning dynamics (e.g. gradient norms) and resolution we have over the range during learning. Specifically, for x t in some fixed interval and similar magnitude W λ , as it is the case at initialisation, a higher value of C implies λ t will concentrate more towards the edges of the range. Note also that this is the dynamic range in which the recurrent weights can vary during inference as a function of the input tokens.

In [[9]](#b8), the authors found that setting λ min = 0.9 and λ max = 0.999 leads to the best results. An eigenvalue of 0.9 implies that it will take at least 10 time steps for the information to decay to roughly 35% of its magnitude, while for an eigenvalue of 0.999 it will take 1000 time steps to decay by the same amount. When using the same range for video modelling, we observed that the eigenvalues are pushed significantly towards λ min during training, with a small number of eigenvalues becoming smaller than λ min ; see Figure [2](#fig_0). We experimented with extending the range and obtained better results with λ min = 0.6. This leads to faster decay of information initially and might reflect the importance for videos of having enough recurrent units focused on short term information, in order to disentangle fast changing dynamics from slow ones. Finally, note that when diagonalising the recurrence matrix, the eigenvalues λ could, in theory, have complex values. We conducted experiments using complex eigenvalues, but we did not see improvements compared to using only real eigenvalues. The same observation was made in [[9,](#b8)[16]](#b15) as well.

## Video block based on gated LRU

We use the gated LRU in a similar block structure as the one employed in [[9]](#b8), see Figure [1b](#). Given a 1D input (temporal tube), the block first applies a normalisation layer, then the signal is routed on two different paths. On the first one, it gets linearly projected to same dimensionality d and then the GeLU activation is applied. On the other path, the signal is also linearly projected to the same dimensionality d, then we apply a 1D convolution followed by the gated LRU described in equation ( [4](#formula_1)). The output of the LRU and the GeLU branch are element-wise multiplied and then linearly projected to the same dimension d. Note that, in line with [[9]](#b8), we use a separable convolution, which allows mixing information only over time, not over chan- nels. We sweep the width of the convolutional kernel and find that a window of 2 is enough compared to [[9]](#b8) which used 4. Also, different from [[9]](#b8), we do not use an MLP block after the LRU for feature mixing. We apply the MLP after the self-attention block, as done in ViT.

Given the diagonal form of the recurrence, on device, the gated LRU computations are memory-bound, i.e. the data transfer takes longer than the actual computations done on that data. Similar to [[9]](#b8) we use a specialised Pallas [[6]](#b5) kernel that minimizes the number of bytes that need to be moved between HBM and VMEM (the Vector Processing Unit's cache). The parameters added by the linear projections within the block, as well as the parameters of the convolution and the LRU, are learned.

## Training TRecViT

The proposed architecture can be trained in a supervised or self-supervised regime. Given a tokenised video input, the output of TRecViT will have the same dimension and shape as the input, meaning that we can easily recover the spatiotemporal structure of the input video, which can be useful for dense tasks like pixel reconstruction, depth estimation, or point tracking. At inference time, the architecture can be applied over all the video frames at once, or frame-byframe by carrying over the state of the LRUs. Depending on the task, one can choose to keep all the outputs from all time-steps to make a prediction (similar to ViViT), or just the outputs from the last step, given that the LRU integrates the previous history in its state. In our experiments, we use mainly the former for fairer comparison with ViViT, but we also experiment with the latter to analyse LRU's capability of remembering over a very long context; see subsection 5.3.

## Self-supervised pre-training

Given the factorised nature of the proposed architecture and the redundancy present in the video signal, it comes natural to apply masked auto-encoding to enable self-supervised pre-training from scratch on large-scale unlabelled datasets.

We follow the same recipe as in the original VideoMAE paper [[40]](#b39). Specifically, we use tube masking where a 2D random mask is generated and repeated for all the frames in the video. For our architecture, this is equivalent to dropping temporal LRUs. The training objective is simply L 2 reconstruction error of the entire frames. We sweep the value of the masking ratio and we find that 0.90 leads to best performance on downstream tasks. When using the pre-trained representations for downstream tasks, we keep all the tokens of the video and we add a decoder or readout head that is fine-tuned for the respective tasks.

## Memory footprint and FLOPs

We compare the memory footprint and the number of FLOPs of TRecViT against ViViT baselines, see Figure [3](#fig_1). The profiling results are obtained by cost and memory analysis of lowered Jax HLO on CPU backend to be aligned with the theoretical numbers [[2]](#b1). We consider as input a video of size 224 × 224 and we vary the length of the video to analyse the savings provided by our architecture as the length of the video increases. Although in number of parameters for TRecViT is in between ViViT-B and ViViT-L (90M > 109M > 320M), the peak memory and number of flops for TRecViT are significantly lower as the number of frames increases, e.g. at 32 frames (the number of frames typically used in video classification experiments), TRecViT's peak memory is ∼12× smaller than that of ViViT-L and the FLOPs count is 5× lower. When going to 64 frames, the peak memory is ∼24× smaller and FLOPs count is 8× lower.

## Experiments

We present results for supervised video classification and self-supervised masked auto-encoding with frozen representations evaluated on two downstream tasks: video classification and point tracking. To analyse the memory capabilities of our model, we also include a reconstruction task of frames seen in the distant past. Using the same task, we study the generalisation capabilities to longer sequences than seen during training. We follow the ViT scaling configurations and, unless otherwise stated, we use the Base version for our model for all our experiments. We specify the number of parameters for all models considered in our experiments, and we include in the supplementary material all the training hyperparameters and data augmentations used in all experiments.

## Supervised video classification

Datasets: We use large-scale real-world datasets for the supervised video classification task. Kinetics400 [[7]](#b6) contains 241,512 videos[foot_1](#foot_1) across train, validation, and test splits, 10slong (25fps), spanning 400 classes. This dataset is known to require modelling appearance for successful action recognition. To challenge our model's capability of understanding motion, we also use SSv2 dataset [[15]](#b14), which contains 220,847 shorter videos (2-6s long), sampled at 12fps, representing 174 classes. This dataset includes actions that differ in finer motion-related details, requiring a deeper temporal understanding, e.g. pouring something into something vs pretending to pour something into something. Baselines: We use ViViT [[1]](#b0) as our main baseline. We consider the full self-attention version, which patchifies and flattens the entire video, prepends a video class token, then runs self-attention blocks. We also consider the factorised encoder version (ViViT FE), which runs a ViT image model over all the frames, and uses temporal self-attention blocks to integrate the information over time. Finally, we also consider a baseline that uses only LRU recurrent and MLP blocks, configured similar to VideoMamba [[27]](#b26), i.e. it does not use self-attention blocks, denoted PureLRU. Similar to ViViT, this model first patchifies and flattens the video, prepends a class token, then applies a sequence of recurrent blocks. All baselines use learnt spatio-temporal positional encoding, whereas the proposed TRecViT uses only spatial positional encoding as the temporal dimension is implicitly modelled through its recurrence. Results: We include results for training from scratch or using Imagenet pre-trained weights to initialise the weights of the ViT blocks. Figure [4](#fig_2) shows a first comparison between TRecViT and the above baselines, with all models being trained from scratch on supervised classification on SSv2. We consider the Small version for all models as the larger Base version shows stability issues when trained from scratch, as reported in other works as well [[1,](#b0)[27]](#b26). As expected, the performance on this challenging dataset when training from scratch is far from SOTA, but it clearly shows that the proposed factorisation has superior video modelling capabilities compared to baselines, ViViT-S with full selfattention being the closest competitor. PureLRU's performance is very poor, which is in line with the findings of other works (e.g. VideoMamba) who report that bidirectional (non-causal) processing of the input is needed for good performance.

We report further results comparing against ViViT-B and ViViT-L with full self-attention when using Imagenet pretrained weights; see Table [1](#) for SSv2 results and Table [2](#) for Kinetics400 results. We can observe that our model achieves better performance compared to ViViT baselines on SSv2, but it is slightly below ViViT-L on Kinetics400. This result could reflect the difference between the two datasets mentioned above: outperforming ViViT-L on SSv2 suggests that TRecViT is superior at modelling motion compared to ViViT, but on Kinetics where the appearance is enough for successful classification, both models are on par. We consider this to be a strong positive result for our model given that it has about 3x less parameters compared to ViViT-L and significantly lower FLOPs count and memory footprint as shown in Figure [3](#fig_1).

## Self-supervised masked autoencoding

We use Kinetics400 for self-supervised pre-training from scratch and we report results on multiple downstream (2, 16, 16) 65.9 320M TRecViT [(1,](#b0)[16,](#b15)[16)](#b15) 66.8 109M

Table [1](#). Performance of TRecViT compared to ViViT-B and ViViT-L baselines on SSv2 dataset with all models initialised from Imagenet pre-training. For ViViT-L, we use the result reported by its authors, for ViViT-B we obtained the results internally as they were not reported in the original paper for this dataset.

$Model Patch size Top-1 acc (%) # params ViViT-B(2, 16, 16) 78.1 90M ViViT-L (2, 16, 16) 78.7 320M TRecViT (1, 16, 16) 78.4 109M$Table [2](#). Performance of TRecViT compared to ViViT-B and ViViT-L baselines on Kinetics400 dataset, with all models initialised from Imagenet pre-training. For ViViT-B and ViViT-L, we include the result we obtained internally by re-training the model on the current Kinetics400 dataset version; see footnote.

In the original paper, the authors reported 80.3% on Kinetics400 for ViViT-L.

datasets and tasks by fine-tuning attention readout heads on top of frozen representations. We choose this setup, as opposed to fine-tuning end-to-end, as the performance in this case more clearly reflects the quality of the pre-trained representations. As mentioned in the previous section, we use a large masking ratio (0.90), which makes pre-training very efficient. We report the number of parameters for every model considered. Note that the number of parameters for TRecViT is different from the one reported in the previous section due to the addition of the readout heads.

Video classification: We report video classification accuracy as downstream task using attention readout heads on SSv2 and Kinetics400. We compare the performance against VideoMAE-L [40] in Table 4. Performance of TRecViT compared to baselines on point tracking task on DAVIS and Perception Test datasets. All models use frozen representations evaluated using the readout head from MooG.

dense(r) tasks as well, we evaluate the same frozen MAE representations for the point tracking task. We use the recurrent architecture in MooG [[41]](#b40) as a readout due to its simplicity. MooG uses light cross-attention layers to process the embeddings of each frame in order, and the readout state is carried over through time. We finetune the MooG readout head using MOVi-E dataset [[25]](#b24) as done in popular point tracking works [[11]](#b10). We evaluate these fine-tuned representations on two datasets: Perception Test [[36]](#b35) and DAVIS dataset [[35]](#b34) with point tracks extracted in [[10]](#b9). We report average Jaccard metric [[10]](#b9) for TRecViT compared with MooG and VideoMAE; see Table [4](#). TRecViT obtains better performance on both datasets compared to baselines, which reinforces the observation that our proposed model has strong motion modelling capabilities. We include qualitative results for this task in Figure [5](#fig_3). We can observe that the results are visibly better compared to VideoMAE. More visualisations are included in the supplementary material. 

## Long video memorisation task

Transformer models for language are known to be excellent at retrieving information from context, as they cache the keys and values for the entire history. On the other hand, LRUs / SSMs and RNNs in general struggle with such needle-in-the-haystack style tasks as they need to perform the retrieval based on the compressed history kept in their recurrent state [[9,](#b8)[24]](#b23). We are interested in studying this aspect in the video domain as well. We set up a simple reconstruction task where the model has to remember the frame seen at a given time-step in the past. For our analysis, we run multiple experiments where the model is tasked to reconstruct the (T -k) th frame from the past, with increasing value for k ∈ {16, 48, 80, 112, 144, 164} frames. We employ Walking Tours dataset [[43]](#b42), which contains hour-long videos, and the scenery changes constantly, hence we are guaranteed that the video frames seen most recently will be very different compared to the frames seen earlier on.

We scale the videos to 224 × 224 pixels. Again, we adopt ViViT-L as baseline, and we train both models using Imagenet pretrained weights. For ViViT-L, we keep all the outputs from all T time steps and apply temporal pooling and a 1 × 1 convolution to get the expected shape for the reconstructed frame. For TRecViT, we simply keep the output of the last layer at time step T and reshape it to the expected shape. We show quantitative and qualitative results respectively in Figures [7](#) and [6](#). We can observe that there is a performance-efficiency trade-off at play for TRecViT: its performance is slightly below ViViT's for shorter memory spans [(16,](#b15)[48,](#)[80)](#), but its efficiency (steps-per-second) is significantly higher. However, beyond 80 frames, ViViT-L goes out of memory, whilst TRecViT continues to give decent results up to 144 frames, going out of memory towards 164 frames. Figure [6](#) shows qualitative results compared to the baseline for the case where the models have to remember the frame seen at T -48 in the past. We can observe that the quality of ViViT-L's reconstruction is good. For TRecViT, whilst the overall structure (encoded in lower frequencies) is correct, it struggles to remember the highfrequency content of the image. This is to be expected due to the compression happening in the recurrent state of the model. However, given how different the last seen frame is from the target frame, we consider this to be a very promising result that warrants further investigation into the memorisation capabilities of our model, which we leave as future work.

## Generalisation to longer sequences

Using the same task as above, we analyse the generalisation capabilities to sequences longer than those used during training. Specifically, we train the models with sequences of length T = 64 frames to reconstruct the T -48 frame, and evaluate them on longer sequences T = 96 to reconstruct the same frame. The TRecViT model can run on longer sequences without any modification. For the ViViT model, we need to adapt the positional encoding to accommodate longer sequences. We use interpolation to nearest neighbour to obtain the desired length; cubic interpolation led to worse results. The performance of TRecViT degrades slightly, with PSNR going down from 29.3 (when evaluated on the same sequence length as in training T = 64) to 26.4 when evaluated with T = 96 frame sequences. ViViT's PSNR, however, drops significantly, from 32.3 when evaluated on the same sequence length, to 15.1 when evaluated on longer sequences. We include qualitative examples in Figure [8](#fig_5) where we can observe that ViViT's output contains stronger artefacts compared to TRecViT. 

## Conclusion

We propose a novel video architecture TRecViT that alternates gated linear recurrent units (LRUs) modelling the temporal dynamics in the video with ViT blocks modelling the spatial and channel dimensions. The proposed model outperforms or obtains competitive performance compared to strong baselines (ViViT-L, VideoMAE) on supervised and self-supervised tasks, while having a much smaller number of parameters and significantly reduced memory footprint and FLOPs count. In terms of limitations, our study focuses on doing a first investigation into using LRUs for the video domain and we obtain favourable results on multiple datasets and tasks compared to strong baselines. However, more experimentation and model scaling are required to obtain SOTA results on all these tasks. Given that the training dynamics for gated LRUs are stable and controllable by design, plus the reliance on (pre-trained) ViT blocks give a strong indication that achieving SOTA is possible. We leave this investigation for future work, together with further analysis of training dynamics, and integration into various downstream tasks, e.g. video-language tasks or Robotics tasks.  

![Figure 2. Distribution of the eigenvalues of the recurrent matrix at the beginning and end of training on long video memorisation task (see subsection 5.3) for different initialisation ranges.]()

![Figure 3. Our model demonstrates increasingly greater memory and compute savings compared to ViViT baselines as the number of frames increases. For clarity, TRecViT's peak memory (left figure) goes from about 4G for 8 frames to 22.4G for 64 frames, but this increase is dwarfed by ViViT's increase, hence TRecViT line appears almost horizontal]()

![Figure 4. TRecViT compared to baselines on supervised video classification on SSv2 dataset, trained from scratch. The plot shows the evolution of the evaluation accuracy as training progresses.]()

![Figure 5. Qualitative results obtained by TRecViT for point tracking on DAVIS dataset compared to VideoMAE. The leftmost image indicates the point to track in the original frame, and the images towards the right show zoom-ins on subsequent frames. Green plus (+) marker indicates the ground truth, yellow circle indicates TRecViT's predictions and red circles indicate VideoMAE's predictions.]()

![Figure 6. Qualitative results obtained by TRecViT on the dense memorisation task compared to ViViT-L. Both models are trained using Imagenet pre-trained weights, on video sequences of T = 64 frames and they reconstruct the (T -48) th frame.]()

![Figure 8. Generalisation to longer sequences. Both models are trained using Imagenet pre-trained weights, on video sequences of T = 64 frames to reconstruct the (T -48) th frame; during evaluation, the models receive sequences of T = 96 frames.]()

![Figure 9. Qualitative results obtained by TRecViT for point tracking on DAVIS dataset (rows 1-2) and Perception Test (rows 3-4) compared to VideoMAE. The leftmost image indicates the point to track in the original frame, and the images towards the right show zoom-ins on subsequent frames. Green plus (+) marker indicates the ground truth, yellow circle indicates TRecViT's predictions and red circles indicate VideoMAE's predictions.]()

![Figure 10. Qualitative results for the task of reconstructing a frame from the past, for increasing distance k to the frame to reconstruct from left to right. First row: last frame seen by the model. Second row: TRecViT output. Third row: ViViT-L output; ViViT-L goes OOM for k > 80, so no predictions are shown.]()

![Our model obtains slightly better performance on both datasets compared to this strong baseline, despite having almost 3× less parameters.Point tracking:To demonstrate that our model can handle]()

![Performance of TRecViT compared to VideoMAE on video classification using frozen MAE representations, pre-trained on Kinetics400.]()

Similar to[[9]](#b8), we implement the recurrence weights λt as exp(-C • softplus(λ) • rt), which is mathematically equivalent but numerically more stable.

Kinetics is a dynamic dataset (videos may be removed from YouTube). Our current version has 241,512 videos, compared to 267,000 videos reported in[[1]](#b0), so a decrease of almost 10%, noticeable in the final performance.

