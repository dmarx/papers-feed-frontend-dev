The TRecViT architecture represents a significant advancement in video understanding by integrating the strengths of gated linear recurrent units (LRUs) for temporal processing and Vision Transformer (ViT) blocks for spatial processing. Below is a detailed technical explanation of the rationale behind the design choices made by the researchers.

### TRecViT Overview

The TRecViT architecture is a hybrid model that aims to efficiently process video data by leveraging the unique properties of both LRUs and ViT blocks. The decision to combine these two components stems from the need to address the challenges posed by video data, which is inherently high-dimensional and temporally complex. 

1. **Temporal Processing with LRUs**: 
   - LRUs are designed to handle sequences efficiently, allowing for O(N) complexity during training and O(1) during inference. This is crucial for video data, where the number of frames (N) can be large. The ability to process frames in a constant time during inference enables real-time applications, such as robotics, where timely decision-making is essential.
   - The use of gated mechanisms in LRUs allows for better control over the flow of information, enabling the model to retain relevant temporal information while discarding less important data.

2. **Spatial Processing with ViT Blocks**: 
   - ViT blocks utilize self-attention mechanisms that allow for effective spatial mixing of information across the entire frame. This is particularly beneficial for video understanding, as it enables the model to capture complex spatial relationships between objects in a frame.
   - The decision to use spatial patches, as introduced in ViT, allows for a more manageable representation of high-dimensional video data, reducing the computational burden associated with processing the entire frame at once.

### Architecture Components

#### Gated Linear Recurrent Units (LRUs)

- **Complexity**: The choice of LRUs is motivated by their linear complexity, which is advantageous for processing long sequences of video frames. Traditional RNNs suffer from sequential processing limitations, making them inefficient for long videos. In contrast, LRUs allow for parallelization during training, significantly speeding up the training process.
- **State Update Mechanism**: The state update equation incorporates eigenvalues of the recurrence matrix, which ensures stability and controls information decay. This design choice is critical for maintaining long-term dependencies in video data, as it allows the model to learn how to retain or forget information over time.

#### ViT Blocks

- **Self-Attention Mechanism**: The self-attention mechanism in ViT blocks allows for global context understanding within each frame. This is essential for tasks that require understanding the relationships between different objects and their movements in a scene.
- **Channel Mixing via MLP**: The use of a multi-layer perceptron (MLP) for channel mixing complements the self-attention mechanism, allowing for richer feature representations. This combination enhances the model's ability to capture complex interactions within the spatial domain.

### Key Equations for Gated LRUs

The equations governing the gated LRUs are designed to facilitate effective information flow:

1. **Input Gate**: 
   \[
   i_t = \sigma(W_x x_t + b_x)
   \]
   This gate determines how much of the current input should be integrated into the hidden state.

2. **Recurrence Gate**: 
   \[
   r_t = \sigma(W_\lambda x_t + b_\lambda)
   \]
   This gate controls the recurrence mechanism, allowing the model to decide how much of the previous state should be retained.

3. **State Update**: 
   \[
   h_t = \lambda_t \odot h_{t-1} + (1 - \lambda_t) \odot (i_t \odot x_t)
   \]
   This equation combines the contributions from the previous state and the current input, modulated by the gates. The inclusion of eigenvalues in \(\lambda_t\) ensures that the model can maintain stability and control information decay.

### Positional Encoding

The addition of spatial positional encoding to token embeddings is a crucial design choice that allows the model to maintain spatial information. This is particularly important in video data, where the spatial arrangement of objects can significantly impact the understanding of the scene.

### Efficiency

TRecViT achieves a lower memory footprint and fewer floating-point operations (FLOPs) compared to traditional transformers. This efficiency is primarily due to the use of LRUs for temporal processing, which allows for constant memory usage during inference, and the spatial processing via self-attention, which is more manageable than processing the entire video sequence at once.

### Training Regimes

The architecture supports both supervised and self-supervised training, including masked auto-encoding (MAE). This flexibility allows the model to leverage large amounts of unlabeled video data, enhancing its performance and generalization capabilities.

### Causal Setup

The causal setup of TRecViT respects the "arrow of time," making it suitable for real-time applications. This design choice is essential for tasks where the temporal order of events