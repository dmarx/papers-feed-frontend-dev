<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TRecViT: A Recurrent Video Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-12-18">18 Dec 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Viorica</forename><surname>Pȃtrȃucean</surname></persName>
							<email>viorica@google.com</email>
							<affiliation key="aff0">
								<address>
									<addrLine>2 3 4 2 3 4 2 3 4 1 1 LRU2 LRU2 LRU2 LRU3 LRU3 LRU3 LRU4 LRU4 LRU4 ViT block ViT block x N 1 2 3 4 1 2 3 4 1 2 3 4 LRU1 LRU1 LRU1</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xu</forename><forename type="middle">Owen</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>2 3 4 2 3 4 2 3 4 1 1 LRU2 LRU2 LRU2 LRU3 LRU3 LRU3 LRU4 LRU4 LRU4 ViT block ViT block x N 1 2 3 4 1 2 3 4 1 2 3 4 LRU1 LRU1 LRU1</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Joseph</forename><surname>Heyward</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>2 3 4 2 3 4 2 3 4 1 1 LRU2 LRU2 LRU2 LRU3 LRU3 LRU3 LRU4 LRU4 LRU4 ViT block ViT block x N 1 2 3 4 1 2 3 4 1 2 3 4 LRU1 LRU1 LRU1</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chuhan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>2 3 4 2 3 4 2 3 4 1 1 LRU2 LRU2 LRU2 LRU3 LRU3 LRU3 LRU4 LRU4 LRU4 ViT block ViT block x N 1 2 3 4 1 2 3 4 1 2 3 4 LRU1 LRU1 LRU1</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mehdi</forename><forename type="middle">S M</forename><surname>Sajjadi</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>2 3 4 2 3 4 2 3 4 1 1 LRU2 LRU2 LRU2 LRU3 LRU3 LRU3 LRU4 LRU4 LRU4 ViT block ViT block x N 1 2 3 4 1 2 3 4 1 2 3 4 LRU1 LRU1 LRU1</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">George-Cristian</forename><surname>Muraru</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>2 3 4 2 3 4 2 3 4 1 1 LRU2 LRU2 LRU2 LRU3 LRU3 LRU3 LRU4 LRU4 LRU4 ViT block ViT block x N 1 2 3 4 1 2 3 4 1 2 3 4 LRU1 LRU1 LRU1</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Artem</forename><surname>Zholus</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>2 3 4 2 3 4 2 3 4 1 1 LRU2 LRU2 LRU2 LRU3 LRU3 LRU3 LRU4 LRU4 LRU4 ViT block ViT block x N 1 2 3 4 1 2 3 4 1 2 3 4 LRU1 LRU1 LRU1</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mahdi</forename><surname>Karami</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>2 3 4 2 3 4 2 3 4 1 1 LRU2 LRU2 LRU2 LRU3 LRU3 LRU3 LRU4 LRU4 LRU4 ViT block ViT block x N 1 2 3 4 1 2 3 4 1 2 3 4 LRU1 LRU1 LRU1</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ross</forename><surname>Goroshin</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>2 3 4 2 3 4 2 3 4 1 1 LRU2 LRU2 LRU2 LRU3 LRU3 LRU3 LRU4 LRU4 LRU4 ViT block ViT block x N 1 2 3 4 1 2 3 4 1 2 3 4 LRU1 LRU1 LRU1</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yutian</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>2 3 4 2 3 4 2 3 4 1 1 LRU2 LRU2 LRU2 LRU3 LRU3 LRU3 LRU4 LRU4 LRU4 ViT block ViT block x N 1 2 3 4 1 2 3 4 1 2 3 4 LRU1 LRU1 LRU1</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Simon</forename><surname>Osindero</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>2 3 4 2 3 4 2 3 4 1 1 LRU2 LRU2 LRU2 LRU3 LRU3 LRU3 LRU4 LRU4 LRU4 ViT block ViT block x N 1 2 3 4 1 2 3 4 1 2 3 4 LRU1 LRU1 LRU1</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">João</forename><surname>Carreira</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>2 3 4 2 3 4 2 3 4 1 1 LRU2 LRU2 LRU2 LRU3 LRU3 LRU3 LRU4 LRU4 LRU4 ViT block ViT block x N 1 2 3 4 1 2 3 4 1 2 3 4 LRU1 LRU1 LRU1</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>2 3 4 2 3 4 2 3 4 1 1 LRU2 LRU2 LRU2 LRU3 LRU3 LRU3 LRU4 LRU4 LRU4 ViT block ViT block x N 1 2 3 4 1 2 3 4 1 2 3 4 LRU1 LRU1 LRU1</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Google</forename><surname>Deepmind</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>2 3 4 2 3 4 2 3 4 1 1 LRU2 LRU2 LRU2 LRU3 LRU3 LRU3 LRU4 LRU4 LRU4 ViT block ViT block x N 1 2 3 4 1 2 3 4 1 2 3 4 LRU1 LRU1 LRU1</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Temporal</forename><forename type="middle">Conv1d</forename><surname>Lru</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>2 3 4 2 3 4 2 3 4 1 1 LRU2 LRU2 LRU2 LRU3 LRU3 LRU3 LRU4 LRU4 LRU4 ViT block ViT block x N 1 2 3 4 1 2 3 4 1 2 3 4 LRU1 LRU1 LRU1</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TRecViT: A Recurrent Video Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-12-18">18 Dec 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">9849158EE1407208617E06A678356C70</idno>
					<idno type="arXiv">arXiv:2412.14294v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Gated LRU (Temporal Mixing) MLP (Channel Mixing) Self-Attention (Spatial Mixing) Input (B</term>
					<term>T</term>
					<term>N</term>
					<term>D) Reshape (BxN</term>
					<term>T</term>
					<term>D) Reshape (BxT</term>
					<term>N</term>
					<term>D) Linear GeLU Linear Linear</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ViT block</head><p>RMSNorm Figure 1. Left: TRecViT architecture. Each video frame is divided into non-overlapping patches that are linearly projected into a token embedding space. We then add a learnt spatial positional encoding. The tokens are passed through gated linear recurrent units (LRUs) that share parameters across space. The outputs of the recurrent blocks are then processed by a ViT block. The recurrent operation followed by ViT is repeated N times. Right: TRecViT block. The input is a batch of videos, each frame with N tokens. We apply recurrent units over temporal tubes to integrate information over time, and self-attention and MLP across tokens within each frame. Note that the recurrent units share parameters, but the information is not mixed across temporal tubes. Similarly, the ViT blocks share parameters, but the information is not mixed across frames.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video understanding requires low-level scene understanding (e.g. how objects move) and high-level reasoning (e.g. causal relations between events) over a signal that is highdimensional, can be noisy, and contains high correlations and redundancies in both spatial and temporal dimensions. Efficient video modelling needs high-capacity models that can represent the sheer diversity and richness of real-world videos, while having reasonable compute and memory footprint both at training and during inference time. Convolutional neural networks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14]</ref> have been a successful family of models for video, but their scaling capabilities (in both data and parameters) are limited due to their inductive biases (locality, invariance). Recurrent neural networks, e.g. <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b37">38]</ref> have some desirable properties for video modelling (constant inference cost per timestep independent of the length of the video), but they are slow to train due to their sequential nature and have difficulties in learning over long complex sequences. Transformers <ref type="bibr" target="#b41">[42]</ref> have emerged as a very powerful family of models for all modalities, with impressive scaling capabilities. However, they have a significant memory footprint and latency due to the quadratic complexity of the self-attention operation. Recently, a new family of linear recurrent networks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b31">32]</ref>, referred to as State Space Models (SSMs), has emerged as an answer to the quadratic complexity of self-attention and the slow training of RNNs, with promising results for vision and language <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>In this paper, we propose a hybrid architecture that combines the best of all worlds. It alternates gated linear recurrent units (LRUs) <ref type="bibr" target="#b8">[9]</ref> applied over time, with self-attention blocks over space, and MLP over feature channels. As opposed to space and channels, time has a natural order ("arrow-of-time") that LRUs can implicitly and efficiently model with O(N ) complexity in the number of input frames at training time and O(1) complexity at inference time, making it possible to process videos that extend even indefinitely. Space, on the other hand, has a fixed limited dimension, for which the quadratic cost of self-attention is more accessible. From a practical perspective, using selfattention over space allows us to naturally process in parallel all the pixels of a given frame, without having to commit to a particular scanning order <ref type="bibr" target="#b26">[27]</ref>, making better use of hardware when parallel resources are available.</p><p>To further limit the self-attention cost, we use spatial patches as introduced in the successful ViT <ref type="bibr" target="#b11">[12]</ref> model. But, compared to existing video transformer models, e.g. ViViT <ref type="bibr" target="#b0">[1]</ref>, the patches do not have a fixed temporal extent. Instead, the embeddings of the spatial patches are integrated continuously into the hidden state of the gated LRUs, providing persistent memory of the entire temporal sequence up to the current frame. Furthermore, similar to convolutional networks, the parameters of the LRUs are shared over space, preventing the number of parameters from exploding as the resolution of the video increases.</p><p>We refer to the resulting model as Temporal Recurrent Video Transformer (TRecViT). TRecViT is highly flexible and can address various video understanding tasks, both sparse (e.g. video classification) and dense (e.g. point tracking), trained in a supervised or self-supervised manner, e.g. using masked auto-encoding. In all our experiments, we use a causal setup that respects the arrow of time, so the model is suitable for any downstream applications, from e.g. video classification where we have offline access to the videos, to e.g. robotics, where online processing is required. Overall, our model is significantly more efficient in both memory footprint and FLOPs compared to vanilla transformers.</p><p>Paper structure: We discuss related works in more depth in section 2 and we introduce the proposed model in section 3. We discuss training regimes and analyse efficiency when comparing to baselines in section 4. In section 5, we present extensive experiments for various training regimes, different tasks and datasets. We conclude in section 6 with a discussion of the limitations of the proposed approach and directions for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Transformers for Video. Proposed initially as language models, transformers <ref type="bibr" target="#b41">[42]</ref> have quickly become the dominant architecture across multiple modalities (images, audio, video). Transformer blocks alternate between a spatial mixing block represented by self-attention and a (feature) channel mixing block, represented by a gated MLP. Given that the self-attention layer treats the input tokens as a set, positional encodings must be used in order to specify the location of each token. It also means that no parsing order is needed, unlike the case with RNNs. Vision transformers (ViT) <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b28">29]</ref> split images into a fixed number of patches that are projected into an embedding space to obtain tokens and these are then processed by a regular transformer. Several works extended ViT to video, e.g. by replacing the regular image patches with spatio-temporal ones. The main challenge with transformers, particularly for video, is the quadratic complexity in the number of input tokens. Multiple approaches have been proposed to address this: e.g. factorisations of the self-attention operation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5]</ref>, iterative attention <ref type="bibr" target="#b22">[23]</ref>, sparse sampling of the input frames <ref type="bibr" target="#b33">[34]</ref>, and distributed self-attention operations across different devices <ref type="bibr" target="#b27">[28]</ref>. Our proposed model uses a novel space-time factorisation, where the temporal dimension is handled by LRUs and the spatial dimension by self-attention.</p><p>As these models scale successfully to large number of parameters, their data needs are efficiently met by using self-supervised pre-training like masked autoencoding (MAE) <ref type="bibr" target="#b39">[40]</ref> or contrastive learning <ref type="bibr" target="#b44">[45]</ref>. Due to the factorisation used in our architecture, using such pre-training strategies is straightforward and we include successful experiments with MAE pre-training in Section 5.</p><p>SSM, a type of Linear Recurrent Model. While transformers <ref type="bibr" target="#b41">[42]</ref> can be efficiently parallelised during training, at inference they need to pay a quadratic cost in the sequence length. On the other hand, recurrent networks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b38">39]</ref> are compact and efficient at inference but slow at training. State Space Models (SSMs) <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b31">32]</ref>, a particular type of linear recurrent networks, have recently been proposed as an answer to the scalability problem of RNNs, and have shown strong performance in language and other long-range dependencies tasks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>SSMs, like S4 <ref type="bibr" target="#b17">[18]</ref>, S4D <ref type="bibr" target="#b18">[19]</ref>, or Mamba <ref type="bibr" target="#b15">[16]</ref> have been introduced as particular discretizations of a continuous time linear system. On the other hand, the linear recurrent unit (LRU) <ref type="bibr" target="#b31">[32]</ref> was designed by identifying the minimal set of changes to a vanilla RNN <ref type="bibr" target="#b12">[13]</ref> that allows it to obtain the same key properties as the S4D architecture <ref type="bibr" target="#b17">[18]</ref>; we discuss the LRU in more detail in Section 3. Improving on the LRU, the gated LRU <ref type="bibr" target="#b8">[9]</ref> introduces gating mechanisms similar to LSTM or GRU architectures, to filter the input sequence, while the recurrent gate controls the rate of the information decay. Importantly, different from LSTM or GRU, these gates do not depend on the previous state, which would prevent parallelisation at training time. In our work, we use gated LRUs, but we expect similar results to be obtained when using other gated SSM blocks like Mamba within our factorisation.</p><p>SSMs for Video. While SSMs have mostly been explored in language, several architectures like S4 and Mamba have also been adapted to image and video modalities <ref type="bibr" target="#b43">[44]</ref>.</p><p>ViS4mer <ref type="bibr" target="#b20">[21]</ref> uses a ViT image encoder to process videos frame by frame, and integrates their representations over time using S4 blocks at the top. TranS4mer <ref type="bibr" target="#b21">[22]</ref> uses selfattention over short clips and integrates these with gated S4 blocks. More recently, the Mamba architecture was extended to images and videos by having it process a flattened 1D sequence of image or video patches. This requires defining a processing order for the patches, and different orders have been proposed, e.g. bidirectional and following a column or row order <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b45">46]</ref>. As opposed to these Mamba-based architectures, our factorisation naturally uses the arrow-of-time to decide the scanning order, resulting in a causal model. Another important benefit of our hybrid architecture is that we can initialise the ViT blocks with strong existing pre-trained weights. This leads to strong performance even at larger scale, as opposed to VideoMamba <ref type="bibr" target="#b26">[27]</ref> where the authors report severe overfitting issues, requiring distillation from smaller models when training in a supervised fashion or distillation from CLIP features <ref type="bibr" target="#b36">[37]</ref> for self-supervised training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">TRecViT Architecture</head><p>The proposed architecture, TRecViT, is composed of repeated identical blocks, each performing a sequence of information mixing steps across the different dimensions of the video signal: time, space, and channels; see Figure <ref type="figure">1</ref>.</p><p>The mixing over the time dimension is handled by gated linear recurrent units (LRUs), similar to the one introduced in <ref type="bibr" target="#b8">[9]</ref> for language. Each spatial token is associated with an LRU that processes the tokens within the same temporal tube over time, without mixing the information across temporal tubes. The LRUs share parameters over space, similar to a convolutional network. When applying this temporal mixing operation, the space dimension is transposed into the batch dimension. The mixing over spatial and channel dimensions is handled by a standard ViT block, which first performs the spatial mixing through a self-attention operation, then the channel mixing by using an MLP. When performing the spatial and channel mixing, the time dimension is transposed into the batch dimension. Empirically, we show that this factorization and choice of building blocks is more efficient for understanding temporal dynamics compared to video transformer approaches (e.g. ViViT <ref type="bibr" target="#b0">[1]</ref>) or pure SSM models. By applying selfattention over the spatial dimensions, we allow all tokens to attend to all the other tokens in parallel, without having to commit to a particular order (unlike in VideoMamba). We employ strong transformer blocks from ViTs for this operation, including their Imagenet pre-trained weights. The recurrence of the temporal processing enables efficient frameby-frame inference over long videos, with constant memory footprint and causal operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Background on LRUs</head><p>Linear Recurrent Units (LRUs) <ref type="bibr" target="#b31">[32]</ref>, similar to SSMs, belong to the family of linear recurrent architectures and have been shown to be competitive with Transformers on language tasks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16]</ref>. One potential interpretation of the success of these models, as outlined in <ref type="bibr" target="#b31">[32]</ref>, is that by sacrificing the nonlinear recurrence typical of a recurrent model, we can improve the scalability and controllability of the system. Specifically, the linearity allows the recurrent matrix to be diagonalised through eigenvalue decomposition and absorbing the (dense) eigenvectors matrix into the neighbouring layers. This gives direct access to the eigenvalues of the Jacobian of the transfer function characterising the system. By initialising these eigenvalues within the unit circle, we have guaranteed stability of the system, bypassing issues like vanishing or exploding gradients. In addition, through the specific initialisation range of the eigenvalues within [0, 1] we can control how quickly the information vanishes, with eigenvalues close to 1 promoting longer-term memory. However, using only linear recurrence can greatly limit the expressivity of the layer. In <ref type="bibr" target="#b30">[31]</ref>, the authors show that by using these layers within a typical transformer structure that alternates linear recurrences with point-wise nonlinearities (e.g. the MLP block), the overall architecture can be shown to be a universal approximator of finite sequence-tosequence maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Gated LRUs for Video</head><p>We adopt the gated variant of the LRU <ref type="bibr" target="#b8">[9]</ref> to design our proposed block for video modelling.</p><p>Let X ∈ [0, 1] T ×H×W ×3 be an RGB video with T frames and H × W pixels. The video frames are split into N non-overlapping patches p k t of size n × n × 3, with t ∈ {1, T } and k ∈ {1, N }. Let x k t be the tokens obtained after the linear projection of the patches and the addition of the spatial positional encoding, with token size 1 × 1 × d, where d is the token feature dimension. Each LRU operates over a temporal tube {x k t |t = 1, T }, following the equations below (we drop the k spatial index for clarity):</p><formula xml:id="formula_0">i t = σ(W x x t + b x ), input gate (1) r t = σ (W λ x t + b λ ) , recurrence gate (2) λ t = σ(λ) C•rt ,<label>(3)</label></formula><formula xml:id="formula_1">h t = λ t ⊙ h t-1 + 1 -λ 2 t ⊙ (i t ⊙ x t ). (<label>4</label></formula><formula xml:id="formula_2">)</formula><p>where</p><formula xml:id="formula_3">h t ∈ R d is</formula><p>the state of the LRU, λ t ∈ R d is a vector containing the eigenvalues of the (diagonal) recurrence matrix<ref type="foot" target="#foot_0">foot_0</ref> , i t ∈ R d is the input gate controlling whether x t ∈ R d is integrated within the state h t of the LRU or not, and r t ∈ R d is the recurrence gate. The weights and biases of the LRU (</p><formula xml:id="formula_4">W x ∈ R d×d , W λ ∈ R d×d , b x ∈ R d , b λ ∈ R d )</formula><p>are initialized using LeCun init <ref type="bibr" target="#b25">[26]</ref>. The (learnable) recurrence weights λ are passed through a sigmoid function to ensure they are between 0 and 1, and are initialised such that σ(λ) is sampled uniformly in [λ min , λ max ]. These recurrent weights are raised to the power C • r t , which effectively acts as a gate controlled by r t given in equation ( <ref type="formula" target="#formula_5">2</ref>). r t is defined as a linear projection, with parameters W λ and b λ , followed by a sigmoid function to ensure again the range [0, 1]. By raising element-wise σ(λ) to r t , the effective recurrence weight at some position j can change between the j-th entry of σ(λ) when the corresponding gate entry is 1 and 1 when the gate entry is 0.</p><p>The additional constant coefficient C ∈ R, typically set to 8 as in <ref type="bibr" target="#b8">[9]</ref>, increases the range to be between σ(λ) C to 1, providing additional flexibility. E.g. if σ(λ) is 0.9 and we set C = 8, we extend the range from [0.9, 1] to [0.43, 1]. More importantly, we change the learning dynamics (e.g. gradient norms) and resolution we have over the range during learning. Specifically, for x t in some fixed interval and similar magnitude W λ , as it is the case at initialisation, a higher value of C implies λ t will concentrate more towards the edges of the range. Note also that this is the dynamic range in which the recurrent weights can vary during inference as a function of the input tokens.</p><p>In <ref type="bibr" target="#b8">[9]</ref>, the authors found that setting λ min = 0.9 and λ max = 0.999 leads to the best results. An eigenvalue of 0.9 implies that it will take at least 10 time steps for the information to decay to roughly 35% of its magnitude, while for an eigenvalue of 0.999 it will take 1000 time steps to decay by the same amount. When using the same range for video modelling, we observed that the eigenvalues are pushed significantly towards λ min during training, with a small number of eigenvalues becoming smaller than λ min ; see Figure <ref type="figure" target="#fig_0">2</ref>. We experimented with extending the range and obtained better results with λ min = 0.6. This leads to faster decay of information initially and might reflect the importance for videos of having enough recurrent units focused on short term information, in order to disentangle fast changing dynamics from slow ones. Finally, note that when diagonalising the recurrence matrix, the eigenvalues λ could, in theory, have complex values. We conducted experiments using complex eigenvalues, but we did not see improvements compared to using only real eigenvalues. The same observation was made in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16]</ref> as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Video block based on gated LRU</head><p>We use the gated LRU in a similar block structure as the one employed in <ref type="bibr" target="#b8">[9]</ref>, see Figure <ref type="figure">1b</ref>. Given a 1D input (temporal tube), the block first applies a normalisation layer, then the signal is routed on two different paths. On the first one, it gets linearly projected to same dimensionality d and then the GeLU activation is applied. On the other path, the signal is also linearly projected to the same dimensionality d, then we apply a 1D convolution followed by the gated LRU described in equation ( <ref type="formula" target="#formula_1">4</ref>). The output of the LRU and the GeLU branch are element-wise multiplied and then linearly projected to the same dimension d. Note that, in line with <ref type="bibr" target="#b8">[9]</ref>, we use a separable convolution, which allows mixing information only over time, not over chan- nels. We sweep the width of the convolutional kernel and find that a window of 2 is enough compared to <ref type="bibr" target="#b8">[9]</ref> which used 4. Also, different from <ref type="bibr" target="#b8">[9]</ref>, we do not use an MLP block after the LRU for feature mixing. We apply the MLP after the self-attention block, as done in ViT.</p><p>Given the diagonal form of the recurrence, on device, the gated LRU computations are memory-bound, i.e. the data transfer takes longer than the actual computations done on that data. Similar to <ref type="bibr" target="#b8">[9]</ref> we use a specialised Pallas <ref type="bibr" target="#b5">[6]</ref> kernel that minimizes the number of bytes that need to be moved between HBM and VMEM (the Vector Processing Unit's cache). The parameters added by the linear projections within the block, as well as the parameters of the convolution and the LRU, are learned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Training TRecViT</head><p>The proposed architecture can be trained in a supervised or self-supervised regime. Given a tokenised video input, the output of TRecViT will have the same dimension and shape as the input, meaning that we can easily recover the spatiotemporal structure of the input video, which can be useful for dense tasks like pixel reconstruction, depth estimation, or point tracking. At inference time, the architecture can be applied over all the video frames at once, or frame-byframe by carrying over the state of the LRUs. Depending on the task, one can choose to keep all the outputs from all time-steps to make a prediction (similar to ViViT), or just the outputs from the last step, given that the LRU integrates the previous history in its state. In our experiments, we use mainly the former for fairer comparison with ViViT, but we also experiment with the latter to analyse LRU's capability of remembering over a very long context; see subsection 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Self-supervised pre-training</head><p>Given the factorised nature of the proposed architecture and the redundancy present in the video signal, it comes natural to apply masked auto-encoding to enable self-supervised pre-training from scratch on large-scale unlabelled datasets.</p><p>We follow the same recipe as in the original VideoMAE paper <ref type="bibr" target="#b39">[40]</ref>. Specifically, we use tube masking where a 2D random mask is generated and repeated for all the frames in the video. For our architecture, this is equivalent to dropping temporal LRUs. The training objective is simply L 2 reconstruction error of the entire frames. We sweep the value of the masking ratio and we find that 0.90 leads to best performance on downstream tasks. When using the pre-trained representations for downstream tasks, we keep all the tokens of the video and we add a decoder or readout head that is fine-tuned for the respective tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Memory footprint and FLOPs</head><p>We compare the memory footprint and the number of FLOPs of TRecViT against ViViT baselines, see Figure <ref type="figure" target="#fig_1">3</ref>. The profiling results are obtained by cost and memory analysis of lowered Jax HLO on CPU backend to be aligned with the theoretical numbers <ref type="bibr" target="#b1">[2]</ref>. We consider as input a video of size 224 × 224 and we vary the length of the video to analyse the savings provided by our architecture as the length of the video increases. Although in number of parameters for TRecViT is in between ViViT-B and ViViT-L (90M &gt; 109M &gt; 320M), the peak memory and number of flops for TRecViT are significantly lower as the number of frames increases, e.g. at 32 frames (the number of frames typically used in video classification experiments), TRecViT's peak memory is ∼12× smaller than that of ViViT-L and the FLOPs count is 5× lower. When going to 64 frames, the peak memory is ∼24× smaller and FLOPs count is 8× lower.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We present results for supervised video classification and self-supervised masked auto-encoding with frozen representations evaluated on two downstream tasks: video classification and point tracking. To analyse the memory capabilities of our model, we also include a reconstruction task of frames seen in the distant past. Using the same task, we study the generalisation capabilities to longer sequences than seen during training. We follow the ViT scaling configurations and, unless otherwise stated, we use the Base version for our model for all our experiments. We specify the number of parameters for all models considered in our experiments, and we include in the supplementary material all the training hyperparameters and data augmentations used in all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Supervised video classification</head><p>Datasets: We use large-scale real-world datasets for the supervised video classification task. Kinetics400 <ref type="bibr" target="#b6">[7]</ref> contains 241,512 videos<ref type="foot" target="#foot_1">foot_1</ref> across train, validation, and test splits, 10slong (25fps), spanning 400 classes. This dataset is known to require modelling appearance for successful action recognition. To challenge our model's capability of understanding motion, we also use SSv2 dataset <ref type="bibr" target="#b14">[15]</ref>, which contains 220,847 shorter videos (2-6s long), sampled at 12fps, representing 174 classes. This dataset includes actions that differ in finer motion-related details, requiring a deeper temporal understanding, e.g. pouring something into something vs pretending to pour something into something. Baselines: We use ViViT <ref type="bibr" target="#b0">[1]</ref> as our main baseline. We consider the full self-attention version, which patchifies and flattens the entire video, prepends a video class token, then runs self-attention blocks. We also consider the factorised encoder version (ViViT FE), which runs a ViT image model over all the frames, and uses temporal self-attention blocks to integrate the information over time. Finally, we also consider a baseline that uses only LRU recurrent and MLP blocks, configured similar to VideoMamba <ref type="bibr" target="#b26">[27]</ref>, i.e. it does not use self-attention blocks, denoted PureLRU. Similar to ViViT, this model first patchifies and flattens the video, prepends a class token, then applies a sequence of recurrent blocks. All baselines use learnt spatio-temporal positional encoding, whereas the proposed TRecViT uses only spatial positional encoding as the temporal dimension is implicitly modelled through its recurrence. Results: We include results for training from scratch or using Imagenet pre-trained weights to initialise the weights of the ViT blocks. Figure <ref type="figure" target="#fig_2">4</ref> shows a first comparison between TRecViT and the above baselines, with all models being trained from scratch on supervised classification on SSv2. We consider the Small version for all models as the larger Base version shows stability issues when trained from scratch, as reported in other works as well <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b26">27]</ref>. As expected, the performance on this challenging dataset when training from scratch is far from SOTA, but it clearly shows that the proposed factorisation has superior video modelling capabilities compared to baselines, ViViT-S with full selfattention being the closest competitor. PureLRU's performance is very poor, which is in line with the findings of other works (e.g. VideoMamba) who report that bidirectional (non-causal) processing of the input is needed for good performance.</p><p>We report further results comparing against ViViT-B and ViViT-L with full self-attention when using Imagenet pretrained weights; see Table <ref type="table">1</ref> for SSv2 results and Table <ref type="table">2</ref> for Kinetics400 results. We can observe that our model achieves better performance compared to ViViT baselines on SSv2, but it is slightly below ViViT-L on Kinetics400. This result could reflect the difference between the two datasets mentioned above: outperforming ViViT-L on SSv2 suggests that TRecViT is superior at modelling motion compared to ViViT, but on Kinetics where the appearance is enough for successful classification, both models are on par. We consider this to be a strong positive result for our model given that it has about 3x less parameters compared to ViViT-L and significantly lower FLOPs count and memory footprint as shown in Figure <ref type="figure" target="#fig_1">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Self-supervised masked autoencoding</head><p>We use Kinetics400 for self-supervised pre-training from scratch and we report results on multiple downstream (2, 16, 16) 65.9 320M TRecViT <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b15">16)</ref> 66.8 109M</p><p>Table <ref type="table">1</ref>. Performance of TRecViT compared to ViViT-B and ViViT-L baselines on SSv2 dataset with all models initialised from Imagenet pre-training. For ViViT-L, we use the result reported by its authors, for ViViT-B we obtained the results internally as they were not reported in the original paper for this dataset.</p><formula xml:id="formula_5">Model Patch size Top-1 acc (%) # params ViViT-B<label>(2, 16, 16) 78.1 90M ViViT-L (2, 16, 16) 78.7 320M TRecViT (1, 16, 16) 78.4 109M</label></formula><p>Table <ref type="table">2</ref>. Performance of TRecViT compared to ViViT-B and ViViT-L baselines on Kinetics400 dataset, with all models initialised from Imagenet pre-training. For ViViT-B and ViViT-L, we include the result we obtained internally by re-training the model on the current Kinetics400 dataset version; see footnote.</p><p>In the original paper, the authors reported 80.3% on Kinetics400 for ViViT-L.</p><p>datasets and tasks by fine-tuning attention readout heads on top of frozen representations. We choose this setup, as opposed to fine-tuning end-to-end, as the performance in this case more clearly reflects the quality of the pre-trained representations. As mentioned in the previous section, we use a large masking ratio (0.90), which makes pre-training very efficient. We report the number of parameters for every model considered. Note that the number of parameters for TRecViT is different from the one reported in the previous section due to the addition of the readout heads.</p><p>Video classification: We report video classification accuracy as downstream task using attention readout heads on SSv2 and Kinetics400. We compare the performance against VideoMAE-L [40] in Table 4. Performance of TRecViT compared to baselines on point tracking task on DAVIS and Perception Test datasets. All models use frozen representations evaluated using the readout head from MooG.</p><p>dense(r) tasks as well, we evaluate the same frozen MAE representations for the point tracking task. We use the recurrent architecture in MooG <ref type="bibr" target="#b40">[41]</ref> as a readout due to its simplicity. MooG uses light cross-attention layers to process the embeddings of each frame in order, and the readout state is carried over through time. We finetune the MooG readout head using MOVi-E dataset <ref type="bibr" target="#b24">[25]</ref> as done in popular point tracking works <ref type="bibr" target="#b10">[11]</ref>. We evaluate these fine-tuned representations on two datasets: Perception Test <ref type="bibr" target="#b35">[36]</ref> and DAVIS dataset <ref type="bibr" target="#b34">[35]</ref> with point tracks extracted in <ref type="bibr" target="#b9">[10]</ref>. We report average Jaccard metric <ref type="bibr" target="#b9">[10]</ref> for TRecViT compared with MooG and VideoMAE; see Table <ref type="table">4</ref>. TRecViT obtains better performance on both datasets compared to baselines, which reinforces the observation that our proposed model has strong motion modelling capabilities. We include qualitative results for this task in Figure <ref type="figure" target="#fig_3">5</ref>. We can observe that the results are visibly better compared to VideoMAE. More visualisations are included in the supplementary material. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Long video memorisation task</head><p>Transformer models for language are known to be excellent at retrieving information from context, as they cache the keys and values for the entire history. On the other hand, LRUs / SSMs and RNNs in general struggle with such needle-in-the-haystack style tasks as they need to perform the retrieval based on the compressed history kept in their recurrent state <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b23">24]</ref>. We are interested in studying this aspect in the video domain as well. We set up a simple reconstruction task where the model has to remember the frame seen at a given time-step in the past. For our analysis, we run multiple experiments where the model is tasked to reconstruct the (T -k) th frame from the past, with increasing value for k ∈ {16, 48, 80, 112, 144, 164} frames. We employ Walking Tours dataset <ref type="bibr" target="#b42">[43]</ref>, which contains hour-long videos, and the scenery changes constantly, hence we are guaranteed that the video frames seen most recently will be very different compared to the frames seen earlier on.</p><p>We scale the videos to 224 × 224 pixels. Again, we adopt ViViT-L as baseline, and we train both models using Imagenet pretrained weights. For ViViT-L, we keep all the outputs from all T time steps and apply temporal pooling and a 1 × 1 convolution to get the expected shape for the reconstructed frame. For TRecViT, we simply keep the output of the last layer at time step T and reshape it to the expected shape. We show quantitative and qualitative results respectively in Figures <ref type="figure">7</ref> and <ref type="figure">6</ref>. We can observe that there is a performance-efficiency trade-off at play for TRecViT: its performance is slightly below ViViT's for shorter memory spans <ref type="bibr" target="#b15">(16,</ref><ref type="bibr">48,</ref><ref type="bibr">80)</ref>, but its efficiency (steps-per-second) is significantly higher. However, beyond 80 frames, ViViT-L goes out of memory, whilst TRecViT continues to give decent results up to 144 frames, going out of memory towards 164 frames. Figure <ref type="figure">6</ref> shows qualitative results compared to the baseline for the case where the models have to remember the frame seen at T -48 in the past. We can observe that the quality of ViViT-L's reconstruction is good. For TRecViT, whilst the overall structure (encoded in lower frequencies) is correct, it struggles to remember the highfrequency content of the image. This is to be expected due to the compression happening in the recurrent state of the model. However, given how different the last seen frame is from the target frame, we consider this to be a very promising result that warrants further investigation into the memorisation capabilities of our model, which we leave as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Generalisation to longer sequences</head><p>Using the same task as above, we analyse the generalisation capabilities to sequences longer than those used during training. Specifically, we train the models with sequences of length T = 64 frames to reconstruct the T -48 frame, and evaluate them on longer sequences T = 96 to reconstruct the same frame. The TRecViT model can run on longer sequences without any modification. For the ViViT model, we need to adapt the positional encoding to accommodate longer sequences. We use interpolation to nearest neighbour to obtain the desired length; cubic interpolation led to worse results. The performance of TRecViT degrades slightly, with PSNR going down from 29.3 (when evaluated on the same sequence length as in training T = 64) to 26.4 when evaluated with T = 96 frame sequences. ViViT's PSNR, however, drops significantly, from 32.3 when evaluated on the same sequence length, to 15.1 when evaluated on longer sequences. We include qualitative examples in Figure <ref type="figure" target="#fig_5">8</ref> where we can observe that ViViT's output contains stronger artefacts compared to TRecViT. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We propose a novel video architecture TRecViT that alternates gated linear recurrent units (LRUs) modelling the temporal dynamics in the video with ViT blocks modelling the spatial and channel dimensions. The proposed model outperforms or obtains competitive performance compared to strong baselines (ViViT-L, VideoMAE) on supervised and self-supervised tasks, while having a much smaller number of parameters and significantly reduced memory footprint and FLOPs count. In terms of limitations, our study focuses on doing a first investigation into using LRUs for the video domain and we obtain favourable results on multiple datasets and tasks compared to strong baselines. However, more experimentation and model scaling are required to obtain SOTA results on all these tasks. Given that the training dynamics for gated LRUs are stable and controllable by design, plus the reliance on (pre-trained) ViT blocks give a strong indication that achieving SOTA is possible. We leave this investigation for future work, together with further analysis of training dynamics, and integration into various downstream tasks, e.g. video-language tasks or Robotics tasks.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Distribution of the eigenvalues of the recurrent matrix at the beginning and end of training on long video memorisation task (see subsection 5.3) for different initialisation ranges.</figDesc><graphic coords="4,340.88,180.81,188.99,221.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Our model demonstrates increasingly greater memory and compute savings compared to ViViT baselines as the number of frames increases. For clarity, TRecViT's peak memory (left figure) goes from about 4G for 8 frames to 22.4G for 64 frames, but this increase is dwarfed by ViViT's increase, hence TRecViT line appears almost horizontal</figDesc><graphic coords="5,70.38,72.00,213.84,159.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. TRecViT compared to baselines on supervised video classification on SSv2 dataset, trained from scratch. The plot shows the evolution of the evaluation accuracy as training progresses.</figDesc><graphic coords="6,329.06,72.00,212.62,126.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Qualitative results obtained by TRecViT for point tracking on DAVIS dataset compared to VideoMAE. The leftmost image indicates the point to track in the original frame, and the images towards the right show zoom-ins on subsequent frames. Green plus (+) marker indicates the ground truth, yellow circle indicates TRecViT's predictions and red circles indicate VideoMAE's predictions.</figDesc><graphic coords="7,58.50,72.00,494.95,89.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .Figure 7 .</head><label>67</label><figDesc>Figure 6. Qualitative results obtained by TRecViT on the dense memorisation task compared to ViViT-L. Both models are trained using Imagenet pre-trained weights, on video sequences of T = 64 frames and they reconstruct the (T -48) th frame.</figDesc><graphic coords="8,58.50,72.00,495.00,136.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Generalisation to longer sequences. Both models are trained using Imagenet pre-trained weights, on video sequences of T = 64 frames to reconstruct the (T -48) th frame; during evaluation, the models receive sequences of T = 96 frames.</figDesc><graphic coords="9,58.50,181.03,236.24,125.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Qualitative results obtained by TRecViT for point tracking on DAVIS dataset (rows 1-2) and Perception Test (rows 3-4) compared to VideoMAE. The leftmost image indicates the point to track in the original frame, and the images towards the right show zoom-ins on subsequent frames. Green plus (+) marker indicates the ground truth, yellow circle indicates TRecViT's predictions and red circles indicate VideoMAE's predictions.</figDesc><graphic coords="13,58.50,455.28,494.95,89.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Qualitative results for the task of reconstructing a frame from the past, for increasing distance k to the frame to reconstruct from left to right. First row: last frame seen by the model. Second row: TRecViT output. Third row: ViViT-L output; ViViT-L goes OOM for k &gt; 80, so no predictions are shown.</figDesc><graphic coords="14,58.50,240.91,494.98,258.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Our model obtains slightly better performance on both datasets compared to this strong baseline, despite having almost 3× less parameters.Point tracking:To demonstrate that our model can handle</figDesc><table><row><cell>Model</cell><cell>Dataset</cell><cell cols="2">Top-1 acc (%) # params</cell></row><row><cell cols="2">VideoMAE Kinetics400</cell><cell>45.8</cell><cell>330M</cell></row><row><cell>TRecViT</cell><cell>Kinetics400</cell><cell>46.0</cell><cell>128M</cell></row><row><cell>VideoMAE</cell><cell>SSv2</cell><cell>53.7</cell><cell>330M</cell></row><row><cell>TRecViT</cell><cell>SSv2</cell><cell>53.9</cell><cell>128M</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Performance of TRecViT compared to VideoMAE on video classification using frozen MAE representations, pre-trained on Kinetics400.</figDesc><table><row><cell>Model</cell><cell>Dataset</cell><cell># frames</cell><cell>AJ</cell><cell># params</cell></row><row><cell>MooG</cell><cell>DAVIS</cell><cell>8</cell><cell>0.687</cell><cell>35M</cell></row><row><cell>VideoMAE</cell><cell>DAVIS</cell><cell>8</cell><cell>0.703</cell><cell>330M</cell></row><row><cell>TRecViT</cell><cell>DAVIS</cell><cell>8</cell><cell>0.706</cell><cell>128M</cell></row><row><cell>MooG</cell><cell>Perception Test</cell><cell>16</cell><cell>0.760</cell><cell>46.5M</cell></row><row><cell cols="2">VideoMAE Perception Test</cell><cell>16</cell><cell>0.761</cell><cell>330M</cell></row><row><cell>TRecViT</cell><cell>Perception Test</cell><cell>16</cell><cell>0.783</cell><cell>128M</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>Similar to<ref type="bibr" target="#b8">[9]</ref>, we implement the recurrence weights λt as exp(-C • softplus(λ) • rt), which is mathematically equivalent but numerically more stable.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>Kinetics is a dynamic dataset (videos may be removed from YouTube). Our current version has 241,512 videos, compared to 267,000 videos reported in<ref type="bibr" target="#b0">[1]</ref>, so a decrease of almost 10%, noticeable in the final performance.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We would like to thank <rs type="person">Caglar Gulcehre</rs>, <rs type="person">Daniel Zoran</rs>, <rs type="person">Dima Damen</rs>, and <rs type="person">Andrew Zisserman</rs> for their insightful feedback throughout this project.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TRecViT: A Recurrent Video Transformer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>We include here all the hyperparameters used in the experiments presented in the main paper, together with more qualitative visualisations of results for the point tracking task (section 5.2) and the long video memorisation task (section 5.3). Videos showing point tracks are also attached. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Training hyperparameters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Point tracking qualitative results</head><p>In Figure <ref type="figure">9</ref>, we include more visualisations for the point tracking task using frozen MAE representations pre-trained on Kinetics400, using TRecViT as backbone. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Long video memorisation task</head><p>Figure <ref type="figure">10</ref> shows qualitative results for the memorisation task. For easier visual comparison, we increase the distance k to the frame to reconstruct while also increasing the video length T , so the frame to reconstruct is always the same. For ViViT-L (3rd row), the quality of the reconstruction is very good and does not degrade as k increases. However, the model goes out-of-memory for T &gt; 96. For TRecViT, the high frequencies are less well reconstructed as k increases, but overall the model is able to perform the task reasonably well even at T = 160, k = 144, i.e. it is able to learn with sequences of up to 5.3s long at 30FPS, and remember a frame seen about 4.8s before.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Vivit: A video vision transformer</title>
		<author>
			<persName><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Lučić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021">2021. 2, 3, 6, 1</date>
			<biblScope unit="page" from="6816" to="6826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m">The Jax Authors</title>
		<imprint/>
	</monogr>
	<note>Jax documentation. 5</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Korbinian</forename><surname>Pöppel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Spanring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksandra</forename><surname>Prudnikova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Kopp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Günter</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Brandstetter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.04517</idno>
		<title level="m">xlstm: Extended long short-term memory</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Is space-time attention all you need for video understanding?</title>
		<author>
			<persName><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<idno>PMLR, 2021. 2</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<biblScope unit="page" from="813" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">JAX: composable transformations of Python+NumPy programs</title>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">James</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dougal</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Necula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Skye</forename><surname>Wanderman-Milne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiao</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName><forename type="first">João</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Griffin: Mixing gated linear recurrences with local attention for efficient language models</title>
		<author>
			<persName><forename type="first">Soham</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anushan</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Botev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Cristian-Muraru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruba</forename><surname>Haroun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonard</forename><surname>Berrada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srivatsan</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnaud</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nando</forename><forename type="middle">De</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2024. 2, 3, 4, 5, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">TAP-vid: A benchmark for tracking any point in a video</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankush</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larisa</forename><surname>Markeeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adria</forename><surname>Recasens Continente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Tapir: Tracking any point with per-frame initialization and temporal refinement</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mel</forename><surname>Vecerík</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilara</forename><surname>Gokay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankush</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">João</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="10027" to="10038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName><surname>Jeffrey L Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6201" to="6210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The&quot; something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susanne</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heuna</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ingo</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Mueller-Freitag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5842" to="5850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Mamba: Linear-time sequence modeling with selective state spaces</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.00752</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hippo: Recurrent memory with optimal polynomial projections</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1474" to="1487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Efficiently modeling long sequences with structured state spaces</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.00396</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">On the parameterization and initialization of diagonal state space models</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.11893</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Mohaiminul Islam and Gedas Bertasius. Long movie clip classification with state-space video models</title>
		<author>
			<persName><surname>Md</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient movie scene detection using state-space transformers</title>
		<author>
			<persName><forename type="first">Md Mohaiminul</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahmudul</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kishan</forename><surname>Shamsundar Athrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Braskich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="18749" to="18758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Perceiver IO: A general architecture for structured inputs &amp; outputs</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Jaegle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Skanda</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joao</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Carreira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Repeat after me: Transformers are better than state space models at copying</title>
		<author>
			<persName><forename type="first">Samy</forename><surname>Jelassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Brandfonbrener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eran</forename><surname>Sham M Kakade</surname></persName>
		</author>
		<author>
			<persName><surname>Malach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.01032</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Abhijit</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anissa</forename><forename type="middle">Yuenming</forename><surname>Mak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Austin</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cengiz</forename><surname>Oztireli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Herrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Gnanapragasam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Duckworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Rebain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">James</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Nowrouzezahrai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Lagun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etienne</forename><surname>Pot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangcheng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Golemo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francois</forename><surname>Belletti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henning</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsueh-Ti (</forename><surname>Derek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">)</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Issam</forename><surname>Laradji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwang</forename><forename type="middle">Moo</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matan</forename><surname>Sela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noha</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Radwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhani</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianhao</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Sitzmann</surname></persName>
		</author>
		<title level="m">Kubric: A scalable dataset generator</title>
		<imprint>
			<publisher>Yilun Du, and Yishu Miao</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Yann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Genevieve</forename><forename type="middle">B</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus-Robert</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Efficient BackProp</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="9" to="48" />
			<date type="published" when="2012">2012</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin Heidelberg, Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Videomamba: State space model for efficient video understanding</title>
		<author>
			<persName><forename type="first">Kunchang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinhao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ringattention with blockwise transformers for near-infinite context</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName><forename type="first">Tomás</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukás</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Cernocký</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH 11th Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">On the universality of linear recurrences followed by nonlinear projections</title>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Orvieto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soham</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.11888</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Resurrecting recurrent neural networks for long sequences</title>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Orvieto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anushan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soham</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><surname>De</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.06349</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Spatio-temporal video autoencoder with differentiable memory</title>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Viorica Patraucean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 International Conference on Learning Representations (ICLR) -Workshop track</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rethinking video vits: Sparse video tubes for joint image and video learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weicheng</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2023">June 17-24, 2023. 2023</date>
			<biblScope unit="page" from="2214" to="2224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergi</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00675</idno>
		<title level="m">The 2017 davis challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Perception test: A diagnostic benchmark for multimodal video models</title>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Viorica Pȃtrȃucean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankush</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrià</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larisa</forename><surname>Recasens Continente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dylan</forename><surname>Markeeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Skanda</forename><surname>Banarse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Heyward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatiana</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yury</forename><surname>Matejovicova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Sulsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><surname>Frechette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raphael</forename><surname>Klimczak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junlin</forename><surname>Koster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusuf</forename><surname>Winkler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dima</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">João</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><surname>Carreira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>PMLR, 2021. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
		<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<date type="published" when="2021-07-24">18-24 July 2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using lstms</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">VideoMAE: Masked autoencoders are data-efficient learners for self-supervised video pre-training</title>
		<author>
			<persName><forename type="first">Zhan</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yibing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022. 2, 5, 7, 1</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Moving off-the-grid: Scene-grounded video representations</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Sjoerd Van Steenkiste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishabh</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Kabra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilara</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gokay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etienne</forename><surname>Heyward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Pot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Drew</forename><forename type="middle">A</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">Albert</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joao</forename><surname>Keck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName><surname>Kipf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-eighth Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Is imagenet worth 1 video? learning strong image encoders from 1 long unlabelled video</title>
		<author>
			<persName><forename type="first">Shashanka</forename><surname>Venkataramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mamshad</forename><surname>Nayeem Rizve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">João</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuki</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannis</forename><surname>Avrithis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Hanwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianxiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Ye</surname></persName>
		</author>
		<idno>2024. 3</idno>
	</analytic>
	<monogr>
		<title level="j">A survey on visual mamba. Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">VideoPrism: A foundational visual encoder for video understanding</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitesh</forename><surname>Bharadwaj Gundavarapu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangzhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><forename type="middle">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Hornung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huisheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Sirotenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<idno>PMLR, 2024. 2</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st International Conference on Machine Learning</title>
		<meeting>the 41st International Conference on Machine Learning</meeting>
		<imprint>
			<biblScope unit="page" from="60785" to="60811" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Vision mamba: Efficient visual representation learning with bidirectional state space model</title>
		<author>
			<persName><forename type="first">Lianghui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bencheng</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Forty-first International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
