{
  "arxivId": "2412.14294",
  "title": "TRecViT: A Recurrent Video Transformer",
  "authors": "Viorica P\u0103tr\u0103ucean, Xu Owen He, Joseph Heyward, Chuhan Zhang, Mehdi S. M. Sajjadi, George-Cristian Muraru, Artem Zholus, Mahdi Karami, Ross Goroshin, Yutian Chen, Simon Osindero, Jo\u00e3o Carreira, Razvan Pascanu",
  "abstract": "We propose a novel block for video modelling. It relies on a\ntime-space-channel factorisation with dedicated blocks for each dimension:\ngated linear recurrent units (LRUs) perform information mixing over time,\nself-attention layers perform mixing over space, and MLPs over channels. The\nresulting architecture TRecViT performs well on sparse and dense tasks, trained\nin supervised or self-supervised regimes. Notably, our model is causal and\noutperforms or is on par with a pure attention model ViViT-L on large scale\nvideo datasets (SSv2, Kinetics400), while having $3\\times$ less parameters,\n$12\\times$ smaller memory footprint, and $5\\times$ lower FLOPs count. Code and\ncheckpoints will be made available online at\nhttps://github.com/google-deepmind/trecvit.",
  "url": "https://arxiv.org/abs/2412.14294",
  "issue_number": 923,
  "issue_url": "https://github.com/dmarx/papers-feed/issues/923",
  "created_at": "2025-01-11T07:35:11.387190",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 9,
  "last_read": "2025-01-11T07:38:52.726106",
  "last_visited": "2025-01-11T07:35:29.299000+00:00",
  "main_tex_file": null,
  "published_date": "2024-12-18T19:44:30Z",
  "arxiv_tags": [
    "cs.CV",
    "cs.LG"
  ]
}