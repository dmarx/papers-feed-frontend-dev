- **Nested Hierarchical Dirichlet Process (nHDP)**: Generalizes the nested Chinese restaurant process (nCRP) to allow each word to follow its own path to a topic node based on a per-document distribution over paths on a shared tree.

- **Key Advantage of nHDP**: Alleviates the rigid single-path assumption of nCRP, enabling complex thematic borrowings across documents.

- **Stochastic Variational Inference**: Developed for nHDP, allowing efficient inference for large text collections (e.g., 1.8 million NYT documents, 2.7 million Wikipedia documents).

- **Hierarchical Topic Models**: Use structured priors to learn tree structures for topics, where topics closer to the root are more general and become specific down the tree.

- **Limitations of nCRP**: Assumes documents select topics from a single path, restricting the number of topics and leading to inefficiencies in modeling complex themes.

- **Dirichlet Process (DP)**: Foundation for Bayesian nonparametric models; allows for infinite mixture models. Basic form: 
  \[
  W_n | \phi_n \sim F_W(\phi_n), \quad \phi_n | G \text{ iid } \sim G, \quad G = \sum_{i=1}^{\infty} p_i \delta_{\theta_i}
  \]

- **Chinese Restaurant Process (CRP)**: Provides a way to model the distribution of parameters without directly working with G. Transition probability:
  \[
  \phi_{n+1} | \phi_1, \ldots, \phi_n \sim \frac{\alpha}{\alpha + n} G_0 + \sum_{i=1}^{n} \frac{n_i}{\alpha + n} \delta_{\phi_i}
  \]

- **Stick-Breaking Construction**: Directly constructs G for DP:
  \[
  G = \sum_{i=1}^{\infty} V_i \prod_{j=1}^{i-1} (1 - V_j) \delta_{\theta_i}, \quad V_i \text{ iid } \sim \text{Beta}(1, \alpha)
  \]

- **Nested Chinese Restaurant Process (nCRP)**: Extends CRP to hierarchical structures, allowing for infinite sequences of parameters through nested selections.

- **Path Representation in nHDP**: Each word can access the entire tree with document-specific distributions on paths, allowing for cross-thematic borrowing while maintaining topic separation.

- **Empirical Results**: nHDP compared with nCRP on small datasets, demonstrating improved performance on large datasets using stochastic variational inference.

- **Algorithm Efficiency**: Stochastic variational inference methods enhance scalability for hierarchical topic modeling, crucial for handling large corpora effectively.