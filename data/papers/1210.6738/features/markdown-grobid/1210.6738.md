# Nested Hierarchical Dirichlet Processes

## Abstract

## 

We develop a nested hierarchical Dirichlet process (nHDP) for hierarchical topic modeling. The nHDP generalizes the nested Chinese restaurant process (nCRP) to allow each word to follow its own path to a topic node according to a per-document distribution over the paths on a shared tree. This alleviates the rigid, single-path formulation assumed by the nCRP, allowing documents to easily express complex thematic borrowings. We derive a stochastic variational inference algorithm for the model, which enables efficient inference for massive collections of text documents. We demonstrate our algorithm on 1.8 million documents from The New York Times and 2.7 million documents from Wikipedia.

## INTRODUCTION

Organizing things hierarchically is a natural aspect of human activity. Walking into a large department store, one might first find the men's section, followed by men's casual, and then see the t-shirts hanging along the wall. Or being hungry, one might choose to eat Italian food, decide whether to spring for the better, more authentic version or go to one of the cheaper chain options, and then end up at the Olive Garden. Similarly with data analysis, a hierarchical tree-structured representation of data can provide an illuminating means for understanding and reasoning about the information it contains.

In this paper, we focus on developing hierarchical topic models to construct tree-structured representations for text data. Hierarchical topic models use a structured prior on the topics underlying a corpus of documents, with the aim of bringing more order to an unstructured set of thematic concepts [[1]](#b0)[2] [[3]](#b2). They do this by learning a tree structure for the underlying topics, with the inferential goal being that topics closer to the root are more general, and gradually become more specific in thematic content when following a path down the tree.

Our work builds on the nested Chinese restaurant process (nCRP) [[4]](#b3). The nCRP is a Bayesian nonparametric prior for hierarchical topic models, but is limited in that it assumes each document selects topics from one path in the tree. We illustrate this limitation in Figure [1](#fig_0). This assumption has practical drawbacks; for trees truncated to a small number of levels this does not allow for many topics per document, and for trees of many levels there are too many nodes to infer.

The nCRP also has drawbacks from a modeling standpoint. As a simple example, consider an article on ESPN.com about an injured player, compared with an • John Paisley is with the Department of Electrical Engineering, Columbia University, New York, NY.

• Chong Wang is with Voleon Capital Management, Berkeley, CA.

• David M. Blei is with the Department of Computer Science, Princeton University, Princeton, NJ

• Michael I. Jordan is with the Departments of EECS and Statistics, UC Berkeley, Berkeley, CA article in a sports medicine journal about a specific type of athletic injury. Both documents will contain words about medicine and words about sports. These areas are different enough, however, that one cannot be considered to be a subset of the other. Yet the single-path structure of the nCRP will require this to be the case in order to model the relevant words in the documents, or it will learn a new "sports/medicine" topic rather than a mixture of separate sports and medicine topics. Continuing this analogy, other documents may only be about sports or medicine. As a result, medical terms in the nCRP will need to appear in multiple places within the tree: in its own subtree separate from sports, and also affiliated with sports, perhaps as a child of the general sports topic (in the case of the ESPN article). A similar fractionation of sports-related terms results from the sports medicine article, where the medical terms dominate and sports can be considered a topic underneath the main medicine topic. The result is a tree where topics appear in multiple places, and so the full statistical power within the corpus is not being used to model each topic; the tree will not be as compact as it could be.

Though the nCRP is a Bayesian nonparametric prior, it performs nonparametric clustering of document-specific paths, which reduces the number of topics available to a document by restricting them to lie on a single path, leading to drawbacks as illustrated above. Our goal is to develop a related Bayesian nonparametric prior that performs word-specific path clustering. We illustrate this objective in Figure [1](#fig_0). In this case, each word has access to the entire tree, but with document-specific distributions on the paths within the tree. To this end, we make use of the hierarchical Dirichlet process [[5]](#b4), developing a novel prior that we refer to as the nested hierarchical Dirichlet process (nHDP). The HDP can be viewed as a nonparametric elaboration of the classical topic model, latent Dirichlet allocation (LDA) [[6]](#b5), providing a mechanism whereby a global Dirichlet process defines a base distribution for a collection of local Dirichlet processes, one for each document. With the nHDP, we extend this arXiv:1210.6738v4 [stat.ML] 2 May 2014 nCRP nHDP Fig. [1](#fig_0). An example of path structures for the nested Chinese restaurant process (nCRP) and the nested hierarchical Dirichlet process (nHDP) for hierarchical topic modeling. With the nCRP, the topics for a document are restricted to lying along a single path. With the nHDP, each document has access to the entire tree, but a document-specific distribution on paths will place high probability on a particular subtree. In both models a word follows a path to its topic. This path is deterministic in the case of the nCRP, and drawn from a highly probable document-specific subset of paths in the case of the nHDP.

idea by letting a global nCRP become a base distribution for a collection of local nCRPs, one for each document. As illustrated in Figure [1](#fig_0), the nested HDP provides the opportunity for cross-thematic borrowing while keeping general topic areas in separate subtrees, which is not possible with the nCRP.

Hierarchical topic models have thus far been applied to corpora of small size. A significant issue, not just with topic models but with Bayesian models in general, is to scale up inference to massive data sets [[7]](#b6). Recent developments in stochastic variational inference methods have shown promising results for LDA and the HDP topic model [[8]](#b7)[9] [[10]](#b9). We continue this development for hierarchical topic modeling with the nested HDP. Using stochastic variational inference, we demonstrate an ability to efficiently handle very large corpora. This is a major benefit to complex models such as tree-structured topic models, which require significant amounts of data to support their large size.

We organize the paper as follows: In Section 2 we review the Bayesian nonparametric priors that we incorporate in our model-the Dirichlet process, nested Chinese restaurant process and hierarchical Dirichlet process. In Section 3 we present our proposed nested HDP model for hierarchical topic modeling. In Section 4 we review stochastic variational inference and present an inference algorithm for nHDPs that scales well to massive data sets. We present empirical results in Section 5. We first compare the nHDP with the nCRP on three relatively small data sets. We then evaluate our stochastic algorithm on 1.8 million documents from The New York Times and 2.7 million documents from Wikipedia, comparing performance with stochastic LDA and HDP.

## BACKGROUND: BAYESIAN NONPARAMET-

## RIC PRIORS FOR TOPIC MODELS

The nested hierarchical Dirichlet process (nHDP) builds on a collection of existing Bayesian nonparametric priors. In this section, we review these priors: the Dirichlet process, nested Chinese restaurant process and hierarchical Dirichlet process. We also review constructive representations for these processes that we will use for posterior inference of the nHDP topic model.

## Dirichlet processes

The Dirichlet process (DP) [[11]](#b10) is the foundation for a large collection of Bayesian nonparametric models that rely on mixtures to represent distributions on data. Mixture models work by partitioning a data set according to statistical traits shared by members of the same cell. Dirichlet process priors are effective in learning a suitable number of traits for representing the data, in addition to the parameters of the mixture. The basic form of a Dirichlet process mixture model is

$W n |ϕ n ∼ F W (ϕ n ), ϕ n |G iid ∼ G, G = ∞ i=1 p i δ θi . (1)$With this representation, data W 1 , . . . , W N are distributed according to a family of distributions F W with respective parameters ϕ 1 , . . . , ϕ N . These parameters are drawn from the distribution G, which is discrete and potentially infinite, as the DP allows it to be. This discreteness induces a partition of the data W according to the sharing of the atoms {θ i } among the parameters {ϕ n } that are selected.

The Dirichlet process is a stochastic process for generating G. To briefly review, let (Θ, B) be a measurable space, G 0 a probability measure on it and α > 0.

Ferguson [[11]](#b10) proved the existence of a stochastic process G where, for all measurable partitions {B 1 , . . . , B k } of Θ, with B i ∈ B, (G(B 1 ), . . . , G(B k )) ∼ Dirichlet(αG 0 (B 1 ), . . . , αG 0 (B k )), abbreviated as G ∼ DP(αG 0 ). It has been shown that G is discrete (with probability one) even when G 0 is nonatomic [[12]](#b11)[[13]](#b12). Thus the DP prior is a good candidate for G in Eq. ( [1](#)) since it generates discrete distributions on continuous parameter spaces. For most applications G 0 is diffuse, and so representations of G at the granularity of the atoms are necessary for inference; we next review two of these approaches to working with this infinitedimensional distribution.

## Chinese restaurant processes

The Chinese restaurant process (CRP) avoids directly working with G by integrating it out [[12]](#b11)[[14]](#b13). In doing so, the values of ϕ 1 , . . . , ϕ N become dependent, with the value of ϕ n+1 given ϕ 1 , . . . , ϕ n distributed as

$ϕ n+1 |ϕ 1 , . . . , ϕ n ∼ α α + n G 0 + n i=1 1 α + n δ ϕi .(2)$That is, ϕ n+1 takes the value of one of the previously observed ϕ i with probability n α+n , and a value drawn from G 0 with probability α α+n , which will be unique when G 0 is continuous. This displays the clustering property of the CRP and also gives insight into the impact of α, since it is evident that the number of unique ϕ i grows like α ln n. In the limit n → ∞, the distribution in Eq. ( [2](#formula_1)) converges to a random measure distributed according to a Dirichlet process [[12]](#b11). The CRP is socalled because of an analogy to a Chinese restaurant, where a new customer (datum) sits at a table (selects a parameter) with probability proportional to the number of previous customers at that table, or selects a new table with probability proportional to α.

## A stick-breaking construction

Where the Chinese restaurant process works with G ∼ DP(αG 0 ) implicitly through ϕ, a stick-breaking construction allows one to directly construct G before drawing any ϕ n . Sethuraman [[13]](#b12) showed that if G is constructed as follows:

$G = ∞ i=1 V i i-1 j=1 (1 -V j )δ θi , V i iid ∼ Beta(1, α), θ i iid ∼ G 0 ,(3)$then G ∼ DP(αG 0 ). The variable V i can be interpreted as the proportion broken from the remainder of a unit length stick, j<i (1 -V j ). As the index i increases, more random variables in [0, 1] are multiplied, and thus the weights decrease to zero exponentially. The expectation

$E[V i j<i (1 -V j )] = α i-1$(1+α) i gives a sense of the impact of α on these weights. This explicit construction of G maintains the independence among ϕ 1 , . . . , ϕ N as written in Eq. ( [1](#)), which is a significant advantage of this representation for mean-field variational inference that is not present in the CRP.

## Nested Chinese restaurant processes

Nested Chinese restaurant processes (nCRP) are a treestructured extension of the CRP that are useful for hierarchical topic modeling [[4]](#b3). They extend the CRP analogy to a nesting of restaurants in the following way: After selecting a table (parameter) according to a CRP, the customer departs for another restaurant uniquely indicated by that table. Upon arrival, the customer acts according to the CRP for the new restaurant, and again departs for a restaurant only accessible through the table selected. This occurs for a potentially infinite sequence of restaurants, which generates a sequence of parameters for the customer according to the selected tables.

A natural interpretation of the nCRP is as a tree where each parent has an infinite number of children. Starting from the root node, a path is traversed down the tree. Given the current node, a child node is selected with probability proportional to the previous number of times it was selected among its siblings, or a new child is selected with probability proportional to α. As with the CRP, the underlying mixing measure of the nCRP also has a constructive representation useful for variational inference, which we will use in our nHDP construction.

## Constructing the nCRP

The nesting of Dirichlet processes that leads to the nCRP gives rise to a stick-breaking construction [[2]](#b1). We develop the notation for this construction here and use it later in our construction of the nested HDP. Let i l = (i 1 , . . . , i l ) be a path to a node at level l of the tree. 1 According to the stick-breaking version of the nCRP, the children of node i l are countably infinite, with the probability of transitioning to child j equal to the jth break of a stick-breaking construction. Each child corresponds to a parameter drawn independently from G 0 . Letting the index of the parameter identify the index of the child, this results in the following DP for the children of node i l ,

$G i l = ∞ j=1 V i l ,j j-1 m=1 (1 -V i l ,m )δ θ (i l ,j) , V i l ,j iid ∼ Beta(1, α), θ (i l ,j) iid ∼ G 0 .(4)$1. That is, from the root node first select the child with index i 1 ; from node i 1 = (i 1 ), select the child with index i 2 ; from node i 2 = (i 1 , i 2 ) select the child with index i 3 , and so on to level l with each i k ∈ N. We ignore the root i 0 , which is shared by all paths.

If the next node is child j, then the nCRP transitions to DP G i l+1 , where i l+1 has index j appended to i l , that is i l+1 = (i l , j). A path down the tree givens a sequence of parameters ϕ = (ϕ 1 , ϕ 2 , . . . ), where the parameter ϕ l correspond to an atom θ i l at level l. Hierarchical topic models use these sequences of parameters to give the topics for generating documents. Other nested DPs have been considered as well, such as a two-leveled nDP where all parameters are selected from the leaves [[15]](#b14).

## Nested CRP topic models

Hierarchical topic models based on the nested CRP use a globally shared tree to generate a corpus of documents. Starting with the construction of nested Dirichlet processes as described above, each document selects a path down the tree according to a Markov process, which produces a sequence of topics ϕ d = (ϕ d,1 , ϕ d,2 , . . . ) used to generate the dth document. As with other topic models, each word in a document, W d,n , is represented by an index in the set {1, . . . , V} and the topics θ i l appearing in ϕ d are V-dimensional probability vectors with Dirichlet prior G 0 = Dirichlet(λ 0 1 V ).

For each document d, an additional stick-breaking process provides a distribution on the topics in ϕ d ,

$G (d) = ∞ j=1 U d,j j-1 m=1 (1 -U d,m )δ ϕ d,j , U d,j iid ∼ Beta(γ 1 , γ 2 ).(5)$Since this is not a DP, U d,j has two free parameters, γ 1 and γ 2 . Following the standard method, words for document d are generated by first drawing a topic i.i.d. from G (d) , and then drawing the word index from the discrete distribution with the selected topic.

## Issues with the nCRP

As discussed in the introduction, a significant drawback of the nCRP for topic modeling is that each document follows one path down the tree. Therefore, all thematic content of a document must be contained within that single sequence of topics. Since the nCRP is meant to characterize the thematic content of a corpus in increasing levels of specificity, this creates a combinatorial problem, where similar topics will appear in many parts of the tree to account for the possibility that they appear as a topic of the document (e.g., the sport/medicine example given in the introduction). In practice, nCRP trees are typically truncated at three levels [[2]](#b1)[4], since learning deeper levels becomes difficult due to the exponential increase in nodes. [2](#foot_0) In this situation each document has three topics for modeling its entire thematic content, which is likely insufficient, and so a blending of multiple topics is bound to occur during inference. The nCRP is a Bayesian nonparametric (BNP) prior, but it performs nonparametric clustering of the paths selected at the document level, rather than at the word level. Though the same distribution on a tree is shared by a corpus, each document can differentiate itself only by the path it choses, as well as the distribution on topics in that path. The key issue with the nCRP is the restrictiveness of this single path allowed to a document. However, if instead each word were allowed to follow its own path according to an nCRP, the distribution on paths would be the same for all documents, which is clearly not desired. Our goal is to develop a hierarchical topic model that does not prohibit a document from using topics in different parts of the tree. Our solution to this problem is to employ the hierarchical Dirichlet process (HDP).

## Hierarchical Dirichlet processes

The HDP is a multi-level version of the Dirichlet process [[5]](#b4). It makes use of the idea that the base distribution on the continuous space Θ can be discrete, which is useful because a discrete distribution allows for multiple draws from the DP prior to place probability mass on the same subset of atoms. Hence different groups of data can share the same atoms, but have different probability distributions on them. A discrete base is needed, but the atoms are unknown in advance. The HDP models these atoms by drawing the base from a DP prior. This leads to the hierarchical process

$G d |G iid ∼ DP(βG), G ∼ DP(αG 0 ),(6)$for groups d = 1, . . . , D. This prior has been used to great effect in topic modeling as a nonparametric extension of LDA [[6]](#b5) and related LDA-based models [[16]](#b15)[17] [[18]](#b17).

As with the DP, explicit representations of the HDP are necessary for inference. The representation we use relies on two levels of Sethuraman's stick breaking construction. For this construction, first sample G as in Eq.

(3), and then sample G d in the same way,

$G d = ∞ i=1 V d i i-1 j=1 (1 -V d j )δ φi , V d i iid ∼ Beta(1, β), φ i iid ∼ G. (7$$)$This form is identical to Eq. ( [3](#formula_2)), with the key difference that G is discrete, and so atoms φ i will repeat. An advantage of this representation is that all random variables are i.i.d., which aids variational inference strategies.

## NESTED HIERARCHICAL DIRICHLET PRO-CESSES FOR TOPIC MODELING

In building on the nCRP framework, our goal is to allow for each document to have access to the entire tree, while still learning document-specific distributions on topics that are thematically coherent. Ideally, each document will still exhibit a dominant path corresponding to its main themes, but with off-shoots allowing for other topics. Our two major changes to the nCRP formulation toward this end are that (i) each word follows its own path to a topic, and (ii) each document has its own distribution on paths in a shared tree. The BNP tools discussed above make this a straightforward task.

In the proposed nested hierarchical Dirichlet process (nHDP), we split the process of generating a document's distribution on topics into two parts: first, generating a document's distribution on paths down the tree, and second, generating a word's distribution on terminating at a particular node within those paths.

## Constructing a distribution on paths

With the nHDP, all documents share a global nCRP drawn according to the stick-breaking construction in Section 2.2.1. Denote this tree by T . As discussed, T is simply an infinite collection of Dirichlet processes with a continuous base distribution G 0 and a transition rule between DPs. According to this rule, from a root Dirichlet process G i0 , a path is followed by drawing ϕ l+1 ∼ G i l for l = 0, 1, 2, . . . , where i 0 is a constant root index, and i l = (i 1 , . . . , i l ) indexes the DP associated with the topic ϕ l = θ i l . With the nested HDP, instead of following paths according to the global T , we use each Dirichlet process in T as a base distribution for a local DP drawn independently for each document.

That is, for document d we construct a tree T d where, for each

$G i l ∈ T , we draw a corresponding G (d) i l ∈ T d according to the Dirichlet process G (d) i l ∼ DP(βG i l ).(8)$As discussed in Section 2.3,

$G (d) i l$will have the same atoms as G i l , but with different probability weights on them. Therefore, the tree T d will have the same nodes as T , but the probability of a path in T d will vary with d, giving each document its own distribution on the tree.

We represent this document-specific DP with a stickbreaking construction as in Section 2.3,

$G (d) i l = ∞ j=1 V (d) i l ,j j-1 m=1 (1 -V (d) i l ,m )δ φ (d) i l ,j , V (d) i l ,j iid ∼ Beta(1, β), φ (d) i l ,j iid ∼ G i l .(9)$This representation retains full independence among random variables, and will lead to a simpler stochastic variational inference algorithm. We note that the atoms from the global DP are randomly permuted and copied with this construction; φ (d) i l ,j does not correspond to the node with parameter θ (i l ,j) . To find the probability mass that G (d)

$i l places on θ (i l ,j) , one can calculate G (d) i l ({θ (i l ,j) }) = m G (d) i l ({φ (d) i l ,m })I(φ (d) i l ,m = θ (i l ,j) ).$Using this nesting of HDPs to construct T d , each document has a tree with transition probabilities defined over the same subset of nodes since T is discrete, but with Algorithm 1 Generating documents with the nHDP 1) Generate a global tree T by constructing an nCRP as in Section 2.2.1. 2) Generate document tree T d and switching probabilities U (d) . For document d, a) For each DP in T , draw a DP with this as a base distribution (Equation [8](#formula_9)). b) For each node in T d , draw a beta random variable (Equation [10](#formula_14)). 3) Generate a document. For word n in document d, a) Sample atom ϕ d,n (Equation [11](#)). b) Sample word W d,n from topic ϕ d,n .

values for these probabilities that are document specific.

To see how this allows each word to follow its own path while still producing a thematically coherent document, consider each

$G (d)$i l when β is small. In this case, most of the probability will be placed on one atom selected from G i l since the first proportion V (d) i l ,1 will be large with high probability. This will leave little probability remaining for other atoms, a feature shared by all DPs in T d . Starting from the root node of T d , each word in the document will have high probability of transitioning to the same node when moving down the tree, with some small probability of diverging into a different topic. In the limit β → 0, each G (d) i l will be a delta function on a φ (d) i l ,j ∼ G i l , and the same path will be selected by each word with probability one, thus recovering the nCRP.

## Generating a document

With the tree T d for document d we have a method for selecting word-specific paths that are thematically coherent, meaning they tend to reuse the same path while allowing for off-shoots. We next discuss how to generate a document with this tree. As discussed in Section 2.2.2, with the nCRP the atoms selected for a document by its path through T have a unique stickbreaking distribution that determines which level any particular word comes from. We generalize this idea to the tree T d with an overlapping stick-breaking construction as follows.

For each node i l , we draw a document-specific beta random variable that acts as a stochastic switch. Given a pointer that is currently at node i l , the beta random variable determines the probability that we draw from the topic at that node or continue further down the tree. That is, given that the path for word W d,n is at node i l , stop with probability U d,i l , where

$U d,i l ∼ Beta(γ 1 , γ 2 ).(10)$If we don't select topic θ i l , then continue by selecting node i l+1 according to G (d) i l . We observe the stickbreaking construction implied by this construction; for word n in document d, the probability that its topic

$ϕ d,n = θ i l is Pr(ϕ d,n = θ i l |T d , U d ) = (11) l-1 m=0 G (d) im {θ im+1 } U d,i l l-1 m=1 (1 -U d,im ) .$Here it is implied that i m equals the first m values in i l for m ≤ l. The leftmost term in this expression is the probability of path i l , the right term is the probability that the word does not select the first l -1 topics, but does select the lth. Since all random variables are independent, a simple product form results that will significantly aid the development of a posterior inference algorithm. The overlapping nature of this stick-breaking construction on the levels of a sequence is evident from the fact that the random variables U are shared for the first l values by all paths along the subtree starting at node i l . A similar tree-structured prior distribution was presented by Adams, et al. [[19]](#b18) in which all groups shared the same distribution on a tree and entire objects (e.g., images or documents) were clustered within a single node. We summarize our model for generating documents with the nHDP in Algorithm 1.

## STOCHASTIC VARIATIONAL INFERENCE FOR THE NESTED HDP

Many text corpora can be viewed as "Big Data"-they are large data sets for which standard inference algorithms can be prohibitively slow. For example, Wikipedia currently indexes several million entries and The New York Times has published almost two million articles in the last 20 years. With so much data, fast inference algorithms are essential. Stochastic variational inference is a development in this direction for hierarchical Bayesian models in which ideas from stochastic optimization are applied to approximate Bayesian inference using meanfield variational Bayes (VB) [[20][8]](#). Stochastic inference algorithms have provided a significant speed-up in inference for probabilistic topic models [[9]](#b8)[10] [[21]](#b20). In this section, after reviewing the ideas behind stochastic variational inference, we present a stochastic variational inference algorithm for the nHDP topic model.

## Stochastic variational inference

Stochastic variational inference exploits the difference between local variables, or those associated with a single unit of data, and global variables, which are shared over an entire data set. In brief, stochastic VB works by splitting a large data set into smaller groups, processing the local variables of one group, updating the global variables, and then moving to another group. This is in contrast to batch inference, which processes all local variables at once before updating the global variables.

In the context of probabilistic topic models, the unit of data is a document, and the global variables include the topics (among other possible variables), while the local variables relate to the distribution on these topics for each document. We next briefly review the relevant ideas from variational inference and its stochastic variant.

## The batch set-up

Mean-field variational inference is a method for approximate posterior inference in Bayesian models [[22]](#b21). It approximates the full posterior of a set of model parameters P (Φ|W ) with a factorized distribution Q(Φ|Ψ) = i q i (φ i |ψ i ). It does this by searching the space of variational approximations for one that is close to the posterior according to their Kullback-Leibler divergence. Algorithmically, this is done by maximizing a variational objective function L with respect to the variational parameters Ψ of Q, where

$L(W, Ψ) = E Q [ln P (W, Φ)] -E Q [ln Q].(12)$We are interested in conjugate exponential models, where the prior and likelihood of all nodes of the model fall within the conjugate exponential family. In this case, variational inference has a simple optimization procedure [[23]](#b22), which we illustrate with the following example-this generic example gives the general form exploited by the stochastic variational inference algorithm that we apply to the nHDP.

Consider D independent samples from an exponential family distribution P (W |η), where η is the natural parameter vector. The likelihood under this model has the generic form

$P (W 1:D |η) = D d=1 h(W d ) exp η T D d=1 t(W d ) -DA(η) .$The sum of vectors t(W d ) forms the sufficient statistics of the likelihood. The conjugate prior on η has a similar form

$P (η|χ, ν) = f (χ, ν) exp η T χ -νA(η) .$Conjugacy between these two distributions motivates selecting a q distribution in this same family to approximate the posterior of η,

$q(η|χ , ν ) = f (χ , ν ) exp η T χ -ν A(η) .$The variational parameters χ and ν are free and are modified to maximize the lower bound in Eq. ( [12](#formula_16)). 3  Inference proceeds by taking the gradient of L with respect to the variational parameters of a particular q, in this case the vector ψ := [χ T , ν ] T , and setting to zero to find their updated values. For the conjugate exponential example we are considering, this gradient is

$∇ ψ L(W, Ψ) = -   ∂ 2 ln f ∂χ ∂χ T ∂ 2 ln f ∂χ ∂ν ∂ 2 ln f ∂ν ∂χ T ∂ 2 ln f ∂ν 2   χ + d t d -χ ν + D -ν .(13)$3. A closed form expression for the lower bound is readily derived for this example.

Setting this to zero, one can immediately read off the variational parameter updates from the rightmost vector. In this case χ = χ + D d=1 t(W d ) and ν = ν + D, which are the sufficient statistics calculated from the data.

## A stochastic extension

Stochastic optimization of the variational lower bound modifies batch inference by forming a noisy gradient of L at each iteration. The variational parameters for a random subset of the data are optimized first, followed by a step in the direction of the noisy gradient of the global variational parameters. Let C s ⊂ {1, . . . , D} index a subset of the data at step s. Also let φ d be the hidden local variables associated with observation W d and let Φ W be the global variables shared among all observations. The stochastic variational objective function L s is the noisy version of L formed by selecting a subset of the data,

$L s (W Cs , Ψ) = D |C s | d∈Cs E Q [ln P (W d , φ d |Φ W )] + E Q [ln P (Φ W ) -ln Q].(14)$Optimizing L s optimizes L in expectation; since each subset C s is equally probable, with p(C s ) = D |Cs| -1 , and

$since d ∈ C s for D-1 |Cs|-1 of the D |Cs| possible subsets, it follows that E p(Cs) [L s (W Cs , Ψ)] = L(W, Ψ).$Stochastic variational inference proceeds by optimizing the objective in [(14)](#b13) with respect to ψ d for d ∈ C s , followed by an update to Ψ W that blends the new information with the old. The update of a global variational parameter ψ at step s is ψ s = ψ s-1 + ρ s B∇ ψ L s (W Cs , Ψ), where the matrix B is a positive definite preconditioning matrix and ρ s is a step size satisfying

$∞ s=1 ρ s = ∞ and ∞ s=1 ρ 2$s < ∞ to ensure convergence [[20]](#b19). The gradient ∇ ψ L s (W Cs , Ψ) has a similar form as Eq. ( [13](#formula_20)), with the exception that the sum is taken over a subset of the data. Though the matrix in Eq. ( [13](#formula_20)) is often very complicated, it is superfluous to batch variational inference for conjugate exponential family models. In the stochastic optimization of Eq. ( [12](#formula_16)), however, this matrix cannot be ignored. The key for conjugate exponential models is in selecting the preconditioning matrix B. Since the gradient of L s has the same form as Eq. ( [13](#formula_20)), B can be set to the inverse of the matrix in [(13)](#b12) to allow for cancellation. An interesting observation is that this matrix is

$B = - ∂ 2 ln q(η|ψ) ∂ψ∂ψ T -1 ,(15)$which is the inverse Fisher information of the variational distribution q(η|ψ). Using this setting for B, the step direction is the natural gradient of the lower bound, and therefore gives an efficient step direction in addition to simplifying the algorithm [[24]](#b23). The resulting variational update is a weighted combination of the old sufficient statistics for q with the new ones calculated over data indexed by C s .

Algorithm 2 Variational inference for the nHDP 1) Randomly subsample documents from the corpus.

2) For each document in the subsample, a) Select a subtree according to a greedy process on the variational objective (Eq. 16). b) Optimize q distributions for subtree.

Iterate between word allocation (Eq. 17) and topic distribution updates (Eqs. [[19]](#b18)[[20]](#b19)[[21]](#b20). 3) Collect the sufficient statistics for the topics and base distribution and step in the direction of the natural gradient (Eqs. [[22]](#b21)[[23]](#b22)[[24]](#b23)[[25]](#b24)[[26]](#b25)[[27]](#b26). 4) Return to Step 1.

## The inference algorithm

We develop a stochastic variational inference algorithm for approximate posterior inference of the nHDP topic model. As discussed in our general review of stochastic inference, this entails optimizing the local variational parameters for a subset of documents, followed by a step along the natural gradient of the global variational parameters. We distinguish between local and global variables for the nHDP in Table [2](#tab_1). In Table [2](#tab_1) we also give the variational q distributions selected for each variable. In almost all cases we select this distribution to be in the same family as the prior. We point out two additional latent indicator variables for inference: c d,n , which indicates the topic of word W d,n , and z (d) i,j , which points to the atom in G i associated with the jth break in G (d) i using the construction given in Eq. ( [9](#formula_11)). Since we wish to consider large trees, and because there is slightly more overhead in calculating the distribution for each document than in models such as LDA and the HDP, the word allocation step is more time consuming for the nHDP. Additionally, we seek an efficient means for learning the indicators z (d) i,j . Since each document will use a small subset of topics, which translates to a small subtree of the entire tree, our goal is to pick out a subtree in advance for the document to work with. This will reduce the number of topics to do inference over for each document, speeding up the algorithm, and determine the delta-function indicators for z (d) i,j , which point to the "activated" nodes. To this end, we introduce a third aspect to our inference algorithm in which we pick a small subtree for each document in advance. By this we mean that we only allow words in a document to be allocated to the subtree selected for that document and fix the probability that the indicator c d,n corresponds to topics outside this subtree to zero. As we will show, by selecting a subtree we are in effect learning a truncated stick-breaking construction of the tree for each document. If a node has two children in the subtree, then algorithmically we will have a two-node truncated construction for that DP of the specific document we are considering.

We select the subtree from T for each document using a greedy algorithm. This greedy algorithm is performed with respect to maximizing the variational objective function. Being an optimization method with one requirement (that we maximize a fixed objective), variational inference has considerable freedom in this regard. We discuss this greedy algorithm below, followed by the variational parameter updates for the local and global q distributions. Algorithm 2 gives an outline.

## Greedy subtree selection

As mentioned, we perform a greedy algorithm with respect to the variational objective function to determine a subtree from T for each document. We first describe the algorithm followed by a mathematical representation.

Starting from the root node, we sequentially add nodes from T , selecting from those currently "activated." An activated node is one whose parent is contained within the subtree but which is not itself in the subtree.

To determine which node to add, we look at which node will give the greatest increase in the variational objective when the q distributions for the documentspecific beta distributions are fixed to their priors and the variational distribution for each word's topic indicator q distribution (ν d,n in Table [2](#tab_1)) is zero on the remaining unactivated nodes. That is, we then ask the question: Which of the activated nodes not currently in the subtree will lead to the greatest increase in the variational objective under this restricted q distribution?

The reason we consider this restricted distribution is that there is a closed form calculation for each node, and so no iterations are required in this step and the algorithm is much faster. Calculating this score only involves optimizing the variational parameter ν d,n for each word over the current subtree plus the candidate node. We continue adding the maximizing node until the marginal increase in the objective falls below a threshold. We give a more formal description of this below. 4.2.1.1 Coordinate update for q(z

$(d) i,j ): As defined in Table 2, z (d)$i,j is the variable that indicates the index of the atom from the global DP G i pointed to by the jth stickbreaking weight in G (d) i . We select a delta q distribution for this variable, meaning we make a hard assignment for this value. These values also define the subtree for document d. Starting with an empty tree, all atoms in G i0 constitute the activated set. Adding the first node is equivalent to determining the value for z For a subtree of size t corresponding to document d, let the set I d,t contain the index values of the included nodes, let S d,t = {i : pa(i) ∈ I d,t , i ∈ I d,t } be the set of candidate nodes to add to T . Then provided the marginal increase in the variational objective is above a preset threshold, we increment the subtree by letting

$I d,t+1 ← I d,t ∪ i * , where i * = arg max i ∈S d,t N d n=1 max ν d,n : C d,t,i E q [ln p(W d,n |c d,n , θ)] + E q [ln p(c d,n , z (d) |V, V d , U d )]-E q [ln q(c d,n )]. (16)$We let C d,t,i denote the discussed conditions, that ν d,n (i) = 0 for all i ∈ I d,t ∪ i and that q(•) is fixed to the prior for all other distributions. The optimal values for ν d,n are given below in Eq. [(17)](#b16).

We note two aspects of this greedy algorithm. First, though the stick-breaking construction of the documentlevel DP given in Eq. ( [9](#formula_11)) allows for atoms to repeat, in this algorithm each additional atom is new, since there is no advantage in duplicating atoms. Therefore, the algorithm approximates each G (d) i by selecting and reordering a subset of atoms from G i for its stickbreaking construction. (The subtree T d may also contain zero atoms or one atom from a G i .) The second aspect we point out is the changing prior on the same node in T . If the atom θ (i,m) is a candidate for addition, then it remains a candidate until it is either selected by a z (d) i,j , or the algorithm terminates. The prior on selecting this atom changes, however, depending on whether it is a candidate for z (d) i,j or z (d) i,j . Therefore, incorporating a sibling of θ (i,m) impacts the prior on incorporating θ (i,m) .

## Coordinate updates for document variables

Given the subtree T d selected for document d, we optimize the variational parameters for the q distributions on c d,n , V (d) i,j and U d,i over that subtree. 4.2.2.1 Coordinate update for q(c d,n ): The variational distribution on the path for word W d,n is

$ν d,n (i) ∝ exp E q [ln θ i,W d,n ] + E q [ln π d,i ] ,(17)$where the prior term π d,i is the tree-structured prior of the nHDP,

$π d,i =   (i ,i)⊆i j V (d) i ,j m<j (1 -V (d) i ,m ) I(z (d) i ,j =i)   × U d,i i ⊂i (1 -U d,i ) . (18$$)$We use the notation i ⊂ i to indicate the subsequences of i starting from the first value. The expectation E q [ln θ i,w ] = ψ(λ i,w ) -ψ( w λ i,w ), where ψ(•) is the digamma function. Also, for a general random variable

$Y ∼ Beta(a, b), E[ln Y ] = ψ(a) -ψ(a + b) and E[ln(1 -Y )] = ψ(b) -ψ(a + b).$The corresponding values of a and b for U and V are given in their respective updates below. We note that this has a familiar feel as LDA, but where LDA uses a flat Dirichlet prior on π d , the nHDP uses a prior that is a tree-structured product of beta random variables. Though the form of the prior is more TABLE [1](#) A list of the local and global variables and their respective q distributions for the nHDP topic model.

## Global variables:

θ i : topic probability vector for node i q(θ i ) = Dirichlet(θ i |λ i,1 , . . . , λ i,V ) V i,j : stick proportion for the global DP for node i q(V i,j ) = Beta(V i,j |τ

$(1) i,j , τ(2)$i,j ) Local variables: V (d) i,j : stick proportion for local DP for node i q(V i,j ) = Beta(V

$(d) i,j |u (d) i,j , v (d) i,j ) z (d) i,j : index pointer to atom in G i for jth break in G (d) i q(z (d) i,j ) = δz. .,. (k), k = 1, 2, . . . U d,i : beta distributed switch probability for node i q(U d,i ) = Beta(U d,i |a d,i , b d,i ) c d,n : topic indicator for word n in document d q(c d,n ) = Discrete(c d,n |ν d,n ) complicated,$the independence results in simple closedform updates for these beta variables that only depend on ν d,n . 4.2.2.2 Coordinate update for q(V (d) i,j ): The variational parameter updates for the document-level stickbreaking proportions are u (d)

$i,j = 1 + i :(i,j)⊆i N d n=1 ν d,n (i ),(19)$$v (d) i,j = β + i :i⊂i I   m>j {z (d) i,m = i (l + 1)}   N d n=1 ν d,n (i ).$In words, the statistic for the first parameter is the expected number of words in document d that pass through or stop at node (i, j). The statistic for the second parameter is the expected number of words from document d whose paths pass through the same parent i, but then transition to a node with index greater than j according to the indicators z

$(d)$i,m from the document-level stick-breaking construction of G (d) i . 4.2.2.3 Coordinate update for q(U d,i ): The variational parameter updates for the switching probabilities are similar to those of the document-level stick-breaking process, but collect the statistics from ν d,n in a slightly different way,

$a d,i = γ 1 + N d n=1 ν d,n (i), (20$$)$$sb d,i = γ 2 + i :i⊂i N d n=1 ν d,n (i ). (21$$)$In words, the statistic for the first parameter is the expected number of words that use the topic at node i. The statistic for the second parameter is the expected number of words that pass through node i but do not terminate there.

## Stochastic updates for corpus variables

After selecting the subtrees and updating the local document-specific variational parameters for each document d in sub-batch s, we take a step in the direction of the natural gradient of the parameters of the q distributions on the global variables. These include the topics θ i and the global stick-breaking proportions V i l ,j .

4.2.3.1 Stochastic update for q(θ i ): For the stochastic update of the Dirichlet q distributions on each topic θ i , first form the vector λ i of sufficient statistics using the data in sub-batch s,

$λ i,w = D |C s | d∈Cs N d n=1 ν d,n (i)I{W d,n = w},(22)$for w = 1, . . . , V. This vector contains the expected number of words with index w that originate from topic θ i over documents indexed by C s . According to the discussion on stochastic inference in Section 4.1.2, we scale this to a corpus of size D. The update for the associated q distribution is

$λ s+1 i,w = λ 0 + (1 -ρ s )λ s i,w + ρ s λ i,w .(23)$We see a blending of the old statistics with the new in this update. Since ρ s → 0 as s increases, the algorithm uses less and less information from new sub-groups of documents, which reflects the increasing confidence in this parameter value as more data is seen. 4.2.3.2 Stochastic update for q(V i l ,j ): As with θ i , we first collect the sufficient statistics for the q distribution on V i l ,j from the documents in sub-batch s,

$τ i l ,j = D |C s | d∈Cs I{i l ∈ I d },(24)$$τ i l ,j = D |C s | d∈Cs j>i l I{(pa(i l ), j) ∈ I d }. (25$$)$The first value scales up the number of documents in sub-batch s that include atom θ (i,j) in their subtree; the second value scales up the number of times an atom of higher index value in the same DP is used by a document in sub-batch s. The update to the global variational parameters are

$τ (1) i l ,j (s + 1) = 1 + (1 -ρ s )τ (1) i l ,j (s) + ρ s τ i l ,j , (26) τ (2) i l ,j (s + 1) = α + (1 -ρ s )τ (2) i l ,j (s) + ρ s τ i l ,j . (27)$Again, we see a blending of old information with new.

## EXPERIMENTS

We present an empirical evaluation of the nested HDP topic model in the stochastic and the batch inference settings. We first present batch results on three smaller data sets to verify that our multi-path approach gives an improvement over the single-path nested CRP. We then move to the stochastic inference setting, where we perform experiments on 1.8 million documents from The New York Times and 2.7 million documents from Wikipedia. We compare with other recent stochastic inference algorithms for topic models: stochastic LDA [[9]](#b8) and the stochastic HDP [[10]](#b9). As is fairly standard with the optimization-based variational inference, we use truncated stick-breaking processes for all DPs [[25][26]](#). With this method, we truncate the posterior approximation by not allowing words to come from topics beyond the truncation index (i.e., fixing c d,n ((i, j)) = 0 for all j > n). The truncation is set to something reasonably large, and the posterior inference procedure then shrinks the number of used topics to something smaller than the number provided. In our large-scale experiments, we truncate to n 1 = 20 first level nodes, n 2 = 10 children for each of these nodes and n 3 = 5 children of each of these second level nodes. We consider three level trees, corresponding intuitively to "general", "specific" and "specialized" levels of words. Though the nHDP is nonparametric in level as well, we are more interested in the nonparametric aspect of the Dirichlet process here.

## Initialization

Before presenting our results, we discuss our method for initializing the topic distributions of the tree. As with most Bayesian models, inference for hierarchical topic models can benefit greatly from a good initialization. Our goal is to find a method for quickly centering the posterior mean of each topic so that they contain some information about their hierarchical relationships. We briefly discuss our approach for initializing the global variational topic parameters λ i of the nHDP. Using a small set of documents (e.g, 10,000) from the training set, we form the empirical distribution for each document on the vocabulary. We then perform k-means clustering of these probability vectors using the L 1 distance measure (i.e., total variation). At the top level, we partition the data into n 1 groups, corresponding to n 1 children of the root node from the truncated stickbreaking process. We then subtract the mean of a group (a probability vector) from all data within that group, set any negative values to zero and renormalize. We loosely think of this as the "probability of what remains"a distribution on words not captured by the parent distributions. Within each group we again perform kmeans clustering, obtaining n 2 probability vectors for each of the n 1 groups, and again subtracting, setting negative values to zero and renormalizing the remainder of each probability vector for a document.

Through this hierarchical k-means clustering, we obtain n 1 probability vectors at the top level, n 2 probability vectors beneath each top-level vector for the second level, n 3 probability vectors beneath each of these second-level vectors, etc. The n i vectors obtained from any sub-group of data are refinements of an already coherent sub-group of data, since that sub-group is itself a cluster from a larger group. Therefore, the resulting tree will have some thematic coherence. The clusters from this algorithm are used to initialize the nodes within the nHDP tree. For a mean probability vector λi obtained from this algorithm, we set the corresponding variational parameter for the topic Dirichlet distribution q to

$λ i = N (κ λi + (1 -κ)(1/V + v i )) for κ ∈ [0, 1], N a scaling factor and v i iid ∼ Dirichlet(1001 V /V).$This initializes the mean of θ i to be slightly peaked around λi , while the uniform vector and κ help determine the variance and v i provides some randomness. In our algorithms we set κ = 0.5 and N equal to the number of documents.

## A batch comparison

Before comparing our stochastic inference algorithm for the nHDP with similar algorithms for LDA and the HDP, we compare a batch version with the nCRP on three smaller data sets. This will verify the advantage of giving each document access to the entire tree versus forcing each document to follow one path. We compare the variational nHDP topic model with both the variational nCRP [[2]](#b1) and the Gibbs sampling nCRP [[4]](#b3), using the parameter settings in those papers to facilitate comparison. We consider three corpora for our experiments: (i) The Journal of the ACM, a collection of 536 abstracts from the years 1987-2004 with vocabulary size 1,539; (ii) The Psychological Review, a collection of 1,272 abstracts from the years 1967-2003 with vocabulary size 1,971; and (iii) The Proceedings of the National Academy of Science, a collection of 5,000 abstracts from the years 1991-2001 with a vocabulary size of 7,762. The average number of words per document for the three corpora are 45, 108 and 179, respectively.

As mentioned, variational inference for Dirichlet process priors uses a truncation of the variational distribution, which limits the number of topics that are learned [[25][26]](#). This truncation is set to a number larger than the anticipated number of topics necessary for modeling the data set, but can be increased if more are needed [[27]](#b26). We use a truncated tree of (10, 7, 5) for modeling

0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 -7.6 -7.55 -7.5 -7.45 -7.4 -7.35 -7.3 -7.25 Number of documents seen (x 10 6 ) Predictive log likelihood nHDP HDP LDA 250 LDA 150 LDA 50

Fig. [2](#). The New York Times: Average predictive log likelihood on a held-out test set as a function of training documents seen.

these corpora, where 10 children of the root node each have 7 children, which themselves each have 5 children for a total of 420 nodes. Because these three data sets contain stop words, we follow [[2]](#b1) and [[4]](#b3) by including a root node shared by all documents for this batch problem only. Following [[2]](#b1), we perform five-fold cross validation to evaluate performance on each corpus. We present our results in Table [2](#tab_1), where we show the predictive log likelihood on a held-out test set. We see that for all data sets, the variational nHDP outperforms the variational nCRP. For the two larger data sets, the variational nHDP also outperforms Gibbs sampling for the nCRP. Given the relative sizes of these corpora, we see that the benefit of learning a per-document distribution on the full tree rather than a shared distribution on paths appears to increase as the corpus size and document size increase. Since we are interested in the "Big Data" regime, this strongly hints at an advantage of our nHDP approach over the nCRP. We omit a comparison with Gibbs nHDP since MCMC methods are not amenable to large data sets for this problem.

## Stochastic inference for large corpora

We next present an evaluation of our stochastic variational inference algorithm on The New York Times and Wikipedia. These are both very large data sets, with The New York Times containing roughly 1.8 million articles and Wikipedia roughly 2.7 million web pages. The average document size is somewhat larger than those considered in our batch experiments as well, with an article from The New York Times containing 254 words on average taken from a vocabulary size of 8,000, and Wikipedia 164 words on average taken from a vocabulary size of 7,702. For this problem we remove stop words 99.9% 99% 95%

Fig. [3](#). New York Times: The total size of the tree as a function of documents seen. We show the smallest number of nodes containing 95%, 99% and 99.9% of the posterior mass. and rare words.

## Setup

We use the algorithm discussed in Section 5.1 to initialize a three-level tree with (20, 10, 5) child nodes per level, giving a total of 1,220 initial topics. For the Dirichlet processes, we set all top-level DP concentration parameters to α = 5 and the second-level DP concentration parameters to β = 1. For the switching probabilities U , we set the beta distribution hyperparameters for the tree level prior to γ 1 = 1/3 and γ 2 = 2/3, slightly encouraging a word to continue down the tree. We set the base Dirichlet parameter λ 0 = 0.1. For our greedy subtree selection algorithm, we stop adding nodes to the subtree

game play team going think season tournament seeded final second victory round cup team soccer americans match goal south club west spade bridge open golf woods tennis tour hole hockey season devils team games football season game quarterback bowl basketball season game team knicks season baseball game league series run inning hit game runs yankees torre sox red jeter mets valentine piazza franco shea points nets game scored half knicks ewing riley game patrick jordan bulls chicago michael jackson phil giants jets season parcells fassel yards touchdown game pass quarter dame notre bowl football season miami florida goal period scored game second penguins pittsburgh mario civic meadowlands rangers messier richter garden madison square board members public yesterday decision house senate republican bill congress senator budget agreement plan unions bills pay court judge justice supreme case decision lawyers case lawyer court judge charges health care medical doctors patients hospitals committee panel ethics chairman subcommittee legislature assembly governor legislative senate senate judiciary committee nomination roberts walsh oliver north iran contra trial jury case evidence crime general attorney spitzer office eliot agency enron lay gates webster union workers labor contract employees benefits rate percent discount previous index percent money government pay business economic agency advertising account media group billion deal companies business corporation largest computer technology system companies software market percent stock investors trading products food industry drug companies cars workers auto ford plant vehicles energy fuel oil gas prices electricity natural carbon power plants steel air electric emissions pollution service phone network telephone wireless internet research scientists institute national laboratory microsoft software windows operating gates sun sales stores retailers mart wal drink cola soft water coke bank interest loans rates debt fund net mutual money investors stocks real estate market mortgage brokers securities firms morgan stanley accounting companies communications telephone cable service boeing military aircraft pentagon defense percent treasury yield prices fed rates share offer shareholders stock bid takeover insurance savings loan estate real government officials foreign administration leaders political washington nations israel palestinian peace arafat minister lebanon syria beirut hezbollah christian hostages arab king egypt jordan hussein military nato troops forces defense aircraft fighter plane air jet nuclear missile arms defense treaty reagan space intelligence afghanistan bin laden taliban qaeda european germany france union east nato milosevic bosnia yugoslavia serbia tribunal switzerland nazi holocaust jewish banks gold soviet union gorbachev moscow communist nato europe poland missiles solidarity russia yeltsin moscow kremlin chechnya clinton treaty arms nuclear reagan range summit bush iraq administration war hussein council nations iraq security resolution saudi arabia bin laden osama nuclear weapons program inspectors chemical iran gulf persian oil shipping china north korea south beijing nuclear taiwan hong kong boat insurance return vietnam asia indonesia singapore cambodia

Fig. [6](#). Tree-structured topics from The New York Times. The shaded node is the top-level node and lines indicate dependencies within the tree. In general, topics are learning in increasing levels of specificity. For clarity, we have removed grammatical variations of the same word, such as "scientist" and "scientists."

when the marginal improvement to the lower bound falls below 10 -3 . When optimizing the local variational parameters of a document given its subtree, we continue iterating until the fractional change in the L 1 distance of the empirical distribution of words falls below 10 -2 .

We hold out a data set for each corpus for testing, 14,268 documents for testing The New York Times and 8,704 documents for testing Wikipedia. To quantitatively assess the performance, at various points in the learning process we calculate the predictive log likelihood on a fraction of the test set as follows: Holding the top-level variational parameters fixed, for each test document we randomly partition the words into a 90/10 percent split. We then learn document-specific variational parameters for the 90% portion. Following [[28]](#b27)[2], we use the mean of each q distribution to form a predictive distribution

0 .5 1 1.5 2 2.5 -6.95 -6.9 -6.85 -6.8 -6.75 -6.7 -6.65 -6.6 -6.55 -6.5 Number of documents seen (x 10 6 ) Predictive log likelihood nHDP HDP LDA 250 LDA 150 LDA 50 Fig. 7. Wikipedia: Average predictive log likelihood on a held-out test set as a function of training documents seen.

for the remaining words of that document. With this distribution, we calculate the average predictive log likelihood of the 10% portion to assess performance. For comparison, we evaluate stochastic inference algorithms for LDA and the HDP in the same manner. In all algorithms, we use an algorithm for adaptively learning the step size ρ s as presented by Ranganath, et al. [[29]](#b28).

## The New York Times

We first present our results for The New York Times.

In Figure [2](#) we show the average predictive log likelihood on unseen words as a function of the number of documents processed during model learning. We see an improvement in performance as the amount of data processed increases. We also note an improvement in the performance of the nHDP compared with LDA and the HDP. In Figure [3](#) we give a sense of the size of the tree as a function of documents seen. Since all topics aren't used equally, we show the minimum number of nodes containing 95%, 99% and 99.9% of all data in the posterior. In Figure [4](#) we show document-level statistics from the test set at the final step of the algorithm. These include the word allocations by level and the number of topics used per level. We note that while the tree has three levels, roughly 12 topics are being used (in varying degrees) per document. This is in contrast to the three topics that would be available to any document with the nCRP. Thus there is a clear advantage in allowing each document to have access to the entire tree. We show the adaptively learned step size in Figure [5](#).

In Figure [6](#) we show example topics from the model and their relative structure. For each node we show the most probable words according to the approximate posterior q distribution of the topic. We show four topics from the top level of the tree (shaded), and connect 99.9% 99% 95%

Fig. [8](#). Wikipedia: The total size of the tree as a function of documents seen. We show the smallest number of nodes containing 95%, 99% and 99.9% of the posterior mass. topics according to parent/child relationship. The model learns a meaningful hierarchical structure; for example, the sports subtree branches into the various sports, which themselves appear to branch by teams. In the foreign affairs subtree, children tend to group by major subregion and then branch out into subregion or issue. If a sports document incorporated topics on foreign affairs, the nHDP would allow words to split into both parts of the tree, but with the nCRP a document would have to pick one or the other, and so a tree could not be learned that distinguished topics with this level of precision.

The algorithm took roughly 20 hours to make one pass through the data set using a single desktop computer, which was sufficient for the model to converge to a set of topics. Runtime for Wikipedia was comparable.

new, area, city, first, built, located, history local, site, years known, large home building, house historic, hall museum, tower long, architecture church, century, cathedral, chapel, stone, wall, castle, built, walls, parish, monument, gothic time, death, first, years, two, made, life, died war, battle, army, forces, troops, attack, force, killed, men, commander british century, roman, king, empire, greek, ancient, name, history leader, family, males, king, household, son, married, lord castle, ireland, cromwell, dublin, english, protestant system, time, based, different, first, control, new castle work, first, life, published, new, years, works, time, world, wrote, known, book, history students, education, research, science, learning, development, school, program, information, international, study, computer system, time, based, different, first, control, new, number, process function, two, value, space, defined, functions, form, case, field, group, order, called, point, number, real, first, elements, definition, theory, sum data, code, computer, information, call, command, object, user, telephone, communication, analysis, calls, read, regular, web language, languages, written, latin, hypertext, writing, translation theory, theories, view, concept, principle, knowledge, science, circle, phenomena, observer, experiment computer science 

## Wikipedia

We show similar results for Wikipedia as for The New York Times. In Figures 7, 8 The New York Times. We again see an improvement in performance for the nHDP over LDA and the HDP, as well as the increased usage of the tree with the nHDP than would be available in the nCRP.

In Figure [11](#fig_5), we see example subtrees used by three documents. We note that the topics contain many more function words than for The New York Times, but an underlying hierarchical structure is uncovered that would be unlikely to arise along one path, as the nCRP would require. As with The New York Times, we see the nonparametric nature of the model in Figure [8](#). Though the model has an 1,220 initial nodes, a small subset are ultimately used by the data.

## Sensitivity analysis

We present a brief sensitivity analysis of some parameters of the nHDP topic model using the Wikipedia corpus. Fig. [12](#fig_0). Wikipedia: Sensitivity to parameter vector (γ 1 , γ 2 ) for the stochastic switches. We show the results from Figure [9](#) for different settings with β = 1. In general, we find that the results were not sensitive to the parameter λ 0 of the base Dirichlet distribution, which is consistent with [[8]](#b7). We note that this is typically not the case for topic models, but because of the massive quantity of data we are working with, the data overwhelms the prior in this case. This was similarly found with the global DP parameter α. The document-specific variables have a more significant impact since they only use the data from a single document in their posteriors. In Figures [12](#fig_0)[13](#fig_0)[14](#fig_0)we show the sensitivity of the model to the parameters β and (γ 1 , γ 2 ). We consider several values for these parameters, holding γ 1 + γ 2 = 1. As can be seen, the model structure is fairly robust to these values. The tree structure does respond as would be expected from the prior, but there is no major change. The quantitative results in Figure [14](#fig_0) indicate that the quality of the model is robust as well. We note that this relative insensitivity is within a parameter range that we believe a priori to be reasonable.

## CONCLUSION

We have presented the nested hierarchical Dirichlet process (nHDP), an extension of the nested Chinese restaurant process (nCRP) that allows each observation to follow its own path to a topic in the tree. Starting with a stick-breaking construction for the nCRP, the new model samples document-specific path distributions for a shared tree using a nested hierarchy of Dirichlet processes. By giving a document access to the entire tree, we are able to borrow thematic content from various parts of the tree in constructing a document. We developed a stochastic variational inference algorithm that is scalable to very large data sets. We compared the stochastic nHDP topic model with stochastic LDA and HDP and showed how the nHDP can learn meaningful topic hierarchies.

![in general, creating a subtree for T d , which we denote as T d , is equivalent to determining which z (d) i,j to include in T d and the atoms to which they point.]()

![Fig.4. The New York Times: Per-document statistics from the test set using the tree at the final step of the algorithm. (left) The average number of words per tree level. (right) The average number of nodes per level with more than one expected observation.]()

![Fig. 9. Wikipedia: Per-document statistics from the test set using the tree at the final step of the algorithm. (left) The average number of words per tree level. (right) The average number of nodes per level with more than one expected observation.]()

![Fig. 11. Examples of subtrees for three articles from Wikipedia. The three sizes of font indicate differentiate the more probable topics from the less probable.]()

![, 9 and 10 we show results corresponding to Figures2, 3, 4 and 5, respectively for]()

![Fig.13. Wikipedia: Sensitivity to parameter vector β for the local DPs. We show the results from Figure9for different settings and γ 1 = 1/3, γ 2 = 2/3.]()

![Comparison of the nHDP with the nCRP in the batch inference setting using the predictive log likelihood.]()

This includes a root node topic, which is shared by all documents and is intended to collect stop words.

