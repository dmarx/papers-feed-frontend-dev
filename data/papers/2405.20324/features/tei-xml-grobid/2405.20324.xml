<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Don&apos;t drop your samples! Coherence-aware training benefits Conditional diffusion</title>
				<funder ref="#_SCfNyf9 #_ZcfKRV2">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-02-18">18 Feb 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Nicolas</forename><surname>Dufour</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">√âcole des Ponts</orgName>
								<orgName type="laboratory">LIGM</orgName>
								<orgName type="institution" key="instit1">Univ Gustave Eiffel</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<settlement>Marne-la-Vall√©e</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">LIX</orgName>
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">√âcole Polytechnique</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Victor</forename><surname>Besnier</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Vicky</forename><surname>Kalogeiton</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">LIX</orgName>
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">√âcole Polytechnique</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Picard</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">√âcole des Ponts</orgName>
								<orgName type="laboratory">LIGM</orgName>
								<orgName type="institution" key="instit1">Univ Gustave Eiffel</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<settlement>Marne-la-Vall√©e</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">I</forename><forename type="middle">P</forename><surname>Paris</surname></persName>
						</author>
						<author>
							<persName><roleName>Prague</roleName><forename type="first">Valeo</forename><surname>Ai</surname></persName>
						</author>
						<title level="a" type="main">Don&apos;t drop your samples! Coherence-aware training benefits Conditional diffusion</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-02-18">18 Feb 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">C6BFE86402A5E23653BB0D63FC174CA4</idno>
					<idno type="arXiv">arXiv:2405.20324v2[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Figure 1. Images generated from our model, CAD. Our model showcase high visual quality, aesthetics and prompt following.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Conditional Diffusion models excel in image generation while affording greater user control over the generation process by integrating additional information <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b45">46]</ref>. This extra data enables the model to guide the generated image towards a specific target, leading to improved various applications including high-quality text-to-image generation <ref type="bibr" target="#b44">[45]</ref>, as well as other modalities such as depth or human body pose <ref type="bibr" target="#b60">[61]</ref>. Furthermore, the accessibility of open-source models like Stable Diffusion has democratized the use of this technology, already causing significant shifts in various domains such as design, art, and marketing.</p><p>Training conditional diffusion models requires substantial volumes of paired data comprising the target image and its corresponding condition. In text-to-image generation, this pairing involves an image and a descriptive caption that characterizes both the content and the style of the image. Similarly, for class conditional generation, the pair consists of an image and its corresponding class label. Besides the technical challenges associated with the acquisition of extremely large quantities of paired data, ensuring accurate alignment between image and text conditions is still an open research question in the community, as attested by the large amount of recent work in the domain <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b58">59]</ref>. In practice, large web- The score varies from 0 (no coherence) to 1 (maximum coherence). Higher coherence scores tend to generate images that adhere more effectively to the prompt. Top prompt: "a raccoon wearing an astronaut suit. The racoon is looking out of the window at a starry night; unreal engine, detailed, digital painting,cinematic,character design by pixar and hayao miyazaki, unreal 5, daz, hyperrealistic, octane render", bottom prompt: "An armchair in the shape of an avocado" (b) Increasing the coherence from 0 to 1, CLIPScore increases and FID decreases.</p><p>scraped datasets, such as LAION-5B <ref type="bibr" target="#b47">[48]</ref> or CC12M <ref type="bibr" target="#b3">[4]</ref>, contain abundant noisy pairs due to their collecting process. To clean the pairs, hence ensuring alignment of higher quality, the prevailing strategy filters out samples that fail to meet an arbitrarily chosen criterion, often done through techniques like thresholding the CLIP-score <ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref>. This approach, however, has two main drawbacks: first, it is challenging to adjust the criterion accurately and more importantly, it discards many high-quality samples that could potentially enhance generation quality irrespective of the condition. For instance, out of the 50B initially-collected text-image pairs, only 10% were left in LAION-5B <ref type="bibr" target="#b47">[48]</ref>, thus discarding 90% of the samples, i.e. 45B images.</p><p>Instead of discarding the vast majority of training samples, in this work, we leverage them to learn simultaneously conditional and unconditional distributions. Specifically, we introduce a novel approach that estimates what we call the coherence score, which evaluates how well the condition corresponds to its associated image. We incorporate this coherence score into the training process by embedding it into a latent vector, which is subsequently merged with the condition. This additional information enables the diffusion model to determine the extent to which the condition should influence the generation of a target image. During inference, our method has the flexibility to take as input the coherence score, thereby allowing users to vary the impact of the condition on the generation process, as illustrated in Figure ??. In addition, to further improve the generated image quality, we refine the Classifier-Free-Guidance method (CFG) introduced in <ref type="bibr" target="#b20">[21]</ref> by leveraging the gap between high and low coherence scores.</p><p>We evaluate our approach across three distinct tasks that involve various types of conditioning: text for text-to-image generation, labels for class-conditioned image generation, and semantic maps for paint-by-word image generation. In text conditioning, we use the CLIP score <ref type="bibr" target="#b40">[41]</ref> to estimate the coherence between the image and its accompanying caption.</p><p>For class-conditional generation, we employ an off-the-shelf confidence estimator to gauge the coherence between the image and its label. Concerning semantic maps, we derive pixel-level coherence scores either by automatically generating them based on class boundaries or by using an off-theshelf confidence estimator. Our evaluations span multiple datasets such as COCO <ref type="bibr" target="#b31">[32]</ref> for zero-shot text-to-image generation, ImageNet <ref type="bibr" target="#b9">[10]</ref> for class-conditioned generation, and ADE-20K <ref type="bibr" target="#b61">[62]</ref> for semantic maps. Our results show that including the coherence score in the training process allows training diffusion models with better image quality and that are more coherent with what they are prompted. In summary, our contributions can be outlined as follows: ‚úì Innovative Training Approach: We present coherenceaware diffusion (CAD), a novel method for training conditional diffusion models in the presence of annotation imperfections. By incorporating a coherence score between the target image and its associated condition, our model can adapt and fine-tune the influence of the condition on the generation process. ‚úì Flexible Inference Scheme: We introduce a versatile inference scheme, in which manual tuning of the coherence score during the generation enables the modulation of the condition's impact on image generation. Additionally, we refine the classifier-free guidance method under this new inference scheme, resulting in enhanced image quality. ‚úì Wide Applicability: Demonstrating the versatility of CAD, we evaluate it across three diverse tasks involving different types of conditions (text, class labels, and semantic maps). CAD produces visually pleasing results across all tasks, emphasizing its generic applicability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Conditional generation. Previous attempts to condition generative models were focused on GANs <ref type="bibr" target="#b14">[15]</ref>. Classconditional <ref type="bibr" target="#b33">[34]</ref> was the first way to introduce conditioning in generative models. This is a simple global conditioning. However, it lacks control over the output. To increase the control power of conditioning, more local conditionings were proposed such as drawings, maps, or segmentation maps <ref type="bibr" target="#b24">[25]</ref>. Segmentation maps conditioning <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b62">63]</ref> propose the most control to the user. Indeed, the user can not only specify the shape of the objects but also per-object class information. Semantic masks are however tedious to draw, which impacts usability. Text-conditioned models <ref type="bibr" target="#b59">[60]</ref> offer a compromise. They can provide both global and local conditioning and are easy to work with. Recently, diffusion models have made great advances in this domain. Diffusion models. Diffusion models <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b49">[50]</ref><ref type="bibr" target="#b50">[51]</ref><ref type="bibr" target="#b51">[52]</ref> have recently attracted the attention of research in image generation. Compared to GANs, they have better coverage over the data distribution, are easier to train and outperform them in terms of image quality <ref type="bibr" target="#b10">[11]</ref>. Architecture-wise, diffusion models rely mostly on modified versions of a U-Net <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b50">51]</ref>. Recent works have however shown that other architectures are possible <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b37">38]</ref>. In particular, RIN <ref type="bibr" target="#b25">[26]</ref> proposes a much simpler architecture than the U-Net achieving more efficient training. They recently have been a lot of works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b45">46]</ref> scaling up these models on huge text-to-image datasets <ref type="bibr" target="#b47">[48]</ref>. Stable Diffusion <ref type="bibr" target="#b44">[45]</ref>, Stable Diffusion XL <ref type="bibr" target="#b39">[40]</ref>, Paella <ref type="bibr" target="#b42">[43]</ref> or Wuerstchen <ref type="bibr" target="#b38">[39]</ref> have provided open-source weights for their networks, which has allowed an explosion in image generation. ControlNet <ref type="bibr" target="#b60">[61]</ref> has shown that fine-tuning these models allows for very fine-grained control over the output with lots of different conditioning modalities. Recently, consistency models <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b52">53]</ref> have shown that by training more with a different loss, inference can be done in small amounts of steps (2-4 steps). All these text-to-image networks have been tuned on very noisy web-scrapped data. We argue in this paper that this noise causes limitations in the training. Concurrent works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b48">49]</ref> propose to tackle this task through re-captioning, but this requires lots of resources to train a good captioner that outputs detailed captions without hallucinating details. As shown by <ref type="bibr" target="#b2">[3]</ref>, it also requires bridging the gap between train and test time prompts. Instead, our approach is much simpler in current training setups. Learning with noisy conditioning has been widely explored when considering classification. For binary classification, <ref type="bibr" target="#b35">[36]</ref> study machine learning robustness when confronted with noisy labels, while <ref type="bibr" target="#b23">[24]</ref>  sively positive labels accompanied by confidence scores. To bring a more practical perspective, <ref type="bibr" target="#b1">[2]</ref> introduced instancedependent noise scored by confidence, where this score aligns with the probability of the assigned label's accuracy. The negative impact of noisy labels has been mitigated with changes in architecture <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14]</ref>, in the loss <ref type="bibr" target="#b43">[44]</ref> or filtering the noisy samples <ref type="bibr" target="#b17">[18]</ref>. More recently, <ref type="bibr" target="#b26">[27]</ref> propose a similar approach to ours by conditioning an image captioner model by the CLIP-score to mitigate the impact of text misalignment. Instead, we focus on image synthesis, where we condition the diffusion model with a coherence score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Coherence-Aware Diffusion (CAD)</head><p>In this work, we want to improve the training of the diffusion model in the presence of misaligned conditioning. We make the assumption that for each training sample, a coherence score measures how coherent the conditioning is with respect to the data. We propose to condition the diffusion model on this coherence score in addition to the original condition. By doing so, the model learns to discard the lowcoherence conditions and focus on the high-coherence ones. Consequently, our model can behave as either a conditional or an unconditional model. Low-coherence samples, lead to unconditional sampling, while high-coherence samples lead to conditional samples. Building on this, we redesign classifier-free guidance to rely on coherence conditioning instead of dropping out the conditioning randomly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Conditional Diffusion Models</head><p>We first provide an overview of conditional diffusion models. These models learn to denoise a target at various noise levels. By denoising sufficiently strong noises, we eventually denoise pure noise, which can then be used to generate images. Each diffusion process is associated with a network œµ Œ∏ , which performs the denoising task. To train such a network, we have X an image and y its associated conditioning coming from p data the data distribution. We use a noise scheduler Œ≥(t), which defines X t which is the input image corrupted with Gaussian noise at the t-th step of diffusion. such as X t = Œ≥(t)X + 1 -Œ≥(t)œµ, where œµ ‚àº N (0, 1) the noise we want to predict and t ‚àà [0, 1] the diffusion timestep. During training, conditioning is provided to œµ Œ∏ . The objective of the diffusion model is to minimize the following loss:</p><formula xml:id="formula_0">L simple = E (X,y)‚àºpdata,t‚àºU [0,1] [‚à•œµ -œµ Œ∏ (X t , y, t)‚à•] ,<label>(1)</label></formula><p>where ‚à•‚Ä¢‚à• denotes the L 2 norm. One observation is that the conditioning is implicitly learned by the diffusion model, as the diffusion loss is only enforced on the image and not on the conditioning itself. This motivates our hypothesis that removing data with low label coherence can harm the training of the diffusion model. Even if the conditioning is not well aligned, the image still belongs to the distribution that we aim to learn. By discarding such data, we weaken the distribution estimator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Integrating label information into the diffusion model</head><p>We assume that for every datapoint (X, y) we have an associated c, the coherence score of y where c ‚àà [0, 1]. Our goal is to incorporate label coherence into the diffusion model to discard only the conditioning that contains low levels of coherence while continuing to train on the image. A value of c=1 indicates that y is the best possible annotation for X, while c=0 suggests that y is a poor annotation for X.</p><p>To achieve this, we modify the conditioning of the diffusion model œµ Œ∏ to include both y and c, using the following loss:</p><formula xml:id="formula_1">L simple = E (X,y,c)‚àºpdata,t‚àºU [0,1] [‚à•œµ -œµ Œ∏ (X t , y, c, t)‚à•] .</formula><p>(2) We refer to this kind of models as coherence coherenceaware diffusion (CAD) models. By informing the diffusion model of the coherence score associated with samples, we avoid filtering out low-confident samples and let the model learn by itself what information to take into account. Avoiding the filtering allows us to still learn X even in the presence of noisy labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Test-time prompting</head><p>After training a model with different levels of coherence, we can thus prompt it with varying degrees of coherence. When we prompt with minimal coherence, we obtain an unconditional model. On the other hand, when we prompt with maximal coherence, we get a model that is very confident about the provided label. However, like any other conditional diffusion model relying on attention, there is no guarantee that the label is actually used.</p><p>To strengthen the use of the label, we propose a modification to the Classifier Free Guidance (CFG) method <ref type="bibr" target="#b20">[21]</ref> that leverages the coherence. CFG uses both a conditional and unconditional model to improve the quality of generated samples. To learn such models, a conditional diffusion model is used and the conditioning is dropped out for a portion of the training samples. The original CFG formulation is as follows:</p><formula xml:id="formula_2">ŒµŒ∏ (x t , y) = œµ Œ∏ (x t , y) + œâ(œµ Œ∏ (x t , y) -œµ Œ∏ (x t , ‚àÖ)) , (3)</formula><p>with œâ the guidance rate. Instead, we propose a coherenceaware version of CFG (CA-CFG): ŒµŒ∏ (x t , y) = œµ Œ∏ (x t , y, 1) + œâ(œµ Œ∏ (x t , y, 1) -œµ Œ∏ (x t , y, 0)) .</p><p>(4) This modification removes the need to dropout the conditioning. Instead, we directly use the noise in the conditioning to drive the guidance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we will analyze 3 tasks: text, class, and semantically conditioned image generation. We describe the experimental setup, and analyze quantitative and qualitative results to better understand the inner workings of Coherenceaware diffusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental setup and Metrics</head><p>Experimental setup. For text-conditional image generation, we use a modified version of RIN <ref type="bibr" target="#b25">[26]</ref> (See Figure <ref type="figure" target="#fig_2">4</ref>). To map the text to an embedding space, we use a frozen FLAN-T5 XL <ref type="bibr" target="#b8">[9]</ref>. We then map the embedding with 2 selfattention transformer layers initialized with LayerScale <ref type="bibr" target="#b54">[55]</ref> COCO-10K</p><p>Method œâ FIDCLIP ‚Üì CLIPScore ‚Üë PCLIP ‚Üë RCLIP ‚Üë DCLIP ‚Üë CCLIP ‚Üë Baseline 10 91.9 25.96 0.281 0.047 0.181 0.222 Weighted 5 98.3 25.15 0.192 0.046 0.111 0.155 Filtered 10 85.8 26.52 0.281 0.061 0.175 0.233 CAD-S (Ours) 15 69.4 26.16 0.373 0.078 0.265 0.315 CAD-B (Ours) 15 55.3 27.04 0.462 0.172 0.349 0.428 CAD-B 512px (Ours) 15 54.3 25.52 0.593 0.148 0.545 0.472 and 16 registers. The text tokens are mapped with the coherence score using these layers, which is the same for every Text RIN block. Unlike the class conditional RIN block, the latent branch contains only the latents without concatenated tokens. Instead, the mapped text tokens, coherence, and timestep embeddings provide information to the latent branch with a cross-attention layer at the beginning of each Text RIN block. The rest of the architecture follows the standard RIN block design. The CAD-S model has 188M parameters and we train it for 240K steps. We train these models on a mix of datasets composed of CC12M <ref type="bibr" target="#b4">[5]</ref> and LAION Aesthetics 6+ <ref type="bibr" target="#b47">[48]</ref>. We also train a 332M parameters version for 500k steps that we name CAD-B. We also finetune it for 200K steps at 512px resolution. To estimate the coherence score, we use MetaCLIP H/14 <ref type="bibr" target="#b56">[57]</ref> that we then bin into 8 equally distributed discrete bins. We then use the normalized index between 0 and 1 as the coherence score. We compare our method to 3 baselines: "Baseline" is a model where we just train without coherence, "Filtered" corresponds to a model where we discard the 3 less coherent bins, and "Weighted" corresponds to a model where we weight the loss of the model by the normalized coherence of the sample. When using Coherent Aware prompting, we sample a random set of characters that we use with coherence-score of zero as the negative prompt.</p><p>For the class-conditional image generation experiments, we rely on RIN <ref type="bibr" target="#b25">[26]</ref> and use the same hyperparameters as the authors. We experiment on conditional image generation for CIFAR-10 <ref type="bibr" target="#b27">[28]</ref> and Imagenet-64 <ref type="bibr" target="#b9">[10]</ref> for which we artificially noise the label. We extract the coherence score from pre-trained classifiers in the following way: We re-sample with some temperature Œ≤ a new label from the label distribution predicted by the classifier. We then consider the entropy of the distribution as the coherence score. After, we use a sinusoidal positional embedding <ref type="bibr" target="#b55">[56]</ref> that we map with an MLP. We add this coherence token in the latent branch of RIN, similar to the class token.</p><p>For semantic segmentation conditioned experiments, we use ControlNet <ref type="bibr" target="#b60">[61]</ref> to condition a pre-trained text-to-image Stable-Diffusion <ref type="bibr" target="#b44">[45]</ref> model with both semantic and coherence maps concatenated. The training and evaluation of our method are performed on the ADE20K dataset <ref type="bibr" target="#b61">[62]</ref>, a large-scale semantic segmentation dataset containing over 20,000 images with fine-detailed labels, covering diverse scenes and classes. Since captions are not available for this dataset, we use BLIP2 <ref type="bibr" target="#b29">[30]</ref> to generate captions for each image in the dataset similarly to <ref type="bibr" target="#b60">[61]</ref>. We use a pre-trained Maskformer <ref type="bibr" target="#b6">[7]</ref> on the COCO Dataset <ref type="bibr" target="#b31">[32]</ref>, to extract the segmentation map and its associated confidence (MCP <ref type="bibr" target="#b18">[19]</ref>) for each image in the ADE20k dataset <ref type="foot" target="#foot_0">1</ref> . We use confidence as our coherence score. More details about the experimental setup are available in the supplementary.</p><p>Metrics. To evaluate image generation for all condition types, we use the Frechet Inception Distance <ref type="bibr" target="#b19">[20]</ref> (FID) that evaluates image quality. We also use Precision <ref type="bibr" target="#b28">[29]</ref> (P), Recall <ref type="bibr" target="#b28">[29]</ref> (R), Density <ref type="bibr" target="#b34">[35]</ref> (D) and Coverage <ref type="bibr" target="#b34">[35]</ref> (C) as manifold metrics, allowing us to evaluate how well the manifold of the generated images overlaps with the manifold of the real images. For text-conditional, we also compute the CLIP Score <ref type="bibr" target="#b40">[41]</ref> and evaluate metrics on CLIP features on a 10K samples subset of COCO <ref type="bibr" target="#b31">[32]</ref> in a zero-shot setting.</p><p>For class-conditional, we compute the Inception Score <ref type="bibr" target="#b46">[47]</ref> (IS). We also add the Accuracy (Acc) metric aiming at evaluating how well the image generator takes into account the conditioning and defined as Acc(g</p><formula xml:id="formula_3">) = E c‚ààCat(N ) [1 f (g(c))=c ],</formula><p>where g(.) is the generator we want to evaluate, f (.) is a classifier, and Cat(N ) is the categorical distribution of N labels.</p><p>For CIFAR-10, we use a Vision Transformer <ref type="bibr" target="#b11">[12]</ref> trained on CIFAR-10, and for ImageNet, we use a DeiT <ref type="bibr" target="#b53">[54]</ref>. For segmentation, instead of Accuracy, we compute the mean Intersection over Union (mIoU) instead of the Accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Analysis</head><p>Coherence conditioning. Here, we explore the behavior of our proposed coherence-aware diffusion model at test time.</p><p>For the text conditional setting, we observe from Figure <ref type="figure" target="#fig_0">2b</ref> that the coherence and the quality of the generated image increase as the coherence increases. Indeed, FID decreases and the CLIPScore increases. In the class-conditional setup, we prompt a CAD model trained on resampled ImageNet and report results in Table <ref type="table">7</ref>(a). Similar to the previous setting, when the model has very high coherence, it achieves the best FID and accuracy. However, when the coherence score decreases, the accuracy decreases as well and drops to 1 N when the coherence goes to 0. This validates our hypothesis that in the presence of low-coherence samples, our proposed model behaves like an unconditional model. Furthermore, even if the FID increases, it remains close to the FID of the conditional model, which implies that our CAD samples images are close to the training distribution.</p><p>Qualitatively, for text-to-image generation, we prompt the model with varying coherence scores from 0 to 1 and display results in Figure <ref type="figure" target="#fig_0">2a</ref>. We observe that when the coherence increases, the outputs are close to the prompt. For instance, in the bottom figure, the generated image displays an avocado armchair, where avocado and armchair are successfully mixed. Even a more complex prompt, like the raccoon at the top, follows closely the textual description. The raccoon does wear an astronaut suit and is looking through the window at a starry night. Similarly, as the coherence decreases, the images start to diverge from the original prompt. The avocado chair starts to first lose the "avocado" traits until there is only a chair and at the end an object that does not look like an avocado or a chair. At the top, we first lose the window, then the raccoon. We note that contrary to class conditional (as seen below), we do not converge to a totally random image. Instead, some features from the prompt are preserved, such as the racoon but the global structure of the image gets lost. This is highly linked to the CLIP network biases, which may pay less attention to less salient parts of an image such as the background, and are more sensitive to the main subject.</p><p>Similarly, for class-conditional, we prompt a CAD model trained on resampled ImageNet, with different coherences and classes. We sample with DDPM but we seed the sampling to have the same noise when sampling different classes. Table <ref type="table">7</ref> (c) illustrates the results, where we observe that when prompted with high coherence, CAD samples have the desired class. However, as the coherence decreases, the samples get converted into samples from random classes. Furthermore, samples that use the same sampling noise converge towards the same image in the low coherence regime. This shows that when the label coherence is low, CAD discards the conditioning and instead samples unconditionally.</p><p>To better understand the underlying mechanism, we design the following experiment. We modify the proposed model so that instead of adding both a class and a coherence token to the RIN network, we merge them into a single token with an MLP. Figure <ref type="figure" target="#fig_5">7</ref> (e) displays the t-SNE plots of the output of the MLP for every class in CIFAR-10 for which we compute different coherence scores. In the plot, high coherence translates to low transparency. We observe that as the coherence decreases, the embeddings of all classes tend to converge into the same embedding (center). This corrob- orates our hypothesis that the model uses the coherence to learn by itself how much to rely on the label.</p><p>Coherence-aware classifier-free guidance. Here, we examine the impact of the coherence-aware classifier guidance.</p><p>For this, we first compute different guidance rates ranging from 0 to 30 with 250 steps of DDIM, and then we plot the FID vs the CLIPScore the classifier accuracy for different rates. Figure <ref type="figure" target="#fig_5">7</ref> (b) illustrates this. We observe a trade-off between classifier accuracy and FID. Specifically, the more we increase the guidance, the more the accuracy increases, but at the cost of higher FID.</p><p>Qualitatively, this behaviour is also present in Figure <ref type="figure" target="#fig_4">6</ref>: at a lower guidance rate, the images are more diverse but at the cost of lower accuracy with respect to the class. This pattern is best shown in the Malamute example when œâ = 20 (first part in Figure <ref type="figure" target="#fig_4">6</ref>), where all malamutes have a similar pose with their tongues hanging and similarly, and in the third part where all ice-creams look similar, i.e., one white ice-cream scoop with red fillings.</p><p>Interestingly, we also observe that some guidance leads to optimal results. In Figure <ref type="figure" target="#fig_5">7</ref>, when œâ = 1, the FID is at its lowest point, and the accuracy is higher than the default model that has œâ = 0. This is also shown in Figure <ref type="figure" target="#fig_4">6</ref>, where, when œâ = 1, samples best combine diversity and fidelity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results</head><p>In this section, we report image generation results conditioned on text, class, and semantic maps.</p><p>Qualitative results for text-conditioned image generation. In Figure <ref type="figure" target="#fig_1">3</ref>, we observe that coherence-aware diffusion for textual conditioning allows for better prompt adherence and better-looking images. In the first row, we observe that CAD is the only method that captures the details of the prompt, such as having the wolf play the guitar. Indeed, the baseline and filtered models output a wolf, but most generations display only a head. The weighted model performs slightly better but it lacks quality. Furthermore, our model displays higher diversity in the output styles. For instance, this is visible in the bottom row where our model displays a variety of street images whereas the other methods tend to have a collapsed output. For this prompt, our model also displays better image quality compared to other methods.</p><p>Quantitative results for text-conditioned image generation. In Figure <ref type="figure" target="#fig_3">5</ref> (c), we compare our method to a classical diffusion model (baseline), to a method where all samples with CLIPScore lower than 0.41 are filtered as is commonly done with LAION-5B (Filtered), and one where the diffusion loss is wheighted by the CLIPScore of the sample</p><p>Semantic map ùëê = 1 Softmax ùúé ùúé + edit ùúé + edit ùúé + edit Canny edges ùëù ="A room with exposed brick wall and a radiator" ùëù + "and a cat in a box" ùëù + "and two tiny children (a) Examples of image generation conditioned on a semantic map. ADE20k COCO Metric Baseline CAD Baseline CAD FID 33.67 30.88 20.1 18.1 mIoU 22.6 23.7 35.1 35.3 P 0.785 0.844 0.7876 0.8404 R 0.757 0.824 0.6760 0.8060 D 1.029 1.0755 1.0811 1.0687 C 0.904 0.934 0.8956 0.9304  (Weighted). We vary the coherence and plot the FID with respect to the CLIPScore. We observe that our method achieves significantly better FID/CLIP tradeoff than the other methods. In Figure <ref type="figure" target="#fig_3">5</ref> (a), we report the metrics for the guidance parameter that gives the best FID. We observe that our method outperforms other methods on all metrics except for the CLIPScore. In particular, our method achieves a better FID by 15 points than the second-best method, i.e. the filtered model. We corroborate these results with a user study in Figure <ref type="figure" target="#fig_3">5</ref> (b), where we generate images for randomly sampled captions in COCO. For each method, we generate 10 pairs of images and we ask 36 users to vote for the image with the best quality, and for the one with the most coherence to the prompt. Our method is overwhelmingly preferred to other methods. In particular, users prefer the image quality of our images 95% of the cases and find our images better aligned with the prompts by 89%. Notably, the user study reveals a well-established limitation in such models, i.e. FID vs CLIPScore tradeoffs do not necessarily correlate well with human perception, as shown also in SD-XL <ref type="bibr" target="#b39">[40]</ref>.</p><p>Quantitative results for class-conditional image generation. In Table <ref type="table">7</ref> (d), we compare to a baseline where we do not use the coherence score, and a filtered model, where we filter all samples with coherence scores lower than 0.5. When filtering, we observe on CIFAR that the model's performance dramatically drops. FIDs are worse than the baseline, showing that dropping images prevents generating high-quality images. CAD displays improved Accuracy over the baseline while having better image quality than the filtered baseline.</p><p>Qualitative results for semantic conditioning. In Figure <ref type="figure" target="#fig_7">8</ref> (a), we present generated images derived from the same semantic map, obtained through the prediction of a segmen-tation network on an image from the ADE20K dataset <ref type="bibr" target="#b61">[62]</ref>. Notably, we vary the prompts and coherence maps in our experiments. When using a uniform coherence map set at c = 1, the generated image aligns correctly with the semantic map but lacks semantic information, resulting in an image that may not appear meaningful. To introduce synthetic coherence maps, we employ Canny edge detection on the semantic map, creating regions of low coherence at class boundaries. This approach gives the model more flexibility in adjusting the shape of different objects, leading to a more realistic image (second column).</p><p>Additionally, we manually edit the coherence map by introducing a low-coherence region in the form of a square. As depicted in the "œÉ+edit" columns of Figure <ref type="figure" target="#fig_7">8</ref> (a), the model tends to adhere to the shape defined by the coherence map. It strategically employs the location of this low-coherence region to incorporate objects that are specified in the prompt but absent from the semantic map. This observation is particularly highlighted in the final image in Figure <ref type="figure" target="#fig_7">8 (a)</ref>, where we overlay the coherence scores of two children obtained from another image onto the manipulated coherence map, while correspondingly adjusting the prompt. The model adeptly uses the degrees of freedom and shape information provided by the low-coherence region of the coherence map to seamlessly insert the children into the image.</p><p>Quantitative results for semantic conditioning. In Figure <ref type="figure" target="#fig_7">8</ref> (b), we demonstrate that incorporating both the segmentation and the coherence map leads to a decrease in FID for both scenarios, with and without the text input, indicating the superior visual quality of the generated images. This behavior is expected as our model possesses greater freedom to generate realistic content instead of strictly adhering to the segmentation map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We proposed a novel method for training conditional diffusion models with additional coherence information. By incorporating coherence scores into the conditioning process, our approach allows the model to dynamically adjust its reliance on the conditioning. We also extend the classifier-free guidance, enabling the derivation of conditional and unconditional models without the need for dropout during training. We have demonstrated that our method, called coherence-aware diffusion (CAD), produces more diverse and realistic samples on various conditional generation tasks, including classification on CIFAR10, ImageNet and semantic segmentation on ADE20k.</p><p>Limitations. The main limitation of CAD lies in the extraction of coherence scores, as unreliable coherence scores can lead to biases in the model. Future research includes focusing on more robust and reliable methods for obtaining coherence scores to further enhance the effectiveness and generalizability of our approach. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. CAD architecture</head><p>In this section, we describe the building blocks of the proposed CAD architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. RIN architecture for class conditional with coherence</head><p>We first explain our adaptation of the RIN architecture <ref type="bibr" target="#b25">[26]</ref>, that we use in our experiments on class conditional generation in the context of coherence aware diffusion. As shown in Figure <ref type="figure" target="#fig_8">9</ref>, the RIN block is composed of two branches: one with the latents and one with the patches. We concatenate the timestep, coherence and conditioning embeddings to the latents (olive, green and orange blocks), with the coherence embedding being an addition of our method to the original RIN architecture. Then, first, the latents gather information from the input patches via Cross-Attention. Second, the latents are processed with N self-attention layers.</p><p>Finally, the patches are updated from the latents via Cross-Attention. The RIN architecture consists of stacking multiple RIN blocks, where the next RIN block receives the updated latents and patches.</p><p>During inference, RIN takes as input a noisy version of the image, a class, a timestep, and a coherence token to predict the noise that has been added to the clean version of the image. To improve the sampling, output latents from a given step are forwarded as input to the next denoising step. For more details, see <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation details</head><p>In this section, we present the implementation details for each experiment as well as the training setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Text Conditining</head><p>For the text conditional, we use the LAMB optimizer <ref type="bibr" target="#b57">[58]</ref> with a weight decay of 0.01. We use a learning rate of 0.001. The batch size is 1024. We use a linear warmup of the learning rate for the first 10k steps and then use a cosine decay. We train all models for 300k steps. We use an EMA decay of 0.9999 for the last 50k steps. We use the Stable diffusion VAE encoder <ref type="bibr" target="#b44">[45]</ref> and perform the diffusion process in its embedding space in which the image tokens have dimension 32x32x4. We have 4 RIN blocks, each having 4 self-attention units. The data tokens dimension is 256 and the latent token dimension is 768. The input data is reduced to 256 data tokens by using a patch-size of 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Class Conditioning</head><p>For the class conditional experiments, we follow the hyperparameters provided by the authors of RIN. We use the LAMB optimizer with weight decay of 0.01. We use a linear warmup of the learning rate for the first 10k steps and then use a cosine decay. We train all models for 150k steps. We use an EMA decay of 0.9999. For CIFAR-10, we have 3 RIN blocks, each having 2 processing units. The data tokens dimension is 256 and the latent token dimension is 512. We use a patch-size of 2. We use a learning rate of 0.003. The batch-size is 256. For ImageNet-64, we have 4 RIN blocks, each having 4 processing units. The data tokens dimension is 512 and the latent token dimension is 768. We use a patch-size of 8. We use a learning rate of 0.002. The batch-size is 1024.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Semantic map conditioning</head><p>Generations conditioned on semantic maps were obtained by training a ControlNet <ref type="bibr" target="#b60">[61]</ref>. To train the ControlNet, we created a dataset of images selected from the ADE20K <ref type="bibr" target="#b61">[62]</ref> and MS COCO <ref type="bibr" target="#b31">[32]</ref> datasets. This dataset contains tuples of the form (image, caption, semantic map, coherence map) that were generated from the original images. The captions are obtained with BLIP2 <ref type="bibr" target="#b30">[31]</ref>, an image captioning language model. We utilize a Maskformer <ref type="bibr" target="#b6">[7]</ref> trained either on the MS COCO dataset or ADE20k to generate the semantic maps. To extract coherence values, we use the maximum class probability obtained from the softmax output of the Maskformer model. We employ a MaskFormer trained on ADE20K to generate the coherence map for MS COCO, and conversely, use a MaskFormer trained on MS COCO to obtain the coherence map for ADE20K. This approach helps mitigate the problem of overconfidence in predictions on the training set, reducing the tendency to have only high coherence scores across all pixels. We used a batch size of 16, using the Adam optimizer with a learning rate of 1e-5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Computational cost</head><p>Our method adds negligible training and inference time because we either modify existing architectures or add noncomputationally expensive components. Specifically, in terms of architecture, we are replacing one of the latent tokens with an embedded coherence score. For the textconditional, we do add a new cross-attention layer, but most of the compute is still in the self-attention blocks.</p><p>In this project, we have used approximately 25,553 V100 hours for preliminary experiments including the CIFAR-10 experiments and 29,489 A100 hours for ImageNet and text-conditional experiments. Each GPU hour accounts for roughly 259 Wh for a total of 14,255kWh. For semantic segmentation, the training of the different ControlNets was performed using about 1,800 hours of Nvidia A100 GPUs in total, or about 470kWh. The training process required approximately 100 GPU hours for each model trained on ADE20k <ref type="bibr" target="#b61">[62]</ref> and 200 GPU hours for MS COCO <ref type="bibr" target="#b31">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Image from semantic map additional experiments</head><p>In this section, we examine the effectiveness of incorporating coherence maps for semantic segmentation. We qualitatively and quantitatively compare the results of our CAD method against a baseline approach not using the coherence information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Quantitative results</head><p>Here, we quantitatively evaluate the effectiveness of incorporating coherence maps for semantic segmentation. For this, we experiment on two of the most popular datasets with segmentation: ADE20K <ref type="bibr" target="#b61">[62]</ref> and MS COCO <ref type="bibr" target="#b31">[32]</ref>. To evaluate our results, we employ the Frechet Inception Distance (FID) and Inception Score (IS) for evaluating image quality. Additionally, Precision (P), Recall (R), Density (D), and Coverage (C) serve as manifold metrics, enabling an evaluation of the overlap between the generated and real image manifolds. Finally, we calculate the mean Intersection over Union (mIoU) by utilizing a pre-trained MaskFormer to predict a segmentation map from the generated image and compare it with the original semantic map. This helps illustrate the fidelity of the generated images to the ground truth.</p><p>Method comparison. We compare the results of our CAD method with a baseline approach that excludes coherence information in both Table <ref type="table" target="#tab_2">1</ref> and Table <ref type="table" target="#tab_3">2</ref>, with the complete results. We conduct experiments in two settings: with text (first two rows) and without text (last four rows). Additionally, we compare against two CAD variations. Similar to the binning strategy in text-to-image generation, 'CAD bin' encodes coherence into 5 equally distributed discrete bins. Furthermore, 'CAD scalar' utilizes a singular scalar coherence score for the entire image, equivalent to the mean of the original coherence map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results.</head><p>In Table <ref type="table" target="#tab_2">1</ref> and Table <ref type="table" target="#tab_3">2</ref>, show the complete results of our method on ADE20k and COCO we demonstrate that using both the segmentation maps and the coherence maps lead to a decrease in FID for both scenarios, including or not the text input. This behavior is expected as our model possesses greater freedom to generate realistic content instead of sticking to the segmentation map uniquely (see e.g., the 4th column of Figure <ref type="figure" target="#fig_9">10</ref>). Furthermore, the improvement in the mIoU score can be attributed to two factors. First, when the input segmentation map is of low quality, the baseline method fails to capture important scene information. In contrast, our method benefits from additional information from the coherence map. Secondly, our method better leverages the caption in the low coherence region, mitigating the limitation of the segmentation map's limited number of classes (as seen in 4th row in Figure <ref type="figure" target="#fig_10">11</ref>, there is no ping-pong table class in the COCO dataset).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Additional Visualizations</head><p>We show additional results in Figure <ref type="figure" target="#fig_9">10</ref>, where, from the left column to the right one, we highlight the segmentation input, the coherence map, the image generated by the baseline, the image generated by our methods and the reference image. The coherence map reveals spatial/shape details of the scene. For instance, when comparing our method to a ControlNet trained solely with the segmentation map, our approach, which incorporates both segmentation and coherence, accurately reconstructs the curtain's shape and the windows in the first row, or reconstructs a cloud in the back of the plane in the second row. Moreover, the efficacy of our method becomes even more apparent when the segmentation map is of poor quality and the coherence score is low. For instance, as shown in the third row, the basic ControlNet attempts to adhere to the limited information provided by the flawed segmentation map, resulting in a scene with multiple arms displayed (third column). In contrast, our approach benefits from the flexibility given by the coherence map, allowing for more consistent image generation. Interestingly, our method also exhibits localized self-correction, as shown on the fourth row. In particular, our model is refraining from generating the hand region due to its low coherence in the input. Finally, we demonstrate the model's responsiveness to textual input in the last row. We present in the last row an example where the original image does not contain snow, but have high coherence in the sky. Both models generate some snow based on the caption, but in line with the coherence map, our model does not generate snow in the sky, thus being closer to the real image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Prompt generalization</head><p>In this subsection, we demonstrate the sensitivity of our method to the caption input. We observe in Figure <ref type="figure" target="#fig_10">11</ref> that our CAD method can successfully generalize to different types of captions, such as 'dining table', 'billiard', or 'ping pong table' and adjust the scene accordingly. Moreover, even when the table is not explicitly mentioned in the caption (as seen on the rightmost side of the figure), our method exhibits strong generalization capabilities and successfully generates the table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4. Coherence Interpolation</head><p>In Figure <ref type="figure" target="#fig_11">12</ref>, we demonstrate the significance of the coherence map in the conditioning of the ControlNet. In this experiment, we make an interpolation of the coherence map from the maximum coherence score everywhere (left) to very low coherence (right). When the input has high coherence throughout, the generated image lacks the presence of a ping pong table as it is not present in the semantic map. However, as the coherence score decreases, our methods recognize the shape of the ping pong table and successfully generates it in the image. It is worth noting that even at a low scale of the coherence score (second column corresponds to 1e-4 times the original value), our method is still able to reconstruct the table, even if it is not in the segmentation input. When we artificially reduce the coherence (two times less coherence, in the last column), our method is still able to generate a consistent scene without any artifacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Class conditional experiments D.1. Simulating annotation noise</head><p>Our approach for class-conditional image generation relies on the assumption that the dataset comes with annotated coherence scores. However, such scores are not always available for traditional image-generation datasets. To address this issue, we propose to simulate annotation noise by resampling from a dataset with clean annotations. We associate an error probability Œ± with each label. We assume that when the annotator is wrong, they misclassify uniformly over all classes, where N is the total number of classes. This leads to the following model:</p><formula xml:id="formula_4">»≥ ‚àº p y,Œ± with p y,Œ± ( »≤ = k) = 1 -Œ± if y = k, Œ± N -1 otherwise.<label>(5)</label></formula><p>We also define a strategy to remap the entire dataset using a normalized entropy-based coherence measure. We use the normalized entropy so that 0 maps to no coherence at all and 1 to total coherence in the label. We define the following coherence function:</p><formula xml:id="formula_5">E(Œ±) = -1 log(N ) (1 -Œ±) log(1 -Œ±) + Œ± log Œ± N -1 for Œ± ‚àà 0, N -1 N .<label>(6)</label></formula><p>To ensure that the dataset has samples with varying levels of confidence, we define a target entropy cumulative distribution. To achieve this, we use a piecewise-linear function:</p><formula xml:id="formula_6">E Œ≤,th (t) = t Œ≤ Œ∫ if t &lt; Œ∫, 1 + (t -1) 1-Œ≤ 1-Œ∫ otherwise. (<label>7</label></formula><formula xml:id="formula_7">)</formula><p>where Œ∫ represents a threshold and Œ≤ represents the entropy at this threshold. This function construction defines a low entropy region before the threshold and a high entropy region after the threshold. Finally, for each sample in the dataset (X, y), we sample t ‚àà U[0, 1], and associate a target entropy u. We then compute the associated error probability Œ± = E -1 (u), and resample according to p y,Œ± to obtain the tuple (X, »≥, 1 -u) <ref type="foot" target="#foot_1">2</ref> . This process allows us to generate synthetic data points with varying degrees of annotation noise and coherence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Quantitative Results</head><p>In this section we add more results on ImageNet with different levels of noise. We observe in Table <ref type="table" target="#tab_5">3</ref> results on ImageNet for Œ≤ ‚àà {0.2, 0.5, 0.8}. We first observe the less coherent the labels are the worse the results get in both image quality (FID) but also in accuracy (Acc). However, our method CAD manages to achieve better results than other methods in the context of un-coherent labels. This is further amplified when leveraging coherence aware guidance Caption: An hotel room with a bed, chair and table:</p><p>Caption: A white airplane on the runway:</p><p>Caption: A man sitting in front of a slot machine:</p><p>Caption: A large room with ping-pong tables and people playing:</p><p>Caption: A drilling rig in the middle of a snowy field:  </p><note type="other">Segmentation Coherence Baseline Ours Real Image</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>High coherence Low coherence</head><p>Figure <ref type="figure" target="#fig_11">12</ref>. interpolation: In the first column, we artificially provide our ControlNet with a coherence map having the maximum value everywhere. Indeed, our models do not generate the ping pong tables. But, as soon as we decrease the coherence toward its original value, the ping pong tables start to appear. Finally, in the last column, we provide the model with a coherence map that is half as confident as the original value and demonstrate that we can generate an image without artifacts</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Theoretical analysis</head><p>In this section, we motivate the use of coherence as an additional conditioning for diffusion models. Under assumptions that are verified empirically, we show that coherence aware diffusion can transition from an unconditional model to a conditional model simply by varying the coherence passed to the model. First, we define a consistency property of the coherence embedding as follows: </p><p>In other words, an embedding is coherence consistent if it tends to produce the same vector as the coherence approaches 0. This property is a sufficient condition to constrain the behavior of the diffusion model. Indeed, the following proposition is easily derived from it: Proposition E.1. Lipschitz continuous conditional neural diffusion models that leverage coherence consistent embed- dings for the conditioning are equivalent to unconditional models at low coherence.</p><p>Proof. We have to prove the following equivalent statement: Let œµ Œ∏ : x t , t, h(y, c) ‚Üí Œµt be a Lipschitz continuous neural diffusion model that predicts the noise Œµt at time t from the noisy sample x t with the help of the condition y embedded using the coherence consistent embedding h under coherence c. Then, ‚àÄŒ∑ &gt; 0 and ‚àÄx t , t, y 1 Ã∏ = y 2 , there exists C &gt; 0 such that for all 0 &lt; c ‚â§ C, we have ‚à•œµ Œ∏ (x t , t, h(y 1 , c)) -œµ Œ∏ (x t , t, h(y 2 , c))‚à• 2 &lt; Œ∑ . <ref type="bibr" target="#b8">(9)</ref> By Lipschitz property of œµ Œ∏ , we have ‚à•œµ Œ∏ (x t , t, h(y 1 , c)) -œµ Œ∏ (x t , t, h(y 2 , c))‚à• 2 ‚â§ L 2 ‚à•h(y 1 , c) -h(y 2 , c)‚à• 2 .</p><p>From the coherence consistent property, there exists C &gt; 0 such that for all 0 &lt; c ‚â§ C, ‚à•h(y 1 , c) -h(y 2 , c)‚à• &lt; ‚àö Œ∑/L.</p><p>The following contrapositive necessary condition on the coherence directly follows from this proposition: Corollary E.2. Lipschitz continuous conditional neural diffusion models that leverage coherence aware embeddings require high coherence to behave like conditional models.</p><p>In practice, we show in the experiments that the coherence consistency property tends to naturally emerge during training and that consequently coherence aware diffusion provides a tunable prompt parameter to sample from unconditional to conditional models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Additional Qualitative Results</head><p>In this section, we provide additional samples from our method. Most of the prompts are sampled from the Lexica.art website. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. (a) Examples of images generated with the input coherence score between the prompt and the target image.The score varies from 0 (no coherence) to 1 (maximum coherence). Higher coherence scores tend to generate images that adhere more effectively to the prompt. Top prompt: "a raccoon wearing an astronaut suit. The racoon is looking out of the window at a starry night; unreal engine, detailed, digital painting,cinematic,character design by pixar and hayao miyazaki, unreal 5, daz, hyperrealistic, octane render", bottom prompt: "An armchair in the shape of an avocado" (b) Increasing the coherence from 0 to 1, CLIPScore increases and FID decreases.</figDesc><graphic coords="2,53.43,153.76,62.41,62.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Images generated with a TextRIN trained with different handling of the misalignment between the image and its associated text at training. Compared to doing nothing (baseline), removing misaligned samples (filtering) or weighting the loss (weighted), our Coherence-Aware Diffusion training (CAD) generates more visually pleasing images while better adhering to the prompt.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Text RIN Block. Architecture of the proposed Text RIN Block used in CAD. We include a cross attention from the text to the latent branch of the RIN block.</figDesc><graphic coords="4,50.11,566.14,236.26,102.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Text-to-image generation results. (a) Quantitative results for text-to-image generation. We show that CAD achieves significantly lower FID, precision, recall, density and coverage while keeping similar CLIP score. (b) User study results. Users had to indicate the highest quality image and the most adhering to the prompt among pairs of images corresponding to our CAD method and one of baseline, filtered or weighted method. (c) FID versus CLIP on the text-to-image task for varying degrees of guidance œâ. We show that CAD achieves a significantly better trade-off with a much lower FID for the same CLIP score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Coherence-Aware Classifier-free Guidance for classes Malamute and Ice Cream with guidance rates œâ ‚àà {0, 1, 5, 20}.</figDesc><graphic coords="6,50.11,208.08,118.80,79.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Impact of coherence on the model. Top: (a) Impact on FID and Accuracy of prompting a model (CAD on ImageNet with Œ≤ = 0.5). (b) FID vs Accuracy (using CA-CFG). We vary the guidance rate from 0 to 25. (c) Impact of prompting with different coherence scores on image generation. Low coherence indicates convergence towards an unconditional model. Bottom: (d) Quantitative results for class-conditional image generation. Our coherence aware diffusion (CAD) is compared to a baseline model and a training set filtering strategy for different levels of label noise Œ≤. We show that CAD achieves higher fidelity and better accuracy. (e) TSNE of a mixed embedding of the class label and the coherence score on CIFAR-10. Each color denotes a class and the transparency shows the coherence level: the more transparent, the less coherence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><figDesc>(b) Quantitative results on ADE20k and MS COCO. We report FID, mIoU, Precision (P), Recall (R), Diversity (D) and Coverage (C).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. (a) Image generation conditioned on a semantic map. The images generated for a given prompt p (shown below) are shown with respect to different pixel-level coherence scores c (shown above). Coherence scores are obtained either synthetically using Canny edge detection on the semantic map, or from the maximum of the softmax probability œÉ of a pre-trained model. They can then be edited either manually (œÉ + edit) or by blitting other coherence maps (rightmost). (b) Quantitative results on ADE20k and MS COCO.</figDesc><graphic coords="8,56.16,79.43,333.81,92.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Architecture of the RIN Block modified to receive as input the coherence.</figDesc><graphic coords="12,50.11,72.00,236.26,79.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Qualitative results on ADE20K: Examples of images generated conditionally to text and semantic maps.</figDesc><graphic coords="15,50.11,538.84,495.00,99.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Caption generalization: Our methods demonstrate remarkable capability in leveraging the coherence map to generalize to diverse prompt inputs.</figDesc><graphic coords="16,148.81,145.28,99.88,76.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Definition E. 1 .</head><label>1</label><figDesc>We denote coherence consistent a conditional embedding h(y, c) of the condition y ‚àà Y under coherence c ‚àà [0, 1], if ‚àÄy 1 , y 2 ‚àà Y we have lim c‚Üí0 ‚à•h(y 1 , c) -h(y 2 , c)‚à• = 0 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 13 .</head><label>13</label><figDesc>Figure 13. Samples from our CAD-B model at 512 resolution with associated caption</figDesc><graphic coords="18,69.88,93.96,148.50,148.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Quantitative results on ADE20K when conditioning on semantic maps. 'CAD bin' encodes the coherence into 5 equally distributed discrete bins. 'CAD scalar' uses a scalar coherence score for the whole image. CAD achieves better FID due to its enhanced ability to generate realistic objects in low coherence regions and superior mIoU as the leaked spatial information from the coherence map and the caption assist it to generate better samples (see Section C.1 for more details).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ADE20K</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ControlNet</cell><cell>FID</cell><cell>IS</cell><cell>mIoU</cell><cell>P</cell><cell>R</cell><cell>D</cell><cell>C</cell></row><row><cell>Baseline</cell><cell cols="2">33.67 14.82</cell><cell>22.6</cell><cell>0.785</cell><cell>0.757</cell><cell cols="2">1.029 0.904</cell></row><row><cell>CAD</cell><cell cols="2">30.88 14.79</cell><cell>23.7</cell><cell>0.844</cell><cell cols="3">0.824 1.0755 0.934</cell></row><row><cell cols="3">Baseline w/o text 74.37 5.88</cell><cell>5.25</cell><cell>0.657</cell><cell>0.351</cell><cell cols="2">0.789 0.515</cell></row><row><cell>CAD w/o text</cell><cell cols="2">60.21 7.93</cell><cell>11.8</cell><cell>0.619</cell><cell>0.536</cell><cell cols="2">0.789 0.682</cell></row><row><cell>CAD bin</cell><cell cols="2">63.47 7.97</cell><cell>10.8</cell><cell cols="4">0.5875 0.4925 0.757 0.663</cell></row><row><cell>CAD scalar</cell><cell cols="2">74.69 6.15</cell><cell>3.11</cell><cell cols="2">0.6495 0.347</cell><cell cols="2">0.858 0.536</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Quantitative results on MS COCO when conditioning on semantic maps. 'CAD bin' encodes the coherence into 5 equally distributed discrete bins. 'CAD scalar' uses a scalar coherence score for the whole image (see Section C.1 for more details).</figDesc><table><row><cell>COCO</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>Quantitative results for class-conditional image generation. Our coherence aware diffusion (CAD) is compared to a baseline model and a training set filtering strategy for different levels of label noise Œ≤. We show that CAD achieves higher fidelity and better accuracy.CFG œâ = 1 6.70 44.27 0.523 0.695 0.483 1.035 0.743</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ImageNet</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Œ≤</cell><cell>Method</cell><cell>FID</cell><cell>IS</cell><cell>Acc</cell><cell>P</cell><cell>R</cell><cell>D</cell><cell>C</cell></row><row><cell></cell><cell>Conditional</cell><cell cols="7">7.56 34.26 0.475 0.595 0.610 0.768 0.706</cell></row><row><cell></cell><cell>Baseline</cell><cell cols="7">11.09 23.54 0.264 0.562 0.598 0.670 0.594</cell></row><row><cell>0.2</cell><cell>Filtered</cell><cell cols="7">8.53 29.27 0.389 0.591 0.609 0.756 0.688</cell></row><row><cell></cell><cell>CAD</cell><cell cols="7">8.17 27.75 0.367 0.585 0.615 0.736 0.658</cell></row><row><cell></cell><cell cols="8">CA-CFG œâ = 1 5.95 68.95 0.679 0.742 0.477 1.203 0.812</cell></row><row><cell></cell><cell>Baseline</cell><cell cols="7">14.38 20.46 0.168 0.539 0.579 0.595 0.505</cell></row><row><cell>0.5</cell><cell>Filtered</cell><cell cols="7">10.20 26.59 0.338 0.573 0.608 0.707 0.645</cell></row><row><cell></cell><cell>CAD</cell><cell cols="7">9.11 25.97 0.327 0.571 0.610 0.714 0.633</cell></row><row><cell></cell><cell cols="8">CA-CFG œâ = 1 5.95 68.95 0.679 0.742 0.477 1.203 0.812</cell></row><row><cell></cell><cell>Baseline</cell><cell cols="7">20.10 17.28 0.100 0.502 0.535 0.526 0.417</cell></row><row><cell>0.8</cell><cell>Filtered</cell><cell cols="7">12.00 24.55 0.292 0.420 0.712 0.647 0.605</cell></row><row><cell></cell><cell>CAD</cell><cell cols="7">11.39 22.08 0.248 0.558 0.590 0.682 0.574</cell></row><row><cell></cell><cell>CA-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>It is worth noting that using a Maskformer trained on the same dataset would result in high confidence map everywhere due to its high performance on the training set<ref type="bibr" target="#b16">[17]</ref> </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>The coherence goes in the opposite direction of the entropy.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>Figure 14. Samples from our CAD-B model at 512 resolution</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3"><p>Figure 15. Samples from our CAD-B model at 512 resolution</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_4"><p>Figure 16. Samples from our CAD-B model at 512 resolution</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_5"><p>Figure 17. Samples from our CAD-B model at 512 resolution</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6.">Acknowledgments</head><p>This work was supported by <rs type="projectName">ANR</rs> project <rs type="grantNumber">TOSAI ANR-20-IADJ-0009</rs>, and was granted access to the HPC resources of IDRIS under the allocation <rs type="grantNumber">2023-AD011014246</rs> made by <rs type="institution" subtype="infrastructure">GENCI</rs>. We would like to thank <rs type="person">Vincent Lepetit</rs>, <rs type="person">Romain Loiseau</rs>, <rs type="person">Robin Courant</rs>, <rs type="person">Mathis Petrovich</rs>, <rs type="person">Teodor Poncu</rs> and the anonymous reviewers for their insightful comments and suggestion.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_SCfNyf9">
					<idno type="grant-number">TOSAI ANR-20-IADJ-0009</idno>
					<orgName type="project" subtype="full">ANR</orgName>
				</org>
				<org type="funding" xml:id="_ZcfKRV2">
					<idno type="grant-number">2023-AD011014246</idno>
				</org>
			</listOrg>

			<listOrg type="infrastructure">
				<org type="infrastructure">					<orgName type="extracted">GENCI</orgName>
					<orgName type="full" lang="fr">Grand √âquipement National de Calcul Intensif</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Text-to-image diffusion models with an ensemble of expert denoisers</title>
		<author>
			<persName><forename type="first">Yogesh</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><surname>Kreis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arxiv</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Confidence scores make instancedependent label-noise learning possible</title>
		<author>
			<persName><forename type="first">Antonin</forename><surname>Berthon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Mach. Lear</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Improving image generation with better captions</title>
		<author>
			<persName><forename type="first">James</forename><surname>Betker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">‚Ä†</forename><surname>Timbrooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">‚Ä†</forename><surname>Longouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">‚Ä†</forename><surname>Juntangzhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">‚Ä†</forename><surname>Joycelee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">‚Ä†</forename><surname>Yufeiguo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">‚Ä†</forename><surname>Wesammanassra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">‚Ä†</forename><surname>Prafulladhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">‚Ä†</forename><surname>Caseychu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">‚Ä†</forename><surname>Yunxinjiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<idno>arxiv, 2023. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Conceptual 12m: Pushing web-scale image-text pretraining to recognize long-tail visual concepts</title>
		<author>
			<persName><forename type="first">Soravit</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Conceptual 12M: Pushing web-scale image-text pretraining to recognize long-tail visual concepts</title>
		<author>
			<persName><forename type="first">Soravit</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis</title>
		<author>
			<persName><forename type="first">Junsong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jincheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongjian</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongdao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<idno>arXiv, 2023. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Per-pixel classification is not all you need for semantic segmentation</title>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Weakly supervised learning with side information for noisy labeled images</title>
		<author>
			<persName><forename type="first">Lele</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangzeng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liming</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dangwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinghui</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Scaling instructionfinetuned language models</title>
		<author>
			<persName><forename type="first">Chung</forename><surname>Hyung Won</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunxuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Brahma</surname></persName>
		</author>
		<idno>arXiv, 2022. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Diffusion models beat gans on image synthesis</title>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Nichol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale. arxiv</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Scam! transferring humans between images with semantic cross attention modulation</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Dufour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicky</forename><surname>Kalogeiton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Training deep neural-networks using a noise adaptation layer</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehud</forename><surname>Ben-Reuven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Generative adversarial nets. NeurIPS</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuangfei</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Matryoshka diffusion models</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Mach. Lear</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Co-teaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivor</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A baseline for detecting misclassified and out-of-distribution examples in neural networks</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Bernhard Nessler, and Sepp Hochreiter</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Classifier-free diffusion guidance</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2004">2022. 2, 4</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">simple diffusion: End-to-end diffusion for high resolution images</title>
		<author>
			<persName><forename type="first">Emiel</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Heek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arxiv</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Binary classification from positive-confidence data</title>
		<author>
			<persName><forename type="first">Takashi</forename><surname>Ishida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Scalable adaptive computation for iterative generation</title>
		<author>
			<persName><forename type="first">Allan</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Mach. Lear</title>
		<imprint>
			<date type="published" when="2022">2022. 3, 4, 5, 12</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Noise-aware learning from web-crawled image-text data for image captioning</title>
		<author>
			<persName><forename type="first">Wooyoung</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonghwan</forename><surname>Mun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungjun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byungseok</forename><surname>Roh</surname></persName>
		</author>
		<idno>arxiv, 2022. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arxiv</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improved precision and recall metric for assessing generative models</title>
		<author>
			<persName><forename type="first">Tuomas</forename><surname>Kynk√§√§nniemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models</title>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Hoi</surname></persName>
		</author>
		<idno>arXiv, 2023. 5</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models</title>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Mach. Lear</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll√°r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zitnick</forename></persName>
		</author>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note>Microsoft coco: Common objects in context. ECCV, 2014. 2, 5</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Latent consistency models: Synthesizing highresolution images with few-step inference</title>
		<author>
			<persName><forename type="first">Simian</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqin</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longbo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<idno>arxiv, 2023. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Conditional generative adversarial nets</title>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Reliable fidelity and diversity metrics for generative models</title>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Ferjad Naeem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seong</forename><surname>Joon Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngjung</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunjey</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaejun</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Mach. Lear</title>
		<imprint>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning with noisy labels</title>
		<author>
			<persName><forename type="first">Nagarajan</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><forename type="middle">K</forename><surname>Inderjit S Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ambuj</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><surname>Tewari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatially-adaptive normalization</title>
		<author>
			<persName><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Scalable diffusion models with transformers</title>
		<author>
			<persName><forename type="first">William</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Wuerstchen: Efficient pretraining of text-to-image models</title>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Pernias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominic</forename><surname>Rampas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Aubreville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Sdxl: Improving latent diffusion models for high-resolution image synthesis</title>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Podell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zion</forename><surname>English</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lacey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dockhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Penna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>arXiv, 2023. 3, 8</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Int. Conf. Mach. Lear., 2021. 2, 5</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Hierarchical text-conditional image generation with clip latents</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Casey</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<idno>arxiv, 2022. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Fast text-conditional discrete denoising on vectorquantized latent spaces</title>
		<author>
			<persName><forename type="first">Dominic</forename><surname>Rampas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Pernias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elea</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Aubreville</surname></persName>
		</author>
		<idno>arXiv, 2022. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning to reweight examples for robust deep learning</title>
		<author>
			<persName><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Mach. Lear</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bj√∂rn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2022">2022. 1, 3, 5, 12</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Photorealistic text-to-image diffusion models with deep language understanding</title>
		<author>
			<persName><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lala</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamyar</forename><surname>Ghasemipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raphael</forename><forename type="middle">Gontijo</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Burcu</forename><surname>Karagol Ayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Laion-5b: An open large-scale dataset for training next generation image-text models</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Schuhmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romain</forename><surname>Beaumont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Vencu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cade</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Cherti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theo</forename><surname>Coombes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aarush</forename><surname>Katta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clayton</forename><surname>Mullis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">A picture is worth a thousand words: Principled recaptioning improves image generation</title>
		<author>
			<persName><forename type="first">Eyal</forename><surname>Segalis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Valevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danny</forename><surname>Lumen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaniv</forename><surname>Leviathan</surname></persName>
		</author>
		<idno>arXiv, 2023. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep unsupervised learning using nonequilibrium thermodynamics</title>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niru</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Mach. Lear</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Generative modeling by estimating gradients of the data distribution</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Improved techniques for training score-based generative models</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Consistency models. arxiv</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herv√©</forename><surname>J√©gou</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Int. Conf. Mach. Lear., 2021. 5</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Going deeper with image transformers</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herv√©</forename><surname>J√©gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">≈Åukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Hu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellen</forename><surname>Xiaoqing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Po-Yao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vasu</forename><surname>Howes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shang-Wen</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gargi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Demystifying clip data. arxiv</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Large batch optimization for deep learning: Training bert in 76 minutes</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sashank</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Hseu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srinadh</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Sigmoid loss for language image pre-training</title>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Basil</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Adding conditional control to text-to-image diffusion models</title>
		<author>
			<persName><forename type="first">Lvmin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anyi</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maneesh</forename><surname>Agrawala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2023">2023. 1, 3, 5, 12</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2017">2017. 2, 5, 8, 12</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Image synthesis with semantic region-adaptive normalization</title>
		<author>
			<persName><forename type="first">Peihao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rameen</forename><surname>Abdal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yipeng</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">Wonka</forename><surname>Sean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
