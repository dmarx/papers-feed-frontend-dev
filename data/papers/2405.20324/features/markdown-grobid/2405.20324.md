# Don't drop your samples! Coherence-aware training benefits Conditional diffusion

## Abstract

## 

Figure 1. Images generated from our model, CAD. Our model showcase high visual quality, aesthetics and prompt following.

## Introduction

Conditional Diffusion models excel in image generation while affording greater user control over the generation process by integrating additional information [[1,](#b0)[46]](#b45). This extra data enables the model to guide the generated image towards a specific target, leading to improved various applications including high-quality text-to-image generation [[45]](#b44), as well as other modalities such as depth or human body pose [[61]](#b60). Furthermore, the accessibility of open-source models like Stable Diffusion has democratized the use of this technology, already causing significant shifts in various domains such as design, art, and marketing.

Training conditional diffusion models requires substantial volumes of paired data comprising the target image and its corresponding condition. In text-to-image generation, this pairing involves an image and a descriptive caption that characterizes both the content and the style of the image. Similarly, for class conditional generation, the pair consists of an image and its corresponding class label. Besides the technical challenges associated with the acquisition of extremely large quantities of paired data, ensuring accurate alignment between image and text conditions is still an open research question in the community, as attested by the large amount of recent work in the domain [[31,](#b30)[59]](#b58). In practice, large web- The score varies from 0 (no coherence) to 1 (maximum coherence). Higher coherence scores tend to generate images that adhere more effectively to the prompt. Top prompt: "a raccoon wearing an astronaut suit. The racoon is looking out of the window at a starry night; unreal engine, detailed, digital painting,cinematic,character design by pixar and hayao miyazaki, unreal 5, daz, hyperrealistic, octane render", bottom prompt: "An armchair in the shape of an avocado" (b) Increasing the coherence from 0 to 1, CLIPScore increases and FID decreases.

scraped datasets, such as LAION-5B [[48]](#b47) or CC12M [[4]](#b3), contain abundant noisy pairs due to their collecting process. To clean the pairs, hence ensuring alignment of higher quality, the prevailing strategy filters out samples that fail to meet an arbitrarily chosen criterion, often done through techniques like thresholding the CLIP-score [[39]](#b38)[[40]](#b39)[[41]](#b40). This approach, however, has two main drawbacks: first, it is challenging to adjust the criterion accurately and more importantly, it discards many high-quality samples that could potentially enhance generation quality irrespective of the condition. For instance, out of the 50B initially-collected text-image pairs, only 10% were left in LAION-5B [[48]](#b47), thus discarding 90% of the samples, i.e. 45B images.

Instead of discarding the vast majority of training samples, in this work, we leverage them to learn simultaneously conditional and unconditional distributions. Specifically, we introduce a novel approach that estimates what we call the coherence score, which evaluates how well the condition corresponds to its associated image. We incorporate this coherence score into the training process by embedding it into a latent vector, which is subsequently merged with the condition. This additional information enables the diffusion model to determine the extent to which the condition should influence the generation of a target image. During inference, our method has the flexibility to take as input the coherence score, thereby allowing users to vary the impact of the condition on the generation process, as illustrated in Figure ??. In addition, to further improve the generated image quality, we refine the Classifier-Free-Guidance method (CFG) introduced in [[21]](#b20) by leveraging the gap between high and low coherence scores.

We evaluate our approach across three distinct tasks that involve various types of conditioning: text for text-to-image generation, labels for class-conditioned image generation, and semantic maps for paint-by-word image generation. In text conditioning, we use the CLIP score [[41]](#b40) to estimate the coherence between the image and its accompanying caption.

For class-conditional generation, we employ an off-the-shelf confidence estimator to gauge the coherence between the image and its label. Concerning semantic maps, we derive pixel-level coherence scores either by automatically generating them based on class boundaries or by using an off-theshelf confidence estimator. Our evaluations span multiple datasets such as COCO [[32]](#b31) for zero-shot text-to-image generation, ImageNet [[10]](#b9) for class-conditioned generation, and ADE-20K [[62]](#b61) for semantic maps. Our results show that including the coherence score in the training process allows training diffusion models with better image quality and that are more coherent with what they are prompted. In summary, our contributions can be outlined as follows: ‚úì Innovative Training Approach: We present coherenceaware diffusion (CAD), a novel method for training conditional diffusion models in the presence of annotation imperfections. By incorporating a coherence score between the target image and its associated condition, our model can adapt and fine-tune the influence of the condition on the generation process. ‚úì Flexible Inference Scheme: We introduce a versatile inference scheme, in which manual tuning of the coherence score during the generation enables the modulation of the condition's impact on image generation. Additionally, we refine the classifier-free guidance method under this new inference scheme, resulting in enhanced image quality. ‚úì Wide Applicability: Demonstrating the versatility of CAD, we evaluate it across three diverse tasks involving different types of conditions (text, class labels, and semantic maps). CAD produces visually pleasing results across all tasks, emphasizing its generic applicability.

## Related Work

Conditional generation. Previous attempts to condition generative models were focused on GANs [[15]](#b14). Classconditional [[34]](#b33) was the first way to introduce conditioning in generative models. This is a simple global conditioning. However, it lacks control over the output. To increase the control power of conditioning, more local conditionings were proposed such as drawings, maps, or segmentation maps [[25]](#b24). Segmentation maps conditioning [[13,](#b12)[37,](#b36)[63]](#b62) propose the most control to the user. Indeed, the user can not only specify the shape of the objects but also per-object class information. Semantic masks are however tedious to draw, which impacts usability. Text-conditioned models [[60]](#b59) offer a compromise. They can provide both global and local conditioning and are easy to work with. Recently, diffusion models have made great advances in this domain. Diffusion models. Diffusion models [[22,](#b21)[[50]](#b49)[[51]](#b50)[[52]](#b51) have recently attracted the attention of research in image generation. Compared to GANs, they have better coverage over the data distribution, are easier to train and outperform them in terms of image quality [[11]](#b10). Architecture-wise, diffusion models rely mostly on modified versions of a U-Net [[11,](#b10)[22,](#b21)[51]](#b50). Recent works have however shown that other architectures are possible [[23,](#b22)[38]](#b37). In particular, RIN [[26]](#b25) proposes a much simpler architecture than the U-Net achieving more efficient training. They recently have been a lot of works [[1,](#b0)[42,](#b41)[46]](#b45) scaling up these models on huge text-to-image datasets [[48]](#b47). Stable Diffusion [[45]](#b44), Stable Diffusion XL [[40]](#b39), Paella [[43]](#b42) or Wuerstchen [[39]](#b38) have provided open-source weights for their networks, which has allowed an explosion in image generation. ControlNet [[61]](#b60) has shown that fine-tuning these models allows for very fine-grained control over the output with lots of different conditioning modalities. Recently, consistency models [[33,](#b32)[53]](#b52) have shown that by training more with a different loss, inference can be done in small amounts of steps (2-4 steps). All these text-to-image networks have been tuned on very noisy web-scrapped data. We argue in this paper that this noise causes limitations in the training. Concurrent works [[3,](#b2)[6,](#b5)[16,](#b15)[49]](#b48) propose to tackle this task through re-captioning, but this requires lots of resources to train a good captioner that outputs detailed captions without hallucinating details. As shown by [[3]](#b2), it also requires bridging the gap between train and test time prompts. Instead, our approach is much simpler in current training setups. Learning with noisy conditioning has been widely explored when considering classification. For binary classification, [[36]](#b35) study machine learning robustness when confronted with noisy labels, while [[24]](#b23)  sively positive labels accompanied by confidence scores. To bring a more practical perspective, [[2]](#b1) introduced instancedependent noise scored by confidence, where this score aligns with the probability of the assigned label's accuracy. The negative impact of noisy labels has been mitigated with changes in architecture [[8,](#b7)[14]](#b13), in the loss [[44]](#b43) or filtering the noisy samples [[18]](#b17). More recently, [[27]](#b26) propose a similar approach to ours by conditioning an image captioner model by the CLIP-score to mitigate the impact of text misalignment. Instead, we focus on image synthesis, where we condition the diffusion model with a coherence score.

## Coherence-Aware Diffusion (CAD)

In this work, we want to improve the training of the diffusion model in the presence of misaligned conditioning. We make the assumption that for each training sample, a coherence score measures how coherent the conditioning is with respect to the data. We propose to condition the diffusion model on this coherence score in addition to the original condition. By doing so, the model learns to discard the lowcoherence conditions and focus on the high-coherence ones. Consequently, our model can behave as either a conditional or an unconditional model. Low-coherence samples, lead to unconditional sampling, while high-coherence samples lead to conditional samples. Building on this, we redesign classifier-free guidance to rely on coherence conditioning instead of dropping out the conditioning randomly.

## Conditional Diffusion Models

We first provide an overview of conditional diffusion models. These models learn to denoise a target at various noise levels. By denoising sufficiently strong noises, we eventually denoise pure noise, which can then be used to generate images. Each diffusion process is associated with a network œµ Œ∏ , which performs the denoising task. To train such a network, we have X an image and y its associated conditioning coming from p data the data distribution. We use a noise scheduler Œ≥(t), which defines X t which is the input image corrupted with Gaussian noise at the t-th step of diffusion. such as X t = Œ≥(t)X + 1 -Œ≥(t)œµ, where œµ ‚àº N (0, 1) the noise we want to predict and t ‚àà [0, 1] the diffusion timestep. During training, conditioning is provided to œµ Œ∏ . The objective of the diffusion model is to minimize the following loss:

$L simple = E (X,y)‚àºpdata,t‚àºU [0,1] [‚à•œµ -œµ Œ∏ (X t , y, t)‚à•] ,(1)$where ‚à•‚Ä¢‚à• denotes the L 2 norm. One observation is that the conditioning is implicitly learned by the diffusion model, as the diffusion loss is only enforced on the image and not on the conditioning itself. This motivates our hypothesis that removing data with low label coherence can harm the training of the diffusion model. Even if the conditioning is not well aligned, the image still belongs to the distribution that we aim to learn. By discarding such data, we weaken the distribution estimator.

## Integrating label information into the diffusion model

We assume that for every datapoint (X, y) we have an associated c, the coherence score of y where c ‚àà [0, 1]. Our goal is to incorporate label coherence into the diffusion model to discard only the conditioning that contains low levels of coherence while continuing to train on the image. A value of c=1 indicates that y is the best possible annotation for X, while c=0 suggests that y is a poor annotation for X.

To achieve this, we modify the conditioning of the diffusion model œµ Œ∏ to include both y and c, using the following loss:

$L simple = E (X,y,c)‚àºpdata,t‚àºU [0,1] [‚à•œµ -œµ Œ∏ (X t , y, c, t)‚à•] .$(2) We refer to this kind of models as coherence coherenceaware diffusion (CAD) models. By informing the diffusion model of the coherence score associated with samples, we avoid filtering out low-confident samples and let the model learn by itself what information to take into account. Avoiding the filtering allows us to still learn X even in the presence of noisy labels.

## Test-time prompting

After training a model with different levels of coherence, we can thus prompt it with varying degrees of coherence. When we prompt with minimal coherence, we obtain an unconditional model. On the other hand, when we prompt with maximal coherence, we get a model that is very confident about the provided label. However, like any other conditional diffusion model relying on attention, there is no guarantee that the label is actually used.

To strengthen the use of the label, we propose a modification to the Classifier Free Guidance (CFG) method [[21]](#b20) that leverages the coherence. CFG uses both a conditional and unconditional model to improve the quality of generated samples. To learn such models, a conditional diffusion model is used and the conditioning is dropped out for a portion of the training samples. The original CFG formulation is as follows:

$ŒµŒ∏ (x t , y) = œµ Œ∏ (x t , y) + œâ(œµ Œ∏ (x t , y) -œµ Œ∏ (x t , ‚àÖ)) , (3)$with œâ the guidance rate. Instead, we propose a coherenceaware version of CFG (CA-CFG): ŒµŒ∏ (x t , y) = œµ Œ∏ (x t , y, 1) + œâ(œµ Œ∏ (x t , y, 1) -œµ Œ∏ (x t , y, 0)) .

(4) This modification removes the need to dropout the conditioning. Instead, we directly use the noise in the conditioning to drive the guidance.

## Experiments

In this section, we will analyze 3 tasks: text, class, and semantically conditioned image generation. We describe the experimental setup, and analyze quantitative and qualitative results to better understand the inner workings of Coherenceaware diffusion.

## Experimental setup and Metrics

Experimental setup. For text-conditional image generation, we use a modified version of RIN [[26]](#b25) (See Figure [4](#fig_2)). To map the text to an embedding space, we use a frozen FLAN-T5 XL [[9]](#b8). We then map the embedding with 2 selfattention transformer layers initialized with LayerScale [[55]](#b54) COCO-10K

Method œâ FIDCLIP ‚Üì CLIPScore ‚Üë PCLIP ‚Üë RCLIP ‚Üë DCLIP ‚Üë CCLIP ‚Üë Baseline 10 91.9 25.96 0.281 0.047 0.181 0.222 Weighted 5 98.3 25.15 0.192 0.046 0.111 0.155 Filtered 10 85.8 26.52 0.281 0.061 0.175 0.233 CAD-S (Ours) 15 69.4 26.16 0.373 0.078 0.265 0.315 CAD-B (Ours) 15 55.3 27.04 0.462 0.172 0.349 0.428 CAD-B 512px (Ours) 15 54.3 25.52 0.593 0.148 0.545 0.472 and 16 registers. The text tokens are mapped with the coherence score using these layers, which is the same for every Text RIN block. Unlike the class conditional RIN block, the latent branch contains only the latents without concatenated tokens. Instead, the mapped text tokens, coherence, and timestep embeddings provide information to the latent branch with a cross-attention layer at the beginning of each Text RIN block. The rest of the architecture follows the standard RIN block design. The CAD-S model has 188M parameters and we train it for 240K steps. We train these models on a mix of datasets composed of CC12M [[5]](#b4) and LAION Aesthetics 6+ [[48]](#b47). We also train a 332M parameters version for 500k steps that we name CAD-B. We also finetune it for 200K steps at 512px resolution. To estimate the coherence score, we use MetaCLIP H/14 [[57]](#b56) that we then bin into 8 equally distributed discrete bins. We then use the normalized index between 0 and 1 as the coherence score. We compare our method to 3 baselines: "Baseline" is a model where we just train without coherence, "Filtered" corresponds to a model where we discard the 3 less coherent bins, and "Weighted" corresponds to a model where we weight the loss of the model by the normalized coherence of the sample. When using Coherent Aware prompting, we sample a random set of characters that we use with coherence-score of zero as the negative prompt.

For the class-conditional image generation experiments, we rely on RIN [[26]](#b25) and use the same hyperparameters as the authors. We experiment on conditional image generation for CIFAR-10 [[28]](#b27) and Imagenet-64 [[10]](#b9) for which we artificially noise the label. We extract the coherence score from pre-trained classifiers in the following way: We re-sample with some temperature Œ≤ a new label from the label distribution predicted by the classifier. We then consider the entropy of the distribution as the coherence score. After, we use a sinusoidal positional embedding [[56]](#b55) that we map with an MLP. We add this coherence token in the latent branch of RIN, similar to the class token.

For semantic segmentation conditioned experiments, we use ControlNet [[61]](#b60) to condition a pre-trained text-to-image Stable-Diffusion [[45]](#b44) model with both semantic and coherence maps concatenated. The training and evaluation of our method are performed on the ADE20K dataset [[62]](#b61), a large-scale semantic segmentation dataset containing over 20,000 images with fine-detailed labels, covering diverse scenes and classes. Since captions are not available for this dataset, we use BLIP2 [[30]](#b29) to generate captions for each image in the dataset similarly to [[61]](#b60). We use a pre-trained Maskformer [[7]](#b6) on the COCO Dataset [[32]](#b31), to extract the segmentation map and its associated confidence (MCP [[19]](#b18)) for each image in the ADE20k dataset [1](#foot_0) . We use confidence as our coherence score. More details about the experimental setup are available in the supplementary.

Metrics. To evaluate image generation for all condition types, we use the Frechet Inception Distance [[20]](#b19) (FID) that evaluates image quality. We also use Precision [[29]](#b28) (P), Recall [[29]](#b28) (R), Density [[35]](#b34) (D) and Coverage [[35]](#b34) (C) as manifold metrics, allowing us to evaluate how well the manifold of the generated images overlaps with the manifold of the real images. For text-conditional, we also compute the CLIP Score [[41]](#b40) and evaluate metrics on CLIP features on a 10K samples subset of COCO [[32]](#b31) in a zero-shot setting.

For class-conditional, we compute the Inception Score [[47]](#b46) (IS). We also add the Accuracy (Acc) metric aiming at evaluating how well the image generator takes into account the conditioning and defined as Acc(g

$) = E c‚ààCat(N ) [1 f (g(c))=c ],$where g(.) is the generator we want to evaluate, f (.) is a classifier, and Cat(N ) is the categorical distribution of N labels.

For CIFAR-10, we use a Vision Transformer [[12]](#b11) trained on CIFAR-10, and for ImageNet, we use a DeiT [[54]](#b53). For segmentation, instead of Accuracy, we compute the mean Intersection over Union (mIoU) instead of the Accuracy.

## Analysis

Coherence conditioning. Here, we explore the behavior of our proposed coherence-aware diffusion model at test time.

For the text conditional setting, we observe from Figure [2b](#fig_0) that the coherence and the quality of the generated image increase as the coherence increases. Indeed, FID decreases and the CLIPScore increases. In the class-conditional setup, we prompt a CAD model trained on resampled ImageNet and report results in Table [7](#)(a). Similar to the previous setting, when the model has very high coherence, it achieves the best FID and accuracy. However, when the coherence score decreases, the accuracy decreases as well and drops to 1 N when the coherence goes to 0. This validates our hypothesis that in the presence of low-coherence samples, our proposed model behaves like an unconditional model. Furthermore, even if the FID increases, it remains close to the FID of the conditional model, which implies that our CAD samples images are close to the training distribution.

Qualitatively, for text-to-image generation, we prompt the model with varying coherence scores from 0 to 1 and display results in Figure [2a](#fig_0). We observe that when the coherence increases, the outputs are close to the prompt. For instance, in the bottom figure, the generated image displays an avocado armchair, where avocado and armchair are successfully mixed. Even a more complex prompt, like the raccoon at the top, follows closely the textual description. The raccoon does wear an astronaut suit and is looking through the window at a starry night. Similarly, as the coherence decreases, the images start to diverge from the original prompt. The avocado chair starts to first lose the "avocado" traits until there is only a chair and at the end an object that does not look like an avocado or a chair. At the top, we first lose the window, then the raccoon. We note that contrary to class conditional (as seen below), we do not converge to a totally random image. Instead, some features from the prompt are preserved, such as the racoon but the global structure of the image gets lost. This is highly linked to the CLIP network biases, which may pay less attention to less salient parts of an image such as the background, and are more sensitive to the main subject.

Similarly, for class-conditional, we prompt a CAD model trained on resampled ImageNet, with different coherences and classes. We sample with DDPM but we seed the sampling to have the same noise when sampling different classes. Table [7](#) (c) illustrates the results, where we observe that when prompted with high coherence, CAD samples have the desired class. However, as the coherence decreases, the samples get converted into samples from random classes. Furthermore, samples that use the same sampling noise converge towards the same image in the low coherence regime. This shows that when the label coherence is low, CAD discards the conditioning and instead samples unconditionally.

To better understand the underlying mechanism, we design the following experiment. We modify the proposed model so that instead of adding both a class and a coherence token to the RIN network, we merge them into a single token with an MLP. Figure [7](#fig_5) (e) displays the t-SNE plots of the output of the MLP for every class in CIFAR-10 for which we compute different coherence scores. In the plot, high coherence translates to low transparency. We observe that as the coherence decreases, the embeddings of all classes tend to converge into the same embedding (center). This corrob- orates our hypothesis that the model uses the coherence to learn by itself how much to rely on the label.

Coherence-aware classifier-free guidance. Here, we examine the impact of the coherence-aware classifier guidance.

For this, we first compute different guidance rates ranging from 0 to 30 with 250 steps of DDIM, and then we plot the FID vs the CLIPScore the classifier accuracy for different rates. Figure [7](#fig_5) (b) illustrates this. We observe a trade-off between classifier accuracy and FID. Specifically, the more we increase the guidance, the more the accuracy increases, but at the cost of higher FID.

Qualitatively, this behaviour is also present in Figure [6](#fig_4): at a lower guidance rate, the images are more diverse but at the cost of lower accuracy with respect to the class. This pattern is best shown in the Malamute example when œâ = 20 (first part in Figure [6](#fig_4)), where all malamutes have a similar pose with their tongues hanging and similarly, and in the third part where all ice-creams look similar, i.e., one white ice-cream scoop with red fillings.

Interestingly, we also observe that some guidance leads to optimal results. In Figure [7](#fig_5), when œâ = 1, the FID is at its lowest point, and the accuracy is higher than the default model that has œâ = 0. This is also shown in Figure [6](#fig_4), where, when œâ = 1, samples best combine diversity and fidelity.

## Results

In this section, we report image generation results conditioned on text, class, and semantic maps.

Qualitative results for text-conditioned image generation. In Figure [3](#fig_1), we observe that coherence-aware diffusion for textual conditioning allows for better prompt adherence and better-looking images. In the first row, we observe that CAD is the only method that captures the details of the prompt, such as having the wolf play the guitar. Indeed, the baseline and filtered models output a wolf, but most generations display only a head. The weighted model performs slightly better but it lacks quality. Furthermore, our model displays higher diversity in the output styles. For instance, this is visible in the bottom row where our model displays a variety of street images whereas the other methods tend to have a collapsed output. For this prompt, our model also displays better image quality compared to other methods.

Quantitative results for text-conditioned image generation. In Figure [5](#fig_3) (c), we compare our method to a classical diffusion model (baseline), to a method where all samples with CLIPScore lower than 0.41 are filtered as is commonly done with LAION-5B (Filtered), and one where the diffusion loss is wheighted by the CLIPScore of the sample

Semantic map ùëê = 1 Softmax ùúé ùúé + edit ùúé + edit ùúé + edit Canny edges ùëù ="A room with exposed brick wall and a radiator" ùëù + "and a cat in a box" ùëù + "and two tiny children (a) Examples of image generation conditioned on a semantic map. ADE20k COCO Metric Baseline CAD Baseline CAD FID 33.67 30.88 20.1 18.1 mIoU 22.6 23.7 35.1 35.3 P 0.785 0.844 0.7876 0.8404 R 0.757 0.824 0.6760 0.8060 D 1.029 1.0755 1.0811 1.0687 C 0.904 0.934 0.8956 0.9304  (Weighted). We vary the coherence and plot the FID with respect to the CLIPScore. We observe that our method achieves significantly better FID/CLIP tradeoff than the other methods. In Figure [5](#fig_3) (a), we report the metrics for the guidance parameter that gives the best FID. We observe that our method outperforms other methods on all metrics except for the CLIPScore. In particular, our method achieves a better FID by 15 points than the second-best method, i.e. the filtered model. We corroborate these results with a user study in Figure [5](#fig_3) (b), where we generate images for randomly sampled captions in COCO. For each method, we generate 10 pairs of images and we ask 36 users to vote for the image with the best quality, and for the one with the most coherence to the prompt. Our method is overwhelmingly preferred to other methods. In particular, users prefer the image quality of our images 95% of the cases and find our images better aligned with the prompts by 89%. Notably, the user study reveals a well-established limitation in such models, i.e. FID vs CLIPScore tradeoffs do not necessarily correlate well with human perception, as shown also in SD-XL [[40]](#b39).

Quantitative results for class-conditional image generation. In Table [7](#) (d), we compare to a baseline where we do not use the coherence score, and a filtered model, where we filter all samples with coherence scores lower than 0.5. When filtering, we observe on CIFAR that the model's performance dramatically drops. FIDs are worse than the baseline, showing that dropping images prevents generating high-quality images. CAD displays improved Accuracy over the baseline while having better image quality than the filtered baseline.

Qualitative results for semantic conditioning. In Figure [8](#fig_7) (a), we present generated images derived from the same semantic map, obtained through the prediction of a segmen-tation network on an image from the ADE20K dataset [[62]](#b61). Notably, we vary the prompts and coherence maps in our experiments. When using a uniform coherence map set at c = 1, the generated image aligns correctly with the semantic map but lacks semantic information, resulting in an image that may not appear meaningful. To introduce synthetic coherence maps, we employ Canny edge detection on the semantic map, creating regions of low coherence at class boundaries. This approach gives the model more flexibility in adjusting the shape of different objects, leading to a more realistic image (second column).

Additionally, we manually edit the coherence map by introducing a low-coherence region in the form of a square. As depicted in the "œÉ+edit" columns of Figure [8](#fig_7) (a), the model tends to adhere to the shape defined by the coherence map. It strategically employs the location of this low-coherence region to incorporate objects that are specified in the prompt but absent from the semantic map. This observation is particularly highlighted in the final image in Figure [8 (a)](#fig_7), where we overlay the coherence scores of two children obtained from another image onto the manipulated coherence map, while correspondingly adjusting the prompt. The model adeptly uses the degrees of freedom and shape information provided by the low-coherence region of the coherence map to seamlessly insert the children into the image.

Quantitative results for semantic conditioning. In Figure [8](#fig_7) (b), we demonstrate that incorporating both the segmentation and the coherence map leads to a decrease in FID for both scenarios, with and without the text input, indicating the superior visual quality of the generated images. This behavior is expected as our model possesses greater freedom to generate realistic content instead of strictly adhering to the segmentation map.

## Conclusions

We proposed a novel method for training conditional diffusion models with additional coherence information. By incorporating coherence scores into the conditioning process, our approach allows the model to dynamically adjust its reliance on the conditioning. We also extend the classifier-free guidance, enabling the derivation of conditional and unconditional models without the need for dropout during training. We have demonstrated that our method, called coherence-aware diffusion (CAD), produces more diverse and realistic samples on various conditional generation tasks, including classification on CIFAR10, ImageNet and semantic segmentation on ADE20k.

Limitations. The main limitation of CAD lies in the extraction of coherence scores, as unreliable coherence scores can lead to biases in the model. Future research includes focusing on more robust and reliable methods for obtaining coherence scores to further enhance the effectiveness and generalizability of our approach. 

## A. CAD architecture

In this section, we describe the building blocks of the proposed CAD architecture.

## A.1. RIN architecture for class conditional with coherence

We first explain our adaptation of the RIN architecture [[26]](#b25), that we use in our experiments on class conditional generation in the context of coherence aware diffusion. As shown in Figure [9](#fig_8), the RIN block is composed of two branches: one with the latents and one with the patches. We concatenate the timestep, coherence and conditioning embeddings to the latents (olive, green and orange blocks), with the coherence embedding being an addition of our method to the original RIN architecture. Then, first, the latents gather information from the input patches via Cross-Attention. Second, the latents are processed with N self-attention layers.

Finally, the patches are updated from the latents via Cross-Attention. The RIN architecture consists of stacking multiple RIN blocks, where the next RIN block receives the updated latents and patches.

During inference, RIN takes as input a noisy version of the image, a class, a timestep, and a coherence token to predict the noise that has been added to the clean version of the image. To improve the sampling, output latents from a given step are forwarded as input to the next denoising step. For more details, see [[26]](#b25).

## B. Implementation details

In this section, we present the implementation details for each experiment as well as the training setup.

## B.1. Text Conditining

For the text conditional, we use the LAMB optimizer [[58]](#b57) with a weight decay of 0.01. We use a learning rate of 0.001. The batch size is 1024. We use a linear warmup of the learning rate for the first 10k steps and then use a cosine decay. We train all models for 300k steps. We use an EMA decay of 0.9999 for the last 50k steps. We use the Stable diffusion VAE encoder [[45]](#b44) and perform the diffusion process in its embedding space in which the image tokens have dimension 32x32x4. We have 4 RIN blocks, each having 4 self-attention units. The data tokens dimension is 256 and the latent token dimension is 768. The input data is reduced to 256 data tokens by using a patch-size of 2.

## B.2. Class Conditioning

For the class conditional experiments, we follow the hyperparameters provided by the authors of RIN. We use the LAMB optimizer with weight decay of 0.01. We use a linear warmup of the learning rate for the first 10k steps and then use a cosine decay. We train all models for 150k steps. We use an EMA decay of 0.9999. For CIFAR-10, we have 3 RIN blocks, each having 2 processing units. The data tokens dimension is 256 and the latent token dimension is 512. We use a patch-size of 2. We use a learning rate of 0.003. The batch-size is 256. For ImageNet-64, we have 4 RIN blocks, each having 4 processing units. The data tokens dimension is 512 and the latent token dimension is 768. We use a patch-size of 8. We use a learning rate of 0.002. The batch-size is 1024.

## B.3. Semantic map conditioning

Generations conditioned on semantic maps were obtained by training a ControlNet [[61]](#b60). To train the ControlNet, we created a dataset of images selected from the ADE20K [[62]](#b61) and MS COCO [[32]](#b31) datasets. This dataset contains tuples of the form (image, caption, semantic map, coherence map) that were generated from the original images. The captions are obtained with BLIP2 [[31]](#b30), an image captioning language model. We utilize a Maskformer [[7]](#b6) trained either on the MS COCO dataset or ADE20k to generate the semantic maps. To extract coherence values, we use the maximum class probability obtained from the softmax output of the Maskformer model. We employ a MaskFormer trained on ADE20K to generate the coherence map for MS COCO, and conversely, use a MaskFormer trained on MS COCO to obtain the coherence map for ADE20K. This approach helps mitigate the problem of overconfidence in predictions on the training set, reducing the tendency to have only high coherence scores across all pixels. We used a batch size of 16, using the Adam optimizer with a learning rate of 1e-5.

## B.4. Computational cost

Our method adds negligible training and inference time because we either modify existing architectures or add noncomputationally expensive components. Specifically, in terms of architecture, we are replacing one of the latent tokens with an embedded coherence score. For the textconditional, we do add a new cross-attention layer, but most of the compute is still in the self-attention blocks.

In this project, we have used approximately 25,553 V100 hours for preliminary experiments including the CIFAR-10 experiments and 29,489 A100 hours for ImageNet and text-conditional experiments. Each GPU hour accounts for roughly 259 Wh for a total of 14,255kWh. For semantic segmentation, the training of the different ControlNets was performed using about 1,800 hours of Nvidia A100 GPUs in total, or about 470kWh. The training process required approximately 100 GPU hours for each model trained on ADE20k [[62]](#b61) and 200 GPU hours for MS COCO [[32]](#b31).

## C. Image from semantic map additional experiments

In this section, we examine the effectiveness of incorporating coherence maps for semantic segmentation. We qualitatively and quantitatively compare the results of our CAD method against a baseline approach not using the coherence information.

## C.1. Quantitative results

Here, we quantitatively evaluate the effectiveness of incorporating coherence maps for semantic segmentation. For this, we experiment on two of the most popular datasets with segmentation: ADE20K [[62]](#b61) and MS COCO [[32]](#b31). To evaluate our results, we employ the Frechet Inception Distance (FID) and Inception Score (IS) for evaluating image quality. Additionally, Precision (P), Recall (R), Density (D), and Coverage (C) serve as manifold metrics, enabling an evaluation of the overlap between the generated and real image manifolds. Finally, we calculate the mean Intersection over Union (mIoU) by utilizing a pre-trained MaskFormer to predict a segmentation map from the generated image and compare it with the original semantic map. This helps illustrate the fidelity of the generated images to the ground truth.

Method comparison. We compare the results of our CAD method with a baseline approach that excludes coherence information in both Table [1](#tab_2) and Table [2](#tab_3), with the complete results. We conduct experiments in two settings: with text (first two rows) and without text (last four rows). Additionally, we compare against two CAD variations. Similar to the binning strategy in text-to-image generation, 'CAD bin' encodes coherence into 5 equally distributed discrete bins. Furthermore, 'CAD scalar' utilizes a singular scalar coherence score for the entire image, equivalent to the mean of the original coherence map.

## Results.

In Table [1](#tab_2) and Table [2](#tab_3), show the complete results of our method on ADE20k and COCO we demonstrate that using both the segmentation maps and the coherence maps lead to a decrease in FID for both scenarios, including or not the text input. This behavior is expected as our model possesses greater freedom to generate realistic content instead of sticking to the segmentation map uniquely (see e.g., the 4th column of Figure [10](#fig_9)). Furthermore, the improvement in the mIoU score can be attributed to two factors. First, when the input segmentation map is of low quality, the baseline method fails to capture important scene information. In contrast, our method benefits from additional information from the coherence map. Secondly, our method better leverages the caption in the low coherence region, mitigating the limitation of the segmentation map's limited number of classes (as seen in 4th row in Figure [11](#fig_10), there is no ping-pong table class in the COCO dataset).

## C.2. Additional Visualizations

We show additional results in Figure [10](#fig_9), where, from the left column to the right one, we highlight the segmentation input, the coherence map, the image generated by the baseline, the image generated by our methods and the reference image. The coherence map reveals spatial/shape details of the scene. For instance, when comparing our method to a ControlNet trained solely with the segmentation map, our approach, which incorporates both segmentation and coherence, accurately reconstructs the curtain's shape and the windows in the first row, or reconstructs a cloud in the back of the plane in the second row. Moreover, the efficacy of our method becomes even more apparent when the segmentation map is of poor quality and the coherence score is low. For instance, as shown in the third row, the basic ControlNet attempts to adhere to the limited information provided by the flawed segmentation map, resulting in a scene with multiple arms displayed (third column). In contrast, our approach benefits from the flexibility given by the coherence map, allowing for more consistent image generation. Interestingly, our method also exhibits localized self-correction, as shown on the fourth row. In particular, our model is refraining from generating the hand region due to its low coherence in the input. Finally, we demonstrate the model's responsiveness to textual input in the last row. We present in the last row an example where the original image does not contain snow, but have high coherence in the sky. Both models generate some snow based on the caption, but in line with the coherence map, our model does not generate snow in the sky, thus being closer to the real image.

## C.3. Prompt generalization

In this subsection, we demonstrate the sensitivity of our method to the caption input. We observe in Figure [11](#fig_10) that our CAD method can successfully generalize to different types of captions, such as 'dining table', 'billiard', or 'ping pong table' and adjust the scene accordingly. Moreover, even when the table is not explicitly mentioned in the caption (as seen on the rightmost side of the figure), our method exhibits strong generalization capabilities and successfully generates the table.

## C.4. Coherence Interpolation

In Figure [12](#fig_11), we demonstrate the significance of the coherence map in the conditioning of the ControlNet. In this experiment, we make an interpolation of the coherence map from the maximum coherence score everywhere (left) to very low coherence (right). When the input has high coherence throughout, the generated image lacks the presence of a ping pong table as it is not present in the semantic map. However, as the coherence score decreases, our methods recognize the shape of the ping pong table and successfully generates it in the image. It is worth noting that even at a low scale of the coherence score (second column corresponds to 1e-4 times the original value), our method is still able to reconstruct the table, even if it is not in the segmentation input. When we artificially reduce the coherence (two times less coherence, in the last column), our method is still able to generate a consistent scene without any artifacts.

## D. Class conditional experiments D.1. Simulating annotation noise

Our approach for class-conditional image generation relies on the assumption that the dataset comes with annotated coherence scores. However, such scores are not always available for traditional image-generation datasets. To address this issue, we propose to simulate annotation noise by resampling from a dataset with clean annotations. We associate an error probability Œ± with each label. We assume that when the annotator is wrong, they misclassify uniformly over all classes, where N is the total number of classes. This leads to the following model:

$»≥ ‚àº p y,Œ± with p y,Œ± ( »≤ = k) = 1 -Œ± if y = k, Œ± N -1 otherwise.(5)$We also define a strategy to remap the entire dataset using a normalized entropy-based coherence measure. We use the normalized entropy so that 0 maps to no coherence at all and 1 to total coherence in the label. We define the following coherence function:

$E(Œ±) = -1 log(N ) (1 -Œ±) log(1 -Œ±) + Œ± log Œ± N -1 for Œ± ‚àà 0, N -1 N .(6)$To ensure that the dataset has samples with varying levels of confidence, we define a target entropy cumulative distribution. To achieve this, we use a piecewise-linear function:

$E Œ≤,th (t) = t Œ≤ Œ∫ if t < Œ∫, 1 + (t -1) 1-Œ≤ 1-Œ∫ otherwise. (7$$)$where Œ∫ represents a threshold and Œ≤ represents the entropy at this threshold. This function construction defines a low entropy region before the threshold and a high entropy region after the threshold. Finally, for each sample in the dataset (X, y), we sample t ‚àà U[0, 1], and associate a target entropy u. We then compute the associated error probability Œ± = E -1 (u), and resample according to p y,Œ± to obtain the tuple (X, »≥, 1 -u) [2](#foot_1) . This process allows us to generate synthetic data points with varying degrees of annotation noise and coherence.

## D.2. Quantitative Results

In this section we add more results on ImageNet with different levels of noise. We observe in Table [3](#tab_5) results on ImageNet for Œ≤ ‚àà {0.2, 0.5, 0.8}. We first observe the less coherent the labels are the worse the results get in both image quality (FID) but also in accuracy (Acc). However, our method CAD manages to achieve better results than other methods in the context of un-coherent labels. This is further amplified when leveraging coherence aware guidance Caption: An hotel room with a bed, chair and table:

Caption: A white airplane on the runway:

Caption: A man sitting in front of a slot machine:

Caption: A large room with ping-pong tables and people playing:

Caption: A drilling rig in the middle of a snowy field:  

Segmentation Coherence Baseline Ours Real Image
## High coherence Low coherence

Figure [12](#fig_11). interpolation: In the first column, we artificially provide our ControlNet with a coherence map having the maximum value everywhere. Indeed, our models do not generate the ping pong tables. But, as soon as we decrease the coherence toward its original value, the ping pong tables start to appear. Finally, in the last column, we provide the model with a coherence map that is half as confident as the original value and demonstrate that we can generate an image without artifacts

## E. Theoretical analysis

In this section, we motivate the use of coherence as an additional conditioning for diffusion models. Under assumptions that are verified empirically, we show that coherence aware diffusion can transition from an unconditional model to a conditional model simply by varying the coherence passed to the model. First, we define a consistency property of the coherence embedding as follows: 

In other words, an embedding is coherence consistent if it tends to produce the same vector as the coherence approaches 0. This property is a sufficient condition to constrain the behavior of the diffusion model. Indeed, the following proposition is easily derived from it: Proposition E.1. Lipschitz continuous conditional neural diffusion models that leverage coherence consistent embed- dings for the conditioning are equivalent to unconditional models at low coherence.

Proof. We have to prove the following equivalent statement: Let œµ Œ∏ : x t , t, h(y, c) ‚Üí Œµt be a Lipschitz continuous neural diffusion model that predicts the noise Œµt at time t from the noisy sample x t with the help of the condition y embedded using the coherence consistent embedding h under coherence c. Then, ‚àÄŒ∑ > 0 and ‚àÄx t , t, y 1 Ã∏ = y 2 , there exists C > 0 such that for all 0 < c ‚â§ C, we have ‚à•œµ Œ∏ (x t , t, h(y 1 , c)) -œµ Œ∏ (x t , t, h(y 2 , c))‚à• 2 < Œ∑ . [(9)](#b8) By Lipschitz property of œµ Œ∏ , we have ‚à•œµ Œ∏ (x t , t, h(y 1 , c)) -œµ Œ∏ (x t , t, h(y 2 , c))‚à• 2 ‚â§ L 2 ‚à•h(y 1 , c) -h(y 2 , c)‚à• 2 .

From the coherence consistent property, there exists C > 0 such that for all 0 < c ‚â§ C, ‚à•h(y 1 , c) -h(y 2 , c)‚à• < ‚àö Œ∑/L.

The following contrapositive necessary condition on the coherence directly follows from this proposition: Corollary E.2. Lipschitz continuous conditional neural diffusion models that leverage coherence aware embeddings require high coherence to behave like conditional models.

In practice, we show in the experiments that the coherence consistency property tends to naturally emerge during training and that consequently coherence aware diffusion provides a tunable prompt parameter to sample from unconditional to conditional models.

## F. Additional Qualitative Results

In this section, we provide additional samples from our method. Most of the prompts are sampled from the Lexica.art website. 

![Figure 2. (a) Examples of images generated with the input coherence score between the prompt and the target image.The score varies from 0 (no coherence) to 1 (maximum coherence). Higher coherence scores tend to generate images that adhere more effectively to the prompt. Top prompt: "a raccoon wearing an astronaut suit. The racoon is looking out of the window at a starry night; unreal engine, detailed, digital painting,cinematic,character design by pixar and hayao miyazaki, unreal 5, daz, hyperrealistic, octane render", bottom prompt: "An armchair in the shape of an avocado" (b) Increasing the coherence from 0 to 1, CLIPScore increases and FID decreases.]()

![Figure 3. Images generated with a TextRIN trained with different handling of the misalignment between the image and its associated text at training. Compared to doing nothing (baseline), removing misaligned samples (filtering) or weighting the loss (weighted), our Coherence-Aware Diffusion training (CAD) generates more visually pleasing images while better adhering to the prompt.]()

![Figure 4. Text RIN Block. Architecture of the proposed Text RIN Block used in CAD. We include a cross attention from the text to the latent branch of the RIN block.]()

![Figure 5. Text-to-image generation results. (a) Quantitative results for text-to-image generation. We show that CAD achieves significantly lower FID, precision, recall, density and coverage while keeping similar CLIP score. (b) User study results. Users had to indicate the highest quality image and the most adhering to the prompt among pairs of images corresponding to our CAD method and one of baseline, filtered or weighted method. (c) FID versus CLIP on the text-to-image task for varying degrees of guidance œâ. We show that CAD achieves a significantly better trade-off with a much lower FID for the same CLIP score.]()

![Figure 6. Coherence-Aware Classifier-free Guidance for classes Malamute and Ice Cream with guidance rates œâ ‚àà {0, 1, 5, 20}.]()

![Figure 7. Impact of coherence on the model. Top: (a) Impact on FID and Accuracy of prompting a model (CAD on ImageNet with Œ≤ = 0.5). (b) FID vs Accuracy (using CA-CFG). We vary the guidance rate from 0 to 25. (c) Impact of prompting with different coherence scores on image generation. Low coherence indicates convergence towards an unconditional model. Bottom: (d) Quantitative results for class-conditional image generation. Our coherence aware diffusion (CAD) is compared to a baseline model and a training set filtering strategy for different levels of label noise Œ≤. We show that CAD achieves higher fidelity and better accuracy. (e) TSNE of a mixed embedding of the class label and the coherence score on CIFAR-10. Each color denotes a class and the transparency shows the coherence level: the more transparent, the less coherence.]()

![(b) Quantitative results on ADE20k and MS COCO. We report FID, mIoU, Precision (P), Recall (R), Diversity (D) and Coverage (C).]()

![Figure 8. (a) Image generation conditioned on a semantic map. The images generated for a given prompt p (shown below) are shown with respect to different pixel-level coherence scores c (shown above). Coherence scores are obtained either synthetically using Canny edge detection on the semantic map, or from the maximum of the softmax probability œÉ of a pre-trained model. They can then be edited either manually (œÉ + edit) or by blitting other coherence maps (rightmost). (b) Quantitative results on ADE20k and MS COCO.]()

![Figure 9. Architecture of the RIN Block modified to receive as input the coherence.]()

![Figure 10. Qualitative results on ADE20K: Examples of images generated conditionally to text and semantic maps.]()

![Figure 11. Caption generalization: Our methods demonstrate remarkable capability in leveraging the coherence map to generalize to diverse prompt inputs.]()

![We denote coherence consistent a conditional embedding h(y, c) of the condition y ‚àà Y under coherence c ‚àà [0, 1], if ‚àÄy 1 , y 2 ‚àà Y we have lim c‚Üí0 ‚à•h(y 1 , c) -h(y 2 , c)‚à• = 0 .]()

![Figure 13. Samples from our CAD-B model at 512 resolution with associated caption]()

![Quantitative results on ADE20K when conditioning on semantic maps. 'CAD bin' encodes the coherence into 5 equally distributed discrete bins. 'CAD scalar' uses a scalar coherence score for the whole image. CAD achieves better FID due to its enhanced ability to generate realistic objects in low coherence regions and superior mIoU as the leaked spatial information from the coherence map and the caption assist it to generate better samples (see Section C.1 for more details).]()

![Quantitative results on MS COCO when conditioning on semantic maps. 'CAD bin' encodes the coherence into 5 equally distributed discrete bins. 'CAD scalar' uses a scalar coherence score for the whole image (see Section C.1 for more details).]()

![Quantitative results for class-conditional image generation. Our coherence aware diffusion (CAD) is compared to a baseline model and a training set filtering strategy for different levels of label noise Œ≤. We show that CAD achieves higher fidelity and better accuracy.CFG œâ = 1 6.70 44.27 0.523 0.695 0.483 1.035 0.743]()

It is worth noting that using a Maskformer trained on the same dataset would result in high confidence map everywhere due to its high performance on the training set[[17]](#b16)

The coherence goes in the opposite direction of the entropy.

Figure 14. Samples from our CAD-B model at 512 resolution

Figure 15. Samples from our CAD-B model at 512 resolution

Figure 16. Samples from our CAD-B model at 512 resolution

Figure 17. Samples from our CAD-B model at 512 resolution

