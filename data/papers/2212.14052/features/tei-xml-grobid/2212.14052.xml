<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hungry Hungry Hippos: Towards Language Modeling with State Space Models</title>
				<funder>
					<orgName type="full">Analog Devices</orgName>
				</funder>
				<funder>
					<orgName type="full">Stanford Data Science Initiative</orgName>
				</funder>
				<funder ref="#_WZJbdfc">
					<orgName type="full">Department of Defense (DoD)</orgName>
				</funder>
				<funder ref="#_DMDWBs8">
					<orgName type="full">Xilinx</orgName>
				</funder>
				<funder ref="#_2XS9pnG">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_H2JXgHy">
					<orgName type="full">ONR</orgName>
				</funder>
				<funder>
					<orgName type="full">Accenture</orgName>
				</funder>
				<funder>
					<orgName type="full">Qualcomm</orgName>
				</funder>
				<funder>
					<orgName type="full">Total</orgName>
				</funder>
				<funder>
					<orgName type="full">VMWare</orgName>
				</funder>
				<funder ref="#_6q9Re8t">
					<orgName type="full">National Institutes of Health</orgName>
					<orgName type="abbreviated">NIH</orgName>
				</funder>
				<funder ref="#_PkGBVb9">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder>
					<orgName type="full">Facebook</orgName>
				</funder>
				<funder ref="#_3wNxTdw">
					<orgName type="full">Interactive Human-AI Teaming)</orgName>
				</funder>
				<funder>
					<orgName type="full">Salesforce</orgName>
				</funder>
				<funder>
					<orgName type="full">Ericsson</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2022-12-28">December 28, 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Daniel</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
							<email>tridao@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Khaled</forename><forename type="middle">K</forename><surname>Saab</surname></persName>
							<email>ksaab@stanford.edu</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Electrical Engineering</orgName>
								<orgName type="department" key="dep2">Department of Psychology</orgName>
								<orgName type="department" key="dep3">Department of Computer Science and Engineering</orgName>
								<orgName type="institution" key="instit1">Stanford University † †</orgName>
								<orgName type="institution" key="instit2">Stanford University ‡ ‡</orgName>
								<orgName type="institution" key="instit3">University at Buffalo</orgName>
								<address>
									<country>SUNY</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Armin</forename><forename type="middle">W</forename><surname>Thomas</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Electrical Engineering</orgName>
								<orgName type="department" key="dep2">Department of Psychology</orgName>
								<orgName type="department" key="dep3">Department of Computer Science and Engineering</orgName>
								<orgName type="institution" key="instit1">Stanford University † †</orgName>
								<orgName type="institution" key="instit2">Stanford University ‡ ‡</orgName>
								<orgName type="institution" key="instit3">University at Buffalo</orgName>
								<address>
									<country>SUNY</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Electrical Engineering</orgName>
								<orgName type="department" key="dep2">Department of Psychology</orgName>
								<orgName type="department" key="dep3">Department of Computer Science and Engineering</orgName>
								<orgName type="institution" key="instit1">Stanford University † †</orgName>
								<orgName type="institution" key="instit2">Stanford University ‡ ‡</orgName>
								<orgName type="institution" key="instit3">University at Buffalo</orgName>
								<address>
									<country>SUNY</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hungry Hungry Hippos: Towards Language Modeling with State Space Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-12-28">December 28, 2022</date>
						</imprint>
					</monogr>
					<idno type="MD5">0575B4FDA2E4348BEEA1BA0C11913553</idno>
					<idno type="arXiv">arXiv:2212.14052v3[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling. Moreover, despite scaling nearly linearly in sequence length instead of quadratically, SSMs are still slower than Transformers due to poor hardware utilization. In this paper, we make progress on understanding the expressivity gap between SSMs and attention in language modeling, and on reducing the hardware barrier between SSMs and attention. First, we use synthetic language modeling tasks to understand the gap between SSMs and attention. We find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence. To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches attention on the synthetic languages and comes within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid 125M-parameter H3-attention model that retains two attention layers surprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to improve the efficiency of training SSMs on modern hardware, we propose FlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on sequences up to 8K, and introduces a novel state passing algorithm that exploits the recurrent properties of SSMs to scale to longer sequences. FlashConv yields 2× speedup on the long-range arena benchmark and allows hybrid language models to generate text 2.4× faster than Transformers. Using FlashConv, we scale hybrid H3-attention language models up to 2.7B parameters on the Pile and find promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero-and few-shot learning on a majority of tasks in the SuperGLUE benchmark.</p><p>* Equal Contribution. Order determined by coin flip.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>State space models (SSMs) have achieved state-of-the-art sequence modeling performance in domains ranging from time series analysis <ref type="bibr" target="#b24">[25]</ref> to audio generation <ref type="bibr" target="#b21">[22]</ref>. However, they have yet to match the performance of Transformers on language modeling, often underperforming Transformers by multiple points in perplexity <ref type="bibr" target="#b24">[25]</ref>. An natural question is whether this gap in performance is due to inherent inductive biases and capabilities in attention <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b48">49]</ref>, or whether it is a function of the significant organizational resources that have been spent training and tuning large attention-based language models <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b65">66]</ref>, as well as specialized hardware support for attention, ranging from tensor cores <ref type="bibr" target="#b44">[45]</ref> to transformer chips <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b47">48]</ref>.</p><p>We take first steps towards answering these questions in this paper. First, we use synthetic language modeling tasks to show that there is an expressivity gap between SSMs and attention. Using our insights, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FlashConv</head><p>Figure <ref type="figure">1</ref>: Left: H3 stacks two discrete SSMs with shift and diagonal matrices and uses multiplicative interactions between input projections and their outputs to model comparisons between points in a sequence. Middle: H3 can perform associative recall-which is easy for attention, but not existing SSMs. Right: FlashConv uses a new state-passing algorithm over fused block FFTConv to increase hardware efficiency of SSMs, allowing H3 to scale to billion-parameter models.</p><p>we design a new SSM layer that nearly matches attention in language modeling. Second, we propose better hardware-aware algorithms for SSMs that allow them to take advantage of modern accelerators-and run faster than attention.</p><p>Understanding the Expressivity Gap. To understand the gap between SSMs and attention, we draw on synthetic language modeling tasks that have been proposed as a mechanistic basis for in-context learning in Transformers <ref type="bibr" target="#b48">[49]</ref> These synthetic languages focus on the ability to manipulate text-recalling tokens from earlier time steps, or comparing tokens from different points in a sequence. We find that existing SSMs struggle to model these synthetic languages. To probe how important these skills are for language modeling, we propose H3 (Hungry Hungry Hippo), a new SSM-based layer designed to solve these language modeling tasks. H3 stacks two SSMs, with multiplicative interactions between their outputs and input projections. The SSMs allow H3 to keep a log of tokens (to recall them later), while the multiplicative interactions allow for comparisons across the sequence.</p><p>H3 matches attention on the synthetic languages and almost closes the gap with Transformers on language modeling-coming within 0.4 perplexity of Transformers on OpenWebText (compared to 3.4 ppl for existing SSMs-even those explicitly designed for language modeling <ref type="bibr" target="#b41">[42]</ref>). Furthermore, a simple hybrid H3-attention model that retains two attention layers surprisingly outperforms Transformers on OpenWebText by 1.0 perplexity. To further evaluate H3 on language modeling, we train 125M-, 355M-, 1.3B-, and 2.7B-parameter hybrid H3-attention language models on the Pile <ref type="bibr" target="#b20">[21]</ref>, using hyperparameters from GPT-3 <ref type="bibr" target="#b6">[7]</ref>. These hybrid models outperform Transformer-based language models of the same size in perplexity, and match or outperform them on a majority of tasks in the SuperGLUE benchmark in zero-and few-shot learning. Since the SSM layers in these hybrid models admit a recurrent view, they can also perform 2.4× faster inference than Transformers.</p><p>Scaling SSMs. Next, we improve the efficiency of SSMs on modern hardware, to reduce the hardware barrier between attention and SSMs. SSMs scale nearly linearly in sequence length instead of quadratically like attention, but still run slower on modern hardware due to poor hardware utilization. To close this gap, we propose FlashConv, a hierarchical algorithm for computing SSMs, inspired by IO-Aware attention <ref type="bibr" target="#b14">[15]</ref>. The technical challenge is that SSMs require a FFT-based convolution over the input sequence, which requires an FFT, pointwise multiply, and inverse FFT. When implemented in cuFFT <ref type="bibr" target="#b46">[47]</ref>, this operation incurs expensive GPU memory reads/writes, and cannot utilize the specialized matrix multiply units available on modern hardware 1 . To use specialized matrix multiply units, we appeal to classical techniques that split the FFT into blocks and compute it using a series of matrix multiplications. Combined with kernel fusion, this "block" FFT solution increases hardware efficiency, but only as long as the sequence length can fit into GPU SRAM 1 An A100 GPU has a maximum of 312 TFLOPs/s of FP16 with tensor cores, but only 20 TFLOPs/s of FP32 (and 40 TFLOPs/s of FP16) without tensor cores <ref type="bibr" target="#b45">[46]</ref>. This trend started with the V100 GPUs <ref type="bibr" target="#b44">[45]</ref> and has continued with the H100 GPUs <ref type="bibr" target="#b47">[48]</ref>.</p><p>(on-chip memory, analogous to L1 cache on the CPU)-up to sequence length 8K on modern A100.</p><p>To scale to sequences longer than 8K, we propose a state passing algorithm (Figure <ref type="figure">1</ref> right), specialized to SSMs. The key insight is that we can use the recurrent properties of SSMs to process the input in chunks-as long as we keep track of an additional state vector. The state passing algorithm splits the input into the largest chunks that can fit into GPU SRAM, efficiently computes the FFT-based convolution using block FFT, and updates an intermediate state to start the next chunk. Using this state-passing algorithm, FlashConv can scale SSMs to any sequence length-even longer than can fit on GPU SRAM at once-while maintaining a near linear compute complexity. FlashConv sets state-of-the-art speed on long range arena using S4 <ref type="bibr" target="#b24">[25]</ref>, outperforming Transformers by 5.8× and previous S4 models by 2×. FlashConv trains H3 4-8× times faster than attention for long sequences, and is a critical component for scaling to billion-parameter models<ref type="foot" target="#foot_0">foot_0</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>We present some background on state space models and linear attention, which inspired our H3 layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">State Space Models</head><p>A continuous-time state-space representation <ref type="bibr" target="#b5">[6]</ref> defines a linear mapping from an input signal u(t) ∈ R (as a function of time t) to an output signal y(t) ∈ R through a state-variable x(t) ∈ R m , with the following differential equation, for some matrices</p><formula xml:id="formula_0">A ∈ R m×m , B ∈ R m×1 , C ∈ R 1×m , D ∈ R 1×1 : ẋ(t) = Ax(t) + Bu(t), y(t) = Cx(t) + Du(t).</formula><p>Similarly, a discrete-time state-space representation defines a linear mapping from a discrete input signal u i (for i = 1, 2, . . . ) to a discrete output signal y i though a state-variable x i ∈ R m :</p><formula xml:id="formula_1">x i = Ax i-1 + Bu i y i = Cx i + Du i .</formula><p>A state-space model (SSM) uses these representations as a layer in a deep learning pipeline, where the matrices A, B, C, D are learned from data (e.g., with gradient-based optimization). One often has d of these SSMs in parallel, each corresponding to one hidden dimension. To preserve the sequence history, HiPPO <ref type="bibr" target="#b23">[24]</ref> projects the history on a basis of orthogonal polynomials, which translates to having SSMs whose A, B matrices are initialized to some special matrices.</p><p>This recurrent form of SSMs allows efficient inference (i.e., generation): to generate the output of the next time-step, one only needs the state of the current time-step, not the entire input history. Furthermore, SSMs can freely extrapolate to sequences longer than seen during training.</p><p>SSMs as Convolution. For efficient training, given the entire sequence of the input u 1 , . . . , u N , the output sequence y 1 , . . . , y N can also be written as the convolution of the input with the filter <ref type="bibr" target="#b26">[27]</ref>:</p><formula xml:id="formula_2">f = [CB, CAB, CA 2 B, . . . , CA N -1 B].</formula><p>That is, from an initial condition x 0 , we have y i = CA i Bx 0 + (f * u) i + Du i , where (f * u) denotes a linear convolution between f and u. If we set the initial condition x 0 to be zero, then y is exactly a linear convolution of u, with a residual connection Du. More generally, any linear time-invariant system (of which SSMs are a special case) can be written as a convolution.</p><p>Given a 1D input sequence u ∈ R N of length N , we denote the 1D output sequence y ∈ R N of an SSM parameterized by matrices A, B, C, D as</p><formula xml:id="formula_3">y = SSM A,B,C,D (u).</formula><p>To simplify notation, we omit the reference to A, B, C, D and write y = SSM(u) if they are clear from context. When u is multidimensional of dimension d, we stack d of these SSMs together that defines a mapping from u ∈ R N ×d to y ∈ R N ×d , using the same notation y = SSM(u).</p><p>To construct the filter f from A, B, C efficiently, A is often constrained to be diagonal <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b28">29]</ref>, or diagonal plus low-rank <ref type="bibr" target="#b24">[25]</ref>.</p><p>SSM through FFTs. Computing the convolution naively through conventional matrix operations is expensive for long kernels, scaling as O(N 2 ). Instead, we can use FFTs: take the FFT of f and u, multiply them together pointwise, and then take the inverse FFT. This yields an O(N log N ) algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Linear attention</head><p>We describe linear attention <ref type="bibr" target="#b34">[35]</ref> and its connection to RNNs, which inspired our model design (Section 3).</p><p>In standard attention <ref type="bibr" target="#b61">[62]</ref>, we have N query/key/value tokens Q i , K i , V i ∈ R d for i = 1, . . . , N , where N is the sequence length and d is the head dimension. For some similarity metric Sim : R d × R d → R, we want to compute the output:</p><formula xml:id="formula_4">O i = i j=1 Sim(Q i , K j )V j i j=1 Sim(Q i , K j ) ∈ R d .</formula><p>For standard softmax attention, Sim(q, k) = e q k (often the dot product is scaled by 1/ √ d). Linear attention makes the assumption that Sim has the form Sim(q, k) = φ(q) φ(k), for some (nonlinear) function φ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The output is then</head><formula xml:id="formula_5">O i = φ(Qi) i j=1 φ(Kj )V j φ(Qi) i j=1 φ(Kj ) . Let S i = i j=1 φ(K j )V j ∈ R d×d , z i = i j=1 φ(K j ) ∈ R d , d i = φ(Q i ) z i ∈ R. Then O i = φ(Qi) Si di</formula><p>. This connects linear attention to RNNs: the output O i is a function of S i and z i , both of which are incrementally updated (as cumulative sums).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Hungry Hungry Hippos Layer to Model Discrete Sequences</head><p>To understand the gap between SSMs and attention on language modeling, we examine two synthetic language modeling tasks. These tasks motivate our H3 layer to add a discrete SSM (based on shift matrix) and multiplicative interaction to effectively model discrete sequences. We then show that the H3 layer is expressive enough to solve these synthetic tasks, and that this understanding leads to better performance on a real language modeling benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Motivation: Synthetic Language Modeling Tasks</head><p>We describe two closely-related synthetic tasks, summarized in Table <ref type="table" target="#tab_0">1</ref>. Olsson et al. <ref type="bibr" target="#b48">[49]</ref> argue that the ability to solve (variants of) these tasks accounts for the majority of the in-context learning capability of Transformers, and more intuition is given in Appendix E. The Induction Head task tests how well a model can recall content after a special token (e.g., in Table <ref type="table" target="#tab_0">1</ref>). At the end of the sequence, the model must recall the token that appeared immediately after the special token earlier in the sequence. Associative Recall <ref type="bibr" target="#b0">[1]</ref> is similar to the induction head task, but requires the model to remember multiple key-value pairs. At the end of the sequence, the model must recall a specific value belonging to a specific key. Table 2 (for two-layer models) shows that S4D <ref type="bibr" target="#b25">[26]</ref> and Gated State Spaces <ref type="bibr" target="#b41">[42]</ref> both fail to model these synthetic languages, which suggests they may not have the expressivity for general language. We argue that these failures suggest two missing capabilities: (i) to remember tokens that appear after a particular event (e.g., the special token in the induction head task), and (ii) to compare tokens across the sequence (e.g., comparing keys to decide which value to recall). Attention has both these capabilities: it can compare tokens by constructing the quadratic attention matrix QK , and it can recall tokens by direct copying (multiplying softmax(QK ) with V). In Section 3.2, we design our new layer H3 to enable these capabilities in SSMs, narrowing the expressivity gap between SSMs and attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">H3 Layer</head><p>H3 uses SSMs with shift and diagonal matrices, along with multiplicative operations against projections of the input to capture the missing capabilities identified by the synthetics.</p><p>High-level Intuition. (i) To remember tokens from the past, we want the state x i to copy from the input u i , and then pass that information to the next state x i+1 . As x i+1 relates to x i by Ax i , we use a discrete SSM with a shift matrix A (described formally below) that shifts the elements of a state vector (e.g., mapping [a, b, c] → [0, a, b]). (ii) To compare tokens across the sequence, we use multiplicative interaction: the output of an SSM, containing information from previous time steps, is multiplied with the input at the current time steps, thus measuring similarity between tokens.</p><p>H3 is loosely inspired by linear attention (Section 2): we project the input u to get three signals Q, K, V. Then we replace the non-linearity φ(K) with an SSM where A is a shift matrix (SSM shift ), and we replace the summation S i with a SSM with diagonal A (SSM diag ). The output, for the case of head dimension d h = 1, is:</p><formula xml:id="formula_6">Q SSM diag (SSM shift (K) V),</formula><p>where denotes pointwise multiplication. We can view this form as stacking two SSMs with multiplicative interaction (each is a "hungry hippo", hence the name of our layer). A more formal connection between linear attention, time-varying systems, and H3 can be found in Appendix B.</p><p>Remembering Key Tokens: Shift and Diagonal SSMs. The shift and diagonal SSMs are designed to address the capability to log tokens after particular events. In the shift SSM, we constrain A ∈ R m×m to be a shift matrix A i,j = 1 for i -1 = j 0 otherwise . The action of this matrix on the hidden state x i is to shift each coordinate down by one-thereby creating a "memory" of the previous states. For example, if B = e 1 , the first basis vector, then</p><formula xml:id="formula_7">x i = [u i , u i-1 , . . . , u i-m+1</formula><p>] contains the inputs from the previous m time steps. We learn B and C (B can also be fixed to e 1 for simplicity, in which case the output is a 1D conv. with kernel size m).</p><p>The diagonal SSM constrains A to be diagonal and initializes it from the diagonal version of HiPPO (S4D <ref type="bibr" target="#b25">[26]</ref>). This parameterization allows the model to remember state over the entire sequence. The shift SSM can detect when a particular event occurs, and the diagonal SSM can remember a token afterwards for the rest of the sequence.</p><p>Multiplicative Interaction for Comparison. We take the multiplicative interactions from linear attention, but they provide another missing capability when combined with a shift matrix: comparing tokens across the sequence. The multiplicative interactions between the output of the shift SSM and the V projection mimics local multiplicative interactions in linear attention (depending on the size of the hidden state). Similarly, multiplicative interactions with the Q projection and the output of the diagonal SSM allows comparisons between tokens over the entire sequence.</p><p>H3 Layer. The overall layer is given in Algorithm 1 and shown schematically in Figure <ref type="figure">1</ref> (left). We use the H3 layer to construct a model in the same style as Transformers by interleaving it with MLPs, connected by residual connection and layer norm (i.e., pre-norm architecture <ref type="bibr" target="#b1">[2]</ref>). We will also consider a hybrid H3-attention model (two attention layers while the rest are H3, Sections 3.3 and 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 H3 Layer</head><p>Require: Input sequence u ∈ R N ×d from the previous layer, weight matrices WQ, WK , WV , WO ∈ R d×d , a shift SSM SSM shift , a diagonal SSM SSM diag , head dimension d h .</p><formula xml:id="formula_8">1: Compute Q = uWQ, K = uWK , V = uWV ∈ R N ×d . 2: Pass K through the shift SSM: K = SSM shift (K) ∈ R N ×d . 3: Split Q, K, V into H "heads" (Q (h) , K (h) , V (h) for h = 1, . . . , H), each a sequence of N vectors of size d h = d/H. 4: for 1 ≤ h ≤ H do 5:</formula><p>Take the batched outer product K (h) (V (h) ) ∈ R N ×d h ×d h (batched in the N -dimension) and pass it through a diagonal SSM:</p><formula xml:id="formula_9">KV (h) = SSM diag (K (h) (V (h) ) ) ∈ R N ×d h ×d h . 6:</formula><p>Batch-multiply by</p><formula xml:id="formula_10">Q: O (h) = [Q (h) 1 KV (h) 1 , . . . , Q (h) N KV<label>(h)</label></formula><p>N ] ∈ R N ×d h (batched in the N -dimension). 7: end for 8: Concatenate the output O (h) of each head, and multiply by the output projection matrix WO ∈ R d×d .</p><p>Efficiency We show that H3 scales as O(N log N ) with sequence length N -asymptotically more efficient than attention, which typically requires O(N 2 d) time and O(N 2 ) space<ref type="foot" target="#foot_1">foot_1</ref> (proof in Appendix D.3). Proposition 1. Let N be the sequence length, d be the hidden dimension, and assume that the head dimension d h is of order O(1). Then the H3 layer takes O(d 2 N + dN log N ) time and O(dN ) space to compute.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Expressivity</head><p>We show that H3 can model our synthetic languages, as well as natural language on OpenWebText <ref type="bibr" target="#b22">[23]</ref>. We also present a hybrid H3-attention extension that outperforms Transformers on OpenWebText.</p><p>Mechanism for Solving Associative Recall with H3. H3 is expressive enough to solve our synthetic language modeling tasks, as shown in Table <ref type="table" target="#tab_1">2</ref>. Figure <ref type="figure">1</ref> (middle) shows a mechanism for a single H3 layer to solve the associative recall task for a particular key-value pair (a, 3). The shift SSM and following multiplicative interaction act as a gate on whether to let a value through to the diagonal SSM, based on whether the previous token was key a. The diagonal SSM stores the value 3 in memory, and continually outputs it. The final multiplicative interaction gates whether to let the diagonal SSM's output through-based on whether the current input token is the key a. We formally construct the weights of an H3 layer to solve this task in Appendix D.1. Better Synthetic Language Modeling Translates to Better Natural Language Modeling. We validate that when H3 can solve these synthetic tasks, it also improves the modeling capability on natural language (e.g., on the OpenWebText dataset). As shown in Table <ref type="table" target="#tab_2">3</ref>, H3 comes within 0.4 perplexity points of Transformers when trained for 50B tokens on OpenWebText, and performs much better than existing SSM variants (S4D, GSS), by 3 -3.9 points.</p><p>Extension: H3-attention Hybrid Model. A simple hybrid H3-attention language model surprisingly outperforms Transformers on OpenWebText by 1.0 point. Our hybrid model simply retains two self-attention layers: one in the second layer, and one in the middle (layer 2 + N/2 for an N -layer model, N even). The H3-attention hybrid also outperforms the GSS-attention hybrid <ref type="bibr" target="#b41">[42]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">FlashConv: Efficiently Training SSMs</head><p>To improve the efficiency of SSMs on modern hardware, we propose FlashConv. FlashConv fuses the FFT, pointwise multiply, and inverse FFT to reduce memory reads/writes. It also uses a block FFT algorithm to make use of specialized matrix multiply units (e.g., tensor cores on A100) for sequence lengths up to 8K. For sequences longer than 8K, the computation no longer fits in GPU SRAM <ref type="foot" target="#foot_2">4</ref> , so we propose a novel state-passing algorithm that splits the sequence into chunks to compute the FFT convolution one chunk at a time. FlashConv can speed up any SSMs (not just H3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Fused Block FFTConv</head><p>We deploy two techniques to speed up the FFT-based convolution for sequences shorter than 8K: kernel fusion and block FFT. Kernel fusion addresses IO bottlenecks due to reading and writing of intermediate results, while block FFT allows the FFT-based convolution to utilize specialized matrix multiplication units. These techniques allow us to speed up FFTConv by 2× (Section 6) for sequences shorter than 8k.</p><p>Kernel Fusion. Naive implementations of FFTConv using standard libraries such as cuFFT are IObound due to repeated reading and writing of intermediate results. The FFT convolution in an SSM with input u and filter f has the form iF F T (F F T (u) F F T (f )) (where denotes pointwise multiplication).</p><p>It requires reading and writing intermediate results to GPU memory-which can dominate the runtime.</p><p>Following FlashAttention <ref type="bibr" target="#b14">[15]</ref>, we first fuse the entire FFTConv into a single kernel and compute it in SRAM to avoid this overhead. Block FFT. To further speed up the computation of FFT-based convolution, we exploit specialized matrix multiplication hardware on modern GPUs (e.g., Tensor Cores on Nvidia GPUs perform fast 16 × 16 matrix multiplication). We appeal to classical results that show that the FFT can be written as a series of block-diagonal matrix multiplications interleaved with permutation. We note that such algorithms are not new, but our setting (fused FFTConv on GPU) introduces new bottlenecks-by removing the IO bottlenecks, compute becomes the bottleneck (note that a single FFT on GPU is usually IO bound).</p><p>Suppose that we want to perform an N -point FFT, which is equivalent to multiply by the DFT matrix F N . Suppose that N = N 1 N 2 for some integers N 1 , N 2 . By the Cooley-Tukey decomposition of the DFT <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11]</ref> (also known as the four-step FFT algorithm), we can write F N = P(I N2 ⊗ F N1 )P D(I N1 ⊗ F N2 )P, where P denotes a fixed permutation that reshapes the input as a N 1 × N 2 array and then transpose it, ⊗ denotes Kroneker product, D is a N × N diagonal matrix (called the twiddle factors) <ref type="bibr" target="#b13">[14]</ref>, and I Ni and F Ni are the identity and DFT matrix of size N i × N i . As I N2 ⊗ F N1 and I N1 ⊗ F N2 are just block-diagonal matrices, we can make use of specialized matmul units to perform these multiplications. Similarly, if N = N 1 N 2 N 3 then we can decompose the N -point FFT into a series of (block) FFT of size N 1 , N 2 , and N 3 , interleaved by permutation.</p><p>The block FFT algorithm incurs O(N r log N/ log r) FLOPs for a sequence length N , if N can be written as r p for two integers r, p. This incurs more FLOPs than standard FFT (O(N log N )), but can run faster when we using specialized matrix multiplication hardware.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">State-Passing</head><p>However, the fused kernel cannot run if the sequence is too long to fit into GPU SRAM (longer than 8K on A100). We show how to exploit the particular form of the FFT in SSM to speed it up for long sequences.</p><p>The recurrent nature of SSMs allows us to split the FFTConv of a length-N sequence into chunks of size N each (N is the longest FFT we can fit into SRAM), assuming N is a multiple of N ). We use FFTConv to compute each chunk and use a recurrence to connect the chunks. In particular, we split the inputs u into C = N/N chunks u (c) ∈ R N for c = 1, . . . , C. Similarly, split the states x into x (c) ∈ R N ×m and the output y into y (c) ∈ R N for i = 1, . . . , C. We will only need the end-state x (c) N of each chunk c. Let f = [CB, CAB, CA 2 B, . . . , CA N -1 B] be the SSM filter. Recall from Section 2 that for each chunk c, y</p><formula xml:id="formula_11">(c) i = CA i Bx (c-1) N + (f * u (c) ) i + Du (c) i , since x (c-1) N</formula><p>, the end-state of the previous chunk (c -1) is the initial condition for the current chunk c. In vector notation, (c) for some matrix M xy ∈ R N ×m . Additionally we need to update the end-state of each chunk with</p><formula xml:id="formula_12">y (c) = M xy x (c-1) N + f * u (c) + Du</formula><formula xml:id="formula_13">x c N = A N x (c-1) N + M ux u (c)</formula><p>for some matrix M m×N ux (derivation in Appendix C.2). In essence, we can compute the output for each chunk with FFT-based convolution as long as we remember the end-state of the previous chunk, and the end-state of each chunk can be updated recurrently. This yields a state-passing algorithm for long sequences, where we only compute FFT of length N , and update some hidden state each iteration.</p><p>Let BlockFFTConv refer to our fused block FFTConv kernel. Then, the state-passing algorithm for 1D input is given by Algorithm 2. For inputs of dimension d where we stack d SSMs, we simply batch Algorithm 2 along the d-dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 State Passing Algorithm</head><formula xml:id="formula_14">Require: Input u ∈ R N , SSM parameterized by matrices A ∈ R m×m , B ∈ R m×1 , C ∈ R 1×m , D ∈ R 1×1 , chunk size N where N is a multiple of N . 1: Precompute A N ∈ R m×m , Mux = [A N -1 B, . . . , B] ∈ R m×N , Mxy = [C, CA, . . . , CA N -1 ] ∈ R N ×m . 2: Split the inputs u1:N into C = N/N chunks u (c) 1:N for c = 1, . . . , C. 3: Let the initial state be x (0) N = 0 ∈ R m . 4: for 1 ≤ c ≤ C do 5: Compute y (c) = Mxyx (c-1) N + BlockFFTConv(f , uj) +Du (c) ∈ R N . 6: Update state: x (c) N = A N x (c-1) N + Muxu (c) .</formula><p>7: end for 8: Return y = [y (1) . . . y (C) ]. We prove that Algorithm 2 yields the same output as if one has computed the SSM using a large FFT of size N (proof in Appendix D.4): Proposition 2. For input u ∈ R N and matrices A, B, C, D, the output y ∈ R N returned by Algorithm 2 is the same as the output defined by the SSM parameterized by A, B, C, D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">H3 Evaluation</head><p>To understand how well capturing the synthetics in Section 3.1 translates to language modeling, we train two hybrid hybrid H3-attention language models at sizes 125M, 355M, 1.3B, and 2.7B, and we evaluate their performance against Transformers. The hybrid models match or exceed the quality of Transformers in perplexity and zero/few-shot learning. We also validate that H3 models retain strong performance on non-text sequence modeling. Appendix F contains additional experiments on more datasets, length extrapolation, and scaling with data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Language Modeling</head><p>We compare hybrid H3-attention language models against Transformer-based language models. We evaluate language modeling performance using perplexity, zero-shot learning, and few-shot learning performance. Hybrid H3 models outperform Transformers, which suggests that closing the gap between SSMs and attention on the synthetic languages translates to real language modeling capabilities. We also report the generation speed of hybrid H3 models compared to Transformers; since SSMs are recurrent models, they can generate tokens 2.4× faster than Transformers. Appendix F shows performance of pure H3 language models on these same evaluation metrics.</p><p>Setup We train hybrid models at sizes 125M, 355M, 1.3B, and 2.7B on the Pile <ref type="bibr" target="#b20">[21]</ref> for 400B tokens. We compare against checkpoints of equivalent sizes from Open-AI <ref type="bibr" target="#b52">[53]</ref> and GPT-Neo<ref type="foot" target="#foot_3">foot_3</ref>  <ref type="bibr" target="#b3">[4]</ref>, from HuggingFace <ref type="bibr" target="#b64">[65]</ref>. Perplexity Table <ref type="table" target="#tab_3">4</ref> shows perplexity on the Pile <ref type="bibr" target="#b20">[21]</ref>, OpenWebText <ref type="bibr" target="#b22">[23]</ref>, and WikiText-103 <ref type="bibr" target="#b42">[43]</ref>. On the Pile, our 125M hybrid model outperforms GPT-Neo, which was also trained on the Pile. Our hybrid models also outperform GPT-Neo models and GPT-2 models on zero-shot transfer to OpenWebText and WikiText103. We report the PPL of GPT-2 models for context, though the performance is not directly comparable since they were trained on different data.</p><p>Zero-and Few-shot Performance We compare the zero-and few-shot performance of hybrid H3 language models against OPT <ref type="bibr" target="#b65">[66]</ref>, GPT-Neo, and GPT-2 models, where public checkpoints are available. We report performance with rank classification on the logits of the possible choices (see Appendix F.7 for raw generation). Table <ref type="table" target="#tab_4">5</ref> reports zero-shot performance on the SuperGLUE benchmark, and Table <ref type="table" target="#tab_5">6</ref> reports the 3-shot performance. Following the perplexity results, the hybrid language models outperform or match the best the Transformer baseline on more than half the tasks.</p><p>Language Modeling Inference Finally, since SSMs are recurrent models, they admit faster text generation than Transformers. Table <ref type="table" target="#tab_6">7</ref> shows inference throughput of a 1.3B-parameter hybrid model compared to a Transformer. The hybrid model has up to 2.4× higher throughput.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">FlashConv Evaluation</head><p>We evaluate how well FlashConv speeds up SSMs. FlashConv sets state-of-the-art performance on the long range arena benchmark <ref type="bibr" target="#b58">[59]</ref> using S4 <ref type="bibr" target="#b24">[25]</ref>. We report performance of training H3 module with FlashConv compared to attention at various sequence lengths, from 256 to 32K and demonstrate nearly linear scaling.</p><p>Long Range Arena The Long Range Arena (LRA) benchmark <ref type="bibr" target="#b58">[59]</ref> is a benchmark for long-range sequence modeling. The state-of-the-art approach, S4 <ref type="bibr" target="#b27">[28]</ref>, is an SSM. Table <ref type="table" target="#tab_7">8</ref> shows that FlashConv accelerates S4 by 2×, outperforming Transformers by 5.8×.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Benchmark H3 Against Attention</head><p>We benchmark the time to run the forward and backward pass of H3 with FlashConv against attention. FlashConv maintains nearly linear scaling, even to very long sequence lengths. Fig. <ref type="figure">2</ref> shows overall 2-3× speedup over FFTConv with cuFFT using our techniques (block FFT, state-passing). Simple kernel fusion (even without block FFT) can yield speedup over cuFFT for short sequences, since memory reads/writes are the bottleneck for short sequences. For long sequences, SSMs using state passing can be dozens of times faster than even the fastest attention implementation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Time (ms)</head><p>Figure <ref type="figure">2</ref>: We compare the speed of different algorithms to perform FFT-based convolution, along with FlashAttention <ref type="bibr" target="#b14">[15]</ref> (the fastest attention implementation we know of). We use batch size 8, hidden dimension 1024, and varying sequence length from 256 to 32k, and measure on an A100-SMX4-40GB GPU. We see that kernel fusion gives up to 3.4× speedup over naive FFTConv for short sequences (up to 512), block FFT gives up to 2× speedup for medium length sequences (1k -8k), and state-passing allows 2.3× faster FFTConv for long sequences (16k and above).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>Our main goal is to understand and narrow the gap between attention and SSMs in language modeling in terms of modeling capabilities and hardware efficiency. Our exploration based on synthetic language tasks motivated us to design the H3 layer, which is surprisingly competitive with attention. Our BlockFFTConv algorithm exploits matrix multiplication units and the dual recurrent-convolution view of SSMs to substantially speed up SSMs, reducing the hardware barrier between attention and SSMs. We are excited about several future directions. Our H3 layer is a simple combination of two SSMs, and more sophisticated designs could be more expressive. Our encouraging results on language models up to 1.3B parameters suggests that scaling SSMs to larger sizes is a promising avenue. Since simply adding two attention layers to H3 models already outperforms both the pure H3 model and Transformers, we are optimistic about combining the complementary strengths of SSMs and attention in the future. notation thereon. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of DARPA, NIH, ONR, or the U.S. Government. Atri Rudra's research is supported by NSF grant CCF-1763481.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Related Work</head><p>State space models have shown promise in modeling sequential data, including time series data <ref type="bibr" target="#b24">[25]</ref>, audio <ref type="bibr" target="#b21">[22]</ref>, and visual data <ref type="bibr" target="#b43">[44]</ref>. Our model builds off work on simplifying and parameterizing diagonal versions of S4 <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>. Gated state spaces <ref type="bibr" target="#b41">[42]</ref> also aim to adapt SSMs to language modeling, but our results suggest that the GSS model does not perform as well as H3 (or even as well as earlier SSMs like S4D). The idea to combine SSMs with attention in hybrid models is not new; Mehta et al. <ref type="bibr" target="#b41">[42]</ref> also showed that interleaving attention with their GSS layer can improve performance, which we also validate on our OpenWebText experiments. These positive results suggest that attention and SSMs are complementary, and that hybrid models may be a promising direction for future work.</p><p>Large language foundation models <ref type="bibr" target="#b4">[5]</ref> have demonstrated the power of scaling attention-based networks to billions of parameters and training them on trillions of tokens <ref type="bibr" target="#b31">[32]</ref>. Understanding the mechanistic basis <ref type="bibr" target="#b17">[18]</ref> behind these models may yield insights into better design choices for future models. These and similar explorations have informed the design of H3 and our selection of synthetic languages. A number of recent works have also explored how to address the shortcomings of attention by approximating the attention computation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr">39,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b62">63]</ref>. We believe these efforts are complementary to SSMs, and we are excited to see how they can be combined in future work.</p><p>Linear attention <ref type="bibr" target="#b34">[35]</ref> and classical sequence models like RNNs serve as inspiration for H3. Appendix B draws a direct connection between linear attention and LTI systems. Luo et al. <ref type="bibr" target="#b39">[40]</ref> also propose a variant of linear attention that can achieve O(n log n) scaling in sequence length. Appendix F evaluates linear attention on language modeling, and finds that it underperforms exact attention, whereas H3 outperforms attention. The multiplicative interactions in H3 are reminiscent of gating mechanisms in LSTMs <ref type="bibr" target="#b30">[31]</ref> and GRUs <ref type="bibr" target="#b7">[8]</ref>, which suggests that architectural lessons from these sequence models may be useful for adapting SSMs to language modeling. A number of algorithms for scaling attention to longer sequences have also been proposed, such as Transformer-XL <ref type="bibr" target="#b12">[13]</ref>, Reformer <ref type="bibr">[39]</ref>, Performer <ref type="bibr" target="#b8">[9]</ref>, and Perceiver AR <ref type="bibr" target="#b29">[30]</ref>. Some of these approaches underperform exact attention on language modeling, and may be slower in wall-clock speed <ref type="bibr" target="#b14">[15]</ref>. A thorough comparison of these alternatives to exact attention and how well they scale in model size and amount of training data is fruitful future work.</p><p>FFT algorithms are used in a wide variety of applications, including signal processing <ref type="bibr" target="#b49">[50]</ref>, control theory <ref type="bibr" target="#b5">[6]</ref>, and more. Various algorithms for computing the FFT have existed for decades <ref type="bibr" target="#b50">[51]</ref>. We hope our work on appealing to these classic algorithms to accelerate new applications such as learned SSMs will inspire future algorithmic exploration, even if hardware is not designed for them <ref type="bibr" target="#b32">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Linear Attention and Time-Varying Systems</head><p>We draw some connections from linear attention to LTI systems and SSMs.</p><p>We first present linear attention as a linear time-varying system, and show how converting it to a linear time-invariant system matches H3.</p><p>Linear time-varying system and linear attention In general, a layer in a sequence model takes in a sequence and outputs a sequence. Many of these take the form of a linear time-varying system (thanks to the Picard-Lindelof theorem, nonlinear systems can be approximated by a series of linear system):</p><formula xml:id="formula_15">x i = A i x i-1 + B i u i , y i = C i x i + D i u i .</formula><p>This has the same form as SSMs (Section 2), except that the matrices can depend on the timestep.</p><p>Recall the form of linear attention from Section 2. For the purpose of approximation, we ignore the denominator in linear attention Section 2 (i.e., assuming d i = 1). We see that S i is just a cumulative sum, satisfying the recurrence S i+1 = S i +φ(K i+1 )V T i+1 . Similarly, O i satisfies the recurrence O i+1 = φ(Q i+1 ) T S i+1 . This is a linear time-varying system of the form x i+1 = Ax i + Bu i+1 and y i+1 = C i+1 x i+1 (with A = I,</p><formula xml:id="formula_16">B = I, u i = φ(K i )V T i , C i = φ(Q i ) T ).</formula><p>That is, A and B are constant, but C is time-variant. To convert this into a linear time-invariant version, we treat the time-variant C i as a post-processing step.</p><p>We instead of a fixed C for the LTI. This yields an LTI:</p><formula xml:id="formula_17">x i+1 = Ax i + Bφ(K i )V T i , y i+1 = Cx i ,</formula><p>for some matrices A, B, C that are learned. We then apply post-processing by multiply y i+1 with φ(Q i ) T . Replacing φ(K i ) with a shift SSM yields an analogue to H3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Method details</head><p>Since we have described the forward pass in Section 3, we describe here the backward pass in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Backward Pass</head><p>We show how to compute the backward pass in a fused kernel.</p><p>Let y = f * u + Du. In our case, we have f and u have the same length, so they are symmetric as far as the convolution is concerned.</p><p>Suppose we are given dy = ∂l ∂y (where l is some loss function). We wish to compute du, df , and dD (which are ∂l ∂u , ∂l ∂f , and ∂l ∂D , respectively). The most challenging part is computing the gradient through the convolution operator -but we'll see that we can re-use our FFT infrastructure for it. The rest of the operations are straightforward; we have dD = dyu T .</p><p>Gradient of the Convolution Here, we'll discuss how to compute df by integrating w.r.t. the convolution operator * . As an immediate consequence, we'll be able to compute du as well.</p><p>Since f and u are the same length L, f * u and u * f have the same result. Thus, we'll start from u * f here.</p><p>For some notation, let O = u * f . Then, dO = dy. Recall that Let's construct a Toeplitz matrix H u such that u * f = H u f :</p><formula xml:id="formula_18">O[i] = i-1 j=0 u[i -j]f [j].</formula><formula xml:id="formula_19">H u =      u [0] 0 . . . 0 u [1] u [0] . . . 0 . . . . . . . . . . . . u [2L -1] u [2L -2] . . . u [0]     </formula><p>Since, we have u [i] = f [i] = 0 for i ≥ L, we can actually fill in the zeros of the above matrix as well:</p><formula xml:id="formula_20">H u =      u [0] u [2L -1] . . . u [1] u [1] u [0] . . . u [2] . . . . . . . . . . . . u [2L -1] u [2L -2] . . . u [0]     </formula><p>Then, we can use the matrix multiplication chain rule to find that:</p><formula xml:id="formula_21">df = H T u dO =      u [0] u [1] . . . u [2L -1] u [2L -1] u [0] . . . u [2L -2] . . . . . . . . . . . . u [1] u [2] . . . u [0]      =      u [0] u [-(2L -1)] . . . u [-1] u [-1] u [0] . . . u [-2] . . . . . . . . . . . . u [-(2L -1)] u [-(2L -2)] . . . u [0]     </formula><p>, where we use u [-i] to mean u [2L -i]. Notice that this matrix has the same format as</p><formula xml:id="formula_22">H u ! Let u * = [u [0], u [-1], . . . , u [-(2N -1)]]. Then: df = (u * * dO ).</formula><p>So how do we compute u * efficiently? Naively, we might incur some nasty memory access issues. But a nice property about the DFT saves us! Let U [i] be the i-th element of the DFT of a signal u. Note that U [i] is complex. We have:</p><formula xml:id="formula_23">U * [i] = U [-i],</formula><p>where here the * represents the complex conjugate. We can use this property to compute df efficiently:</p><formula xml:id="formula_24">df = u * * dO = iF F T (F F T * (u )F F T (dO )) ⇒ df = df [: N ] = iF F T (F F T * (u )F F T (dy ))[: N ],</formula><p>where F F T * denotes taking the complex conjugate of the FFT, and dy denotes dy, padded with zeros.</p><p>Computing du We can use this same trick to compute du, except we need to add in the contribution from the original Du term. We end up with:</p><formula xml:id="formula_25">du = du [: N ] + Ddy = iF F T (F F T * (f )F F T (dy ))[: N ] + Ddy.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 State-Passing Matrices</head><p>We show how to derive M ux for the state update in our state-passing algorithm.</p><p>We wish to construct a matrix</p><formula xml:id="formula_26">vM ux ∈ R m×N such that M ux u = N i=1 A N -1 Bu i . Note that A i B ∈ R d×1</formula><p>is a column vector. We can simply stack these column vectors to form a matrix:</p><formula xml:id="formula_27">M ux = [A N -1 B, A N -2 B, . . . , B].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Proofs</head><p>We show parameterizations of H3 and attention that solves the associative recall task. We prove Proposition 1 and Proposition 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 H3 Expressivity</head><p>This section formally describes a parameterization of H3 that solves the associative recall task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1.1 Example Language Λ</head><p>Consider a simple language with 4 keys and 4 values. For concreteness, we will use the keys {k 1 , k 2 , k 3 , k 4 } = L K and the values {v 1 , v 2 , v 3 , v 4 } = L V , i.e. our language L = L K ∪ L V . Given a sequence of key-value pairs with one key at the end, we want a model to generate the value associated with the key at the end. Assume that the key at the end appeared in the sequence.</p><p>More formally, let N + 1 be the length of the sequence, N even. The language Λ consists of sequences x ∈ L N +1 . Each sequence has an associated mapping f x : L K → L V . For each sequence, the odd indices are randomly sampled from L K , for x 1 , x 3 , . . . , x N -1 . The even indices are defined by f x :</p><formula xml:id="formula_28">x 2 * i = f x (x 2 * i-1 ), for 1 ≤ i ≤ N/2.</formula><p>The last item in the sequence x N +1 , is randomly drawn from the keys that have appeared in x already, i.e. x N +1 ∈ ∪{x 1 , x 3 , . . . , x N -1 }. The goal of this language modeling task is to produce f x (x N +1 ) at the end of the sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1.2 H3 Model to Solve Λ</head><p>We describe a toy H3 model that can solve Λ.</p><p>Consider a model consisting of an embedding layer, an H3 model, and an output projection with softmax. Recall that d is the dimension of the H3 model, m is the dimension of its hidden states, and H is the number of heads. Let d = 8, m = 2, H = 4. Let the embedding layer map each key k i to the e i basis vector, and map each value v i to the e 4+i basis vector.</p><p>Let B shif t and C shif t denote the parameters of the shift SSM, and A diag , B diag , and C diag denote the parameters of the diagonal SSM (let D be zero for both). Let B shif t = B diag = C diag = e 1 . Let C shif t = [01]. Let A diag be a diagonal matrix with 1s along its diagonal for each H3.</p><p>Remark. The action of a diagonal SSM parameterized by A diag , B diag , and C diag is to act as a cumulative sum over all its input. The action of shift SSM parameterized by B shif t and C shif t is to shift its input by one time step.</p><p>Recall that the H3 layer maps its input to Q, K, and V by applying uW Q , uW K , and uW V . Let W Q and W K be the following:</p><formula xml:id="formula_29">W Q = W K =           </formula><p>1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</p><formula xml:id="formula_30">          </formula><p>Recall that Q and K are split into H heads (Q (i) , K (i) for i ∈ {1, 2, 3, 4}), each of which is sent to an independent H3.</p><p>Remark. The action of W Q and W K are to "assign" each key to a different H3 head, i.e., Q</p><formula xml:id="formula_31">(i) t is only non-zero when x t = k i . Similarly, K (i)</formula><p>t is only non-zero when x t-1 = k i (since K t = K t-1 due to the time delay of the shift SSM).</p><p>Let W V be the following:</p><formula xml:id="formula_32">W V =           </formula><p>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 1 0</p><formula xml:id="formula_33">1 1 1 1 1 1 1 1           </formula><p>Remark. The action of this matrix is to encode the input value (as "binary"), and send it to all H3 heads. E.g., V</p><formula xml:id="formula_34">= V (2) t = V (3) t = V (4) t<label>(1) t</label></formula><p>for all i, and V</p><formula xml:id="formula_35">(i) t = [0, 0] ⇔ x t = v 1 , V (i) t = [0, 1] ⇔ x t = v 2 , etc. We claim that for x N +1 = k i , O<label>(i)</label></formula><p>N +1 will be a multiple of the binary encoding of f x (k i ), and all the other heads of the output O (j) N +1 , 1 ≤ j ≤ 4, j = i, will be zero. Let the output projection W O be such that, with a non-linearity afterwards, it inverts the binary encoding to produce the embedding of the desired output f x (k i ). We will assume such a projection exists, proof left to the reader.</p><p>Proposition 3. The model described above solves the associative recall problem for the language Λ.</p><formula xml:id="formula_36">Proof. Proof sketch. WLOG, let x N +1 = k i . Then Q (i) = [1, 1], but Q (j) = [0, 0] for j = i. Thus, O (j) = [0, 0] for j = i due to the multiplicative interaction. Since Q (i) = [1, 1], O (i)</formula><p>is the output of the diag SSMs in the H3 head corresponding to k i (recall that each head has two independent shift SSMs and two independent diag SSMs). The output of the diag SSMs are the cumulative sum of all the inputs they have seen in the sequence.</p><p>For one of the diag SSMs to see a non-zero input, its preceding shift SSM must have a non-zero output. The only times t this can happen in the sequence are when x t-1 = k i . But then x t = f x (k i ). Thus, the input to the diag SSMs are precisely the binary encoding of f x (k i ). Then the output O (i) is a multiple of the binary encoding of f x (k i ), W O decodes this output to the embedding of f x (k i ).</p><p>generate the right answer for the output. For example, the induction head task requires memorizing the token that appears after the special token in the input sequence, and the associative recall task requires learning the mapping from keys to tokens from the input sequence.</p><p>We evaluate synthetics by training two-layer versions of our GPT models, with different modules replacing attention. We train models with inner dimension 32, and MLP dimension 128. For all the synthetics, we use a learning rate of 5e-4 and a weight decay of 0.1. We sample 5000 training examples and 500 test examples from the same distribution, and we train for 200 epochs. Again, we use embedding dropout of 0.1 and residual dropout of 0.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Model Architecture</head><p>For our 125M models, we use 12 layers, with hidden dimension 1024, and MLP dimension 4096. For our 355M models, we use 24 layers, with the same hidden dimension and MLP dimension. The 1.3B models have 24 layers, with hidden dimension 2048, and MLP dimension 8192. The 2.7B models have 32 layers, hidden dimension 2560, and MLP dimension 10240. The hybrid models have 12, 16, 16, and 20 heads for the 125M, 355M, 1.3B, and 2.7B models, respectively. The 125M hybrid model has an attention layers at layers 1 and 7, the 355M and 1.3B hybrid models have attention layers at layers 1 and 13, and the 2.7B hybrid model has attention layers at layers 10 and 21. For both our hybrid models and our H3 models, we use SSM state size 64. Our hybrid model uses head dimension 1 for H3, while our pure H3 model uses head dimension 8. We run models with mixed-precision training, with bf16 for the MLP's and attention. When training language models, we use fp32 for the FFTConv.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 OpenWebText Training</head><p>For the 125M models trained on OpenWebText, we follow the training recipe of the Megatron-LM repo.</p><p>We use an effective batch size of 512, and use gradient accumulation to fit into available GPU memory. We use the AdamW optimizer, with learning rate 6e-4 for GPT-2 small and 1.5e-4 for GPT-2 medium, and weight decay of 0.1. All models are trained with the same hyperparameters for 100K steps. We run all implementations with mixed-precision training (PyTorch AMP). We train models with sequence length 1024.</p><p>We use the Openwebtext dataset, with the GPT-2 BPE tokenizer. We randomly select 0.5% of the dataset as the validation set, with the rest being used as training set. This random selection of validation set is done once, and all models are evaluated on the same validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4 The Pile Training</head><p>For the 125M and 355M models trained on the Pile, we follow the training recipe of GPT-3. We use batch size 256, with sequence length 2048. We train our models for 800K steps. We use residual dropout 0.0 and embedding dropout 0.1. We use the AdamW optimizer, with learning rate 6e-4 for the 125M model and 3e-4 for the 355M model, and a weight decay of 0.1. We use a cosine schedule with 8000 steps for linear warmup, and decay the learning rate to 10% by 300B tokens, then continue training at 10% learning rate for another 100B tokens. We suspect that there exist better hyperparameters for H3 language models, but we did not have the resources to tune them.</p><p>For the 1.3B models, we double the batch size to 512 (with sequence length 2048), again following the training recipe of GPT-3. The number of training steps are halved so that we train on the same number of tokens.</p><p>For the Pile dataset, we again use the GPT-2 BPE tokenizer, similar to GPT-3 and GPT-Neo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.5 SuperGLUE</head><p>We follow the prompts used in the GPT-3 paper <ref type="bibr" target="#b6">[7]</ref>. For rank classification on the binary classification tasks, we use yes/no for WSC, WIC, MultiRC, and BoolQ, and we use true/false for RTE. For CB, we use true/false/neither as the three choices. For COPA and ReCoRD, we use the continuations provided by the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.6 Hardware</head><p>All models were trained on either a single 16xA100-40GB node or a cluster of 8xA100-80GB nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Additional Experiments F.1 LRA Accuracy</head><p>We evaluate the accuracy of H3 on LRA. We compare accuracy to S4D <ref type="bibr" target="#b25">[26]</ref>, since H3 uses an S4D kernel as a component in its layer. We use the same hyperparameters as S4D, and make the layer bidirectional by making two copies and running them in opposite directions. Table <ref type="table" target="#tab_8">9</ref> shows that H3 performs well on the LRA benchmark, even thought it was designed for autoregressive language modeling. H3 outperforms S4D on two of the LRA tasks, and comes within 1 point on the others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 WikiText103</head><p>We train 125M-sized models on WikiText103 <ref type="bibr" target="#b42">[43]</ref> and compare their test PPL to transformers, as well as other variants of efficient or long-range attention. We use the same hyperparameters and setup as training on OpenWebText. We also provide results from Transformer-XL and Perceiver AR for context, though the results may not be directly comparable due to differences in model size and tokenizer. 18.5 Performer (125M) <ref type="bibr" target="#b8">[9]</ref> 26.8 Reformer (125M) <ref type="bibr">[39]</ref> 26.0 Linear Attention (125M) <ref type="bibr" target="#b34">[35]</ref> 25. <ref type="bibr" target="#b5">6</ref> Perceiver AR (358M) <ref type="bibr" target="#b29">[30]</ref> 18.4 Transformer-XL (285M) <ref type="bibr" target="#b12">[13]</ref> 18.4</p><p>Table <ref type="table" target="#tab_9">10</ref> shows that the Hybrid H3 model is competitive with Transformers of the same size, as well as larger models such as the 358M Perceiver AR and 285M Transformer-XL. The hybrid H3 model also significantly outperforms transformers with performer, reformer, and linear attention.</p><p>We note that the Transformer-XL and Perceiver AR PPl numbers are from the original papers, and may not be directly comparable to our results. In particular, they use a tokenizer with a different vocab size, which means that the PPLs are not directly comparable. In addition, the larger vocab size necessitates a change in the model (adaptive softmax) that may affect performance. The top five numbers in Table <ref type="table" target="#tab_9">10</ref> are trained with the same setup and are directly comparable to each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3 PG-19</head><p>We evaluate models trained on the PG-19 dataset <ref type="bibr" target="#b53">[54]</ref>, a natural language dataset comprised of texts from books. We compare the performance of Hybrid H3 compared against Transformers and linear attention. We use the same setup as evaluating on OpenWebText. Table 11 shows that Hybrid H3 outperforms transformers and linear attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.4 Length Extrapolation</head><p>One property of SSMs is that they can naturally extrapolate to sequence lengths longer than those seen during training. We use the synthetic associative recall task to demonstrate that H3 maintains this capability. To do so, we train a two-layer H3 model on sequences of length 20 drawn from the associative recall synthetic language. Then, we evaluate accuracy of the last token prediction on sequences of length 20 and 40. Table <ref type="table" target="#tab_12">12</ref> shows that H3 maintains accuracy on sequences of length 40, which is twice the length of the training sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.5 Scaling in Number of Tokens</head><p>We evaluate how well a Hybrid H3 model scales with the number of tokens seen during training, compared to a Transformer. For these experiments, we train a 125M Hybrid H3 model and a 125M Transformer on the Pile for 5B, 10B, and 15B tokens. We use a learning rate of 6e-4, adjusting the warmup to be 1% of the total training time, and adjusting the decay rate to decay the learning rate to 6e-5 by the end of training. Table <ref type="table" target="#tab_13">13</ref> shows the results. Both the Hybrid H3 model and Transformer model improve as the number of training tokens increases. and matches S4 on speech classification-which suggests that H3, or one of its hybrids, may be a strong candidate for a multimodal foundation model. Appendix E gives experimental details, and Appendix F gives an additional experiment on brain fMRI data.</p><p>Seizure Classification from EEG Seizures, which are characterized by uncontrolled brain activity, are some of the most common neurological disorders <ref type="bibr" target="#b19">[20]</ref>. Chronic seizures, or epilepsy, cause a range of psychiatric and psycho-social disorders and impact the lives of roughly one percent of the global population <ref type="bibr" target="#b36">[37]</ref>. The first step to treating epilepsy is manual analysis of scalp EEG by board-certified neurologists. However, the vast amount of EEG data produced by each patient (which can be up to days of data) makes manual EEG analysis a costly and time-consuming process.</p><p>To mitigate the costs associated with EEG monitoring, recent deep learning techniques have began to show promise in flagging abnormal EEG segments for potential seizure events <ref type="bibr" target="#b56">[57]</ref>. A challenge with classifying EEG data is the trade-off between increasing input sequence length, where more context has been shown to improve seizure classification performance <ref type="bibr" target="#b54">[55]</ref>, with the increased difficulty of training deep learning models on long sequences (e.g., an EEG signal sampled at 200Hz produces 12,000 time steps per minute). As a result, many techniques involve domain-specialized models and pre-processing steps, such as FFT transforms and graphical representations <ref type="bibr" target="#b57">[58]</ref>.</p><p>We use the largest publicly available EEG corpus, TUSZ v1.5.2 <ref type="bibr" target="#b55">[56]</ref>, which includes 5,612 EEG signals from 636 patients, with 3,050 annotated seizures. Signals are segmented into 60-second clips, and split into train/val/test by patient. The train set contains 39765 clips, the val set contains 4351 clips, and the test set contains 10001 clips.</p><p>We evaluate binary seizure classification of 60-sec EEG clips, sampled at 200Hz, with 19 electrodes: x ∈ R 12,000×19 and y ∈ {0, 1} on the TUSZ v1.5.2 <ref type="bibr" target="#b55">[56]</ref> corpus. Transformers cannot process the long sequence length of EEG signals without running out of GPU memory, whereas H3 can-and sets state-of-the-art performance.</p><p>Raw Speech Classification The SC10 speech commands task <ref type="bibr" target="#b63">[64]</ref> contains raw audio signals one second in length, sampled at 16kHz. Similarly to EEG signals, Transformers cannot process the long sequence length. Table <ref type="table" target="#tab_18">19</ref> shows that H3 comes within half a point of S4, the state-of-the-art method.</p><p>Functional Magnetic Resonance Imaging Data Functional Magnetic Resonance Imaging (fMRI) data are typically represented in four dimensions, indicating the measured blood-oxygen-level-dependent (BOLD) signal in temporal sequences S = {V 1 , ..., V t } of 3-dimensional volumes V ∈ R x×y×z , each indicating the BOLD signal for all spatial locations of the brain (as defined by three spatial dimensions x, y, and z). A key challenge for the analysis of fMRI data is the high dimensionality and low sample size of its datasets, which typically contain many hundred thousand dimensions (i.e., voxels) for each of several hundred volumes V in each of tens to hundreds of sequences S. In this setting, where the number of features exceed the number of samples, standard machine learning approaches are prone to overfitting.</p><p>In spite of the low sample size of individual datasets, neuroimaging research can be considered as recently entering a big data era because researchers more frequently share their collected datasets publicly <ref type="bibr" target="#b40">[41]</ref>. The  availability of data open up the opportunity for pre-training in neuroimaging at scale, as recently demonstrated by <ref type="bibr" target="#b59">[60]</ref>, enabling models to utilize the knowledge that can be learned from public functional neuroimaging data for the analysis of individual datasets. Specifically, <ref type="bibr" target="#b59">[60]</ref> evaluate the performance of several self-supervised learning frameworks for functional neuroimaging data by first pre-training models on a broad fMRI dataset spanning 11, 980 fMRI runs from 1, 726 individuals across 34 datasets and subsequently adapting the pre-trained models to two downstream mental state decoding datasets (namely, the HCP <ref type="bibr" target="#b60">[61]</ref> and MDTB <ref type="bibr" target="#b37">[38]</ref> datasets). In mental state decoding, predictive models are tasked with identifying (i.e., decoding) some set of mental states (e.g., answering questions about a prose story or math problem) from measured brain activity. The authors find that a GPT-based model, pre-trained in a causal learning framework, performs best in decoding the 20 (HCP) and 26 (MDTB) mental states of the two downstream datasets.</p><p>To evaluate the performance of H3 on fMRI data, we replicate this analysis, using the up-and downstream fMRI datasets that were published by <ref type="bibr" target="#b59">[60]</ref>, treating H3 as a drop-in replacement for the GPT model. To alleviate the high dimensionality challenge of fMRI data, and due to the generally high spatial correlation of brain activity, the original authors have reduced the volumetric time series S to a set Θ ∈ θ 1 , ..., θ n of n = 1, 024 functionally-independent brain networks θ (as defined by the dictionaries of functional modes (DiFuMo) brain atlas <ref type="bibr" target="#b11">[12]</ref>), each describing the BOLD signal for some subset of voxels v x,y,z ∈ V , such that the resulting sequences X ∈ R t×n describe the activity pattern of each brain network n for time points t.</p><p>In line with <ref type="bibr" target="#b59">[60]</ref>, we first pre-train models f (•) to predict the distribution of brain activity for the next time point j of an input sequence X, using a mean absolute error (L rec ) training objective given the model's output X ∈ R t×n : L rec = 1 n n i=1 |X j,i -Xj,i |; Xt,n = b n + n f (E X ) t,e w e,n ; E X t,e = E T R + E pos + b e + n X t,n w n,e . Here, E T R ∈ R e and E pos ∈ R e represent learnable embeddings for each possible time point and position of an input sequence (for details, see <ref type="bibr" target="#b59">[60]</ref>). As the sampling frequency of fMRI is variable, the same position of an input sequence can correspond to different time points. Note that f (•) processes the input in a lower-dimensional embedding representation E X ∈ R t×e (with e = 768 dimensions).</p><p>We evaluate two model architectures for f (•), namely, the GPT variant used in <ref type="bibr" target="#b59">[60]</ref>, with 4 hidden layers and 12 attention heads, and a corresponding H3 variant with 4 hidden layers (with H = 64 and m = 1; see section 3). For both models, the sequence of hidden-states outputs of the last model layer are used to determine X.</p><p>Just as <ref type="bibr" target="#b59">[60]</ref>, we randomly divide the upstream data into distinct training and validation datasets by randomly designating 5% of the fMRI runs of each of the 34 upstream datasets as validation data (at a minimum of 2 runs per dataset) and using the rest of the runs for training. During upstream learning, we then randomly sample sequences of 10 to 100 time points from the fMRI runs and train models with the ADAM optimizer (with β 1 = 0.9, β 2 = 0.999, and = 1e -8 ) for 5, 000 steps at a mini-batch size of 512, and a learning rate of 5e -4 . We apply a linear learning rate decay schedule (with a warm-up phase of 1% of the total number of training steps), gradient norm clipping at 1.0, and L2-regularisation (weighted by 0.1). We also apply dropout at a rate of 0.1 for the GPT-based model (based on <ref type="bibr" target="#b59">[60]</ref>) and evaluate three dropout rates for H3: 0.1, 0.2, and 0.3.</p><p>We find that the H3 variant trained with 0.2 dropout performs on par with the GPT model, in terms of mean absolute error (Fig. <ref type="figure" target="#fig_2">3</ref>), and therefore continue all further analyses with this model variant. We also find that both models exhibit almost identify L rec error distributions throughout the brain, with relatively higher errors in the posterior parietal, occipital, and cingulate cortices as well parts of the limbic system (Fig. <ref type="figure">4</ref>).</p><p>To adapt the pre-trained models to mental state decoding, we add a learnable classification embedding E cls ∈ R n to the end of input sequences X and forward the model's prediction f (E X ) to a decoding  head p(•), composed of a dense hidden layer with e model units (one for each embedding dimension, with tanh activation) as well as a sof tmax output layer with one model unit i for each considered mental state in the data. Accordingly, we adapt models by optimizing a standard cross entropy loss objective: L cls =i y i log p(f (E X )) i , where y i indicates a binary variable that is 1 if i is the correct mental state and 0 otherwise. During downstream adaptation, we begin training with the respective pre-trained model parameters and then allow all parameters to change freely. Similar to <ref type="bibr" target="#b59">[60]</ref>, we randomly split each of the two downstream datasets into distinct training and test datasets, each comprising 40 (HCP) or 10 (MDTB) distinct individuals. We adapt models for 750 steps at a mini-batch size of 256 and a learning rate of 5e -5 (otherwise using the same learning parameters as for upstream training). Importantly, we repeat each downstream training run 20 times using different random seeds, leading to different random splits of the data and variability in other non-deterministic training factors (such as random initialization and data shuffling).</p><p>As for the upstream data, the H3 and GPT-based models generally perform on par in their mental state decoding performances in the two left-out test datasets (Table <ref type="table" target="#tab_19">20</ref>). </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><figDesc>We'll start by extending u and f with zeros, to give them length 2L. Let u = [u[0], u[1], . . . , u[L -1], 0, . . . , 0], and f extended similarly. Let O = u * f , and O = O [: N ]. Assume that we have all the values of dO (we only have them for the first half, but we'll see that it doesn't matter in the end).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Upstream mean absolute error (Lrec) in training and evaluation datasets over the course of model training.</figDesc><graphic coords="26,153.90,118.53,304.21,152.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Synthetic language modeling tasks.</figDesc><table><row><cell>Task</cell><cell>Input</cell><cell cols="3">Output Sequence Length Vocab Size</cell></row><row><cell>Induction Head</cell><cell>a b c d e f g h i . . . x y z</cell><cell>f</cell><cell>30</cell><cell>20</cell></row><row><cell cols="2">Associative Recall a 2 c 4 b 3 d 1 a</cell><cell>2</cell><cell>20</cell><cell>10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Evaluation of 2-layer models on synthetic language tasks.</figDesc><table><row><cell>Task</cell><cell cols="3">Random S4D Gated State Spaces</cell><cell>H3</cell><cell>Attention</cell></row><row><cell>Induction Head</cell><cell>5.0</cell><cell>35.6</cell><cell>6.8</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>Associative Recall</cell><cell>25.0</cell><cell>86.0</cell><cell>78.0</cell><cell>99.8</cell><cell>100.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Perplexity of SSM variants compared to Transformers on OpenWebText. All models have 12 layers, with size around 125M, and are trained with the same hyperpameters, for 50B tokens.</figDesc><table><row><cell>H3</cell><cell cols="4">H3 Hybrid (2 Attn) S4D GSS GSS Hybrid (2 Attn) Transformer</cell></row><row><cell>21.0</cell><cell>19.6</cell><cell>24.9 24.0</cell><cell>19.8</cell><cell>20.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Perplexity (lower is better) of models on the Pile, OpenWebText and WikiText-103. GPT-Neo and hybrid H3 are trained on the Pile, while GPT2 is trained on WebText. All models use the same GPT2 tokenizer. We report the perplexity of GPT-2 models on the Pile ( * ) for context, though the performance is not directly comparable since they were trained on different data.</figDesc><table><row><cell>Model</cell><cell>Pile</cell><cell cols="2">OpenWebText WikiText103</cell></row><row><cell>GPT-2 small (125M)</cell><cell>19.0*</cell><cell>22.6</cell><cell>29.9</cell></row><row><cell>GPT-Neo-125M</cell><cell>9.4</cell><cell>22.6</cell><cell>26.3</cell></row><row><cell>Hybrid H3-125M</cell><cell>8.8</cell><cell>20.9</cell><cell>23.7</cell></row><row><cell cols="2">GPT-2 medium (355M) 13.9*</cell><cell>17.0</cell><cell>21.8</cell></row><row><cell>Hybrid H3-355M</cell><cell>7.1</cell><cell>15.9</cell><cell>16.9</cell></row><row><cell>GPT-2 XL (1.5B)</cell><cell>12.4*</cell><cell>12.9</cell><cell>17.0</cell></row><row><cell>GPT-Neo-1.3B</cell><cell>6.2</cell><cell>13.1</cell><cell>13.3</cell></row><row><cell>Hybrid H3-1.3B</cell><cell>6.0</cell><cell>12.4</cell><cell>12.5</cell></row><row><cell>GPT-Neo-2.7B</cell><cell>5.7</cell><cell>11.7</cell><cell>11.5</cell></row><row><cell>Hybrid H3-2.7B</cell><cell>5.4</cell><cell>11.0</cell><cell>10.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Zero-shot acc. on SuperGLUE with logit scoring. Best results in bold, second best underline.</figDesc><table><row><cell>Model</cell><cell>WSC</cell><cell>WIC</cell><cell>RTE</cell><cell>CB</cell><cell>MultiRC</cell><cell>ReCoRD</cell><cell>BoolQ</cell><cell>COPA</cell><cell>Average</cell></row><row><cell>OPT-125M</cell><cell>39.4</cell><cell>52.0</cell><cell>48.7</cell><cell>37.4</cell><cell>58.9</cell><cell>44.9</cell><cell>59.6</cell><cell>60.0</cell><cell>50.1</cell></row><row><cell>GPT-Neo-125M</cell><cell>36.5</cell><cell>53.6</cell><cell>53.1</cell><cell>41.1</cell><cell>59.9</cell><cell>39.6</cell><cell>62.2</cell><cell>60.0</cell><cell>50.8</cell></row><row><cell>Hybrid H3-125M</cell><cell>39.4</cell><cell>51.4</cell><cell>59.2</cell><cell>48.2</cell><cell>51.4</cell><cell>55.0</cell><cell>59.6</cell><cell>67.0</cell><cell>53.9</cell></row><row><cell>GPT-2 medium (355M)</cell><cell>50.0</cell><cell>52.0</cell><cell>51.3</cell><cell>28.6</cell><cell>59.5</cell><cell>53.3</cell><cell>61.0</cell><cell>65.0</cell><cell>52.6</cell></row><row><cell>OPT-350M</cell><cell>53.5</cell><cell>50.8</cell><cell>53.4</cell><cell>35.7</cell><cell>58.9</cell><cell>51.4</cell><cell>60.9</cell><cell>60.0</cell><cell>53.1</cell></row><row><cell>Hybrid H3-355M</cell><cell>37.5</cell><cell>51.7</cell><cell>55.2</cell><cell>41.1</cell><cell>59.5</cell><cell>62.3</cell><cell>61.5</cell><cell>69.0</cell><cell>54.7</cell></row><row><cell>OPT-1.3B</cell><cell>36.5</cell><cell>49.5</cell><cell>53.4</cell><cell>39.3</cell><cell>58.3</cell><cell>61.8</cell><cell>55.0</cell><cell>69.0</cell><cell>52.9</cell></row><row><cell>GPT-Neo-1.3B</cell><cell>41.3</cell><cell>50.0</cell><cell>52.3</cell><cell>33.9</cell><cell>57.9</cell><cell>55.5</cell><cell>59.9</cell><cell>66.0</cell><cell>52.1</cell></row><row><cell>Hybrid H3-1.3B</cell><cell>52.9</cell><cell>50.3</cell><cell>53.4</cell><cell>33.9</cell><cell>58.2</cell><cell>67.8</cell><cell>61.7</cell><cell>74.0</cell><cell>56.5</cell></row><row><cell>OPT-2.7B</cell><cell>51.0</cell><cell>50.8</cell><cell>50.5</cell><cell>41.1</cell><cell>57.4</cell><cell>65.9</cell><cell>60.9</cell><cell>66.0</cell><cell>55.5</cell></row><row><cell>GPT-Neo-2.7B</cell><cell>37.5</cell><cell>50.0</cell><cell>52.3</cell><cell>50.0</cell><cell>59.1</cell><cell>60.0</cell><cell>61.1</cell><cell>67.0</cell><cell>54.6</cell></row><row><cell>Hybrid H3-2.7B</cell><cell>36.5</cell><cell>51.3</cell><cell>57.0</cell><cell>37.5</cell><cell>58.7</cell><cell>71.3</cell><cell>61.1</cell><cell>81.0</cell><cell>56.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>3-shot acc. on SuperGLUE with logit scoring. Best results in bold, second best underline.</figDesc><table><row><cell>Model</cell><cell>WSC</cell><cell>WIC</cell><cell>RTE</cell><cell>CB</cell><cell>MultiRC</cell><cell>ReCoRD</cell><cell>BoolQ</cell><cell>COPA</cell><cell>Average</cell></row><row><cell>OPT-125M</cell><cell>36.5</cell><cell>50.2</cell><cell>47.3</cell><cell>44.6</cell><cell>57.9</cell><cell>44.9</cell><cell>41.9</cell><cell>60.0</cell><cell>47.9</cell></row><row><cell>GPT-Neo-125M</cell><cell>38.5</cell><cell>50.0</cell><cell>53.1</cell><cell>17.9</cell><cell>56.3</cell><cell>39.6</cell><cell>62.1</cell><cell>60.0</cell><cell>47.2</cell></row><row><cell>Hybrid H3-125M</cell><cell>43.3</cell><cell>49.1</cell><cell>58.1</cell><cell>51.8</cell><cell>48.9</cell><cell>55.0</cell><cell>56.1</cell><cell>67.0</cell><cell>53.7</cell></row><row><cell>GPT-2 medium (355M)</cell><cell>36.5</cell><cell>50.5</cell><cell>48.0</cell><cell>8.9</cell><cell>43.5</cell><cell>53.3</cell><cell>58.8</cell><cell>65.0</cell><cell>45.6</cell></row><row><cell>OPT-350M</cell><cell>37.5</cell><cell>50.0</cell><cell>45.8</cell><cell>44.6</cell><cell>49.8</cell><cell>51.4</cell><cell>61.7</cell><cell>60.0</cell><cell>50.1</cell></row><row><cell>Hybrid H3-355M</cell><cell>42.3</cell><cell>47.5</cell><cell>50.5</cell><cell>28.6</cell><cell>59.7</cell><cell>62.3</cell><cell>60.5</cell><cell>69.0</cell><cell>52.6</cell></row><row><cell>OPT-1.3B</cell><cell>44.2</cell><cell>51.1</cell><cell>53.4</cell><cell>16.1</cell><cell>59.9</cell><cell>62.1</cell><cell>38.3</cell><cell>70.0</cell><cell>49.4</cell></row><row><cell>GPT-Neo-1.3B</cell><cell>35.6</cell><cell>50.6</cell><cell>47.3</cell><cell>32.1</cell><cell>59.9</cell><cell>55.7</cell><cell>61.2</cell><cell>67.0</cell><cell>51.2</cell></row><row><cell>Hybrid H3-1.3B</cell><cell>36.5</cell><cell>49.2</cell><cell>55.2</cell><cell>23.2</cell><cell>59.3</cell><cell>67.6</cell><cell>56.9</cell><cell>76.0</cell><cell>53.0</cell></row><row><cell>OPT-2.7B</cell><cell>44.2</cell><cell>50.5</cell><cell>53.4</cell><cell>17.9</cell><cell>59.2</cell><cell>66.0</cell><cell>62.0</cell><cell>71.0</cell><cell>53.0</cell></row><row><cell>GPT-Neo-2.7B</cell><cell>49.0</cell><cell>51.9</cell><cell>51.6</cell><cell>21.4</cell><cell>57.0</cell><cell>60.0</cell><cell>56.0</cell><cell>68.0</cell><cell>51.9</cell></row><row><cell>Hybrid H3-2.7B</cell><cell>36.5</cell><cell>45.6</cell><cell>47.3</cell><cell>46.4</cell><cell>59.4</cell><cell>71.1</cell><cell>60.6</cell><cell>77.0</cell><cell>55.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Inference throughput on A100 80GB, 1.3B models. Batch size 64, prompt length 512, 1024, or 1536, and generating 128 tokens per sequence in the batch (i.e., 64 × 128 tokens in a batch). Hybrid H3 is up to 2.4× faster than a Transformer of similar size in inference. The difference is larger for longer sequences.</figDesc><table><row><cell>Tokens/s</cell><cell cols="3">Prompt length 512 Prompt length 1024 Prompt length 1536</cell></row><row><cell>Transformer-1.3B</cell><cell>1340</cell><cell>770</cell><cell>520</cell></row><row><cell>Hybrid H3-1.3B</cell><cell>1980</cell><cell>1580</cell><cell>1240</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Speedup on the LRA benchmark.</figDesc><table><row><cell></cell><cell></cell><cell>Models</cell><cell>Speedup</cell></row><row><cell></cell><cell></cell><cell>Transformer</cell><cell>1×</cell></row><row><cell></cell><cell></cell><cell>FlashAttention [15]</cell><cell>2.4×</cell></row><row><cell></cell><cell></cell><cell>Block-sparse FlashAttention [15]</cell><cell>2.8×</cell></row><row><cell></cell><cell></cell><cell>S4 [28]</cell><cell>2.9×</cell></row><row><cell></cell><cell></cell><cell>S4 with FlashConv</cell><cell>5.8×</cell></row><row><cell></cell><cell cols="2">Benchmarking FlashConv</cell><cell></cell><cell>Inset (256-4K)</cell></row><row><cell></cell><cell></cell><cell>35x</cell><cell>5</cell></row><row><cell>Time (ms)</cell><cell>10 20 30</cell><cell>2x</cell><cell>1 3</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1K</cell><cell>2K</cell><cell>4K</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Sequence Length</cell></row><row><cell></cell><cell>1K 4K</cell><cell>16K</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Sequence Length</cell><cell></cell></row><row><cell></cell><cell>FlashAttention</cell><cell>cuFFT Conv</cell><cell></cell><cell>Fused Block FFT Conv</cell></row><row><cell></cell><cell>FlashConv</cell><cell>Fused Conv</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>LRA performance of H3 compared to S4D.</figDesc><table><row><cell>Model</cell><cell cols="7">ListOps Text Retrieval Image Pathfinder Path-X Avg</cell></row><row><cell>S4D [26]</cell><cell>58.3</cell><cell>87.3</cell><cell>90.7</cell><cell>87.5</cell><cell>93.6</cell><cell>92.3</cell><cell>85.0</cell></row><row><cell>H3</cell><cell>57.5</cell><cell>88.2</cell><cell>91.0</cell><cell>87.3</cell><cell>93.0</cell><cell>91.8</cell><cell>84.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 :</head><label>10</label><figDesc>Test PPL on WikiText103.</figDesc><table><row><cell>Models</cell><cell>PPL</cell></row><row><cell>Transformer (125M)</cell><cell>18.6</cell></row><row><cell>Hybrid H3 (125M)</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11 :</head><label>11</label><figDesc>Test PPL on PG-19.</figDesc><table><row><cell>Models</cell><cell>PPL</cell></row><row><cell>Transformer (125M)</cell><cell>17.0</cell></row><row><cell>Hybrid H3 (125M)</cell><cell>16.2</cell></row><row><cell cols="2">Linear Attention (125M) [35] 19.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 12 :</head><label>12</label><figDesc>Accuracy of an H3 model trained for associative recall on sequences of length 20, evaluated on sequences of length 20 and 40.</figDesc><table><row><cell cols="3">Models Acc, seqlen 20 Acc, seqlen 40</cell></row><row><cell>H3</cell><cell>99.8</cell><cell>98.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 13 :</head><label>13</label><figDesc>Test PPL on the Pile for models trained with fewer tokens.</figDesc><table><row><cell cols="3">Train Tokens Hybrid H3 (125M) Transformer (125M)</cell></row><row><cell>5B</cell><cell>11.8</cell><cell>12.7</cell></row><row><cell>10B</cell><cell>10.7</cell><cell>11.3</cell></row><row><cell>15B</cell><cell>10.2</cell><cell>10.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 15 :</head><label>15</label><figDesc>3-shot performance on SuperGLUE with rank classification. Best results for each size in bold, second best underline.</figDesc><table><row><cell>Model</cell><cell cols="3">WSC WIC RTE</cell><cell>CB</cell><cell cols="5">MultiRC ReCoRD BoolQ COPA Average</cell></row><row><cell>OPT-125M</cell><cell>36.5</cell><cell>50.2</cell><cell>47.3</cell><cell>44.6</cell><cell>57.9</cell><cell>44.9</cell><cell>41.9</cell><cell>60.0</cell><cell>47.9</cell></row><row><cell>GPT-Neo-125M</cell><cell>38.5</cell><cell>50.0</cell><cell>53.1</cell><cell>17.9</cell><cell>56.3</cell><cell>39.6</cell><cell>62.1</cell><cell>60.0</cell><cell>47.2</cell></row><row><cell>H3-125M</cell><cell>63.5</cell><cell>50.0</cell><cell>52.3</cell><cell>48.2</cell><cell>32.6</cell><cell>15.8</cell><cell>37.8</cell><cell>51.0</cell><cell>43.9</cell></row><row><cell>Hybrid H3-125M</cell><cell>43.3</cell><cell>49.1</cell><cell cols="2">58.1 51.8</cell><cell>48.9</cell><cell>55.0</cell><cell>56.1</cell><cell>67.0</cell><cell>53.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 16 :</head><label>16</label><figDesc>Zero-shot performance on SuperGLUE. Best results for each size in bold, second best underline.</figDesc><table><row><cell>Model</cell><cell>WSC</cell><cell>WIC</cell><cell>RTE</cell><cell>CB</cell><cell>MultiRC</cell><cell>ReCoRD</cell><cell>BoolQ</cell><cell>COPA</cell><cell>Average</cell></row><row><cell>OPT-125M</cell><cell>36.5</cell><cell>48.4</cell><cell>49.8</cell><cell>8.9</cell><cell>39.1</cell><cell>44.9</cell><cell>45.9</cell><cell>60.0</cell><cell>41.7</cell></row><row><cell>GPT-Neo-125M</cell><cell>27.9</cell><cell>11.3</cell><cell>45.8</cell><cell>8.9</cell><cell>19.1</cell><cell>39.6</cell><cell>56.4</cell><cell>60.0</cell><cell>33.6</cell></row><row><cell>Hybrid H3-125M</cell><cell>0.0</cell><cell>0.0</cell><cell>47.3</cell><cell>8.9</cell><cell>4.4</cell><cell>55.0</cell><cell>47.6</cell><cell>67.0</cell><cell>28.8</cell></row><row><cell>GPT-2 medium (355M)</cell><cell>50.0</cell><cell>50.2</cell><cell>16.2</cell><cell>21.4</cell><cell>10.5</cell><cell>53.3</cell><cell>38.4</cell><cell>65.0</cell><cell>38.1</cell></row><row><cell>OPT-350M</cell><cell>41.3</cell><cell>34.8</cell><cell>49.5</cell><cell>16.1</cell><cell>23.6</cell><cell>51.4</cell><cell>39.7</cell><cell>60.0</cell><cell>39.6</cell></row><row><cell>Hybrid H3-355M</cell><cell>22.1</cell><cell>21.5</cell><cell>47.3</cell><cell>8.9</cell><cell>17.1</cell><cell>62.3</cell><cell>44.4</cell><cell>69.0</cell><cell>36.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 17 :</head><label>17</label><figDesc>3-shot performance on SuperGLUE with generation. Best results for each size in bold, second best underline.</figDesc><table><row><cell>Model</cell><cell>WSC</cell><cell>WIC</cell><cell>RTE</cell><cell>CB</cell><cell>MultiRC</cell><cell>ReCoRD</cell><cell>BoolQ</cell><cell>COPA</cell><cell>Average</cell></row><row><cell>OPT-125M</cell><cell>36.5</cell><cell>49.1</cell><cell>47.3</cell><cell>33.9</cell><cell>35.5</cell><cell>44.8</cell><cell>38.5</cell><cell>60.0</cell><cell>43.2</cell></row><row><cell>GPT-Neo-125M</cell><cell>38.5</cell><cell>50.0</cell><cell>53.1</cell><cell>42.9</cell><cell>22.5</cell><cell>39.7</cell><cell>61.2</cell><cell>68.0</cell><cell>47.0</cell></row><row><cell>H3-125M</cell><cell>0.0</cell><cell>0.0</cell><cell>47.3</cell><cell>8.9</cell><cell>0.0</cell><cell>15.4</cell><cell>37.8</cell><cell>53.0</cell><cell>20.3</cell></row><row><cell>Hybrid H3-125M</cell><cell>43.3</cell><cell>49.1</cell><cell>58.1</cell><cell>41.1</cell><cell>40.3</cell><cell>55.2</cell><cell>49.5</cell><cell>67.0</cell><cell>50.5</cell></row><row><cell>GPT-2 medium (355M)</cell><cell>36.5</cell><cell>50.5</cell><cell>47.3</cell><cell>28.6</cell><cell>35.3</cell><cell>53.1</cell><cell>37.8</cell><cell>63.0</cell><cell>44.0</cell></row><row><cell>OPT-350M</cell><cell>37.5</cell><cell>50.0</cell><cell>46.2</cell><cell>41.1</cell><cell>40.6</cell><cell>51.3</cell><cell>39.4</cell><cell>59.0</cell><cell>45.6</cell></row><row><cell>Hybrid H3-355M</cell><cell>42.3</cell><cell>47.5</cell><cell>50.5</cell><cell>37.5</cell><cell>57.5</cell><cell>61.4</cell><cell>45.4</cell><cell>73.0</cell><cell>51.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 18 :</head><label>18</label><figDesc>Performance (AUROC) on 60s seizure classification from raw EEG (sequence length 12000).</figDesc><table><row><cell>H3</cell><cell cols="5">Transformer Dense-CNN CNN-LSTM LSTM 1D-CNN</cell></row><row><cell>83.2</cell><cell>x</cell><cell>78.0</cell><cell>68.6</cell><cell>69.3</cell><cell>69.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 19 :</head><label>19</label><figDesc>SC 10-class classification on raw audio (sequence length 16000).</figDesc><table><row><cell>H3</cell><cell>S4</cell><cell cols="4">WaveGan-D Transformer Performer CKConv</cell></row><row><cell cols="2">97.04 97.50</cell><cell>96.25</cell><cell>x</cell><cell>30.77</cell><cell>71.66</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 20 :</head><label>20</label><figDesc>Downstream adaptation performance of models pre-trained on fMRI data, averaged over 20 training runs with varying random seeds. F1-scores are macro-averaged.</figDesc><table><row><cell cols="4">Dataset Model Acc. (±95%CI) F1 (±95%CI)</cell></row><row><cell>HCP</cell><cell>GPT</cell><cell>88.44 (±0.39)</cell><cell>87.24 (±0.39)</cell></row><row><cell></cell><cell>H3</cell><cell>88.75 (±0.33)</cell><cell>87.16 (±0.37)</cell></row><row><cell>MDTB</cell><cell>GPT</cell><cell>89.47 (±0.44)</cell><cell>88.74 (±0.54)</cell></row><row><cell></cell><cell>H3</cell><cell>88.25 (±0.45)</cell><cell>85.76 (±0.61)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>Code for H3 is available at https://github.com/HazyResearch/H3</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>There are several memory-efficient algorithms for attention<ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b51">52]</ref>, though their time complexity is still quadratic in N , which is a lower-bound for attention<ref type="bibr" target="#b35">[36]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>SRAM, or on-chip memory, is much faster than off-chip GPU memory, but usually much smaller, on the order of around 100KB for each streaming processor.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>There is no pretrained GPT-Neo at the 350M size.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We thank <rs type="person">Albert Gu</rs> for helpful discussion regarding the model architecture, and more importantly for sending us daily hippo videos. We thank <rs type="person">Together Computer</rs> for providing portions of the compute used to train models in this paper. We gratefully acknowledge the support of <rs type="funder">NIH</rs> under No. <rs type="grantNumber">U54EB020405</rs> (Mobilize), <rs type="funder">NSF</rs> under Nos. <rs type="grantNumber">CCF1763315</rs> (<rs type="affiliation">Beyond Sparsity</rs>), CCF1563078 (Volume to Velocity), and 1937301 (RTML); ARL under No. <rs type="grantNumber">W911NF-21-2-0251</rs> (<rs type="funder">Interactive Human-AI Teaming)</rs>; <rs type="funder">ONR</rs> under No. <rs type="grantNumber">N000141712266</rs> (<rs type="affiliation">Unifying Weak Supervision</rs>); <rs type="grantNumber">ONR N00014-20-1-2480</rs>: <rs type="projectName">Understanding and Applying Non-Euclidean Geometry in Machine Learning</rs>; <rs type="grantNumber">N000142012275</rs> (NEPTUNE); NXP, <rs type="funder">Xilinx</rs>, <rs type="affiliation">LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF</rs>, <rs type="funder">Accenture</rs>, <rs type="funder">Ericsson</rs>, <rs type="funder">Qualcomm</rs>, <rs type="funder">Analog Devices</rs>, <rs type="person">Google Cloud</rs>, <rs type="funder">Salesforce</rs>, <rs type="funder">Total</rs>, the <rs type="institution">HAI-GCP Cloud Credits for Research program</rs>, the <rs type="funder">Stanford Data Science Initiative</rs> (SDSI), <rs type="funder">Department of Defense (DoD)</rs> through the <rs type="grantName">National Defense Science and Engineering Graduate Fellowship (NDSEG) Program, Wu Tsai Neuroscience Stanford Interdisciplinary Graduate Fellowship</rs>, and members of the Stanford DAWN project: <rs type="funder">Facebook</rs>, <rs type="person">Google</rs>, and <rs type="funder">VMWare</rs>. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_6q9Re8t">
					<idno type="grant-number">U54EB020405</idno>
				</org>
				<org type="funding" xml:id="_2XS9pnG">
					<idno type="grant-number">CCF1763315</idno>
				</org>
				<org type="funding" xml:id="_3wNxTdw">
					<idno type="grant-number">W911NF-21-2-0251</idno>
				</org>
				<org type="funding" xml:id="_H2JXgHy">
					<idno type="grant-number">N000141712266</idno>
				</org>
				<org type="funded-project" xml:id="_PkGBVb9">
					<idno type="grant-number">ONR N00014-20-1-2480</idno>
					<orgName type="project" subtype="full">Understanding and Applying Non-Euclidean Geometry in Machine Learning</orgName>
				</org>
				<org type="funding" xml:id="_DMDWBs8">
					<idno type="grant-number">N000142012275</idno>
				</org>
				<org type="funding" xml:id="_WZJbdfc">
					<orgName type="grant-name">National Defense Science and Engineering Graduate Fellowship (NDSEG) Program, Wu Tsai Neuroscience Stanford Interdisciplinary Graduate Fellowship</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Attention Expressivity</head><p>We provide an informal sketch of a two-layer attention model that can solve the associative recall task, inspired by the construction of <ref type="bibr" target="#b48">[49]</ref>. The first layer of the attention model outputs the embedding of the previous token in the sequence, and concatenates it with the current token in the sequence. The second layer compares the current token to the previous token embeddings, and outputs the paired embedding when there is a match-which is exactly the key-value lookup.</p><p>The construction proceeds as follows:</p><p>• In the first layer, let Q i be mapped to the positional embedding of token x i-1 (e.g., p i-1 if p i denotes the positional embedding of token x i ), and K i be mapped to the positional embedding of token x i .</p><p>• The attention matrix A is computed as QK T , with a causal mask (i.e., A i,j = 0 if j &gt; i).</p><p>• Then, sof tmax(A) approximates the shift matrix (see Section 3).</p><p>• Let V i be an encoding of token x i , constrained to the first half of the hidden dimension.</p><p>• Then, for output O = sof tmax(QK T )V , the first half of the vector O i is the encoding of token x i-1 .</p><p>• In the second layer, assume that you have a skip connection, that maps the encoding of the input token x i to the second half of the vector O i .</p><p>• Then, the input to the second layer encodes both x i-1 and x i .</p><p>• In the second layer, let Q i extract the encoding of x i , and let K i extract the encoding of x i-1 .</p><p>• Apply a causal mask on QK T . Then, the value of sof tmax(QK T ) i,j is large if x i = x j-1 , and i &gt; j -1.</p><p>• Let V i extract the encoding of x i .</p><p>• Then, output O i is the sum of values x j such as x j-1 = x i . But then O i is exactly a lookup of the token that came after x i when it appeared previously in the sequence-which exactly solves associative recall.</p><p>We note that the above construction requires the ability for the positional encodings to select the previous token based on the dot product and softmax, and for token comparisons through the dot product and softmax. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 H3 Complexity</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 State Passing Correctness</head><p>We prove Proposition 2. We assume that the BlockFFTConv algorithm is correct, i.e., the output y =BlockFFTConv(f, u) is equal to the output of an SSM with convolution kernel f and input u.</p><p>Proof. Proof by induction on C.</p><p>In this case, note that N = N . Then y (1) = M xy x (0) N +BlockFFTConv(f, u 1 ) =BlockFFTConv(f, u 1 ). But u = u 1 , so y = y (1) = [y (1) ].</p><p>Additionally, by the recursive definition of a state space, (1) .</p><p>Inductive step: C &gt; 1. Assume that [y (1) , . . . , y (C-1) ] = y[: N (C -1)], and x</p><p>For i &gt; (C -1)N , we have:</p><p>Similarly,</p><p>A N -i Bu </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Experimental Details E.1 Synthetics</head><p>Our synthetic tasks, inspired by <ref type="bibr" target="#b48">[49]</ref>, are designed to mimic the in-context learning capability of large language models-the ability to learn from examples in the input sequence, and use information from the input to</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Using fast weights to attend to the recent past</title>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adaptive input representations for neural language modeling</title>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">FFTs in external or hierarchical memory</title>
		<author>
			<persName><forename type="first">H</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of Supercomputing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="35" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Leahy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><surname>Gpt-Neo</surname></persName>
		</author>
		<title level="m">Large Scale Autoregressive Language Modeling with Mesh-Tensorflow</title>
		<imprint>
			<date type="published" when="2021-03">March 2021</date>
		</imprint>
	</monogr>
	<note>If you use this software, please cite it using these metadata</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">On the opportunities and risks of foundation models</title>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Drew</forename><forename type="middle">A</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simran</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><surname>Sydney Von Arx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeannette</forename><surname>Michael S Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bohg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emma</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><surname>Brunskill</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07258</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Willian</forename><forename type="middle">L</forename><surname>Brogan</surname></persName>
		</author>
		<title level="m">Modern control theory</title>
		<imprint>
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1259</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rethinking attention with performers</title>
		<author>
			<persName><forename type="first">Valerii</forename><surname>Krzysztof Marcin Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyou</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreea</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamas</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Sarlos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">Quincy</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Afroz</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName><surname>Kaiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.02311</idno>
		<title level="m">Scaling language modeling with pathways</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An algorithm for the machine calculation of complex fourier series</title>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">W</forename><surname>Cooley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">W</forename><surname>Tukey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Computation</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">90</biblScope>
			<biblScope unit="page" from="297" to="301" />
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fine-grain atlases of functional modes for fmri analysis</title>
		<author>
			<persName><forename type="first">Kamalaker</forename><surname>Dadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaël</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonia</forename><surname>Machlouzarides-Shalit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krzysztof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demian</forename><surname>Gorgolewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bertrand</forename><surname>Wassermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><surname>Mensch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">221</biblScope>
			<biblScope unit="page">117126</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2978" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Monarch: Expressive structured matrices for efficient and accurate training</title>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beidi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nimit</forename><surname>Sohoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arjun</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Poli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Grogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aniruddh</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Flashattention: Fast and memory-efficient exact attention with io-awareness</title>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Smyrf-efficient attention using asymmetric clustering</title>
		<author>
			<persName><forename type="first">Giannis</forename><surname>Daras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><forename type="middle">G</forename><surname>Dimakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6476" to="6489" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Inductive biases and variable creation in self-attention mechanisms</title>
		<author>
			<persName><forename type="first">Surbhi</forename><surname>Benjamin L Edelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sham</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cyril</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5793" to="5831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A mathematical framework for transformer circuits</title>
		<author>
			<persName><forename type="first">Nelson</forename><surname>Elhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Nanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuntao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Conerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nova</forename><surname>Dassarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Drain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deep</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zac</forename><surname>Hatfield-Dodds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danny</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackson</forename><surname>Kernion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liane</forename><surname>Lovitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamal</forename><surname>Ndousse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<ptr target="https://transformer-circuits.pub/2021/framework/index.html" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Transformer Circuits Thread</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Bruce</forename><surname>Fischl</surname></persName>
		</author>
		<author>
			<persName><surname>Freesurfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="774" to="781" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ilae official report: a practical clinical definition of epilepsy</title>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Robert S Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Acevedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alicia</forename><surname>Arzimanoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helen</forename><surname>Bogacz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><forename type="middle">E</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerome</forename><surname>Elger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Engel</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">Jacqueline</forename><forename type="middle">A</forename><surname>Forsgren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName><surname>Glynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Epilepsia</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="475" to="482" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Travis</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anish</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noa</forename><surname>Nabeshima</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00027</idno>
		<title level="m">The pile: An 800gb dataset of diverse text for language modeling</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">It&apos;s raw! audio generation with state-space models</title>
		<author>
			<persName><forename type="first">Karan</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.09729</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Gokaslan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vanya</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Tellex</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Openwebtext corpus</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hippo: Recurrent memory with optimal polynomial projections</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1474" to="1487" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficiently modeling long sequences with structured state spaces</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On the parameterization and initialization of diagonal state space models</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Combining recurrent, convolutional, and continuous-time models with linear state-space layers</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isys</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khaled</forename><surname>Saab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">How to train your hippo: State space models with generalized orthogonal basis projections</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isys</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aman</forename><surname>Timalsina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.12037</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Diagonal state spaces are as effective as structured state spaces</title>
		<author>
			<persName><forename type="first">Ankit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Curtis</forename><surname>Hawthorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Jaegle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cȃtȃlina</forename><surname>Cangea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charlie</forename><surname>Nash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Simon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.07765</idno>
		<title level="m">General-purpose, long-context autoregressive modeling with perceiver ar</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Lstm can solve hard long time lag problems</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Training compute-optimal large language models</title>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>De Las</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.15556</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The hardware lottery</title>
		<author>
			<persName><forename type="first">Sara</forename><surname>Hooker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="58" to="65" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">An optimized dataflow for mitigating attention performance bottlenecks</title>
		<author>
			<persName><forename type="first">Sheng-Chun</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suvinay</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Krishna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.06419</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Transformers are RNNs: Fast autoregressive transformers with linear attention</title>
		<author>
			<persName><forename type="first">Angelos</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolaos</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">François</forename><surname>Fleuret</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5156" to="5165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">On the computational complexity of self-attention</title>
		<author>
			<persName><forename type="first">Pruthuvi</forename><surname>Feyza Duman Keles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chinmay</forename><surname>Mahesakya Wijewardena</surname></persName>
		</author>
		<author>
			<persName><surname>Hegde</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.04881</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The impact of epilepsy on patients&apos; lives</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><surname>Kerr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Neurologica Scandinavica</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Functional boundaries in the human cerebellum revealed by a multi-domain task battery</title>
		<author>
			<persName><forename type="first">Maedbh</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><forename type="middle">R</forename><surname>Hernandez-Castillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><forename type="middle">A</forename><surname>Poldrack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">B</forename><surname>Ivry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jörn</forename><surname>Diedrichsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature neuroscience</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1371" to="1378" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Stable, fast and accurate: Kernelized attention with relative positional encoding</title>
		<author>
			<persName><forename type="first">Shengjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianle</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinglan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guolin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="22795" to="22807" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">The openneuro resource for sharing of neuroscience data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krzysztof</forename><forename type="middle">J</forename><surname>Markiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franklin</forename><surname>Gorgolewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Feingold</surname></persName>
		</author>
		<author>
			<persName><surname>Blair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Yaroslav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Halchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nell</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Hardcastle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oscar</forename><surname>Wexler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Esteban</surname></persName>
		</author>
		<author>
			<persName><surname>Goncavles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Elife</publisher>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">71774</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Long range language modeling via gated state spaces</title>
		<author>
			<persName><forename type="first">Harsh</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashok</forename><surname>Cutkosky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Behnam</forename><surname>Neyshabur</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.13947</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Pointer sentinel mixture models</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">S4nd: Modeling images and videos as multidimensional signals with state spaces</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gordon</forename><surname>Downs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preey</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Baccus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<title level="m">Tesla V100 GPU architecture</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Nvidia A100 tensor core GPU architecture</title>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<author>
			<persName><surname>Cufft</surname></persName>
		</author>
		<ptr target="https://docs.nvidia.com/cuda/cufft/index.html" />
		<imprint>
			<date type="published" when="2022">v11.7.1 documentation, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Nvidia H100 tensor core GPU architecture</title>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nelson</forename><surname>Elhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Nanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nova</forename><surname>Dassarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuntao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Conerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Drain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deep</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zac</forename><surname>Hatfield-Dodds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danny</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackson</forename><surname>Kernion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liane</forename><surname>Lovitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamal</forename><surname>Ndousse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<ptr target="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html" />
		<title level="m">-context learning and induction heads. Transformer Circuits Thread</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Applications of digital signal processing</title>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">V</forename><surname>Oppenheim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1978">1978</date>
			<pubPlace>Englewood Cliffs</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">V</forename><surname>Oppenheim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">R</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">W</forename><surname>Schafer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete-time signal processing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2001">2001</date>
			<publisher>Prentice Hall</publisher>
			<pubPlace>Upper Saddle River, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Self-attention does not need O(n 2 ) memory</title>
		<author>
			<persName><forename type="first">N</forename><surname>Markus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Rabe</surname></persName>
		</author>
		<author>
			<persName><surname>Staats</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.05682</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Compressive transformers for long-range sequence modelling</title>
		<author>
			<persName><forename type="first">Anna</forename><surname>Jack W Rae</surname></persName>
		</author>
		<author>
			<persName><surname>Potapenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chloe</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Weak supervision as an efficient approach for automated seizure detection in electroencephalography</title>
		<author>
			<persName><forename type="first">Khaled</forename><surname>Saab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Dunnmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Lee-Messer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NPJ digital medicine</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">The temple university hospital seizure detection corpus</title>
		<author>
			<persName><forename type="first">Vinit</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eva</forename><surname>Von Weltin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvia</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">Riley</forename><surname>Mchugh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lillian</forename><surname>Veloso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meysam</forename><surname>Golmohammadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iyad</forename><surname>Obeid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Picone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in neuroinformatics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">83</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A review of epileptic seizure detection using machine learning classifiers</title>
		<author>
			<persName><forename type="first">Mohammad Khubeb</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruben</forename><surname>Morales-Menendez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nasir</forename><surname>Hussain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain informatics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Self-supervised graph neural networks for improved electroencephalographic seizure analysis</title>
		<author>
			<persName><forename type="first">Siyi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Dunnmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khaled</forename><surname>Kamal Saab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianying</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Dubost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Lee-Messer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Long range arena: A benchmark for efficient transformers</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samira</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Self-supervised learning of brain dynamics from broad neuroimaging data</title>
		<author>
			<persName><forename type="first">W</forename><surname>Armin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><forename type="middle">A</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName><surname>Poldrack</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.11417</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">The wu-minn human connectome project: an overview</title>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">M</forename><surname>David C Van Essen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deanna</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">Ej</forename><surname>Barch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Essa</forename><surname>Behrens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamil</forename><surname>Yacoub</surname></persName>
		</author>
		<author>
			<persName><surname>Ugurbil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hcp</forename><surname>Wu-Minn</surname></persName>
		</author>
		<author>
			<persName><surname>Consortium</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="62" to="79" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Linformer: Self-attention with linear complexity</title>
		<author>
			<persName><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Belinda</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04768</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Speech commands: A dataset for limited-vocabulary speech recognition</title>
		<author>
			<persName><forename type="first">Pete</forename><surname>Warden</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03209</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-10">October 2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">OPT: Open pre-trained transformer language models</title>
		<author>
			<persName><forename type="first">Susan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Dewan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Victoria Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.01068</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">H3 Language Model Table 14: Zero-shot performance on SuperGLUE with rank classification</title>
		<author>
			<persName><forename type="first">F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Best results for each model size in bold. Model WSC WIC RTE CB MultiRC ReCoRD BoolQ COPA Average OPT-125M</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Tables 14 and 15 show zero-shot and few-shot performance on SuperGLUE, respectively. F.7 Generation Performance We report results on SuperGLUE for generation. Instead of taking rank classification, we instead let the model generate a response, and we search for the gold label (i.e., &quot;yes&quot; or &quot;no&quot; for the yes/no questions) in the output. Tables 16 and 17 report the results. The trends for few-shot learning match with the logit results, but the hybrid and H3 models perform very poorly in zero-shot performance on some tasks. In these cases, the models tend to generate long text responses that are not relevant to the answer. The few-shot learning examples help the models generate answers in a parsable format. F.8 Non-Text Sequence Modeling We show that H3 outperforms Transformers on two non-text sequence modeling tasks: raw speech classification and seizure classification over raw EEG signals. H3 sets state-of-the-art performance on seizure classification Figure</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
	<note>pure H3 language model on NLP evaluations. We train a 125M model on the Pile for 400B tokens Mean absolute error (Lrec) of the final pre-trained models for each voxel of the brain projected onto the inflated cortical surface of the FsAverage template [19</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
