- Decision to use State Space Models (SSMs) for language modeling
- Choice of synthetic language modeling tasks to evaluate expressivity gap
- Design of the H3 layer to enhance token recall and comparison capabilities
- Decision to implement a hybrid H3-attention model
- Selection of FlashConv for improving hardware efficiency
- Choice of FFT-based convolution for SSMs
- Decision to scale models up to 2.7B parameters
- Choice of training datasets (e.g., OpenWebText, Pile)
- Decision to evaluate performance on SuperGLUE benchmark
- Choice of hyperparameters based on GPT-3
- Decision to utilize a state passing algorithm for longer sequences
- Choice of evaluation metrics (e.g., perplexity)
- Decision to implement kernel fusion in FlashConv
- Choice of model architecture (e.g., stacking SSMs)
- Decision to use discrete SSMs for modeling sequences
- Choice of initial conditions for SSMs
- Decision to explore the expressivity gap between SSMs and attention
- Choice of hardware for training and evaluation (e.g., A100 GPU)
- Decision to document the development process through ADRs