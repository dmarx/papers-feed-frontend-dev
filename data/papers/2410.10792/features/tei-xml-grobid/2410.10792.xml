<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SEMANTIC IMAGE INVERSION AND EDITING USING RECTIFIED STOCHASTIC DIFFERENTIAL EQUATIONS</title>
				<funder ref="#_jXBG6Vh">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-10-14">14 Oct 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Litu</forename><surname>Rout</surname></persName>
							<email>litu.rout@utexas.edu</email>
							<affiliation key="aff0">
								<address>
									<country>Austin</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yujia</forename><surname>Chen</surname></persName>
							<email>yujiachen@google.com</email>
							<affiliation key="aff0">
								<address>
									<country>Austin</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nataniel</forename><surname>Ruiz</surname></persName>
							<email>natanielruiz@google.com</email>
							<affiliation key="aff0">
								<address>
									<country>Austin</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Constantine</forename><surname>Caramanis</surname></persName>
							<email>constantine@utexas.edu</email>
							<affiliation key="aff0">
								<address>
									<country>Austin</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sanjay</forename><surname>Shakkottai</surname></persName>
							<email>sanjay.shakkottai@utexas.edu</email>
							<affiliation key="aff0">
								<address>
									<country>Austin</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wen-Sheng</forename><surname>Chu</surname></persName>
							<email>wschu@google.com</email>
							<affiliation key="aff0">
								<address>
									<country>Austin</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Google</surname></persName>
							<affiliation key="aff0">
								<address>
									<country>Austin</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SEMANTIC IMAGE INVERSION AND EDITING USING RECTIFIED STOCHASTIC DIFFERENTIAL EQUATIONS</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-10-14">14 Oct 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">2FB6298CE4054B5CD2B7F98EC4D941D2</idno>
					<idno type="arXiv">arXiv:2410.10792v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>(c) Ref. content "sleeping cat" "tiger" "lion" "origami cat" "silver cat sculpture" (b)</p><p>Ref. style "face of a boy" (a)</p><p>Ref. style "a girl" "a panda" "a dwarf" (d) Ref. content "smiling cartoon" "girl" "old man" "young boy+glasses" "angry cartoon"</p><p>Figure <ref type="figure">1</ref>: Rectified flows for image inversion and editing. Our approach efficiently inverts reference style images in (a) and (b) without requiring text descriptions of the images and applies desired edits based on new prompts (e.g. "a girl" or "a dwarf"). For a reference content image (e.g. a cat in (c) or a face in (d)), it performs semantic image editing (e.g. "sleeping cat") and stylization (e.g. "a photo of a cat in origmai style") based on prompts, without leaking unwanted content from the reference image. Input images have orange borders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ABSTRACT</head><p>Generative models transform random noise into images; their inversion aims to transform images back to structured noise for recovery and editing. This paper addresses two key tasks: (i) inversion and (ii) editing of a real image using stochastic equivalents of rectified flow models (such as Flux). Although Diffusion Models (DMs) have recently dominated the field of generative modeling for images, their inversion presents faithfulness and editability challenges due to nonlinearities in drift and diffusion. Existing state-of-the-art DM inversion approaches rely on training of additional parameters or test-time optimization of latent variables; both are expensive in practice. Rectified Flows (RFs) offer a promising alternative to diffusion models, yet their inversion has been underexplored. We propose RF inversion using dynamic optimal control derived via a linear quadratic regulator. We prove that the resulting vector field is equivalent to a rectified stochastic differential equation. Additionally, we extend our framework to design a stochastic sampler for Flux. Our inversion method allows for state-of-the-art performance in zero-shot inversion and editing, outperforming prior works in stroke-to-image synthesis and semantic image editing, with large-scale human evaluations confirming user preference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Vision generative models typically transform noise into images. Inverting such models, given a reference image, involves finding the structured noise that can regenerate the original image. Efficient inversion must satisfy two crucial properties. First, the structured noise should produce an image that is faithful to the reference image. Second, the resulting image should be easily editable using new prompts, allowing fine modifications over the image. Diffusion Models (DMs) have become the mainstream approach for generative modeling of images <ref type="bibr" target="#b42">(Sohl-Dickstein et al., 2015;</ref><ref type="bibr" target="#b44">Song &amp; Ermon, 2019;</ref><ref type="bibr" target="#b14">Ho et al., 2020)</ref>, excelling at sampling from high-dimensional distributions <ref type="bibr" target="#b32">(Ramesh et al., 2021;</ref><ref type="bibr" target="#b41">Saharia et al., 2022;</ref><ref type="bibr" target="#b33">Ramesh et al., 2022;</ref><ref type="bibr" target="#b34">Rombach et al., 2022;</ref><ref type="bibr" target="#b30">Podell et al., 2023;</ref><ref type="bibr" target="#b29">Pernias et al., 2024)</ref>. The sampling process follows a Stochastic Differential Equation known as reverse SDE <ref type="bibr" target="#b2">(Anderson, 1982;</ref><ref type="bibr" target="#b9">Efron, 2011;</ref><ref type="bibr">Song et al., 2021b)</ref>. Notably, these models can invert a given image. Recent advances in DM inversion have shown a significant impact on conditional sampling, such as stroke-to-image synthesis <ref type="bibr" target="#b23">(Meng et al., 2022)</ref>, image editing <ref type="bibr" target="#b12">(Hertz et al., 2022;</ref><ref type="bibr" target="#b25">Mokady et al., 2023;</ref><ref type="bibr">Couairon et al., 2023;</ref><ref type="bibr">Rout et al., 2023a;</ref><ref type="bibr">b;</ref><ref type="bibr">2024a;</ref><ref type="bibr" target="#b8">Delbracio &amp; Milanfar, 2023)</ref> and stylization <ref type="bibr" target="#b13">(Hertz et al., 2023;</ref><ref type="bibr">Rout et al., 2024b)</ref>.</p><p>Despite its widespread usage, DM inversion faces critical challenges in faithfulness and editability.</p><p>First, the stochastic nature of the process requires fine discretization of the reverse SDE <ref type="bibr" target="#b14">(Ho et al., 2020;</ref><ref type="bibr">Song et al., 2021b)</ref>, which increases expensive Neural Function Evaluations (NFEs). Coarse discretization, on the other hand, leads to less faithful outputs <ref type="bibr" target="#b23">(Meng et al., 2022)</ref>, even with deterministic methods like DDIM <ref type="bibr">(Song et al., 2021a;</ref><ref type="bibr">b)</ref>. Second, nonlinearities in the reverse trajectory introduce unwanted drift, reducing the accuracy of reconstruction <ref type="bibr" target="#b18">(Karras et al., 2024)</ref>. While existing methods enhance faithfulness by optimizing latent variables <ref type="bibr">(Rout et al., 2024a)</ref> or prompt embeddings <ref type="bibr" target="#b25">(Mokady et al., 2023;</ref><ref type="bibr" target="#b24">Miyake et al., 2023)</ref>, they tend to be less efficient, harder to edit, and rely on complex attention processors to align with a given prompt <ref type="bibr" target="#b12">(Hertz et al., 2022;</ref><ref type="bibr">Rout et al., 2024a)</ref>. These added complexities make such methods less suitable for real-world deployment.</p><p>For inversion and editing, we introduce a zero-shot conditional sampling algorithm using Rectified Flows (RFs) <ref type="bibr" target="#b22">(Liu et al., 2022;</ref><ref type="bibr">Albergo &amp; Vanden-Eijnden, 2023;</ref><ref type="bibr" target="#b21">Lipman et al., 2022;</ref><ref type="bibr" target="#b10">Esser et al., 2024)</ref>, a powerful alternative to DMs. Unlike DMs, where sampling is governed by a reverse SDE, RFs use an Ordinary Differential Equation known as reverse ODE, offering advantages in both efficient training and fast sampling. We construct a controlled forward ODE, initialized from a given image, to generate the initial conditions for the reverse ODE. The reverse ODE is then guided by an optimal controller, obtained through solving a Linear Quadratic Regulator (LQR) problem. We prove that the resulting new vector fields have a stochastic interpretation with an appropriate drift and diffusion. We evaluate RF inversion on stroke-to-image generation and image editing tasks, and show extensive qualitative results on other applications like cartoonization. Our method significantly improves photo realism in stroke-to-image generation, surpassing a state-of-the-art (SoTA) method <ref type="bibr" target="#b25">(Mokady et al., 2023)</ref> by 89%, while maintaining faithfulness to the input stroke. In addition, we show that RF inversion outperforms DM inversion <ref type="bibr" target="#b23">(Meng et al., 2022)</ref> in faithfulness by 4.7% and in realism by 13.8% on LSUN-bedroom dataset <ref type="bibr" target="#b48">(Wang et al., 2017)</ref>. Figure <ref type="figure">1</ref> and Figure <ref type="figure">2</ref> show the qualitative results of our approach and a graphical illustration, respectively.</p><p>Our theoretical and practical contributions can be summarized as: • We present an efficient inversion method for RF models, including Flux, that requires no additional training, latent optimization, prompt tuning, or complex attention processors. • We develop a new vector field for RF inversion, interpolating between two competing objectives: consistency with a possibly corrupted input image, and consistency with the "true" distribution of clean images ( §3.3). We prove that this vector field is equivalent to a rectified SDE that interpolates between the stochastic equivalents of these competing objectives ( §3.4). We extend the theoretical results to design a stochastic sampler for Flux. • We demonstrate the faithfulness and editability of RF inversion across three benchmarks: (i) LSUN-Bedroom, (ii) LSUN-Church, and (iii) SFHQ, on two tasks: stroke-to-image synthesis and image editing. In addition, we provide extensive qualitative results and conduct large-scale human evaluations to assess user preference metrics ( §5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>DM Inversion. Diffusion models have become the mainstream approach for generative modeling, making DM inversion an exciting area of research <ref type="bibr" target="#b23">(Meng et al., 2022;</ref><ref type="bibr">Couairon et al., 2023</ref>; Song Figure 2: Graphical model illustrating (a) DDIM inversion and (b) RF inversion. Due to nonlinearities in DM trajectory, the DDIM inverted latent x 1 significantly deviates from the original image y 0 . RF inversion without controller reduces this deviation, resulting in x 1 . With controller, RF inversion further eliminates the reconstruction error, making x 1 nearly identical to y 0 , which enhances the faithfulness. 𝑦!"# 𝑦$ 𝑦# 𝑥!"# 𝑥! 𝑥̅# 𝑥$ 𝑥# Fwd. latent DDIM step Rev. latent 𝑦! 𝑦!"# 𝑦$ 𝑦# 𝑥!"# 𝑥! 𝑥$ 𝑥# Fwd. latent (a) DDIM Inversion (b) RF Inversion 𝑥# 𝑥!"# Rev. latent w/ controller Rev. latent w/o controller 𝑦! <ref type="bibr">et al., 2021b;</ref><ref type="bibr" target="#b13">Hertz et al., 2023;</ref><ref type="bibr" target="#b25">Mokady et al., 2023;</ref><ref type="bibr">Rout et al., 2024a)</ref>. Among training-free methods, SDEdit <ref type="bibr" target="#b23">(Meng et al., 2022)</ref> adds noise to an image and uses the noisy latent as structured noise. For semantic image editing based on a given prompt, it simulates the standard reverse SDE starting from this structured noise. SDEdit requires no additional parameter training, latent variable optimization, or complex attention mechanisms. However, it is less faithful to the original image because adding noise in one step is equivalent to linear interpolation between the image and noise, while the standard reverse SDE follows a nonlinear path <ref type="bibr" target="#b22">(Liu et al., 2022;</ref><ref type="bibr" target="#b17">Karras et al., 2022</ref>).</p><p>An alternate method, DDIM inversion <ref type="bibr">(Song et al., 2021a;</ref><ref type="bibr">b)</ref>, recursively adds predicted noise at each forward step and returns the final state as the structured noise (illustrated by Y t process in Figure <ref type="figure">2</ref>(a)). However, DDIM inversion often deviates significantly from the original image due to nonlinearities in the drift and diffusion coefficients, as well as inexact score estimates <ref type="bibr" target="#b25">(Mokady et al., 2023)</ref>. To reduce this deviation, recent approaches optimize prompt embeddings <ref type="bibr" target="#b25">(Mokady et al., 2023)</ref> or latent variables <ref type="bibr">(Rout et al., 2024a</ref>), but they have high time complexity. Negative prompt inversion <ref type="bibr" target="#b24">(Miyake et al., 2023)</ref> speeds up the inversion process but sacrifices faithfulness. Methods like CycleDiffusion (Wu &amp; De la Torre, 2023) and Direction Inversion <ref type="bibr" target="#b16">(Ju et al., 2023)</ref> use inverted latents as references during editing, but they are either computationally expensive or not applicable to rectified flow models like Flux or SD3 <ref type="bibr" target="#b10">(Esser et al., 2024)</ref>.</p><p>DM Editing. Efficient inversion is crucial for real image editing. Once a structured noise is obtained by inverting the image, a new prompt is fed into the T2I generative model. Inefficient inversion often fails to preserve the original content and therefore requires complex editing algorithms. These editing algorithms can be broadly classified into (i) attention control, such as prompt-to-prompt <ref type="bibr" target="#b12">(Hertz et al., 2022)</ref>, plug-and-play (PnP) <ref type="bibr" target="#b46">(Tumanyan et al., 2023)</ref>, (ii) optimization-based methods like Dif-fusionCLIP <ref type="bibr" target="#b19">(Kim et al., 2022)</ref>, DiffuseIT <ref type="bibr" target="#b20">(Kwon &amp; Ye, 2023)</ref>, STSL <ref type="bibr">(Rout et al., 2024a)</ref>, and (iii) latent masking to edit specific regions of an image using masks provided by the user <ref type="bibr">(Nichol et al., 2022)</ref> or automatically extracted from the generative model <ref type="bibr">(Couairon et al., 2023)</ref>. We focus on efficient inversion, avoiding the need for complex editing algorithms.</p><p>Challenges in RF Inversion. Previous inversion or editing approaches have been tailored towards diffusion models and do not directly apply to SoTA rectified flow models like Flux. This limitation arises because the network architecture of Flux is MM-DiT <ref type="bibr" target="#b28">(Peebles &amp; Xie, 2023)</ref>, which is fundamentally different from the traditional UNet used in DMs <ref type="bibr" target="#b14">(Ho et al., 2020;</ref><ref type="bibr">Song et al., 2021a;</ref><ref type="bibr">b)</ref>. In MM-DiT, text and image information are entangled within the architecture itself, whereas in UNet, text conditioning is handled via cross-attention layers. Additionally, Flux primarily uses T5 text encoder, which lacks an aligned latent space for images, unlike CLIP encoders. Therefore, extending these prior methods to modern T2I generative models requires a thorough investigation. We take the first step by inverting and editing a given image using Flux.</p><p>RF Inversion and Editing. DMs <ref type="bibr" target="#b14">(Ho et al., 2020;</ref><ref type="bibr">Song et al., 2021a;</ref><ref type="bibr" target="#b34">Rombach et al., 2022)</ref> traditionally outperform RFs <ref type="bibr" target="#b21">(Lipman et al., 2022;</ref><ref type="bibr" target="#b22">Liu et al., 2022;</ref><ref type="bibr">Albergo &amp; Vanden-Eijnden, 2023)</ref> in high-resolution image generation. However, recent advances have shown that RF models like Flux can surpass SoTA DMs in text-to-image (T2I) generation tasks <ref type="bibr" target="#b10">(Esser et al., 2024)</ref>. Despite this, their inversion and editing capabilities remain underexplored. In this paper, we introduce an efficient RF inversion method that avoids the need for training additional parameters <ref type="bibr" target="#b15">(Hu et al., 2021;</ref><ref type="bibr" target="#b39">Ruiz et al., 2023)</ref>, optimizing latent variables <ref type="bibr">(Rout et al., 2024a)</ref>, prompt tuning <ref type="bibr" target="#b25">(Mokady et al., 2023)</ref>, or using complex attention processors <ref type="bibr" target="#b12">(Hertz et al., 2022)</ref>. While our focus is on inversion and editing, we also show that our framework can be easily extended to generative modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Filtering, Control and SDEs.</head><p>There is a rich literature on the connections between nonlinear filtering, optimal control and SDEs <ref type="bibr" target="#b11">(Fleming &amp; Rishel, 1975;</ref><ref type="bibr" target="#b27">Øksendal, 2003;</ref><ref type="bibr" target="#b47">Tzen &amp; Raginsky, 2019;</ref><ref type="bibr" target="#b50">Zhang &amp; Chen, 2022</ref>). These connections are grounded in the Fokker-Planck equation <ref type="bibr">(Øksendal, 2003)</ref>, which RF methods <ref type="bibr" target="#b21">(Lipman et al., 2022;</ref><ref type="bibr" target="#b22">Liu et al., 2022;</ref><ref type="bibr">Albergo &amp; Vanden-Eijnden, 2023;</ref><ref type="bibr">Albergo et al., 2023)</ref> heavily exploit in sampling. Our study focuses on rectified flows for conditional sampling, and shows that the resulting drift field also has an optimal control interpretation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>3.1 PRELIMINARIES In generative modeling, the goal is to sample from a target distribution p 0 given a finite number of samples from that distribution. Rectified flows <ref type="bibr" target="#b21">(Lipman et al., 2022;</ref><ref type="bibr" target="#b22">Liu et al., 2022)</ref> represent a class of generative models that construct a source distribution q 0 and a time varying vector field v t (x t ) to sample p 0 using an ODE:</p><formula xml:id="formula_0">dX t = v t (X t )dt, X 0 ∼ q 0 , t ∈ [0, 1].<label>(1)</label></formula><p>Starting from X 0 = x 0 , the ODE (1) is integrated from t : 0 → 1 to yield a sample x 1 distributed according to p 0 (e.g., the distribution over images). A common choice of q 0 is standard Gaussian N (0, I) and v t (X t ) = -u(X t , 1 -t; φ), where u is a neural network parameterized by φ. The neural network is trained using the conditional flow matching objective as discussed below.</p><p>Training Rectified Flows. To train a neural network to serve as the vector field for the ODE (1), we couple samples from p 0 with samples from q 0 -which we call p 1 to simplify the notation -via a linear path:</p><formula xml:id="formula_1">Y t = tY 1 + (1 -t)Y 0 .</formula><p>The resulting marginal distribution of Y t becomes:</p><formula xml:id="formula_2">p t (y t ) = E Y1∼p1 [p t (y t |Y 1 )] = p t (y t |y 1 )p 1 (y 1 )dy 1 .<label>(2)</label></formula><p>Given an initial state Y 0 = y 0 and a terminal state Y 1 = y 1 , the linear path induces an ODE: dY t = u t (Y t |y 1 ) dt with the conditional vector field u t (Y t |y 1 ) = y 1 -y 0 . The marginal vector field is derived from the conditional vector field using the following relation <ref type="bibr" target="#b21">(Lipman et al., 2022)</ref>:</p><formula xml:id="formula_3">u t (y t ) = E Y1∼p1 u t (y t |Y 1 ) p t (y t |Y 1 ) p t (y t ) = u t (y t |y 1 ) p t (y t |y 1 ) p t (y t ) p 1 (y 1 )dy 1 .<label>(3)</label></formula><p>We can then use a neural network u(y t , t; φ), parameterized by φ, to approximate the marginal vector field u t (y t ) through the flow matching objective defined as:</p><formula xml:id="formula_4">L F M (φ) := E t∼U [0,1],Yt∼pt ∥u t (Y t ) -u(Y t , t; φ)∥ 2 2 . (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>For tractability, we can instead consider a different objective, called conditional flow matching:</p><formula xml:id="formula_6">L CF M (φ) := E t∼U [0,1],Yt∼pt(•|Y1),Y1∼p1 ∥u t (Y t |Y 1 ) -u(Y t , t; φ)∥ 2 2 .<label>(5)</label></formula><p>L CF M and L F M have the identical gradients <ref type="bibr">(Lipman et al., 2022, Theorem 2)</ref>, and are hence equivalent. However, L CF M (φ) is computationally tractable, unlike L F M (φ), and therefore preferred during training. Finally, the required vector field in (1) is computed as v t (X t ) = -u(X t , 1 -t; φ). In this way, rectified flows sample a data distribution by an ODE with a learned vector field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">CONNECTION BETWEEN RECTIFIED FLOWS AND LINEAR QUADRATIC REGULATOR</head><p>The unconditional rectified flows (RFs) (e.g., Flux) from Section §3.1 above, enable image generation by simulating the vector field v t (•) initialized with a sample of random noise. Subsequently, by simulating the reversed vector field -v 1-t (•) starting from the image, we get back the sample of noise that we started with. We formalize this statement below. Proposition 3.1. Given an image y 0 and the vector field v t (•) of the generative ODE (1), suppose the structured noise y 1 is obtained by simulating an ODE:</p><formula xml:id="formula_7">dY t = u t (Y t )dt, Y 0 = y 0 , t ∈ [0, 1]. (<label>6</label></formula><formula xml:id="formula_8">)</formula><formula xml:id="formula_9">If u t (•) = -v 1-t (•)</formula><p>and X 0 = y 1 , then the ODE (1) recovers the original image, i.e., X 1 = y 0 .</p><p>Implication. Rectified flows enable exact inversion of a given image when the vector field of the generative ODE (1) is precisely known. Employing ODE (6) for the structured noise and ODE (1) to transform that noise back into an image, RF inversion accurately recovers the given image.</p><p>Suppose instead that we start with a corrupted image and simulate the reversed vector field -v 1-t (•).</p><p>Then we obtain a noise sample. There are two salient aspects of this noise sample. First, it is consistent with the original image: when processed through v t (•) it results in the same corrupted image. Second, if the image sample is "atypical" (e.g., corrupted, or, say, a stroke painting as in §5), then the sample of noise is also likely to be atypical. In other words, the noise sample is only consistent to the (possibly corrupted) image sample.</p><p>Our goal is to modify the pipeline above so that even when we start with a corrupted image, we can get back a clean image (see stroke-to-image synthesis in Figure <ref type="figure">5</ref>), but for this, we need to processs by v t (•) a noise sample that is closer to being "typical". More generally, the goal is to create a pipeline that supports semantic editing of real images ( §5), e.g., changing age, or gender without relying on additional training, optimization, or complex attention processors.</p><p>Thus, as a first step, we derive an optimal controller that takes a minimum energy path to convert any image Y 0 (whether corrupted or not) to a given sample of random noise Y 1 ∼ p 1 -i.e., noise that is typical for p 1 . Specifically, we consider optimal control in a d-dimensional vector space R d :</p><formula xml:id="formula_10">V (c) := 1 0 1 2 ∥c (Z t , t)∥ 2 2 dt + λ 2 ∥Z 1 -Y 1 ∥ 2 2 , dZ t = c (Z t , t) dt, Z 0 = y 0 , Y 1 ∼ p 1 , (<label>7</label></formula><formula xml:id="formula_11">)</formula><p>where λ is the weight assigned to the terminal cost and V (c) denotes the total cost of the control c :</p><formula xml:id="formula_12">R d × [0, 1] → R d .</formula><p>The minimization of V (c) over the admissible set of controls, denoted by C, is known as the Linear Quadratic Regulator (LQR) problem. The solution of the LQR problem ( <ref type="formula" target="#formula_10">7</ref>) is given in Proposition 3.2, which minimizes the quadratic transport cost of the dynamical system. Proposition 3.2. For Z 0 = y 0 and Y 1 = y 1 , the optimal controller of the LQR problem ( <ref type="formula" target="#formula_10">7</ref>), denoted by c * (•, t) is equal to the conditional vector field u t (•|y 1 ) of the rectified linear path</p><formula xml:id="formula_13">Y t = tY 1 + (1 -t)Y 0 when Y 0 = y 0 , i.e., c * (z t , t) = u t (z t |y 1 ) = (y 1 -z t )/(1 -t).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">INVERTING RECTIFIED FLOWS WITH DYNAMIC CONTROL</head><p>So far, we have two vector fields. The first, from the RFs, transforms an image Y 0 typical for distribution p 0 to a typical sample of random Gaussian noise Y 1 ∼ p 1 . As discussed above, if the image sample is atypical, then the sample of noise is also likely to be atypical.</p><p>We also have a second vector field resulting from the optimal control formulation that transforms any image (whether corrupted or not) to a noise sample that is typical-by-design from the distribution p 1 . Therefore, this sample, when passed through the rectified flow ODE (1) results in a "typical" image from the "true" distribution p 0 . This image is clean, i.e., typical for p 0 , but it is not related to the image Y 0 . Our controlled ODE, defined below, interpolates between these two differing objectives consistency with the given (possibly corrupted) image, and consistency with the distribution of images p 0 -with a tunable parameter γ:</p><formula xml:id="formula_14">dY t = u t (Y t ) + γ (u t (Y t |y 1 ) -u t (Y t )) dt, Y 0 = y 0 ,<label>(8)</label></formula><p>where</p><formula xml:id="formula_15">u t (Y t |y 1 ) = c * (Y t , t) is computed based on the insights from Proposition 3.2, and u t (Y t ) = -v 1-t (Y t ) as established in Proposition 3.1.</formula><p>Here, we call γ ∈ [0, 1] the controller guidance. Thus, ODE ( <ref type="formula" target="#formula_14">8</ref>) generalizes ( <ref type="formula" target="#formula_7">6</ref>) to editing applications, while keeping its inversion accuracy comparable.</p><p>When γ = 1, the drift field of the ODE (8) becomes optimal controller of LQR problem (7), ensuring that the structured noise Y 1 = y 1 adheres to the distribution p 1 . Consequently, initializing the generative ODE (1) with y 1 results in samples with high likelihood under the data distribution p 0 .</p><p>Conversely, when γ = 0, the system follows the ODE (6) described in Proposition 3.1, resulting a structured noise Y 1 that is not guaranteed to follow the noise distribution p 1 . However, initializing the generative ODE (1) with this noise precisely recovers the reference image y 0 .</p><p>Beyond this vector field interpolation intuition, we show in the next section §3.4 that the controlled ODE ( <ref type="formula" target="#formula_14">8</ref>) has an SDE interpretation. As is well known <ref type="bibr" target="#b14">(Ho et al., 2020;</ref><ref type="bibr">Song et al., 2021a;</ref><ref type="bibr" target="#b23">Meng et al., 2022;</ref><ref type="bibr">Song et al., 2021b)</ref>, SDEs are robust to initial conditions, in proportion to the variance of the additive noise. Specifically, errors propagate over time in an ODE initialized with an incorrect or corrupted sample. However, SDEs (Markov processes) under appropriate conditions converge to samples from a carefully constructed invariant distribution with reduced sensitivity to the initial condition, resulting in a form of robustness to initialization. As we see, the parameter γ (the controller Preprint.</p><p>guidance) appears in the noise term to the SDE, thus the SDE analysis in the next section again provides intuition on the trade-off between consistency to the (corrupted) image and consistency to the terminal invariant distribution. Remark 3.3. We note that our analysis extends to the case where γ is time-varying, though we omit these results for simplicity of notation. This is useful in practice, especially when y 0 is a corrupted image, because for large γ the stochastic evolution ( <ref type="formula" target="#formula_34">22</ref>) moves toward a sample from the invariant measure N (0, I). This noise encodes clean images. Starting from this noise, the corresponding reverse process operates in pure diffusion mode, resulting in a clean image. As the process approaches the terminal state, γ is gradually reduced to ensure that y 0 is encoded through u t (•) into the final structured noise sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">CONTROLLED RECTIFIED FLOWS AS STOCHASTIC DIFFERENTIAL EQUATIONS</head><p>An SDE <ref type="bibr" target="#b14">(Ho et al., 2020)</ref> is known to have an equivalent ODE formulation <ref type="bibr">(Song et al., 2021a)</ref> under certain regularity conditions <ref type="bibr" target="#b2">(Anderson, 1982;</ref><ref type="bibr">Song et al., 2021b)</ref>. In this section, we derive the opposite: an SDE formulation for our controlled ODE (8) from §3.3. Let W t be a d-dimensional Brownian motion in a filtered probability space (Ω, F, {F t }, P). Theorem 3.4. Fix any T ∈ (0, 1). For any t ∈ [0, T ], the controlled ODE ( <ref type="formula" target="#formula_14">8</ref>) is explicitly given by:</p><formula xml:id="formula_16">dY t = - 1 1 -t (Y t -γy 1 ) - (1 -γ)t 1 -t ∇ log p t (Y t ) dt, Y 0 ∼ p 0 . (<label>9</label></formula><formula xml:id="formula_17">)</formula><p>Its density evolution is identical to the density evolution of the following SDE:</p><formula xml:id="formula_18">dY t = - 1 1 -t (Y t -γy 1 ) dt + 2(1 -γ)t 1 -t dW t , Y 0 ∼ p 0 .<label>(10)</label></formula><p>Finally, denoting p t (•) as the marginal pdf of Y t , the density evolution is explicitly given by:</p><formula xml:id="formula_19">∂p t (Y t ) ∂t = ∇ • 1 1 -t (Y t -γy 1 ) + (1 -γ)t 1 -t ∇ log p t (Y t ) p t (Y t ) . (<label>11</label></formula><formula xml:id="formula_20">)</formula><p>Properties of SDE (10). Elaborating on the intuition discussed at the end of §3.3, when the controller guidance parameter γ = 0, it becomes the stochastic equivalent of the standard RFs; see Lemma A.2 for a precise statement. The resulting SDE is given by</p><formula xml:id="formula_21">dY t = - 1 1 -t Y t dt + 2t 1 -t dW t , Y 0 ∼ p 0 ,<label>(12)</label></formula><p>which improves faithfulness to the image Y 0 . When γ = 1, the SDE (10) solves the LQR problem (7) and drives towards the terminal state Y 1 = y 1 . This improves the generation quality, because the sample Y 1 is from the correct noise distribution p 1 as previously discussed in §3.3. Therefore, a suitable choice of γ retains faithfulness while simultaneously applying the desired edits.</p><p>Finally, we assume T = 1 -δ for sufficiently small δ (such that 0 &lt; δ ≪ 1) to avoid irregularities at the boundary. This is typically considered in practice for numerical stability (even for diffusion models). Thus, in practice, the final sample y 1-δ is returned as y 1 .</p><p>Comparison with DMs. Analogous to the SDE (12), the stochastic noising process of DMs is typically modeled by the Ornstein-Uhlenbeck (OU) process, governed by the following SDE:</p><formula xml:id="formula_22">dY t = -Y t dt + √ 2dW t .<label>(13)</label></formula><p>The corresponding ODE formulation is given by:</p><formula xml:id="formula_23">dY t = [-Y t -∇ log p t (Y t )] dt.<label>(14)</label></formula><p>Instead, our approach is based on rectified flows (1), which leads to a different ODE and consequently translates into a different SDE. As an additional result, we formalize the ODE derivation in Lemma A.1. In Lemma A.2, we show that the marginal distribution of this ODE is equal to that of an SDE with appropriate drift and diffusion terms. In Proposition A.3, we show that the stationary distribution of this new SDE (12) converges to the standard Gaussian N (0, I) in the limit as t → 1.</p><p>Preprint.</p><p>The standard OU process (13) interpolates between the data distribution at time t = 0 and a standard Gaussian as t → ∞. The SDE (12), however, interpolates between the data distribution at time t = 0 and a standard Gaussian at t = 1. In other words, it effectively "accelerates" time as it progresses to achieve the terminal Gaussian distribution. This is accomplished by modifying the coefficients of drift and diffusion as in (12) to depend explicitly on time t. Thus, a sample path of ( <ref type="formula" target="#formula_21">12</ref>) appears like a noisy line, unlike that of the OU process (see Appendix C.3 for numerical simulations).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">CONTROLLED REVERSE FLOW USING RECTIFIED ODES AND SDES</head><p>In this section, we develop an ODE and an SDE similar to our discussions above, but for the reverse direction (i.e., from noise to images).</p><p>Reverse process using ODE. Starting from the structured noise y 1 obtained by integrating the controlled ODE (8), we construct another controlled ODE (15) for the reverse process (i.e., noise to image). In this process, the optimal controller uses the reference image y 0 for guidance:</p><formula xml:id="formula_24">dX t = v t (X t ) + η (v t (X t |y 0 ) -v t (X t )) dt, X 0 = y 1 , t ∈ [0, 1],<label>(15)</label></formula><p>where η ∈ [0, 1] is the controller guidance parameter as before that controls faithfulness and editability of the given image y 0 . Similar to the analysis in Proposition 3.2, v t (X t |y 0 ) is obtained by solving the modified LQR problem ( <ref type="formula" target="#formula_25">16</ref>):</p><formula xml:id="formula_25">V (c) = 1 0 1 2 ∥c (Z t , t)∥ 2 2 dt + λ 2 ∥Z 1 -y 0 ∥ 2 2 , dZ t = c (Z t , t) dt, Z 0 = y 1 .<label>(16)</label></formula><p>Solving ( <ref type="formula" target="#formula_25">16</ref>), we get c(Z t , t) = y0-Zt 1-t . Our controller steers the samples toward the given image y 0 . Thus, the controlled reverse ODE (15) effectively reduces the reconstruction error incurred in the standard reverse ODE (1) of RF models (e.g. Flux).</p><p>Reverse process using SDE. Finally, in Theorem 3.5, we provide the stochastic equivalent of our controlled reverse ODE (15) for generation. Recall that we initialize with the terminal structured noise by running the controlled forward ODE (8), along with a reference image y 0 . As discussed above, we terminate the inversion process at a time T = 1 -δ for numerical stability, resulting in a vector y 1-δ . Our reverse SDE thus starts at a corresponding time δ with this vector y 1-δ at initialization, and terminates at time T ′ &lt; 1. Theorem 3.5. Fix any T ′ ∈ (δ, 1), and for any t ∈ [δ, T ′ ], the density evolution of the controlled ODE (15) initialized at X 0 = y 1-δ is identical to the density evolution of the following SDE:</p><formula xml:id="formula_26">dX t = (1 -t -η)X t + ηty 0 t(1 -t) + 2(1 -t)(1 -η) t ∇ log p 1-t (X t ) dt + 2(1 -t)(1 -η) t dW t .<label>(17)</label></formula><p>Furthermore, denoting q t (•) as the marginal pdf of X t , its density evolution is given by:</p><formula xml:id="formula_27">∂q t (X t ) ∂t = ∇ • - 1 -t -η t(1 -t) X t + η 1 -t y 0 + (1 -t) t (1 -η)∇ log p 1-t (X t ) q t (Y t ) . (<label>18</label></formula><formula xml:id="formula_28">)</formula><p>Properties of SDE (17). When the controller parameter η = 0, we obtain a stochastic sampler (22) for the pre-trained Flux, as given in Lemma A.4 and compared qualitatively in Figure <ref type="figure" target="#fig_0">24</ref>. This case of our SDE (17) corresponds to the stochastic variant of standard RFs <ref type="bibr" target="#b22">(Liu et al., 2022;</ref><ref type="bibr" target="#b21">Lipman et al., 2022;</ref><ref type="bibr">Albergo &amp; Vanden-Eijnden, 2023)</ref>. Our key contribution lies in conditioning on X 1 = y 0 for inverting rectified flows. Importantly, our explicit construction does not require additional training or test-time optimization, enabling for the first time an efficient sampler for zeroshot inversion and editing using Flux. When η = 1, the score term and Brownian motion vanish from the SDE (17). The resulting drift becomes y0-Xt 1-t , the optimal controller for the LQR problem ( <ref type="formula" target="#formula_25">16</ref>), exactly recovering the given image y 0 . Remark 3.6. Similar to Remark 3.3, our analysis extends to the case when η is time-varying. This is useful in editing, as it allows the flow to initially move toward the given image y 0 by choosing a large η. As the flow approaches y 0 on the image manifold, η is gradually reduced, ensuring that the text-guided edits are enforced through the unconditional vector field v t (•) provided by Flux. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ALGORITHM: INVERSION AND EDITING VIA CONTROLLED ODES</head><p>We describe the algorithm for RF inversion and editing using our controlled ODEs ( <ref type="formula" target="#formula_14">8</ref>) and ( <ref type="formula" target="#formula_24">15</ref>).</p><p>Problem Setup. The user provides a text "prompt" to edit reference content, which could be a corrupt or a clean image. For the corrupt image guide, we use the dataset from SDEdit <ref type="bibr" target="#b23">(Meng et al., 2022)</ref>, which contains color strokes to convey high-level details. In this setting, the reference guide y 0 is typically not a realistic image under the data distribution p 0 . The objective is to transform this guide into a more realistic image under p 0 while maintaining faithfulness to the original guide.</p><p>For the clean image guide, the user provides a real image y 0 along with an accompanying text "prompt" to specify the desired edits. The task is to apply text-guided edits to y 0 while preserving its content. Examples include face editing, where the text might instruct change in age or gender.</p><p>Procedure. Our algorithm has two key steps: inversion and editing. We discuss each step below. During the inversion process, we use null prompt in the Flux model, i.e., u t (y t ) = u(y t , t, Φ(""); φ). For the conditional vector field, we apply the analytical solution derived in Proposition 3.2. The inversion process yields a latent variable that is then used to initialize our controlled ODE (15), i.e., X 0 = y 1 . In this phase, we again use the null prompt to compute the vector field v t (x t ) = -u(x t , 1 -t, Φ(""); φ): see Figure <ref type="figure" target="#fig_1">3</ref> for the final output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original RF inversion</head><p>Editing. The second step involves text-guided editing of the reference content y 0 . This process is governed by our controlled ODE (15), where the vector field is computed using the desired text prompt within Flux: v t (X t ) = -u(x t , 1 -t, Φ(prompt); φ). The controller guidance η in (15) balances faithfulness and editability: higher η improves faithfulness but limits editability, while lower η allows significant edits at the cost of reduced faithfulness. Consequently, the controller guidance η provides a smooth interpolation between faithfulness and editability, a crucial feature in semantic image editing. Motivated by Remark 3.3 and 3.6, we consider a time-varying controller guidance η t , such that for a fixed η ∈ [0, 1] and τ ∈ [0, 1], η t = η ∀t ≤ τ and 0 otherwise. Figure <ref type="figure" target="#fig_0">4</ref> illustrates the effect of controller guidance η for τ = 0.3; see Appendix C.2 for a detailed ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL EVALUATION</head><p>We show that RF inversion outperforms DM inversion across three benchmarks: LSUN-church, LSUN-bedroom <ref type="bibr" target="#b48">(Wang et al., 2017)</ref>, and SFHQ <ref type="bibr" target="#b4">(Beniaguev, 2022)</ref> on two tasks: Stroke2Image generation and semantic image editing. Stroke2Image generation shows the robustness of our algorithm to initial corruption. In semantic image editing, we emphasize the ability to edit clean images without additional training, optimization, or complex attention processors.</p><p>Baselines. As this paper focuses on inverting flows, we compare with SoTA inversion approaches, such as NTI <ref type="bibr" target="#b25">(Mokady et al., 2023)</ref>, DDIM Inversion <ref type="bibr">(Song et al., 2021a)</ref>, and SDEdit <ref type="bibr" target="#b23">(Meng et al., 2022)</ref>. We use the official NTI implementation for both NTI and DDIM inversion, and Diffusers Stroke2Image generation. As discussed in §4, our goal is to generate a photo-realistic image from a stroke paint (a corrupted image) and the text prompt "photo-realistic picture of a bedroom". In this case, the high level details in the stroke painting guide the reverse process toward a clean image.</p><p>In Figure <ref type="figure">5</ref>, we compare RF inversion (ours) with DM inversions. DM inversions propagate the corruption from the stroke painting into the structured noise, which leads to outputs resembling the input stroke painting. NTI optimizes null embeddings to align the reverse process with the DDIM forward trajectory. Although adding P2P to the NTI pipeline helps localized editing as in Figure <ref type="figure">6</ref>, for corrupted images, it drives the reverse process even closer to the corruption. In contrast, our controlled ODE (8) yields a structured noise that is consistent with the corrupted image and also the invariant terminal distribution, as discussed in §3.3, resulting in more realistic images.</p><p>In Table <ref type="table" target="#tab_1">1</ref>, we show that our method outperforms prior works in faithfulness and realism. On the test split of LSUN bedroom dataset, our approach is 4.7% more faithful and 13.79% more realistic than the best optimization free method SDEdit-SD1.5. Ours is 73% more realistic than the optimizationbased method NTI, but comparable in L2. As discussed, NTI+P2P gets closer toward the corrupt image, which gives a very low L2 error, but the resulting image becomes unrealistic. Our approach is 89% more realistic than NTI+P2P. We observe similar gains on LSUN church dataset.</p><p>User study. We conduct a user study using Amazon Mechanical Turk to evaluate the overall performance of the our method. With 3 responses for each question, we collected in total 9,000 compar-    <ref type="table" target="#tab_1">1</ref>, our method outperforms all the other baselines by at least 59.67% in terms of overall satisfaction. More details are provided in Appendix §C.6.</p><p>Semantic Image Editing. Given a clean image and a text "prompt", the objective is to modify the image according to the given text while preserving the contents of the image (identity for face images). In rectified linear paths, editing from a noisy latent becomes straightforward, further enhancing the efficiency of our approach. Compared with SoTA approaches (Figure <ref type="figure">6</ref>), our method requires no additional optimization or complex attention processors as in NTI <ref type="bibr" target="#b25">(Mokady et al., 2023)</ref>+P2P <ref type="bibr" target="#b12">(Hertz et al., 2022)</ref>. Thus, it is more efficient than a current SoTA approach, and importantly, more faithful to the original image while applying the desired edits. In Table <ref type="table" target="#tab_2">2</ref>, we show that our method outperforms the optimization-free methods by at least 29% in face reconstruction, 6.6% in DINO patch-wise similarity, and 26.4% in CLIP-Image similarity while being comparable in prompt alignment metric CLIP-T. Importantly, our approach offers 54.11% gain in runtime, though it uses a larger (∼12X) model, while staying comparable to NTI+P2P.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original SDEdit (Flux) Flux Inversion Ours</head><p>In Figure <ref type="figure" target="#fig_4">7</ref>, we showcase four complex editing tasks: (a) prompt-based stylization with the prompt: "face of a boy in disney 3d cartoon style", where facial expressions, such as "laugh" or "angry" are used for editing; (b) ability to control the age of a person; (c) interpolating between two concepts: "A man" ↔ "A woman"; (d) sequentially inserting pepperoni and mushroom to an image of a pizza. We provide more examples of editing in the wild in Appendix §C.</p><p>Comparison using the same backbone: Flux. In Figure <ref type="figure" target="#fig_5">8</ref>, we compare our method with SDEdit and DDIM inversion both adapted to Flux. NTI optimizes null embeddings to align with forward latents before applying text-guided edits via P2P, an approach well-suited for DMs that use both Preprint. null and text embedding. However, this strategy cannot be applied to Flux, as it does not explicitly use null embedding. Consequently, we only reimplement SDEdit and DDIM inversion for Flux and compare them to our method. Since all methods leverage the same generative model, the improvements clearly stem from our controlled ODEs, grounded in a solid theoretical foundation ( §3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We present the first efficient approach for inversion and editing with the state-of-art rectified flow models such as Flux. Our method interpolates between two vector fields: (i) the unconditional RF field that transforms a "clean" image to "typical" noise, and (ii) a conditional vector field derived from optimal control that transforms any image (clean or not) to "typical" noise. Our new field thus navigates between these two competing objectives of consistency with the given (possibly corrupted) image, and consistency with the distribution of clean images. Theoretically, we show that this is equivalent to a new rectified SDE formulation, sharing this intuition of interpolation. Practically, we show that our method results in state-of-art zero-shot performance, without the need of additional training, optimization of latent variables, prompt tuning, or complex attention processors.</p><p>We demonstrate the effectiveness of our method in stroke-to-image synthesis, face editing, object insertion, and stylization tasks, with large-scale human evaluation confirming user preference.</p><p>Limitation. The lack of comparison with expensive diffusion-based editing solutions may be viewed as a limitation. However, these implementations are either not available for Flux or not directly applicable due to Flux's distinct multi-modal architecture. The key contribution of this paper lies in its theoretical foundations, validated using standard benchmarks and relevant baselines.</p><p>Reproducibility. The pseudocode and hyper-parameter details have been provided to reproduce the reported results in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BROADER IMPACT STATEMENT</head><p>Semantic image inversion and editing have both positive and negative social impacts.</p><p>On the positive side, this technology enables (i) the generation of photo-realistic images from high level descriptions, such as stroke paintings, and (ii) the modification of clean images by changing various attributes like the age, gender, or adding glasses ( §5).</p><p>On the negative side, it can be misused by malicious users to manipulate photographs of individuals with inappropriate or offensive edits. Additionally, it carries the inherent risks associated with the underlying generative model.</p><p>To mitigate the negative social impacts, we enable safety features such as NSFW filters in the underlying generative model. Furthermore, we believe watermarking images generated by this technology can reduce misuse, especially in inversion and editing applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A ADDITIONAL THEORETICAL RESULTS</head><p>In this section, we present the theoretical results omitted from the main draft due to space constraints. We formalize the ODE derivation of the standard rectified flows in Lemma A.1. Lemma A.1. Given a coupling (Y 0 , Y 1 ) ∼ p 0 × p 1 , consider the noising process</p><formula xml:id="formula_29">Y t = tY 1 + (1 - t)Y 0 .</formula><p>Then, the rectified flow ODE formulation with the optimal vector field is given by</p><formula xml:id="formula_30">dY t = - 1 1 -t Y t - t 1 -t ∇ log p t (Y t ) dt, Y 0 ∼ p 0 . (<label>19</label></formula><formula xml:id="formula_31">)</formula><p>Furthermore, denoting p t (•) as the marginal pdf of Y t , its density evolution is given by:</p><formula xml:id="formula_32">∂p t (Y t ) ∂t = ∇ • 1 1 -t Y t + t 1 -t ∇ log p t (Y t ) p t (Y t ) .<label>(20)</label></formula><p>In Lemma A.2, we show that the marginal distribution of the rectified flow ( <ref type="formula" target="#formula_7">6</ref>) is equal to that of an SDE with appropriate drift and diffusion terms. Lemma A.2. Fix any T ∈ (0, 1), and for any t ∈ [0, T ], the density evolution (20) of the rectified flow model ( <ref type="formula" target="#formula_30">19</ref>) is identical to the density evolution of the following SDE:</p><formula xml:id="formula_33">dY t = - 1 1 -t Y t dt + 2t 1 -t dW t , Y 0 ∼ p 0 .<label>(21)</label></formula><p>In Proposition A.3, we show that the stationary distribution of the SDE ( <ref type="formula" target="#formula_33">21</ref>) converges to the standard Gaussian N (0, I) in the limit as t → 1. Proposition A.3. Fix any T ∈ (0, 1), and for any t ∈ [0, T ], the density evolution for the rectified flow ODE ( <ref type="formula" target="#formula_7">6</ref>) is same as that of the SDE (12). Furthermore, denoting p t (•) as the marginal pdf of Y t , its stationary distribution p t (Y t ) ∝ exp (-∥Yt∥ 2 2t ), which converges to N (0, I) as t → 1. We note that Lemma A.1 and Lemma A.2 follow from the duality between the heat equation and the continuity equation <ref type="bibr">(Øksendal, 2003)</ref>, where it is classically known that one can interpret a diffusive term as a vector field that is affine in the score function, and vice-versa. This connection has been carefully used to study a large family of stochastic interpolants (that generalize rectified flows) in <ref type="bibr">(Albergo &amp; Vanden-Eijnden, 2023;</ref><ref type="bibr">Albergo et al., 2023)</ref>, and which can lead to a family of ODE-SDE pairs. In the lemmas above, we have provided explicit coefficients that have been directly derived, instead of using the stochastic interpolant formulation. Our key contribution lies in constructing a controlled ODEs ( <ref type="formula" target="#formula_14">8</ref>) and ( <ref type="formula" target="#formula_24">15</ref>), along with their equivalent SDEs ( <ref type="formula" target="#formula_18">10</ref>) and ( <ref type="formula" target="#formula_26">17</ref>) in Theorem 3.4 and Theorem 3.5, respectively. This aids faithfulness and editability as discussed in §4.</p><p>In Lemma A.4, we derive a rectified SDE that transforms noise into images by reversing the stochastic equivalent of rectified flows (12). Lemma A.4. Fix any small δ ∈ (0, 1), and for any t ∈ [δ, 1], the process X t governed by the SDE:</p><formula xml:id="formula_34">dX t = 1 t X t + 2(1 -t) t ∇ log p 1-t (X t ) dt + 2(1 -t) t dW t , X 0 ∼ p 1 ,<label>(22)</label></formula><p>is the time-reversal of the SDE (12).</p><p>Implication. The reverse SDE (22) provides a stochastic sampler for SoTA rectified flow models like Flux. Unlike diffusion-based generative models that explicitly model the score function ∇ log p t (•) in ( <ref type="formula" target="#formula_34">22</ref>), rectified flows model a vector field, as discussed in §3.1. However, given a neural network u(y t , t; φ)) approximating the vector field u t (y t ), Lemma A.1 offers an explicit formula for computing the score function:</p><formula xml:id="formula_35">∇ log p t (Y t ) = - 1 t Y t - 1 -t t u(Y t , t; φ).<label>(23)</label></formula><p>This score function is used to compute the drift and diffusion coefficients of the SDE ( <ref type="formula" target="#formula_34">22</ref>), resulting in a practically implementable stochastic sampler for Flux. This extends the applicability of Flux to downstream tasks where SDE-based samplers have demonstrated practical benefits, as seen in diffusion models <ref type="bibr" target="#b14">(Ho et al., 2020;</ref><ref type="bibr">Song et al., 2021b;</ref><ref type="bibr" target="#b34">Rombach et al., 2022;</ref><ref type="bibr" target="#b30">Podell et al., 2023)</ref>.</p><p>Preprint.</p><p>Now, the controlled ODE (8) becomes:</p><formula xml:id="formula_36">dY t = u t (Y t ) + γ (u t (Y t |Y 1 ) -u t (Y t )) dt, Y 0 ∼ p 0 , Y 1 = y 1 = (1 -γ) - 1 1 -t Y t - t 1 -t ∇ log p t (Y t ) + γ Y 1 -Y t 1 -t dt = - 1 1 -t Y t - t 1 -t (1 -γ)∇ log p t (Y t ) + γ 1 -t Y 1 dt = - 1 1 -t (Y t -γY 1 ) - t 1 -t (1 -γ)∇ log p t (Y t ) dt.</formula><p>Using continuity equation <ref type="bibr">(Øksendal, 2003)</ref>, the density evolution of the controlled ODE (8) then becomes:</p><formula xml:id="formula_37">∂p t (Y t ) ∂t = ∇ • 1 1 -t (Y t -γY 1 ) + t 1 -t (1 -γ)∇ log p t (Y t ) p t (Y t ) .<label>(31)</label></formula><p>Applying Fokker-Planck equation <ref type="bibr">(Øksendal, 2003)</ref> to the SDE (10), we have</p><formula xml:id="formula_38">∂p t (Y t ) ∂t + ∇ • - 1 1 -t (Y t -γY 1 ) p t (Y t ) = ∇ • t 1 -t (1 -γ)∇p t (Y t ) ,</formula><p>which can be rearranged to equal (31) completing the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 PROOF OF LEMMA A.1</head><p>Proof. Given (Y 0 , Y 1 ) ∼ p 0 × p 1 , the conditional flow matching loss (5) can be reparameterized as:</p><formula xml:id="formula_39">L CF M (φ) := E t∼U [0,1],(Y0,Y1)∼p1×p0 ∥(Y 1 -Y 0 ) -u(Y t , t; φ)∥ 2 2 , Y t = tY 1 + (1 -t)Y 0 ,</formula><p>where the optimal solution is given by the minimum mean squared estimator:</p><formula xml:id="formula_40">u t (y t ) = E (Y0,Y1)∼p1×p0 [Y 1 -Y 0 |Y t = y t ] .<label>(32)</label></formula><p>Since Y t = tY 1 + (1 -t)Y 0 , we use Tweedie's formula <ref type="bibr" target="#b9">(Efron, 2011)</ref> to compute</p><formula xml:id="formula_41">E [Y 0 |Y t = y t ] = 1 1 -t y t + t 2 1 -t ∇ log p t (y t ).<label>(33)</label></formula><p>Using the above relation, we obtain the following:</p><formula xml:id="formula_42">E [Y 1 |Y t = y t ] = 1 t E [Y t -(1 -t)Y 0 |Y t = y t ] = 1 t (y t -(1 -t)E [Y 0 |Y t = y t ]) = 1 t y t -(1 -t) 1 1 -t y t + t 2 1 -t ∇ log p t (y t ) = -t ∇ log p t (y t ).<label>(34)</label></formula><p>Combining ( <ref type="formula" target="#formula_41">33</ref>) and (34) using linearity of expectation, we get</p><formula xml:id="formula_43">u t (y t ) = E [Y 1 |Y t = y t ] -E [Y 0 |Y t = y t ] (35) = -t ∇ log p t (y t ) - 1 1 -t y t - t 2 1 -t ∇ log p t (y t ) (36) = - 1 1 -t y t - t 1 -t ∇ log p t (y t ),<label>(37)</label></formula><p>The density evolution of Y t now immediately follows from the continuity equation <ref type="bibr">(Øksendal, 2003)</ref> applied to (19).</p><p>Preprint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 PROOF OF LEMMA A.2</head><p>Proof. The Fokker-Planck equation of the SDE ( <ref type="formula" target="#formula_21">12</ref>) is given by</p><formula xml:id="formula_44">∂p t (Y t ) ∂t + ∇ • - 1 1 -t Y t p t (Y t ) = ∇ • t 1 -t ∇p t (Y t ) .<label>(38)</label></formula><p>Rearranging ( <ref type="formula" target="#formula_44">38</ref>) by multiplying and dividing p t (Y t ) in the right hand side, we get</p><formula xml:id="formula_45">∂p t (Y t ) ∂t = ∇ • 1 1 -t Y t + t 1 -t ∇ log p t (Y t ) p t (Y t ) .<label>(39)</label></formula><p>To conclude, observe that that the density evolution above is identical to (20).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.6 PROOF OF PROPOSITION A.3</head><p>Proof. The optimal vector field of the rectified flow ODE ( <ref type="formula" target="#formula_7">6</ref>) is given by Lemma A.1. The proof then immediately follows from the Fokker-Planck equations in Lemma A.1 and Lemma A.2.</p><p>From Lemma A.2, the density evolution of the SDE ( <ref type="formula" target="#formula_21">12</ref>) is given by</p><formula xml:id="formula_46">∂p t (Y t ) ∂t = ∇ • 1 1 -t Y t + t 1 -t ∇ log p t (Y t ) p t (Y t ) .</formula><p>The stationary (or steady state) distribution satisfies the following:</p><formula xml:id="formula_47">∂p t (Y t ) ∂t = 0 = ∇ • 1 1 -t Y t + t 1 -t ∇ log p t (Y t ) p t (Y t ) .</formula><p>Using the boundary conditions <ref type="bibr">(Øksendal, 2003)</ref>, we get</p><formula xml:id="formula_48">1 1 -t Y t + t 1 -t ∇ log p t (Y t ) = 0, which immediately implies p t (Y t ) ∝ e -∥Y t ∥ 2 2t .</formula><p>B.7 PROOF OF THEOREM 3.5</p><p>Proof. Using Fokker-Planck equation <ref type="bibr">(Øksendal, 2003)</ref>, Lemma A.4 implies</p><formula xml:id="formula_49">∂q t (X t ) ∂t = ∇ • -q t (X t ) 1 t X t + 1 -t t ∇ log q t (X t ) .</formula><p>Therefore, the optimal vector field v t (X t ) of the controlled ODE ( <ref type="formula" target="#formula_24">15</ref>) is given by</p><formula xml:id="formula_50">v t (X t ) = 1 t X t + 1 -t t ∇ log p 1-t (X t ).<label>(40)</label></formula><p>The LQR problem ( <ref type="formula" target="#formula_25">16</ref>) is identical to the LQR problem (7) with changes in the initial and terminal states. Similar to Proposition 3.2, we compute the closed-form solution for the conditional vector field of the ODE (15) as:</p><formula xml:id="formula_51">v t (X t |X 1 ) = X 1 -X t 1 -t .<label>(41)</label></formula><p>Combining ( <ref type="formula" target="#formula_50">40</ref>) and (41), we have</p><formula xml:id="formula_52">dX t = [v t (X t ) + η(v t (X t |X 1 ) -v t (X t ))] dt = (1 -η) 1 t X t + 1 -t t ∇ log p 1-t (X t ) + η X 1 -X t 1 -t dt = (1 -η)(1 -t) -ηt t(1 -t) X t + η 1 -t X 1 + (1 -η)(1 -t) t ∇ log p 1-t (X t ) dt = 1 -t -η t(1 -t) X t + η 1 -t X 1 + (1 -η)(1 -t) t ∇ log p 1-t (X t ) dt.</formula><p>Preprint.</p><p>The resulting continuity equation <ref type="bibr">(Øksendal, 2003)</ref> becomes:</p><formula xml:id="formula_53">∂q t (X t ) ∂t = ∇ • - 1 -t -η t(1 -t) X t + η 1 -t X 1 + (1 -η)(1 -t) t ∇ log p 1-t (X t ) q t (X t ) = ∇ • - 1 -t -η t(1 -t) X t + η 1 -t X 1 + 2(1 -η)(1 -t) t ∇ log p 1-t (X t ) q t (X t ) + (1 -η)(1 -t) t ∇ log p 1-t (X t ) q t (X t ) .</formula><p>Using time-reversal property from Propsition 3.2, the above expression simplifies to</p><formula xml:id="formula_54">∂q t (X t ) ∂t + ∇ • 1 -t -η t(1 -t) X t + η 1 -t X 1 + 2(1 -η)(1 -t) t ∇ log p 1-t (X t ) q t (X t ) = ∇ • (1 -η)(1 -t) t ∇q t (X t ) ,</formula><p>which yields the following SDE:</p><formula xml:id="formula_55">dX t = 1 -t -η t(1 -t) X t + η 1 -t X 1 + 2(1 -η)(1 -t) t ∇ log p 1-t (X t ) dt + 2(1 -η)(1 -t) t dW t ,</formula><p>and thus, completes the proof.</p><p>B.8 PROOF OF LEMMA A.4</p><p>Proof. It suffices to show that the Fokker-Planck equations of the SDE ( <ref type="formula" target="#formula_34">22</ref>) and ( <ref type="formula" target="#formula_21">12</ref>) are the same after time-reversal. Let q t (•) denote the marginal pdf of X t such that q 0 (•) = p 1 (•). The Fokker-Planck equations of the SDE (22) becomes</p><formula xml:id="formula_56">∂q t (X t ) ∂t + ∇ • q t (X t ) 1 t X t + 2(1 -t) t ∇ log p 1-t (X t ) = ∇ • 1 -t t ∇q t (X t ) ,</formula><p>which can be rearranged to give</p><formula xml:id="formula_57">∂q t (X t ) ∂t = ∇ • -q t (X t ) 1 t X t + 2(1 -t) t ∇ log p 1-t (X t ) + 1 -t t ∇q t (X t ) = ∇ • -q t (X t ) 1 t X t + 2(1 -t) t ∇ log p 1-t (X t ) - 1 -t t ∇ log q t (X t )</formula><p>Since Y t is the time-reversal process of X t as discussed in Proposition (3.1),</p><formula xml:id="formula_58">∂q t (X t ) ∂t = ∇ • -q t (X t ) 1 t X t + 1 -t t ∇ log q t (X t ) . Substituting t → 1 -t, ∂q 1-t (X 1-t ) ∂t = ∇ • q 1-t (X 1-t ) 1 1 -t X 1-t + t 1 -t ∇ log q 1-t (X 1-t ) ,</formula><p>which implies the density evolution of (12):</p><formula xml:id="formula_59">∂p t (Y t ) ∂t = ∇ • p t (Y t ) 1 1 -t Y t + t 1 -t ∇ log p t (Y t ) .</formula><p>This completes the proof of the statement.</p><p>Preprint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C ADDITIONAL EXPERIMENTS</head><p>This section substantiates our contributions further by providing additional experimental details.</p><p>Baselines. We use the official NTI codebase<ref type="foot" target="#foot_0">foot_0</ref> for the implementations of NTI <ref type="bibr" target="#b25">(Mokady et al., 2023)</ref>, P2P <ref type="bibr" target="#b12">(Hertz et al., 2022)</ref>, and DDIM <ref type="bibr">(Song et al., 2021a)</ref> inversion. We use the official Diffusers implementation<ref type="foot" target="#foot_1">foot_1</ref> for SDEdit and Flux<ref type="foot" target="#foot_2">foot_2</ref> . We modify the pipelines for SDEdit and DDIM inversion to adapt to the Flux backbone.</p><p>For completeness, we include qualitative comparison with a leading training-based approach In-structPix2Pix <ref type="bibr" target="#b6">(Brooks et al., 2023)</ref> <ref type="foot" target="#foot_3">foot_3</ref> and a higher-order differential equation based LEDIT++ <ref type="bibr" target="#b5">(Brack et al., 2024)</ref> <ref type="foot" target="#foot_4">foot_4</ref> ( §C). Table <ref type="table">3</ref> summarizes the requirements of the compared baselines.</p><p>Table <ref type="table">3</ref>: Requirements of compared baselines. Our method outperforms prior works while requiring no additional training, optimization of prompt embedding, or attention manipulation scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Training Optimization Attention Manipulation</head><p>SDEdit <ref type="bibr" target="#b23">(Meng et al., 2022)</ref> DDIM <ref type="bibr">(Song et al., 2021a</ref>) NTI <ref type="bibr" target="#b25">(Mokady et al., 2023)</ref> NTI+P2P <ref type="bibr" target="#b12">(Hertz et al., 2022)</ref> LEDIT++ <ref type="bibr" target="#b5">(Brack et al., 2024)</ref> InstructPix2Pix <ref type="bibr" target="#b6">(Brooks et al., 2023)</ref> Ours</p><p>Metrics. Following SDEdit <ref type="bibr" target="#b23">(Meng et al., 2022)</ref>, we measure faithfulness using L2 loss between the stroke input and the output image, and assess realism using Kernel Inception Distance (KID) between real and generated images. Stroke inputs are generated from RGB images using the algorithm provided in SDEdit. Given the subjective nature of image editing, we conduct a large-scale user study to calculate the user preference metric.</p><p>For face editing, we evaluate identity preservation, prompt alignment, and overall image quality using a face recognition metric <ref type="bibr" target="#b40">(Ruiz et al., 2024)</ref>, CLIP-T scores <ref type="bibr" target="#b31">(Radford et al., 2021)</ref>, and using CLIP-I scores <ref type="bibr" target="#b31">(Radford et al., 2021)</ref>, respectively. For the face recognition score, we calculate the L2 distance between the face embedding of the original image and the edited image, obtained from Inception ResNet trained on CASIA-Webface dataset. Similar to SDEdit <ref type="bibr" target="#b23">(Meng et al., 2022)</ref>, we conduct extensive experiments on Stroke2Image generation, and showcase additional capabilities qualitatively on a wide variety of semantic image editing tasks.</p><p>Algorithm. The pseudo-code for getting the structured noise is provided in Algorithm 1, and transforming that noise back to an image is given in Algorithm 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 HYPER-PARAMETER CONFIGURATIONS</head><p>In Table <ref type="table" target="#tab_4">4</ref>, we provide the hyper-parameters for the empirical results reported in §5. We use a fix γ = 0.5 in our controlled forward ODE (8) and a time-varying guidance parameter η t in our controlled reverse ODE (15), as motivated in Remark 3.3 and Remark 3.6. Thus, our algorithm introduces one additional hyper-parameter η t into the Flux pipeline. For each experiment, we use a fixed time-varying schedule of η t described by starting time (s), stopping time τ , and strength (η). We use the default config for Flux model: 3.5 for classifier-free guidance and 28 for the total number of inference steps.</p><p>Algorithm 1: Controlled Forward ODE (8) Input: Discretization steps N , reference image y 0 , prompt embedding network Φ, Flux model u(•, •, •; φ), Flux noise scheduler σ : [0, 1] → R Tunable parameter: Controller guidance γ Output: Structured noise Y 1 1 Initialize Y 0 = y 0 2 Fix a noise sample y 1 3 for i = 0 to N -1 do 4 Current time step: t i = i N 5 Next time step: t i+1 = i+1 N 6 Unconditional vector field: u ti (Y ti ) = u(Y ti , t i , Φ(""); φ) ▷ Proposition 3.1 7 Conditional vector field: u ti (Y ti |y 1 ) = y1-Yt i 1-ti ▷ Proposition 3.2 8 Controlled vector field: ûti (Y ti ) u ti (Y ti ) + γ (u ti (Y ti |y 1 ) -u ti (Y ti )) ▷ODE (8) 9 Next state: Y ti+1 = Y ti + ûti (Y ti ) (σ(t i+1 ) -σ(t i )) 10 end 11 return Y 1 Algorithm 2: Controlled Reverse ODE (15) Input: Discretization steps N , reference text "prompt", reference image y 0 , prompt embedding network Φ, Flux model u(•, •, •; φ), Flux noise scheduler σ : [0, 1] → R, structured noise y 1 Tunable parameter: Controller guidance η In this section, we conduct ablation study for our controller guidance parameter η t . We consider two different time-varying schedules for η t , and show that our controller strength allows for a smooth interpolation between unconditional and conditional generation.</p><formula xml:id="formula_60">Output: Edited image X 1 1 Initialize X 0 = y 1 2 for i = 0 to N -1 do 3 Current time step: t i = i N 4 Next time step: t i+1 = i+1 N 5 Unconditional vector field: v ti (X ti ) = -u(X ti , 1 -t i , Φ(prompt); φ) ▷ Proposition 3.1 6 Conditional vector field: v ti (X ti |y 0 ) = y0-Xt i 1-ti ▷ Proposition 3.2 7 Controlled vector field: vti (X ti ) = v ti (X ti ) + η (v ti (X ti |y 0 ) -v ti (X ti )) ▷ODE (15) 8 Next state: X ti+1 = X ti + vti (X ti ) (σ(t i+1 ) -σ(t i )) 9 end 10 return X 1</formula><p>Preprint.</p><p>In Figure <ref type="figure" target="#fig_6">9</ref>, we show the effect of starting time in controlling the faithfulness of inversion; starting time s ∈ [0, 1] is defined as the time at which our controlled reverse ODE ( <ref type="formula" target="#formula_24">15</ref>) is initialized. The initial state X s = y 1-s is obtained by integrating the controlled forward ODE (8) from 0 → 1 -s. In Figure <ref type="figure" target="#fig_7">10</ref>, we study the effect of stopping time. We find that increasing controller guidance η t by increasing the stopping time τ guides the reverse flow towards the original image. However, we observe a phase transition around τ = 0.14 = 4/28, indicating that the resulting drift in our controlled reverse ODE ( <ref type="formula" target="#formula_24">15</ref>) is dominated by the conditional vector field v t (X t |y 0 ) for t ≥ τ . Therefore, the reverse flow solves the LQR problem ( <ref type="formula" target="#formula_25">16</ref>) and drives toward the terminal state (i.e., the original image). In Figure <ref type="figure" target="#fig_9">11</ref>, we visualize the effect of our controller guidance for another time-varying schedule.</p><p>We make a similar observation as in Figure <ref type="figure" target="#fig_7">10</ref>: increasing η t improves faithfulness. However, we notice a smooth transition from the unconditional to the conditional vector field, evidence from the smooth interpolation between "A young man" at the top left (η = 0) and the original image at the bottom right.</p><p>Preprint.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 NUMERICAL SIMULATION</head><p>In this section, we design synthetic experiments to compare reconstruction accuracy of DM and RF inversion. Given Y 0 ∼ p 0 , where the data distribution p 0 := N (µ, I) and the source distribution q 0 := N (0, I), we numerically simulate the ODEs and SDEs associated with DM and RF inversion; see our discussion in §3.</p><p>For µ = 10, we fix γ = 0.5 in the controlled forward ODE (8), and η = 0.5 in the controlled reverse ODE (15). These ODEs are simulated using the Euler discretization scheme with 100 steps. Additionally, we simulate the uncontrolled rectified flow ODEs (6) → (1) as a special case of our controlled ODEs (8) → (15) by setting γ = η = 0, and the deterministic diffusion model DDIM <ref type="bibr">(Song et al., 2021a)</ref> in the same experimental setup.</p><p>The inversion accuracy is reported in Table <ref type="table">5</ref>. Observe that RF inversion has less L2 and L1 error compared to DDIM inversion (14). The minimum error is obtained by setting γ = η = 0 (i.e., reversing the standard rectified flows), which supports our discussion in §3.3.</p><p>Furthermore, we simulate the stochastic samplers corresponding to these ODEs in Table <ref type="table">5</ref>, highlighted in orange. Similar to the deterministic samplers, we observe that stochastic equivalents of rectified flows more accurately recover the original sample compared to diffusion models. Our controller in RF Inversion (10) → (17) effectively reduces the reconstruction error in the uncontrolled RF Inversion (12) → (22), which are special cases when γ = η = 0. Thus, we demonstrate that (controlled) rectified stochastic processes are better at inverting a given sample from the target distribution, outperforming the typical OU process used in diffusion models <ref type="bibr" target="#b44">(Song &amp; Ermon, 2019;</ref><ref type="bibr" target="#b14">Ho et al., 2020;</ref><ref type="bibr">Song et al., 2021a;</ref><ref type="bibr">b)</ref>.</p><p>In Figure <ref type="figure">12</ref>, we compare sample paths of diffusion models and recitified flows using 10 IID samples drawn from p 0 . In Figure <ref type="figure" target="#fig_1">13</ref>, we visualize paths for those samples using our controlled ODEs and SDEs with γ = η = 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 ADDITIONAL RESULTS ON STROKE2IMAGE GENERATION</head><p>In Figure <ref type="figure" target="#fig_0">14</ref> and Figure <ref type="figure">15</ref>, we show additional qualitative results on Stroke2Image generation. Our method generates more realistic images compared to leading training-free approaches in semantic image editing including optimization-based NTI <ref type="bibr" target="#b25">(Mokady et al., 2023)</ref> and attention-based NTI+P2P <ref type="bibr" target="#b12">(Hertz et al., 2022)</ref>. Furthermore, it gives a competitive advantage over the training-based approach InstructPix2Pix <ref type="bibr" target="#b6">(Brooks et al., 2023)</ref>. Figure <ref type="figure" target="#fig_13">19</ref> shows the insertion of multiple objects by text prompts, such as "pepperoni", "mushroom", and "green leaves" to an image of a pizza. Interestingly, pepperoni is not deleted while inserting mushroom, and mushroom is not deleted while inserting green leaves. The product is finally presented in a lego style.</p><p>Figure <ref type="figure" target="#fig_14">20</ref> captures a variety of facial expressions that stylize a reference image. Given the original image and text prompt: e.g. "Face of a girl in disney 3d cartoon style", we first invert the image to generate the stylized version of the original image. Then, we add the prompt for the expression (e.g., "surprised") at the end of the prompt and run our editing algorithm (15) with this new prompt: "Face of a girl in disney 3d cartoon style, surprised". By changing the expression, we are able to preserve the identity of the stylized girl and generate prompt-based facial expressions. For inversion, all methods perform well at recovering the stroke input when given a null prompt. However, when a new prompt like "a photo-realistic picture of a bedroom" is provided, only our method successfully generates realistic images. The other methods continue to suffer from the initial corruption, failing to make the output more realistic.</p><p>Figure <ref type="figure" target="#fig_4">17</ref>: Gender editing. Our method smoothly interpolates between "A man" ↔ "A woman".</p><p>"A woman" "old" "older" "A man" "young" "younger" Figure <ref type="figure">21</ref> shows stylization based on a single reference style image and 12 different text prompts, covering both living and non-living objects. The generated images contain various style attributes that includes melting elements, golden color, and 3d rendering from the reference style image.</p><p>Figure <ref type="figure">22</ref> visualizes stylization results based on different reference style images. In this experiment, we use text prompt to describe both the content of the generated image and the style of the given reference style image.</p><p>Original + "pepperoni" + "mushroom" + "green leaves" + "in lego style" "Face of a girl in disney 3d cartoon style" "scared" "surprised" "frowning" "yawning" "laughing" Original "gasping" "sad" "smiling" "winking" "grinning" "angry"</p><p>"Face of a boy in disney 3d cartoon style" "laugh" "smirking" "frown" "angry" "yelling" Original </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.6 HUMAN EVALUATION</head><p>We conduct a user study on the test splits of both LSUN Bedroom and LSUN Church dataset using Amazon Mechanical Turk, with 126 participants in total. As shown in Figure <ref type="figure" target="#fig_1">23</ref>, each question was accompanied by an explanation of the task, the question, and the evaluation criteria. Participants were shown a pair of stroke-to-image outputs from different models, in random order, along with the input stroke image. They were asked to select one of three options based on their preference using the following two criteria: 1. Realism: which of these two images look more like a real, photorealistic image? 2. Faithfulness: which of these two images match more closely to the input stroke image?</p><p>We collect 3 responses per question. With 300 images in the test dataset and 10 pairwise comparisons, we gathered 9,000 responses for this evaluation. The example in Figure <ref type="figure" target="#fig_1">23</ref> is for the LSUN Church dataset; for LSUN Bedroom dataset, we simply replace the word "church" to "bedroom" in the instructions.</p><p>"line drawing" "melting golden" Flux Flux Ours Ours "3d rendering" Flux Ours "wooden sculpture" Flux Ours</p><p>Figure <ref type="figure">22</ref>: Stylization using a single prompt and various reference style images: "melting golden", "line drawing", "3d rendering", and "wooden sculpture". Given a style image (e.g. "3d rendering") and a text prompt (e.g. "face of a boy in 3d rendering style"), our method generates images that are consistent with the reference style image and the text prompt. The standard output from Flux is obtained by disabling our controller, which clearly highlights the importance of the controller.</p><p>Figure <ref type="figure" target="#fig_1">23</ref>: Interface for human evaluation. Each participant is asked to select their preferred image based on two criteria: realism and faithfulness.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Effect of controller guidance η given the original image and the prompt: "A young man". Increasing η improves the faithfulness to the original image, which is reconstructed at η = 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Inverting flows by controlled ODEs (8) and (15).</figDesc><graphic coords="8,385.34,425.81,58.99,58.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure 5: Stroke2Image generation. Our method generates photo-realistic images of bedroom or church given stroke paints, showing robustness to initial corruptions.</figDesc><graphic coords="9,174.50,263.80,63.36,63.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Editing (a) stylized expression, (b) age, (c) gender, and (d) object insert. Given an original image and a text prompt, our algorithm performs semantic image editing in the wild.</figDesc><graphic coords="10,240.49,132.13,63.50,63.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Comparison using Flux backbone.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Effect of starting time. Prompt: "A young man". The number below each figure denotes the starting time scaled by 28 (the total number of denoising steps) for better interpretation. In the absence of controller guidance (η t = 0), increasing the starting time (s) in our controlled ODE (15) improves faithfulness to the original image.</figDesc><graphic coords="22,108.40,207.48,63.22,63.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Effect of controller guidance. Prompt: "A young man". For a fixed starting time s = 0, consider a time-varying controller guidance schedule η t = η ∀t ≤ τ and 0 otherwise. The number below each figure denotes the stopping time τ scaled by 28 (the total number of denoising steps) for better interpretation. Increasing τ increases the controller guidance (η t ) that improves faithfulness to the original image.</figDesc><graphic coords="22,440.22,515.47,63.25,63.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Effect of controller guidance for another time-varying schedule. Prompt: "A young man". The number below each figure denotes the starting time scaled by 28 (the total number of denoising steps) for better interpretation. For a fixed starting time s = 0 and stopping time τ = 8, consider a time-varying controller guidance schedule η t = η ∀t ≤ τ and 0 otherwise. Increasing η increases the controller guidance (η t ) that improves faithfulness to the original image.</figDesc><graphic coords="23,440.59,154.71,63.32,63.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 14 :Figure 15 :</head><label>1415</label><figDesc>Figure 14: Stroke2Image generation. Additional qualitative results on LSUN-Bedroom dataset comparing our method with SoTA training-free and training-based editing approaches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 16 :</head><label>16</label><figDesc>Figure16: Robustness. For inversion, all methods perform well at recovering the stroke input when given a null prompt. However, when a new prompt like "a photo-realistic picture of a bedroom" is provided, only our method successfully generates realistic images. The other methods continue to suffer from the initial corruption, failing to make the output more realistic.</figDesc><graphic coords="26,240.17,205.21,53.86,53.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 18 :</head><label>18</label><figDesc>Figure 18: Age editing. Our method regulates the extent of age editing.</figDesc><graphic coords="26,310.57,527.70,62.39,62.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 19 :</head><label>19</label><figDesc>Figure 19: Object insert. Text-guided insertion of multiple objects sequentially.</figDesc><graphic coords="27,110.53,230.89,63.86,63.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 20 :</head><label>20</label><figDesc>Figure 20: Stylization using reference text. Stylization of a reference image given prompt-based facial expressions in "disney 3d cartoon style".</figDesc><graphic coords="27,110.53,399.34,63.86,63.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="29,108.00,467.50,396.01,183.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Quantitative results for Stroke2Image generation. L2 and Kernel Inception Distance (KID) capture faithfulness and realism, respectively. Optimization-based methods are colored gray. User Pref. shows the percentage of users that prefer our method over each alternative in pairwise comparisons (and ties). E.g.: 62.11% (+ 8% ties) prefer ours over SDEdit-Flux for LSUN Bedroom.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">LSUN Bedroom</cell><cell></cell><cell cols="2">LSUN Church</cell></row><row><cell>Method</cell><cell>L2 ↓</cell><cell>KID ↓</cell><cell>User Pref. (%) ↑</cell><cell>L2 ↓</cell><cell>KID ↓</cell><cell>User Pref. (%) ↑</cell></row><row><cell>SDEdit-SD1.5</cell><cell>86.72</cell><cell>0.029</cell><cell>59.67 (5.33)</cell><cell>90.72</cell><cell>0.089</cell><cell>65.33 (4.11)</cell></row><row><cell>SDEdit-Flux</cell><cell>94.89</cell><cell>0.032</cell><cell>62.11 (8.00)</cell><cell>92.47</cell><cell>0.081</cell><cell>66.22 (5.22)</cell></row><row><cell>DDIM Inv.</cell><cell>87.95</cell><cell>0.113</cell><cell>82.56 (1.67)</cell><cell>97.36</cell><cell>0.107</cell><cell>85.44 (2.78)</cell></row><row><cell>NTI</cell><cell>82.77</cell><cell>0.095</cell><cell>80.89 (4.33)</cell><cell>87.88</cell><cell>0.098</cell><cell>77.11 (4.89)</cell></row><row><cell>NTI+P2P</cell><cell>46.46</cell><cell>0.234</cell><cell>98.11 (1.78)</cell><cell>34.48</cell><cell>0.168</cell><cell>99.22 (0.56)</cell></row><row><cell>Ours</cell><cell>82.65</cell><cell>0.025</cell><cell>-</cell><cell>80.36</cell><cell>0.059</cell><cell>-</cell></row></table><note><p>isons from 126 participants. As given in Table</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Quantitative results for face editing on SFHQ for "wearing glasses".</figDesc><table><row><cell>SDEdit-SD1.5</cell><cell>0.626</cell><cell>0.885</cell><cell>0.300</cell><cell>0.712</cell><cell>8</cell></row><row><cell>SDEdit-Flux</cell><cell>0.632</cell><cell>0.892</cell><cell>0.292</cell><cell>0.710</cell><cell>24</cell></row><row><cell>DDIM Inv.</cell><cell>0.709</cell><cell>0.884</cell><cell>0.311</cell><cell>0.669</cell><cell>15</cell></row><row><cell>NTI</cell><cell>0.707</cell><cell>0.876</cell><cell>0.304</cell><cell>0.666</cell><cell>78</cell></row><row><cell>NTI+P2P</cell><cell>0.443</cell><cell>0.953</cell><cell>0.293</cell><cell>0.845</cell><cell>85</cell></row><row><cell>Ours</cell><cell>0.442</cell><cell>0.951</cell><cell>0.300</cell><cell>0.900</cell><cell>39</cell></row></table><note><p>Method</p><p>Face Rec. ↓ DINO ↑ CLIP-T ↑ CLIP-I ↑ Runtime(s) ↓</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Hyper-parameter configuration of our method for inversion and editing tasks.</figDesc><table><row><cell>Task</cell><cell>Starting Time (s)</cell><cell cols="2">Controller Guidance (η t )</cell></row><row><cell></cell><cell></cell><cell cols="2">Stopping Time (τ ) Strength (η)</cell></row><row><cell>Stroke2Image</cell><cell>3</cell><cell>5</cell><cell>0.9</cell></row><row><cell>Object insert</cell><cell>0</cell><cell>6</cell><cell>1.0</cell></row><row><cell>Gender editing</cell><cell>0</cell><cell>8</cell><cell>1.0</cell></row><row><cell>Age editing</cell><cell>0</cell><cell>5</cell><cell>1.0</cell></row><row><cell>Adding glasses</cell><cell>6</cell><cell>25</cell><cell>0.7</cell></row><row><cell>Stylization</cell><cell>0</cell><cell>6</cell><cell>0.9</cell></row><row><cell>C.2 ABLATION STUDY</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/google/prompt-to-prompt</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://github.com/huggingface/diffusers</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://github.com/black-forest-labs/flux</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://huggingface.co/spaces/timbrooks/instruct-pix2pix</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>https://huggingface.co/spaces/editing-images/leditsplusplus</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This research has been supported by <rs type="funder">NSF</rs> Grant <rs type="grantNumber">2019844</rs>, a <rs type="grantName">Google research collaboration award</rs>, and the <rs type="institution">UT Austin Machine Learning Lab</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_jXBG6Vh">
					<idno type="grant-number">2019844</idno>
					<orgName type="grant-name">Google research collaboration award</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Preprint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B TECHNICAL PROOFS</head><p>This section contains technical proofs of the theoretical results presented in this paper.</p><p>B.1 PROOF OF PROPOSITION 3.2</p><p>Proof. The standard approach to solving an LQR problem is the minimum principle theorem that can be found in control literature <ref type="bibr" target="#b11">(Fleming &amp; Rishel, 1975;</ref><ref type="bibr" target="#b3">Basar et al., 2020)</ref>. We follow this approach and provide the full proof below for completeness.</p><p>The Hamiltonian of the LQR problem ( <ref type="formula">7</ref>) is given by</p><p>For c * t = -p t , the Hamiltonian attains its minumum value:</p><p>Using minimum principle theorem <ref type="bibr" target="#b11">(Fleming &amp; Rishel, 1975;</ref><ref type="bibr" target="#b3">Basar et al., 2020)</ref>, we get</p><p>From ( <ref type="formula">25</ref>), we know p t is a constant p. Using this constant in (26) and integrating from t → 1, we have</p><p>(z t -y 1 ).</p><p>Taking the limit λ → ∞, we get p = zt-y1 1-t and the optimal controller c * t = y1-zt 1-t . Since u t (z t |y 1 ) = y 1 -y 0 , the proof follows by substituting y 0 = zt-ty1 1-t .</p><p>B.2 PROOF OF PROPOSITION 3.1</p><p>Proof. Initializing the generative ODE (1) with the structured noise y 1 , we get</p><p>Since ( <ref type="formula">29</ref>) and (30) hold ∀t ∈ [0, 1] and</p><p>Preprint.    In Figure <ref type="figure">16</ref>, we demonstrate the robustness of our approach to corruption at initialization. All the methods transform the stroke input (corrupt image) to a structured noise, which is again transformed back to a similar looking stroke input, highlighting the faithfulness of these methods. However, unlike our approach, the resulting images in other methods are not editable given a new prompt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5 ADDITIONAL RESULTS ON SEMANTIC IMAGE EDITING</head><p>Figure <ref type="figure">17</ref> illustrates a smooth interpolation between "A man" → "A woman" (top row) and "A woman" → "A man" (bottom row). The facial expression and the hair style are gradually morphed from one person to the other.</p><p>In Figure <ref type="figure">18</ref>, we show the ability to regulate the extent of age editing. Given an image of a young woman and the prompt "An old woman", we gradually reduce the controller strength η t to make the person look older. Similarly, we reduce the strength to make an old man look younger.</p><p>Preprint.</p><p>"melting golden 3d rendering" (reference style image) "a butterfly" "a boat" "a piano" "a robot" "an f1 car" "a wood cabin" "a bench" "a boy" "a dragon" "a cat" "a dwarf" "a baby penguin" In Figure <ref type="figure">24</ref>, we compare images generated by Flux (an ODE-based sampler ( <ref type="formula">19</ref>)). The similarity between the images generated by the ODE and SDE versions of Flux strengthens the practical significance of our theoretical results ( §3).</p><p>Preprint. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Nicholas</forename><forename type="middle">M</forename><surname>Michael S Albergo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Boffi</surname></persName>
		</author>
		<author>
			<persName><surname>Vanden-Eijnden</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.08797</idno>
		<title level="m">Stochastic interpolants: A unifying framework for flows and diffusions</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Building normalizing flows with stochastic interpolants</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albergo</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Vanden-Eijnden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Reverse-time diffusion equation models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">O</forename><surname>Brian</surname></persName>
		</author>
		<author>
			<persName><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Stochastic Processes and their Applications</title>
		<imprint>
			<date type="published" when="1982">1982</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="313" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Tamer</forename><surname>Basar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Meyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">R</forename><surname>Perkins</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01367</idno>
		<title level="m">Lecture notes on control system theory and design</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Synthetic faces high quality (sfhq) dataset</title>
		<author>
			<persName><forename type="first">David</forename><surname>Beniaguev</surname></persName>
		</author>
		<ptr target="https://github.com/SelfishGene/SFHQ-dataset" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Ledits++: Limitless image editing using text-to-image models</title>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Brack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katharia</forename><surname>Kornmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linoy</forename><surname>Tsaban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Schramowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristian</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apolinário</forename><surname>Passos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="8861" to="8870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Instructpix2pix: Learning to follow image editing instructions</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Holynski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="18392" to="18402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Diffedit: Diffusionbased semantic image editing with mask guidance</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Couairon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2023 (Eleventh International Conference on Learning Representations)</title>
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Inversion by direct iteration: An alternative to denoising diffusion for image restoration</title>
		<author>
			<persName><forename type="first">Mauricio</forename><surname>Delbracio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peyman</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Machine Learning Research</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Tweedie&apos;s formula and selection bias</title>
		<author>
			<persName><forename type="first">Bradley</forename><surname>Efron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">496</biblScope>
			<biblScope unit="page" from="1602" to="1614" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Scaling rectified flow transformers for high-resolution image synthesis</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumith</forename><surname>Kulal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahim</forename><surname>Entezari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harry</forename><surname>Saini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yam</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Axel</forename><surname>Sauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederic</forename><surname>Boesel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Forty-first International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Wendell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><forename type="middle">W</forename><surname>Fleming</surname></persName>
		</author>
		<author>
			<persName><surname>Rishel</surname></persName>
		</author>
		<title level="m">Deterministic and stochastic optimal control</title>
		<imprint>
			<publisher>Springer Science &amp; Business Media</publisher>
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Prompt-to-prompt image editing with cross-attention control</title>
		<author>
			<persName><forename type="first">Amir</forename><surname>Hertz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><surname>Mokady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kfir</forename><surname>Aberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yael</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Style aligned image generation via shared attention</title>
		<author>
			<persName><forename type="first">Amir</forename><surname>Hertz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Voynov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shlomi</forename><surname>Fruchter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.02133</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Lora: Low-rank adaptation of large language models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shean</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Humansd: A native skeleton-guided diffusion model for human image generation</title>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ailing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenchen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="15988" to="15998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Elucidating the design space of diffusionbased generative models</title>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="26565" to="26577" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Analyzing and improving the training dynamics of diffusion models</title>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="24174" to="24184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Diffusionclip: Text-guided diffusion models for robust image manipulation</title>
		<author>
			<persName><forename type="first">Gwanghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taesung</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><surname>Chul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2426" to="2435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Diffusion-based image translation using disentangled style and content representation</title>
		<author>
			<persName><forename type="first">Gihyun</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><surname>Chul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Nayau9fwXU" />
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Yaron</forename><surname>Lipman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricky Tq</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heli</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.02747</idno>
		<title level="m">Maximilian Nickel, and Matt Le. Flow matching for generative modeling</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Flow straight and fast: Learning to generate and transfer data with rectified flow</title>
		<author>
			<persName><forename type="first">Xingchao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengyue</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sdedit: Guided image synthesis and editing with stochastic differential equations</title>
		<author>
			<persName><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Negative-prompt inversion: Fast image inversion for editing with text-guided diffusion models</title>
		<author>
			<persName><forename type="first">Daiki</forename><surname>Miyake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akihiro</forename><surname>Iohara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toshiyuki</forename><surname>Tanaka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.16807</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Null-text inversion for editing real images using guided diffusion models</title>
		<author>
			<persName><forename type="first">Ron</forename><surname>Mokady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Hertz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kfir</forename><surname>Aberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yael</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="6038" to="6047" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Glide: Towards photorealistic image generation and editing with text-guided diffusion models</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nichol</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bob</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="16784" to="16804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Bernt</forename><surname>Øksendal</surname></persName>
		</author>
		<title level="m">Stochastic differential equations</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Scalable diffusion models with transformers</title>
		<author>
			<persName><forename type="first">William</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="4195" to="4205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Würstchen: An efficient architecture for large-scale text-to-image diffusion models</title>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Pernias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominic</forename><surname>Rampas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leon</forename><surname>Mats</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><surname>Aubreville</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=gU58d5QeGv" />
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sdxl: Improving latent diffusion models for high-resolution image synthesis</title>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Podell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zion</forename><surname>English</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lacey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dockhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Penna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Zero-shot text-to-image generation</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8821" to="8831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Hierarchical textconditional image generation with clip latents</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Casey</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.06125</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Highresolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Björn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
	<note>Preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">A theoretical justification for image inpainting using denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Litu</forename><surname>Rout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Advait</forename><surname>Parulekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Constantine</forename><surname>Caramanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Shakkottai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.01217</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Solving inverse problems provably via posterior sampling with latent diffusion models</title>
		<author>
			<persName><forename type="first">Litu</forename><surname>Rout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Negin</forename><surname>Raoof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giannis</forename><surname>Daras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Constantine</forename><surname>Caramanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><forename type="middle">G</forename><surname>Dimakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Shakkottai</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=XKBFdYwfRo" />
	</analytic>
	<monogr>
		<title level="m">Thirty-seventh Conference on Neural Information Processing Systems, 2023b</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Beyond first-order tweedie: Solving inverse problems using latent diffusion</title>
		<author>
			<persName><forename type="first">Litu</forename><surname>Rout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Constantine</forename><surname>Caramanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Shakkottai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Sheng</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Litu</forename><surname>Rout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nataniel</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Constantine</forename><surname>Caramanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Shakkottai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Sheng</forename><surname>Chu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.17401</idno>
		<title level="m">Rb-modulation: Training-free personalization of diffusion models using stochastic optimal control</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation</title>
		<author>
			<persName><forename type="first">Nataniel</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yael</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kfir</forename><surname>Aberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="22500" to="22510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Hyperdreambooth: Hypernetworks for fast personalization of text-to-image models</title>
		<author>
			<persName><forename type="first">Nataniel</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingbo</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yael</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neal</forename><surname>Wadhwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kfir</forename><surname>Aberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="6527" to="6536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Palette: Image-to-image diffusion models</title>
		<author>
			<persName><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huiwen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep unsupervised learning using nonequilibrium thermodynamics</title>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niru</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="https://proceedings.mlr.press/v37/sohl-dickstein15.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">David</forename><surname>Blei</surname></persName>
		</editor>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07">Jul 2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="7" to="09" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Denoising diffusion implicit models</title>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=St1giarCHLP" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations, 2021a</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Generative modeling by estimating gradients of the data distribution</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Score-based generative modeling through stochastic differential equations</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><surname>Poole</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=PxTIG12RRHS" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations, 2021b</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Plug-and-play diffusion features for text-driven image-to-image translation</title>
		<author>
			<persName><forename type="first">Narek</forename><surname>Tumanyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Geyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shai</forename><surname>Bagon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tali</forename><surname>Dekel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1921" to="1930" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Theoretical guarantees for sampling and inference in generative models with latent diffusions</title>
		<author>
			<persName><forename type="first">Belinda</forename><surname>Tzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Raginsky</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3084" to="3114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Knowledge guided disambiguation for large-scale scene classification with multi-resolution cnns</title>
		<author>
			<persName><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2055" to="2068" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A latent space of stochastic diffusion models for zeroshot image editing and guidance</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wu</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="7378" to="7387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Path integral sampler: A stochastic control approach for sampling</title>
		<author>
			<persName><forename type="first">Qinsheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongxin</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=_uCb2ynRu7Y" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
