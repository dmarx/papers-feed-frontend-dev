# SEMANTIC IMAGE INVERSION AND EDITING USING RECTIFIED STOCHASTIC DIFFERENTIAL EQUATIONS

## Abstract

## 

(c) Ref. content "sleeping cat" "tiger" "lion" "origami cat" "silver cat sculpture" (b)

Ref. style "face of a boy" (a)

Ref. style "a girl" "a panda" "a dwarf" (d) Ref. content "smiling cartoon" "girl" "old man" "young boy+glasses" "angry cartoon"

Figure [1](#): Rectified flows for image inversion and editing. Our approach efficiently inverts reference style images in (a) and (b) without requiring text descriptions of the images and applies desired edits based on new prompts (e.g. "a girl" or "a dwarf"). For a reference content image (e.g. a cat in (c) or a face in (d)), it performs semantic image editing (e.g. "sleeping cat") and stylization (e.g. "a photo of a cat in origmai style") based on prompts, without leaking unwanted content from the reference image. Input images have orange borders.

## ABSTRACT

Generative models transform random noise into images; their inversion aims to transform images back to structured noise for recovery and editing. This paper addresses two key tasks: (i) inversion and (ii) editing of a real image using stochastic equivalents of rectified flow models (such as Flux). Although Diffusion Models (DMs) have recently dominated the field of generative modeling for images, their inversion presents faithfulness and editability challenges due to nonlinearities in drift and diffusion. Existing state-of-the-art DM inversion approaches rely on training of additional parameters or test-time optimization of latent variables; both are expensive in practice. Rectified Flows (RFs) offer a promising alternative to diffusion models, yet their inversion has been underexplored. We propose RF inversion using dynamic optimal control derived via a linear quadratic regulator. We prove that the resulting vector field is equivalent to a rectified stochastic differential equation. Additionally, we extend our framework to design a stochastic sampler for Flux. Our inversion method allows for state-of-the-art performance in zero-shot inversion and editing, outperforming prior works in stroke-to-image synthesis and semantic image editing, with large-scale human evaluations confirming user preference.

## INTRODUCTION

Vision generative models typically transform noise into images. Inverting such models, given a reference image, involves finding the structured noise that can regenerate the original image. Efficient inversion must satisfy two crucial properties. First, the structured noise should produce an image that is faithful to the reference image. Second, the resulting image should be easily editable using new prompts, allowing fine modifications over the image. Diffusion Models (DMs) have become the mainstream approach for generative modeling of images [(Sohl-Dickstein et al., 2015;](#b42)[Song & Ermon, 2019;](#b44)[Ho et al., 2020)](#b14), excelling at sampling from high-dimensional distributions [(Ramesh et al., 2021;](#b32)[Saharia et al., 2022;](#b41)[Ramesh et al., 2022;](#b33)[Rombach et al., 2022;](#b34)[Podell et al., 2023;](#b30)[Pernias et al., 2024)](#b29). The sampling process follows a Stochastic Differential Equation known as reverse SDE [(Anderson, 1982;](#b2)[Efron, 2011;](#b9)[Song et al., 2021b)](#). Notably, these models can invert a given image. Recent advances in DM inversion have shown a significant impact on conditional sampling, such as stroke-to-image synthesis [(Meng et al., 2022)](#b23), image editing [(Hertz et al., 2022;](#b12)[Mokady et al., 2023;](#b25)[Couairon et al., 2023;](#)[Rout et al., 2023a;](#)[b;](#)[2024a;](#)[Delbracio & Milanfar, 2023)](#b8) and stylization [(Hertz et al., 2023;](#b13)[Rout et al., 2024b)](#).

Despite its widespread usage, DM inversion faces critical challenges in faithfulness and editability.

First, the stochastic nature of the process requires fine discretization of the reverse SDE [(Ho et al., 2020;](#b14)[Song et al., 2021b)](#), which increases expensive Neural Function Evaluations (NFEs). Coarse discretization, on the other hand, leads to less faithful outputs [(Meng et al., 2022)](#b23), even with deterministic methods like DDIM [(Song et al., 2021a;](#)[b)](#). Second, nonlinearities in the reverse trajectory introduce unwanted drift, reducing the accuracy of reconstruction [(Karras et al., 2024)](#b18). While existing methods enhance faithfulness by optimizing latent variables [(Rout et al., 2024a)](#) or prompt embeddings [(Mokady et al., 2023;](#b25)[Miyake et al., 2023)](#b24), they tend to be less efficient, harder to edit, and rely on complex attention processors to align with a given prompt [(Hertz et al., 2022;](#b12)[Rout et al., 2024a)](#). These added complexities make such methods less suitable for real-world deployment.

For inversion and editing, we introduce a zero-shot conditional sampling algorithm using Rectified Flows (RFs) [(Liu et al., 2022;](#b22)[Albergo & Vanden-Eijnden, 2023;](#)[Lipman et al., 2022;](#b21)[Esser et al., 2024)](#b10), a powerful alternative to DMs. Unlike DMs, where sampling is governed by a reverse SDE, RFs use an Ordinary Differential Equation known as reverse ODE, offering advantages in both efficient training and fast sampling. We construct a controlled forward ODE, initialized from a given image, to generate the initial conditions for the reverse ODE. The reverse ODE is then guided by an optimal controller, obtained through solving a Linear Quadratic Regulator (LQR) problem. We prove that the resulting new vector fields have a stochastic interpretation with an appropriate drift and diffusion. We evaluate RF inversion on stroke-to-image generation and image editing tasks, and show extensive qualitative results on other applications like cartoonization. Our method significantly improves photo realism in stroke-to-image generation, surpassing a state-of-the-art (SoTA) method [(Mokady et al., 2023)](#b25) by 89%, while maintaining faithfulness to the input stroke. In addition, we show that RF inversion outperforms DM inversion [(Meng et al., 2022)](#b23) in faithfulness by 4.7% and in realism by 13.8% on LSUN-bedroom dataset [(Wang et al., 2017)](#b48). Figure [1](#) and Figure [2](#) show the qualitative results of our approach and a graphical illustration, respectively.

Our theoretical and practical contributions can be summarized as: ‚Ä¢ We present an efficient inversion method for RF models, including Flux, that requires no additional training, latent optimization, prompt tuning, or complex attention processors. ‚Ä¢ We develop a new vector field for RF inversion, interpolating between two competing objectives: consistency with a possibly corrupted input image, and consistency with the "true" distribution of clean images ( ¬ß3.3). We prove that this vector field is equivalent to a rectified SDE that interpolates between the stochastic equivalents of these competing objectives ( ¬ß3.4). We extend the theoretical results to design a stochastic sampler for Flux. ‚Ä¢ We demonstrate the faithfulness and editability of RF inversion across three benchmarks: (i) LSUN-Bedroom, (ii) LSUN-Church, and (iii) SFHQ, on two tasks: stroke-to-image synthesis and image editing. In addition, we provide extensive qualitative results and conduct large-scale human evaluations to assess user preference metrics ( ¬ß5).

## RELATED WORKS

DM Inversion. Diffusion models have become the mainstream approach for generative modeling, making DM inversion an exciting area of research [(Meng et al., 2022;](#b23)[Couairon et al., 2023](#); Song Figure 2: Graphical model illustrating (a) DDIM inversion and (b) RF inversion. Due to nonlinearities in DM trajectory, the DDIM inverted latent x 1 significantly deviates from the original image y 0 . RF inversion without controller reduces this deviation, resulting in x 1 . With controller, RF inversion further eliminates the reconstruction error, making x 1 nearly identical to y 0 , which enhances the faithfulness. ùë¶!"# ùë¶$ ùë¶# ùë•!"# ùë•! ùë•ÃÖ# ùë•$ ùë•# Fwd. latent DDIM step Rev. latent ùë¶! ùë¶!"# ùë¶$ ùë¶# ùë•!"# ùë•! ùë•$ ùë•# Fwd. latent (a) DDIM Inversion (b) RF Inversion ùë•# ùë•!"# Rev. latent w/ controller Rev. latent w/o controller ùë¶! [et al., 2021b;](#)[Hertz et al., 2023;](#b13)[Mokady et al., 2023;](#b25)[Rout et al., 2024a)](#). Among training-free methods, SDEdit [(Meng et al., 2022)](#b23) adds noise to an image and uses the noisy latent as structured noise. For semantic image editing based on a given prompt, it simulates the standard reverse SDE starting from this structured noise. SDEdit requires no additional parameter training, latent variable optimization, or complex attention mechanisms. However, it is less faithful to the original image because adding noise in one step is equivalent to linear interpolation between the image and noise, while the standard reverse SDE follows a nonlinear path [(Liu et al., 2022;](#b22)[Karras et al., 2022](#b17)).

An alternate method, DDIM inversion [(Song et al., 2021a;](#)[b)](#), recursively adds predicted noise at each forward step and returns the final state as the structured noise (illustrated by Y t process in Figure [2](#)(a)). However, DDIM inversion often deviates significantly from the original image due to nonlinearities in the drift and diffusion coefficients, as well as inexact score estimates [(Mokady et al., 2023)](#b25). To reduce this deviation, recent approaches optimize prompt embeddings [(Mokady et al., 2023)](#b25) or latent variables [(Rout et al., 2024a](#)), but they have high time complexity. Negative prompt inversion [(Miyake et al., 2023)](#b24) speeds up the inversion process but sacrifices faithfulness. Methods like CycleDiffusion (Wu & De la Torre, 2023) and Direction Inversion [(Ju et al., 2023)](#b16) use inverted latents as references during editing, but they are either computationally expensive or not applicable to rectified flow models like Flux or SD3 [(Esser et al., 2024)](#b10).

DM Editing. Efficient inversion is crucial for real image editing. Once a structured noise is obtained by inverting the image, a new prompt is fed into the T2I generative model. Inefficient inversion often fails to preserve the original content and therefore requires complex editing algorithms. These editing algorithms can be broadly classified into (i) attention control, such as prompt-to-prompt [(Hertz et al., 2022)](#b12), plug-and-play (PnP) [(Tumanyan et al., 2023)](#b46), (ii) optimization-based methods like Dif-fusionCLIP [(Kim et al., 2022)](#b19), DiffuseIT [(Kwon & Ye, 2023)](#b20), STSL [(Rout et al., 2024a)](#), and (iii) latent masking to edit specific regions of an image using masks provided by the user [(Nichol et al., 2022)](#) or automatically extracted from the generative model [(Couairon et al., 2023)](#). We focus on efficient inversion, avoiding the need for complex editing algorithms.

Challenges in RF Inversion. Previous inversion or editing approaches have been tailored towards diffusion models and do not directly apply to SoTA rectified flow models like Flux. This limitation arises because the network architecture of Flux is MM-DiT [(Peebles & Xie, 2023)](#b28), which is fundamentally different from the traditional UNet used in DMs [(Ho et al., 2020;](#b14)[Song et al., 2021a;](#)[b)](#). In MM-DiT, text and image information are entangled within the architecture itself, whereas in UNet, text conditioning is handled via cross-attention layers. Additionally, Flux primarily uses T5 text encoder, which lacks an aligned latent space for images, unlike CLIP encoders. Therefore, extending these prior methods to modern T2I generative models requires a thorough investigation. We take the first step by inverting and editing a given image using Flux.

RF Inversion and Editing. DMs [(Ho et al., 2020;](#b14)[Song et al., 2021a;](#)[Rombach et al., 2022)](#b34) traditionally outperform RFs [(Lipman et al., 2022;](#b21)[Liu et al., 2022;](#b22)[Albergo & Vanden-Eijnden, 2023)](#) in high-resolution image generation. However, recent advances have shown that RF models like Flux can surpass SoTA DMs in text-to-image (T2I) generation tasks [(Esser et al., 2024)](#b10). Despite this, their inversion and editing capabilities remain underexplored. In this paper, we introduce an efficient RF inversion method that avoids the need for training additional parameters [(Hu et al., 2021;](#b15)[Ruiz et al., 2023)](#b39), optimizing latent variables [(Rout et al., 2024a)](#), prompt tuning [(Mokady et al., 2023)](#b25), or using complex attention processors [(Hertz et al., 2022)](#b12). While our focus is on inversion and editing, we also show that our framework can be easily extended to generative modeling.

## Filtering, Control and SDEs.

There is a rich literature on the connections between nonlinear filtering, optimal control and SDEs [(Fleming & Rishel, 1975;](#b11)[√òksendal, 2003;](#b27)[Tzen & Raginsky, 2019;](#b47)[Zhang & Chen, 2022](#b50)). These connections are grounded in the Fokker-Planck equation [(√òksendal, 2003)](#), which RF methods [(Lipman et al., 2022;](#b21)[Liu et al., 2022;](#b22)[Albergo & Vanden-Eijnden, 2023;](#)[Albergo et al., 2023)](#) heavily exploit in sampling. Our study focuses on rectified flows for conditional sampling, and shows that the resulting drift field also has an optimal control interpretation.

## METHOD

3.1 PRELIMINARIES In generative modeling, the goal is to sample from a target distribution p 0 given a finite number of samples from that distribution. Rectified flows [(Lipman et al., 2022;](#b21)[Liu et al., 2022)](#b22) represent a class of generative models that construct a source distribution q 0 and a time varying vector field v t (x t ) to sample p 0 using an ODE:

$dX t = v t (X t )dt, X 0 ‚àº q 0 , t ‚àà [0, 1].(1)$Starting from X 0 = x 0 , the ODE (1) is integrated from t : 0 ‚Üí 1 to yield a sample x 1 distributed according to p 0 (e.g., the distribution over images). A common choice of q 0 is standard Gaussian N (0, I) and v t (X t ) = -u(X t , 1 -t; œÜ), where u is a neural network parameterized by œÜ. The neural network is trained using the conditional flow matching objective as discussed below.

Training Rectified Flows. To train a neural network to serve as the vector field for the ODE (1), we couple samples from p 0 with samples from q 0 -which we call p 1 to simplify the notation -via a linear path:

$Y t = tY 1 + (1 -t)Y 0 .$The resulting marginal distribution of Y t becomes:

$p t (y t ) = E Y1‚àºp1 [p t (y t |Y 1 )] = p t (y t |y 1 )p 1 (y 1 )dy 1 .(2)$Given an initial state Y 0 = y 0 and a terminal state Y 1 = y 1 , the linear path induces an ODE: dY t = u t (Y t |y 1 ) dt with the conditional vector field u t (Y t |y 1 ) = y 1 -y 0 . The marginal vector field is derived from the conditional vector field using the following relation [(Lipman et al., 2022)](#b21):

$u t (y t ) = E Y1‚àºp1 u t (y t |Y 1 ) p t (y t |Y 1 ) p t (y t ) = u t (y t |y 1 ) p t (y t |y 1 ) p t (y t ) p 1 (y 1 )dy 1 .(3)$We can then use a neural network u(y t , t; œÜ), parameterized by œÜ, to approximate the marginal vector field u t (y t ) through the flow matching objective defined as:

$L F M (œÜ) := E t‚àºU [0,1],Yt‚àºpt ‚à•u t (Y t ) -u(Y t , t; œÜ)‚à• 2 2 . (4$$)$For tractability, we can instead consider a different objective, called conditional flow matching:

$L CF M (œÜ) := E t‚àºU [0,1],Yt‚àºpt(‚Ä¢|Y1),Y1‚àºp1 ‚à•u t (Y t |Y 1 ) -u(Y t , t; œÜ)‚à• 2 2 .(5)$L CF M and L F M have the identical gradients [(Lipman et al., 2022, Theorem 2)](#), and are hence equivalent. However, L CF M (œÜ) is computationally tractable, unlike L F M (œÜ), and therefore preferred during training. Finally, the required vector field in (1) is computed as v t (X t ) = -u(X t , 1 -t; œÜ). In this way, rectified flows sample a data distribution by an ODE with a learned vector field.

## CONNECTION BETWEEN RECTIFIED FLOWS AND LINEAR QUADRATIC REGULATOR

The unconditional rectified flows (RFs) (e.g., Flux) from Section ¬ß3.1 above, enable image generation by simulating the vector field v t (‚Ä¢) initialized with a sample of random noise. Subsequently, by simulating the reversed vector field -v 1-t (‚Ä¢) starting from the image, we get back the sample of noise that we started with. We formalize this statement below. Proposition 3.1. Given an image y 0 and the vector field v t (‚Ä¢) of the generative ODE (1), suppose the structured noise y 1 is obtained by simulating an ODE:

$dY t = u t (Y t )dt, Y 0 = y 0 , t ‚àà [0, 1]. (6$$)$$If u t (‚Ä¢) = -v 1-t (‚Ä¢)$and X 0 = y 1 , then the ODE (1) recovers the original image, i.e., X 1 = y 0 .

Implication. Rectified flows enable exact inversion of a given image when the vector field of the generative ODE (1) is precisely known. Employing ODE (6) for the structured noise and ODE (1) to transform that noise back into an image, RF inversion accurately recovers the given image.

Suppose instead that we start with a corrupted image and simulate the reversed vector field -v 1-t (‚Ä¢).

Then we obtain a noise sample. There are two salient aspects of this noise sample. First, it is consistent with the original image: when processed through v t (‚Ä¢) it results in the same corrupted image. Second, if the image sample is "atypical" (e.g., corrupted, or, say, a stroke painting as in ¬ß5), then the sample of noise is also likely to be atypical. In other words, the noise sample is only consistent to the (possibly corrupted) image sample.

Our goal is to modify the pipeline above so that even when we start with a corrupted image, we can get back a clean image (see stroke-to-image synthesis in Figure [5](#)), but for this, we need to processs by v t (‚Ä¢) a noise sample that is closer to being "typical". More generally, the goal is to create a pipeline that supports semantic editing of real images ( ¬ß5), e.g., changing age, or gender without relying on additional training, optimization, or complex attention processors.

Thus, as a first step, we derive an optimal controller that takes a minimum energy path to convert any image Y 0 (whether corrupted or not) to a given sample of random noise Y 1 ‚àº p 1 -i.e., noise that is typical for p 1 . Specifically, we consider optimal control in a d-dimensional vector space R d :

$V (c) := 1 0 1 2 ‚à•c (Z t , t)‚à• 2 2 dt + Œª 2 ‚à•Z 1 -Y 1 ‚à• 2 2 , dZ t = c (Z t , t) dt, Z 0 = y 0 , Y 1 ‚àº p 1 , (7$$)$where Œª is the weight assigned to the terminal cost and V (c) denotes the total cost of the control c :

$R d √ó [0, 1] ‚Üí R d .$The minimization of V (c) over the admissible set of controls, denoted by C, is known as the Linear Quadratic Regulator (LQR) problem. The solution of the LQR problem ( [7](#formula_10)) is given in Proposition 3.2, which minimizes the quadratic transport cost of the dynamical system. Proposition 3.2. For Z 0 = y 0 and Y 1 = y 1 , the optimal controller of the LQR problem ( [7](#formula_10)), denoted by c * (‚Ä¢, t) is equal to the conditional vector field u t (‚Ä¢|y 1 ) of the rectified linear path

$Y t = tY 1 + (1 -t)Y 0 when Y 0 = y 0 , i.e., c * (z t , t) = u t (z t |y 1 ) = (y 1 -z t )/(1 -t).$
## INVERTING RECTIFIED FLOWS WITH DYNAMIC CONTROL

So far, we have two vector fields. The first, from the RFs, transforms an image Y 0 typical for distribution p 0 to a typical sample of random Gaussian noise Y 1 ‚àº p 1 . As discussed above, if the image sample is atypical, then the sample of noise is also likely to be atypical.

We also have a second vector field resulting from the optimal control formulation that transforms any image (whether corrupted or not) to a noise sample that is typical-by-design from the distribution p 1 . Therefore, this sample, when passed through the rectified flow ODE (1) results in a "typical" image from the "true" distribution p 0 . This image is clean, i.e., typical for p 0 , but it is not related to the image Y 0 . Our controlled ODE, defined below, interpolates between these two differing objectives consistency with the given (possibly corrupted) image, and consistency with the distribution of images p 0 -with a tunable parameter Œ≥:

$dY t = u t (Y t ) + Œ≥ (u t (Y t |y 1 ) -u t (Y t )) dt, Y 0 = y 0 ,(8)$where

$u t (Y t |y 1 ) = c * (Y t , t) is computed based on the insights from Proposition 3.2, and u t (Y t ) = -v 1-t (Y t ) as established in Proposition 3.1.$Here, we call Œ≥ ‚àà [0, 1] the controller guidance. Thus, ODE ( [8](#formula_14)) generalizes ( [6](#formula_7)) to editing applications, while keeping its inversion accuracy comparable.

When Œ≥ = 1, the drift field of the ODE (8) becomes optimal controller of LQR problem (7), ensuring that the structured noise Y 1 = y 1 adheres to the distribution p 1 . Consequently, initializing the generative ODE (1) with y 1 results in samples with high likelihood under the data distribution p 0 .

Conversely, when Œ≥ = 0, the system follows the ODE (6) described in Proposition 3.1, resulting a structured noise Y 1 that is not guaranteed to follow the noise distribution p 1 . However, initializing the generative ODE (1) with this noise precisely recovers the reference image y 0 .

Beyond this vector field interpolation intuition, we show in the next section ¬ß3.4 that the controlled ODE ( [8](#formula_14)) has an SDE interpretation. As is well known [(Ho et al., 2020;](#b14)[Song et al., 2021a;](#)[Meng et al., 2022;](#b23)[Song et al., 2021b)](#), SDEs are robust to initial conditions, in proportion to the variance of the additive noise. Specifically, errors propagate over time in an ODE initialized with an incorrect or corrupted sample. However, SDEs (Markov processes) under appropriate conditions converge to samples from a carefully constructed invariant distribution with reduced sensitivity to the initial condition, resulting in a form of robustness to initialization. As we see, the parameter Œ≥ (the controller Preprint.

guidance) appears in the noise term to the SDE, thus the SDE analysis in the next section again provides intuition on the trade-off between consistency to the (corrupted) image and consistency to the terminal invariant distribution. Remark 3.3. We note that our analysis extends to the case where Œ≥ is time-varying, though we omit these results for simplicity of notation. This is useful in practice, especially when y 0 is a corrupted image, because for large Œ≥ the stochastic evolution ( [22](#formula_34)) moves toward a sample from the invariant measure N (0, I). This noise encodes clean images. Starting from this noise, the corresponding reverse process operates in pure diffusion mode, resulting in a clean image. As the process approaches the terminal state, Œ≥ is gradually reduced to ensure that y 0 is encoded through u t (‚Ä¢) into the final structured noise sample.

## CONTROLLED RECTIFIED FLOWS AS STOCHASTIC DIFFERENTIAL EQUATIONS

An SDE [(Ho et al., 2020)](#b14) is known to have an equivalent ODE formulation [(Song et al., 2021a)](#) under certain regularity conditions [(Anderson, 1982;](#b2)[Song et al., 2021b)](#). In this section, we derive the opposite: an SDE formulation for our controlled ODE (8) from ¬ß3.3. Let W t be a d-dimensional Brownian motion in a filtered probability space (‚Ñ¶, F, {F t }, P). Theorem 3.4. Fix any T ‚àà (0, 1). For any t ‚àà [0, T ], the controlled ODE ( [8](#formula_14)) is explicitly given by:

$dY t = - 1 1 -t (Y t -Œ≥y 1 ) - (1 -Œ≥)t 1 -t ‚àá log p t (Y t ) dt, Y 0 ‚àº p 0 . (9$$)$Its density evolution is identical to the density evolution of the following SDE:

$dY t = - 1 1 -t (Y t -Œ≥y 1 ) dt + 2(1 -Œ≥)t 1 -t dW t , Y 0 ‚àº p 0 .(10)$Finally, denoting p t (‚Ä¢) as the marginal pdf of Y t , the density evolution is explicitly given by:

$‚àÇp t (Y t ) ‚àÇt = ‚àá ‚Ä¢ 1 1 -t (Y t -Œ≥y 1 ) + (1 -Œ≥)t 1 -t ‚àá log p t (Y t ) p t (Y t ) . (11$$)$Properties of SDE (10). Elaborating on the intuition discussed at the end of ¬ß3.3, when the controller guidance parameter Œ≥ = 0, it becomes the stochastic equivalent of the standard RFs; see Lemma A.2 for a precise statement. The resulting SDE is given by

$dY t = - 1 1 -t Y t dt + 2t 1 -t dW t , Y 0 ‚àº p 0 ,(12)$which improves faithfulness to the image Y 0 . When Œ≥ = 1, the SDE (10) solves the LQR problem (7) and drives towards the terminal state Y 1 = y 1 . This improves the generation quality, because the sample Y 1 is from the correct noise distribution p 1 as previously discussed in ¬ß3.3. Therefore, a suitable choice of Œ≥ retains faithfulness while simultaneously applying the desired edits.

Finally, we assume T = 1 -Œ¥ for sufficiently small Œ¥ (such that 0 < Œ¥ ‚â™ 1) to avoid irregularities at the boundary. This is typically considered in practice for numerical stability (even for diffusion models). Thus, in practice, the final sample y 1-Œ¥ is returned as y 1 .

Comparison with DMs. Analogous to the SDE (12), the stochastic noising process of DMs is typically modeled by the Ornstein-Uhlenbeck (OU) process, governed by the following SDE:

$dY t = -Y t dt + ‚àö 2dW t .(13)$The corresponding ODE formulation is given by:

$dY t = [-Y t -‚àá log p t (Y t )] dt.(14)$Instead, our approach is based on rectified flows (1), which leads to a different ODE and consequently translates into a different SDE. As an additional result, we formalize the ODE derivation in Lemma A.1. In Lemma A.2, we show that the marginal distribution of this ODE is equal to that of an SDE with appropriate drift and diffusion terms. In Proposition A.3, we show that the stationary distribution of this new SDE (12) converges to the standard Gaussian N (0, I) in the limit as t ‚Üí 1.

Preprint.

The standard OU process (13) interpolates between the data distribution at time t = 0 and a standard Gaussian as t ‚Üí ‚àû. The SDE (12), however, interpolates between the data distribution at time t = 0 and a standard Gaussian at t = 1. In other words, it effectively "accelerates" time as it progresses to achieve the terminal Gaussian distribution. This is accomplished by modifying the coefficients of drift and diffusion as in (12) to depend explicitly on time t. Thus, a sample path of ( [12](#formula_21)) appears like a noisy line, unlike that of the OU process (see Appendix C.3 for numerical simulations).

## CONTROLLED REVERSE FLOW USING RECTIFIED ODES AND SDES

In this section, we develop an ODE and an SDE similar to our discussions above, but for the reverse direction (i.e., from noise to images).

Reverse process using ODE. Starting from the structured noise y 1 obtained by integrating the controlled ODE (8), we construct another controlled ODE (15) for the reverse process (i.e., noise to image). In this process, the optimal controller uses the reference image y 0 for guidance:

$dX t = v t (X t ) + Œ∑ (v t (X t |y 0 ) -v t (X t )) dt, X 0 = y 1 , t ‚àà [0, 1],(15)$where Œ∑ ‚àà [0, 1] is the controller guidance parameter as before that controls faithfulness and editability of the given image y 0 . Similar to the analysis in Proposition 3.2, v t (X t |y 0 ) is obtained by solving the modified LQR problem ( [16](#formula_25)):

$V (c) = 1 0 1 2 ‚à•c (Z t , t)‚à• 2 2 dt + Œª 2 ‚à•Z 1 -y 0 ‚à• 2 2 , dZ t = c (Z t , t) dt, Z 0 = y 1 .(16)$Solving ( [16](#formula_25)), we get c(Z t , t) = y0-Zt 1-t . Our controller steers the samples toward the given image y 0 . Thus, the controlled reverse ODE (15) effectively reduces the reconstruction error incurred in the standard reverse ODE (1) of RF models (e.g. Flux).

Reverse process using SDE. Finally, in Theorem 3.5, we provide the stochastic equivalent of our controlled reverse ODE (15) for generation. Recall that we initialize with the terminal structured noise by running the controlled forward ODE (8), along with a reference image y 0 . As discussed above, we terminate the inversion process at a time T = 1 -Œ¥ for numerical stability, resulting in a vector y 1-Œ¥ . Our reverse SDE thus starts at a corresponding time Œ¥ with this vector y 1-Œ¥ at initialization, and terminates at time T ‚Ä≤ < 1. Theorem 3.5. Fix any T ‚Ä≤ ‚àà (Œ¥, 1), and for any t ‚àà [Œ¥, T ‚Ä≤ ], the density evolution of the controlled ODE (15) initialized at X 0 = y 1-Œ¥ is identical to the density evolution of the following SDE:

$dX t = (1 -t -Œ∑)X t + Œ∑ty 0 t(1 -t) + 2(1 -t)(1 -Œ∑) t ‚àá log p 1-t (X t ) dt + 2(1 -t)(1 -Œ∑) t dW t .(17)$Furthermore, denoting q t (‚Ä¢) as the marginal pdf of X t , its density evolution is given by:

$‚àÇq t (X t ) ‚àÇt = ‚àá ‚Ä¢ - 1 -t -Œ∑ t(1 -t) X t + Œ∑ 1 -t y 0 + (1 -t) t (1 -Œ∑)‚àá log p 1-t (X t ) q t (Y t ) . (18$$)$Properties of SDE (17). When the controller parameter Œ∑ = 0, we obtain a stochastic sampler (22) for the pre-trained Flux, as given in Lemma A.4 and compared qualitatively in Figure [24](#fig_0). This case of our SDE (17) corresponds to the stochastic variant of standard RFs [(Liu et al., 2022;](#b22)[Lipman et al., 2022;](#b21)[Albergo & Vanden-Eijnden, 2023)](#). Our key contribution lies in conditioning on X 1 = y 0 for inverting rectified flows. Importantly, our explicit construction does not require additional training or test-time optimization, enabling for the first time an efficient sampler for zeroshot inversion and editing using Flux. When Œ∑ = 1, the score term and Brownian motion vanish from the SDE (17). The resulting drift becomes y0-Xt 1-t , the optimal controller for the LQR problem ( [16](#formula_25)), exactly recovering the given image y 0 . Remark 3.6. Similar to Remark 3.3, our analysis extends to the case when Œ∑ is time-varying. This is useful in editing, as it allows the flow to initially move toward the given image y 0 by choosing a large Œ∑. As the flow approaches y 0 on the image manifold, Œ∑ is gradually reduced, ensuring that the text-guided edits are enforced through the unconditional vector field v t (‚Ä¢) provided by Flux. 

## ALGORITHM: INVERSION AND EDITING VIA CONTROLLED ODES

We describe the algorithm for RF inversion and editing using our controlled ODEs ( [8](#formula_14)) and ( [15](#formula_24)).

Problem Setup. The user provides a text "prompt" to edit reference content, which could be a corrupt or a clean image. For the corrupt image guide, we use the dataset from SDEdit [(Meng et al., 2022)](#b23), which contains color strokes to convey high-level details. In this setting, the reference guide y 0 is typically not a realistic image under the data distribution p 0 . The objective is to transform this guide into a more realistic image under p 0 while maintaining faithfulness to the original guide.

For the clean image guide, the user provides a real image y 0 along with an accompanying text "prompt" to specify the desired edits. The task is to apply text-guided edits to y 0 while preserving its content. Examples include face editing, where the text might instruct change in age or gender.

Procedure. Our algorithm has two key steps: inversion and editing. We discuss each step below. During the inversion process, we use null prompt in the Flux model, i.e., u t (y t ) = u(y t , t, Œ¶(""); œÜ). For the conditional vector field, we apply the analytical solution derived in Proposition 3.2. The inversion process yields a latent variable that is then used to initialize our controlled ODE (15), i.e., X 0 = y 1 . In this phase, we again use the null prompt to compute the vector field v t (x t ) = -u(x t , 1 -t, Œ¶(""); œÜ): see Figure [3](#fig_1) for the final output.

## Original RF inversion

Editing. The second step involves text-guided editing of the reference content y 0 . This process is governed by our controlled ODE (15), where the vector field is computed using the desired text prompt within Flux: v t (X t ) = -u(x t , 1 -t, Œ¶(prompt); œÜ). The controller guidance Œ∑ in (15) balances faithfulness and editability: higher Œ∑ improves faithfulness but limits editability, while lower Œ∑ allows significant edits at the cost of reduced faithfulness. Consequently, the controller guidance Œ∑ provides a smooth interpolation between faithfulness and editability, a crucial feature in semantic image editing. Motivated by Remark 3.3 and 3.6, we consider a time-varying controller guidance Œ∑ t , such that for a fixed Œ∑ ‚àà [0, 1] and œÑ ‚àà [0, 1], Œ∑ t = Œ∑ ‚àÄt ‚â§ œÑ and 0 otherwise. Figure [4](#fig_0) illustrates the effect of controller guidance Œ∑ for œÑ = 0.3; see Appendix C.2 for a detailed ablation study.

## EXPERIMENTAL EVALUATION

We show that RF inversion outperforms DM inversion across three benchmarks: LSUN-church, LSUN-bedroom [(Wang et al., 2017)](#b48), and SFHQ [(Beniaguev, 2022)](#b4) on two tasks: Stroke2Image generation and semantic image editing. Stroke2Image generation shows the robustness of our algorithm to initial corruption. In semantic image editing, we emphasize the ability to edit clean images without additional training, optimization, or complex attention processors.

Baselines. As this paper focuses on inverting flows, we compare with SoTA inversion approaches, such as NTI [(Mokady et al., 2023)](#b25), DDIM Inversion [(Song et al., 2021a)](#), and SDEdit [(Meng et al., 2022)](#b23). We use the official NTI implementation for both NTI and DDIM inversion, and Diffusers Stroke2Image generation. As discussed in ¬ß4, our goal is to generate a photo-realistic image from a stroke paint (a corrupted image) and the text prompt "photo-realistic picture of a bedroom". In this case, the high level details in the stroke painting guide the reverse process toward a clean image.

In Figure [5](#), we compare RF inversion (ours) with DM inversions. DM inversions propagate the corruption from the stroke painting into the structured noise, which leads to outputs resembling the input stroke painting. NTI optimizes null embeddings to align the reverse process with the DDIM forward trajectory. Although adding P2P to the NTI pipeline helps localized editing as in Figure [6](#), for corrupted images, it drives the reverse process even closer to the corruption. In contrast, our controlled ODE (8) yields a structured noise that is consistent with the corrupted image and also the invariant terminal distribution, as discussed in ¬ß3.3, resulting in more realistic images.

In Table [1](#tab_1), we show that our method outperforms prior works in faithfulness and realism. On the test split of LSUN bedroom dataset, our approach is 4.7% more faithful and 13.79% more realistic than the best optimization free method SDEdit-SD1.5. Ours is 73% more realistic than the optimizationbased method NTI, but comparable in L2. As discussed, NTI+P2P gets closer toward the corrupt image, which gives a very low L2 error, but the resulting image becomes unrealistic. Our approach is 89% more realistic than NTI+P2P. We observe similar gains on LSUN church dataset.

User study. We conduct a user study using Amazon Mechanical Turk to evaluate the overall performance of the our method. With 3 responses for each question, we collected in total 9,000 compar-    [1](#tab_1), our method outperforms all the other baselines by at least 59.67% in terms of overall satisfaction. More details are provided in Appendix ¬ßC.6.

Semantic Image Editing. Given a clean image and a text "prompt", the objective is to modify the image according to the given text while preserving the contents of the image (identity for face images). In rectified linear paths, editing from a noisy latent becomes straightforward, further enhancing the efficiency of our approach. Compared with SoTA approaches (Figure [6](#)), our method requires no additional optimization or complex attention processors as in NTI [(Mokady et al., 2023)](#b25)+P2P [(Hertz et al., 2022)](#b12). Thus, it is more efficient than a current SoTA approach, and importantly, more faithful to the original image while applying the desired edits. In Table [2](#tab_2), we show that our method outperforms the optimization-free methods by at least 29% in face reconstruction, 6.6% in DINO patch-wise similarity, and 26.4% in CLIP-Image similarity while being comparable in prompt alignment metric CLIP-T. Importantly, our approach offers 54.11% gain in runtime, though it uses a larger (‚àº12X) model, while staying comparable to NTI+P2P.

## Original SDEdit (Flux) Flux Inversion Ours

In Figure [7](#fig_4), we showcase four complex editing tasks: (a) prompt-based stylization with the prompt: "face of a boy in disney 3d cartoon style", where facial expressions, such as "laugh" or "angry" are used for editing; (b) ability to control the age of a person; (c) interpolating between two concepts: "A man" ‚Üî "A woman"; (d) sequentially inserting pepperoni and mushroom to an image of a pizza. We provide more examples of editing in the wild in Appendix ¬ßC.

Comparison using the same backbone: Flux. In Figure [8](#fig_5), we compare our method with SDEdit and DDIM inversion both adapted to Flux. NTI optimizes null embeddings to align with forward latents before applying text-guided edits via P2P, an approach well-suited for DMs that use both Preprint. null and text embedding. However, this strategy cannot be applied to Flux, as it does not explicitly use null embedding. Consequently, we only reimplement SDEdit and DDIM inversion for Flux and compare them to our method. Since all methods leverage the same generative model, the improvements clearly stem from our controlled ODEs, grounded in a solid theoretical foundation ( ¬ß3).

## CONCLUSION

We present the first efficient approach for inversion and editing with the state-of-art rectified flow models such as Flux. Our method interpolates between two vector fields: (i) the unconditional RF field that transforms a "clean" image to "typical" noise, and (ii) a conditional vector field derived from optimal control that transforms any image (clean or not) to "typical" noise. Our new field thus navigates between these two competing objectives of consistency with the given (possibly corrupted) image, and consistency with the distribution of clean images. Theoretically, we show that this is equivalent to a new rectified SDE formulation, sharing this intuition of interpolation. Practically, we show that our method results in state-of-art zero-shot performance, without the need of additional training, optimization of latent variables, prompt tuning, or complex attention processors.

We demonstrate the effectiveness of our method in stroke-to-image synthesis, face editing, object insertion, and stylization tasks, with large-scale human evaluation confirming user preference.

Limitation. The lack of comparison with expensive diffusion-based editing solutions may be viewed as a limitation. However, these implementations are either not available for Flux or not directly applicable due to Flux's distinct multi-modal architecture. The key contribution of this paper lies in its theoretical foundations, validated using standard benchmarks and relevant baselines.

Reproducibility. The pseudocode and hyper-parameter details have been provided to reproduce the reported results in this paper.

## BROADER IMPACT STATEMENT

Semantic image inversion and editing have both positive and negative social impacts.

On the positive side, this technology enables (i) the generation of photo-realistic images from high level descriptions, such as stroke paintings, and (ii) the modification of clean images by changing various attributes like the age, gender, or adding glasses ( ¬ß5).

On the negative side, it can be misused by malicious users to manipulate photographs of individuals with inappropriate or offensive edits. Additionally, it carries the inherent risks associated with the underlying generative model.

To mitigate the negative social impacts, we enable safety features such as NSFW filters in the underlying generative model. Furthermore, we believe watermarking images generated by this technology can reduce misuse, especially in inversion and editing applications.

## A ADDITIONAL THEORETICAL RESULTS

In this section, we present the theoretical results omitted from the main draft due to space constraints. We formalize the ODE derivation of the standard rectified flows in Lemma A.1. Lemma A.1. Given a coupling (Y 0 , Y 1 ) ‚àº p 0 √ó p 1 , consider the noising process

$Y t = tY 1 + (1 - t)Y 0 .$Then, the rectified flow ODE formulation with the optimal vector field is given by

$dY t = - 1 1 -t Y t - t 1 -t ‚àá log p t (Y t ) dt, Y 0 ‚àº p 0 . (19$$)$Furthermore, denoting p t (‚Ä¢) as the marginal pdf of Y t , its density evolution is given by:

$‚àÇp t (Y t ) ‚àÇt = ‚àá ‚Ä¢ 1 1 -t Y t + t 1 -t ‚àá log p t (Y t ) p t (Y t ) .(20)$In Lemma A.2, we show that the marginal distribution of the rectified flow ( [6](#formula_7)) is equal to that of an SDE with appropriate drift and diffusion terms. Lemma A.2. Fix any T ‚àà (0, 1), and for any t ‚àà [0, T ], the density evolution (20) of the rectified flow model ( [19](#formula_30)) is identical to the density evolution of the following SDE:

$dY t = - 1 1 -t Y t dt + 2t 1 -t dW t , Y 0 ‚àº p 0 .(21)$In Proposition A.3, we show that the stationary distribution of the SDE ( [21](#formula_33)) converges to the standard Gaussian N (0, I) in the limit as t ‚Üí 1. Proposition A.3. Fix any T ‚àà (0, 1), and for any t ‚àà [0, T ], the density evolution for the rectified flow ODE ( [6](#formula_7)) is same as that of the SDE (12). Furthermore, denoting p t (‚Ä¢) as the marginal pdf of Y t , its stationary distribution p t (Y t ) ‚àù exp (-‚à•Yt‚à• 2 2t ), which converges to N (0, I) as t ‚Üí 1. We note that Lemma A.1 and Lemma A.2 follow from the duality between the heat equation and the continuity equation [(√òksendal, 2003)](#), where it is classically known that one can interpret a diffusive term as a vector field that is affine in the score function, and vice-versa. This connection has been carefully used to study a large family of stochastic interpolants (that generalize rectified flows) in [(Albergo & Vanden-Eijnden, 2023;](#)[Albergo et al., 2023)](#), and which can lead to a family of ODE-SDE pairs. In the lemmas above, we have provided explicit coefficients that have been directly derived, instead of using the stochastic interpolant formulation. Our key contribution lies in constructing a controlled ODEs ( [8](#formula_14)) and ( [15](#formula_24)), along with their equivalent SDEs ( [10](#formula_18)) and ( [17](#formula_26)) in Theorem 3.4 and Theorem 3.5, respectively. This aids faithfulness and editability as discussed in ¬ß4.

In Lemma A.4, we derive a rectified SDE that transforms noise into images by reversing the stochastic equivalent of rectified flows (12). Lemma A.4. Fix any small Œ¥ ‚àà (0, 1), and for any t ‚àà [Œ¥, 1], the process X t governed by the SDE:

$dX t = 1 t X t + 2(1 -t) t ‚àá log p 1-t (X t ) dt + 2(1 -t) t dW t , X 0 ‚àº p 1 ,(22)$is the time-reversal of the SDE (12).

Implication. The reverse SDE (22) provides a stochastic sampler for SoTA rectified flow models like Flux. Unlike diffusion-based generative models that explicitly model the score function ‚àá log p t (‚Ä¢) in ( [22](#formula_34)), rectified flows model a vector field, as discussed in ¬ß3.1. However, given a neural network u(y t , t; œÜ)) approximating the vector field u t (y t ), Lemma A.1 offers an explicit formula for computing the score function:

$‚àá log p t (Y t ) = - 1 t Y t - 1 -t t u(Y t , t; œÜ).(23)$This score function is used to compute the drift and diffusion coefficients of the SDE ( [22](#formula_34)), resulting in a practically implementable stochastic sampler for Flux. This extends the applicability of Flux to downstream tasks where SDE-based samplers have demonstrated practical benefits, as seen in diffusion models [(Ho et al., 2020;](#b14)[Song et al., 2021b;](#)[Rombach et al., 2022;](#b34)[Podell et al., 2023)](#b30).

Preprint.

Now, the controlled ODE (8) becomes:

$dY t = u t (Y t ) + Œ≥ (u t (Y t |Y 1 ) -u t (Y t )) dt, Y 0 ‚àº p 0 , Y 1 = y 1 = (1 -Œ≥) - 1 1 -t Y t - t 1 -t ‚àá log p t (Y t ) + Œ≥ Y 1 -Y t 1 -t dt = - 1 1 -t Y t - t 1 -t (1 -Œ≥)‚àá log p t (Y t ) + Œ≥ 1 -t Y 1 dt = - 1 1 -t (Y t -Œ≥Y 1 ) - t 1 -t (1 -Œ≥)‚àá log p t (Y t ) dt.$Using continuity equation [(√òksendal, 2003)](#), the density evolution of the controlled ODE (8) then becomes:

$‚àÇp t (Y t ) ‚àÇt = ‚àá ‚Ä¢ 1 1 -t (Y t -Œ≥Y 1 ) + t 1 -t (1 -Œ≥)‚àá log p t (Y t ) p t (Y t ) .(31)$Applying Fokker-Planck equation [(√òksendal, 2003)](#) to the SDE (10), we have

$‚àÇp t (Y t ) ‚àÇt + ‚àá ‚Ä¢ - 1 1 -t (Y t -Œ≥Y 1 ) p t (Y t ) = ‚àá ‚Ä¢ t 1 -t (1 -Œ≥)‚àáp t (Y t ) ,$which can be rearranged to equal (31) completing the proof.

## B.4 PROOF OF LEMMA A.1

Proof. Given (Y 0 , Y 1 ) ‚àº p 0 √ó p 1 , the conditional flow matching loss (5) can be reparameterized as:

$L CF M (œÜ) := E t‚àºU [0,1],(Y0,Y1)‚àºp1√óp0 ‚à•(Y 1 -Y 0 ) -u(Y t , t; œÜ)‚à• 2 2 , Y t = tY 1 + (1 -t)Y 0 ,$where the optimal solution is given by the minimum mean squared estimator:

$u t (y t ) = E (Y0,Y1)‚àºp1√óp0 [Y 1 -Y 0 |Y t = y t ] .(32)$Since Y t = tY 1 + (1 -t)Y 0 , we use Tweedie's formula [(Efron, 2011)](#b9) to compute

$E [Y 0 |Y t = y t ] = 1 1 -t y t + t 2 1 -t ‚àá log p t (y t ).(33)$Using the above relation, we obtain the following:

$E [Y 1 |Y t = y t ] = 1 t E [Y t -(1 -t)Y 0 |Y t = y t ] = 1 t (y t -(1 -t)E [Y 0 |Y t = y t ]) = 1 t y t -(1 -t) 1 1 -t y t + t 2 1 -t ‚àá log p t (y t ) = -t ‚àá log p t (y t ).(34)$Combining ( [33](#formula_41)) and (34) using linearity of expectation, we get

$u t (y t ) = E [Y 1 |Y t = y t ] -E [Y 0 |Y t = y t ] (35) = -t ‚àá log p t (y t ) - 1 1 -t y t - t 2 1 -t ‚àá log p t (y t ) (36) = - 1 1 -t y t - t 1 -t ‚àá log p t (y t ),(37)$The density evolution of Y t now immediately follows from the continuity equation [(√òksendal, 2003)](#) applied to (19).

Preprint.

## B.5 PROOF OF LEMMA A.2

Proof. The Fokker-Planck equation of the SDE ( [12](#formula_21)) is given by

$‚àÇp t (Y t ) ‚àÇt + ‚àá ‚Ä¢ - 1 1 -t Y t p t (Y t ) = ‚àá ‚Ä¢ t 1 -t ‚àáp t (Y t ) .(38)$Rearranging ( [38](#formula_44)) by multiplying and dividing p t (Y t ) in the right hand side, we get

$‚àÇp t (Y t ) ‚àÇt = ‚àá ‚Ä¢ 1 1 -t Y t + t 1 -t ‚àá log p t (Y t ) p t (Y t ) .(39)$To conclude, observe that that the density evolution above is identical to (20).

## B.6 PROOF OF PROPOSITION A.3

Proof. The optimal vector field of the rectified flow ODE ( [6](#formula_7)) is given by Lemma A.1. The proof then immediately follows from the Fokker-Planck equations in Lemma A.1 and Lemma A.2.

From Lemma A.2, the density evolution of the SDE ( [12](#formula_21)) is given by

$‚àÇp t (Y t ) ‚àÇt = ‚àá ‚Ä¢ 1 1 -t Y t + t 1 -t ‚àá log p t (Y t ) p t (Y t ) .$The stationary (or steady state) distribution satisfies the following:

$‚àÇp t (Y t ) ‚àÇt = 0 = ‚àá ‚Ä¢ 1 1 -t Y t + t 1 -t ‚àá log p t (Y t ) p t (Y t ) .$Using the boundary conditions [(√òksendal, 2003)](#), we get

$1 1 -t Y t + t 1 -t ‚àá log p t (Y t ) = 0, which immediately implies p t (Y t ) ‚àù e -‚à•Y t ‚à• 2 2t .$B.7 PROOF OF THEOREM 3.5

Proof. Using Fokker-Planck equation [(√òksendal, 2003)](#), Lemma A.4 implies

$‚àÇq t (X t ) ‚àÇt = ‚àá ‚Ä¢ -q t (X t ) 1 t X t + 1 -t t ‚àá log q t (X t ) .$Therefore, the optimal vector field v t (X t ) of the controlled ODE ( [15](#formula_24)) is given by

$v t (X t ) = 1 t X t + 1 -t t ‚àá log p 1-t (X t ).(40)$The LQR problem ( [16](#formula_25)) is identical to the LQR problem (7) with changes in the initial and terminal states. Similar to Proposition 3.2, we compute the closed-form solution for the conditional vector field of the ODE (15) as:

$v t (X t |X 1 ) = X 1 -X t 1 -t .(41)$Combining ( [40](#formula_50)) and (41), we have

$dX t = [v t (X t ) + Œ∑(v t (X t |X 1 ) -v t (X t ))] dt = (1 -Œ∑) 1 t X t + 1 -t t ‚àá log p 1-t (X t ) + Œ∑ X 1 -X t 1 -t dt = (1 -Œ∑)(1 -t) -Œ∑t t(1 -t) X t + Œ∑ 1 -t X 1 + (1 -Œ∑)(1 -t) t ‚àá log p 1-t (X t ) dt = 1 -t -Œ∑ t(1 -t) X t + Œ∑ 1 -t X 1 + (1 -Œ∑)(1 -t) t ‚àá log p 1-t (X t ) dt.$Preprint.

The resulting continuity equation [(√òksendal, 2003)](#) becomes:

$‚àÇq t (X t ) ‚àÇt = ‚àá ‚Ä¢ - 1 -t -Œ∑ t(1 -t) X t + Œ∑ 1 -t X 1 + (1 -Œ∑)(1 -t) t ‚àá log p 1-t (X t ) q t (X t ) = ‚àá ‚Ä¢ - 1 -t -Œ∑ t(1 -t) X t + Œ∑ 1 -t X 1 + 2(1 -Œ∑)(1 -t) t ‚àá log p 1-t (X t ) q t (X t ) + (1 -Œ∑)(1 -t) t ‚àá log p 1-t (X t ) q t (X t ) .$Using time-reversal property from Propsition 3.2, the above expression simplifies to

$‚àÇq t (X t ) ‚àÇt + ‚àá ‚Ä¢ 1 -t -Œ∑ t(1 -t) X t + Œ∑ 1 -t X 1 + 2(1 -Œ∑)(1 -t) t ‚àá log p 1-t (X t ) q t (X t ) = ‚àá ‚Ä¢ (1 -Œ∑)(1 -t) t ‚àáq t (X t ) ,$which yields the following SDE:

$dX t = 1 -t -Œ∑ t(1 -t) X t + Œ∑ 1 -t X 1 + 2(1 -Œ∑)(1 -t) t ‚àá log p 1-t (X t ) dt + 2(1 -Œ∑)(1 -t) t dW t ,$and thus, completes the proof.

B.8 PROOF OF LEMMA A.4

Proof. It suffices to show that the Fokker-Planck equations of the SDE ( [22](#formula_34)) and ( [12](#formula_21)) are the same after time-reversal. Let q t (‚Ä¢) denote the marginal pdf of X t such that q 0 (‚Ä¢) = p 1 (‚Ä¢). The Fokker-Planck equations of the SDE (22) becomes

$‚àÇq t (X t ) ‚àÇt + ‚àá ‚Ä¢ q t (X t ) 1 t X t + 2(1 -t) t ‚àá log p 1-t (X t ) = ‚àá ‚Ä¢ 1 -t t ‚àáq t (X t ) ,$which can be rearranged to give

$‚àÇq t (X t ) ‚àÇt = ‚àá ‚Ä¢ -q t (X t ) 1 t X t + 2(1 -t) t ‚àá log p 1-t (X t ) + 1 -t t ‚àáq t (X t ) = ‚àá ‚Ä¢ -q t (X t ) 1 t X t + 2(1 -t) t ‚àá log p 1-t (X t ) - 1 -t t ‚àá log q t (X t )$Since Y t is the time-reversal process of X t as discussed in Proposition (3.1),

$‚àÇq t (X t ) ‚àÇt = ‚àá ‚Ä¢ -q t (X t ) 1 t X t + 1 -t t ‚àá log q t (X t ) . Substituting t ‚Üí 1 -t, ‚àÇq 1-t (X 1-t ) ‚àÇt = ‚àá ‚Ä¢ q 1-t (X 1-t ) 1 1 -t X 1-t + t 1 -t ‚àá log q 1-t (X 1-t ) ,$which implies the density evolution of (12):

$‚àÇp t (Y t ) ‚àÇt = ‚àá ‚Ä¢ p t (Y t ) 1 1 -t Y t + t 1 -t ‚àá log p t (Y t ) .$This completes the proof of the statement.

Preprint.

## C ADDITIONAL EXPERIMENTS

This section substantiates our contributions further by providing additional experimental details.

Baselines. We use the official NTI codebase[foot_0](#foot_0) for the implementations of NTI [(Mokady et al., 2023)](#b25), P2P [(Hertz et al., 2022)](#b12), and DDIM [(Song et al., 2021a)](#) inversion. We use the official Diffusers implementation[foot_1](#foot_1) for SDEdit and Flux[foot_2](#foot_2) . We modify the pipelines for SDEdit and DDIM inversion to adapt to the Flux backbone.

For completeness, we include qualitative comparison with a leading training-based approach In-structPix2Pix [(Brooks et al., 2023)](#b6)[foot_3](#foot_3) and a higher-order differential equation based LEDIT++ [(Brack et al., 2024)](#b5)[foot_4](#foot_4) ( ¬ßC). Table [3](#) summarizes the requirements of the compared baselines.

Table [3](#): Requirements of compared baselines. Our method outperforms prior works while requiring no additional training, optimization of prompt embedding, or attention manipulation scheme.

## Method Training Optimization Attention Manipulation

SDEdit [(Meng et al., 2022)](#b23) DDIM [(Song et al., 2021a](#)) NTI [(Mokady et al., 2023)](#b25) NTI+P2P [(Hertz et al., 2022)](#b12) LEDIT++ [(Brack et al., 2024)](#b5) InstructPix2Pix [(Brooks et al., 2023)](#b6) Ours

Metrics. Following SDEdit [(Meng et al., 2022)](#b23), we measure faithfulness using L2 loss between the stroke input and the output image, and assess realism using Kernel Inception Distance (KID) between real and generated images. Stroke inputs are generated from RGB images using the algorithm provided in SDEdit. Given the subjective nature of image editing, we conduct a large-scale user study to calculate the user preference metric.

For face editing, we evaluate identity preservation, prompt alignment, and overall image quality using a face recognition metric [(Ruiz et al., 2024)](#b40), CLIP-T scores [(Radford et al., 2021)](#b31), and using CLIP-I scores [(Radford et al., 2021)](#b31), respectively. For the face recognition score, we calculate the L2 distance between the face embedding of the original image and the edited image, obtained from Inception ResNet trained on CASIA-Webface dataset. Similar to SDEdit [(Meng et al., 2022)](#b23), we conduct extensive experiments on Stroke2Image generation, and showcase additional capabilities qualitatively on a wide variety of semantic image editing tasks.

Algorithm. The pseudo-code for getting the structured noise is provided in Algorithm 1, and transforming that noise back to an image is given in Algorithm 2.

## C.1 HYPER-PARAMETER CONFIGURATIONS

In Table [4](#tab_4), we provide the hyper-parameters for the empirical results reported in ¬ß5. We use a fix Œ≥ = 0.5 in our controlled forward ODE (8) and a time-varying guidance parameter Œ∑ t in our controlled reverse ODE (15), as motivated in Remark 3.3 and Remark 3.6. Thus, our algorithm introduces one additional hyper-parameter Œ∑ t into the Flux pipeline. For each experiment, we use a fixed time-varying schedule of Œ∑ t described by starting time (s), stopping time œÑ , and strength (Œ∑). We use the default config for Flux model: 3.5 for classifier-free guidance and 28 for the total number of inference steps.

Algorithm 1: Controlled Forward ODE (8) Input: Discretization steps N , reference image y 0 , prompt embedding network Œ¶, Flux model u(‚Ä¢, ‚Ä¢, ‚Ä¢; œÜ), Flux noise scheduler œÉ : [0, 1] ‚Üí R Tunable parameter: Controller guidance Œ≥ Output: Structured noise Y 1 1 Initialize Y 0 = y 0 2 Fix a noise sample y 1 3 for i = 0 to N -1 do 4 Current time step: t i = i N 5 Next time step: t i+1 = i+1 N 6 Unconditional vector field: u ti (Y ti ) = u(Y ti , t i , Œ¶(""); œÜ) ‚ñ∑ Proposition 3.1 7 Conditional vector field: u ti (Y ti |y 1 ) = y1-Yt i 1-ti ‚ñ∑ Proposition 3.2 8 Controlled vector field: √ªti (Y ti ) u ti (Y ti ) + Œ≥ (u ti (Y ti |y 1 ) -u ti (Y ti )) ‚ñ∑ODE (8) 9 Next state: Y ti+1 = Y ti + √ªti (Y ti ) (œÉ(t i+1 ) -œÉ(t i )) 10 end 11 return Y 1 Algorithm 2: Controlled Reverse ODE (15) Input: Discretization steps N , reference text "prompt", reference image y 0 , prompt embedding network Œ¶, Flux model u(‚Ä¢, ‚Ä¢, ‚Ä¢; œÜ), Flux noise scheduler œÉ : [0, 1] ‚Üí R, structured noise y 1 Tunable parameter: Controller guidance Œ∑ In this section, we conduct ablation study for our controller guidance parameter Œ∑ t . We consider two different time-varying schedules for Œ∑ t , and show that our controller strength allows for a smooth interpolation between unconditional and conditional generation.

$Output: Edited image X 1 1 Initialize X 0 = y 1 2 for i = 0 to N -1 do 3 Current time step: t i = i N 4 Next time step: t i+1 = i+1 N 5 Unconditional vector field: v ti (X ti ) = -u(X ti , 1 -t i , Œ¶(prompt); œÜ) ‚ñ∑ Proposition 3.1 6 Conditional vector field: v ti (X ti |y 0 ) = y0-Xt i 1-ti ‚ñ∑ Proposition 3.2 7 Controlled vector field: vti (X ti ) = v ti (X ti ) + Œ∑ (v ti (X ti |y 0 ) -v ti (X ti )) ‚ñ∑ODE (15) 8 Next state: X ti+1 = X ti + vti (X ti ) (œÉ(t i+1 ) -œÉ(t i )) 9 end 10 return X 1$Preprint.

In Figure [9](#fig_6), we show the effect of starting time in controlling the faithfulness of inversion; starting time s ‚àà [0, 1] is defined as the time at which our controlled reverse ODE ( [15](#formula_24)) is initialized. The initial state X s = y 1-s is obtained by integrating the controlled forward ODE (8) from 0 ‚Üí 1 -s. In Figure [10](#fig_7), we study the effect of stopping time. We find that increasing controller guidance Œ∑ t by increasing the stopping time œÑ guides the reverse flow towards the original image. However, we observe a phase transition around œÑ = 0.14 = 4/28, indicating that the resulting drift in our controlled reverse ODE ( [15](#formula_24)) is dominated by the conditional vector field v t (X t |y 0 ) for t ‚â• œÑ . Therefore, the reverse flow solves the LQR problem ( [16](#formula_25)) and drives toward the terminal state (i.e., the original image). In Figure [11](#fig_9), we visualize the effect of our controller guidance for another time-varying schedule.

We make a similar observation as in Figure [10](#fig_7): increasing Œ∑ t improves faithfulness. However, we notice a smooth transition from the unconditional to the conditional vector field, evidence from the smooth interpolation between "A young man" at the top left (Œ∑ = 0) and the original image at the bottom right.

Preprint.  

## C.3 NUMERICAL SIMULATION

In this section, we design synthetic experiments to compare reconstruction accuracy of DM and RF inversion. Given Y 0 ‚àº p 0 , where the data distribution p 0 := N (¬µ, I) and the source distribution q 0 := N (0, I), we numerically simulate the ODEs and SDEs associated with DM and RF inversion; see our discussion in ¬ß3.

For ¬µ = 10, we fix Œ≥ = 0.5 in the controlled forward ODE (8), and Œ∑ = 0.5 in the controlled reverse ODE (15). These ODEs are simulated using the Euler discretization scheme with 100 steps. Additionally, we simulate the uncontrolled rectified flow ODEs (6) ‚Üí (1) as a special case of our controlled ODEs (8) ‚Üí (15) by setting Œ≥ = Œ∑ = 0, and the deterministic diffusion model DDIM [(Song et al., 2021a)](#) in the same experimental setup.

The inversion accuracy is reported in Table [5](#). Observe that RF inversion has less L2 and L1 error compared to DDIM inversion (14). The minimum error is obtained by setting Œ≥ = Œ∑ = 0 (i.e., reversing the standard rectified flows), which supports our discussion in ¬ß3.3.

Furthermore, we simulate the stochastic samplers corresponding to these ODEs in Table [5](#), highlighted in orange. Similar to the deterministic samplers, we observe that stochastic equivalents of rectified flows more accurately recover the original sample compared to diffusion models. Our controller in RF Inversion (10) ‚Üí (17) effectively reduces the reconstruction error in the uncontrolled RF Inversion (12) ‚Üí (22), which are special cases when Œ≥ = Œ∑ = 0. Thus, we demonstrate that (controlled) rectified stochastic processes are better at inverting a given sample from the target distribution, outperforming the typical OU process used in diffusion models [(Song & Ermon, 2019;](#b44)[Ho et al., 2020;](#b14)[Song et al., 2021a;](#)[b)](#).

In Figure [12](#), we compare sample paths of diffusion models and recitified flows using 10 IID samples drawn from p 0 . In Figure [13](#fig_1), we visualize paths for those samples using our controlled ODEs and SDEs with Œ≥ = Œ∑ = 0.5.

## C.4 ADDITIONAL RESULTS ON STROKE2IMAGE GENERATION

In Figure [14](#fig_0) and Figure [15](#), we show additional qualitative results on Stroke2Image generation. Our method generates more realistic images compared to leading training-free approaches in semantic image editing including optimization-based NTI [(Mokady et al., 2023)](#b25) and attention-based NTI+P2P [(Hertz et al., 2022)](#b12). Furthermore, it gives a competitive advantage over the training-based approach InstructPix2Pix [(Brooks et al., 2023)](#b6). Figure [19](#fig_13) shows the insertion of multiple objects by text prompts, such as "pepperoni", "mushroom", and "green leaves" to an image of a pizza. Interestingly, pepperoni is not deleted while inserting mushroom, and mushroom is not deleted while inserting green leaves. The product is finally presented in a lego style.

Figure [20](#fig_14) captures a variety of facial expressions that stylize a reference image. Given the original image and text prompt: e.g. "Face of a girl in disney 3d cartoon style", we first invert the image to generate the stylized version of the original image. Then, we add the prompt for the expression (e.g., "surprised") at the end of the prompt and run our editing algorithm (15) with this new prompt: "Face of a girl in disney 3d cartoon style, surprised". By changing the expression, we are able to preserve the identity of the stylized girl and generate prompt-based facial expressions. For inversion, all methods perform well at recovering the stroke input when given a null prompt. However, when a new prompt like "a photo-realistic picture of a bedroom" is provided, only our method successfully generates realistic images. The other methods continue to suffer from the initial corruption, failing to make the output more realistic.

Figure [17](#fig_4): Gender editing. Our method smoothly interpolates between "A man" ‚Üî "A woman".

"A woman" "old" "older" "A man" "young" "younger" Figure [21](#) shows stylization based on a single reference style image and 12 different text prompts, covering both living and non-living objects. The generated images contain various style attributes that includes melting elements, golden color, and 3d rendering from the reference style image.

Figure [22](#) visualizes stylization results based on different reference style images. In this experiment, we use text prompt to describe both the content of the generated image and the style of the given reference style image.

Original + "pepperoni" + "mushroom" + "green leaves" + "in lego style" "Face of a girl in disney 3d cartoon style" "scared" "surprised" "frowning" "yawning" "laughing" Original "gasping" "sad" "smiling" "winking" "grinning" "angry"

"Face of a boy in disney 3d cartoon style" "laugh" "smirking" "frown" "angry" "yelling" Original 

## C.6 HUMAN EVALUATION

We conduct a user study on the test splits of both LSUN Bedroom and LSUN Church dataset using Amazon Mechanical Turk, with 126 participants in total. As shown in Figure [23](#fig_1), each question was accompanied by an explanation of the task, the question, and the evaluation criteria. Participants were shown a pair of stroke-to-image outputs from different models, in random order, along with the input stroke image. They were asked to select one of three options based on their preference using the following two criteria: 1. Realism: which of these two images look more like a real, photorealistic image? 2. Faithfulness: which of these two images match more closely to the input stroke image?

We collect 3 responses per question. With 300 images in the test dataset and 10 pairwise comparisons, we gathered 9,000 responses for this evaluation. The example in Figure [23](#fig_1) is for the LSUN Church dataset; for LSUN Bedroom dataset, we simply replace the word "church" to "bedroom" in the instructions.

"line drawing" "melting golden" Flux Flux Ours Ours "3d rendering" Flux Ours "wooden sculpture" Flux Ours

Figure [22](#): Stylization using a single prompt and various reference style images: "melting golden", "line drawing", "3d rendering", and "wooden sculpture". Given a style image (e.g. "3d rendering") and a text prompt (e.g. "face of a boy in 3d rendering style"), our method generates images that are consistent with the reference style image and the text prompt. The standard output from Flux is obtained by disabling our controller, which clearly highlights the importance of the controller.

Figure [23](#fig_1): Interface for human evaluation. Each participant is asked to select their preferred image based on two criteria: realism and faithfulness.

![Figure 4: Effect of controller guidance Œ∑ given the original image and the prompt: "A young man". Increasing Œ∑ improves the faithfulness to the original image, which is reconstructed at Œ∑ = 1.]()

![Figure 3: Inverting flows by controlled ODEs (8) and (15).]()

![Figure 5: Stroke2Image generation. Our method generates photo-realistic images of bedroom or church given stroke paints, showing robustness to initial corruptions.]()

![Figure 7: Editing (a) stylized expression, (b) age, (c) gender, and (d) object insert. Given an original image and a text prompt, our algorithm performs semantic image editing in the wild.]()

![Figure 8: Comparison using Flux backbone.]()

![Figure 9: Effect of starting time. Prompt: "A young man". The number below each figure denotes the starting time scaled by 28 (the total number of denoising steps) for better interpretation. In the absence of controller guidance (Œ∑ t = 0), increasing the starting time (s) in our controlled ODE (15) improves faithfulness to the original image.]()

![Figure 10: Effect of controller guidance. Prompt: "A young man". For a fixed starting time s = 0, consider a time-varying controller guidance schedule Œ∑ t = Œ∑ ‚àÄt ‚â§ œÑ and 0 otherwise. The number below each figure denotes the stopping time œÑ scaled by 28 (the total number of denoising steps) for better interpretation. Increasing œÑ increases the controller guidance (Œ∑ t ) that improves faithfulness to the original image.]()

![Figure 11: Effect of controller guidance for another time-varying schedule. Prompt: "A young man". The number below each figure denotes the starting time scaled by 28 (the total number of denoising steps) for better interpretation. For a fixed starting time s = 0 and stopping time œÑ = 8, consider a time-varying controller guidance schedule Œ∑ t = Œ∑ ‚àÄt ‚â§ œÑ and 0 otherwise. Increasing Œ∑ increases the controller guidance (Œ∑ t ) that improves faithfulness to the original image.]()

![Figure 14: Stroke2Image generation. Additional qualitative results on LSUN-Bedroom dataset comparing our method with SoTA training-free and training-based editing approaches.]()

![Figure16: Robustness. For inversion, all methods perform well at recovering the stroke input when given a null prompt. However, when a new prompt like "a photo-realistic picture of a bedroom" is provided, only our method successfully generates realistic images. The other methods continue to suffer from the initial corruption, failing to make the output more realistic.]()

![Figure 18: Age editing. Our method regulates the extent of age editing.]()

![Figure 19: Object insert. Text-guided insertion of multiple objects sequentially.]()

![Figure 20: Stylization using reference text. Stylization of a reference image given prompt-based facial expressions in "disney 3d cartoon style".]()

![]()

![Quantitative results for Stroke2Image generation. L2 and Kernel Inception Distance (KID) capture faithfulness and realism, respectively. Optimization-based methods are colored gray. User Pref. shows the percentage of users that prefer our method over each alternative in pairwise comparisons (and ties). E.g.: 62.11% (+ 8% ties) prefer ours over SDEdit-Flux for LSUN Bedroom.]()

![Quantitative results for face editing on SFHQ for "wearing glasses".]()

![Hyper-parameter configuration of our method for inversion and editing tasks.]()

https://github.com/google/prompt-to-prompt

https://github.com/huggingface/diffusers

https://github.com/black-forest-labs/flux

https://huggingface.co/spaces/timbrooks/instruct-pix2pix

https://huggingface.co/spaces/editing-images/leditsplusplus

