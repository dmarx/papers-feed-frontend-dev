<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">JPEG COMPRESSED IMAGES CAN BYPASS PROTEC-TIONS AGAINST AI EDITING</title>
				<funder ref="#_3KrcQkv">
					<orgName type="full">DARPA GARD</orgName>
				</funder>
				<funder>
					<orgName type="full">Amazon Lab126 Diversity in Robotics and AI Fellowship</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-04-07">7 Apr 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Pedro</forename><surname>Sandoval-Segura</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jonas</forename><surname>Geiping</surname></persName>
							<email>jgeiping@cs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
							<email>tomg@cs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">JPEG COMPRESSED IMAGES CAN BYPASS PROTEC-TIONS AGAINST AI EDITING</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-04-07">7 Apr 2023</date>
						</imprint>
					</monogr>
					<idno type="MD5">B066D478B04A5BC91FD6BBA1CD221CF2</idno>
					<idno type="arXiv">arXiv:2304.02234v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently developed text-to-image diffusion models make it easy to edit or create high-quality images. Their ease of use has raised concerns about the potential for malicious editing or deepfake creation. Imperceptible perturbations have been proposed as a means of protecting images from malicious editing by preventing diffusion models from generating realistic images. However, we find that the aforementioned perturbations are not robust to JPEG compression, which poses a major weakness because of the common usage and availability of JPEG. We discuss the importance of robustness for additive imperceptible perturbations and encourage alternative approaches to protect images against editing.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Given a user-defined text description of an image, diffusion models <ref type="bibr" target="#b2">(Ho et al., 2020;</ref><ref type="bibr" target="#b4">Rombach et al., 2022)</ref> excel at generating high-quality image samples. Diffusion models can also be used to edit specific parts of an existing image, guided by a text-prompt <ref type="bibr" target="#b4">(Rombach et al., 2022)</ref>. Previously, photo editing software and expertise was required to make serious scene adjustments or object additions, but now it can be done with a simple natural language phrase. The barrier to creating new images is relatively low, opening the door to malicious actors who could edit images or create deepfakes with a single click. To prevent the malicious editing of images, recent work has proposed using imperceptible perturbations to protect images from being modified <ref type="bibr" target="#b6">(Salman et al., 2023;</ref><ref type="bibr" target="#b7">Shan et al., 2023)</ref>. As with other methods which modify images to prevent a deep learning model from accomplishing a task, releasing protected images online exposes the owner to a static defense vs. adaptive attack scenario <ref type="bibr" target="#b3">(Radiya-Dixit &amp; Tramer, 2021)</ref>. In this setting, the protected images are required to withstand adaptive attacks from current methods and all future methods, a nearly impossible requirement. By successfully editing protected images, we demonstrate that JPEG compression remains a necessary baseline for protection methods that use imperceptible noise. The widespread use of JPEG transformations (w3t), which may occur inadvertently (e.g., if JPEG is the default file format for capturing screenshots), suggests that optimized perturbations for image protection are exceedingly fragile. The robustness of perturbations is especially important in situations where image misuse by adversaries if of concern. All experiments are conducted using the open-source notebooks from the photoguard repository <ref type="bibr" target="#b6">(Salman et al., 2023)</ref>, which employs the Stable Diffusion Model (SDM) v1.5 <ref type="bibr" target="#b4">(Rombach et al., 2022)</ref>.</p><p>1.1 PHOTOGUARD: PROTECTING AGAINST MALICIOUS AI-POWERED IMAGE EDITING <ref type="bibr" target="#b6">Salman et al. (2023)</ref> propose methods for protecting images against manipulation by Stable Diffusion <ref type="bibr" target="#b4">(Rombach et al., 2022)</ref>. Their software, photoguard, protects images against manipulation by adding an imperceptible adversarial perturbation designed to force a target diffusion model to generate unrealistic images. The authors propose two different protection techniques: an encoder attack and a diffusion attack.</p><p>Encoder attack. The goal is to find an imperceptible perturbation δ encoder that, when added to the original image, results in a protected image that the SDM considers similar to a gray target image: First row: Given a text prompt, an adversary can make desired edits to an input image using a diffusion model. Second row: photoguard (Encoder attack) <ref type="bibr" target="#b6">(Salman et al., 2023)</ref> protects the original image before an adversary can access it by adding an imperceptible perturbation. When the adversary edits the photoguard image, they are unable to maintain the original subject. Third row: By JPEG compressing the photoguard image, an adversary can edit the photoguard image while maintaining the original subject and adding key visual features of the text prompt.</p><formula xml:id="formula_0">δ encoder = arg min δ ∞≤ E(x + δ) -z target 2 2 (1)</formula><p>where x is the image to be protected, z target is some target latent representation, and E is the image encoder. For a 512 × 512 × 3 dimensional image, = 0.1 and 40 steps of PGD with step-size 0.1 40 × 6 is used to solve the optimization.</p><p>Diffusion attack. The goal is to find an imperceptible perturbation δ diffusion that, when added to the original image, results in a protected image that targets the diffusion process itself.</p><formula xml:id="formula_1">δ diffusion = arg min δ ∞≤ f (x + δ) -x target 2 2 (2)</formula><p>where f is the SDM, x is the image to be protected, x target is the target image to be generated. Eq. 2 is solved approximately using PGD, backpropagating through only 4 diffusion steps due to GPU memory requirements <ref type="bibr" target="#b6">(Salman et al., 2023)</ref>. <ref type="bibr" target="#b6">Salman et al. (2023)</ref> note that photoguard's effectiveness may diminish when protected images undergo transformations, and propose methods for making the imperceptible perturbation more robust <ref type="bibr" target="#b0">(Athalye et al., 2018)</ref>. Protecting images using an imperceptible perturbation optimized using either of the attacks of Section 1.1 is a interesting and novel contribution, but our research has found it is more brittle than expected to JPEG distortion, which poses a major weakness due to the common usage and availability of JPEG. Previous work has shown that a differentiable JPEG module can be added to the loss function to create JPEG-resistant adversarial examples <ref type="bibr" target="#b8">(Shin &amp; Song, 2017)</ref>. One can imagine making a similar, simple modification to Eq. 1 and Eq. 2 so that the perturbations are JPEG-resistant. But the question remains: how many times does one want to continue the back and forth between the data protector and adversary? We take a photoguard (Encoder attack) image and JPEG compress it with varying quality. An image at 100% JPEG quality is almost equivalent to the original photoguard image, while 65% JPEG quality loses significant high-frequency information. Second row: Starting from the compressed image above, the adversary uses a diffusion model to make edits according to the same prompt setup as Figure <ref type="figure" target="#fig_0">1</ref>. With more compression, the generated content better maintains the original subject. Between compression quality of 95% and 85%, enough of the photoguard noise is diminished, allowing stable image edits by an adversary.</p><p>1.2 GLAZE: PROTECTING AGAINST STYLE MIMICRY <ref type="bibr" target="#b7">Shan et al. (2023)</ref> propose a method to prevent fine-tuned diffusion models from learning a particular artist's style. In essence, imperceptible perturbations are optimized so that SDM finetuning on protected artist images, in the manner described by DreamBooth <ref type="bibr" target="#b5">(Ruiz et al., 2022)</ref>, is unsuccessful. In this way, new images generated by the finetuned model are unable to replicate the style captured in the protected subset of artist images. The proposed solution is unique and the authors perform experiments to test the robustness of their perturbations to Gaussian noise and JPEG image compression. However, only three compression levels between 20 and 10 are considered. Our experiments suggest that considering more subtle compression levels (e.g., between compression levels 95 and 65) are necessary. Excessive compression, even without the use of optimized imperceptible perturbations, could potentially damage the image to the extent that the style appears to not have been replicated.</p><p>Evaluating whether generated images replicate a particular style, or whether generated content satisfies some other criteria in general, presents a challenge due to its subjective nature. Our experiments suggest that imperceptible perturbations must possess greater robustness against a wider range of image transformations than those commonly considered, as final transformations could be made by a malicious actor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BYPASSING PHOTOGUARD PROTECTION</head><p>At a high level, photoguard adds imperceptible noise to an image so that the image encoder in a diffusion model is unable to encode the image reasonably, resulting in downstream error by the denoising network. The idea being that if an adversary can only produce unrealistic images, they would choose not to edit the protected image. In this section, we demonstrate that if the adversary JPEG compresses the photoguard-protected image, they can effectively perform edits using the same SDM used to protect the image. We also discuss the possibility of optimizing against this attack and analyze how other simple image transforms affect photoguard protection. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">JPEG COMPRESSION AS A LOW-COST TRANSFORM</head><p>Given a source image (e.g., an image of a dog in the grass) and a text prompt (e.g., "dog under heavy rain and muddy ground real"), SDM can edit the source image to match the prompt. In Figure <ref type="figure" target="#fig_0">1</ref>, we show that when the photoguard-protected image (from the Encoder attack) is used as the source, the edited image no longer maintains the original subject and the dog's front paw seems to blend with some kind of foam. However, if we JPEG compress the photoguard image with quality of 65, the edited image looks like the edit from the original, and is potentially even more faithful to the text prompt due to the more prominent rain.</p><p>To perform inpainting, SDM is conditioned on both a source image and a mask indicating the regions of the image that are to remain unedited. In the last row of Figure <ref type="figure" target="#fig_2">3</ref>, we show that an adversary can take a JPEG compressed photoguard image (from the Diffusion attack) and generate a new image that contains more realistic background and clothing.</p><p>We explore the effect that JPEG compression of varying quality has on the resulting edited image in Figure <ref type="figure" target="#fig_1">2</ref> for the Encoder attack and Figure <ref type="figure">4</ref> for the Diffusion attack, finding that there exists a compression level range in which image editing performs well. If the photoguard perturbations were mostly high-frequency, using a Gaussian blur should also be effective at restoring an adversary's editing abilities. However, in Appendix A.1 we find that Gaussian and median blurs are ineffective. Other simple image transforms like rotations and flips (Appendix A.2) are also ineffective. This is likely because the image encoder is mostly invariant to these transforms. While these simpler transforms were unsuccessful, we believe they should still be part of extensive robustness evaluations of future methods utilizing imperceptible perturbations. JPEG's ability to undermine the protective perturbation highlights the transforms' uniqueness in eliminating the perturbation while maintaining important semantic content.</p><p>Prior work has shown that JPEG could be used as a reasonable pre-processing step to defend against adversarial examples <ref type="bibr" target="#b1">(Das et al., 2017)</ref>, but such pre-processing defenses are not as robust to adap-Figure <ref type="figure">4</ref>: More JPEG compression more reliably undermines photoguard protection. First row: We take a photoguard (Diffusion attack) image and JPEG compress it with varying quality. Second row: Starting from the compressed image above, the adversary uses a diffusion model to make edits according to the same prompt setup as Figure <ref type="figure" target="#fig_2">3</ref>. With more compression, the generated content background and clothing is more realistic. Between compression quality of 85% and 75%, enough of the photoguard noise is diminished, allowing stable image edits by an adversary.</p><p>tive attackers, which go last in the classical game of adversarial examples. As pointed out in Salman et al. ( <ref type="formula">2023</ref>), this could imply that an approach like photoguard could be improved to be robust against these transformations. Yet, we want to highlight that the game between attack and defense is flipped for data modifications, a modification like photoguard has to anticipate future adaptive attacks that scrub it, and previous results on robustness of adversarial attacks might not apply.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CONCLUSION</head><p>Ultimately, the attack presented in this note is surprisingly simple, to the point where the (re-) encoding with different JPEG settings could also be considered just a benign editing operation, or a standard operation when storing the modified image. This makes it harder to verify how high the bar is really raised by adversarial noise image protection methods. Our results suggest that an adversary can bypass protections by JPEG compressing images that have been imperceptibly modified for protection. Protecting images from misuse remains an extremely difficult problem. As long as semantic information of an image is recognizable by a human, as is the case with imperceptibly perturbed images, the human visual system is a constructive proof that there exists a function to recover the semantic information. Thus, for high-stakes settings where image misuse is of concern, imperceptible perturbations of any kind, regardless of the optimization details, are so far inadequate for protection against editing by an adversary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX</head><p>A.1 GAUSSIAN AND MEDIAN BLURS OF THE PROTECTED IMAGE Gaussian blurs are low-pass filters which attenuate high-frequency signals. If the most important component of a photoguard perturbation were the high-frequency components, then a Gaussian blur should be successful at diminishing the protection. In Figure <ref type="figure">5</ref>, we show Gaussian blurs of photoguard images do not enable SDM to generate reasonable images. Since Gaussian blurs also destroy a significant amount of semantic information, it is difficult to pinpoint whether the most important features of the photoguard perturbations are high-frequency. We also experiment with median blurs in Figure <ref type="figure" target="#fig_3">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 ROTATIONS AND HORIZONTAL FLIP OF THE PROTECTED IMAGE</head><p>The adversary need not edit the photoguard-protected image as-is. Performing other simple transforms seems reasonable before editing the image. In Figures <ref type="figure" target="#fig_4">7</ref> and <ref type="figure" target="#fig_5">8</ref>, we experiment with rotating and flipping the protected image before using SDM. While the photoguard perturbation was not optimized so that rotations and flips induce the same embedding from the encoder, generated images did not retain the original subject of the source image, suggesting the photoguard perturbation was effective under these transforms.</p><p>Figure <ref type="figure">5</ref>: Gaussian blurs of photoguard images. First row: We perform a Gaussian blur on a photoguard image with varying kernel sizes. Second row: Starting from the blurred image above, the adversary uses a diffusion model to make edits according to the same prompt setup as Figure <ref type="figure" target="#fig_0">1</ref>. With a larger kernel, the generated images lose significant visual features and do not maintain the original subject, unlike JPEG compression.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: JPEG compression allows an adversary to modify a protected image found online. First row: Given a text prompt, an adversary can make desired edits to an input image using a diffusion model. Second row: photoguard (Encoder attack) (Salman et al., 2023) protects the original image before an adversary can access it by adding an imperceptible perturbation. When the adversary edits the photoguard image, they are unable to maintain the original subject. Third row: By JPEG compressing the photoguard image, an adversary can edit the photoguard image while maintaining the original subject and adding key visual features of the text prompt.</figDesc><graphic coords="2,167.40,81.86,277.20,241.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: More JPEG compression more reliably undermines photoguard protection. First row:We take a photoguard (Encoder attack) image and JPEG compress it with varying quality. An image at 100% JPEG quality is almost equivalent to the original photoguard image, while 65% JPEG quality loses significant high-frequency information. Second row: Starting from the compressed image above, the adversary uses a diffusion model to make edits according to the same prompt setup as Figure1. With more compression, the generated content better maintains the original subject. Between compression quality of 95% and 85%, enough of the photoguard noise is diminished, allowing stable image edits by an adversary.</figDesc><graphic coords="3,157.50,81.86,296.99,157.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: JPEG compression allows an adversary to edit the background of a protected image found online. First row: Given a text prompt, an adversary can make desired edits to an input image using a diffusion model. Second row: photoguard (Diffusion attack) (Salman et al., 2023) protects the original image before an adversary can access it by adding an imperceptible perturbation. When the adversary edits the photoguard image, the background is unrealistic. Third row: By JPEG compressing the photoguard image, an adversary can edit the photoguard image while maintaining the original subjects and adding key visual features of the text prompt.</figDesc><graphic coords="4,173.34,81.86,265.31,231.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Median blurs of photoguard images. First row:We perform a median blur on a photoguard image with varying kernel sizes. Second row: Starting from the blurred image above, the adversary uses a diffusion model to make edits according to the same prompt setup as Figure1. With a larger kernel, the generated images seemingly begin to retain the original subject.</figDesc><graphic coords="7,197.10,449.39,217.79,185.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Rotations of photoguard images. First row: We perform counter-clockwise rotations of varying angles on a photoguard image. Second row: Starting from the rotated image above, the adversary uses a diffusion model to make edits according to the same prompt setup as Figure 1. The generated image does not retain the original subject in any of the rotations we tried.</figDesc><graphic coords="8,108.00,160.61,395.98,142.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Flip of photoguard image. Left: We perform a horizontal flip on a photoguard image. Right: Starting from the flipped image, the adversary uses a diffusion model to make edits according to the same prompt setup as Figure 1. The generated image does not retain the original subject.</figDesc><graphic coords="8,191.16,529.13,229.68,72.00" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This work is funded in part by an <rs type="funder">Amazon Lab126 Diversity in Robotics and AI Fellowship</rs>. Additional funding is provided by <rs type="funder">DARPA GARD</rs> (<rs type="grantNumber">HR00112020007</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_3KrcQkv">
					<idno type="grant-number">HR00112020007</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Synthesizing robust adversarial examples</title>
		<author>
			<persName><forename type="first">Anish</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="284" to="293" />
		</imprint>
	</monogr>
	<note>Preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Keeping the bad guys out: Protecting and vaccinating deep learning with jpeg compression</title>
		<author>
			<persName><forename type="first">Nilaksh</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madhuri</forename><surname>Shanbhogue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fred</forename><surname>Shang-Tse Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Hohman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duen</forename><surname>Kounavis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chau</forename><surname>Horng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02900</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Data poisoning won&apos;t save you from facial recognition</title>
		<author>
			<persName><forename type="first">Evani</forename><surname>Radiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Dixit</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Tramer</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=__sp5PEix2H" />
	</analytic>
	<monogr>
		<title level="m">ICML 2021 Workshop on Adversarial Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Highresolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Björn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation</title>
		<author>
			<persName><forename type="first">Nataniel</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yael</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kfir</forename><surname>Aberman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Raising the cost of malicious ai-powered image editing</title>
		<author>
			<persName><forename type="first">Alaa</forename><surname>Hadi Salman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Khaddaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Leclerc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><surname>Madry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.06588</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Shawn</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenna</forename><surname>Cryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Wenger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haitao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rana</forename><surname>Hanocka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><forename type="middle">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><surname>Glaze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.04222</idno>
		<title level="m">Protecting artists from style mimicry by text-to-image models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Jpeg-resistant adversarial images</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2017 Workshop on Machine Learning and Computer Security</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
