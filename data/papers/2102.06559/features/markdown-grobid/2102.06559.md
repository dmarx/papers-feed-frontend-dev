# Infinitely Deep Bayesian Neural Networks with Stochastic Differential Equations

## Abstract

## 

We perform scalable approximate inference in continuous-depth Bayesian neural networks. In this model class, uncertainty about separate weights in each layer gives hidden units that follow a stochastic differential equation. We demonstrate gradient-based stochastic variational inference in this infinite-parameter setting, producing arbitrarily-flexible approximate posteriors. We also derive a novel gradient estimator that approaches zero variance as the approximate posterior over weights approaches the true posterior. This approach brings continuous-depth Bayesian neural nets to a competitive comparison against discrete-depth alternatives, while inheriting the memory-efficient training and tunable precision of Neural ODEs.

## INTRODUCTION

Taking the limit of neural networks to be the composition of infinitely many residual layers provides a way to implicitly define its output as the solution to an ODE [(Haber and Ruthotto, 2017;](#b17)[E, 2017)](#). This continuous-depth parameterization decouples the specification of the model from its computation. While the paradigm adds complexity, it has several benefits:

(1) Computational cost can be traded for precision in a fine-grained manner by specifying error tolerances for adaptive computation, and (2) memory costs for training can be significantly reduced by running the dynamics backwards in time to reconstruct activations of intermediate states needed for backpropagation.

On the other hand, the Bayesian treatment for neural networks modifies the typical training pipeline where  Figure [1](#): Hidden unit trajectories in an ODE-Net and an SDE-BNN. Left: A continuous-depth residual network has deterministic transformations of its hidden units from depths t = 0 to t = 1. Right: Uncertainty in the weights of a Bayesian continuous-depth residual network implies uncertainty in its hidden unit activation trajectories. Shaded regions show densities over samples from the learned posterior dynamics. Both: Each distinct color corresponds to a different initial state corresponding to different data inputs. instead of performing point estimates, a distribution over parameters is inferred. Although this approach adds complexity, it automatically accounts for model uncertainty. In turn, model averaging can be done to combat overfitting and improve calibration, especially on out-of-distribution data [(Zhang et al., 2018;](#b52)[Osawa et al., 2019)](#b39).

How can we combine the benefits of continuous-depth models with those of Bayesian neural networks? The simplest approach is a "Bayesian neural ODE" [(Yıldız et al., 2019;](#b50)[Dandekar et al., 2020)](#b6), which integrates out the finitely-many parameters of a standard neural ODE for prediction. This approach is straightforward to implement, and can inherit the advantages of both Bayesian and continuousdepth neural nets. However, empirically, standard Gaussian approximate posteriors are a relatively poor arXiv:2102.06559v4 [stat.ML] 30 Jan 2022 match for neural ODEs, not to mention the drawbacks of also being used in the prior. Additionally, it does not exploit the special synergy available between continuous-time models and approximate inference.

In this paper, we show that an alternative construction of Bayesian continuous-depth neural networks has additional practical benefits. Specifically, we consider the limit of infinite-depth Bayesian neural networks with separate unknown weights at each layer, a model class that we refer to as SDE-BNNs. We develop a unique network architecture that enhances model expressivity through time-correlated weights and scales linearly, instead of quadratically, with the parameter dimensionality. Combined with our novel formulation of a zerovariance gradient estimator, we show that approximate inference can be realized through the maximization of our modified variational lower bound, effectively scaling up the gradient-based variational inference scheme described by [Li et al. (2020)](#b31) (preliminary forms of which appeared in earlier works [(Archambeau et al., 2008;](#b0)[Opper, 2019;](#b38)[Tzen and Raginsky, 2019a)](#b46)).

With this approach, the state of the output layer is computed by a black-box adaptive SDE solver. Figure 1 contrasts our neural SDE with the neural ODE parameterization. This approach maintains the adaptive computation and constant-memory cost of training Bayesian neural ODEs and adds two unique benefits:

• The variational posterior can be made arbitrarily expressive by simply enlarging the neural network that parameterizes the dynamics of the approximate posterior. Under mild conditions, this approach can approximate the true posterior arbitrarily closely.

• The variational objective admits a variance-reduced gradient estimator that is a natural extension of the "sticking the landing" trick [(Roeder et al., 2017)](#b43).

Combined with arbitrarily expressive approximate posteriors, it is consistent and has vanishing variance as the approximate posterior approaches the true.

Notably, our low-variance gradient estimator can also be applied to variational inference in SDEs more generally, such as for time-series modeling, but such applications are beyond the scope of this paper.

## BACKGROUND

Bayesian Neural Networks Given a dataset, there are often many functions that fit the data well, which a given neural network can express with different parameter values. Instead of making point estimates of the parameters, the Bayesian paradigm frames learning as posterior inference. Predictions are obtained through integrating over many possible parameter settings. Formally, given a dataset D = {(x i , y i )} N i=1 and prior distribution over model weights p(w), we want to compute a posterior p(w|D) ∝ p(D|w)p(w). We can optimize an approximate posterior distribution q(w) that minimizes the Kullback-Leibler (KL) divergence, i.e. maximizing the Evidence Lower Bound (ELBO):

$L ELBO (φ) =E q(w) [log p(D|w)] -D KL (q(w)||p(w)) .$(1)

Estimating gradients of this objective using simple Monte Carlo is known as stochastic variational inference (SVI) [(Hoffman et al., 2013;](#b20)[Rezende et al., 2014)](#b42)).

One of the main technical challenges of SVI is choosing a parametric family of approximate posteriors that is tractable to sample from and evaluate, while being flexible enough to approximate the true posterior well. Most scalable inference techniques use Gaussian approximate posteriors with restricted covariance structure between network parameters [(Graves, 2011;](#b14)[Blundell et al., 2015;](#b2)[Zhang et al., 2018;](#b52)[Mishkin et al., 2018)](#b35).

Others construct complex approximate posteriors with normalizing flows [(Krueger et al., 2018;](#b30)[Louizos and Welling, 2017)](#b34) or through distillation [(Balan et al., 2015;](#b1)[Wang et al., 2018)](#b48).

Neural Ordinary Differential Equations Neural ordinary differential equations [(Chen et al., 2018)](#b5) define ODEs using neural networks:

$dh t = f θ (h t , t) dt, h 0 ∈ R d ,(2)$where f :

$R d × R → R d is$a Lipschitz function defined by a neural network with parameters θ. Starting at an initial value h 0 = x given by a data example and z(t) t0 t1 5.0 2.5 0.0 2.5 5.0 s(t) t z(t 1 ) 8 6 4 2 0 2 4 6 8 6 4 2 0 2 4 6 z(t 0 ) Figure 3: Neural SDEs can learn arbitrarily expressive approximate posteriors. Left: Samples from an approximate posterior, trained with an OU prior and conditioned on two observations with Cauchy likelihoods. Right: Joint distribution and marginals of the approximate posterior process z at times t 0 and t 1 .

integrating these dynamics forward for a finite time can be seen as passing the input through an infinitely-deep residual network. For learning scalar-valued functions, adding extra dimensions to h and a linear final layer induces similar universal approximations to standard neural networks [(Dupont et al., 2019;](#b10)[Zhang et al., 2019b)](#b54) trained by standard stochastic gradient descent methods. Using adaptive ODE solvers can trade evaluation speed for precision. The adjoint sensitivity method saves memory during training through reconstructing the trajectory of the hidden units h by running the dynamics backwards in backpropagation.

## Latent Stochastic Differential Equations

Informally, an SDE can be viewed as an ODE with infinitesimal noise added throughout time. Formally:

$dw t = f θ (w t , t) dt + g θ (w t , t) dB t ,(3)$where w 0 ∈ R d is the initial state, f θ : R d ×R → R d and g θ : R d × R → R d×m are functions Lipschitz in both arguments, dubbed the drift and diffusion, respectively, and {B t } is an m-dimensional Brownian motion.

Some works have considered training SDEs with dynamics parameterized by neural networks [(Li et al., 2020;](#b31)[Tzen and Raginsky, 2019a;](#b46)[Peluchetti and Favaro, 2020b;](#b41)[Innes et al., 2019;](#b21)[Kong et al., 2020;](#b28)[Liu et al., 2019)](#b32). Note that directly optimizing the drift and diffusion to maximize the average log-likelihood of an observation log p(y t |w t ) would result in the diffusion approaching 0, conditional on the ODE fitting the training data well with the diffusion somewhat unconstrained.

Instead of directly optimizing the parameters of an SDE to match the data, a better approach is to use an SDE to define a prior over trajectories of w, and optimize the marginal likelihood of the data, integrating over all trajectories of w weighted by the prior. Luckily, we can specify an approximate posterior over trajectories using a second SDE. We define the approximate posterior by

$dw t = f φ (w t , t) dt + g θ (w t , t) dB t .(4)$When the dynamics of the approximate posterior f φ is parameterized by a neural network, this family of posteriors is extremely expressive. Figure [3](#) shows that such a variational family can easily approximate non-Gaussian and multi-modal posteriors on path space.

If both the SDE defined by equation 3 and equation 4 share the same diffusion function, then the KL between the two induced measures on path space has the following form [(Li et al., 2020;](#b31)[Tzen and Raginsky, 2019a)](#b46):

$D KL (µ q ||µ p ) = E q φ (w) 1 0 1 2 u(t, φ)$2 2 dt where ( [5](#))

$u(t, φ) = g θ (w t , t) -1 [f θ (w t , t) -f φ (w t , t)] (6)$where µ q and µ p are path space probability measures induced respectively by equation 4 and equation 3, and the expectation is taken under the approximate posterior, denoted q φ (w). Intuitively, this KL divergence resembles the summative difference over time horizon [[0,](#)[1]](#) between the prior drift f θ and f φ , scaled by the diffusion. This divergence can be estimated up to a constant with simple Monte Carlo, sampling trajectories from the dynamics given by the approximate posterior.

## SDEs as expressive approximate posteriors

To ensure that the KL divergence between the prior and approximate posterior on path space is finite, the same diffusion function g θ (w t , t) must be used for the approximate posterior and prior. Surprisingly, this does not limit the expressivity of the approximate posterior. [Boué et al. (1998)](#b3) show that there is a one-to-one correspondence between the space of path measures and drift functions that result in the same path space KL divergence. This implies that any path space measure close to the true posterior can be instantiated by SDEs with appropriate drifts. It follows that an approximate posterior parameterized by a sufficiently expressive family of function approximators can be made arbitrarily close to the true posterior. The Girsanov reparameterization of the variational formula, derived from Boue (Tzen and Raginsky, 2019a, Section 4), proves that the ELBO is tight when the drift is optimal. This means that there exists a ground truth drift function that can make the ELBO tight, the approximation of which can be achieved with a high capacity neural network.

Standard discrete-depth residual networks can be defined as a composition of layers of the form:

$h t+ = h t + f (h t , w t ), t = 1 . . . T, (7$$)$where t is the layer index, h t ∈ R D h denotes a vector of hidden unit activations at layer t, the input h 0 = x, and w t ∈ R Dw represents the parameters for layer t.

In the discrete setting, = 1, ∈ R.

We can construct a continuous-depth variant of residual networks by setting = 1 /T and taking the limit as T → ∞. This yields a differential equation that describes the hidden unit evolution as a function of depth t.

Since standard residual networks are parameterized with different layerwise "weights", we denote them w t .

To specify different weights at each layer with a finite number of parameters, we introduce a hypernetwork f w that specifies the change in weights as a function of depth and the current weights [(Ha et al., 2016)](#b16). The evolution of the hidden unit activations and weights can then be combined into a single differential equation:

$d dt h t w t = f h (t, h t , w t ) f w (t, w t )(8)$with some learned initial weight value w t0 . Using time-varying weights is similar to augmenting the state [(Dupont et al., 2019;](#b10)[Zhang et al., 2019c)](#b55). See Appendix Figure [8](#) on the effects of augmentation. We perform Bayesian inference on the weight process w t , assigning a suitable prior stochastic process and performing variational inference in this infinitesimal limit.

Like all Bayesian neural networks with observation likelihoods, our framework models uncertainty both about parameters and about individual observations:

The likelihood p(y|h 1 ) captures the observational noise, while the SDE encodes weight uncertainty.

Prior process on weights Typical priors for Bayesian neural networks are independent Gaussians across all weights and layers. Taking the infinitesimal limit of such a prior gives a white noise process prior on the weights w(•). However, initializing this noise while maintaining finite variance at scale is difficult [(Peluchetti and Favaro, 2020a,b)](#).

Instead, we use the Ornstein-Uhlenbeck (OU) process as the prior on weights. The process is characterized by an SDE with drift and diffusion:

$f p (w t , t) = -w t , g(w t , t) = σI d ,(9)$respectively, where σ is a hyperparameter. We choose this prior for its simplicity and bounded marginal variance at a constant in the large time limit.

## Approximate posterior over weights

We parameterize the approximate posterior on weights implicitly using another SDE with the following drift function:

$f q (w t , t, φ) = NN φ (w t , t, φ) -f p (w t , t).(10)$This drift f q is parameterized by a small neural network (NN) with parameters φ. With this drift, the approximate posterior process will generally have non-Gaussian, non-factorized marginals; its expressive capacity can be increased by making the neural net larger.

Evaluating the network Given an input, we marginalize over weight and hidden unit trajectories. This can be done with simple Monte Carlo, sampling a weight path {w t } from the posterior process and evaluating the network activations {h t } given the sampled weights and input. Both steps require solving a differential equation. Luckily, both can be solved simultaneously with the augmented state SDE:

$d w t h t = f w (w t , t, φ) f h (h t , t, w t ) dt + g w (w t , t) 0 dB t ,(11)$where h 0 = x, the input. The learnable parameters are the initial weight values at time zero w 0 (either point estimated or inferred) and those of drift function φ.

## Output likelihood

The final state of the hidden units h 1 is used to parameterize the likelihood of the target output y: log p(y|x, w) = log p(y|h 1 ). For instance, p(y|h 1 ) could be a Cauchy likelihood for regression, or categorical likelihood for classification.

Training objective To fit the network to data, we maximize the lower bound on marginal likelihood given by the infinite-dimensional ELBO:

$L ELBO∞ (φ) = E q φ (w) log p(D|w)- 1 0 1 2 u(w t , t, φ) 2 2 dt .$The sampled weights, the hidden activations, and the training objective are all computed simultaneously with a single call to an adaptive SDE solver. Gradients of the sampled loss can also be efficiently computed using adaptive solvers, following [Li et al. (2020)](#b31).

## VARIANCE-REDUCED GRADIENTS

Roeder et al. [(2017)](#) showed that when optimizing expectations using the reparameterization gradient, a gradient estimator with lower variance can be constructed by removing a score function term that has zero expectation, and that the variance of this gradient estimator approaches zero as the approximate posterior approaches the true posterior. We refer to this trick as "sticking the landing" (STL). We generalize this to our SDE setting by replacing the original estimator of the path space KL with the following STL estimator:

$KL STL = 1 0 1 2 u(w t , t, φ) 2 2 dt+ 1 0 u(w t , t, ⊥(φ)) dB t ,(12)$where w(•) ∼ q φ (w) and u is defined in equation 6, the path {w t } t∈[0,T ] is sampled from the approximate posterior process, and ⊥(•) is the stop gradient function that renders the input a constant with respect to which gradient propagation is stopped. Note that 12 is the fully Monte Carlo version referred to in 4 from which our STL variant is derived.

The second term in equation 12 is a martingale and has expectation zero. Therefore, in prior works [(Li et al., 2020;](#b31)[Tzen and Raginsky, 2019a,b)](#), Monte Carlo estimation was only performed for the first term, but we find that this approach does not necessarily reduce the variance of the gradient (Figure [4](#fig_3)).

Because our approximate posterior can be made arbitrarily expressive, we conjecture that our approach can achieve arbitrarily low gradient variance towards the end of training if the f w parameterization is expressive enough. See Appendix is A.2 for a heuristic derivation.

We show the variance of different gradient estimators in Figure [4](#fig_3), averaged across the parameters θ, in a 1D regression setting. We compare STL against a "Full Monte Carlo" estimate which includes the second additional term without gradient stopping, as well as the estimator that was previously used by [Li et al. (2020)](#b31) which ignores the second term. Figure [4](#fig_3) shows that STL obtains lower variance than alternatives, when matching an exponentiated Brownian motion. Table [4](#tab_3) shows training performance improvements. 

## EXPERIMENTS

We investigate the effectiveness of our proposed approximate inference method for training continuous-depth neural nets, referred to as SDE-BNN, in terms of classification accuracy, calibration, perturbation robustness, and speed-precision trade-offs. Our code is publicly available here and experimental settings in Table [2](#).

We consider toy regression and image classification tasks on MNIST and CIFAR-10. We also investigate out-of-distribution generalization. Notably, our approach does not require post hoc recalibration methods such as training with temperature scaling [(Guo et al., 2017)](#b15) or isotonic regression [(Zadrozny and Elkan, 2002)](#b51).

## Backpropagation through solvers vs. adjoint

We experimented with fixed-and adaptive-step SDE solvers, and the stochastic adjoint of [Li et al. (2020)](#b31).

Figure [5](#fig_4) shows similar convergence for both approaches.

Appendix C shows that both had similar numbers of dynamics function evaluations and wall-clock time.

The overhead for estimating error in our adaptive solvers was substantial; therefore, for final model evaluation, we trained with fixed-step solvers, where the number of steps is chosen to be large enough to match the convergence speed of our adaptive-step solvers.

Baselines For a fixed-depth network baseline, we compare to standard residual networks. We then test variational inference on the weights of these models. We also perform ablation studies to compare with standard variational inference approaches over continuousdepth networks. Specifically, we compare to a mean field variational inference (MFVI) ODEnet where stochastic variational inference is performed over depthinvariant weights. This baseline is a fully-factorized Gaussian approximate posterior, i.e. mean-field approximation, and been used for Neural ODEs by [Look and Kandemir (2019)](#b33); [Dandekar et al. (2020)](#b6).

We further compare our model to a MFVI HyperO-DEnet, where a learned drift is applied to w, but meanfield inference is instead performed over the parameters of the hypernetwork. Alternatively, one can interpret this as another MFVI ODEnet with a larger state and a more complex drift function but with similar computational complexity to SDE-BNN. This setting contrasts our approach of doing Bayesian inference over the entire continuous-depth network as a stochastic process.

Parameterizing the drift function We parameterized the drift of the variational posterior f w with a simple multilayer perceptron. To ensure optimization starts at a stable set of dynamics, we subtract the prior drift so that the approximate posterior equals the prior when the final layer is initialized to output zero.

Hyperparameters We swept learning rates in the range [1e-4, 1e-3], selecting the optimal based on the validation set. We train with the default Adam optimizer [(Kingma and Ba, 2015)](#b27). In image classification experiments, all convolutional layers of the drift network are time-conditional and use the tanh nonlinearity. The diffusion coefficient σ was selected from validation performance over {0.1, 0.2, 0.5}.

## 1D Regression

We first verify the capabilities of the SDE-BNN on a 1D regression problem. Conditioned on a sample from the diffusion process, each sample from a onedimensional SDE-BNN is a bijective mapping from the inputs to the outputs. This implies that every function sampled from a 1D SDE-BNN is monotonic.

To be able to sample non-monotonic functions, we augment the state with 2 extra dimensions initialized to zero, as in [Dupont et al. (2019)](#b10). Figure [2](#fig_2) shows that our model learns a reasonably flexible approximate posterior on a synthetic non-monotonic 1D dataset. We emphasize that the samples from our model are smooth w.r.t. depth because the hidden states h do not receive additive instantaneous noise, only the weights w do.

## Image Classification

Instantaneous changes to the hidden state (f h ) are parameterized using a convolutional neural network, including one strided convolution for downsampling and a transposed convolution layer for upsampling. We then set the w to be the filters and biases of all the convolutional layers. The approximate posterior drift dynamics (f w ) is a multilayer perceptron with hidden layer widths of 2, 128, and 2. The small hidden width of the bottleneck layers was chosen to reduce the number of variational parameters and promote linear scaling with respect to the dimension of w. On MNIST, we used one such SDE-BNN block, while on CIFAR-10, we used a multi-scale variant where multiple SDE-BNN blocks were stacked with the invertible downsampling from [Dinh et al. (2016)](#b9) in between.

We report classification results in Table [1](#tab_1). Our SDE-  The SDE-BNN matches and outperforms the accuracy of standard residual networks on MNIST and CIFAR-10, respectively, while obtaining lower expected calibration errors (ECE). From ablation studies, we found that it was harder to achieve similar performance with either of the mean field variants of an ODEnet as they had a poorer trade-off between performance and calibration.

Figure [6a](#fig_6) shows the ability of SDE-BNNs to trade off computation time for precision. Figure [12](#fig_2) in Appendix C.4 indicates that calibration is insensitive to solver tolerances close to the value used during training.

## Calibration

Table [1](#tab_1) quantifies our model's calibration with expected calibration error (ECE; [Guo et al. (2017)](#b15)). The SDE-BNN appears better calibrated than the Neural ODE [(Chen et al., 2018)](#b5) and mean field ResNet baselines. Figure [6b](#fig_6) shows better calibration than neural ODEs with similar accuracy. Appendix Figure [11](#) shows the insensitivity of these results to solver step size. 

## Robustness to Input Corruption

We report the robustness of SDE-BNNs by evaluating on all 19 non-adversarial corruptions across 5 severity levels in CIFAR10-C [Hendrycks and Dietterich (2019)](#b19). These corruptions mimic real-world perturbations such as noise, blur, and weather. To evaluate the classification robustness of SDE-BNN, we compare the mean corruption error (mCE), an average error for each intensity level summed across all 19 perturbations, to the top-1 error rate on the corresponding clean CIFAR-10.

Figure [7](#fig_7) shows error on the corrupted test set relative to uncorrupted data, demonstrating a steady increase in mCE across increasing perturbation levels along with the overall error measurement summarized in Table [1](#tab_1).

On both CIFAR-10 and CIFAR10-C, the SDE-BNN and SDE-BNN + STL models achieve lower overall test error and better calibration than the baselines.

Compared to standard baselines (ResNet32 and Mean Field (MF) ResNet32), SDE-BNN achieves around 4.4% lower absolute corruption error (CE), the total classification error for all corruption tasks across all 5 severity levels [(Hendrycks and Dietterich, 2019)](#b19), in comparison to the clean errors. The effectiveness of learned uncertainty on out-of-domain inputs indicates that SDE-BNN is more robust to observation noise despite not being trained on such diverse forms of corruptions.

Computational speed The cost of evaluating our model grows in O(DT ), where D is the number of weights, and T the number of iterations taken by the solver. This may seem advantageous compared to the O(D 3 ) cost for non-factorized Gaussian approximate posteriors, but the number of steps required is difficult to characterize. Although our approach allows adjustment of the computational cost at test time, it is harder to control the cost of evaluation during training time, making our method relatively slow to train. However, it should be straightforward to regularize these models to be faster to solve, as in [Kelly et al. (2020)](#b24). Relatedly, [Dusenberry et al. (2020)](#b11) recently demonstrated an O(DK) cost approximate posterior in standard BNNs.

## Batch norm

We did not incorporate batch normalization [(Ioffe and Szegedy, 2015)](#b22) in any of our neural network components. Introducing any normalization (e.g. batchnorm, layernorm, etc.) compromises the Lipschitz property required for SDEs to have a unique solution. Since BN introduces dependence between samples within a batch, it is also unclear how to incorporate BN while maintaining the consistency properties of Bayesian inference. [Zhang et al. (2019a)](#b53); [Chang et al. (2020)](#b4) proposed initializations that yield the same performance without needing batch normalization.

Low-variance gradients for other domains Our extended STL gradient estimator [(Roeder et al., 2017)](#b43) to the infinite-dimensional variational objective could be applied to other settings for faster convergence, e.g. time series applications [Li et al. (2020)](#b31) investigated.

## RELATED WORK

Initial theoretical investigations The earliest theoretical treatment of infinitely-deep Bayesian neural networks was made by Neal (1996, Chapter 2), but no practical training or evaluation method was proposed. [Duvenaud et al. (2014)](#b12) also investigated the theoretical properties of kernel-based constructions of infinitely-deep Bayesian neural networks.

## Diffusion limits of discrete-time models

We expect existing discrete-depth constructions to converge to diffusion limits in the infinitesimal limit if a system is updated with appropriately scaled Gaussian noise at each timestep. [Peluchetti and Favaro (2020b,a)](#) show this holds for the output of residual networks with shallow residual blocks whose weight initializations are appropriately scaled. While our construction of SDE-BNN given by equation 11 seems similar Peluchetti and Favaro (2020b), there are two key differences: (i)

We strictly enforce hidden states to follow a diffusion throughout training by directly learning a neural SDE, whereas [Peluchetti and Favaro (2020b)](#b41) only ensures SDE-driven dynamics at initialization. (ii) We adopt a more general neural net architecture for the residual blocks than the shallow ones considered in [(Peluchetti and Favaro, 2020b)](#b41). Their work mainly discusses the convergence of shallow ResNets to SDEs, in order to analyze training stability for regular ResNets and verify that a scaled gradient formulation leads to faster convergence at the first epoch. The consequence of (i) is that operations on diffusions (e.g., computing path-space KL) remain applicable even after our model has been trained. While (ii) appears to be a minor difference, it actually uncovers a fundamental distinction in our analysis: Since we start out with an SDE, and only discretize for numerical computations, our model is able to incorporate any type of Lipschitz smooth residual block. Additionally, no training algorithm was specified for learning SDE models. The analysis by [Peluchetti and Favaro (2020b)](#b41) relies on Taylor expanding the residual block function, which is not easy in the presence of complex residual block architectures and would require modifications to the initialization. [Tzen and Raginsky (2019a)](#b46) show that particle trajectories of the approximate posterior in discrete deep latent Gaussian models converge to a diffusion, and that the ELBO may be written with KL of measures on path space. This construction has been explored in various forms in the past [(Opper, 2019;](#b38)[Archambeau et al., 2008)](#b0).

Neural SDEs with other training objectives Models making use of SDEs have appeared in the past, though many make use of somewhat ad-hoc combinations of methods involving both discrete and continuous components. [Kong et al. (2020)](#b28) proposed fitting a neural SDE by using a heuristic training objective based on encouraging the diffusion to be large away from the training data and a fixed Euler-Maruyama (E-M) discretization. [Innes et al. (2019)](#b21) trained neural SDEs by backpropagating through the operations of the solver, however their training objective simply matched the first two moments of the training data, implying that it could not consistently estimate diffusion functions. This approach is also relatively memory-intensive. [Liu et al. (2019)](#b32) and [Oganesyan et al. (2020)](#b37) add noise to the solver operations in a neural ODE, although the diffusion must be tuned as a hyperparameter. [Hegde et al. (2018)](#b18) proposed a form of neural SDE using Gaussian processes to parameterize the drift and diffusion functions for a fixed E-M discretization. However, the diffusion functions are based on an ad-hoc construction from a Gaussian process posterior conditioned on inducing points. Ryder et al. ( [2018](#)) used a Gaussian process variational posterior, effectively a continuoustime analog of a mean field approximation that may not always be expressive enough to model the true posterior. [Kidger et al. (2021)](#b25) learn neural SDEs by jointly learning a discriminator [(Kidger et al., 2020)](#b26) and formalize the problem as learning generative adversarial networks. However, this would involve many more hyperparameters and require extensive tuning compared to our variational inference approach.

ODEnets with finite-dimensional stochasticity Some methods based on building variational autoencoders with a neural ODE share similar training objectives, since the ELBO appears frequently in posterior inference. The Latent ODE model [(Rubanova et al., 2019)](#b44) only performs inference on the distribution at an initial time of a continuous hidden state. De Brouwer et al. (2019) introduced stochastic jumps at data locations, and do not perform continuous-time inference. While performing amortized inference for time series modeling, Yıldız et al. (2019) also infer the weights of an ODE drift function. Dandekar et al. (2020) have a similar setting but for supervised learning.

Approximate posteriors defined as neural nets [Krueger et al. (2018)](#b30) and [Louizos and Welling (2017)](#b34) use normalizing flows to construct an unfactorized, non-Gaussian approximate posterior in BNNs. However, normalizing flows have poor scaling with dimension and point estimates were used for most of the weights in the neural network. Table [5](#) in Appendix 5 compares qualities of our approach to existing methods for stochastic variational inference in BNNs.

## CONCLUSION

We developed a practical method for approximate inference in continuous-depth Bayesian neural networks. Our approach exploits a special synergy between continuous-depth models and variational inference for SDEs, providing additional benefits over standard approaches. In particular, our method allows arbitrarily-expressive, non-factorized approximate posteriors implicitly defined through neural SDEs. We also developed an unbiased gradient estimator for SDE variational inference whose variance approaches zero as the approximate posterior approaches the true posterior. This combination gives our family of Bayesian continuous-depth neural networks a special property, which is that the gradients' bias and variance can be made arbitrarily small during training. Where standard applications of MFVI on continuous-depth models perform poorly, our approach brings continuous-depth Bayesian neural networks to a comparable performance with standard Bayesian neural networks. Furthermore, we demonstrated the ability of this continuous-depth model class to use adaptive SDE solvers. This allows a memory-efficient training, and a fine-grained trade-off between precision and speed.

## Supplementary Material: Infinitely Deep Bayesian Neural Networks with Stochastic Differential Equations

## A PROOFS

Notation. Denote as φ the vector of variational parameters, f q as the approximate posterior on weights, f p as the prior on weights, f h as the dynamics of hidden units, and σ as the diffusion function. Denote the Euclidean norm of a vector u by |u|. For function f denote its Jacobian as ∇f .

## A.1 Derivation of an Alternative Monte Carlo Estimator

The goal of this section is to derive a Monte Carlo estimator of the KL-divergence on path space that is similar to the fully Monte Carlo estimator described in [Roeder et al. (2017)](#b43). This will serve as the basis for the subsequent heuristic derivation of the continuous-time sticking-the-landing trick.

Let w 0 be a fixed initial state. Let w 1 , ..., w N be states at times ∆t, 2∆t, . . . , N ∆t = T generated by the Euler discretization:

$w i+1 = w i + f q (w i )∆t + σ(w i )(B t+∆t -B t )(13)$$= w i + f q (w i )∆t + σ(w i )∆t 1/2 i+1 , i+1 ∼ N (0, 1).(14)$where {B t } t≥0 is the Brownian motion. This implies that conditional on the previous state, the current state is normally distributed:

$w i+1 |w i ∼ N (w i + f q (w i )∆t, σ(w i ) 2 ∆t).$Thus, the log-densities can be evaluated as

$log q(w i+1 |w i ) = - 1 2 log(2πσ(w i ) 2 ∆t) - 1 2 (w i+1 -(w i + f q (w i )∆t)) 2 σ(w i ) 2 ∆t , i = 0, . . . N -1.(15)$On the other hand, if at any time, the next state was generated from the current state based on the prior process, we would have the following log-densities:

$log p(w i+1 |w i ) = - 1 2 log(2πσ(w i ) 2 ∆t) - 1 2 (w i+1 -(w i + f p (w i )∆t)) 2 σ(w i ) 2 ∆t , i = 0, . . . N -1.(16)$Now, we substitute the form of w i+1 based on equation 13 into equation 15 and equation 16 and obtain

$log q(w i+1 |w i ) = - 1 2 log(2πσ(w i ) 2 ∆t) - 1 2 2 i+1 , log p(w i+1 |w i ) = - 1 2 log(2πσ(w i ) 2 ∆t) - 1 2 (f q (w i ) -f p (w i )) 2 σ(w i ) 2 ∆t + 2(f q (w i ) -f p (w i )) i+1 σ(w i ) ∆t 1/2 + 2 i+1 .$The KL divergence on the path space could then be regarded as a sum of infinitely many KL-divergences between E wi E wi+1∼q(wi+1|wi) log q(w i+1 |w i ) p(w i+1 |w i )

= lim

$N →∞ N i=0 E wi E i+1 (f q (w i ) -f p (w i )) 2 2σ(w i ) 2 ∆t + (f q (w i ) -f p (w i )) σ(w i ) ∆t 1/2 i+1 (19) = E 1 2 T 0 |u t | 2 dt + T 0 u t dB t .(20)$A.2 Sticking-the-landing in Continuous Time

For a non-sequential latent variable model, the sticking-the-landing (STL) trick removes from the fully Monte Carlo ELBO estimator a score function term of the form ∂ log q(w, φ)/∂φ, where w is sampled using the reparameterization trick and may depend on φ. The score function term has 0 expectation, but may affect the variance of the gradient estimator for the inference distribution's parameters.

Here, we exploit this intuition and apply it to each step before taking the limit. More precisely, we apply the STL trick to estimate the gradient of D KL (q(w i+1 |w i )||p(w i+1 |w i )) for i = 1, 2, . . . , N , and thereafter take the limit as the mesh size of the discretization goes to 0. For each individual term, the score function term to be removed is

$∂ ∂φ log q(w i+1 |w i , φ) = - 1 2σ 2 (w i )∆t ∂ ∂φ (w i+1 -(w i + f q (w i , φ)∆t)) 2 = ∂ ∂φ f q (w i , φ) σ(w i ) i+1 ∆t 1/2 .$Now, we sum up all of these terms and take the limit as ∆t → 0. This gives us u(w t , t, stop_gradient(φ)) dB t , w(•) ∼ q φ ().   Figure [9d](#fig_11) indicates that the accuracy of predictions is relatively consistent across all severity levels with the SDE-BNN and SDE-BNN + STL models having relatively better calibrated predictions.

## C.3 Sticking the Landing Results

![Proceedings of the 25 th International Conference on Artificial Intelligence and Statistics (AISTATS) 2022, Valencia, Spain. PMLR: Volume 151. Copyright 2022 by the author(s).]()

![Figure 2: Predictive prior and posterior of the SDE-BNN on a non-monotonic toy dataset. Blue areas indicate density percentiles, and distinct colored lines show model samples.]()

![Figure 4: Comparison of the variance in three gradient estimators. On this toy problem, our new gradient estimator reduces variance by a factor of roughly 4.]()

![Figure 5: Benchmarking two gradient computation methods: (1) Back-propagation through the SDE solver, and (2) the memory-efficient stochastic adjoint of Li et al. (2020). Both methods have similar optimization dynamics, final performance, and wall-clock time, but the adjoint approach is more memory-efficient. Detailed comparisons of wall-clock time and evaluation step results in Appendix C.8.]()

![CIFAR-10. Left: Negative log likelihood. Right: ECE. Adjusting SDE-BNN solver tolerance at test time trades off computational speed for predictive performance. Grey line is solver's training tolerance. Averaged across 3 seeds. Calibration on the CIFAR-10 test set for a neural ODE (left) and a SDE-BNN (right). The SDE-BNN displays better calibration and generalization.]()

![Figure 6: Performance of SDE-BNN on standard CIFAR-10 classification task.]()

![Figure 7: CIFAR10-C. Robustness to distributional shifts on CIFAR-10. SDE-based neural nets show better accuracy and calibration than non-Bayesian and meanfield methods. Black bars show standard deviation over 3 seeds.]()

![wi [D KL (q(w i+1 |w i )||p(w i+1 |w i ))]]()

![] dB t .Removing this term from the fully Monte Carlo estimator in equation 20 gives rise to the following estimator of a surrogate objective that facilitates implementation: ELBO = log p(D | w) -]()

![Figure9: Figures9a-9cshow that the SDE BNN and SDE BNN + STL models outperform their non-continuous depth ResNet counterparts on all three robustness metrics when evaluated on the corrupt CIFAR-10C benchmarks. Figure9dindicates that the accuracy of predictions is relatively consistent across all severity levels with the SDE-BNN and SDE-BNN + STL models having relatively better calibrated predictions.]()

![Classification accuracy and expected calibration error (ECE) on MNIST and CIFAR-10. We separate models into point estimates, discrete-time models, and continuous-time models. Our SDE-BNN outperforms other continuous-time Bayesian neural nets (BNNs) and perform competitively against discrete-time BNNs. † Results byIzmailov et al. (2021) where a modified residual network architecture was used; only one seed was reported.]()

![Training with STL estimator on CIFAR-10 shows training time improvements in accuracy, negative log likelihood, and ELBO objective in addition to reducing variance. This improvement to the standard gradient estimator can be especially useful in settings where the approximate posterior is sufficiently flexible (i.e. the drift neural net is very large relative to the state size).]()

