- Choice of continuous-depth parameterization for neural networks
- Decision to use stochastic differential equations (SDEs) for weight uncertainty
- Selection of Ornstein-Uhlenbeck process as prior for weights
- Implementation of gradient-based stochastic variational inference
- Design of a novel zero-variance gradient estimator
- Trade-off between computational cost and precision in adaptive computation
- Use of black-box adaptive SDE solvers for output layer computation
- Decision to model uncertainty in both parameters and observations
- Choice of variational family for approximate posteriors
- Integration of time-correlated weights in the network architecture
- Approach to model averaging for combating overfitting
- Decision to run dynamics backwards for memory-efficient training
- Selection of prior distribution for model weights
- Implementation of variational inference in the context of SDEs
- Design considerations for the architecture of the neural network
- Decision to use Monte Carlo methods for estimating gradients
- Choice of loss function for optimizing the Evidence Lower Bound (ELBO)
- Decision to compare continuous-depth Bayesian neural networks with discrete-depth alternatives
- Consideration of empirical performance of Gaussian approximate posteriors
- Decision to focus on scalability of approximate inference methods