<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Infinitely Deep Bayesian Neural Networks with Stochastic Differential Equations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Winnie</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Toronto University of Toronto Stanford Unviersity University of Toronto</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ricky</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Toronto University of Toronto Stanford Unviersity University of Toronto</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xuechen</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Toronto University of Toronto Stanford Unviersity University of Toronto</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Duvenaud</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Toronto University of Toronto Stanford Unviersity University of Toronto</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Infinitely Deep Bayesian Neural Networks with Stochastic Differential Equations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">24CD3A57732399F9E20A6C6BDF5F6454</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-28T01:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We perform scalable approximate inference in continuous-depth Bayesian neural networks. In this model class, uncertainty about separate weights in each layer gives hidden units that follow a stochastic differential equation. We demonstrate gradient-based stochastic variational inference in this infinite-parameter setting, producing arbitrarily-flexible approximate posteriors. We also derive a novel gradient estimator that approaches zero variance as the approximate posterior over weights approaches the true posterior. This approach brings continuous-depth Bayesian neural nets to a competitive comparison against discrete-depth alternatives, while inheriting the memory-efficient training and tunable precision of Neural ODEs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Taking the limit of neural networks to be the composition of infinitely many residual layers provides a way to implicitly define its output as the solution to an ODE <ref type="bibr" target="#b17">(Haber and Ruthotto, 2017;</ref><ref type="bibr">E, 2017)</ref>. This continuous-depth parameterization decouples the specification of the model from its computation. While the paradigm adds complexity, it has several benefits:</p><p>(1) Computational cost can be traded for precision in a fine-grained manner by specifying error tolerances for adaptive computation, and (2) memory costs for training can be significantly reduced by running the dynamics backwards in time to reconstruct activations of intermediate states needed for backpropagation.</p><p>On the other hand, the Bayesian treatment for neural networks modifies the typical training pipeline where  Figure <ref type="figure">1</ref>: Hidden unit trajectories in an ODE-Net and an SDE-BNN. Left: A continuous-depth residual network has deterministic transformations of its hidden units from depths t = 0 to t = 1. Right: Uncertainty in the weights of a Bayesian continuous-depth residual network implies uncertainty in its hidden unit activation trajectories. Shaded regions show densities over samples from the learned posterior dynamics. Both: Each distinct color corresponds to a different initial state corresponding to different data inputs. instead of performing point estimates, a distribution over parameters is inferred. Although this approach adds complexity, it automatically accounts for model uncertainty. In turn, model averaging can be done to combat overfitting and improve calibration, especially on out-of-distribution data <ref type="bibr" target="#b52">(Zhang et al., 2018;</ref><ref type="bibr" target="#b39">Osawa et al., 2019)</ref>.</p><p>How can we combine the benefits of continuous-depth models with those of Bayesian neural networks? The simplest approach is a "Bayesian neural ODE" <ref type="bibr" target="#b50">(Yıldız et al., 2019;</ref><ref type="bibr" target="#b6">Dandekar et al., 2020)</ref>, which integrates out the finitely-many parameters of a standard neural ODE for prediction. This approach is straightforward to implement, and can inherit the advantages of both Bayesian and continuousdepth neural nets. However, empirically, standard Gaussian approximate posteriors are a relatively poor arXiv:2102.06559v4 [stat.ML] 30 Jan 2022 match for neural ODEs, not to mention the drawbacks of also being used in the prior. Additionally, it does not exploit the special synergy available between continuous-time models and approximate inference.</p><p>In this paper, we show that an alternative construction of Bayesian continuous-depth neural networks has additional practical benefits. Specifically, we consider the limit of infinite-depth Bayesian neural networks with separate unknown weights at each layer, a model class that we refer to as SDE-BNNs. We develop a unique network architecture that enhances model expressivity through time-correlated weights and scales linearly, instead of quadratically, with the parameter dimensionality. Combined with our novel formulation of a zerovariance gradient estimator, we show that approximate inference can be realized through the maximization of our modified variational lower bound, effectively scaling up the gradient-based variational inference scheme described by <ref type="bibr" target="#b31">Li et al. (2020)</ref> (preliminary forms of which appeared in earlier works <ref type="bibr" target="#b0">(Archambeau et al., 2008;</ref><ref type="bibr" target="#b38">Opper, 2019;</ref><ref type="bibr" target="#b46">Tzen and Raginsky, 2019a)</ref>).</p><p>With this approach, the state of the output layer is computed by a black-box adaptive SDE solver. Figure 1 contrasts our neural SDE with the neural ODE parameterization. This approach maintains the adaptive computation and constant-memory cost of training Bayesian neural ODEs and adds two unique benefits:</p><p>• The variational posterior can be made arbitrarily expressive by simply enlarging the neural network that parameterizes the dynamics of the approximate posterior. Under mild conditions, this approach can approximate the true posterior arbitrarily closely.</p><p>• The variational objective admits a variance-reduced gradient estimator that is a natural extension of the "sticking the landing" trick <ref type="bibr" target="#b43">(Roeder et al., 2017)</ref>.</p><p>Combined with arbitrarily expressive approximate posteriors, it is consistent and has vanishing variance as the approximate posterior approaches the true.</p><p>Notably, our low-variance gradient estimator can also be applied to variational inference in SDEs more generally, such as for time-series modeling, but such applications are beyond the scope of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>Bayesian Neural Networks Given a dataset, there are often many functions that fit the data well, which a given neural network can express with different parameter values. Instead of making point estimates of the parameters, the Bayesian paradigm frames learning as posterior inference. Predictions are obtained through integrating over many possible parameter settings. Formally, given a dataset D = {(x i , y i )} N i=1 and prior distribution over model weights p(w), we want to compute a posterior p(w|D) ∝ p(D|w)p(w). We can optimize an approximate posterior distribution q(w) that minimizes the Kullback-Leibler (KL) divergence, i.e. maximizing the Evidence Lower Bound (ELBO):</p><formula xml:id="formula_0">L ELBO (φ) =E q(w) [log p(D|w)] -D KL (q(w)||p(w)) .</formula><p>(1)</p><p>Estimating gradients of this objective using simple Monte Carlo is known as stochastic variational inference (SVI) <ref type="bibr" target="#b20">(Hoffman et al., 2013;</ref><ref type="bibr" target="#b42">Rezende et al., 2014)</ref>).</p><p>One of the main technical challenges of SVI is choosing a parametric family of approximate posteriors that is tractable to sample from and evaluate, while being flexible enough to approximate the true posterior well. Most scalable inference techniques use Gaussian approximate posteriors with restricted covariance structure between network parameters <ref type="bibr" target="#b14">(Graves, 2011;</ref><ref type="bibr" target="#b2">Blundell et al., 2015;</ref><ref type="bibr" target="#b52">Zhang et al., 2018;</ref><ref type="bibr" target="#b35">Mishkin et al., 2018)</ref>.</p><p>Others construct complex approximate posteriors with normalizing flows <ref type="bibr" target="#b30">(Krueger et al., 2018;</ref><ref type="bibr" target="#b34">Louizos and Welling, 2017)</ref> or through distillation <ref type="bibr" target="#b1">(Balan et al., 2015;</ref><ref type="bibr" target="#b48">Wang et al., 2018)</ref>.</p><p>Neural Ordinary Differential Equations Neural ordinary differential equations <ref type="bibr" target="#b5">(Chen et al., 2018)</ref> define ODEs using neural networks:</p><formula xml:id="formula_1">dh t = f θ (h t , t) dt, h 0 ∈ R d ,<label>(2)</label></formula><p>where f :</p><formula xml:id="formula_2">R d × R → R d is</formula><p>a Lipschitz function defined by a neural network with parameters θ. Starting at an initial value h 0 = x given by a data example and z(t) t0 t1 5.0 2.5 0.0 2.5 5.0 s(t) t z(t 1 ) 8 6 4 2 0 2 4 6 8 6 4 2 0 2 4 6 z(t 0 ) Figure 3: Neural SDEs can learn arbitrarily expressive approximate posteriors. Left: Samples from an approximate posterior, trained with an OU prior and conditioned on two observations with Cauchy likelihoods. Right: Joint distribution and marginals of the approximate posterior process z at times t 0 and t 1 .</p><p>integrating these dynamics forward for a finite time can be seen as passing the input through an infinitely-deep residual network. For learning scalar-valued functions, adding extra dimensions to h and a linear final layer induces similar universal approximations to standard neural networks <ref type="bibr" target="#b10">(Dupont et al., 2019;</ref><ref type="bibr" target="#b54">Zhang et al., 2019b)</ref> trained by standard stochastic gradient descent methods. Using adaptive ODE solvers can trade evaluation speed for precision. The adjoint sensitivity method saves memory during training through reconstructing the trajectory of the hidden units h by running the dynamics backwards in backpropagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Latent Stochastic Differential Equations</head><p>Informally, an SDE can be viewed as an ODE with infinitesimal noise added throughout time. Formally:</p><formula xml:id="formula_3">dw t = f θ (w t , t) dt + g θ (w t , t) dB t ,<label>(3)</label></formula><p>where w 0 ∈ R d is the initial state, f θ : R d ×R → R d and g θ : R d × R → R d×m are functions Lipschitz in both arguments, dubbed the drift and diffusion, respectively, and {B t } is an m-dimensional Brownian motion.</p><p>Some works have considered training SDEs with dynamics parameterized by neural networks <ref type="bibr" target="#b31">(Li et al., 2020;</ref><ref type="bibr" target="#b46">Tzen and Raginsky, 2019a;</ref><ref type="bibr" target="#b41">Peluchetti and Favaro, 2020b;</ref><ref type="bibr" target="#b21">Innes et al., 2019;</ref><ref type="bibr" target="#b28">Kong et al., 2020;</ref><ref type="bibr" target="#b32">Liu et al., 2019)</ref>. Note that directly optimizing the drift and diffusion to maximize the average log-likelihood of an observation log p(y t |w t ) would result in the diffusion approaching 0, conditional on the ODE fitting the training data well with the diffusion somewhat unconstrained.</p><p>Instead of directly optimizing the parameters of an SDE to match the data, a better approach is to use an SDE to define a prior over trajectories of w, and optimize the marginal likelihood of the data, integrating over all trajectories of w weighted by the prior. Luckily, we can specify an approximate posterior over trajectories using a second SDE. We define the approximate posterior by</p><formula xml:id="formula_4">dw t = f φ (w t , t) dt + g θ (w t , t) dB t .<label>(4)</label></formula><p>When the dynamics of the approximate posterior f φ is parameterized by a neural network, this family of posteriors is extremely expressive. Figure <ref type="figure">3</ref> shows that such a variational family can easily approximate non-Gaussian and multi-modal posteriors on path space.</p><p>If both the SDE defined by equation 3 and equation 4 share the same diffusion function, then the KL between the two induced measures on path space has the following form <ref type="bibr" target="#b31">(Li et al., 2020;</ref><ref type="bibr" target="#b46">Tzen and Raginsky, 2019a)</ref>:</p><formula xml:id="formula_5">D KL (µ q ||µ p ) = E q φ (w) 1 0 1 2 u(t, φ)</formula><p>2 2 dt where ( <ref type="formula">5</ref>)</p><formula xml:id="formula_6">u(t, φ) = g θ (w t , t) -1 [f θ (w t , t) -f φ (w t , t)] (6)</formula><p>where µ q and µ p are path space probability measures induced respectively by equation 4 and equation 3, and the expectation is taken under the approximate posterior, denoted q φ (w). Intuitively, this KL divergence resembles the summative difference over time horizon <ref type="bibr">[0,</ref><ref type="bibr">1]</ref> between the prior drift f θ and f φ , scaled by the diffusion. This divergence can be estimated up to a constant with simple Monte Carlo, sampling trajectories from the dynamics given by the approximate posterior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SDEs as expressive approximate posteriors</head><p>To ensure that the KL divergence between the prior and approximate posterior on path space is finite, the same diffusion function g θ (w t , t) must be used for the approximate posterior and prior. Surprisingly, this does not limit the expressivity of the approximate posterior. <ref type="bibr" target="#b3">Boué et al. (1998)</ref> show that there is a one-to-one correspondence between the space of path measures and drift functions that result in the same path space KL divergence. This implies that any path space measure close to the true posterior can be instantiated by SDEs with appropriate drifts. It follows that an approximate posterior parameterized by a sufficiently expressive family of function approximators can be made arbitrarily close to the true posterior. The Girsanov reparameterization of the variational formula, derived from Boue (Tzen and Raginsky, 2019a, Section 4), proves that the ELBO is tight when the drift is optimal. This means that there exists a ground truth drift function that can make the ELBO tight, the approximation of which can be achieved with a high capacity neural network.</p><p>Standard discrete-depth residual networks can be defined as a composition of layers of the form:</p><formula xml:id="formula_7">h t+ = h t + f (h t , w t ), t = 1 . . . T, (<label>7</label></formula><formula xml:id="formula_8">)</formula><p>where t is the layer index, h t ∈ R D h denotes a vector of hidden unit activations at layer t, the input h 0 = x, and w t ∈ R Dw represents the parameters for layer t.</p><p>In the discrete setting, = 1, ∈ R.</p><p>We can construct a continuous-depth variant of residual networks by setting = 1 /T and taking the limit as T → ∞. This yields a differential equation that describes the hidden unit evolution as a function of depth t.</p><p>Since standard residual networks are parameterized with different layerwise "weights", we denote them w t .</p><p>To specify different weights at each layer with a finite number of parameters, we introduce a hypernetwork f w that specifies the change in weights as a function of depth and the current weights <ref type="bibr" target="#b16">(Ha et al., 2016)</ref>. The evolution of the hidden unit activations and weights can then be combined into a single differential equation:</p><formula xml:id="formula_9">d dt h t w t = f h (t, h t , w t ) f w (t, w t )<label>(8)</label></formula><p>with some learned initial weight value w t0 . Using time-varying weights is similar to augmenting the state <ref type="bibr" target="#b10">(Dupont et al., 2019;</ref><ref type="bibr" target="#b55">Zhang et al., 2019c)</ref>. See Appendix Figure <ref type="figure">8</ref> on the effects of augmentation. We perform Bayesian inference on the weight process w t , assigning a suitable prior stochastic process and performing variational inference in this infinitesimal limit.</p><p>Like all Bayesian neural networks with observation likelihoods, our framework models uncertainty both about parameters and about individual observations:</p><p>The likelihood p(y|h 1 ) captures the observational noise, while the SDE encodes weight uncertainty.</p><p>Prior process on weights Typical priors for Bayesian neural networks are independent Gaussians across all weights and layers. Taking the infinitesimal limit of such a prior gives a white noise process prior on the weights w(•). However, initializing this noise while maintaining finite variance at scale is difficult <ref type="bibr">(Peluchetti and Favaro, 2020a,b)</ref>.</p><p>Instead, we use the Ornstein-Uhlenbeck (OU) process as the prior on weights. The process is characterized by an SDE with drift and diffusion:</p><formula xml:id="formula_10">f p (w t , t) = -w t , g(w t , t) = σI d ,<label>(9)</label></formula><p>respectively, where σ is a hyperparameter. We choose this prior for its simplicity and bounded marginal variance at a constant in the large time limit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approximate posterior over weights</head><p>We parameterize the approximate posterior on weights implicitly using another SDE with the following drift function:</p><formula xml:id="formula_11">f q (w t , t, φ) = NN φ (w t , t, φ) -f p (w t , t).<label>(10)</label></formula><p>This drift f q is parameterized by a small neural network (NN) with parameters φ. With this drift, the approximate posterior process will generally have non-Gaussian, non-factorized marginals; its expressive capacity can be increased by making the neural net larger.</p><p>Evaluating the network Given an input, we marginalize over weight and hidden unit trajectories. This can be done with simple Monte Carlo, sampling a weight path {w t } from the posterior process and evaluating the network activations {h t } given the sampled weights and input. Both steps require solving a differential equation. Luckily, both can be solved simultaneously with the augmented state SDE:</p><formula xml:id="formula_12">d w t h t = f w (w t , t, φ) f h (h t , t, w t ) dt + g w (w t , t) 0 dB t ,<label>(11)</label></formula><p>where h 0 = x, the input. The learnable parameters are the initial weight values at time zero w 0 (either point estimated or inferred) and those of drift function φ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output likelihood</head><p>The final state of the hidden units h 1 is used to parameterize the likelihood of the target output y: log p(y|x, w) = log p(y|h 1 ). For instance, p(y|h 1 ) could be a Cauchy likelihood for regression, or categorical likelihood for classification.</p><p>Training objective To fit the network to data, we maximize the lower bound on marginal likelihood given by the infinite-dimensional ELBO:</p><formula xml:id="formula_13">L ELBO∞ (φ) = E q φ (w) log p(D|w)- 1 0 1 2 u(w t , t, φ) 2 2 dt .</formula><p>The sampled weights, the hidden activations, and the training objective are all computed simultaneously with a single call to an adaptive SDE solver. Gradients of the sampled loss can also be efficiently computed using adaptive solvers, following <ref type="bibr" target="#b31">Li et al. (2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">VARIANCE-REDUCED GRADIENTS</head><p>Roeder et al. <ref type="bibr">(2017)</ref> showed that when optimizing expectations using the reparameterization gradient, a gradient estimator with lower variance can be constructed by removing a score function term that has zero expectation, and that the variance of this gradient estimator approaches zero as the approximate posterior approaches the true posterior. We refer to this trick as "sticking the landing" (STL). We generalize this to our SDE setting by replacing the original estimator of the path space KL with the following STL estimator:</p><formula xml:id="formula_14">KL STL = 1 0 1 2 u(w t , t, φ) 2 2 dt+ 1 0 u(w t , t, ⊥(φ)) dB t ,<label>(12)</label></formula><p>where w(•) ∼ q φ (w) and u is defined in equation 6, the path {w t } t∈[0,T ] is sampled from the approximate posterior process, and ⊥(•) is the stop gradient function that renders the input a constant with respect to which gradient propagation is stopped. Note that 12 is the fully Monte Carlo version referred to in 4 from which our STL variant is derived.</p><p>The second term in equation 12 is a martingale and has expectation zero. Therefore, in prior works <ref type="bibr" target="#b31">(Li et al., 2020;</ref><ref type="bibr">Tzen and Raginsky, 2019a,b)</ref>, Monte Carlo estimation was only performed for the first term, but we find that this approach does not necessarily reduce the variance of the gradient (Figure <ref type="figure" target="#fig_3">4</ref>).</p><p>Because our approximate posterior can be made arbitrarily expressive, we conjecture that our approach can achieve arbitrarily low gradient variance towards the end of training if the f w parameterization is expressive enough. See Appendix is A.2 for a heuristic derivation.</p><p>We show the variance of different gradient estimators in Figure <ref type="figure" target="#fig_3">4</ref>, averaged across the parameters θ, in a 1D regression setting. We compare STL against a "Full Monte Carlo" estimate which includes the second additional term without gradient stopping, as well as the estimator that was previously used by <ref type="bibr" target="#b31">Li et al. (2020)</ref> which ignores the second term. Figure <ref type="figure" target="#fig_3">4</ref> shows that STL obtains lower variance than alternatives, when matching an exponentiated Brownian motion. Table <ref type="table" target="#tab_3">4</ref> shows training performance improvements. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>We investigate the effectiveness of our proposed approximate inference method for training continuous-depth neural nets, referred to as SDE-BNN, in terms of classification accuracy, calibration, perturbation robustness, and speed-precision trade-offs. Our code is publicly available here and experimental settings in Table <ref type="table">2</ref>.</p><p>We consider toy regression and image classification tasks on MNIST and CIFAR-10. We also investigate out-of-distribution generalization. Notably, our approach does not require post hoc recalibration methods such as training with temperature scaling <ref type="bibr" target="#b15">(Guo et al., 2017)</ref> or isotonic regression <ref type="bibr" target="#b51">(Zadrozny and Elkan, 2002)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Backpropagation through solvers vs. adjoint</head><p>We experimented with fixed-and adaptive-step SDE solvers, and the stochastic adjoint of <ref type="bibr" target="#b31">Li et al. (2020)</ref>.</p><p>Figure <ref type="figure" target="#fig_4">5</ref> shows similar convergence for both approaches.</p><p>Appendix C shows that both had similar numbers of dynamics function evaluations and wall-clock time.</p><p>The overhead for estimating error in our adaptive solvers was substantial; therefore, for final model evaluation, we trained with fixed-step solvers, where the number of steps is chosen to be large enough to match the convergence speed of our adaptive-step solvers.</p><p>Baselines For a fixed-depth network baseline, we compare to standard residual networks. We then test variational inference on the weights of these models. We also perform ablation studies to compare with standard variational inference approaches over continuousdepth networks. Specifically, we compare to a mean field variational inference (MFVI) ODEnet where stochastic variational inference is performed over depthinvariant weights. This baseline is a fully-factorized Gaussian approximate posterior, i.e. mean-field approximation, and been used for Neural ODEs by <ref type="bibr" target="#b33">Look and Kandemir (2019)</ref>; <ref type="bibr" target="#b6">Dandekar et al. (2020)</ref>.</p><p>We further compare our model to a MFVI HyperO-DEnet, where a learned drift is applied to w, but meanfield inference is instead performed over the parameters of the hypernetwork. Alternatively, one can interpret this as another MFVI ODEnet with a larger state and a more complex drift function but with similar computational complexity to SDE-BNN. This setting contrasts our approach of doing Bayesian inference over the entire continuous-depth network as a stochastic process.</p><p>Parameterizing the drift function We parameterized the drift of the variational posterior f w with a simple multilayer perceptron. To ensure optimization starts at a stable set of dynamics, we subtract the prior drift so that the approximate posterior equals the prior when the final layer is initialized to output zero.</p><p>Hyperparameters We swept learning rates in the range [1e-4, 1e-3], selecting the optimal based on the validation set. We train with the default Adam optimizer <ref type="bibr" target="#b27">(Kingma and Ba, 2015)</ref>. In image classification experiments, all convolutional layers of the drift network are time-conditional and use the tanh nonlinearity. The diffusion coefficient σ was selected from validation performance over {0.1, 0.2, 0.5}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">1D Regression</head><p>We first verify the capabilities of the SDE-BNN on a 1D regression problem. Conditioned on a sample from the diffusion process, each sample from a onedimensional SDE-BNN is a bijective mapping from the inputs to the outputs. This implies that every function sampled from a 1D SDE-BNN is monotonic.</p><p>To be able to sample non-monotonic functions, we augment the state with 2 extra dimensions initialized to zero, as in <ref type="bibr" target="#b10">Dupont et al. (2019)</ref>. Figure <ref type="figure" target="#fig_2">2</ref> shows that our model learns a reasonably flexible approximate posterior on a synthetic non-monotonic 1D dataset. We emphasize that the samples from our model are smooth w.r.t. depth because the hidden states h do not receive additive instantaneous noise, only the weights w do.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Image Classification</head><p>Instantaneous changes to the hidden state (f h ) are parameterized using a convolutional neural network, including one strided convolution for downsampling and a transposed convolution layer for upsampling. We then set the w to be the filters and biases of all the convolutional layers. The approximate posterior drift dynamics (f w ) is a multilayer perceptron with hidden layer widths of 2, 128, and 2. The small hidden width of the bottleneck layers was chosen to reduce the number of variational parameters and promote linear scaling with respect to the dimension of w. On MNIST, we used one such SDE-BNN block, while on CIFAR-10, we used a multi-scale variant where multiple SDE-BNN blocks were stacked with the invertible downsampling from <ref type="bibr" target="#b9">Dinh et al. (2016)</ref> in between.</p><p>We report classification results in Table <ref type="table" target="#tab_1">1</ref>. Our SDE-  The SDE-BNN matches and outperforms the accuracy of standard residual networks on MNIST and CIFAR-10, respectively, while obtaining lower expected calibration errors (ECE). From ablation studies, we found that it was harder to achieve similar performance with either of the mean field variants of an ODEnet as they had a poorer trade-off between performance and calibration.</p><p>Figure <ref type="figure" target="#fig_6">6a</ref> shows the ability of SDE-BNNs to trade off computation time for precision. Figure <ref type="figure" target="#fig_2">12</ref> in Appendix C.4 indicates that calibration is insensitive to solver tolerances close to the value used during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Calibration</head><p>Table <ref type="table" target="#tab_1">1</ref> quantifies our model's calibration with expected calibration error (ECE; <ref type="bibr" target="#b15">Guo et al. (2017)</ref>). The SDE-BNN appears better calibrated than the Neural ODE <ref type="bibr" target="#b5">(Chen et al., 2018)</ref> and mean field ResNet baselines. Figure <ref type="figure" target="#fig_6">6b</ref> shows better calibration than neural ODEs with similar accuracy. Appendix Figure <ref type="figure">11</ref> shows the insensitivity of these results to solver step size. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Robustness to Input Corruption</head><p>We report the robustness of SDE-BNNs by evaluating on all 19 non-adversarial corruptions across 5 severity levels in CIFAR10-C <ref type="bibr" target="#b19">Hendrycks and Dietterich (2019)</ref>. These corruptions mimic real-world perturbations such as noise, blur, and weather. To evaluate the classification robustness of SDE-BNN, we compare the mean corruption error (mCE), an average error for each intensity level summed across all 19 perturbations, to the top-1 error rate on the corresponding clean CIFAR-10.</p><p>Figure <ref type="figure" target="#fig_7">7</ref> shows error on the corrupted test set relative to uncorrupted data, demonstrating a steady increase in mCE across increasing perturbation levels along with the overall error measurement summarized in Table <ref type="table" target="#tab_1">1</ref>.</p><p>On both CIFAR-10 and CIFAR10-C, the SDE-BNN and SDE-BNN + STL models achieve lower overall test error and better calibration than the baselines.</p><p>Compared to standard baselines (ResNet32 and Mean Field (MF) ResNet32), SDE-BNN achieves around 4.4% lower absolute corruption error (CE), the total classification error for all corruption tasks across all 5 severity levels <ref type="bibr" target="#b19">(Hendrycks and Dietterich, 2019)</ref>, in comparison to the clean errors. The effectiveness of learned uncertainty on out-of-domain inputs indicates that SDE-BNN is more robust to observation noise despite not being trained on such diverse forms of corruptions.</p><p>Computational speed The cost of evaluating our model grows in O(DT ), where D is the number of weights, and T the number of iterations taken by the solver. This may seem advantageous compared to the O(D 3 ) cost for non-factorized Gaussian approximate posteriors, but the number of steps required is difficult to characterize. Although our approach allows adjustment of the computational cost at test time, it is harder to control the cost of evaluation during training time, making our method relatively slow to train. However, it should be straightforward to regularize these models to be faster to solve, as in <ref type="bibr" target="#b24">Kelly et al. (2020)</ref>. Relatedly, <ref type="bibr" target="#b11">Dusenberry et al. (2020)</ref> recently demonstrated an O(DK) cost approximate posterior in standard BNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Batch norm</head><p>We did not incorporate batch normalization <ref type="bibr" target="#b22">(Ioffe and Szegedy, 2015)</ref> in any of our neural network components. Introducing any normalization (e.g. batchnorm, layernorm, etc.) compromises the Lipschitz property required for SDEs to have a unique solution. Since BN introduces dependence between samples within a batch, it is also unclear how to incorporate BN while maintaining the consistency properties of Bayesian inference. <ref type="bibr" target="#b53">Zhang et al. (2019a)</ref>; <ref type="bibr" target="#b4">Chang et al. (2020)</ref> proposed initializations that yield the same performance without needing batch normalization.</p><p>Low-variance gradients for other domains Our extended STL gradient estimator <ref type="bibr" target="#b43">(Roeder et al., 2017)</ref> to the infinite-dimensional variational objective could be applied to other settings for faster convergence, e.g. time series applications <ref type="bibr" target="#b31">Li et al. (2020)</ref> investigated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RELATED WORK</head><p>Initial theoretical investigations The earliest theoretical treatment of infinitely-deep Bayesian neural networks was made by Neal (1996, Chapter 2), but no practical training or evaluation method was proposed. <ref type="bibr" target="#b12">Duvenaud et al. (2014)</ref> also investigated the theoretical properties of kernel-based constructions of infinitely-deep Bayesian neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Diffusion limits of discrete-time models</head><p>We expect existing discrete-depth constructions to converge to diffusion limits in the infinitesimal limit if a system is updated with appropriately scaled Gaussian noise at each timestep. <ref type="bibr">Peluchetti and Favaro (2020b,a)</ref> show this holds for the output of residual networks with shallow residual blocks whose weight initializations are appropriately scaled. While our construction of SDE-BNN given by equation 11 seems similar Peluchetti and Favaro (2020b), there are two key differences: (i)</p><p>We strictly enforce hidden states to follow a diffusion throughout training by directly learning a neural SDE, whereas <ref type="bibr" target="#b41">Peluchetti and Favaro (2020b)</ref> only ensures SDE-driven dynamics at initialization. (ii) We adopt a more general neural net architecture for the residual blocks than the shallow ones considered in <ref type="bibr" target="#b41">(Peluchetti and Favaro, 2020b)</ref>. Their work mainly discusses the convergence of shallow ResNets to SDEs, in order to analyze training stability for regular ResNets and verify that a scaled gradient formulation leads to faster convergence at the first epoch. The consequence of (i) is that operations on diffusions (e.g., computing path-space KL) remain applicable even after our model has been trained. While (ii) appears to be a minor difference, it actually uncovers a fundamental distinction in our analysis: Since we start out with an SDE, and only discretize for numerical computations, our model is able to incorporate any type of Lipschitz smooth residual block. Additionally, no training algorithm was specified for learning SDE models. The analysis by <ref type="bibr" target="#b41">Peluchetti and Favaro (2020b)</ref> relies on Taylor expanding the residual block function, which is not easy in the presence of complex residual block architectures and would require modifications to the initialization. <ref type="bibr" target="#b46">Tzen and Raginsky (2019a)</ref> show that particle trajectories of the approximate posterior in discrete deep latent Gaussian models converge to a diffusion, and that the ELBO may be written with KL of measures on path space. This construction has been explored in various forms in the past <ref type="bibr" target="#b38">(Opper, 2019;</ref><ref type="bibr" target="#b0">Archambeau et al., 2008)</ref>.</p><p>Neural SDEs with other training objectives Models making use of SDEs have appeared in the past, though many make use of somewhat ad-hoc combinations of methods involving both discrete and continuous components. <ref type="bibr" target="#b28">Kong et al. (2020)</ref> proposed fitting a neural SDE by using a heuristic training objective based on encouraging the diffusion to be large away from the training data and a fixed Euler-Maruyama (E-M) discretization. <ref type="bibr" target="#b21">Innes et al. (2019)</ref> trained neural SDEs by backpropagating through the operations of the solver, however their training objective simply matched the first two moments of the training data, implying that it could not consistently estimate diffusion functions. This approach is also relatively memory-intensive. <ref type="bibr" target="#b32">Liu et al. (2019)</ref> and <ref type="bibr" target="#b37">Oganesyan et al. (2020)</ref> add noise to the solver operations in a neural ODE, although the diffusion must be tuned as a hyperparameter. <ref type="bibr" target="#b18">Hegde et al. (2018)</ref> proposed a form of neural SDE using Gaussian processes to parameterize the drift and diffusion functions for a fixed E-M discretization. However, the diffusion functions are based on an ad-hoc construction from a Gaussian process posterior conditioned on inducing points. Ryder et al. ( <ref type="formula">2018</ref>) used a Gaussian process variational posterior, effectively a continuoustime analog of a mean field approximation that may not always be expressive enough to model the true posterior. <ref type="bibr" target="#b25">Kidger et al. (2021)</ref> learn neural SDEs by jointly learning a discriminator <ref type="bibr" target="#b26">(Kidger et al., 2020)</ref> and formalize the problem as learning generative adversarial networks. However, this would involve many more hyperparameters and require extensive tuning compared to our variational inference approach.</p><p>ODEnets with finite-dimensional stochasticity Some methods based on building variational autoencoders with a neural ODE share similar training objectives, since the ELBO appears frequently in posterior inference. The Latent ODE model <ref type="bibr" target="#b44">(Rubanova et al., 2019)</ref> only performs inference on the distribution at an initial time of a continuous hidden state. De Brouwer et al. (2019) introduced stochastic jumps at data locations, and do not perform continuous-time inference. While performing amortized inference for time series modeling, Yıldız et al. (2019) also infer the weights of an ODE drift function. Dandekar et al. (2020) have a similar setting but for supervised learning.</p><p>Approximate posteriors defined as neural nets <ref type="bibr" target="#b30">Krueger et al. (2018)</ref> and <ref type="bibr" target="#b34">Louizos and Welling (2017)</ref> use normalizing flows to construct an unfactorized, non-Gaussian approximate posterior in BNNs. However, normalizing flows have poor scaling with dimension and point estimates were used for most of the weights in the neural network. Table <ref type="table">5</ref> in Appendix 5 compares qualities of our approach to existing methods for stochastic variational inference in BNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>We developed a practical method for approximate inference in continuous-depth Bayesian neural networks. Our approach exploits a special synergy between continuous-depth models and variational inference for SDEs, providing additional benefits over standard approaches. In particular, our method allows arbitrarily-expressive, non-factorized approximate posteriors implicitly defined through neural SDEs. We also developed an unbiased gradient estimator for SDE variational inference whose variance approaches zero as the approximate posterior approaches the true posterior. This combination gives our family of Bayesian continuous-depth neural networks a special property, which is that the gradients' bias and variance can be made arbitrarily small during training. Where standard applications of MFVI on continuous-depth models perform poorly, our approach brings continuous-depth Bayesian neural networks to a comparable performance with standard Bayesian neural networks. Furthermore, we demonstrated the ability of this continuous-depth model class to use adaptive SDE solvers. This allows a memory-efficient training, and a fine-grained trade-off between precision and speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material: Infinitely Deep Bayesian Neural Networks with Stochastic Differential Equations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A PROOFS</head><p>Notation. Denote as φ the vector of variational parameters, f q as the approximate posterior on weights, f p as the prior on weights, f h as the dynamics of hidden units, and σ as the diffusion function. Denote the Euclidean norm of a vector u by |u|. For function f denote its Jacobian as ∇f .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Derivation of an Alternative Monte Carlo Estimator</head><p>The goal of this section is to derive a Monte Carlo estimator of the KL-divergence on path space that is similar to the fully Monte Carlo estimator described in <ref type="bibr" target="#b43">Roeder et al. (2017)</ref>. This will serve as the basis for the subsequent heuristic derivation of the continuous-time sticking-the-landing trick.</p><p>Let w 0 be a fixed initial state. Let w 1 , ..., w N be states at times ∆t, 2∆t, . . . , N ∆t = T generated by the Euler discretization:</p><formula xml:id="formula_15">w i+1 = w i + f q (w i )∆t + σ(w i )(B t+∆t -B t )<label>(13)</label></formula><formula xml:id="formula_16">= w i + f q (w i )∆t + σ(w i )∆t 1/2 i+1 , i+1 ∼ N (0, 1).<label>(14)</label></formula><p>where {B t } t≥0 is the Brownian motion. This implies that conditional on the previous state, the current state is normally distributed:</p><formula xml:id="formula_17">w i+1 |w i ∼ N (w i + f q (w i )∆t, σ(w i ) 2 ∆t).</formula><p>Thus, the log-densities can be evaluated as</p><formula xml:id="formula_18">log q(w i+1 |w i ) = - 1 2 log(2πσ(w i ) 2 ∆t) - 1 2 (w i+1 -(w i + f q (w i )∆t)) 2 σ(w i ) 2 ∆t , i = 0, . . . N -1.<label>(15)</label></formula><p>On the other hand, if at any time, the next state was generated from the current state based on the prior process, we would have the following log-densities:</p><formula xml:id="formula_19">log p(w i+1 |w i ) = - 1 2 log(2πσ(w i ) 2 ∆t) - 1 2 (w i+1 -(w i + f p (w i )∆t)) 2 σ(w i ) 2 ∆t , i = 0, . . . N -1.<label>(16)</label></formula><p>Now, we substitute the form of w i+1 based on equation 13 into equation 15 and equation 16 and obtain</p><formula xml:id="formula_20">log q(w i+1 |w i ) = - 1 2 log(2πσ(w i ) 2 ∆t) - 1 2 2 i+1 , log p(w i+1 |w i ) = - 1 2 log(2πσ(w i ) 2 ∆t) - 1 2 (f q (w i ) -f p (w i )) 2 σ(w i ) 2 ∆t + 2(f q (w i ) -f p (w i )) i+1 σ(w i ) ∆t 1/2 + 2 i+1 .</formula><p>The KL divergence on the path space could then be regarded as a sum of infinitely many KL-divergences between E wi E wi+1∼q(wi+1|wi) log q(w i+1 |w i ) p(w i+1 |w i )</p><p>= lim</p><formula xml:id="formula_22">N →∞ N i=0 E wi E i+1 (f q (w i ) -f p (w i )) 2 2σ(w i ) 2 ∆t + (f q (w i ) -f p (w i )) σ(w i ) ∆t 1/2 i+1 (19) = E 1 2 T 0 |u t | 2 dt + T 0 u t dB t .<label>(20)</label></formula><p>A.2 Sticking-the-landing in Continuous Time</p><p>For a non-sequential latent variable model, the sticking-the-landing (STL) trick removes from the fully Monte Carlo ELBO estimator a score function term of the form ∂ log q(w, φ)/∂φ, where w is sampled using the reparameterization trick and may depend on φ. The score function term has 0 expectation, but may affect the variance of the gradient estimator for the inference distribution's parameters.</p><p>Here, we exploit this intuition and apply it to each step before taking the limit. More precisely, we apply the STL trick to estimate the gradient of D KL (q(w i+1 |w i )||p(w i+1 |w i )) for i = 1, 2, . . . , N , and thereafter take the limit as the mesh size of the discretization goes to 0. For each individual term, the score function term to be removed is</p><formula xml:id="formula_23">∂ ∂φ log q(w i+1 |w i , φ) = - 1 2σ 2 (w i )∆t ∂ ∂φ (w i+1 -(w i + f q (w i , φ)∆t)) 2 = ∂ ∂φ f q (w i , φ) σ(w i ) i+1 ∆t 1/2 .</formula><p>Now, we sum up all of these terms and take the limit as ∆t → 0. This gives us u(w t , t, stop_gradient(φ)) dB t , w(•) ∼ q φ ().   Figure <ref type="figure" target="#fig_11">9d</ref> indicates that the accuracy of predictions is relatively consistent across all severity levels with the SDE-BNN and SDE-BNN + STL models having relatively better calibrated predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Sticking the Landing Results</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc>Proceedings of the 25 th International Conference on Artificial Intelligence and Statistics (AISTATS) 2022, Valencia, Spain. PMLR: Volume 151. Copyright 2022 by the author(s).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Predictive prior and posterior of the SDE-BNN on a non-monotonic toy dataset. Blue areas indicate density percentiles, and distinct colored lines show model samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Comparison of the variance in three gradient estimators. On this toy problem, our new gradient estimator reduces variance by a factor of roughly 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Benchmarking two gradient computation methods: (1) Back-propagation through the SDE solver, and (2) the memory-efficient stochastic adjoint of Li et al. (2020). Both methods have similar optimization dynamics, final performance, and wall-clock time, but the adjoint approach is more memory-efficient. Detailed comparisons of wall-clock time and evaluation step results in Appendix C.8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><figDesc>CIFAR-10. Left: Negative log likelihood. Right: ECE. Adjusting SDE-BNN solver tolerance at test time trades off computational speed for predictive performance. Grey line is solver's training tolerance. Averaged across 3 seeds. Calibration on the CIFAR-10 test set for a neural ODE (left) and a SDE-BNN (right). The SDE-BNN displays better calibration and generalization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Performance of SDE-BNN on standard CIFAR-10 classification task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: CIFAR10-C. Robustness to distributional shifts on CIFAR-10. SDE-based neural nets show better accuracy and calibration than non-Bayesian and meanfield methods. Black bars show standard deviation over 3 seeds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>E</head><figDesc>wi [D KL (q(w i+1 |w i )||p(w i+1 |w i ))]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><figDesc>] dB t .Removing this term from the fully Monte Carlo estimator in equation 20 gives rise to the following estimator of a surrogate objective that facilitates implementation: ELBO = log p(D | w) -</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :</head><label>9</label><figDesc>Figure9: Figures9a-9cshow that the SDE BNN and SDE BNN + STL models outperform their non-continuous depth ResNet counterparts on all three robustness metrics when evaluated on the corrupt CIFAR-10C benchmarks. Figure9dindicates that the accuracy of predictions is relatively consistent across all severity levels with the SDE-BNN and SDE-BNN + STL models having relatively better calibrated predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Classification accuracy and expected calibration error (ECE) on MNIST and CIFAR-10. We separate models into point estimates, discrete-time models, and continuous-time models. Our SDE-BNN outperforms other continuous-time Bayesian neural nets (BNNs) and perform competitively against discrete-time BNNs. † Results by<ref type="bibr" target="#b23">Izmailov et al. (2021)</ref> where a modified residual network architecture was used; only one seed was reported.</figDesc><table><row><cell></cell><cell cols="2">MNIST</cell><cell cols="2">CIFAR-10</cell></row><row><cell>Model</cell><cell>Accuracy (%)</cell><cell>ECE (×10 -2 )</cell><cell>Accuracy (%)</cell><cell>ECE (×10 -2 )</cell></row><row><cell>ResNet32</cell><cell>99.46 ± 0.00</cell><cell>2.88 ± 0.94</cell><cell>87.35 ± 0.00</cell><cell>8.47 ± 0.39</cell></row><row><cell>ODEnet</cell><cell>98.90 ± 0.04</cell><cell>1.11 ± 0.10</cell><cell>88.30 ± 0.29</cell><cell>8.71 ± 0.21</cell></row><row><cell>HyperODEnet</cell><cell>99.04 ± 0.00</cell><cell>1.04 ± 0.09</cell><cell>87.92 ± 0.46</cell><cell>15.86 ± 1.25</cell></row><row><cell>MFVI ResNet32</cell><cell>99.44 ± 0.00</cell><cell>2.76 ± 1.28</cell><cell>86.97 ± 0.00</cell><cell>3.04 ± 0.94</cell></row><row><cell>MFVI  †</cell><cell>-</cell><cell>-</cell><cell>86.48</cell><cell>1.95</cell></row><row><cell>Deep Ensemble  †</cell><cell>-</cell><cell>-</cell><cell>89.22</cell><cell>2.79</cell></row><row><cell>HMC ("gold standard")  †</cell><cell>98.31</cell><cell>1.79</cell><cell>90.70</cell><cell>5.94</cell></row><row><cell>MFVI ODEnet</cell><cell>98.81 ± 0.00</cell><cell>2.63 ± 0.31</cell><cell>81.59 ± 0.01</cell><cell>3.62 ± 0.40</cell></row><row><cell>MFVI HyperODEnet</cell><cell>98.77 ± 0.01</cell><cell>2.82 ± 1.34</cell><cell>80.62 ± 0.00</cell><cell>4.29 ± 1.10</cell></row><row><cell>SDE BNN</cell><cell>99.30 ± 0.09</cell><cell>0.63 ± 0.10</cell><cell>89.84 ± 0.94</cell><cell>7.19 ± 0.37</cell></row><row><cell>SDE BNN (+ STL)</cell><cell>99.10 ± 0.09</cell><cell>0.78 ± 0.12</cell><cell>89.10 ± 0.45</cell><cell>7.97 ± 0.51</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Training with STL estimator on CIFAR-10 shows training time improvements in accuracy, negative log likelihood, and ELBO objective in addition to reducing variance. This improvement to the standard gradient estimator can be especially useful in settings where the approximate posterior is sufficiently flexible (i.e. the drift neural net is very large relative to the state size).</figDesc><table><row><cell>Method</cell><cell>Accuracy (%)</cell><cell>Negative Log-likelihood (×10 -4 )</cell><cell>ELBO</cell></row><row><cell>SDE BNN</cell><cell>95.91 ± 0.2</cell><cell>1.17 ± 0.309</cell><cell>1.40 ± 0.2</cell></row><row><cell>SDE BNN (+STL)</cell><cell>96.89 ± 0.2</cell><cell>0.309 ± 0.15</cell><cell>1.183 ± 0.2</cell></row><row><cell>C.4 Calibration Results</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We thank <rs type="person">Jesse Bettencourt</rs>, <rs type="person">Radford M. Neal</rs>, and <rs type="person">Patrick Kidger</rs> for helpful technical discussions and revisions on earlier drafts of this work. We also thank <rs type="person">James Bradbury</rs> for his support while implementing differential equation solvers in JAX.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B EXPERIMENTAL SETTINGS</head><p>Table <ref type="table">2</ref>: These are the hyper-parameters for each method of evaluation pertaining to results in the toy and classification tasks of Table <ref type="table">1</ref>. Each model was run on a single Nvidia RTX6000 GPU on our compute clusters. SDE and learning optimization parameters were tuned according to a validation set sampled randomly from 10% of the training set. No schedules of any kind on the hyper-parameters were used in training. Settings with high overlap with another model are indicated using &lt;model&gt; with additional parameters overridden as necessary. Each block is separated either by a downsampling or upsampling convolutional layer (i.e., the -'s).  The SDE-BNN learns meaningful parameterizations on the non-extraneous dimensions of the input state vector. In the case of a true function being monotonic, the augmented dimensions simply help the main output. Middle: The model learns to ignore dimensions that are not necessary to train on, especially on simpler tasks as in the toy setting. Samples in augmented dimensions can overlap for different input values in the given domain <ref type="bibr">(-5, 5)</ref>. Right: Similarly, the last output dimension was also associated with augmentation and was not a well learned representation of the data, ignoring the initial inputs entirely (all values are 0).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Classification Results</head><p>Table <ref type="table">3</ref>: Classification accuracy and expected calibration error on MNIST (100 th epoch) and CIFAR-10 (300 th epoch) for additional baseline to the ones in Table <ref type="table">1</ref>. Values are compared at the 100th epoch for MNIST and 300th for CIFAR-10. Here w0, the initial drift of the posterior SDE, is inferred using a Gaussian prior rather than being a fixed value. The best prior variance was selected in a preliminary sweep between values in the range [0.1, 0.44]. The performance is slightly worse than the point estimate but displays better calibration as a trade-off. It can be noted that calibration may appear better earlier on in training, as in prior to converging and reaching non-uniform confidence, but model predictions are not necessarily correct.   Adaptive-order optimization trajectories were comparable to fixed-order solvers and were thus not applied to the classification tasks since computational resources were not under constraint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MNIST</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Variational inference for diffusion processes</title>
		<author>
			<persName><forename type="first">C</forename><surname>Archambeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Opper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cornford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bayesian dark knowledge</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3438" to="3446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cornebise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<title level="m">Weight uncertainty in neural networks. International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A variational representation for certain functionals of brownian motion</title>
		<author>
			<persName><forename type="first">M</forename><surname>Boué</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dupuis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Probability</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1641" to="1659" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Principled weight initialization for hypernetworks</title>
		<author>
			<persName><forename type="first">O</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Flokas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<title level="m">Neural ordinary differential equations. Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Dandekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dixit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tarek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garcia-Valadez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rackauckas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.07244</idno>
		<title level="m">Bayesian neural ordinary differential equations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>De Brouwer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Simm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Arany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Moreau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.12374</idno>
		<title level="m">Gru-ode-bayes: Continuous modeling of sporadically-observed time series</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The mnist database of handwritten digit images for machine learning research</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="141" to="142" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08803</idno>
		<title level="m">Density estimation using real nvp</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Augmented neural odes</title>
		<author>
			<persName><forename type="first">E</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient and scalable bayesian neural nets with rank-1 factors</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dusenberry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jerfel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Heller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2782" to="2792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Avoiding pathologies in very deep networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A Proposal on Machine Learning via Dynamical Systems</title>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. Math. Stat</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Practical variational inference for neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2348" to="2356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1321" to="1330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.09106</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Hypernetworks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Stable architectures for deep neural networks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ruthotto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inverse Problems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">14004</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Hegde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Heinonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lähdesmäki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kaski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04066</idno>
		<title level="m">Deep learning with differential gaussian process flows</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Stochastic variational inference</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Paisley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1303" to="1347" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Innes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Edelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rackauckus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Saba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">B</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tebbutt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.07587</idno>
		<title level="m">Zygote: A differentiable programming system to bridge machine learning and scientific computing</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">140</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vikram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<title level="m">What are bayesian neural network posteriors really like? International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning differential equations that are easy to solve</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Kidger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Oberhauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lyons</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.03657</idno>
		<title level="m">Neural sdes as infinite-dimensional gans</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Kidger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Morrill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lyons</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.08926</idno>
		<title level="m">Neural controlled differential equations for irregular time series</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>CoRR, abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.10546</idno>
		<title level="m">Sde-net: Equipping deep neural networks with uncertainty estimates</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">The cifar-10 dataset</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://www.cs.toronto.edu/kriz/cifar.html" />
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.04759</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Bayesian hypernetworks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-K</forename><forename type="middle">L</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.01328</idno>
		<title level="m">Scalable gradients for stochastic differential equations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02355</idno>
		<title level="m">Neural sde: Stabilizing neural ode networks with stochastic noise</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Look</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kandemir</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.00796</idno>
		<title level="m">Differential bayesian neural nets</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multiplicative normalizing flows for variational bayesian neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2218" to="2227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Slang: Fast structured covariance approximations for bayesian deep learning with natural gradient</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kunstner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6245" to="6255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Bayesian learning for neural networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">118</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Oganesyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Volokhova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vetrov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.09779</idno>
		<title level="m">Stochasticity in neural odes: An empirical study</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Variational inference for stochastic differential equations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Opper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annalen der Physik</title>
		<imprint>
			<biblScope unit="volume">531</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">1800233</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Osawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Swaroop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Eschenhagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yokota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Khan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02506</idno>
		<title level="m">Practical deep learning with bayesian principles</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Doubly infinite residual networks: a diffusion process approach</title>
		<author>
			<persName><forename type="first">S</forename><surname>Peluchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Favaro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Infinitely deep neural networks as diffusion processes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Peluchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Favaro</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2020">2020b</date>
			<biblScope unit="page" from="1126" to="1136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1401.4082</idno>
		<title level="m">Stochastic backpropagation and approximate inference in deep generative models</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Sticking the landing: Simple, lower-variance gradient estimators for variational inference</title>
		<author>
			<persName><forename type="first">G</forename><surname>Roeder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Latent odes for irregularly-sampled time series</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.03907</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Black-box variational inference for stochastic differential equations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Golightly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Mcgough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Prangle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Tzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raginsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09883</idno>
		<title level="m">Neural stochastic differential equations: Deep latent gaussian models in the diffusion limit</title>
		<imprint>
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Theoretical guarantees for sampling and inference in generative models with latent diffusions</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raginsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.01608</idno>
		<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">K.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vicol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.10317</idno>
		<title level="m">Adversarial distillation of bayesian neural network posteriors</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">How good is the bayes posterior in deep neural networks really</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wenzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Veeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Swiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Ode 2 vae: Deep generative second order odes with bayesian neural networks</title>
		<author>
			<persName><forename type="first">Ç</forename><surname>Yıldız</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Heinonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lähdesmäki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.10994</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Transforming classifier scores into accurate multiclass probability estimates</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zadrozny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Elkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Noisy natural gradient as variational inference</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5852" to="5861" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.09321</idno>
		<title level="m">Fixup initialization: Residual learning without normalization</title>
		<imprint>
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Unterman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Arodz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.12998</idno>
		<title level="m">Approximation capabilities of neural ordinary differential equations</title>
		<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">ANODEV2: A coupled neural ODE evolution framework</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Biros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<idno>CoRR, abs/1906.04596</idno>
		<imprint>
			<date type="published" when="2019">2019c</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
