- **Continuous-Depth Bayesian Neural Networks**: Model class that defines outputs as solutions to ODEs through infinitely many residual layers, enhancing expressivity and reducing memory costs.

- **Stochastic Differential Equations (SDEs)**: Framework for modeling uncertainty in weights, defined as:
  \[
  dw_t = f_\theta(w_t, t) dt + g_\theta(w_t, t) dB_t
  \]
  where \(f_\theta\) is the drift and \(g_\theta\) is the diffusion function.

- **Variational Inference**: The goal is to compute the posterior \(p(w|D) \propto p(D|w)p(w)\) using an approximate posterior \(q(w)\) that minimizes KL divergence:
  \[
  L_{ELBO}(\phi) = E_{q(w)}[\log p(D|w)] - D_{KL}(q(w) || p(w))
  \]

- **Gradient-Based Stochastic Variational Inference**: Introduces a novel zero-variance gradient estimator that improves the efficiency of inference in SDE-BNNs.

- **Expressive Approximate Posteriors**: The variational posterior can be made arbitrarily expressive by enlarging the neural network that parameterizes the dynamics of the approximate posterior.

- **Adaptive SDE Solver**: The output layer state is computed using a black-box adaptive SDE solver, maintaining constant memory costs while allowing for adaptive computation.

- **KL Divergence in Path Space**: The KL divergence between prior and approximate posterior on path space is given by:
  \[
  D_{KL}(\mu_q || \mu_p) = E_{q_\phi(w)} \left[ \int_0^1 \frac{1}{2} u(t, \phi)^2 dt \right]
  \]
  where \(u(t, \phi) = g_\theta(w_t, t)^{-1} [f_\theta(w_t, t) - f_\phi(w_t, t)]\).

- **Hypernetwork for Weight Evolution**: Introduces a hypernetwork \(f_w\) to specify weight changes as a function of depth, allowing for time-correlated weights:
  \[
  \frac{d}{dt} h_t, w_t = f_h(t, h_t, w_t) + f_w(t, w_t)
  \]

- **Prior Process on Weights**: Utilizes a white noise process prior on weights, maintaining finite variance while encoding uncertainty.

- **Comparison with Discrete-Depth Networks**: Continuous-depth networks can be seen as a limit of discrete-depth networks, allowing for more flexible modeling of weight uncertainty and dynamics.

- **Memory-Efficient Training**: The adjoint sensitivity method is employed to save memory during training by reconstructing hidden unit trajectories through backward dynamics.

- **Applications Beyond Neural Networks**: The low-variance gradient estimator can be applied to variational inference in SDEs for time-series modeling, extending the utility of the proposed methods.