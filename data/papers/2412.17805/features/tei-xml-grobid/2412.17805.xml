<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Large Motion Video Autoencoding with Cross-modal Video VAE</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-12-23">23 Dec 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yazhou</forename><surname>Xing</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Fei</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yingqing</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">†</forename><surname>Jingye</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chen</forename><surname>Jiaxin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xie</forename><surname>Xiaowei</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chi</forename><forename type="middle">Qifeng</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Large Motion Video Autoencoding with Cross-modal Video VAE</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-12-23">23 Dec 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">4B378AF5D3078EB57270C3A0CDC373EB</idno>
					<idno type="arXiv">arXiv:2412.17805v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning a robust video Variational Autoencoder (VAE) is essential for reducing video redundancy and facilitating efficient video generation. Directly applying image VAEs to individual frames in isolation can result in temporal inconsistencies and suboptimal compression rates due to a lack of temporal compression. Existing Video VAEs have begun to address temporal compression; however, they often suffer from inadequate reconstruction performance. In this paper, we present a novel and powerful video autoencoder capable of high-fidelity video encoding. First, we observe that entangling spatial and temporal compression by merely extending the image VAE to a 3D VAE can introduce motion blur and detail distortion artifacts. Thus, we propose temporal-aware spatial compression to better encode and decode the spatial information. Additionally, we integrate a lightweight motion compression model for further temporal compression. Second, we propose to leverage the textual information inherent in text-to-video datasets and incorporate text guidance into our model. This significantly enhances reconstruction quality, particularly in terms of detail preservation and temporal stability. Third, we further improve the versatility of our model through joint training on both images and videos, which not only enhances reconstruction quality but also enables the model to perform both image and video autoencoding. Extensive evaluations against strong recent baselines demonstrate the superior performance of our method. The project website can be found at <ref type="url" target="https://yzxing87.github.io/vae/">https://yzxing87.github.io/vae/</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Given the significant attention in the field of video generation, Latent Video Diffusion Models (LVDMs) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b35">36]</ref> have emerged as a popular framework. They have been successfully applied to powerful text-to-video models such as Sora <ref type="bibr" target="#b5">[6]</ref>, VideoCrafter <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>, and CogVideoX <ref type="bibr" target="#b30">[31]</ref>. Different from directly generating video pixels, LVDMs gen-erate latent video representations in a compact latent space. This is achieved by first training a Video VAE to encode videos into this latent space. Thus, Video VAE, as a key and fundamental component of LVDMs, has attracted great attention recently. An effective Video VAE can help to reduce the training costs of video diffusion models while improving the final quality of the generated videos. Initially, a series of studies adopt the image VAE from Stable Diffusion <ref type="bibr" target="#b24">[25]</ref> for video generation tasks, including An-imateDiff <ref type="bibr" target="#b12">[13]</ref>, MagicVideo <ref type="bibr" target="#b35">[36]</ref>, VideoCrafter1 <ref type="bibr" target="#b6">[7]</ref>, and VideoCrafter2 <ref type="bibr" target="#b7">[8]</ref>. However, directly adopting an image VAE and compressing video on a frame-by-frame basis leads to temporal flickering due to the lack of temporal correlation. Additionally, the information redundancy along the temporal dimension is not reduced, leading to low training efficiency for subsequent latent video diffusion models. From the introduction of Sora, which compresses videos both temporally and spatially through a Video VAE, a series of studies have emerged that aim to replicate Sora and train their own Video VAEs, including Open Sora <ref type="bibr" target="#b34">[35]</ref>, Open Sora Plan <ref type="bibr" target="#b18">[19]</ref>, CV-VAE <ref type="bibr" target="#b33">[34]</ref>, CogVideoX <ref type="bibr" target="#b30">[31]</ref>, EasyAnimate <ref type="bibr" target="#b29">[30]</ref>, and Cosmos Tokenizer <ref type="bibr" target="#b22">[23]</ref>. However, the performance of the current video VAE suffers from many problems, including motion ghost, low-level temporal flickering, blurring (faces, hands, edges, texts), and motion stuttering (lack of correct temporal transition).</p><p>In this work, we propose a novel cross-modal Video VAE with better spatial and temporal modeling ability in order to solve the aforementioned challenge problems and obtain a robust and high-quality Video VAE. First, we examine different designs for spatial and temporal compression, including simultaneous spatial-temporal (ST) compression and sequential ST compression. We observed that simultaneous ST compression achieves better low-level temporal smoothness and texture stability, while sequential ST compression achieves better motion recovery, particularly in scenarios of large motion. Thus, we propose a novel architecture that integrates the advantages of both methods and enables effective video detail and motion reconstruction. Second, we observed that the normally used datasets for text-to-video generation contain text-video pairs. Also, dur- • Our video VAE is designed and trained to be versatile to conduct both image and video compression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Video Variational Autoencoder Video Variational Autoencoders (VAEs) <ref type="bibr" target="#b16">[17]</ref> can be broadly categorized into discrete and continuous types. Discrete video VAEs compress videos into discrete tokens by learning a codebook for quantization and have achieved state-of-the-art performance in video reconstruction, as demonstrated by models like MAGVIT-v2 <ref type="bibr" target="#b31">[32]</ref>. However, these VAEs are not suitable for Latent Video Diffusion Models (LVDMs) <ref type="bibr" target="#b13">[14]</ref> due to the lack of necessary gradients for backpropagation, which hinders smooth optimization.</p><p>In contrast, continuous Video VAEs compress videos into continuous latent representations that are widely adopted in LVDMs. In earlier video generation studies, including Stable Video Diffusion <ref type="bibr" target="#b3">[4]</ref>, the Video VAE was directly adapted from the image VAE used in Stable Diffusion <ref type="bibr" target="#b24">[25]</ref>, achieving a compression ratio of 1 × 8 × 8 by processing each frame independently. To further reduce the temporal redundancy, more recent studies <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref> have trained their VAEs to achieve a more efficient compression ratio of 4 × 8 × 8.</p><p>Despite these advancements, all of the aforementioned video VAEs struggle with accurately reconstructing videos with large motions due primarily to their limited ability to handle the temporal dimension effectively. A high-quality Video VAE that can robustly reconstruct videos with significant motion is critical in the LVDM pipeline, as it ensures efficient latent space compression, maintains temporal coherence and reduces computational overhead <ref type="bibr" target="#b26">[27]</ref>. Without a robust VAE, large motions in videos can lead to poor latent representations, negatively impacting the quality and overall performance of the LVDMs.</p><p>Latent Video Diffusion Models Latent Video Diffusion Models (LVDMs) are widely used in foundational video generation models including Sora <ref type="bibr" target="#b5">[6]</ref>, OpenSora <ref type="bibr" target="#b34">[35]</ref>, Open Sora Plan <ref type="bibr" target="#b18">[19]</ref>, VideoCrafter1 <ref type="bibr" target="#b6">[7]</ref>, VideoCrafter2 <ref type="bibr" target="#b7">[8]</ref>, Latte <ref type="bibr" target="#b19">[20]</ref>, CogVideoX <ref type="bibr" target="#b30">[31]</ref>, DynamiCrafter <ref type="bibr" target="#b27">[28]</ref>, Vidu <ref type="bibr" target="#b2">[3]</ref>, Hunyuan Video <ref type="bibr" target="#b17">[18]</ref>, controllable video generation <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref>, and multimodal video generation models <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b28">29]</ref>. The general pipeline for these LVDMs consists of two primary steps. First, the raw video is compressed into a latent space via a video Variational Autoencoder (VAE), significantly reducing computational complexity. In the second step, a diffusion model operates within this latent space, learning the desired transformations. The performance of LVDMs is critically dependent on video VAEs, as the quality of the generated video is heavily influenced by the latent space representation and the encoding-decoding capabilities of the VAE.</p><p>In image generation tasks, Stable Diffusion series <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref> has excelled, largely due to its efficient VAE that reconstructs diverse image types with high fidelity. However, no existing VAE in video generation achieves comparable quality, particularly due to challenges in compressing the temporal dimension. This limitation hinders the performance of LVDMs, especially in high-motion scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>The video autoencoding problem can be defined as follows. Let X ∈ R C×T ×H×W represent a video or image tensor, where C, T , H, and W denote the number of channels, frame(s), height, and width, respectively. We want to train an encoder E that compresses the input tensor X into a compact latent representation</p><formula xml:id="formula_0">Z ∈ R C ′ ×T ′ ×H ′ ×W ′</formula><p>. The learned compact latent Z can be further reconstructed back to RGB space with decoder D:</p><formula xml:id="formula_1">Z = E(X), X = D(Z). (<label>1</label></formula><formula xml:id="formula_2">)</formula><p>Our goal is to design and learn such an autoencoder that can reduce the spatial and temporal dimension of video data in latent space and reconstruct the video with highly spatial and temporal fidelity, especially for large-motion scenarios.</p><p>We first examine two inherited video VAE designs from the pre-trained Stable Diffusion model. We then combine the best of two designs and propose our spatiotemporal modeling that can reconstruct high-dynamic contents with fine details. We then investigate the text-conditioned video autoencoding and propose an effective text-guided video VAE architecture. Moreover, we propose a joint image and video compression training method, that enables text-aided joint image and video autoencoding. Our method does not rely on causal convolution as adopted by prior works. Finally, we carefully study the effects of different loss functions on the reconstruction performance and present the state-of-the-art video VAE architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Optimal Spatiotemporal Modeling</head><p>Designing a video VAE that is inherited from a pre-trained 2D spatial VAE is a good practice to leverage the spatial compression prior. There are typically two options to inflate a 2D spatial VAE to its 3D video counterpart.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Option 1: Simultaneous Spatiotemporal Compression</head><p>One common way to inherit the weight from pre-trained 2D VAE is to inflate the 2D spatial blocks to 3D temporal blocks and simultaneously do the spatial and temporal compression. We first examine this design. Specifically, we replace the 2D convolution in SD VAE with 3D convolution of kernel size <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b2">3)</ref>, whose weights are initialized from the 2D convolution. Then we add an additional temporal convolution layer with kernel size <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b2">3)</ref> to learn spatiotemporal patterns. In this middle block of the inflated VAE, we inflate the 2D attention to 3D attention and we also include a temporal attention to capture both the spatial and temporal information. We keep other components unchanged to maximumly leverage the learned prior of SD VAE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Option 2: Sequential Spatiotemporal Compression</head><p>Another reasonable way to cooperate the SD VAE to video VAE is to keep the SD VAE unchanged: first utilize the SD VAE to compress the input video frame-by-frame, and then learn a temporal autoencoding process to further compress the temporal redundancy, as shown in Fig. <ref type="figure">2</ref>. Specifically, we adopt a lightweight temporal autoencoder for temporal compression. The encoder consists of one convolutional layer to process the input, and two or three 3D ResNet blocks with convolutional downsampling layers to compress the temporal redundancy. Notably, we design the decoder to be asymmetric as the encoder, i.e., there will be two 3D ResNet blocks following each upsampling layer in the decoder. Through this asymmetric design, our decoder can potentially gain some hallucination ability beyond the reconstruction. Surprisingly, we find this sequential spatiotemporal de-sign can better compress and recover the dynamic of the input video than option 1, but is not good at recovering spatial details, which is proved by consistent improvement under large-motion video autoencoding as shown in Fig. <ref type="figure" target="#fig_3">4</ref>.</p><p>Our Solution We find simultaneous spatiotemporal compression leads to better detail-recovering capability, and the sequential spatiotemporal compression will exceed at motion-recovering ability. Thus, we propose to combine the best of two worlds, and introduce the two-stage spatiotemporal modeling for video VAE. As the first stage, we inflate the 2D convolution to 3D convolution with kernel size (1,3,3), and similarly to option 1, we add additional temporal convolution layers through 3D convolution. We denote our first-stage model as a temporal-aware spatial autoencoder. Different from option 1, we only compress the spatial information and do not compress the temporal information at the first stage, but introduce another temporal encoder to further encode the temporal dimensions, which serves as the second stage compression. We follow the same design of option 2 for our temporal encoder and decoder.</p><p>After that, we decode the reconstructed latent of the second stage to the RGB space, with the inflated 3D decoder. We jointly train the inflated 3D VAE and the temporal autoencoder. The main idea is illustrated in Fig. <ref type="figure">2</ref> and Fig. <ref type="figure" target="#fig_2">3</ref>.</p><p>Input Block Downsample Blocks Middle Block Input Video Middle Block Upsample Blocks Ouput Block Output Video Output Latent Input Latent H * W * T H * W * T H/8 * W/8 * T H/8 * W/8 * T STBlock3D Cross Attention Conv3D Down3D STBlock3D Cross Attention STBlock3D Cross Attention STBlock3D STBlock3D Cross Attention Up3D STBlock3D Cross Attention Conv3D STBlock3D Cross Attention STBlock3D We expand the 2D convolution of SD VAE <ref type="bibr" target="#b24">[25]</ref> to 3D convolution and append one additional 3D convolution as temporal convolution after the expanded 3D convolution, which forms the STBlock3D. We also inject the cross-attention layers for crossmodal learning with textual conditions.</p><p>Formulation Recall X ∈ R C×T ×H×W represent a video, where C, T , H, and W denote the number of channels, frames, height, and width, respectively. The i-th frame the is denoted x i ∈ R C×H×W . The aware encoder encodes X into a latent representation Z 1 ∈ R c×T ×h×w , where c is the number of latent channels, and h = H 8 , w = W 8 , formulated by:</p><formula xml:id="formula_3">1 = E 1 (X).<label>(2)</label></formula><p>the temporal autoencoder encodes Z 1 into Z 2 ∈ R c ′ ×t×h×w , where c ′ is the number of latent channels for Z 2 and t = T 4 , as given by:</p><formula xml:id="formula_4">Z 2 = E 2 (Z 1 ).<label>(3)</label></formula><p>Reconstruction is achieved by decoding Z 2 back into the original video space, X ∈ R C×T ×H×W , through the following inverse process:</p><formula xml:id="formula_5">X = D 1 (D 2 (Z 2 )) = D 1 (Z 1 ).<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Cross-modal Modeling</head><p>Since textual information is a native component for text-tovideo generation datasets, we examine if the textual information can improve the autoencoding process of the model.</p><p>To achieve that, we split the feature maps into patches as tokens after each ResNet block in the encoder and decoder, and compute the cross attention by taking visual tokens as query (Q) and value (V), the text embeddings as key (K). We try to keep the patch size trackable for each layer. Specifically, we use patch size to 8×8, 4×4, 2×2, and 1×1 for each layer in the temporal-aware spatial autoencoder respectively. We directly use each pixel as one patch in the temporal autoencoder. We adopt LayerNorm as the normalization function. We use Flan-T5 <ref type="bibr" target="#b11">[12]</ref> as the text embedder. A projection convolution is applied to the result, which is then added to the input via a residual connection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Joint Image and Video Compression</head><p>In contrast to existing architectures such as MagVitV2 <ref type="bibr" target="#b31">[32]</ref>, OD-VAE <ref type="bibr" target="#b8">[9]</ref>, and OPS-VAE <ref type="bibr" target="#b34">[35]</ref>, which use Causalconv3D layers, we rely primarily on standard Conv3D layer.</p><p>A notable feature of our architecture is the ability to mask out the temporal autoencoder, allowing the first-stage model to operate as a standalone image compressor. During training, our model is flexible to take both image and video as input: when the current batch is composed of images, we will disable the temporal convolution and temporal attention layers, as well as the temporal autoencoder. We train our model on both the image dataset and video dataset to let the model learn the image and video compression ability simultaneously. Besides, training on more high-quality images can also help improve the video autoencoding performance. We quantitatively evaluate the performance of our joint image and video compression in Table <ref type="table" target="#tab_1">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Loss Functions</head><p>We use the reconstruction loss, the KL divergence loss, and the video adversarial loss (3D GAN loss) to optimize our model. The reconstruction loss, L recon , ensures that the generated frames are perceptually and structurally similar to the input frames. It combines a pixel-wise error term with a perceptual loss, weighted by a hyperparameter. The KL divergence loss, L KL , regularizes the latent space by encouraging it to conform to a prior distribution, ensuring smoothness and continuity in the learned latent representations. Given the hierarchical structure of our latent space, we only regularize the innermost latent Z 2 , with dimensions T 4 × H 8 × W 8 , where T , H, and W represent the temporal, height, and width dimensions, respectively. The 3D GAN loss, L GAN , is introduced to enhance the realism of the generated video sequences, leveraging a discriminator to distinguish between real and generated sequences. The total loss function is expressed as:</p><formula xml:id="formula_6">L total = L recon + λ KL L KL + λ GAN L GAN .</formula><p>(5)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments 4.1. Experimental Setup</head><p>Datasets We conduct experiments on three datasets: the public Panda2M <ref type="bibr" target="#b9">[10]</ref> and MMTrailer <ref type="bibr" target="#b10">[11]</ref> datasets, and a private text-video dataset with over 6M pairs. To evaluate reconstruction performance, we use three test sets: the We-bVid test set, the Inter4K test set (similar to <ref type="bibr" target="#b33">[34]</ref>), and a large motion test set. The WebVid test set contains 1,000 256x256, 16-frame videos from the WebVid dataset <ref type="bibr" target="#b1">[2]</ref>. The Inter4K test set consists of 500 640x864, 16-frame videos from the Inter4K dataset <ref type="bibr" target="#b25">[26]</ref>. To assess the model's ability to handle challenging motion patterns, we introduce a large motion test set. This set includes 80 videos from WebVid and 20 from Inter4K, manually selected for their complex motion dynamics.</p><note type="other">Simultaneous Sequential Ours Ground Truth Simultaneous Sequential Ours Ground Truth</note><p>Implementation Details We initialize our 4-channel and 16-channel latent Video VAEs from SD-1.4 <ref type="bibr" target="#b24">[25]</ref> and SD-3.5 <ref type="bibr" target="#b0">[1]</ref>, respectively. For both models, we enable the video GAN loss after 50K warmup steps. We initially train the  Table 2. JT * means joint training. We evaluate image reconstruction performance w/ or w/o our joint image-video training strategy.</p><p>cross-modal VAE, both models are initialized with their pretrained weights. We train them on video-text pairs for 160K steps, enabling the model to learn the alignment between visual and textual modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with State-of-the-arts</head><p>We compare our proposed Video VAE models with the state-of-the-art video compression models: Open-Sora-Plan <ref type="bibr" target="#b18">[19]</ref>, Open-Sora <ref type="bibr" target="#b34">[35]</ref>, CV-VAE <ref type="bibr" target="#b33">[34]</ref> on 4-channel latent models, and Cosmos-Tokenizer <ref type="bibr" target="#b22">[23]</ref>, CogVideoX <ref type="bibr" target="#b30">[31]</ref>, EasyAnimate <ref type="bibr" target="#b29">[30]</ref>, CV-VAE <ref type="bibr" target="#b33">[34]</ref> on 16-channel models.</p><p>Quantitative Evaluation We use PSNR, SSIM, and LPIPS <ref type="bibr" target="#b32">[33]</ref> to quantitatively measure the quality of the reconstructed videos. We compare our method with baselines on our three test sets, as listed in Table <ref type="table" target="#tab_1">1</ref>. Among these, our 4-channel latent Video VAE demonstrates superior performance across most datasets and metrics. Specifically, our model achieves the best reconstruction quality on the We-bVid test set, shown as more than 1dB improvements over baselines and a significant improvement on the LPIPS metrics, which indicates our reconstruction is both with highfidelity and better perceptual quality. A similar conclusion can be made on the Inter4K test set. On the Large-Motion test set, our model maintains strong performance with a sig-nificant SSIM and LPIPS improvement, showcasing its robustness in handling complex motion scenarios.</p><p>For models with 16-channel latent space, our model consistently outperforms these baselines across all test sets. For example, on the WebVid test set, our model achieves more than 2dB in terms of PSNR, significantly higher than Cosmos-Tokenizer and CogVideoX. Moreover, our model achieves the best SSIM and LPIPS, demonstrating substantial improvements in both fidelity and perceptual quality.</p><p>In summary, our Video VAE models consistently outperform existing baselines across all test sets and metrics, highlighting their effectiveness in both low-channel (4-channel latent) and high-channel (16-channel latent) configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative Evaluation</head><p>We provide qualitative comparisons with the baselines in Fig. <ref type="figure" target="#fig_0">1</ref>. Our method demonstrates significantly improved motion recovery, greatly reducing ghosting artifacts even in rapid motion scenarios. In contrast, Open-Sora-Plan and CV-VAE struggle to reconstruct fast-moving objects, leading to ghosting artifacts. Additionally, Open-Sora VAE introduces color reconstruction errors, as seen in the clothing of the moving figure. Increasing the latent channels to 16 improves motion reconstruction across all baselines, but noticeable detail errors remain. Our 16channel model further mitigates these errors, resulting in more accurate detail reconstruction. We further compare the reconstruction results with and without the cross-modal training, as shown in Fig. <ref type="figure" target="#fig_5">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>Joint Training We evaluate the effectiveness of our image-video joint training by comparing the performance of our 4-channel latent and 16-channel latent Video VAEs with the video-only training VAE, as well as the image VAE, SD 1.4 and SD 3.5, respectively. The results are shown in Table <ref type="table" target="#tab_1">1</ref> and <ref type="table">Table 2</ref>. The video reconstruction comparison is conducted on the three benchmark datasets. The  image reconstruction comparison is conducted on a set of 500 images with a resolution of 480x864, randomly sampled from a UHD-4K video dataset. During inference, we mask out the temporal autoencoder and the temporal part of the temporal-aware spatial autoencoder, ensuring that the models process the images without considering temporal information, effectively treating them as independent images.</p><p>The joint training can further boost the performance of video reconstruction, which is consistent in both the 4channel and 16-channel experiments. For the image reconstruction, our 4-channel latent Video VAE slightly outperforms SD1.4, and also improves on SSIM and LPIPS, indicating better perceptual quality.</p><p>For the 16-channel VAE, while our model achieves competitive results in terms of PSNR, it falls slightly short of SD3.5. However, our model still demonstrates strong performance in terms of SSIM and LPIPS, suggesting that our joint training approach maintains high perceptual quality despite the slight drop in PSNR.</p><p>We further show the visual effectiveness of the joint image and video training in Fig Architecture Variants We evaluate the effectiveness of different spatiotemporal compression strategies, including simultaneous spatiotemporal compression, sequential spatiotemporal compression, and our proposed solution. These architecture variants are tested on the Large-Motion Test Set to determine which model handles challenging scenarios most effectively, as shown in Table <ref type="table" target="#tab_3">3</ref>. Table 4. Ablation study comparing temporal-aware spatial autoencoder with image/video GAN loss, and different kernel sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Component Ablation</head><p>We perform ablation studies on several key components of our model. First, we investigate the impact of the kernel size in the temporal convolutional layer of temporal-aware spatial autoencoder. The results of this study are shown in Table <ref type="table">4</ref>. Additionally, we explore the significance of the loss function by comparing the performance of temporal-aware spatial autoencoder trained with either the raw image GAN loss or the video GAN loss, with the results also presented in Table <ref type="table">4</ref>. These ablations are conducted on a validation set comprising 98 videos, each with a resolution of 256x256 pixels and a length of 16 frames, sourced from the MMTrailer dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We propose a novel video variational autoencoder (VAE) to address high-fidelity video autoencoding and compression, especially for videos with large motion. Our approach extends pre-trained image VAEs to the video domain by decoupling spatial and temporal compression, mitigating motion blur and detail loss. We design a temporal-aware spatial encoder and a lightweight motion compression model to enhance motion modeling, temporal consistency, and detail preservation. To improve reconstruction quality and versatility, we leverage detailed captions and employ joint image-video training.</p><p>Extensive experiments on challenging datasets demonstrate superior performance over state-of-the-art baselines. Our model sets a new standard for video compression by efficiently handling spatiotemporal compression while benefiting from cross-modal learning and joint training.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Our reconstruction results compared with a line of three recent strong baseline approaches. The ground truth frame is (0). Our model significantly outperforms previous methods, especially under large motion scenarios such as people doing sports.</figDesc><graphic coords="2,392.73,435.11,160.72,89.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>H/ 8 Figure 2 .</head><label>82</label><figDesc>Figure2. Comparison of our optimal spatiotemporal modeling and the two other options. Simultaneous modeling is achieved by inflating pre-trained 2D spatial VAE to 3D VAE. Sequential modeling indicates first compressing the spatial dimension with a spatial encoder and then compressing the temporal information with a temporal encoder. We identify the issues of these two options and propose to combine both advantages and achieve a much better video reconstruction quality. Our VAE also benefits from cross-modality, i.e., text information.</figDesc><graphic coords="4,470.51,277.09,80.30,56.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure3. The architecture of our temporal-aware spatial autoencoder. We expand the 2D convolution of SD VAE<ref type="bibr" target="#b24">[25]</ref> to 3D convolution and append one additional 3D convolution as temporal convolution after the expanded 3D convolution, which forms the STBlock3D. We also inject the cross-attention layers for crossmodal learning with textual conditions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Comparisons among simultaneous spatiotemporal modeling, sequential spatiotemporal modeling and our proposed solution.</figDesc><graphic coords="6,123.10,471.13,55.71,55.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>4 -</head><label>4</label><figDesc>channel and 16-channel latent Video VAEs for 230K and 310K steps, respectively. Subsequently, we conduct joint image-video training, using an 8:2 video-to-image ratio to balance video and image reconstruction. For each training step, we sample 16 videos from Panda2M and our private text-video dataset, concatenating their frames into a single image batch. By masking the temporal dimension and bypassing the temporal autoencoder, we treat these images as independent static frames, allowing the model to learn from both temporal and spatial information. The 4-channel and 16-channel latent Video VAEs undergo additional joint training for 100K and 185K steps, respectively. For the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. The effectiveness of the cross-modal learning for our video VAE. The introduction of textural information improves the detail recovery. We visualize the learned attention map using keywords of the input prompts.</figDesc><graphic coords="8,58.50,277.37,248.24,100.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. The effectiveness of joint image and video training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Quantitative comparison with state-of-the-art methods.</figDesc><table><row><cell>Model</cell><cell></cell><cell cols="4">Downsample Factor #Channels</cell><cell cols="3">WebVid Test Set [2]</cell><cell cols="3">Inter4K Test Set [26]</cell><cell>Large Motion Test Set</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">PSNR (↑) SSIM (↑) LPIPS (↓) PSNR (↑) SSIM (↑) LPIPS (↓) PSNR (↑) SSIM (↑) LPIPS (↓)</cell></row><row><cell cols="2">Open-Sora-Plan (OD VAE [9])</cell><cell></cell><cell>4x8x8</cell><cell>4</cell><cell></cell><cell>29.1646</cell><cell>0.8334</cell><cell>0.0789</cell><cell>28.6690</cell><cell>0.8381</cell><cell>0.0906</cell><cell>27.5697</cell><cell>0.8045</cell><cell>0.1065</cell></row><row><cell cols="2">Open-Sora (OPS VAE [35])</cell><cell></cell><cell>4x8x8</cell><cell>4</cell><cell></cell><cell>29.3753</cell><cell>0.8284</cell><cell>0.1240</cell><cell>29.2721</cell><cell>0.8431</cell><cell>0.1316</cell><cell>27.7586</cell><cell>0.8032</cell><cell>0.1540</cell></row><row><cell>CV-VAE [34]</cell><cell></cell><cell></cell><cell>4x8x8</cell><cell>4</cell><cell></cell><cell>28.6795</cell><cell>0.8154</cell><cell>0.1072</cell><cell>27.7437</cell><cell>0.8124</cell><cell>0.1284</cell><cell>26.9456</cell><cell>0.7849</cell><cell>0.1411</cell></row><row><cell cols="3">Video VAE w/o Joint Training (Ours)</cell><cell>4x8x8</cell><cell>4</cell><cell></cell><cell>30.2091</cell><cell>0.8656</cell><cell>0.0566</cell><cell>28.9048</cell><cell>0.8543</cell><cell>0.0688</cell><cell>27.3917</cell><cell>0.8078</cell><cell>0.0867</cell></row><row><cell>Video VAE (Ours)</cell><cell></cell><cell></cell><cell>4x8x8</cell><cell>4</cell><cell></cell><cell>30.3140</cell><cell>0.8676</cell><cell>0.0538</cell><cell>28.9227</cell><cell>0.8565</cell><cell>0.0665</cell><cell>27.6236</cell><cell>0.8136</cell><cell>0.0841</cell></row><row><cell cols="2">Cross-Modal VAE (Ours)</cell><cell></cell><cell>4x8x8</cell><cell>4</cell><cell></cell><cell>30.1110</cell><cell>0.8608</cell><cell>0.0544</cell><cell>29.0357</cell><cell>0.8510</cell><cell>0.0678</cell><cell>27.1754</cell><cell>0.7999</cell><cell>0.0846</cell></row><row><cell>Cosmos-Tokenizer [23]</cell><cell></cell><cell></cell><cell>4x8x8</cell><cell>16</cell><cell></cell><cell>31.2545</cell><cell>0.8861</cell><cell>0.1030</cell><cell>31.2002</cell><cell>0.8957</cell><cell>0.1071</cell><cell>30.1619</cell><cell>0.8675</cell><cell>0.1194</cell></row><row><cell>CogVideoX-VAE [31]</cell><cell></cell><cell></cell><cell>4x8x8</cell><cell>16</cell><cell></cell><cell>32.8940</cell><cell>0.9208</cell><cell>0.0504</cell><cell>32.5122</cell><cell>0.9229</cell><cell>0.0532</cell><cell>31.0906</cell><cell>0.8978</cell><cell>0.0685</cell></row><row><cell>EasyAnimate-VAE [30]</cell><cell></cell><cell></cell><cell>4x8x8</cell><cell>16</cell><cell></cell><cell>32.1233</cell><cell>0.9085</cell><cell>0.0405</cell><cell>31.5066</cell><cell>0.9048</cell><cell>0.0572</cell><cell>30.5213</cell><cell>0.8846</cell><cell>0.0598</cell></row><row><cell>CV-VAE [34]</cell><cell></cell><cell></cell><cell>4x8x8</cell><cell>16</cell><cell></cell><cell>32.2766</cell><cell>0.9080</cell><cell>0.0546</cell><cell>31.6129</cell><cell>0.9060</cell><cell>0.0642</cell><cell>30.7136</cell><cell>0.8868</cell><cell>0.0726</cell></row><row><cell cols="3">Video VAE w/o Joint Training (Ours)</cell><cell>4x8x8</cell><cell>16</cell><cell></cell><cell>33.8844</cell><cell>0.9334</cell><cell>0.0344</cell><cell>32.9416</cell><cell>0.9297</cell><cell>0.0409</cell><cell>31.8471</cell><cell>0.9073</cell><cell>0.0499</cell></row><row><cell>Video VAE (Ours)</cell><cell></cell><cell></cell><cell>4x8x8</cell><cell>16</cell><cell></cell><cell>34.1558</cell><cell>0.9362</cell><cell>0.0271</cell><cell>33.3184</cell><cell>0.9328</cell><cell>0.0316</cell><cell>32.1503</cell><cell>0.9122</cell><cell>0.0409</cell></row><row><cell cols="2">Cross-Modal VAE (Ours)</cell><cell></cell><cell>4x8x8</cell><cell>16</cell><cell></cell><cell>34.5022</cell><cell>0.9365</cell><cell>0.0323</cell><cell>33.5687</cell><cell>0.9347</cell><cell>0.0379</cell><cell>32.2387</cell><cell>0.9117</cell><cell>0.0481</cell></row><row><cell>Model</cell><cell cols="6"># Ch PSNR (↑) SSIM (↑) LPIPS (↓)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SD1.4 [4]</cell><cell>4</cell><cell cols="2">30.2199</cell><cell>0.8974</cell><cell cols="2">0.0440</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours w/o JT  *</cell><cell>4</cell><cell cols="2">15.1001</cell><cell>0.5561</cell><cell cols="2">0.4339</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell>4</cell><cell cols="2">30.8650</cell><cell>0.9042</cell><cell cols="2">0.0397</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SD3.5 [1]</cell><cell>16</cell><cell cols="2">36.5208</cell><cell>0.9646</cell><cell cols="2">0.0116</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Ours w/o JT  *  16</cell><cell>9.2603</cell><cell></cell><cell>0.2770</cell><cell cols="2">0.6802</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell>16</cell><cell cols="2">35.3437</cell><cell>0.9590</cell><cell cols="2">0.0167</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>6. Overall, these results demonstrate that our joint image-video training strategy allows the model to retain strong image reconstruction capabilities while simultaneously learning to handle video data.Ablation study comparing simultaneous modeling, sequential modeling, and ours on the large-motion test set.</figDesc><table><row><cell>Model</cell><cell cols="2">PSNR (↑) SSIM (↑) LPIPS (↓)</cell></row><row><cell cols="2">Simultaneous 24.0593 0.7315</cell><cell>0.1293</cell></row><row><cell>Sequential</cell><cell>23.3681 0.6917</cell><cell>0.1481</cell></row><row><cell>Ours</cell><cell>24.6722 0.7234</cell><cell>0.1162</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Stability</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2023. 3, 6, 7</date>
		</imprint>
	</monogr>
	<note>Stable diffusion 3.5 large</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Frozen in time: A joint video and image encoder for end-to-end retrieval</title>
		<author>
			<persName><forename type="first">Max</forename><surname>Bain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gül</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Vidu: a highly consistent, dynamic and skilled text-to-video generator with diffusion models</title>
		<author>
			<persName><forename type="first">Fan</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chendong</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guande</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiwen</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shilong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaole</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.04233</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Stable video diffusion: Scaling latent video diffusion models to large datasets</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dockhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumith</forename><surname>Kulal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Mendelevitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maciej</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yam</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zion</forename><surname>English</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikram</forename><surname>Voleti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Letts</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.15127</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Align your latents: High-resolution video synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dockhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seung</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><surname>Kreis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="22563" to="22575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Video generation models as world simulators</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Depue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Schnurr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Troy</forename><surname>Luhman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Luhman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clarence</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricky</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Videocrafter1: Open diffusion models for high-quality video generation</title>
		<author>
			<persName><forename type="first">Haoxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menghan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingqing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoshu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinbo</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaofang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.19512</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Videocrafter2: Overcoming data limitations for high-quality video diffusion models</title>
		<author>
			<persName><forename type="first">Haoxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menghan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Od-vae: An omni-dimensional video compressor for improving latent video diffusion model</title>
		<author>
			<persName><forename type="first">Liuhan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongjian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenghai</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinhua</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Panda-70m: Captioning 70m videos with multiple cross-modality teachers</title>
		<author>
			<persName><forename type="first">Aliaksandr</forename><surname>Tsai-Shien Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Willi</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekaterina</forename><surname>Menapace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiang-Wei</forename><surname>Deyneka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byung</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Eun Jeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsin-Ying</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="13320" to="13331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Mmtrail: A multimodal trailer video dataset with language and music descriptions</title>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yatian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aosong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengjun</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyue</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingqing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingqun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahao</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengfei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruibin</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanbing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yike</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Scaling instructionfinetuned language models</title>
		<author>
			<persName><forename type="first">Chung</forename><surname>Hyung Won</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunxuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Brahma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">70</biblScope>
			<biblScope unit="page" from="1" to="53" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Animatediff: Animate your personalized textto-image diffusion models without specific tuning</title>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anyi</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaohui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maneesh</forename><surname>Agrawala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Latent video diffusion models for high-fidelity video generation with arbitrary lengths</title>
		<author>
			<persName><forename type="first">Yingqing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.13221</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Animate-a-story: Storytelling with retrieval-augmented video generation</title>
		<author>
			<persName><forename type="first">Yingqing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menghan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinbo</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.06940</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Llms meet multimodal generation and editing: A survey</title>
		<author>
			<persName><forename type="first">Yingqing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingye</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyue</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruibin</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhou</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.19334</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">stat</title>
		<imprint>
			<biblScope unit="volume">1050</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Weijie</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rox</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuozhuo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangfeng</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.03603</idno>
		<title level="m">A systematic framework for large video generative models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Open-sora-plan</title>
		<author>
			<persName><forename type="first">Pku-Yuan</forename><surname>Lab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Tuzhan</surname></persName>
		</author>
		<author>
			<persName><surname>Etc</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2024. 1, 3, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Latte: Latent diffusion transformer for video generation</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaohui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gengyun</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan-Fang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cunjian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.03048</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Follow your pose: Poseguided text-to-video generation using pose-free videos</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingqing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siran</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="4117" to="4125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Yue</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongfa</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingqing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junkun</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ailing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.01900</idno>
		<title level="m">Follow-your-emoji: Fine-controllable and expressive freestyle portrait animation</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Cosmos tokenizer: A suite of image and video neural tokenizers</title>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Sdxl: Improving latent diffusion models for high-resolution image synthesis</title>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Podell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zion</forename><surname>English</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lacey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dockhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Penna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.01952</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Björn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2006">2022. 1, 3, 5, 6</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Stergiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><surname>Poppe</surname></persName>
		</author>
		<title level="m">Adapool: Exponential adaptive pooling for information-retaining downsampling</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The Movie Gen team @ Meta. Movie gen: A cast of media foundation models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Dynamicrafter: Animating open-domain images with video diffusion priors</title>
		<author>
			<persName><forename type="first">Jinbo</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menghan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wangbo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tien-Tsin</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Seeing and hearing: Open-domain visualaudio generation with diffusion latent aligners</title>
		<author>
			<persName><forename type="first">Yazhou</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingqing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyue</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="7151" to="7161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Easyanimate: A high-performance long video generation method based on transformer architecture</title>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyi</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunzhe</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunkuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengli</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2024. 1, 3, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Cogvideox: Text-to-video diffusion models with an expert transformer</title>
		<author>
			<persName><forename type="first">Zhuoyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayan</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wendi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiazheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyi</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanyu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaotao</forename><surname>Da Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yean</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Language model beats diffusion-tokenizer is key to visual generation</title>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José</forename><surname>Lezama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nitesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Gundavarapu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kihyuk</forename><surname>Versari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Minnen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agrim</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuye</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Cv-vae: A compatible video vae for latent generative video models</title>
		<author>
			<persName><forename type="first">Sijie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoshu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muyao</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbo</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.20279</idno>
		<imprint>
			<date type="published" when="2007">2024. 1, 3, 5, 7</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Open-sora: Democratizing efficient video production for all</title>
		<author>
			<persName><forename type="first">Zangwei</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianji</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenhui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenggui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2024. 1, 3, 5, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Magicvideo: Efficient video generation with latent diffusion models</title>
		<author>
			<persName><forename type="first">Daquan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weimin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanshu</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhe</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.11018</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
