# Large Motion Video Autoencoding with Cross-modal Video VAE

## Abstract

## 

Learning a robust video Variational Autoencoder (VAE) is essential for reducing video redundancy and facilitating efficient video generation. Directly applying image VAEs to individual frames in isolation can result in temporal inconsistencies and suboptimal compression rates due to a lack of temporal compression. Existing Video VAEs have begun to address temporal compression; however, they often suffer from inadequate reconstruction performance. In this paper, we present a novel and powerful video autoencoder capable of high-fidelity video encoding. First, we observe that entangling spatial and temporal compression by merely extending the image VAE to a 3D VAE can introduce motion blur and detail distortion artifacts. Thus, we propose temporal-aware spatial compression to better encode and decode the spatial information. Additionally, we integrate a lightweight motion compression model for further temporal compression. Second, we propose to leverage the textual information inherent in text-to-video datasets and incorporate text guidance into our model. This significantly enhances reconstruction quality, particularly in terms of detail preservation and temporal stability. Third, we further improve the versatility of our model through joint training on both images and videos, which not only enhances reconstruction quality but also enables the model to perform both image and video autoencoding. Extensive evaluations against strong recent baselines demonstrate the superior performance of our method. The project website can be found at [https://yzxing87.github.io/vae/](https://yzxing87.github.io/vae/).

## Introduction

Given the significant attention in the field of video generation, Latent Video Diffusion Models (LVDMs) [[4,](#b3)[5,](#b4)[7,](#b6)[14,](#b13)[36]](#b35) have emerged as a popular framework. They have been successfully applied to powerful text-to-video models such as Sora [[6]](#b5), VideoCrafter [[7,](#b6)[8]](#b7), and CogVideoX [[31]](#b30). Different from directly generating video pixels, LVDMs gen-erate latent video representations in a compact latent space. This is achieved by first training a Video VAE to encode videos into this latent space. Thus, Video VAE, as a key and fundamental component of LVDMs, has attracted great attention recently. An effective Video VAE can help to reduce the training costs of video diffusion models while improving the final quality of the generated videos. Initially, a series of studies adopt the image VAE from Stable Diffusion [[25]](#b24) for video generation tasks, including An-imateDiff [[13]](#b12), MagicVideo [[36]](#b35), VideoCrafter1 [[7]](#b6), and VideoCrafter2 [[8]](#b7). However, directly adopting an image VAE and compressing video on a frame-by-frame basis leads to temporal flickering due to the lack of temporal correlation. Additionally, the information redundancy along the temporal dimension is not reduced, leading to low training efficiency for subsequent latent video diffusion models. From the introduction of Sora, which compresses videos both temporally and spatially through a Video VAE, a series of studies have emerged that aim to replicate Sora and train their own Video VAEs, including Open Sora [[35]](#b34), Open Sora Plan [[19]](#b18), CV-VAE [[34]](#b33), CogVideoX [[31]](#b30), EasyAnimate [[30]](#b29), and Cosmos Tokenizer [[23]](#b22). However, the performance of the current video VAE suffers from many problems, including motion ghost, low-level temporal flickering, blurring (faces, hands, edges, texts), and motion stuttering (lack of correct temporal transition).

In this work, we propose a novel cross-modal Video VAE with better spatial and temporal modeling ability in order to solve the aforementioned challenge problems and obtain a robust and high-quality Video VAE. First, we examine different designs for spatial and temporal compression, including simultaneous spatial-temporal (ST) compression and sequential ST compression. We observed that simultaneous ST compression achieves better low-level temporal smoothness and texture stability, while sequential ST compression achieves better motion recovery, particularly in scenarios of large motion. Thus, we propose a novel architecture that integrates the advantages of both methods and enables effective video detail and motion reconstruction. Second, we observed that the normally used datasets for text-to-video generation contain text-video pairs. Also, dur- • Our video VAE is designed and trained to be versatile to conduct both image and video compression.

## Related Work

Video Variational Autoencoder Video Variational Autoencoders (VAEs) [[17]](#b16) can be broadly categorized into discrete and continuous types. Discrete video VAEs compress videos into discrete tokens by learning a codebook for quantization and have achieved state-of-the-art performance in video reconstruction, as demonstrated by models like MAGVIT-v2 [[32]](#b31). However, these VAEs are not suitable for Latent Video Diffusion Models (LVDMs) [[14]](#b13) due to the lack of necessary gradients for backpropagation, which hinders smooth optimization.

In contrast, continuous Video VAEs compress videos into continuous latent representations that are widely adopted in LVDMs. In earlier video generation studies, including Stable Video Diffusion [[4]](#b3), the Video VAE was directly adapted from the image VAE used in Stable Diffusion [[25]](#b24), achieving a compression ratio of 1 × 8 × 8 by processing each frame independently. To further reduce the temporal redundancy, more recent studies [[19,](#b18)[30,](#b29)[31,](#b30)[34,](#b33)[35]](#b34) have trained their VAEs to achieve a more efficient compression ratio of 4 × 8 × 8.

Despite these advancements, all of the aforementioned video VAEs struggle with accurately reconstructing videos with large motions due primarily to their limited ability to handle the temporal dimension effectively. A high-quality Video VAE that can robustly reconstruct videos with significant motion is critical in the LVDM pipeline, as it ensures efficient latent space compression, maintains temporal coherence and reduces computational overhead [[27]](#b26). Without a robust VAE, large motions in videos can lead to poor latent representations, negatively impacting the quality and overall performance of the LVDMs.

Latent Video Diffusion Models Latent Video Diffusion Models (LVDMs) are widely used in foundational video generation models including Sora [[6]](#b5), OpenSora [[35]](#b34), Open Sora Plan [[19]](#b18), VideoCrafter1 [[7]](#b6), VideoCrafter2 [[8]](#b7), Latte [[20]](#b19), CogVideoX [[31]](#b30), DynamiCrafter [[28]](#b27), Vidu [[3]](#b2), Hunyuan Video [[18]](#b17), controllable video generation [[15,](#b14)[21,](#b20)[22]](#b21), and multimodal video generation models [[16,](#b15)[29]](#b28). The general pipeline for these LVDMs consists of two primary steps. First, the raw video is compressed into a latent space via a video Variational Autoencoder (VAE), significantly reducing computational complexity. In the second step, a diffusion model operates within this latent space, learning the desired transformations. The performance of LVDMs is critically dependent on video VAEs, as the quality of the generated video is heavily influenced by the latent space representation and the encoding-decoding capabilities of the VAE.

In image generation tasks, Stable Diffusion series [[1,](#b0)[24,](#b23)[25]](#b24) has excelled, largely due to its efficient VAE that reconstructs diverse image types with high fidelity. However, no existing VAE in video generation achieves comparable quality, particularly due to challenges in compressing the temporal dimension. This limitation hinders the performance of LVDMs, especially in high-motion scenarios.

## Method

## Overview

The video autoencoding problem can be defined as follows. Let X ∈ R C×T ×H×W represent a video or image tensor, where C, T , H, and W denote the number of channels, frame(s), height, and width, respectively. We want to train an encoder E that compresses the input tensor X into a compact latent representation

$Z ∈ R C ′ ×T ′ ×H ′ ×W ′$. The learned compact latent Z can be further reconstructed back to RGB space with decoder D:

$Z = E(X), X = D(Z). (1$$)$Our goal is to design and learn such an autoencoder that can reduce the spatial and temporal dimension of video data in latent space and reconstruct the video with highly spatial and temporal fidelity, especially for large-motion scenarios.

We first examine two inherited video VAE designs from the pre-trained Stable Diffusion model. We then combine the best of two designs and propose our spatiotemporal modeling that can reconstruct high-dynamic contents with fine details. We then investigate the text-conditioned video autoencoding and propose an effective text-guided video VAE architecture. Moreover, we propose a joint image and video compression training method, that enables text-aided joint image and video autoencoding. Our method does not rely on causal convolution as adopted by prior works. Finally, we carefully study the effects of different loss functions on the reconstruction performance and present the state-of-the-art video VAE architecture.

## Optimal Spatiotemporal Modeling

Designing a video VAE that is inherited from a pre-trained 2D spatial VAE is a good practice to leverage the spatial compression prior. There are typically two options to inflate a 2D spatial VAE to its 3D video counterpart.

## Option 1: Simultaneous Spatiotemporal Compression

One common way to inherit the weight from pre-trained 2D VAE is to inflate the 2D spatial blocks to 3D temporal blocks and simultaneously do the spatial and temporal compression. We first examine this design. Specifically, we replace the 2D convolution in SD VAE with 3D convolution of kernel size [(1,](#b0)[3,](#b2)[3)](#b2), whose weights are initialized from the 2D convolution. Then we add an additional temporal convolution layer with kernel size [(3,](#b2)[3,](#b2)[3)](#b2) to learn spatiotemporal patterns. In this middle block of the inflated VAE, we inflate the 2D attention to 3D attention and we also include a temporal attention to capture both the spatial and temporal information. We keep other components unchanged to maximumly leverage the learned prior of SD VAE.

## Option 2: Sequential Spatiotemporal Compression

Another reasonable way to cooperate the SD VAE to video VAE is to keep the SD VAE unchanged: first utilize the SD VAE to compress the input video frame-by-frame, and then learn a temporal autoencoding process to further compress the temporal redundancy, as shown in Fig. [2](#). Specifically, we adopt a lightweight temporal autoencoder for temporal compression. The encoder consists of one convolutional layer to process the input, and two or three 3D ResNet blocks with convolutional downsampling layers to compress the temporal redundancy. Notably, we design the decoder to be asymmetric as the encoder, i.e., there will be two 3D ResNet blocks following each upsampling layer in the decoder. Through this asymmetric design, our decoder can potentially gain some hallucination ability beyond the reconstruction. Surprisingly, we find this sequential spatiotemporal de-sign can better compress and recover the dynamic of the input video than option 1, but is not good at recovering spatial details, which is proved by consistent improvement under large-motion video autoencoding as shown in Fig. [4](#fig_3).

Our Solution We find simultaneous spatiotemporal compression leads to better detail-recovering capability, and the sequential spatiotemporal compression will exceed at motion-recovering ability. Thus, we propose to combine the best of two worlds, and introduce the two-stage spatiotemporal modeling for video VAE. As the first stage, we inflate the 2D convolution to 3D convolution with kernel size (1,3,3), and similarly to option 1, we add additional temporal convolution layers through 3D convolution. We denote our first-stage model as a temporal-aware spatial autoencoder. Different from option 1, we only compress the spatial information and do not compress the temporal information at the first stage, but introduce another temporal encoder to further encode the temporal dimensions, which serves as the second stage compression. We follow the same design of option 2 for our temporal encoder and decoder.

After that, we decode the reconstructed latent of the second stage to the RGB space, with the inflated 3D decoder. We jointly train the inflated 3D VAE and the temporal autoencoder. The main idea is illustrated in Fig. [2](#) and Fig. [3](#fig_2).

Input Block Downsample Blocks Middle Block Input Video Middle Block Upsample Blocks Ouput Block Output Video Output Latent Input Latent H * W * T H * W * T H/8 * W/8 * T H/8 * W/8 * T STBlock3D Cross Attention Conv3D Down3D STBlock3D Cross Attention STBlock3D Cross Attention STBlock3D STBlock3D Cross Attention Up3D STBlock3D Cross Attention Conv3D STBlock3D Cross Attention STBlock3D We expand the 2D convolution of SD VAE [[25]](#b24) to 3D convolution and append one additional 3D convolution as temporal convolution after the expanded 3D convolution, which forms the STBlock3D. We also inject the cross-attention layers for crossmodal learning with textual conditions.

Formulation Recall X ∈ R C×T ×H×W represent a video, where C, T , H, and W denote the number of channels, frames, height, and width, respectively. The i-th frame the is denoted x i ∈ R C×H×W . The aware encoder encodes X into a latent representation Z 1 ∈ R c×T ×h×w , where c is the number of latent channels, and h = H 8 , w = W 8 , formulated by:

$1 = E 1 (X).(2)$the temporal autoencoder encodes Z 1 into Z 2 ∈ R c ′ ×t×h×w , where c ′ is the number of latent channels for Z 2 and t = T 4 , as given by:

$Z 2 = E 2 (Z 1 ).(3)$Reconstruction is achieved by decoding Z 2 back into the original video space, X ∈ R C×T ×H×W , through the following inverse process:

$X = D 1 (D 2 (Z 2 )) = D 1 (Z 1 ).(4)$
## Cross-modal Modeling

Since textual information is a native component for text-tovideo generation datasets, we examine if the textual information can improve the autoencoding process of the model.

To achieve that, we split the feature maps into patches as tokens after each ResNet block in the encoder and decoder, and compute the cross attention by taking visual tokens as query (Q) and value (V), the text embeddings as key (K). We try to keep the patch size trackable for each layer. Specifically, we use patch size to 8×8, 4×4, 2×2, and 1×1 for each layer in the temporal-aware spatial autoencoder respectively. We directly use each pixel as one patch in the temporal autoencoder. We adopt LayerNorm as the normalization function. We use Flan-T5 [[12]](#b11) as the text embedder. A projection convolution is applied to the result, which is then added to the input via a residual connection.

## Joint Image and Video Compression

In contrast to existing architectures such as MagVitV2 [[32]](#b31), OD-VAE [[9]](#b8), and OPS-VAE [[35]](#b34), which use Causalconv3D layers, we rely primarily on standard Conv3D layer.

A notable feature of our architecture is the ability to mask out the temporal autoencoder, allowing the first-stage model to operate as a standalone image compressor. During training, our model is flexible to take both image and video as input: when the current batch is composed of images, we will disable the temporal convolution and temporal attention layers, as well as the temporal autoencoder. We train our model on both the image dataset and video dataset to let the model learn the image and video compression ability simultaneously. Besides, training on more high-quality images can also help improve the video autoencoding performance. We quantitatively evaluate the performance of our joint image and video compression in Table [1](#tab_1).

## Loss Functions

We use the reconstruction loss, the KL divergence loss, and the video adversarial loss (3D GAN loss) to optimize our model. The reconstruction loss, L recon , ensures that the generated frames are perceptually and structurally similar to the input frames. It combines a pixel-wise error term with a perceptual loss, weighted by a hyperparameter. The KL divergence loss, L KL , regularizes the latent space by encouraging it to conform to a prior distribution, ensuring smoothness and continuity in the learned latent representations. Given the hierarchical structure of our latent space, we only regularize the innermost latent Z 2 , with dimensions T 4 × H 8 × W 8 , where T , H, and W represent the temporal, height, and width dimensions, respectively. The 3D GAN loss, L GAN , is introduced to enhance the realism of the generated video sequences, leveraging a discriminator to distinguish between real and generated sequences. The total loss function is expressed as:

$L total = L recon + λ KL L KL + λ GAN L GAN .$(5)

## Experiments 4.1. Experimental Setup

Datasets We conduct experiments on three datasets: the public Panda2M [[10]](#b9) and MMTrailer [[11]](#b10) datasets, and a private text-video dataset with over 6M pairs. To evaluate reconstruction performance, we use three test sets: the We-bVid test set, the Inter4K test set (similar to [[34]](#b33)), and a large motion test set. The WebVid test set contains 1,000 256x256, 16-frame videos from the WebVid dataset [[2]](#b1). The Inter4K test set consists of 500 640x864, 16-frame videos from the Inter4K dataset [[26]](#b25). To assess the model's ability to handle challenging motion patterns, we introduce a large motion test set. This set includes 80 videos from WebVid and 20 from Inter4K, manually selected for their complex motion dynamics.

Simultaneous Sequential Ours Ground Truth Simultaneous Sequential Ours Ground TruthImplementation Details We initialize our 4-channel and 16-channel latent Video VAEs from SD-1.4 [[25]](#b24) and SD-3.5 [[1]](#b0), respectively. For both models, we enable the video GAN loss after 50K warmup steps. We initially train the  Table 2. JT * means joint training. We evaluate image reconstruction performance w/ or w/o our joint image-video training strategy.

cross-modal VAE, both models are initialized with their pretrained weights. We train them on video-text pairs for 160K steps, enabling the model to learn the alignment between visual and textual modalities.

## Comparison with State-of-the-arts

We compare our proposed Video VAE models with the state-of-the-art video compression models: Open-Sora-Plan [[19]](#b18), Open-Sora [[35]](#b34), CV-VAE [[34]](#b33) on 4-channel latent models, and Cosmos-Tokenizer [[23]](#b22), CogVideoX [[31]](#b30), EasyAnimate [[30]](#b29), CV-VAE [[34]](#b33) on 16-channel models.

Quantitative Evaluation We use PSNR, SSIM, and LPIPS [[33]](#b32) to quantitatively measure the quality of the reconstructed videos. We compare our method with baselines on our three test sets, as listed in Table [1](#tab_1). Among these, our 4-channel latent Video VAE demonstrates superior performance across most datasets and metrics. Specifically, our model achieves the best reconstruction quality on the We-bVid test set, shown as more than 1dB improvements over baselines and a significant improvement on the LPIPS metrics, which indicates our reconstruction is both with highfidelity and better perceptual quality. A similar conclusion can be made on the Inter4K test set. On the Large-Motion test set, our model maintains strong performance with a sig-nificant SSIM and LPIPS improvement, showcasing its robustness in handling complex motion scenarios.

For models with 16-channel latent space, our model consistently outperforms these baselines across all test sets. For example, on the WebVid test set, our model achieves more than 2dB in terms of PSNR, significantly higher than Cosmos-Tokenizer and CogVideoX. Moreover, our model achieves the best SSIM and LPIPS, demonstrating substantial improvements in both fidelity and perceptual quality.

In summary, our Video VAE models consistently outperform existing baselines across all test sets and metrics, highlighting their effectiveness in both low-channel (4-channel latent) and high-channel (16-channel latent) configurations.

## Qualitative Evaluation

We provide qualitative comparisons with the baselines in Fig. [1](#fig_0). Our method demonstrates significantly improved motion recovery, greatly reducing ghosting artifacts even in rapid motion scenarios. In contrast, Open-Sora-Plan and CV-VAE struggle to reconstruct fast-moving objects, leading to ghosting artifacts. Additionally, Open-Sora VAE introduces color reconstruction errors, as seen in the clothing of the moving figure. Increasing the latent channels to 16 improves motion reconstruction across all baselines, but noticeable detail errors remain. Our 16channel model further mitigates these errors, resulting in more accurate detail reconstruction. We further compare the reconstruction results with and without the cross-modal training, as shown in Fig. [5](#fig_5).

## Ablation Study

Joint Training We evaluate the effectiveness of our image-video joint training by comparing the performance of our 4-channel latent and 16-channel latent Video VAEs with the video-only training VAE, as well as the image VAE, SD 1.4 and SD 3.5, respectively. The results are shown in Table [1](#tab_1) and [Table 2](#). The video reconstruction comparison is conducted on the three benchmark datasets. The  image reconstruction comparison is conducted on a set of 500 images with a resolution of 480x864, randomly sampled from a UHD-4K video dataset. During inference, we mask out the temporal autoencoder and the temporal part of the temporal-aware spatial autoencoder, ensuring that the models process the images without considering temporal information, effectively treating them as independent images.

The joint training can further boost the performance of video reconstruction, which is consistent in both the 4channel and 16-channel experiments. For the image reconstruction, our 4-channel latent Video VAE slightly outperforms SD1.4, and also improves on SSIM and LPIPS, indicating better perceptual quality.

For the 16-channel VAE, while our model achieves competitive results in terms of PSNR, it falls slightly short of SD3.5. However, our model still demonstrates strong performance in terms of SSIM and LPIPS, suggesting that our joint training approach maintains high perceptual quality despite the slight drop in PSNR.

We further show the visual effectiveness of the joint image and video training in Fig Architecture Variants We evaluate the effectiveness of different spatiotemporal compression strategies, including simultaneous spatiotemporal compression, sequential spatiotemporal compression, and our proposed solution. These architecture variants are tested on the Large-Motion Test Set to determine which model handles challenging scenarios most effectively, as shown in Table [3](#tab_3). Table 4. Ablation study comparing temporal-aware spatial autoencoder with image/video GAN loss, and different kernel sizes.

## Component Ablation

We perform ablation studies on several key components of our model. First, we investigate the impact of the kernel size in the temporal convolutional layer of temporal-aware spatial autoencoder. The results of this study are shown in Table [4](#). Additionally, we explore the significance of the loss function by comparing the performance of temporal-aware spatial autoencoder trained with either the raw image GAN loss or the video GAN loss, with the results also presented in Table [4](#). These ablations are conducted on a validation set comprising 98 videos, each with a resolution of 256x256 pixels and a length of 16 frames, sourced from the MMTrailer dataset.

## Conclusion

We propose a novel video variational autoencoder (VAE) to address high-fidelity video autoencoding and compression, especially for videos with large motion. Our approach extends pre-trained image VAEs to the video domain by decoupling spatial and temporal compression, mitigating motion blur and detail loss. We design a temporal-aware spatial encoder and a lightweight motion compression model to enhance motion modeling, temporal consistency, and detail preservation. To improve reconstruction quality and versatility, we leverage detailed captions and employ joint image-video training.

Extensive experiments on challenging datasets demonstrate superior performance over state-of-the-art baselines. Our model sets a new standard for video compression by efficiently handling spatiotemporal compression while benefiting from cross-modal learning and joint training.

![Figure 1. Our reconstruction results compared with a line of three recent strong baseline approaches. The ground truth frame is (0). Our model significantly outperforms previous methods, especially under large motion scenarios such as people doing sports.]()

![Figure2. Comparison of our optimal spatiotemporal modeling and the two other options. Simultaneous modeling is achieved by inflating pre-trained 2D spatial VAE to 3D VAE. Sequential modeling indicates first compressing the spatial dimension with a spatial encoder and then compressing the temporal information with a temporal encoder. We identify the issues of these two options and propose to combine both advantages and achieve a much better video reconstruction quality. Our VAE also benefits from cross-modality, i.e., text information.]()

![Figure3. The architecture of our temporal-aware spatial autoencoder. We expand the 2D convolution of SD VAE[25] to 3D convolution and append one additional 3D convolution as temporal convolution after the expanded 3D convolution, which forms the STBlock3D. We also inject the cross-attention layers for crossmodal learning with textual conditions.]()

![Figure 4. Comparisons among simultaneous spatiotemporal modeling, sequential spatiotemporal modeling and our proposed solution.]()

![channel and 16-channel latent Video VAEs for 230K and 310K steps, respectively. Subsequently, we conduct joint image-video training, using an 8:2 video-to-image ratio to balance video and image reconstruction. For each training step, we sample 16 videos from Panda2M and our private text-video dataset, concatenating their frames into a single image batch. By masking the temporal dimension and bypassing the temporal autoencoder, we treat these images as independent static frames, allowing the model to learn from both temporal and spatial information. The 4-channel and 16-channel latent Video VAEs undergo additional joint training for 100K and 185K steps, respectively. For the]()

![Figure 5. The effectiveness of the cross-modal learning for our video VAE. The introduction of textural information improves the detail recovery. We visualize the learned attention map using keywords of the input prompts.]()

![Figure 6. The effectiveness of joint image and video training.]()

![Quantitative comparison with state-of-the-art methods.]()

![6. Overall, these results demonstrate that our joint image-video training strategy allows the model to retain strong image reconstruction capabilities while simultaneously learning to handle video data.Ablation study comparing simultaneous modeling, sequential modeling, and ours on the large-motion test set.]()

