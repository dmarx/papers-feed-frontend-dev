{
  "arxivId": "2212.07677",
  "title": "Transformers learn in-context by gradient descent",
  "authors": "Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, Jo\u00e3o Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, Max Vladymyrov",
  "abstract": "At present, the mechanisms of in-context learning in Transformers are not\nwell understood and remain mostly an intuition. In this paper, we suggest that\ntraining Transformers on auto-regressive objectives is closely related to\ngradient-based meta-learning formulations. We start by providing a simple\nweight construction that shows the equivalence of data transformations induced\nby 1) a single linear self-attention layer and by 2) gradient-descent (GD) on a\nregression loss. Motivated by that construction, we show empirically that when\ntraining self-attention-only Transformers on simple regression tasks either the\nmodels learned by GD and Transformers show great similarity or, remarkably, the\nweights found by optimization match the construction. Thus we show how trained\nTransformers become mesa-optimizers i.e. learn models by gradient descent in\ntheir forward pass. This allows us, at least in the domain of regression\nproblems, to mechanistically understand the inner workings of in-context\nlearning in optimized Transformers. Building on this insight, we furthermore\nidentify how Transformers surpass the performance of plain gradient descent by\nlearning an iterative curvature correction and learn linear models on deep data\nrepresentations to solve non-linear regression tasks. Finally, we discuss\nintriguing parallels to a mechanism identified to be crucial for in-context\nlearning termed induction-head (Olsson et al., 2022) and show how it could be\nunderstood as a specific case of in-context learning by gradient descent\nlearning within Transformers. Code to reproduce the experiments can be found at\nhttps://github.com/google-research/self-organising-systems/tree/master/transformers_learn_icl_by_gd .",
  "url": "https://arxiv.org/abs/2212.07677",
  "issue_number": 91,
  "issue_url": "https://github.com/dmarx/papers-feed/issues/91",
  "created_at": "2025-01-05T08:25:08.591396",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 0,
  "last_read": null,
  "last_visited": "2024-12-21T08:19:06.235Z",
  "main_tex_file": null,
  "published_date": "2022-12-15T09:21:21Z",
  "arxiv_tags": [
    "cs.LG",
    "cs.AI",
    "cs.CL"
  ]
}