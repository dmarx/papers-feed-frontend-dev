# Transformers Learn In-Context by Gradient Descent

## Abstract

## 

At present, the mechanisms of in-context learning in Transformers are not well understood and remain mostly an intuition. In this paper, we suggest that training Transformers on auto-regressive objectives is closely related to gradient-based metalearning formulations. We start by providing a simple weight construction that shows the equivalence of data transformations induced by 1) a single linear self-attention layer and by 2) gradientdescent (GD) on a regression loss. Motivated by that construction, we show empirically that when training self-attention-only Transformers on simple regression tasks either the models learned by GD and Transformers show great similarity or, remarkably, the weights found by optimization match the construction. Thus we show how trained Transformers become mesa-optimizers i.e. learn models by gradient descent in their forward pass. This allows us, at least in the domain of regression problems, to mechanistically understand the inner workings of in-context learning in optimized Transformers. Building on this insight, we furthermore identify how Transformers surpass the performance of plain gradient descent by learning an iterative curvature correction and learn linear models on deep data representations to solve non-linear regression tasks. Finally, we discuss intriguing parallels to a mechanism identified to be crucial for in-context learning termed induction-head [(Olsson et al., 2022)](#b37) and show how it could be understood as a specific case of in-context learning by gradient descent learning within Transformers.

## Introduction

In recent years Transformers (TFs; [Vaswani et al., 2017)](#b47) have demonstrated their superiority in numerous benchmarks and various fields of modern machine learning, and have emerged as the de-facto neural network architecture used for modern AI [(Dosovitskiy et al., 2021;](#b15)[Yun et al., 2019;](#b51)[Carion et al., 2020;](#b9)[Gulati et al., 2020)](#b23). It has been hypothesised that their success is due in part to a phenomenon called in-context learning [(Brown et al., 2020;](#b8)[Liu et al., 2021)](#b35): an ability to flexibly adjust their prediction based on additional data given in context (i.e. in the input sequence itself). In-context learning offers a seemingly different approach to few-shot and meta-learning [(Brown et al., 2020)](#b8), but as of today the exact mechanisms of how it works are not fully understood. It is thus of great interest to understand what makes Transformers pay attention to their context, what the mechanisms are, and under which circumstances, they come into play [(Chan et al., 2022b;](#)[Olsson et al., 2022)](#b37).

In this paper, we aim to bridge the gap between in-context and meta-learning, and show that in-context learning in Transformers can be an emergent property approximating gradient-based few-shot learning within its forward pass, see Figure [1](#fig_0). For this to be realized, we show how Transformers (1) construct a loss function dependent on the data given in sequence and (2) learn based on gradients of that loss. We will first focus on the latter, the more elaborate learning task, in sections 2 and 3, after which we provide evidence for the former in section 4.

We summarize our contributions as follows[foot_0](#foot_0) :

• We construct explicit weights for a linear self-attention layer that induces an update identical to a single step of gradient descent (GD) on a mean squared error loss. Additionally, we show how several self-attention layers can iteratively perform curvature correction improving on plain gradient descent.

• When optimized on linear regression datasets, we demonstrate that linear self-attention-only Transform- ers either converge to our weight construction and therefore implement gradient descent, or generate linear models that closely align with models trained by GD, both in in-and out-of-distribution validation tasks.

• By incorporating multi-layer-perceptrons (MLPs) into the Transformer architecture, we enable solving nonlinear regression tasks within Transformers by showing its equivalence to learning a linear model on deep representations. We discuss connections to kernel regression as well as nonparametric kernel smoothing methods. Empirically, we compare meta-learned MLPs and a single step of GD on its output layer with trained Transformers and demonstrate striking similarities between the identified solutions.

• We resolve the dependency on the specific token construction by providing evidence that learned Transformers first encode incoming tokens into a format amenable to the in-context gradient descent learning that occurs in the later layers of the Transformer.

These findings allow us to connect learning Transformer weights and the concept of meta-learning a learning algorithm [(Schmidhuber, 1987;](#b44)[Hinton & Plaut, 1987;](#b25)[Bengio et al., 1990;](#b5)[Chalmers, 1991;](#b10)[Schmidhuber, 1992;](#b45)[Thrun & Pratt, 1998;](#b46)[Hochreiter et al., 2001;](#b26)[Andrychowicz et al., 2016;](#b2)[Ba et al., 2016;](#b3)[Kirsch & Schmidhuber, 2021)](#b30). In this extensive research field, meta-learning is typically regarded as learning that takes place on various time scales namely fast and slow. The slowly changing parameters control and prepare for fast adaptation reacting to sudden changes in the incoming data by e.g. a context switch. Notably, we build heavily on the concept of fast weights [(Schmidhuber, 1992)](#b45) which has shown to be equivalent to linear self-attention [(Schlag et al., 2021)](#b43) and show how optimized Transformers implement interpretable learning algorithms within their weights.

Another related meta-learning concept, termed MAML [(Finn et al., 2017)](#b18), aims to meta-learn a deep neural network initialization which allows for fast adaptation on novel tasks.

It has been shown that in many circumstances, the solution found can be approximated well when only adapting the output layer i.e. learning a linear model on a meta-learned deep data representations [(Finn et al., 2017;](#b18)[Finn & Levine, 2018;](#b17)[Gordon et al., 2019;](#b21)[Lee et al., 2019;](#b32)[Rusu et al., 2019;](#b42)[Raghu et al., 2020;](#b40)[von Oswald et al., 2021)](#b48). In section 3, we show the equivalence of this framework to in-context learning implemented in a common Transformer block i.e. when combining self-attention layers with a multi-layerperceptron.

In the light of meta-learning we show how optimizing Transformer weights can be regarded as learning on two time scales. More concretely, we find that solely through the pressure to predict correctly Transformers discover learning algorithms inside their forward computations, effectively meta-learning a learning algorithm. Recently, this concept of an emergent optimizer within a learned neural network, such as a Transformer, has been termed "mesa-optimization" [(Hubinger et al., 2019)](#b27). We find and describe one possible realization of this concept and hypothesize that the in-context learning capabilities of language models emerge through mechanisms similar to the ones we discuss here.

Transformers come in different "shapes and sizes", operate on vastly different domains, and exhibit varying forms of phase transitions of in-context learning [(Kirsch et al., 2022;](#b31)[Chan et al., 2022a)](#), suggesting variance and significant complexity of the underlying learning mechanisms. As a result, we expect our findings on linear self-attention-only Transformers to only explain a limited part of a complex process, and it may be one of many possible methods giving rise to in-context learning. Nevertheless, our approach provides an intriguing perspective on, and novel evidence for, an incontext learning mechanism that significantly differs from existing mechanisms based on associative memory [(Ramsauer et al., 2020)](#b41), or by the copying mechanism termed induction heads identified by [(Olsson et al., 2022)](#b37). We, therefore, state the following Hypothesis 1 (Transformers learn in-context by gradient descent). When training Transformers on auto-regressive tasks, in-context learning in the Transformer forward pass is implemented by gradient-based optimization of an implicit auto-regressive inner loss constructed from its in-context data.

We acknowledge work done in parallel, investigating the same hypothesis. [Akyürek et al. (2023)](#b0) puts forward a weight construction based on a chain of Transformer layers (including MLPs) that together implement a single step of gradient descent with weight decay. Similar to work done by [Garg et al. (2022)](#b20), they then show that trained Transformers match the performance of models obtained by gradient descent. Nevertheless, it is not clear that optimization finds Transformer weights that coincide with their construction.

Here, we present a much simpler construction that builds on [Schlag et al. (2021)](#b43) and only requires a single linear selfattention layer to implement a step of gradient descent. This allows us to (1) show that optimizing self-attention-only Transformers finds weights that match our weight construction (Proposition 1), demonstrating its practical relevance, and (2) explain in-context learning in shallow two layer Transformers intensively studied by [Olsson et al. (2022)](#b37). Therefore, although related work provides comprehensive empirical evidence that Transformers indeed seem to implement gradient descent based learning on the data given in-context, we will in the following present mechanistic verification of this hypothesis and provide compelling evidence that our construction, which implements GD in a Transformer forward pass, is found in practice.

## Linear self-attention can emulate gradient descent on a linear regression task

We start by reviewing a standard multi-head self-attention (SA) layer with parameters θ. A SA layer updates each element e j of a set of tokens {e 1 , . . . , e N } according to

$e j ← e j + SA θ (j, {e 1 , . . . , e N }) = e j + h P h V h softmax(K T h q h,j )(1)$with P h , V h , K h the projection, value and key matrices, respectively, and q h,i the query, all for the h-th head. To simplify the presentation, we omit bias terms here and throughout. The columns of the value

$V h = [v h,1 , . . . , v h,N ] and key K h = [k h,1 , . . . , k h,N ] matrices consist of vectors v h,i = W h,V e i and k h,i = W h,K e i ;$likewise, the query is produced by linearly projecting the tokens, q h,j = W h,Q e j .

The parameters θ = {P h , W h,V , W h,K , W h,Q } h of a SA layer consist of all the projection matrices, of all heads.

The self-attention layer described above corresponds to the one used in the standard Transformer model. Follow-ing [Schlag et al. (2021)](#b43), we now introduce our first (and only) departure from the standard model, and omit the softmax operation in equation 1, leading to the linear selfattention (LSA) layer e j ← e j + LSA θ (j, {e 1 , . . . , e N }) = e j + h P h V h K T h q h,j We next show that with some simple manipulations we can relate the update performed by an LSA layer to one step of gradient descent on a linear regression loss.

## Data transformations induced by gradient descent

We now introduce a reference linear model y(x) = W x parameterized by the weight matrix W ∈ R Ny×Nx , and a training dataset D = {(x i , y i )} N i=1 comprising of input samples x i ∈ R Nx and respective labels y i ∈ R Ny . The goal of learning is to minimize the squared-error loss:

$L(W ) = 1 2N N i=1 ∥W x i -y i ∥ 2 . (2$$)$One step of gradient descent on L with learning rate η yields the weight change

$∆W = -η∇ W L(W ) = - η N N i=1 (W x i -y i )x T i .(3)$Considering the loss after changing the weights, we obtain

$L(W + ∆W ) = 1 2N N i=1 ∥(W + ∆W )x i -y i ∥ 2 = 1 2N N i=1 ∥W x i -(y i -∆y i )∥ 2 (4)$where we introduced the transformed targets y i -∆y i with ∆y i = ∆W x i . Thus, we can view the outcome of a gradient descent step as an update to our regression loss (equation 2), where data, and not weights, are updated. Note that this formulation is closely linked to predicting based on nonparametric kernel smoothing, see Appendix A.8 for a discussion.

Returning to self-attention mechanisms and Transformers, we consider an in-context learning problem where we are given N context tokens together with an extra query token, indexed by N + 1. In terms of our linear regression problem, the N context tokens e j = (x j , y j ) ∈ R Nx+Ny correspond to the N training points in D, and the N +1-th token e N +1 = (x N +1 , y N +1 ) = (x test , ŷtest ) = e test to the test input x test and the corresponding prediction ŷtest . We use the terms training and in-context data interchangeably, as well as query and test token/data, as we establish their equivalence now.

Transformations induced by gradient descent and a linear self-attention layer can be equivalent

We have re-cast the task of learning a linear model as directly modifying the data, instead of explicitly computing and returning the weights of the model (equation 4). We proceed to establish a connection between self-attention and gradient descent. We provide a construction where learning takes place simultaneously by directly updating all tokens, including the test token, through a linear self-attention layer.

In other words, the token produced in response to a query (test) token is transformed from its initial value W 0 x test , where W 0 is the initial value of W , to the post-learning prediction ŷ = (W 0 + ∆W )x test obtained after one gradient descent step. Proposition 1. Given a 1-head linear attention layer and the tokens e j = (x j , y j ), for j = 1, . . . , N , one can construct key, query and value matrices W K , W Q , W V as well as the projection matrix P such that a Transformer step on every token e j is identical to the gradient-induced dynamics e j ← (x j , y j ) + (0, -∆W x j ) = (x j , y j ) + P V K T q j such that e j = (x j , y j -∆y j ). For the test data token (x N +1 , y N +1 ) the dynamics are identical.

The simple construction can be found in Appendix A.1 and we denote the corresponding self-attention weights by θ GD .

Below, we provide some additional insights on what is needed to implement the provided LSA-layer weight construction, and further details on what it can achieve:

• Full self-attention. Our dynamics model training is based on in-context tokens only, i.e., only e 1 , . . . , e N are used for computing key and value matrices; the query token e N +1 (containing test data) is excluded. This leads to a linear function in x test as well as to the correct ∆W , induced by gradient descent on a loss consisting only of the training data. This is a minor deviation from full self-attention. In practice, this modification can be dropped, which corresponds to assuming that the underlying initial weight matrix is zero, W 0 ≈ 0, which makes ∆W in equation 8 independent of the test token even if incorporating it in the key and value matrices. In our experiments, we see that these assumptions are met when initializing the attention weights θ to small values.

• Reading out predictions. When initializing the yentry of the test-data token with -W 0 x N +1 , i.e. e test = (x test , -W 0 x test ), the test-data prediction ŷ can be easily read out by simply multiplying again by -1 the updated token, since -y N +1 + ∆y

$N +1 = -(y N +1 - ∆y N +1 ) = y N +1 + ∆W x N +1$. This can easily be done by a final projection matrix, which incidentally is usually found in Transformer architectures. Importantly, we see that a single head of self-attention is sufficient to transform our training targets as well as the test prediction simultaneously.

• Uniqueness. We note that the construction is not unique; in particular, it is only required that the products P W V as well as W K W Q match the construction. Furthermore, since no nonlinearity is present, any rescaling s of the matrix products, i.e., P W V s and W K W Q /s, leads to an equivalent result. If we correct for these equivalent formulations, we can experimentally verify that weights of our learned Transformers indeed match the presented construction.

• Meta-learned task-shared learning rates. When training self-attention parameters θ across a family of in-context learning tasks τ , where the data (x τ,i , y τ,i ) follows a certain distribution, the learning rate can be implicitly (meta-)learned such that an optimal loss reduction (averaged over tasks) is achieved given a fixed number of update steps. In our experiments, we find this to be the case. This kind of meta-learning to improve upon plain gradient descent has been leveraged in numerous previous approaches for deep neural networks [(Li et al., 2017;](#b34)[Lee & Choi, 2018;](#b33)[Park & Oliva, 2019;](#b38)[Zhao et al., 2020;](#b53)[Flennerhag et al., 2020)](#b19).

• Task-specific data transformations. A self-attention layer is in principle further capable of exploiting statistics in the current training data samples, beyond modeling task-shared curvature information in θ. More concretely, a LSA layer updates an input sample according to a data transformation x j ← x j + ∆x j = (I + P (X)V (X)K(X) T W Q )x j = H θ (X)x j , with X the N x × N input training data matrix, when neglecting influences by target data y i . Through H θ (X), a LSA layer can encode in θ an algorithm for carrying out data transformations which depend on the actual input training samples in X. In our experiments, we see that trained self-attention learners employ a simple form of H(X) and that this leads to substantial speed ups in for GD and TF learning.

## Trained Transformers do mimic gradient descent on linear regression tasks

We now experimentally investigate whether trained attention-based models implement gradient-based incontext learning in their forward passes. We gradually build up from single linear self-attention layers to multi-layer nonlinear models, approaching full Transformers. In this section, we follow the assumption of Proposition 1 tightly and construct our tokens by concatenating input and target data, e j = (x j , y j ) for 1 ≤ j ≤ N , and our query token by concatenating the test input and a zero vector, e N +1 = (x test , 0). We show how to lift this assumption in the last section of the paper. The prediction ŷθ ({e τ,1 , . . . , e τ,N }, e τ,N +1 ) of the attention-based model, which depends on all tokens and on the parameters θ, is read-out from the y-entry of the updated N + 1-th token as explained in the previous section.

The objective of training, visualized in Figure [1](#fig_0), is to minimize the expected squared prediction error, averaged over tasks

$min θ E τ [||ŷ θ ({e τ,1 , . . . , e τ,N }, e τ,N +1 ) -y τ,test || 2 ].$We achieve this by minibatch online minimization (by Adam [(Kingma & Ba, 2014)](#b29)): At every optimization step, we construct a batch of novel training tasks and take a step of stochastic gradient descent on the loss function:

$L(θ) = 1 B B τ =1 ||ŷ θ ({e τ,i } N i=1 , e τ,N +1 ) -y τ,test || 2 (5)$where each task (context) τ consists of in-context training data D τ = {(x τ,i , y τ,i )} N i=1 and test point (x τ,N +1 , y τ,N +1 ), which we use to construct our tokens {e τ,i } N +1 i=1 as described above. We denote the optimal parameters found by this optimization process by θ * . In our setup, finding θ * may be thought of as meta-learning, while learning a particular task τ corresponds to simply evaluating the model ŷθ ({e τ,1 , . . . , e τ,N }, e τ,N +1 ). Note that we therefore never see the exact same training task twice during training. See Appendix A.12, especially Figure [16](#fig_0) for an analyses when using a fixed dataset size which we cycle over during training.

We focus on solvable tasks and similarly to [Garg et al. (2022)](#b20) generate data for each task using a teacher model with parameters W τ ∼ N (0, I). We then sample x τ,i ∼ U (-1, 1) n I and construct targets using the task-specific teacher model, y τ,i = W τ x τ,i . In the majority of our experiments we set the dimensions to N = n I = 10 and n O = 1. Since we use a noiseless teacher for simplicity, we can expect our regression tasks to be well-posed and analytically solvable as we only compute a loss on the Transformers last token, which stands in contrast to usual autoregressive training and the training setup of [Garg et al. (2022)](#b20). Full details and results for training with a fixed training set size may be found in Appendix A.12.

## One-step of gradient descent vs. a single trained self-attention layer

Our first goal is to investigate whether a trained single, linear self-attention layer can be explained by the provided weight construction that implements GD. To that end, we compare the predictions made by a LSA layer with trained weights θ * (which minimize equation 5) and with constructed weights θ GD (which satisfy Proposition 1).

Recall that a LSA layer yields the prediction ŷθ (x test ) = e N +1 + LSA θ ({e 1 , . . . , e N }, e N +1 ) = ∆W θ,D x test , which is linear in x test . We denote by ∆W θ,D the matrix generated by the LSA layer following the construction provided in Proposition 1, with query token e N +1 set such that the initial prediction is set to zero, ŷtest = 0. We compare ŷθ (x test ) to the prediction of the control LSA ŷθGD (x test ), which under our token construction corresponds to a linear model trained by one step of gradient descent starting from W 0 = 0. For this control model, we determine the optimal learning rate η by minimizing L(η) over a training set of 10 4 tasks through line search, with L(η) defined analogously to equation 5.

More concretely, to compare trained and constructed LSA layers, we sample T val = 10 4 validation tasks and record the following quantities, averaged over validation tasks: (1) the difference in predictions measured with the L2 norm, ∥ŷ θ (x τ,test ) -ŷθGD (x τ,test )∥, (2) the cosine similarity between the sensitivities

∂ ŷθ GD (xτ,test) ∂xtest and ∂ ŷθ (xτ,test) ∂xtest as well as (3) their difference ∥ ∂ ŷθ GD (xτ,test) ∂xtest -∂ ŷθ (xτ,test) ∂xtest

∥ again according to the L2 norm, which in both cases yields the explicit models computed by the algorithm. We show the results of these comparisons in Figure [2](#fig_1). We find an excellent agreement between the two models over a wide range of hyperparameters. We note that as we do not have direct access to the initialization of W in the attention-based learners (it is hidden in θ), we cannot expect the models to agree exactly.

Although the above metrics are important to show similarities between the resulting learned models (in-context vs. gradient-based), the underlying algorithms could still be different. We therefore carry out an extended set of analyses:

1. Interpolation. We take inspiration on recent work [(Benzing et al., 2022;](#b6)[Entezari et al., 2021)](#b16) that showed approximate equivalence of models found by SGD after permuting weights within the trained neural networks. Since our models are deep linear networks with respect to x test we only correct for scaling mismatches between the two models -in this case the construction that implements GD and the trained weights. As shown in Figure [2](#fig_1), we observe (and can actually inspect by eye, see Appendix Figure [9](#)) that a simple scaling correction on the trained weights is enough to recover the weight construction implementing GD. This leads to an identical loss of GD, the trained Transformer and the linearly interpolated weights θ I = (θ + θ GD )/2. See details in Appendix A.3 on how our weight correction and interpolation is obtained.

2. Out-of-distribution validation tasks. To test if our in-context learner has found a generalizable update rule, we investigate how GD, the trained LSA layer and its interpolation behave when providing in-context data in regimes different to the ones used during training. We therefore visualize the loss increase when (1) sampling the input data from U (-α, α) Nx or (2) scaling the teacher weights by α as αW when sampling validation tasks. For both cases, we set α = 1 during training. We again observe that when training a single linear self-attention Transformer, for both interventions, the Transformer performs equally to gradient descent outside of this training setups, see Figure [2](#fig_1) as well Appendix Figure [6](#). Note that the loss obtained through gradient descent also starts degrading quickly outside the training regime. Since we tune the learning rate for the input range [-1, 1] and one gradient step, tasks with larger input range will have higher curvature and the optimal learning rate for smaller ranges will lead to divergence and a drastic increase in loss also for GD.

3. Repeating the LSA update. Since we claim that a single trained LSA layer implements a GD-like learning rule, we further test its behavior when applying it repeatedly, not only once as in training. After we correct the learning rate of both algorithms, i.e. for GD and the trained Transformer with a dampening parameter λ = 0.75 (details in Appendix A.6), we see an identical loss decrease of both GD and the Transformer, see Figure [1](#fig_0).

To conclude, we present evidence that optimizing a single LSA layer to solve linear regression tasks finds weights that (approximately) coincide with the LSA-layer weight construction of Proposition 1, hence implementing a step of gradient descent, leading to the same learning capabilities on in-and out-of-distribution tasks. We comment on the random seed dependent phase transition of the loss during training in Appendix A.11.

Multiple steps of gradient descent vs. multiple layers of self-attention

We now turn to deep linear self-attention-only Transformers. The construction we put forth in Proposition 1, can be immediately stacked up over K layers; in this case, the final prediction can be read out from the last layer as before by negating the y-entry of the last test token:

$-y N +1 + K k=1 ∆y k,N +1 = -(y N +1 - K k=1 ∆y k,N +1 ) = y N +1 + K k=1 ∆W k x N +1$, where y k,N +1 are the test token values at layer k, and ∆y k,N +1 the change in the y-entry of the test token after applying the k-th step of self-attention, and ∆W k the k-th implicit change in the underlying linear model parameters W . When optimizing such Transformers with K layers, we observe that these models generally outperform K steps of plain gradient descent, see Figure [3](#fig_2). Their behavior is however well described by a variant of gradient descent, for which we tune a single parameter γ defined through the transformation function H(X) which transforms the input data according to x j ← H(X)x j , with H(X) = (I -γXX T ). We term this gradient descent variant GD ++ which we explain and analyze in Appendix A.10.

To analyze the effect of adding more layers to the architecture, we first turn to the arguably simplest extension of a single SA layer and analyze a recurrent or looped 2-layer LSA model. Here, we simply repeatably apply the same layer (with the same weights) multiple times i.e. drawing the analogy to learning an iterative algorithm that applies the same logic multiple times.

Somewhat surprisingly, we find that the trained model surpasses plain gradient descent, which also results in decreasing alignment between the two models (see center left column), and the recurrent Transformer realigns perfectly with GD ++ while matching its performance on in-and out-of distribution tasks. Again, we can interpolate between the Transformer weights found by optimization and the LSAweight construction with learned η, γ, see Figure [3](#fig_2) & 6.

We next consider deeper, non-recurrent 5-layer LSA-only Transformers, with different parameters per layer (i.e. no weight tying). We see that a different GD learning rate as well as γ per step (layer) need to be tuned to match the Transformer performance. This slight modification leads again to almost perfect alignment between the trained TF and GD ++ with in this case 10 additional parameters and The trained TF performance surpasses standard GD but matches GD ++ , our GD variant with simple iterative data transformation. On both cases, we tuned the gradient descent learning rates as well as the scalar γ which governs the data transformation H(X). Center left & center right columns: We measure the alignment between the GD as well as the GD ++ models and the trained TF. In both cases the TF aligns well with GD in the beginning of training but aligns much better with GD ++ after training. Far right column: TF performance (in log-scale) mimics the one of GD ++ well when testing on OOD tasks (α ̸ = 1).

loss close to 0, see Figure [3](#fig_2). Nevertheless, we see that the naive correction necessary for model interpolation used in the aforementioned experiments is not enough to interpolate without a loss increase. We leave a search for better weight corrections to future work. We further study Transformers with different depths for recurrent as well as non-recurrent architectures with multiple heads and equipped with MLPs, and find qualitatively equivalent results, see Appendix Figure [7](#fig_6) and Figure [8](#fig_7). Additionally, in Appendix A.9, we provide results obtained when using softmax SA layers as well as LayerNorm, thus essentially retrieving the standard Transformer architecture. We again observe and are able to explain (after slight architectural modifications) good learning performance and as well as alignment with the construction of Proposition 1, though worse than when using linear self-attention. These findings suggest that the incontext learning abilities of the standard Transformer with these common architecture choices can be explained by the gradient-based learning hypothesis explored here. Our findings also question the ubiquitous use of softmax attention, and suggest further investigation is warranted into the performance of linear vs. softmax SA layers in real-world learning tasks, as initiated by [Schlag et al. (2021)](#b43).

## Transformers solve nonlinear regression tasks by gradient descent on deep data representations

It is unreasonable to assume that the astonishing in-context learning flexibility observed in large Transformers is ex-plained by gradient descent on linear models. We now show that this limitation can be resolved by incorporating one additional element of fully-fledged Transformers: preceding self-attention layers by MLPs enables learning linear models by gradient descent on deep representations which motivates our illustration in Figure [1](#fig_0). Empirically, we demonstrate this by solving non-linear sine-wave regression tasks, see Figure [4](#fig_4). Experimental details can be found in Appendix A.7. We state Proposition 2. Given a Transformer block i.e. a MLP m(e) which transforms the tokens e j = (x j , y j ) followed by an attention layer, we can construct weights that lead to gradient descent dynamics descending

$1 2N N i=1 ||W m(x i ) -y i || 2 .$Iteratively applying Transformer blocks therefore can solve kernelized least-squares regression problems with kernel function k(x, y) = m(x) ⊤ m(y) induced by the MLP m(•).

A detailed discussion on this form of kernel regression as well as kernel smoothing w/wo softmax nonlinearity through gradient descent on the data can be found in Appendix A.8. The way MLPs transform data in Transformers diverges from the standard meta-learning approach, where a task-shared input embedding network is optimized by backpropagation-through-training to improve the learning performance of a task-specific readout (e.g., [Raghu et al., 2020;](#b40)[Lee et al., 2019;](#b32)[Bertinetto et al., 2019)](#b7). On the other hand, given our token construction in Proposition 1, MLPs in Transformers intriguingly process both inputs and targets. The output of this transformation is then processed by a sin-gle linear self-attention layer, which, according to our theory, is capable of implementing gradient descent learning. We compare the performance of this Transformer model, where all weights are learned, to a control Transformer where the final LSA weights are set to the construction θ GD which is therefore identical to training an MLP by backpropagation through a GD updated output layer.

Intriguingly, both obtained functions show again surprising similarity on (1) the initial (meta-learned) prediction, read out after the MLP, and (2) the final prediction, after altering the output of the MLP through GD or the self-attention layer. This is again reflected in our alignment measures that now, since the obtained models are nonlinear w.r.t. x test , only represent the two first parts of the Taylor approximation of the obtained functions. Our results serve as a first demonstration of how MLPs and self-attention layers can interplay to support nonlinear in-context learning, allowing to fine-tune deep data representations by gradient descent. Investigating the interplay between MLPs and SA-layer in deep TFs is left for future work.

## Do self-attention layers build regression tasks?

The construction provided in Proposition 1 and the previous experimental section relied on a token structure where both input and output data are concatenated into a single token. This design is different from the way tokens are typically built in most of the related work dealing with simple few-shot learning problems as well as in e.g. language modeling. We therefore ask: Can we overcome the assumption required in Proposition 1 and allow a Transformer to build the required token construction on its own? This motivates Proposition 3. Given a 1-head linear or softmax attention layer and the token construction e 2j = (x j ), e 2j+1 = (0, y j ) with a zero vector 0 of dim N x -N y and concatenated positional encodings, one can construct key, query and value matrix W K , W Q , W V as well as the projection matrix P such that all tokens e j are transformed into tokens equivalent to the ones required in Proposition 1.

The construction and its discussion can be found in Appendix A.5. To provide evidence that copying is performed in trained Transformers, we optimize a two-layer self-attention circuit on in-context data where alternating tokens include input or output data i.e. e 2j = (x j ) and e 2j+1 = (0, y j ). We again measure the loss as well as the mean of the norm of the partial derivative of the first layer's output w.r.t. the input tokens during training, see Figure [5](#fig_3). First, the training speeds are highly variant given different training seeds, also reported in [Garg et al. (2022)](#b20). Nevertheless, the Transformer is able to match the performance of a single (not two) step gradient descent. Interestingly, before the Transformer performance jumps to the one of GD, token e j transformed by the first self-attention layer becomes notably dependant on the neighboring token e j+1 while staying independent on the others which we denote as e other in Figure [5](#fig_3). We interpret this as evidence for a copying mechanism of the Transformer's first layer to merge input and output data into single tokens as required by Proposition 1. Then, in the second layer the Transformer performs a single step of GD. Notably, we were not able to train the Transformer with linear self-attention layers, but had to incorporate the softmax operation in the first layer. These preliminary findings support the study of [Olsson et al. (2022)](#b37) showing that softmax self-attention layers easily learn to copy; we confirm this claim, and further show that such copying allows the Transformer to proceed by emulating gradient-based learning in the second or deeper attention layers.

We conclude that copying through (softmax) attention layers is the second crucial mechanism for in-context learning in Transformers. This operation enables Transformers to merge data from different tokens and then to compute dot products of input and target data downstream, allowing for in-context learning by gradient descent to emerge.

## Discussion

Transformers show remarkable in-context learning behavior. Mechanisms based on attention, associative memory and copying by induction heads are currently the leading explanations for this remarkable feature of learning within the Transformer forward pass. In this paper, we put forward the hypothesis, similar to [Garg et al. (2022)](#b20) and [Akyürek et al. (2023)](#b0), that Transformer's in-context learning is driven by gradient descent, in short -Transformers learn to learn by gradient descent based on their context. Viewed through the lens of meta-learning, learning Transformer weights corresponds to the outer-loop which then enables the forward pass to transform tokens by gradient-based optimization.

To provide evidence for this hypothesis, we build on Schlag et al. ( [2021](#)) that already provide a linear self-attention layer variant with (fast-)inner loop learning by the error-correcting delta rule [(Widrow & Hoff, 1960)](#b50). We diverge from their setting and focus on (in-context) learning where we specifically construct a dataset by considering neighboring elements in the input sequence as input-and target training pairs, see assumptions of Proposition 1. This construction could be realized, for example, due to the model learning to implement a copying layer, see section 4 and proposition 3, and allows us to provide a simple and different construction to [Schlag et al. (2021)](#b43) that solely is built on the standard linear, and approximately softmax, self-attention layer but still implements gradient descent based learning dynamics. We, therefore, are able to explain gradient descent based learning in these standard architectures. Furthermore, we extend this construction based on a single self-attention layer and provide an explanation of how deeper K-layer Transformer models implement principled K-step gradient descent learning, which deviates again from Schlag et al. and allows us to identify that deep Transformers implement GD++, an accelerated version of gradient descent.

We highlight that our construction of gradient descent and GD++ is not suggestive but when training multi-layer selfattention-only Transformers on simple regression tasks, we provide strong evidence that the construction is actually found. This allows us, at least in our restricted problems settings, to explain mechanistically in-context learning in trained Transformers and its close resemblance to GD observed by related work. Further work is needed to incorporate regression problems with noisy data and weight regularization into our hypothesis. We speculate aspects of learning in these settings are meta-learned -e.g., the weight magnitudes to be encoded in the self-attention weights. Additionally, we did not analyze logistic regression for which one possible weight construction is already presented in [Zhmoginov et al. (2022)](#b54).

Our refined understanding of in-context learning based on gradient descent motives us to investigate how to improve it. We are excited about several avenues of future research. First, to exceed upon a single step of gradient descent in every self-attention layer it could be advantageous to incorporate so called declarative nodes [(Amos & Kolter, 2017;](#b1)[Bai et al., 2019;](#b4)[Gould et al., 2021;](#b22)[Zucchet & Sacramento, 2022)](#b55) into Transformer architectures. This way, we would treat a single self-attention layer as the solution of a fully optimized regression loss leading to possibly more efficient architectures. Second, our findings are restricted to small Transformers and simple regression problems. We are excited to delve deeper into research trying to understand how further mechanistic understanding of Transformers and incontext learning in larger models is possible and to what extend. Third, we are excited about targeted modifications to Transformer architectures, or their training protocols, leading to improved gradient descent based learning algorithms or allow for alternative in-context learners to be implemented within Transformer weights, augmenting their functionality, as e.g. in [Dai et al. (2023)](#b14). Finally, it would be interesting to analyze in-context learning in HyperTransformers [(Zhmoginov et al., 2022)](#b54) that produce weights for target networks and already offer a different perspective on merging Transformers and meta-learning. There, Transformers transform weights instead of data and could potentially allow for gradient computations of weights deep inside the target network lifting the limitation of GD on linear models analyzed here.

## A. Appendix

A.1. Proposition 1

First, we highlight the dependency on the tokens e i of the linear self-attention operation

$e j ← e j + LSA θ ({e 1 , . . . , e N }) = e j + h P h V h K T h q h,j = e j + h P h i v h,i ⊗ k h,i q h,j = e j + h P h W h,V i e h,i ⊗ e h,i W T h,K W h,Q e j (6)$with ⊗ the outer product between two vectors. With this we can now easily draw connections to one step of gradient descent on

$L(W ) = 1 2N N i=1 ∥W x i -y i ∥ 2$with learning rate η which yields weight change

$∆W = -η∇ W L(W ) = - η N N i=1 (W x i -y i )x T i . (7$$)$We first restate Proposition 1. Given a 1-head linear attention layer and the tokens e j = (x j , y j ), for j = 1, . . . , N , one can construct key, query and value matrices W K , W Q , W V as well as the projection matrix P such that a Transformer step on every token e j is identical to the gradient-induced dynamics e j ← (x j , y j ) + (0, -∆W x j ) = (x i , y i ) + P V K T q j such that e j = (x j , y j -∆y j ). For the test data token (x N +1 , y N +1 ) the dynamics are identical.

We provide the weight matrices in block form: W K = W Q = I x 0 0 0 with I x and I y the identity matrices of size N x and N y respectively. Furthermore, we set W V = 0 0 W 0 -I y with the weight matrix W 0 ∈ R Ny×Nx of the linear model we wish to train and P = η N I with identity matrix of size N x + N y . With this simple construction we obtain the following dynamics

$x j y j ← x j y j + η N I N i=1 0 0 W 0 -I y x i y i ⊗ I x 0 0 0 x i y i I x 0 0 0 x j y j = x j y j + η N I N i=1 0 W 0 x i -y i ⊗ x i 0 x j 0 = x j y j + 0 -∆W x j . (8$$)$for every token e j = (x j , y j ) including the query token e N +1 = e test = (x test , -W 0 x test ) which will give us the desired result.

## A.2. Comparing the out-of-distribution behavior of trained Transformers and GD

We provide more experimental results when comparing GD with tuned learning rate η and data transformation scalar γ and the trained Transformer on other data distributions than provided during training, see Figure [6](#). We do so by changing the in-context data distribution and measure the loss of both methods averaged over 10.000 tasks when either changing α that 1) affects the input data range x ∼ U (-α, α) Nx or 2) the teacher by αW with W ∼ N (0, I). This setups leads to results shown in the main text, in the first two columns of Figure [6](#) and in the corresponding plots of Figure [7](#fig_6). Although the match for deeper architectures starts to become worse, overall the trained Transformers behaves remarkably similar to GD and GD ++ for layer depth greater than 1.

Furthermore, we try GD and the trained Transformer on input distributions that it never has seen during training. Here, we chose by chance of 1/3 either a normal, exponential or Laplace distribution (with JAX default parameters) and depict the average loss value over 10.000 tasks where the α value now simply scales the input values that are sampled from one of the distributions αx. The teacher scaling is identical to the one described above. See for results the two right columns of Figure [6](#), where we see almost identical behavior for recurrent architectures with less good match for deeper non-recurrent architectures far away from the training range of α = 1. Note that for deeper Transformers (K > 2) the corresponding GD and GD ++ version, see for more experimental details Appendix section A.12, we include a harsh clipping of the token values after every step of transformation between [-10, 10] (for the trained TF and GD) to improve training stability. Therefore, the loss increase is restricted to a certain value and plateaus.

## A.3. Linear mode connectivity between the weight construction of Prop 1 and trained Transformers

In order to interpolate between the construction θ GD and the trained weights of the Transformer θ, we need to correct for some scaling ambiguity. For clarification, we restate here the linear self-attention operation for a single head

$e j ←e j + P W V i e i ⊗ e i W T K W Q e j (9) = e j + W P V i e i ⊗ e i W KQ e j(10)$Now, to match the weight construction of Prop. 1 we have the aim for the matrix product W KQ to match an identify matrix (except for the last diagonal entry) after re-scaling. Therefore we compute the mean of the diagonal of the matrix product of the trained Transformer weights W KQ which we denote by β. After resealing both operations i.e. W KQ ← W KQ /β and W P V ← W P V β we interpolate linearly between the matrix products of GD as well as these rescaled trained matrix products i.e. W I,KQ = (W GD,KQ + W T F,KQ )/2 as well as W I,P V = (W GD,P V + W T F,P V )/2. We use these parameters to obtain results throughout the paper denote with Interpolated. We do so for GD as well as GD ++ when comparing to  Results comparable to the deep recurrent Transformer, see Figure [7](#fig_6), but now with 12 independent Transformer blocks including MLPs and 4-head linear self-attention. We omit LayerNorm. We again observe a close resemblance of the trained Transformers and GD ++ . We hypotheses that even when equipped with multiple heads and MLPs, Transformers approximate GD ++ .

To get a simple and clean construction, we choose wlog x j ∈ R 2N +1 and (0, y j ) ∈ R 2N +1 as well as model the positional encodings as unit vectors p j ∈ R 2N +1 and concatenate them to the tokens i.e. e j = (x j/2 , p j ). We wish for a construction that realizes

$e j ← x j/2 p j + P V K T W Q x j/2 p j (11) = x j/2 p j + 0 y j/2+1 -p j . (12$$)$This means that a token replaces its own positional encoding by coping the target data of the next token to itself leading to e j = (x j/2 , 0, y j/2+1 ), with slight abusive of notation. This can simply be realized by (for example) setting P = I,

$W V = 0 0 I x -I x,of f , W K = 0 0 0 I x and W Q = 0 0 0 I T x,of f$with I x,of f the lower diagonal identity matrix fo size N x . Note that then simply K T W Q e j = p j+1 i.e. it chooses the j + 1 element of V which stays p j+1 if we apply the softmax operation on K T q j . Since the j + 1 entry of V is (0, y j/2+1 -p j ) we obtain the desired result.

For the (toy-)regression problems considered in this manuscript, the provided result would give N/2 tokens for which we also copy (parts) of x j underneath y j . This is desired for modalities such as language where every two tokens could be considered an in-and output pair for the implicit autoregressive inner-loop loss. These tokens do not have be necessarily next to each other, see for this behavior experimental findings presented in [(Olsson et al., 2022)](#b37). For the experiments conducted here, one solution is to zero out these tokens which could be constructed by a two-head self-attention layer that given uneven j simply subtracts itself resulting in a zero token. For all even tokens, we use the construction from above which effectively coincides with the token construction required in Proposition 1. We observe that different dampening strengths affect the generalization of both methods with slightly better robustness for GD which matching performance for 50 steps when λ = 0.75.

## A.6. Dampening the self-attention layer

As an additional out-of-distribution experiment, we test the behavior when repeating a single LSA-layer trained to lower our objective, see equation 5, with the aim to repeat the learned learning/update rule. Note that GD as well as the selfattention layer were optimized to be optimal for one step. For GD we line search the otpimal learning rate η on 10.000 task. Interestingly, for both methods we observe quick divergence when applied multiple times, see left plot of Figure [10](#fig_8). Nevertheless, both of our update functions are described by a linear self-attention layer for which we can control the norm, post training, by a simple scale which we denote as λ. This results in the new update y test + λ∆W x test for GD and y test + λP V K T W Q x test for the trained self-attention layer which effectively re-tunes the learning rate for GD and the trained self-attention layer. Intriguingly, both methods do generalize similarly well (or poorly) on this out-of-distribution experiment when changing λ, see again Figure [10](#fig_8). We show in Figure [1](#fig_0) the behavior for λ = 0.75 for which we see both methods steadily decreasing the loss within 50 steps.

## A.7. Sine wave regression

For the sine wave regression tasks, we follow [(Finn et al., 2017)](#b18) and other meta-learning literature and sample for each task an amplitude a ∼ U (0.1, 5) and a phase ρ ∼ U (0, π). Each tasks consist of N = 10 data points where inputs are sampled x ∼ U (-5, 5) and targets computed by y = a sin(ρ + x). We choose here for the first time, for GD as well as for the Transformer, an input embedding emb that maps tokens e i = (x i , y i ) into a 40 dimensional space emb(e i ) = W emb e i through an affine projection without bias. We skip the first self-attention layer but, as usually done in Transformers, then transform the embedded tokens through an MLP m with a single hidden layer, widening factor of 4 (160 hidden neuros) and GELU nonlinearity [(Hendrycks & Gimpel, 2016)](#b24) i.e. e j ← m(emb(e j )) + emb(e j ).

We interpret the last entry of the transformed tokens as the (transformed) targets and the rest as a higher-dimensional input data representation on which we train a model with a single gradient descent step. We compare the obtained meta-learned GD solution with training a Transformer on the same token embeddings but instead learn a self-attention layer. Note that the embeddings of the tokens, including the transformation through the MLP, are not dependent on an interplay between the tokens. Furthermore, the initial transformation is dependent on e i = (x i , y i ), i.e., input as well as on the target data except for the query token for which y test = 0. This means that this construction is, except for the additional dependency on targets, close to a large corpus of meta-learning literature that aims to find a deep representation optimized for (fast) fine tuning and few-shot learning. In order to compare the meta-training of the MLP and the Transformer, we choose the same seed to initialize the network weights for the MLPs and the input embedding trained by meta-learning i.e. backprop through training or the Transformer. This leads to the plots and almost identical learned initial function and updated functions shown in Figure [4](#fig_4).

A.8. Proposition 2 and connections between gradient descent, kernelized regression and kernel smoothing

Let's consider the data transformation induced by an MLP m(x) and a residual connection commonly used in Transformer blocks i.e. e j ← e j + m(e j ) = (x j , y j ) + ( m(x j ), 0) = (m(x j ), y j ) with m(x j ) = x j + m(x j ) and m not changing the targets y. When simply applying Proposition 1, it is easy to see that given this new token construction, a linear self-attention layer can induce the token dynamics e j ← (m(x j ), y j ) + (0, -∆W m(x j )) with ∆W = -η∇L(W ) given the loss function

$L(W ) = 1 2N N i=1 ||W m(x i ) -y i || 2 .$Interestingly, for the test token e test = (x test , 0) this induces, after a multiplication with -1, an initial prediction after a single Transformer block given by

$ŷ = ∆W m(x test ) = -η∇ W L(0)m(x test ) = N i=1 y i m(x i ) T m(x test ) = N i=1 y i k(x i , x test )(13)$with m(x i ) T m(x test ) = k(x i , x test ) ∈ R interpreted as a kernel function. Concluding, we see that the combination of MLPs and a single self-attention layer can lead to dynamics induced when descending a kernelized regression (squared error) loss with a single step of gradient-descent.

Interestingly, when choosing W 0 = 0, we furthermore see that a single self-attention layer or Transformer block can be regarded as doing nonparametric kernel smoothing ŷ = N i=1 y i k(x i , x test ) based on the data given in-context [(Nadaraya, 1964;](#b36)[Watson, 1964)](#b49). Note that we made a particular choice of kernel function here and that this view still holds when m(x j ) = 1 i.e. consider Transformers without MLPs or leverage the well-known view of softmax self-attention layer as a kernel function used to measure similarity between tokens (e.g. [Choromanski et al., 2021;](#b13)[Zhang et al., 2021)](#b52). Thus, implementing one step of gradient descent through a self-attention layer (w/wo softmax nonlinearity) is equivalent to performing kernel smoothing estimation. We however argue that this nonparametric kernel smoothing view of in-context learning is limited, and arises from looking only at a single self-attention layer. When considering deeper Transformer architectures, we see that multiple Transformer blocks can iteratively transform the targets based on multiple steps of gradient descent leading to minimization of a kernelized squared error loss L(W ). One way to obtain a suitable construction is by neglecting MLPs everywhere except in the first Transformer block. We leave the study of the exact mechanics, especially how the Transformer makes use of possibility transforming the targets through the MLPs, and the possibility of iteratively changing the kernel function throughout depth for future study.

A.9. Linear vs. softmax self-attention as well LayerNorm Transformers

Although linear Transformers and their variants have been shown to be competitive with their softmax counterpart [(Irie et al., 2021)](#b28), the removal of this nonlinearity is still a major departure from classic Transformers and more importantly from the Transformers used in related studies analyzing in-context learning. In this section we investigate whether and when gradient-based learning emerges in trained softmax self-attention layers, and we provide an analytical argument to back our findings.

First, we show, see Figure [12](#fig_1), that a single layer of softmax self-attention is not able to match GD performance. We tuned the learning rate as well as the weight initialization but found no significant difference over the hyperparameters we used througout this study. In general, we hypothesize that GD is an optimal update given the limited capacity of a single layer of (single-head) self-attention. We therefore argue that the softmax induces (at best) a linear offset of the matrix product of training data and query vector softmax

$(K T q j ) = (e k T 1 qj , . . . , e k T N qj ) T /( i e k T i qj ) (14) = (e x T 1 W KQ xj , . . . , e x T N W KQ xj ) T /( i e x T i W KQ xj )(15)$$≈ (1 + x T 1 W KQ x j , . . . , 1 + x T N W KQ x j ) T /( i 1 + x T i W KQ x j ) (16) ∝ K T q j + ϵ(17)$proportional to a factor dependent on all {x τ,i } N +1 i=1 . We speculate that the dependency on the specific task τ , for large N x vanishes or that the x-dependent value matrix could introduce a correcting effect. In this case the softmax operation introduces an additive error w.r.t. to the optimal GD update. To overcome this disadvantageous offset, the Transformer can (approximately) introduce a correction with a second self-attention head by a simple subtraction i.e.

$P 1 V 1 softmax(K T 1 W Q x j ) + P 2 V 2 softmax(K T 2 W Q x j )(18)$≈ P V ((1 + x T 1 W 1,KQ x j , . . . , 1 + x T N W 1,KQ x j ) -(1 + x T 1 W 2,KQ x j , . . . , 1 + x T N W 2,KQ x j ))

= P V (x T 1 (W 1,KQ -W 2,KQ )x j , . . . , x T N (W 1,KQ -W 2,KQ )x j ) (20)

$∝ P V K T q j .(21)$(a) Comparing one step of GD with a trained softmax one-headed self-attention layer. Comparing trained two-headed and one-headed single-layer softmax self-attention with 1 step of gradient descent on linear regression tasks. Left column: Softmax self-attention is not able to match gradient descent performance with hand-tuned learning rate, but adding a second attention head significantly reduces the gap, as expected by our analytical argument. Center left: The alignment suffers significantly for single-head softmax SA. We observe good but not as precise alignment when compared to linear Transformers for the two-headed softmax SA layer. Center right & right: The two-headed self-attention compared to the single-head layer shows similar robust out-of-distribution behavior compared to gradient descent.

## A.10. Details of curvature correction

We give here a precise construction showing how to implement in a single head, a step of GD and the discussed data transformation, resulting in GD ++ . Recall again the linear self-attention operation with a single head e j ←e j + P W V i e i ⊗ e i W T K .

(

$)22$We provide again the weight matrices in block form of the construction of Prop. 1 but now enabling additionally our described data transformation: W K = W Q = I x 0 0 0 with I x the identity matrix of size N x , I y od size N y resp. x i W x i -y i ⊗ x i 0

$x j 0 = x j y j + -γXX T x j -∆W x j . (23$$)$for every token e j = (x j , y j ) including the query token e N +1 = e test = (x test , 0) which will give us the desired result.

Why does GD ++ perform better? We give here one possible explanation of the superior performance of GD ++ compared to GD. Note that there is a close resemblance of the GD transformation and a heavily truncated Neuman series approximation of the inverse XX T . We provide here a more heuristic explanation for the observed acceleration.

Given γ ∈ R, GD ++ transforms every input according to x i ← x i -γXX T x i = (I -γXX T )x i . We can therefore look at the change of squared regression loss L(W ) = 1 instable when we approach GD with an optimal learning rate. In order to stabilize training, we simply clipped the token values to be in the range of [[-10, 10]](#).

• When applicable we use standard positional encodings of size 20 which we concatenated to all tokens.

• For simplicity, and to follow the provided weight construction closely, we did use square key, value and query parameter matrix in all experiments.

• The training length varied throughout our experimental setups and can be read off our training plots in the article.

• When training meta-parameters for gradient descent i.e. η and γ we used an identical training setup but usually training required much less iterations.

• In all experiments we choose inital W 0 = 0 for gradient descent trained models.

Inspired by [(Garg et al., 2022)](#b20), we additionally provide results when training a single linear self-attention layer on a fixed number of training tasks. Therefore, we iterate over a single fixed batch of size B instead of drawing new batch of tasks at every iteration. Results can be found in Figure [16](#fig_0). Intriguingly, we find that (meta-)gradient descent finds Transformer weights that align remarkable well with the provided construction and therefore gradient descent even when provided with an arguably very small number of training tasks. We argue that this again highlights the strong inductive bias of the LSA-layer to match (approximately) gradient descent learning in its forward pass.

![Figure 1. Illustration of our hypothesis: gradient-based optimization and attention-based in-context learning are equivalent. Left: Learning a neural network output layer by gradient descent on a dataset D train . The task-shared meta-parameters θ are obtained by meta-learning with the goal that after adjusting the neural network output layer, the model generalizes well on unseen data. Center: Illustration of a Transformer that adjusts its query prediction on the data given in-context i.e. t θ (xquery; D context ). The weights of the Transformer are optimized to predict the next token yquery. Right: Our results confirm the hypothesis that learning with K steps of gradient descent on a dataset D train (green part of the left plot) matches trained Transformers with K linear self-attention layers (central plot) when given D train as in-context data D context .]()

![Figure 2. Comparing one step of GD with a trained single linear self-attention layer. Outer left: Trained single LSA layer performance is identical to the one of gradient descent. Center left: Almost perfect alignment of GD and the model generated by the SA layer after training, measured by cosine similarity and the L2 distance between models as well as their predictions. Center right: Identical loss of GD, the LSA layer model as well as the model obtained by interpolating between the construction and the optimized LSA layer weights for different N = Nx. Outer right: The trained LSA layer, gradient descent and their interpolation show identically loss (in log-scale) when provided input data different than during training i.e. with scale of 1. We display the mean/std. or the single runs of 5 seeds.]()

![Figure3. Far left column: The trained TF performance surpasses standard GD but matches GD ++ , our GD variant with simple iterative data transformation. On both cases, we tuned the gradient descent learning rates as well as the scalar γ which governs the data transformation H(X). Center left & center right columns: We measure the alignment between the GD as well as the GD ++ models and the trained TF. In both cases the TF aligns well with GD in the beginning of training but aligns much better with GD ++ after training. Far right column: TF performance (in log-scale) mimics the one of GD ++ well when testing on OOD tasks (α ̸ = 1).]()

![Figure5. Training a two layer SA-only Transformer using the standard token construction. Left: The loss of trained TFs matches one step of GD, not two, and takes an order of magnitude longer to train. Right: Norm of the partial derivatives of the output of the first self-attention layer w.r.t. input tokens. Before the Transformer performance jumps to the one of GD, the first layer becomes highly sensitive to the next token.]()

![Figure4. Sine wave regression: comparing trained Transformers with meta-learned MLPs for which we adjust the output layer with one step of gradient descent. Left: Plots of the learned initial functions as well as the adjusted functions through either a layer of self-attention or a step of GD. We observe similar initial functions as well as solutions for the trained TF compared fine-tuning a meta-learned MLP. Center: The performance of the trained Transformer is matched by meta-learned MLPs. Left: We observe strong alignment when comparing the prediction as well as the partial derivatives of the the meta-learned MLP and the trained Transformer.]()

![Figure 6. Left & center left column: Comparing Transformers, GD and their weight interpolation on rescaled training distributions. In all setups, the trained Transformer behaves remarkably similar to GD or GD ++ . Right & center right: Comparing Transformers, GD and their weight interpolation on data distributions never seen during training. Again, in all setups, the trained Transformer behaves remarkably similar to GD or GD ++ with less good match for deep non-recurrent Transformers far away from training regimes.]()

![Figure 7. Comparing ten steps of gradient descent with trained recurrent ten-layer Transformers. Results comparable to recurrent Transformer with two layers, see Figure 3, but now with 10 repeated layers. We again observe for deeper recurrent linear self-attention only Transformers that overall GD ++ and the trained Transformer align very well with one another and are again interpolatable leading to very similar behavior insight as well as outside training situations. Note the inferior performance to the non-recurrent five-layer Transformer which highlights the importance on specific learning rate as well γ parameter per layer/step.]()

![Figure 8. Comparing twelve steps of GD ++ with a trained twelve-layer Transformers with MLPs and 4 headed linear self-attention layer. Results comparable to the deep recurrent Transformer, see Figure7, but now with 12 independent Transformer blocks including MLPs and 4-head linear self-attention. We omit LayerNorm. We again observe a close resemblance of the trained Transformers and GD ++ . We hypotheses that even when equipped with multiple heads and MLPs, Transformers approximate GD ++ .]()

![Figure10. Roll-out experiments: applying a trained single linear self-attention layer multiple times. We observe that different dampening strengths affect the generalization of both methods with slightly better robustness for GD which matching performance for 50 steps when λ = 0.75.]()

![Figure12. Comparing trained two-headed and one-headed single-layer softmax self-attention with 1 step of gradient descent on linear regression tasks. Left column: Softmax self-attention is not able to match gradient descent performance with hand-tuned learning rate, but adding a second attention head significantly reduces the gap, as expected by our analytical argument. Center left: The alignment suffers significantly for single-head softmax SA. We observe good but not as precise alignment when compared to linear Transformers for the two-headed softmax SA layer. Center right & right: The two-headed self-attention compared to the single-head layer shows similar robust out-of-distribution behavior compared to gradient descent.]()

![, we set W V = I x 0 W -I y with the weight matrix W ∈ R Ny×Nx of the linear model we wish to train and]()

![Figure 15. Phase transitions during training. Left: Loss based on 10 different random seeds when optimizing a single-headed selfattention layer. We observe for some seeds very long initial phases of virtually zero progress after which the loss drops suddenly to the desired GD loss. Center: The same experiment but optimizing a two-headed self-attention layer. We observe fast and robust convergence to the loss of GD. Right: Training a single Transformer block i.e. a self-attention layer with MLP and a reduced training set size of 8192 tasks. We observe grokking like train and test loss phase transitions where test set first increases drastically before experiencing a sudden drop in loss almost matching the desired GD loss of 0.2.]()

Department of Computer Science, ETH Zürich, Zürich, Switzerland

Google Research. Correspondence to: Johannes von Oswald <voswaldj@ethz.ch>.

Main experiments can be reproduced with notebooks provided under the following link: https://github.com/ google-research/self-organising-systems/ tree/master/transformers_learn_icl_by_gd

Ni=0 (W x i -y i ) 2 induced by this transformation i.e. L ++ (W ) =

