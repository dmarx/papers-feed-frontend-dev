<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transformers Learn In-Context by Gradient Descent</title>
				<funder ref="#_EGxB53Q">
					<orgName type="full">Swiss National Science Foundation</orgName>
				</funder>
				<funder ref="#_XkSXpKd">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-05-31">31 May 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Johannes</forename><surname>Von Oswald</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Eyvind</forename><surname>Niklasson</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ettore</forename><surname>Randazzo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">João</forename><surname>Sacramento</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Alexander</forename><surname>Mordvintsev</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Max</forename><surname>Vladymyrov</surname></persName>
						</author>
						<title level="a" type="main">Transformers Learn In-Context by Gradient Descent</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-05-31">31 May 2023</date>
						</imprint>
					</monogr>
					<idno type="MD5">3BE8023F02140FCA5A958C171C05EB4F</idno>
					<idno type="arXiv">arXiv:2212.07677v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>At present, the mechanisms of in-context learning in Transformers are not well understood and remain mostly an intuition. In this paper, we suggest that training Transformers on auto-regressive objectives is closely related to gradient-based metalearning formulations. We start by providing a simple weight construction that shows the equivalence of data transformations induced by 1) a single linear self-attention layer and by 2) gradientdescent (GD) on a regression loss. Motivated by that construction, we show empirically that when training self-attention-only Transformers on simple regression tasks either the models learned by GD and Transformers show great similarity or, remarkably, the weights found by optimization match the construction. Thus we show how trained Transformers become mesa-optimizers i.e. learn models by gradient descent in their forward pass. This allows us, at least in the domain of regression problems, to mechanistically understand the inner workings of in-context learning in optimized Transformers. Building on this insight, we furthermore identify how Transformers surpass the performance of plain gradient descent by learning an iterative curvature correction and learn linear models on deep data representations to solve non-linear regression tasks. Finally, we discuss intriguing parallels to a mechanism identified to be crucial for in-context learning termed induction-head <ref type="bibr" target="#b37">(Olsson et al., 2022)</ref> and show how it could be understood as a specific case of in-context learning by gradient descent learning within Transformers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years Transformers (TFs; <ref type="bibr" target="#b47">Vaswani et al., 2017)</ref> have demonstrated their superiority in numerous benchmarks and various fields of modern machine learning, and have emerged as the de-facto neural network architecture used for modern AI <ref type="bibr" target="#b15">(Dosovitskiy et al., 2021;</ref><ref type="bibr" target="#b51">Yun et al., 2019;</ref><ref type="bibr" target="#b9">Carion et al., 2020;</ref><ref type="bibr" target="#b23">Gulati et al., 2020)</ref>. It has been hypothesised that their success is due in part to a phenomenon called in-context learning <ref type="bibr" target="#b8">(Brown et al., 2020;</ref><ref type="bibr" target="#b35">Liu et al., 2021)</ref>: an ability to flexibly adjust their prediction based on additional data given in context (i.e. in the input sequence itself). In-context learning offers a seemingly different approach to few-shot and meta-learning <ref type="bibr" target="#b8">(Brown et al., 2020)</ref>, but as of today the exact mechanisms of how it works are not fully understood. It is thus of great interest to understand what makes Transformers pay attention to their context, what the mechanisms are, and under which circumstances, they come into play <ref type="bibr">(Chan et al., 2022b;</ref><ref type="bibr" target="#b37">Olsson et al., 2022)</ref>.</p><p>In this paper, we aim to bridge the gap between in-context and meta-learning, and show that in-context learning in Transformers can be an emergent property approximating gradient-based few-shot learning within its forward pass, see Figure <ref type="figure" target="#fig_0">1</ref>. For this to be realized, we show how Transformers (1) construct a loss function dependent on the data given in sequence and (2) learn based on gradients of that loss. We will first focus on the latter, the more elaborate learning task, in sections 2 and 3, after which we provide evidence for the former in section 4.</p><p>We summarize our contributions as follows<ref type="foot" target="#foot_0">foot_0</ref> :</p><p>• We construct explicit weights for a linear self-attention layer that induces an update identical to a single step of gradient descent (GD) on a mean squared error loss. Additionally, we show how several self-attention layers can iteratively perform curvature correction improving on plain gradient descent.</p><p>• When optimized on linear regression datasets, we demonstrate that linear self-attention-only Transform- ers either converge to our weight construction and therefore implement gradient descent, or generate linear models that closely align with models trained by GD, both in in-and out-of-distribution validation tasks.</p><p>• By incorporating multi-layer-perceptrons (MLPs) into the Transformer architecture, we enable solving nonlinear regression tasks within Transformers by showing its equivalence to learning a linear model on deep representations. We discuss connections to kernel regression as well as nonparametric kernel smoothing methods. Empirically, we compare meta-learned MLPs and a single step of GD on its output layer with trained Transformers and demonstrate striking similarities between the identified solutions.</p><p>• We resolve the dependency on the specific token construction by providing evidence that learned Transformers first encode incoming tokens into a format amenable to the in-context gradient descent learning that occurs in the later layers of the Transformer.</p><p>These findings allow us to connect learning Transformer weights and the concept of meta-learning a learning algorithm <ref type="bibr" target="#b44">(Schmidhuber, 1987;</ref><ref type="bibr" target="#b25">Hinton &amp; Plaut, 1987;</ref><ref type="bibr" target="#b5">Bengio et al., 1990;</ref><ref type="bibr" target="#b10">Chalmers, 1991;</ref><ref type="bibr" target="#b45">Schmidhuber, 1992;</ref><ref type="bibr" target="#b46">Thrun &amp; Pratt, 1998;</ref><ref type="bibr" target="#b26">Hochreiter et al., 2001;</ref><ref type="bibr" target="#b2">Andrychowicz et al., 2016;</ref><ref type="bibr" target="#b3">Ba et al., 2016;</ref><ref type="bibr" target="#b30">Kirsch &amp; Schmidhuber, 2021)</ref>. In this extensive research field, meta-learning is typically regarded as learning that takes place on various time scales namely fast and slow. The slowly changing parameters control and prepare for fast adaptation reacting to sudden changes in the incoming data by e.g. a context switch. Notably, we build heavily on the concept of fast weights <ref type="bibr" target="#b45">(Schmidhuber, 1992)</ref> which has shown to be equivalent to linear self-attention <ref type="bibr" target="#b43">(Schlag et al., 2021)</ref> and show how optimized Transformers implement interpretable learning algorithms within their weights.</p><p>Another related meta-learning concept, termed MAML <ref type="bibr" target="#b18">(Finn et al., 2017)</ref>, aims to meta-learn a deep neural network initialization which allows for fast adaptation on novel tasks.</p><p>It has been shown that in many circumstances, the solution found can be approximated well when only adapting the output layer i.e. learning a linear model on a meta-learned deep data representations <ref type="bibr" target="#b18">(Finn et al., 2017;</ref><ref type="bibr" target="#b17">Finn &amp; Levine, 2018;</ref><ref type="bibr" target="#b21">Gordon et al., 2019;</ref><ref type="bibr" target="#b32">Lee et al., 2019;</ref><ref type="bibr" target="#b42">Rusu et al., 2019;</ref><ref type="bibr" target="#b40">Raghu et al., 2020;</ref><ref type="bibr" target="#b48">von Oswald et al., 2021)</ref>. In section 3, we show the equivalence of this framework to in-context learning implemented in a common Transformer block i.e. when combining self-attention layers with a multi-layerperceptron.</p><p>In the light of meta-learning we show how optimizing Transformer weights can be regarded as learning on two time scales. More concretely, we find that solely through the pressure to predict correctly Transformers discover learning algorithms inside their forward computations, effectively meta-learning a learning algorithm. Recently, this concept of an emergent optimizer within a learned neural network, such as a Transformer, has been termed "mesa-optimization" <ref type="bibr" target="#b27">(Hubinger et al., 2019)</ref>. We find and describe one possible realization of this concept and hypothesize that the in-context learning capabilities of language models emerge through mechanisms similar to the ones we discuss here.</p><p>Transformers come in different "shapes and sizes", operate on vastly different domains, and exhibit varying forms of phase transitions of in-context learning <ref type="bibr" target="#b31">(Kirsch et al., 2022;</ref><ref type="bibr">Chan et al., 2022a)</ref>, suggesting variance and significant complexity of the underlying learning mechanisms. As a result, we expect our findings on linear self-attention-only Transformers to only explain a limited part of a complex process, and it may be one of many possible methods giving rise to in-context learning. Nevertheless, our approach provides an intriguing perspective on, and novel evidence for, an incontext learning mechanism that significantly differs from existing mechanisms based on associative memory <ref type="bibr" target="#b41">(Ramsauer et al., 2020)</ref>, or by the copying mechanism termed induction heads identified by <ref type="bibr" target="#b37">(Olsson et al., 2022)</ref>. We, therefore, state the following Hypothesis 1 (Transformers learn in-context by gradient descent). When training Transformers on auto-regressive tasks, in-context learning in the Transformer forward pass is implemented by gradient-based optimization of an implicit auto-regressive inner loss constructed from its in-context data.</p><p>We acknowledge work done in parallel, investigating the same hypothesis. <ref type="bibr" target="#b0">Akyürek et al. (2023)</ref> puts forward a weight construction based on a chain of Transformer layers (including MLPs) that together implement a single step of gradient descent with weight decay. Similar to work done by <ref type="bibr" target="#b20">Garg et al. (2022)</ref>, they then show that trained Transformers match the performance of models obtained by gradient descent. Nevertheless, it is not clear that optimization finds Transformer weights that coincide with their construction.</p><p>Here, we present a much simpler construction that builds on <ref type="bibr" target="#b43">Schlag et al. (2021)</ref> and only requires a single linear selfattention layer to implement a step of gradient descent. This allows us to (1) show that optimizing self-attention-only Transformers finds weights that match our weight construction (Proposition 1), demonstrating its practical relevance, and (2) explain in-context learning in shallow two layer Transformers intensively studied by <ref type="bibr" target="#b37">Olsson et al. (2022)</ref>. Therefore, although related work provides comprehensive empirical evidence that Transformers indeed seem to implement gradient descent based learning on the data given in-context, we will in the following present mechanistic verification of this hypothesis and provide compelling evidence that our construction, which implements GD in a Transformer forward pass, is found in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Linear self-attention can emulate gradient descent on a linear regression task</head><p>We start by reviewing a standard multi-head self-attention (SA) layer with parameters θ. A SA layer updates each element e j of a set of tokens {e 1 , . . . , e N } according to</p><formula xml:id="formula_0">e j ← e j + SA θ (j, {e 1 , . . . , e N }) = e j + h P h V h softmax(K T h q h,j )<label>(1)</label></formula><p>with P h , V h , K h the projection, value and key matrices, respectively, and q h,i the query, all for the h-th head. To simplify the presentation, we omit bias terms here and throughout. The columns of the value</p><formula xml:id="formula_1">V h = [v h,1 , . . . , v h,N ] and key K h = [k h,1 , . . . , k h,N ] matrices consist of vectors v h,i = W h,V e i and k h,i = W h,K e i ;</formula><p>likewise, the query is produced by linearly projecting the tokens, q h,j = W h,Q e j .</p><p>The parameters θ = {P h , W h,V , W h,K , W h,Q } h of a SA layer consist of all the projection matrices, of all heads.</p><p>The self-attention layer described above corresponds to the one used in the standard Transformer model. Follow-ing <ref type="bibr" target="#b43">Schlag et al. (2021)</ref>, we now introduce our first (and only) departure from the standard model, and omit the softmax operation in equation 1, leading to the linear selfattention (LSA) layer e j ← e j + LSA θ (j, {e 1 , . . . , e N }) = e j + h P h V h K T h q h,j We next show that with some simple manipulations we can relate the update performed by an LSA layer to one step of gradient descent on a linear regression loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data transformations induced by gradient descent</head><p>We now introduce a reference linear model y(x) = W x parameterized by the weight matrix W ∈ R Ny×Nx , and a training dataset D = {(x i , y i )} N i=1 comprising of input samples x i ∈ R Nx and respective labels y i ∈ R Ny . The goal of learning is to minimize the squared-error loss:</p><formula xml:id="formula_2">L(W ) = 1 2N N i=1 ∥W x i -y i ∥ 2 . (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>One step of gradient descent on L with learning rate η yields the weight change</p><formula xml:id="formula_4">∆W = -η∇ W L(W ) = - η N N i=1 (W x i -y i )x T i .<label>(3)</label></formula><p>Considering the loss after changing the weights, we obtain</p><formula xml:id="formula_5">L(W + ∆W ) = 1 2N N i=1 ∥(W + ∆W )x i -y i ∥ 2 = 1 2N N i=1 ∥W x i -(y i -∆y i )∥ 2 (4)</formula><p>where we introduced the transformed targets y i -∆y i with ∆y i = ∆W x i . Thus, we can view the outcome of a gradient descent step as an update to our regression loss (equation 2), where data, and not weights, are updated. Note that this formulation is closely linked to predicting based on nonparametric kernel smoothing, see Appendix A.8 for a discussion.</p><p>Returning to self-attention mechanisms and Transformers, we consider an in-context learning problem where we are given N context tokens together with an extra query token, indexed by N + 1. In terms of our linear regression problem, the N context tokens e j = (x j , y j ) ∈ R Nx+Ny correspond to the N training points in D, and the N +1-th token e N +1 = (x N +1 , y N +1 ) = (x test , ŷtest ) = e test to the test input x test and the corresponding prediction ŷtest . We use the terms training and in-context data interchangeably, as well as query and test token/data, as we establish their equivalence now.</p><p>Transformations induced by gradient descent and a linear self-attention layer can be equivalent</p><p>We have re-cast the task of learning a linear model as directly modifying the data, instead of explicitly computing and returning the weights of the model (equation 4). We proceed to establish a connection between self-attention and gradient descent. We provide a construction where learning takes place simultaneously by directly updating all tokens, including the test token, through a linear self-attention layer.</p><p>In other words, the token produced in response to a query (test) token is transformed from its initial value W 0 x test , where W 0 is the initial value of W , to the post-learning prediction ŷ = (W 0 + ∆W )x test obtained after one gradient descent step. Proposition 1. Given a 1-head linear attention layer and the tokens e j = (x j , y j ), for j = 1, . . . , N , one can construct key, query and value matrices W K , W Q , W V as well as the projection matrix P such that a Transformer step on every token e j is identical to the gradient-induced dynamics e j ← (x j , y j ) + (0, -∆W x j ) = (x j , y j ) + P V K T q j such that e j = (x j , y j -∆y j ). For the test data token (x N +1 , y N +1 ) the dynamics are identical.</p><p>The simple construction can be found in Appendix A.1 and we denote the corresponding self-attention weights by θ GD .</p><p>Below, we provide some additional insights on what is needed to implement the provided LSA-layer weight construction, and further details on what it can achieve:</p><p>• Full self-attention. Our dynamics model training is based on in-context tokens only, i.e., only e 1 , . . . , e N are used for computing key and value matrices; the query token e N +1 (containing test data) is excluded. This leads to a linear function in x test as well as to the correct ∆W , induced by gradient descent on a loss consisting only of the training data. This is a minor deviation from full self-attention. In practice, this modification can be dropped, which corresponds to assuming that the underlying initial weight matrix is zero, W 0 ≈ 0, which makes ∆W in equation 8 independent of the test token even if incorporating it in the key and value matrices. In our experiments, we see that these assumptions are met when initializing the attention weights θ to small values.</p><p>• Reading out predictions. When initializing the yentry of the test-data token with -W 0 x N +1 , i.e. e test = (x test , -W 0 x test ), the test-data prediction ŷ can be easily read out by simply multiplying again by -1 the updated token, since -y N +1 + ∆y</p><formula xml:id="formula_6">N +1 = -(y N +1 - ∆y N +1 ) = y N +1 + ∆W x N +1</formula><p>. This can easily be done by a final projection matrix, which incidentally is usually found in Transformer architectures. Importantly, we see that a single head of self-attention is sufficient to transform our training targets as well as the test prediction simultaneously.</p><p>• Uniqueness. We note that the construction is not unique; in particular, it is only required that the products P W V as well as W K W Q match the construction. Furthermore, since no nonlinearity is present, any rescaling s of the matrix products, i.e., P W V s and W K W Q /s, leads to an equivalent result. If we correct for these equivalent formulations, we can experimentally verify that weights of our learned Transformers indeed match the presented construction.</p><p>• Meta-learned task-shared learning rates. When training self-attention parameters θ across a family of in-context learning tasks τ , where the data (x τ,i , y τ,i ) follows a certain distribution, the learning rate can be implicitly (meta-)learned such that an optimal loss reduction (averaged over tasks) is achieved given a fixed number of update steps. In our experiments, we find this to be the case. This kind of meta-learning to improve upon plain gradient descent has been leveraged in numerous previous approaches for deep neural networks <ref type="bibr" target="#b34">(Li et al., 2017;</ref><ref type="bibr" target="#b33">Lee &amp; Choi, 2018;</ref><ref type="bibr" target="#b38">Park &amp; Oliva, 2019;</ref><ref type="bibr" target="#b53">Zhao et al., 2020;</ref><ref type="bibr" target="#b19">Flennerhag et al., 2020)</ref>.</p><p>• Task-specific data transformations. A self-attention layer is in principle further capable of exploiting statistics in the current training data samples, beyond modeling task-shared curvature information in θ. More concretely, a LSA layer updates an input sample according to a data transformation x j ← x j + ∆x j = (I + P (X)V (X)K(X) T W Q )x j = H θ (X)x j , with X the N x × N input training data matrix, when neglecting influences by target data y i . Through H θ (X), a LSA layer can encode in θ an algorithm for carrying out data transformations which depend on the actual input training samples in X. In our experiments, we see that trained self-attention learners employ a simple form of H(X) and that this leads to substantial speed ups in for GD and TF learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Trained Transformers do mimic gradient descent on linear regression tasks</head><p>We now experimentally investigate whether trained attention-based models implement gradient-based incontext learning in their forward passes. We gradually build up from single linear self-attention layers to multi-layer nonlinear models, approaching full Transformers. In this section, we follow the assumption of Proposition 1 tightly and construct our tokens by concatenating input and target data, e j = (x j , y j ) for 1 ≤ j ≤ N , and our query token by concatenating the test input and a zero vector, e N +1 = (x test , 0). We show how to lift this assumption in the last section of the paper. The prediction ŷθ ({e τ,1 , . . . , e τ,N }, e τ,N +1 ) of the attention-based model, which depends on all tokens and on the parameters θ, is read-out from the y-entry of the updated N + 1-th token as explained in the previous section.</p><p>The objective of training, visualized in Figure <ref type="figure" target="#fig_0">1</ref>, is to minimize the expected squared prediction error, averaged over tasks</p><formula xml:id="formula_7">min θ E τ [||ŷ θ ({e τ,1 , . . . , e τ,N }, e τ,N +1 ) -y τ,test || 2 ].</formula><p>We achieve this by minibatch online minimization (by Adam <ref type="bibr" target="#b29">(Kingma &amp; Ba, 2014)</ref>): At every optimization step, we construct a batch of novel training tasks and take a step of stochastic gradient descent on the loss function:</p><formula xml:id="formula_8">L(θ) = 1 B B τ =1 ||ŷ θ ({e τ,i } N i=1 , e τ,N +1 ) -y τ,test || 2 (5)</formula><p>where each task (context) τ consists of in-context training data D τ = {(x τ,i , y τ,i )} N i=1 and test point (x τ,N +1 , y τ,N +1 ), which we use to construct our tokens {e τ,i } N +1 i=1 as described above. We denote the optimal parameters found by this optimization process by θ * . In our setup, finding θ * may be thought of as meta-learning, while learning a particular task τ corresponds to simply evaluating the model ŷθ ({e τ,1 , . . . , e τ,N }, e τ,N +1 ). Note that we therefore never see the exact same training task twice during training. See Appendix A.12, especially Figure <ref type="figure" target="#fig_0">16</ref> for an analyses when using a fixed dataset size which we cycle over during training.</p><p>We focus on solvable tasks and similarly to <ref type="bibr" target="#b20">Garg et al. (2022)</ref> generate data for each task using a teacher model with parameters W τ ∼ N (0, I). We then sample x τ,i ∼ U (-1, 1) n I and construct targets using the task-specific teacher model, y τ,i = W τ x τ,i . In the majority of our experiments we set the dimensions to N = n I = 10 and n O = 1. Since we use a noiseless teacher for simplicity, we can expect our regression tasks to be well-posed and analytically solvable as we only compute a loss on the Transformers last token, which stands in contrast to usual autoregressive training and the training setup of <ref type="bibr" target="#b20">Garg et al. (2022)</ref>. Full details and results for training with a fixed training set size may be found in Appendix A.12.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>One-step of gradient descent vs. a single trained self-attention layer</head><p>Our first goal is to investigate whether a trained single, linear self-attention layer can be explained by the provided weight construction that implements GD. To that end, we compare the predictions made by a LSA layer with trained weights θ * (which minimize equation 5) and with constructed weights θ GD (which satisfy Proposition 1).</p><p>Recall that a LSA layer yields the prediction ŷθ (x test ) = e N +1 + LSA θ ({e 1 , . . . , e N }, e N +1 ) = ∆W θ,D x test , which is linear in x test . We denote by ∆W θ,D the matrix generated by the LSA layer following the construction provided in Proposition 1, with query token e N +1 set such that the initial prediction is set to zero, ŷtest = 0. We compare ŷθ (x test ) to the prediction of the control LSA ŷθGD (x test ), which under our token construction corresponds to a linear model trained by one step of gradient descent starting from W 0 = 0. For this control model, we determine the optimal learning rate η by minimizing L(η) over a training set of 10 4 tasks through line search, with L(η) defined analogously to equation 5.</p><p>More concretely, to compare trained and constructed LSA layers, we sample T val = 10 4 validation tasks and record the following quantities, averaged over validation tasks: (1) the difference in predictions measured with the L2 norm, ∥ŷ θ (x τ,test ) -ŷθGD (x τ,test )∥, (2) the cosine similarity between the sensitivities</p><p>∂ ŷθ GD (xτ,test) ∂xtest and ∂ ŷθ (xτ,test) ∂xtest as well as (3) their difference ∥ ∂ ŷθ GD (xτ,test) ∂xtest -∂ ŷθ (xτ,test) ∂xtest</p><p>∥ again according to the L2 norm, which in both cases yields the explicit models computed by the algorithm. We show the results of these comparisons in Figure <ref type="figure" target="#fig_1">2</ref>. We find an excellent agreement between the two models over a wide range of hyperparameters. We note that as we do not have direct access to the initialization of W in the attention-based learners (it is hidden in θ), we cannot expect the models to agree exactly.</p><p>Although the above metrics are important to show similarities between the resulting learned models (in-context vs. gradient-based), the underlying algorithms could still be different. We therefore carry out an extended set of analyses:</p><p>1. Interpolation. We take inspiration on recent work <ref type="bibr" target="#b6">(Benzing et al., 2022;</ref><ref type="bibr" target="#b16">Entezari et al., 2021)</ref> that showed approximate equivalence of models found by SGD after permuting weights within the trained neural networks. Since our models are deep linear networks with respect to x test we only correct for scaling mismatches between the two models -in this case the construction that implements GD and the trained weights. As shown in Figure <ref type="figure" target="#fig_1">2</ref>, we observe (and can actually inspect by eye, see Appendix Figure <ref type="figure">9</ref>) that a simple scaling correction on the trained weights is enough to recover the weight construction implementing GD. This leads to an identical loss of GD, the trained Transformer and the linearly interpolated weights θ I = (θ + θ GD )/2. See details in Appendix A.3 on how our weight correction and interpolation is obtained.</p><p>2. Out-of-distribution validation tasks. To test if our in-context learner has found a generalizable update rule, we investigate how GD, the trained LSA layer and its interpolation behave when providing in-context data in regimes different to the ones used during training. We therefore visualize the loss increase when (1) sampling the input data from U (-α, α) Nx or (2) scaling the teacher weights by α as αW when sampling validation tasks. For both cases, we set α = 1 during training. We again observe that when training a single linear self-attention Transformer, for both interventions, the Transformer performs equally to gradient descent outside of this training setups, see Figure <ref type="figure" target="#fig_1">2</ref> as well Appendix Figure <ref type="figure">6</ref>. Note that the loss obtained through gradient descent also starts degrading quickly outside the training regime. Since we tune the learning rate for the input range [-1, 1] and one gradient step, tasks with larger input range will have higher curvature and the optimal learning rate for smaller ranges will lead to divergence and a drastic increase in loss also for GD.</p><p>3. Repeating the LSA update. Since we claim that a single trained LSA layer implements a GD-like learning rule, we further test its behavior when applying it repeatedly, not only once as in training. After we correct the learning rate of both algorithms, i.e. for GD and the trained Transformer with a dampening parameter λ = 0.75 (details in Appendix A.6), we see an identical loss decrease of both GD and the Transformer, see Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>To conclude, we present evidence that optimizing a single LSA layer to solve linear regression tasks finds weights that (approximately) coincide with the LSA-layer weight construction of Proposition 1, hence implementing a step of gradient descent, leading to the same learning capabilities on in-and out-of-distribution tasks. We comment on the random seed dependent phase transition of the loss during training in Appendix A.11.</p><p>Multiple steps of gradient descent vs. multiple layers of self-attention</p><p>We now turn to deep linear self-attention-only Transformers. The construction we put forth in Proposition 1, can be immediately stacked up over K layers; in this case, the final prediction can be read out from the last layer as before by negating the y-entry of the last test token:</p><formula xml:id="formula_9">-y N +1 + K k=1 ∆y k,N +1 = -(y N +1 - K k=1 ∆y k,N +1 ) = y N +1 + K k=1 ∆W k x N +1</formula><p>, where y k,N +1 are the test token values at layer k, and ∆y k,N +1 the change in the y-entry of the test token after applying the k-th step of self-attention, and ∆W k the k-th implicit change in the underlying linear model parameters W . When optimizing such Transformers with K layers, we observe that these models generally outperform K steps of plain gradient descent, see Figure <ref type="figure" target="#fig_2">3</ref>. Their behavior is however well described by a variant of gradient descent, for which we tune a single parameter γ defined through the transformation function H(X) which transforms the input data according to x j ← H(X)x j , with H(X) = (I -γXX T ). We term this gradient descent variant GD ++ which we explain and analyze in Appendix A.10.</p><p>To analyze the effect of adding more layers to the architecture, we first turn to the arguably simplest extension of a single SA layer and analyze a recurrent or looped 2-layer LSA model. Here, we simply repeatably apply the same layer (with the same weights) multiple times i.e. drawing the analogy to learning an iterative algorithm that applies the same logic multiple times.</p><p>Somewhat surprisingly, we find that the trained model surpasses plain gradient descent, which also results in decreasing alignment between the two models (see center left column), and the recurrent Transformer realigns perfectly with GD ++ while matching its performance on in-and out-of distribution tasks. Again, we can interpolate between the Transformer weights found by optimization and the LSAweight construction with learned η, γ, see Figure <ref type="figure" target="#fig_2">3</ref> &amp; 6.</p><p>We next consider deeper, non-recurrent 5-layer LSA-only Transformers, with different parameters per layer (i.e. no weight tying). We see that a different GD learning rate as well as γ per step (layer) need to be tuned to match the Transformer performance. This slight modification leads again to almost perfect alignment between the trained TF and GD ++ with in this case 10 additional parameters and The trained TF performance surpasses standard GD but matches GD ++ , our GD variant with simple iterative data transformation. On both cases, we tuned the gradient descent learning rates as well as the scalar γ which governs the data transformation H(X). Center left &amp; center right columns: We measure the alignment between the GD as well as the GD ++ models and the trained TF. In both cases the TF aligns well with GD in the beginning of training but aligns much better with GD ++ after training. Far right column: TF performance (in log-scale) mimics the one of GD ++ well when testing on OOD tasks (α ̸ = 1).</p><p>loss close to 0, see Figure <ref type="figure" target="#fig_2">3</ref>. Nevertheless, we see that the naive correction necessary for model interpolation used in the aforementioned experiments is not enough to interpolate without a loss increase. We leave a search for better weight corrections to future work. We further study Transformers with different depths for recurrent as well as non-recurrent architectures with multiple heads and equipped with MLPs, and find qualitatively equivalent results, see Appendix Figure <ref type="figure" target="#fig_6">7</ref> and Figure <ref type="figure" target="#fig_7">8</ref>. Additionally, in Appendix A.9, we provide results obtained when using softmax SA layers as well as LayerNorm, thus essentially retrieving the standard Transformer architecture. We again observe and are able to explain (after slight architectural modifications) good learning performance and as well as alignment with the construction of Proposition 1, though worse than when using linear self-attention. These findings suggest that the incontext learning abilities of the standard Transformer with these common architecture choices can be explained by the gradient-based learning hypothesis explored here. Our findings also question the ubiquitous use of softmax attention, and suggest further investigation is warranted into the performance of linear vs. softmax SA layers in real-world learning tasks, as initiated by <ref type="bibr" target="#b43">Schlag et al. (2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transformers solve nonlinear regression tasks by gradient descent on deep data representations</head><p>It is unreasonable to assume that the astonishing in-context learning flexibility observed in large Transformers is ex-plained by gradient descent on linear models. We now show that this limitation can be resolved by incorporating one additional element of fully-fledged Transformers: preceding self-attention layers by MLPs enables learning linear models by gradient descent on deep representations which motivates our illustration in Figure <ref type="figure" target="#fig_0">1</ref>. Empirically, we demonstrate this by solving non-linear sine-wave regression tasks, see Figure <ref type="figure" target="#fig_4">4</ref>. Experimental details can be found in Appendix A.7. We state Proposition 2. Given a Transformer block i.e. a MLP m(e) which transforms the tokens e j = (x j , y j ) followed by an attention layer, we can construct weights that lead to gradient descent dynamics descending</p><formula xml:id="formula_10">1 2N N i=1 ||W m(x i ) -y i || 2 .</formula><p>Iteratively applying Transformer blocks therefore can solve kernelized least-squares regression problems with kernel function k(x, y) = m(x) ⊤ m(y) induced by the MLP m(•).</p><p>A detailed discussion on this form of kernel regression as well as kernel smoothing w/wo softmax nonlinearity through gradient descent on the data can be found in Appendix A.8. The way MLPs transform data in Transformers diverges from the standard meta-learning approach, where a task-shared input embedding network is optimized by backpropagation-through-training to improve the learning performance of a task-specific readout (e.g., <ref type="bibr" target="#b40">Raghu et al., 2020;</ref><ref type="bibr" target="#b32">Lee et al., 2019;</ref><ref type="bibr" target="#b7">Bertinetto et al., 2019)</ref>. On the other hand, given our token construction in Proposition 1, MLPs in Transformers intriguingly process both inputs and targets. The output of this transformation is then processed by a sin-gle linear self-attention layer, which, according to our theory, is capable of implementing gradient descent learning. We compare the performance of this Transformer model, where all weights are learned, to a control Transformer where the final LSA weights are set to the construction θ GD which is therefore identical to training an MLP by backpropagation through a GD updated output layer.</p><p>Intriguingly, both obtained functions show again surprising similarity on (1) the initial (meta-learned) prediction, read out after the MLP, and (2) the final prediction, after altering the output of the MLP through GD or the self-attention layer. This is again reflected in our alignment measures that now, since the obtained models are nonlinear w.r.t. x test , only represent the two first parts of the Taylor approximation of the obtained functions. Our results serve as a first demonstration of how MLPs and self-attention layers can interplay to support nonlinear in-context learning, allowing to fine-tune deep data representations by gradient descent. Investigating the interplay between MLPs and SA-layer in deep TFs is left for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Do self-attention layers build regression tasks?</head><p>The construction provided in Proposition 1 and the previous experimental section relied on a token structure where both input and output data are concatenated into a single token. This design is different from the way tokens are typically built in most of the related work dealing with simple few-shot learning problems as well as in e.g. language modeling. We therefore ask: Can we overcome the assumption required in Proposition 1 and allow a Transformer to build the required token construction on its own? This motivates Proposition 3. Given a 1-head linear or softmax attention layer and the token construction e 2j = (x j ), e 2j+1 = (0, y j ) with a zero vector 0 of dim N x -N y and concatenated positional encodings, one can construct key, query and value matrix W K , W Q , W V as well as the projection matrix P such that all tokens e j are transformed into tokens equivalent to the ones required in Proposition 1.</p><p>The construction and its discussion can be found in Appendix A.5. To provide evidence that copying is performed in trained Transformers, we optimize a two-layer self-attention circuit on in-context data where alternating tokens include input or output data i.e. e 2j = (x j ) and e 2j+1 = (0, y j ). We again measure the loss as well as the mean of the norm of the partial derivative of the first layer's output w.r.t. the input tokens during training, see Figure <ref type="figure" target="#fig_3">5</ref>. First, the training speeds are highly variant given different training seeds, also reported in <ref type="bibr" target="#b20">Garg et al. (2022)</ref>. Nevertheless, the Transformer is able to match the performance of a single (not two) step gradient descent. Interestingly, before the Transformer performance jumps to the one of GD, token e j transformed by the first self-attention layer becomes notably dependant on the neighboring token e j+1 while staying independent on the others which we denote as e other in Figure <ref type="figure" target="#fig_3">5</ref>. We interpret this as evidence for a copying mechanism of the Transformer's first layer to merge input and output data into single tokens as required by Proposition 1. Then, in the second layer the Transformer performs a single step of GD. Notably, we were not able to train the Transformer with linear self-attention layers, but had to incorporate the softmax operation in the first layer. These preliminary findings support the study of <ref type="bibr" target="#b37">Olsson et al. (2022)</ref> showing that softmax self-attention layers easily learn to copy; we confirm this claim, and further show that such copying allows the Transformer to proceed by emulating gradient-based learning in the second or deeper attention layers.</p><p>We conclude that copying through (softmax) attention layers is the second crucial mechanism for in-context learning in Transformers. This operation enables Transformers to merge data from different tokens and then to compute dot products of input and target data downstream, allowing for in-context learning by gradient descent to emerge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>Transformers show remarkable in-context learning behavior. Mechanisms based on attention, associative memory and copying by induction heads are currently the leading explanations for this remarkable feature of learning within the Transformer forward pass. In this paper, we put forward the hypothesis, similar to <ref type="bibr" target="#b20">Garg et al. (2022)</ref> and <ref type="bibr" target="#b0">Akyürek et al. (2023)</ref>, that Transformer's in-context learning is driven by gradient descent, in short -Transformers learn to learn by gradient descent based on their context. Viewed through the lens of meta-learning, learning Transformer weights corresponds to the outer-loop which then enables the forward pass to transform tokens by gradient-based optimization.</p><p>To provide evidence for this hypothesis, we build on Schlag et al. ( <ref type="formula">2021</ref>) that already provide a linear self-attention layer variant with (fast-)inner loop learning by the error-correcting delta rule <ref type="bibr" target="#b50">(Widrow &amp; Hoff, 1960)</ref>. We diverge from their setting and focus on (in-context) learning where we specifically construct a dataset by considering neighboring elements in the input sequence as input-and target training pairs, see assumptions of Proposition 1. This construction could be realized, for example, due to the model learning to implement a copying layer, see section 4 and proposition 3, and allows us to provide a simple and different construction to <ref type="bibr" target="#b43">Schlag et al. (2021)</ref> that solely is built on the standard linear, and approximately softmax, self-attention layer but still implements gradient descent based learning dynamics. We, therefore, are able to explain gradient descent based learning in these standard architectures. Furthermore, we extend this construction based on a single self-attention layer and provide an explanation of how deeper K-layer Transformer models implement principled K-step gradient descent learning, which deviates again from Schlag et al. and allows us to identify that deep Transformers implement GD++, an accelerated version of gradient descent.</p><p>We highlight that our construction of gradient descent and GD++ is not suggestive but when training multi-layer selfattention-only Transformers on simple regression tasks, we provide strong evidence that the construction is actually found. This allows us, at least in our restricted problems settings, to explain mechanistically in-context learning in trained Transformers and its close resemblance to GD observed by related work. Further work is needed to incorporate regression problems with noisy data and weight regularization into our hypothesis. We speculate aspects of learning in these settings are meta-learned -e.g., the weight magnitudes to be encoded in the self-attention weights. Additionally, we did not analyze logistic regression for which one possible weight construction is already presented in <ref type="bibr" target="#b54">Zhmoginov et al. (2022)</ref>.</p><p>Our refined understanding of in-context learning based on gradient descent motives us to investigate how to improve it. We are excited about several avenues of future research. First, to exceed upon a single step of gradient descent in every self-attention layer it could be advantageous to incorporate so called declarative nodes <ref type="bibr" target="#b1">(Amos &amp; Kolter, 2017;</ref><ref type="bibr" target="#b4">Bai et al., 2019;</ref><ref type="bibr" target="#b22">Gould et al., 2021;</ref><ref type="bibr" target="#b55">Zucchet &amp; Sacramento, 2022)</ref> into Transformer architectures. This way, we would treat a single self-attention layer as the solution of a fully optimized regression loss leading to possibly more efficient architectures. Second, our findings are restricted to small Transformers and simple regression problems. We are excited to delve deeper into research trying to understand how further mechanistic understanding of Transformers and incontext learning in larger models is possible and to what extend. Third, we are excited about targeted modifications to Transformer architectures, or their training protocols, leading to improved gradient descent based learning algorithms or allow for alternative in-context learners to be implemented within Transformer weights, augmenting their functionality, as e.g. in <ref type="bibr" target="#b14">Dai et al. (2023)</ref>. Finally, it would be interesting to analyze in-context learning in HyperTransformers <ref type="bibr" target="#b54">(Zhmoginov et al., 2022)</ref> that produce weights for target networks and already offer a different perspective on merging Transformers and meta-learning. There, Transformers transform weights instead of data and could potentially allow for gradient computations of weights deep inside the target network lifting the limitation of GD on linear models analyzed here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix</head><p>A.1. Proposition 1</p><p>First, we highlight the dependency on the tokens e i of the linear self-attention operation</p><formula xml:id="formula_11">e j ← e j + LSA θ ({e 1 , . . . , e N }) = e j + h P h V h K T h q h,j = e j + h P h i v h,i ⊗ k h,i q h,j = e j + h P h W h,V i e h,i ⊗ e h,i W T h,K W h,Q e j (6)</formula><p>with ⊗ the outer product between two vectors. With this we can now easily draw connections to one step of gradient descent on</p><formula xml:id="formula_12">L(W ) = 1 2N N i=1 ∥W x i -y i ∥ 2</formula><p>with learning rate η which yields weight change</p><formula xml:id="formula_13">∆W = -η∇ W L(W ) = - η N N i=1 (W x i -y i )x T i . (<label>7</label></formula><formula xml:id="formula_14">)</formula><p>We first restate Proposition 1. Given a 1-head linear attention layer and the tokens e j = (x j , y j ), for j = 1, . . . , N , one can construct key, query and value matrices W K , W Q , W V as well as the projection matrix P such that a Transformer step on every token e j is identical to the gradient-induced dynamics e j ← (x j , y j ) + (0, -∆W x j ) = (x i , y i ) + P V K T q j such that e j = (x j , y j -∆y j ). For the test data token (x N +1 , y N +1 ) the dynamics are identical.</p><p>We provide the weight matrices in block form: W K = W Q = I x 0 0 0 with I x and I y the identity matrices of size N x and N y respectively. Furthermore, we set W V = 0 0 W 0 -I y with the weight matrix W 0 ∈ R Ny×Nx of the linear model we wish to train and P = η N I with identity matrix of size N x + N y . With this simple construction we obtain the following dynamics</p><formula xml:id="formula_15">x j y j ← x j y j + η N I N i=1 0 0 W 0 -I y x i y i ⊗ I x 0 0 0 x i y i I x 0 0 0 x j y j = x j y j + η N I N i=1 0 W 0 x i -y i ⊗ x i 0 x j 0 = x j y j + 0 -∆W x j . (<label>8</label></formula><formula xml:id="formula_16">)</formula><p>for every token e j = (x j , y j ) including the query token e N +1 = e test = (x test , -W 0 x test ) which will give us the desired result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Comparing the out-of-distribution behavior of trained Transformers and GD</head><p>We provide more experimental results when comparing GD with tuned learning rate η and data transformation scalar γ and the trained Transformer on other data distributions than provided during training, see Figure <ref type="figure">6</ref>. We do so by changing the in-context data distribution and measure the loss of both methods averaged over 10.000 tasks when either changing α that 1) affects the input data range x ∼ U (-α, α) Nx or 2) the teacher by αW with W ∼ N (0, I). This setups leads to results shown in the main text, in the first two columns of Figure <ref type="figure">6</ref> and in the corresponding plots of Figure <ref type="figure" target="#fig_6">7</ref>. Although the match for deeper architectures starts to become worse, overall the trained Transformers behaves remarkably similar to GD and GD ++ for layer depth greater than 1.</p><p>Furthermore, we try GD and the trained Transformer on input distributions that it never has seen during training. Here, we chose by chance of 1/3 either a normal, exponential or Laplace distribution (with JAX default parameters) and depict the average loss value over 10.000 tasks where the α value now simply scales the input values that are sampled from one of the distributions αx. The teacher scaling is identical to the one described above. See for results the two right columns of Figure <ref type="figure">6</ref>, where we see almost identical behavior for recurrent architectures with less good match for deeper non-recurrent architectures far away from the training range of α = 1. Note that for deeper Transformers (K &gt; 2) the corresponding GD and GD ++ version, see for more experimental details Appendix section A.12, we include a harsh clipping of the token values after every step of transformation between [-10, 10] (for the trained TF and GD) to improve training stability. Therefore, the loss increase is restricted to a certain value and plateaus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Linear mode connectivity between the weight construction of Prop 1 and trained Transformers</head><p>In order to interpolate between the construction θ GD and the trained weights of the Transformer θ, we need to correct for some scaling ambiguity. For clarification, we restate here the linear self-attention operation for a single head</p><formula xml:id="formula_17">e j ←e j + P W V i e i ⊗ e i W T K W Q e j (9) = e j + W P V i e i ⊗ e i W KQ e j<label>(10)</label></formula><p>Now, to match the weight construction of Prop. 1 we have the aim for the matrix product W KQ to match an identify matrix (except for the last diagonal entry) after re-scaling. Therefore we compute the mean of the diagonal of the matrix product of the trained Transformer weights W KQ which we denote by β. After resealing both operations i.e. W KQ ← W KQ /β and W P V ← W P V β we interpolate linearly between the matrix products of GD as well as these rescaled trained matrix products i.e. W I,KQ = (W GD,KQ + W T F,KQ )/2 as well as W I,P V = (W GD,P V + W T F,P V )/2. We use these parameters to obtain results throughout the paper denote with Interpolated. We do so for GD as well as GD ++ when comparing to  Results comparable to the deep recurrent Transformer, see Figure <ref type="figure" target="#fig_6">7</ref>, but now with 12 independent Transformer blocks including MLPs and 4-head linear self-attention. We omit LayerNorm. We again observe a close resemblance of the trained Transformers and GD ++ . We hypotheses that even when equipped with multiple heads and MLPs, Transformers approximate GD ++ .</p><p>To get a simple and clean construction, we choose wlog x j ∈ R 2N +1 and (0, y j ) ∈ R 2N +1 as well as model the positional encodings as unit vectors p j ∈ R 2N +1 and concatenate them to the tokens i.e. e j = (x j/2 , p j ). We wish for a construction that realizes</p><formula xml:id="formula_18">e j ← x j/2 p j + P V K T W Q x j/2 p j (11) = x j/2 p j + 0 y j/2+1 -p j . (<label>12</label></formula><formula xml:id="formula_19">)</formula><p>This means that a token replaces its own positional encoding by coping the target data of the next token to itself leading to e j = (x j/2 , 0, y j/2+1 ), with slight abusive of notation. This can simply be realized by (for example) setting P = I,</p><formula xml:id="formula_20">W V = 0 0 I x -I x,of f , W K = 0 0 0 I x and W Q = 0 0 0 I T x,of f</formula><p>with I x,of f the lower diagonal identity matrix fo size N x . Note that then simply K T W Q e j = p j+1 i.e. it chooses the j + 1 element of V which stays p j+1 if we apply the softmax operation on K T q j . Since the j + 1 entry of V is (0, y j/2+1 -p j ) we obtain the desired result.</p><p>For the (toy-)regression problems considered in this manuscript, the provided result would give N/2 tokens for which we also copy (parts) of x j underneath y j . This is desired for modalities such as language where every two tokens could be considered an in-and output pair for the implicit autoregressive inner-loop loss. These tokens do not have be necessarily next to each other, see for this behavior experimental findings presented in <ref type="bibr" target="#b37">(Olsson et al., 2022)</ref>. For the experiments conducted here, one solution is to zero out these tokens which could be constructed by a two-head self-attention layer that given uneven j simply subtracts itself resulting in a zero token. For all even tokens, we use the construction from above which effectively coincides with the token construction required in Proposition 1. We observe that different dampening strengths affect the generalization of both methods with slightly better robustness for GD which matching performance for 50 steps when λ = 0.75.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6. Dampening the self-attention layer</head><p>As an additional out-of-distribution experiment, we test the behavior when repeating a single LSA-layer trained to lower our objective, see equation 5, with the aim to repeat the learned learning/update rule. Note that GD as well as the selfattention layer were optimized to be optimal for one step. For GD we line search the otpimal learning rate η on 10.000 task. Interestingly, for both methods we observe quick divergence when applied multiple times, see left plot of Figure <ref type="figure" target="#fig_8">10</ref>. Nevertheless, both of our update functions are described by a linear self-attention layer for which we can control the norm, post training, by a simple scale which we denote as λ. This results in the new update y test + λ∆W x test for GD and y test + λP V K T W Q x test for the trained self-attention layer which effectively re-tunes the learning rate for GD and the trained self-attention layer. Intriguingly, both methods do generalize similarly well (or poorly) on this out-of-distribution experiment when changing λ, see again Figure <ref type="figure" target="#fig_8">10</ref>. We show in Figure <ref type="figure" target="#fig_0">1</ref> the behavior for λ = 0.75 for which we see both methods steadily decreasing the loss within 50 steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7. Sine wave regression</head><p>For the sine wave regression tasks, we follow <ref type="bibr" target="#b18">(Finn et al., 2017)</ref> and other meta-learning literature and sample for each task an amplitude a ∼ U (0.1, 5) and a phase ρ ∼ U (0, π). Each tasks consist of N = 10 data points where inputs are sampled x ∼ U (-5, 5) and targets computed by y = a sin(ρ + x). We choose here for the first time, for GD as well as for the Transformer, an input embedding emb that maps tokens e i = (x i , y i ) into a 40 dimensional space emb(e i ) = W emb e i through an affine projection without bias. We skip the first self-attention layer but, as usually done in Transformers, then transform the embedded tokens through an MLP m with a single hidden layer, widening factor of 4 (160 hidden neuros) and GELU nonlinearity <ref type="bibr" target="#b24">(Hendrycks &amp; Gimpel, 2016)</ref> i.e. e j ← m(emb(e j )) + emb(e j ).</p><p>We interpret the last entry of the transformed tokens as the (transformed) targets and the rest as a higher-dimensional input data representation on which we train a model with a single gradient descent step. We compare the obtained meta-learned GD solution with training a Transformer on the same token embeddings but instead learn a self-attention layer. Note that the embeddings of the tokens, including the transformation through the MLP, are not dependent on an interplay between the tokens. Furthermore, the initial transformation is dependent on e i = (x i , y i ), i.e., input as well as on the target data except for the query token for which y test = 0. This means that this construction is, except for the additional dependency on targets, close to a large corpus of meta-learning literature that aims to find a deep representation optimized for (fast) fine tuning and few-shot learning. In order to compare the meta-training of the MLP and the Transformer, we choose the same seed to initialize the network weights for the MLPs and the input embedding trained by meta-learning i.e. backprop through training or the Transformer. This leads to the plots and almost identical learned initial function and updated functions shown in Figure <ref type="figure" target="#fig_4">4</ref>.</p><p>A.8. Proposition 2 and connections between gradient descent, kernelized regression and kernel smoothing</p><p>Let's consider the data transformation induced by an MLP m(x) and a residual connection commonly used in Transformer blocks i.e. e j ← e j + m(e j ) = (x j , y j ) + ( m(x j ), 0) = (m(x j ), y j ) with m(x j ) = x j + m(x j ) and m not changing the targets y. When simply applying Proposition 1, it is easy to see that given this new token construction, a linear self-attention layer can induce the token dynamics e j ← (m(x j ), y j ) + (0, -∆W m(x j )) with ∆W = -η∇L(W ) given the loss function</p><formula xml:id="formula_21">L(W ) = 1 2N N i=1 ||W m(x i ) -y i || 2 .</formula><p>Interestingly, for the test token e test = (x test , 0) this induces, after a multiplication with -1, an initial prediction after a single Transformer block given by</p><formula xml:id="formula_22">ŷ = ∆W m(x test ) = -η∇ W L(0)m(x test ) = N i=1 y i m(x i ) T m(x test ) = N i=1 y i k(x i , x test )<label>(13)</label></formula><p>with m(x i ) T m(x test ) = k(x i , x test ) ∈ R interpreted as a kernel function. Concluding, we see that the combination of MLPs and a single self-attention layer can lead to dynamics induced when descending a kernelized regression (squared error) loss with a single step of gradient-descent.</p><p>Interestingly, when choosing W 0 = 0, we furthermore see that a single self-attention layer or Transformer block can be regarded as doing nonparametric kernel smoothing ŷ = N i=1 y i k(x i , x test ) based on the data given in-context <ref type="bibr" target="#b36">(Nadaraya, 1964;</ref><ref type="bibr" target="#b49">Watson, 1964)</ref>. Note that we made a particular choice of kernel function here and that this view still holds when m(x j ) = 1 i.e. consider Transformers without MLPs or leverage the well-known view of softmax self-attention layer as a kernel function used to measure similarity between tokens (e.g. <ref type="bibr" target="#b13">Choromanski et al., 2021;</ref><ref type="bibr" target="#b52">Zhang et al., 2021)</ref>. Thus, implementing one step of gradient descent through a self-attention layer (w/wo softmax nonlinearity) is equivalent to performing kernel smoothing estimation. We however argue that this nonparametric kernel smoothing view of in-context learning is limited, and arises from looking only at a single self-attention layer. When considering deeper Transformer architectures, we see that multiple Transformer blocks can iteratively transform the targets based on multiple steps of gradient descent leading to minimization of a kernelized squared error loss L(W ). One way to obtain a suitable construction is by neglecting MLPs everywhere except in the first Transformer block. We leave the study of the exact mechanics, especially how the Transformer makes use of possibility transforming the targets through the MLPs, and the possibility of iteratively changing the kernel function throughout depth for future study.</p><p>A.9. Linear vs. softmax self-attention as well LayerNorm Transformers</p><p>Although linear Transformers and their variants have been shown to be competitive with their softmax counterpart <ref type="bibr" target="#b28">(Irie et al., 2021)</ref>, the removal of this nonlinearity is still a major departure from classic Transformers and more importantly from the Transformers used in related studies analyzing in-context learning. In this section we investigate whether and when gradient-based learning emerges in trained softmax self-attention layers, and we provide an analytical argument to back our findings.</p><p>First, we show, see Figure <ref type="figure" target="#fig_1">12</ref>, that a single layer of softmax self-attention is not able to match GD performance. We tuned the learning rate as well as the weight initialization but found no significant difference over the hyperparameters we used througout this study. In general, we hypothesize that GD is an optimal update given the limited capacity of a single layer of (single-head) self-attention. We therefore argue that the softmax induces (at best) a linear offset of the matrix product of training data and query vector softmax</p><formula xml:id="formula_23">(K T q j ) = (e k T 1 qj , . . . , e k T N qj ) T /( i e k T i qj ) (14) = (e x T 1 W KQ xj , . . . , e x T N W KQ xj ) T /( i e x T i W KQ xj )<label>(15)</label></formula><formula xml:id="formula_24">≈ (1 + x T 1 W KQ x j , . . . , 1 + x T N W KQ x j ) T /( i 1 + x T i W KQ x j ) (16) ∝ K T q j + ϵ<label>(17)</label></formula><p>proportional to a factor dependent on all {x τ,i } N +1 i=1 . We speculate that the dependency on the specific task τ , for large N x vanishes or that the x-dependent value matrix could introduce a correcting effect. In this case the softmax operation introduces an additive error w.r.t. to the optimal GD update. To overcome this disadvantageous offset, the Transformer can (approximately) introduce a correction with a second self-attention head by a simple subtraction i.e.</p><formula xml:id="formula_25">P 1 V 1 softmax(K T 1 W Q x j ) + P 2 V 2 softmax(K T 2 W Q x j )<label>(18)</label></formula><p>≈ P V ((1 + x T 1 W 1,KQ x j , . . . , 1 + x T N W 1,KQ x j ) -(1 + x T 1 W 2,KQ x j , . . . , 1 + x T N W 2,KQ x j ))</p><p>= P V (x T 1 (W 1,KQ -W 2,KQ )x j , . . . , x T N (W 1,KQ -W 2,KQ )x j ) (20)</p><formula xml:id="formula_27">∝ P V K T q j .<label>(21)</label></formula><p>(a) Comparing one step of GD with a trained softmax one-headed self-attention layer. Comparing trained two-headed and one-headed single-layer softmax self-attention with 1 step of gradient descent on linear regression tasks. Left column: Softmax self-attention is not able to match gradient descent performance with hand-tuned learning rate, but adding a second attention head significantly reduces the gap, as expected by our analytical argument. Center left: The alignment suffers significantly for single-head softmax SA. We observe good but not as precise alignment when compared to linear Transformers for the two-headed softmax SA layer. Center right &amp; right: The two-headed self-attention compared to the single-head layer shows similar robust out-of-distribution behavior compared to gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.10. Details of curvature correction</head><p>We give here a precise construction showing how to implement in a single head, a step of GD and the discussed data transformation, resulting in GD ++ . Recall again the linear self-attention operation with a single head e j ←e j + P W V i e i ⊗ e i W T K .</p><p>(</p><formula xml:id="formula_28">)<label>22</label></formula><p>We provide again the weight matrices in block form of the construction of Prop. 1 but now enabling additionally our described data transformation: W K = W Q = I x 0 0 0 with I x the identity matrix of size N x , I y od size N y resp. x i W x i -y i ⊗ x i 0</p><formula xml:id="formula_29">x j 0 = x j y j + -γXX T x j -∆W x j . (<label>23</label></formula><formula xml:id="formula_30">)</formula><p>for every token e j = (x j , y j ) including the query token e N +1 = e test = (x test , 0) which will give us the desired result.</p><p>Why does GD ++ perform better? We give here one possible explanation of the superior performance of GD ++ compared to GD. Note that there is a close resemblance of the GD transformation and a heavily truncated Neuman series approximation of the inverse XX T . We provide here a more heuristic explanation for the observed acceleration.</p><p>Given γ ∈ R, GD ++ transforms every input according to x i ← x i -γXX T x i = (I -γXX T )x i . We can therefore look at the change of squared regression loss L(W ) = 1 instable when we approach GD with an optimal learning rate. In order to stabilize training, we simply clipped the token values to be in the range of <ref type="bibr">[-10, 10]</ref>.</p><p>• When applicable we use standard positional encodings of size 20 which we concatenated to all tokens.</p><p>• For simplicity, and to follow the provided weight construction closely, we did use square key, value and query parameter matrix in all experiments.</p><p>• The training length varied throughout our experimental setups and can be read off our training plots in the article.</p><p>• When training meta-parameters for gradient descent i.e. η and γ we used an identical training setup but usually training required much less iterations.</p><p>• In all experiments we choose inital W 0 = 0 for gradient descent trained models.</p><p>Inspired by <ref type="bibr" target="#b20">(Garg et al., 2022)</ref>, we additionally provide results when training a single linear self-attention layer on a fixed number of training tasks. Therefore, we iterate over a single fixed batch of size B instead of drawing new batch of tasks at every iteration. Results can be found in Figure <ref type="figure" target="#fig_0">16</ref>. Intriguingly, we find that (meta-)gradient descent finds Transformer weights that align remarkable well with the provided construction and therefore gradient descent even when provided with an arguably very small number of training tasks. We argue that this again highlights the strong inductive bias of the LSA-layer to match (approximately) gradient descent learning in its forward pass.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Illustration of our hypothesis: gradient-based optimization and attention-based in-context learning are equivalent. Left: Learning a neural network output layer by gradient descent on a dataset D train . The task-shared meta-parameters θ are obtained by meta-learning with the goal that after adjusting the neural network output layer, the model generalizes well on unseen data. Center: Illustration of a Transformer that adjusts its query prediction on the data given in-context i.e. t θ (xquery; D context ). The weights of the Transformer are optimized to predict the next token yquery. Right: Our results confirm the hypothesis that learning with K steps of gradient descent on a dataset D train (green part of the left plot) matches trained Transformers with K linear self-attention layers (central plot) when given D train as in-context data D context .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Comparing one step of GD with a trained single linear self-attention layer. Outer left: Trained single LSA layer performance is identical to the one of gradient descent. Center left: Almost perfect alignment of GD and the model generated by the SA layer after training, measured by cosine similarity and the L2 distance between models as well as their predictions. Center right: Identical loss of GD, the LSA layer model as well as the model obtained by interpolating between the construction and the optimized LSA layer weights for different N = Nx. Outer right: The trained LSA layer, gradient descent and their interpolation show identically loss (in log-scale) when provided input data different than during training i.e. with scale of 1. We display the mean/std. or the single runs of 5 seeds.</figDesc><graphic coords="5,309.35,60.54,111.77,96.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure3. Far left column: The trained TF performance surpasses standard GD but matches GD ++ , our GD variant with simple iterative data transformation. On both cases, we tuned the gradient descent learning rates as well as the scalar γ which governs the data transformation H(X). Center left &amp; center right columns: We measure the alignment between the GD as well as the GD ++ models and the trained TF. In both cases the TF aligns well with GD in the beginning of training but aligns much better with GD ++ after training. Far right column: TF performance (in log-scale) mimics the one of GD ++ well when testing on OOD tasks (α ̸ = 1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure5. Training a two layer SA-only Transformer using the standard token construction. Left: The loss of trained TFs matches one step of GD, not two, and takes an order of magnitude longer to train. Right: Norm of the partial derivatives of the output of the first self-attention layer w.r.t. input tokens. Before the Transformer performance jumps to the one of GD, the first layer becomes highly sensitive to the next token.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure4. Sine wave regression: comparing trained Transformers with meta-learned MLPs for which we adjust the output layer with one step of gradient descent. Left: Plots of the learned initial functions as well as the adjusted functions through either a layer of self-attention or a step of GD. We observe similar initial functions as well as solutions for the trained TF compared fine-tuning a meta-learned MLP. Center: The performance of the trained Transformer is matched by meta-learned MLPs. Left: We observe strong alignment when comparing the prediction as well as the partial derivatives of the the meta-learned MLP and the trained Transformer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>( a )Figure 6 .</head><label>a6</label><figDesc>Figure 6. Left &amp; center left column: Comparing Transformers, GD and their weight interpolation on rescaled training distributions. In all setups, the trained Transformer behaves remarkably similar to GD or GD ++ . Right &amp; center right: Comparing Transformers, GD and their weight interpolation on data distributions never seen during training. Again, in all setups, the trained Transformer behaves remarkably similar to GD or GD ++ with less good match for deep non-recurrent Transformers far away from training regimes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Comparing ten steps of gradient descent with trained recurrent ten-layer Transformers. Results comparable to recurrent Transformer with two layers, see Figure 3, but now with 10 repeated layers. We again observe for deeper recurrent linear self-attention only Transformers that overall GD ++ and the trained Transformer align very well with one another and are again interpolatable leading to very similar behavior insight as well as outside training situations. Note the inferior performance to the non-recurrent five-layer Transformer which highlights the importance on specific learning rate as well γ parameter per layer/step.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Comparing twelve steps of GD ++ with a trained twelve-layer Transformers with MLPs and 4 headed linear self-attention layer. Results comparable to the deep recurrent Transformer, see Figure7, but now with 12 independent Transformer blocks including MLPs and 4-head linear self-attention. We omit LayerNorm. We again observe a close resemblance of the trained Transformers and GD ++ . We hypotheses that even when equipped with multiple heads and MLPs, Transformers approximate GD ++ .</figDesc><graphic coords="15,225.54,489.47,145.80,130.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 .</head><label>10</label><figDesc>Figure10. Roll-out experiments: applying a trained single linear self-attention layer multiple times. We observe that different dampening strengths affect the generalization of both methods with slightly better robustness for GD which matching performance for 50 steps when λ = 0.75.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><figDesc>Figure12. Comparing trained two-headed and one-headed single-layer softmax self-attention with 1 step of gradient descent on linear regression tasks. Left column: Softmax self-attention is not able to match gradient descent performance with hand-tuned learning rate, but adding a second attention head significantly reduces the gap, as expected by our analytical argument. Center left: The alignment suffers significantly for single-head softmax SA. We observe good but not as precise alignment when compared to linear Transformers for the two-headed softmax SA layer. Center right &amp; right: The two-headed self-attention compared to the single-head layer shows similar robust out-of-distribution behavior compared to gradient descent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Furthermore</head><figDesc>, we set W V = I x 0 W -I y with the weight matrix W ∈ R Ny×Nx of the linear model we wish to train and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 15 .</head><label>15</label><figDesc>Figure 15. Phase transitions during training. Left: Loss based on 10 different random seeds when optimizing a single-headed selfattention layer. We observe for some seeds very long initial phases of virtually zero progress after which the loss drops suddenly to the desired GD loss. Center: The same experiment but optimizing a two-headed self-attention layer. We observe fast and robust convergence to the loss of GD. Right: Training a single Transformer block i.e. a self-attention layer with MLP and a reduced training set size of 8192 tasks. We observe grokking like train and test loss phase transitions where test set first increases drastically before experiencing a sudden drop in loss almost matching the desired GD loss of 0.2.</figDesc><graphic coords="23,75.30,67.06,139.97,122.48" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Department of Computer Science, ETH Zürich, Zürich, Switzerland</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Google Research. Correspondence to: Johannes von Oswald &lt;voswaldj@ethz.ch&gt;.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_2"><p>Main experiments can be reproduced with notebooks provided under the following link: https://github.com/ google-research/self-organising-systems/ tree/master/transformers_learn_icl_by_gd</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_3"><p>Ni=0 (W x i -y i ) 2 induced by this transformation i.e. L ++ (W ) =</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p><rs type="person">João Sacramento</rs> and <rs type="person">Johannes von Oswald</rs> deeply thank <rs type="person">Angelika Steger</rs> for her support and guidance. The authors also thank <rs type="person">Seijin Kobayashi</rs>, <rs type="person">Marc Kaufmann</rs>, <rs type="person">Nicolas Zucchet</rs>, <rs type="person">Yassir Akram</rs>, <rs type="person">Guillaume Obozinski</rs> and <rs type="person">Mark Sandler</rs> for many valuable insights throughout the project and <rs type="person">Dale Schuurmans</rs> and <rs type="person">Timothy Nguyen</rs> for their valuable comments on the manuscript. <rs type="person">João Sacramento</rs> was supported by an <rs type="grantName">Ambizione grant</rs> (<rs type="grantNumber">PZ00P3 186027</rs>) from the <rs type="funder">Swiss National Science Foundation</rs> and an <rs type="grantName">ETH Research Grant</rs> (<rs type="grantNumber">ETH-23 21-1</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_EGxB53Q">
					<idno type="grant-number">PZ00P3 186027</idno>
					<orgName type="grant-name">Ambizione grant</orgName>
				</org>
				<org type="funding" xml:id="_XkSXpKd">
					<idno type="grant-number">ETH-23 21-1</idno>
					<orgName type="grant-name">ETH Research Grant</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="table">1 2 3 4 5 6 7 8 9</ref> <p>recurrent Transformers. Note that for non-recurrent Transformers, we face more ambiguity that we have to correct for since e.g. scalings influence each other across layer. We also see this in practice and are not able (only for some seeds) to interpolate between weights with our simple correction from above. We leave the search for more elaborate corrections for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Visualizing the trained Transformer weights</head><p>The simplicity of our construction enables us to visually compare trained Transformers and the construction put forward in Proposition A.1 in weight space. As discussed in the previous section A.3 there is redundancy in the way the trained Transformer can construct the matrix products leading to the weights corresponding to gradient descent. We therefore visualize W KQ = W T K W Q as well as W P V = P K W V in Figure <ref type="figure">9</ref>.</p><p>A.5. Proof and discussion of Proposition 3</p><p>We state here again Proposition 3, provide the necessary construction and a short discussion. Proposition 3. Given a 1-head linear-or softmax attention and the token construction e 2j = (x j ), e 2j+1 = (0, y j ) with a zero vector 0 of dim N x -N y and concatenated positional encodings, one can construct key, query and value matrix W K , W Q , W V as well as the projection matrix P such that all tokens e j are transformed into tokens equivalent to the ones required in proposition 1. K WQ including its scaling by η induced through P WV of the two heads of the trained softmax self-attention layer. We observe that both of the matrices are approximate diagonal almost perfect sign reversed values on the off-diagonal terms. After adding the matrices (right plot), we observe a diagonal matrix and therefore to much improved approximation of our construction and therefore gradient descent dynamics.</p><p>Here we assume that P V 1) subsumes the dividing factor of the softmax and that 2) is the same (up to scaling) for each head. Note that if (W 1,KQ -W 2,KQ ) is diagonal, and P and V chosen as in the Proposition of Appendix A.1, we recover our gradient descent construction.</p><p>We base this derivation on empirical findings, see Figure <ref type="figure">12</ref>, that, first of all, show the softmax self-attention performance increases drastically when using two heads instead of one. Nevertheless, the self-attention layer has difficulties to match the loss values of a model trained with GD. Furthermore, this architecture change leads to a very much improved alignment of the trained model and GD. Second, we can observe that when training a two-headed softmax self-attention layer on regression tasks the correction proposed above is actually observed in weight space, see Figure <ref type="figure">11</ref>. Here, we visualize the matrix product within the softmax operation W h,KQ per head which we scale with the last diagonal entry of P h W h,V which we denote by η h = P h W h,V (-1, -1). Intriguingly, this results in an almost perfect cancellation (right plot) of the off-diagonal terms and therefore in sum to an improved approximation of our construction, see the derivation above.</p><p>We would like to reiterate that the stronger inductive bias for copying data of the softmax layer remains, and is not invalidated by the analysis above. Therefore, even for our shallow and simple constructions they indeed fulfill an important role in support for our hypotheses: The ability to merge or copy input and target data into single tokens allowing for their dot product computation necessary for the construction in Proposition 1, see Section 4 in the main text.</p><p>We end this section by analysing Transformers equipped with LayerNorm which we apply as usually done before the self-attention layer: Overall, we observe qualitatively similar results to Transformers with softmax self-attention layer i.e. a decrease in performance compared to GD accompanied with a decrease in alignment between models generated by the Transformer and models trained with GD, see Figure <ref type="figure">14</ref>. Here, we test again a single linear self-attention layer succeeding LayerNorm as well as two layers where we skip the first LayerNorm and only include a LayerNorm between the two. Including more heads does not help substantially. We again assume the optimality of GD and argue that information of targets and inputs present in the tokens is lost by averaging when applying LayerNorm. This naturally leads to decreasing performance compared to GD, see first row of Figure <ref type="figure">14</ref>. Although the alignment to GD and GD ++ , especially for two layers, is high, we overall see inferior performance to one or two steps of GD or two steps of GD ++ . Nevertheless, we speculate that LayerNorm might not only stabilize Transformer training but could also act as some form of data normalization procedure that implicitly enables better generalization for larger inputs as well as targets provided in-context, see OOD experiments in Figure <ref type="figure">14</ref>.</p><p>Overall we conclude that common architecture choices like softmax and LayerNorm seem supoptimal for the constructed in-context learning settings when comparing to GD or linear self-attention. Nevertheless, we speculate that the potentially small performance drops of in-context learning are negligible when turning to deep and wide Transformers for which these architecture choices have empirically proven to be superior. </p><p>which in turn leads to a change of the loss Hessian from</p><p>Given the original Hessian H = XX T = U ΣU T with it's set of sorted eigenvalues {λ 1 , . . . , λ n } and λ i ≥ 0 on the diagonal matrix Σ we can express the new Hessian through U, Σ i.e.</p><p>We can simplify H ++ further as</p><p>Given the eigenspectrum {λ 1 , . . . , λ n } of H, we obtain an (unsorted) eigenspecturm for H ++ with {λ 1 -2γλ 2 1 + γ 2 λ 3 1 , . . . , λ n -2γλ 2 n + γ 2 λ 3 n } which we visualize in Figure <ref type="figure">13</ref> for different γ observed in practice. We hypotheses that the Transformer chooses γ in a way that on average, across the distribution of tasks, the data transformation (iteratively) decreases the condition number λ 1 /λ n leading to accelerated learning. This could be achieved, for example, by keeping the smallest eigenvalue λ n ≈ λ ++ n fixed and choosing γ such that the largest eigenvalue of the transformed data λ ++ 1 is reduced, while the original λ 1 stays within [λ ++ 1 , λ ++ n ]. To support our hypotheses empirically, we computed the minimum and maximum eigenvalues of XX T across 10000 tasks while changing the number of datapoints N ∈ [10, 25, 50, 100] i.e. X = (x 0 , . . . , x N ) leading to better conditioned loss Hessians i.e. [1e-10, 0.097, 0.666, 2.870] and <ref type="bibr">[4.6, 7.712, 10.845, 17.196]</ref> as the minimum and maximum eigenvalues of XX T across all tasks where we cut the smallest eigenvalue for N = 10 at 1e-10. Furthermore, we extract the γ values from the weights of optimized recurrent 2-layer Transformers trained on different task distributions and obtain γ values of [0.179, 0.099, 0.056, 0.029], see again Figure <ref type="figure">13</ref>. Note that the observed eigenvalues stay within [0, 1/γ] i.e. the two roots of f (λ, γ) = λ -2γλ 2 + γ 2 λ 3 .</p><p>Given the derived function of eigenvalue change f (λ, γ), we compute the condition number of H ++ by dividing the novel maximum eigenvalues λ ++ 1 = f (1/(3γ), γ) where λ = 1/(3γ) as the local maximum of f (λ, γ), for fixed γ, and the novel minimum eigenvalue λ ++ n = min(f (λ 1 , γ), f (λ n , γ)). Note that with too small γ, we move the original λ n closer to the root of f (λ, γ) i.e. λ = 1/γ and therefore can change the smallest eigenvalue.</p><p>Given the task distribution and its corresponding eigenvalue distribution, we see that choosing γ reduces the new condition number κ ++ = λ ++ 1 /λ ++ n which leads to better conditioned learning, see center plot of Figure <ref type="figure">13</ref>. Note that the optimal γ based on our derivation above is based on the maximum and minimum eigenvalue across all tasks and does not take the change of the eigenvalue distribution into account. We argue therefore that the simplicity of the arguments above does not capture the task statistics and distribution shifts entirely and therefore obtains a slightly larger γ as an optimal value. Left column: The Transformers is not able to match the gradient descent performance with hand-tuned learning rate. Alignment plots: The alignment suffers significantly when comparing to linear self-attention layers although still reasonable alignment is obtained which decreases slightly when comparing to GD ++ for the two-layer Transformer.Center right &amp; right: The LayerNorm Transformer outperforms when GD when providing training input data that is significantly larger than the data provided during training.</p><p>We furthermore visualize the condition number change for N = 25 and 10000 tasks in the right plot of Figure <ref type="figure">13</ref> and observe the distribution moving to desirable κ values close to 1. For γ values larger than 0.1 the distribution quickly exhibits exploding condition numbers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.11. Phase transitions</head><p>We comment shorty on the curiously looking phase transitions of the training loss observed in many of our experiments, see Figure <ref type="figure">2</ref>. Nevertheless, simply switching from a single-headed self-attention layer to a two-headed self-attention layer mitigates the random seed dependent training instabilities in our experiments presented in the main text, see left and center plot of Figure <ref type="figure">15</ref>.</p><p>Furthermore, these transitions look reminiscent of the recently observed "grokking" behaviour <ref type="bibr" target="#b39">(Power et al., 2022)</ref>. Interestingly, when carefully tuning the learning rate and batchsize we can also make the Transformers trained in these linear regression tasks grokk. For this, we train a single Transformer block (self-attention layer and MLP) on a limited amount of data (8192 tasks), see right plot of Figure <ref type="figure">15</ref>, and observe grokking like train and test loss phase transitions where test set first increases drastically before experiencing a sudden drop in loss almost matching the desired GD loss of 0.2. We leave a thorough investigation of these phenomena for future study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.12. Experimental details</head><p>We use for most experiments identical hyperparameters that were tuned by hand which we list here</p><p>• Optimizer: Adam <ref type="bibr" target="#b29">(Kingma &amp; Ba, 2014)</ref> with default parameters and learning rate of 0.001 for Transformer with depth K &lt; 3 and 0.0005 otherwise. We use a batchsize of 2048 and applied gradient clipping to obtain gradients with global norm of 10. We used the Optax library.</p><p>• Haiku weight initialisation (fan-in) with truncated normal and std 0.002/K where K the number of layers.</p><p>• We did not use any regularisation and observed for deeper Transformers with K &gt; 2 instabilities when reaching GD performance. We speculate that this occurs since the GD performance is, for the given training tasks, already close to divergence as seen when providing tasks with larger input ranges. Therefore, training Transformers also becomes </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">What learning algorithm is in-context learning? investigations with linear models</title>
		<author>
			<persName><forename type="first">E</forename><surname>Akyürek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum" />
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>id=0g0X4H8yN4I</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Optnet: Differentiable optimization as a layer in neural networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Amos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning to learn by gradient descent by gradient descent</title>
		<author>
			<persName><forename type="first">M</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shillingford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Using fast weights to attend to the recent past</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep equilibrium models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning a synaptic learning rule</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cloutier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Département d&apos;Informatique et de Recherche opérationnelle</title>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
		<respStmt>
			<orgName>Université de Montréal</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Random initialisations performing above chance and how to find them</title>
		<author>
			<persName><forename type="first">F</forename><surname>Benzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schug</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Oswald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Akram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zucchet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Aitchison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OPT2022: 14th Annual Workshop on Optimization for Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable closed-form solvers</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The evolution of learning: an experiment in genetic connectionism</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Chalmers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Connectionist Models</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Touretzky</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</editor>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="81" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C Y</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Lampinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.05675</idno>
		<title level="m">Transformers generalize differently from information stored in context vs in weights</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Data distributional properties drive emergent in-context learning in transformers</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C Y</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Lampinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rethinking attention with performers</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sarlos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Q</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Colwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Weller</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Ua6zuk0WRH" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Why can GPT learn in-context? language models implicitly perform gradient descent as meta-optimizers</title>
		<author>
			<persName><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=fzbHRjAd8U" />
	</analytic>
	<monogr>
		<title level="m">ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=YicbFdNTTy" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The role of permutation invariance in linear mode connectivity of neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Entezari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sedghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Saukh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Neyshabur</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.06296</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Meta-learning and universality: Deep representations and gradient descent can approximate any learning algorithm</title>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HyjC5yWCW" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Model-agnostic metalearning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Meta-learning with warped gradient descent</title>
		<author>
			<persName><forename type="first">S</forename><surname>Flennerhag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Visin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">What can transformers learn in-context? a case study of simple function classes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Valiant</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=flNZJ2eOet" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Oh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Belgrave</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Meta-learning probabilistic inference for prediction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bronskill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Turner</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HkxStoC5F7" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep declarative networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Campbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><surname>Conformer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.08100</idno>
		<title level="m">Convolution-augmented transformer for speech recognition</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Gaussian error linear units (gelus)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Using fast weights to deblur old memories</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Plaut</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning to learn using gradient descent</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Younger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Conwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Neural Networks -ICANN 2001</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Dorffner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Hornik</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="87" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Risks from learned optimization in advanced machine learning systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hubinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Van Merwijk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mikulik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Skalse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Garrabrant</surname></persName>
		</author>
		<idno>arXiv [cs.AI]</idno>
		<ptr target="http://arxiv.org/abs/1906.01820" />
		<imprint>
			<date type="published" when="2019-06">Jun 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Going beyond linear transformers with recurrent fast weight programmers</title>
		<author>
			<persName><forename type="first">K</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Schlag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Csordás</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno>CoRR, abs/2106.06295</idno>
		<ptr target="https://arxiv.org/abs/2106.06295" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
		<title level="m">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Meta learning backpropagation and improving it</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kirsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=hhU9TEvB6AF" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">General-purpose in-context learning by meta-learning transformers</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kirsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=t6tA-KB4dO" />
	</analytic>
	<monogr>
		<title level="m">Sixth Workshop on Meta-Learning at the Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Metalearning with differentiable convex optimization</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Gradient-based meta-learning with learned layerwise metric and subspace</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Meta</surname></persName>
		</author>
		<author>
			<persName><surname>Sgd</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09835</idno>
		<title level="m">Learning to learn quickly for few shot learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.13586</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Nadaraya</surname></persName>
		</author>
		<title level="m">On estimating regression. Theory of Probability &amp; its Applications</title>
		<imprint>
			<date type="published" when="1964">1964</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="141" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Elhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dassarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Conerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Drain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hatfield-Dodds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kernion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lovitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ndousse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.11895</idno>
		<title level="m">context learning and induction heads</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Meta-curvature</title>
		<author>
			<persName><forename type="first">E</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Power</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><surname>Grokking</surname></persName>
		</author>
		<idno>datasets. abs/2201.02177</idno>
		<title level="m">Generalization beyond overfitting on small algorithmic</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Rapid learning or feature reuse? Towards understanding the effectiveness of MAML</title>
		<author>
			<persName><forename type="first">A</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schäfl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Seidl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Widrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gruber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Holzleitner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pavlović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">K</forename><surname>Sandve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Greiff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kreil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kopp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Brandstetter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.02217</idno>
		<title level="m">Hopfield networks is all you need</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Meta-learning with latent embedding optimization</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sygnowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Linear transformers are secretly fast weight programmers</title>
		<author>
			<persName><forename type="first">I</forename><surname>Schlag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-... hook. Diploma thesis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
		</imprint>
		<respStmt>
			<orgName>Institut für Informatik, Technische Universität München</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning to control fast-weight memories: An alternative to dynamic recurrent networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1992.4.1.131</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="131" to="139" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Learning to learn</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pratt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Springer US</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Attention is all you need</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning where to learn: Gradient sparsity in meta and continual learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Von Oswald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schug</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Caccia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zucchet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sacramento</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Watson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Smooth regression analysis</title>
		<imprint>
			<date type="published" when="1964">1964</date>
			<biblScope unit="page" from="359" to="372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Adaptive switching circuits</title>
		<author>
			<persName><forename type="first">B</forename><surname>Widrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Hoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1960 IRE WESCON Convention Record</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>IRE</publisher>
			<date type="published" when="1960">1960</date>
			<biblScope unit="page" from="96" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Graph transformer networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>D Álché-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.11342</idno>
		<title level="m">Dive into deep learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Meta-learning via hypernetworks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sacramento</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Oswald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Workshop on Meta-Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Model generation for supervised and semi-supervised few-shot learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vladymyrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hy-Pertransformer ; Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szepesvari</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="https://proceedings.mlr.press/v162/zhmoginov22a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Niu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Sabato</surname></persName>
		</editor>
		<meeting>the 39th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2022-07">Jul 2022</date>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page" from="17" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Beyond backpropagation: bilevel optimization through implicit differentiation and equilibrium propagation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Zucchet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sacramento</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2022-12">December 2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
