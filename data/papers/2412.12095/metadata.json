{
  "arxivId": "2412.12095",
  "title": "Causal Diffusion Transformers for Generative Modeling",
  "authors": "Chaorui Deng, Deyao Zhu, Kunchang Li, Shi Guang, Haoqi Fan",
  "abstract": "We introduce Causal Diffusion as the autoregressive (AR) counterpart of\nDiffusion models. It is a next-token(s) forecasting framework that is friendly\nto both discrete and continuous modalities and compatible with existing\nnext-token prediction models like LLaMA and GPT. While recent works attempt to\ncombine diffusion with AR models, we show that introducing sequential\nfactorization to a diffusion model can substantially improve its performance\nand enables a smooth transition between AR and diffusion generation modes.\nHence, we propose CausalFusion - a decoder-only transformer that\ndual-factorizes data across sequential tokens and diffusion noise levels,\nleading to state-of-the-art results on the ImageNet generation benchmark while\nalso enjoying the AR advantage of generating an arbitrary number of tokens for\nin-context reasoning. We further demonstrate CausalFusion's multimodal\ncapabilities through a joint image generation and captioning model, and\nshowcase CausalFusion's ability for zero-shot in-context image manipulations.\nWe hope that this work could provide the community with a fresh perspective on\ntraining multimodal models over discrete and continuous data.",
  "url": "https://arxiv.org/abs/2412.12095",
  "issue_number": 912,
  "issue_url": "https://github.com/dmarx/papers-feed/issues/912",
  "created_at": "2025-01-10T20:40:46.392642",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 36,
  "last_read": "2025-01-10T21:03:37.384735",
  "last_visited": "2025-01-10T21:02:08.602000+00:00",
  "main_tex_file": null,
  "published_date": "2024-12-16T18:59:29Z",
  "arxiv_tags": [
    "cs.CV"
  ]
}