- Decision to introduce Causal Diffusion as an autoregressive counterpart to diffusion models
- Choice of using a decoder-only transformer architecture for CausalFusion
- Decision to dual-factorize data across sequential tokens and diffusion noise levels
- Selection of ImageNet as the benchmark for evaluating CausalFusion
- Decision to allow for arbitrary token generation for in-context reasoning
- Choice to implement a joint image generation and captioning model
- Decision to explore zero-shot in-context image manipulations
- Choice of training recipes and model configurations for initial studies
- Decision to use loss weighting across noise levels during training
- Selection of the DiT architecture as the base configuration for CausalFusion
- Decision to optimize the number of AR steps during training
- Choice to implement a generalized causal attention mask
- Decision to minimize inductive biases in the model design
- Choice to evaluate model performance using FID-10k metrics
- Decision to explore the impact of training difficulties on model effectiveness
- Choice to allow flexibility in the number of AR steps and token processing at each step