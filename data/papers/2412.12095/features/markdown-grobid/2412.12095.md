# Causal Diffusion Transformers for Generative Modeling

## Abstract

## 

We introduce Causal Diffusion as the autoregressive (AR) counterpart of Diffusion models. It is a next-token(s) forecasting framework that is friendly to both discrete and continuous modalities and compatible with existing nexttoken prediction models like LLaMA and GPT. While recent works attempt to combine diffusion with AR models, we show that introducing sequential factorization to a diffusion model can substantially improve its performance and enables a smooth transition between AR and diffusion generation modes. Hence, we propose CausalFusion -a decoderonly transformer that dual-factorizes data across sequential tokens and diffusion noise levels, leading to state-of-the-art results on the ImageNet generation benchmark while also enjoying the AR advantage of generating an arbitrary number of tokens for in-context reasoning. We further demonstrate CausalFusion's multimodal capabilities through a joint image generation and captioning model, and showcase CausalFusion's ability for zero-shot in-context image manipulations. We hope that this work could provide the community with a fresh perspective on training multimodal models over discrete and continuous data.

## Introduction

Autoregressive (AR) and diffusion models are two powerful paradigms for data distribution modeling. AR models, also known as the next token prediction approach, dominate language modeling and are considered central to the success of large language models (LLMs) [[5,](#b4)[16,](#b15)[46,](#b45)[47,](#b46)[61,](#b60)[62]](#b61). On the other hand, diffusion models [[13,](#b12)[26,](#b25)[29,](#b28)[44]](#b43), or scorebased generative models [[37,](#b36)[54]](#b53), have emerged as the leading approach for visual generation, driving unprecedented progress in the era of visual content generation [[4,](#b3)[17,](#b16)[50]](#b49).

The intrinsic distinction between AR and diffusion models lies in their approach to data distribution factorization. AR models treat data as an ordered sequence, factorizing it along the sequential axis, where the probability of each token is conditioned on all preceding tokens. This factorization enables the AR paradigm to generalize effectively and efficiently across arbitrary number of tokens, making it well-suited for long-sequence reasoning and in-context generation. In contrast, diffusion models factorize data along the noise-level axis, where the tokens at each step are a refined (denoised) version of themselves from the previous step. As a result, the diffusion paradigm is generalizable to arbitrary number of data refinement steps, enabling iterative quality improvement with scaled inference compute. While AR and diffusion models each excel within their respective domains, their distinct factorization approaches reveal complementary potential. Although recent studies [[21,](#b20)[72,](#b71)[75]](#b74) have attempted to integrate AR and diffusion within a sin-  gle model, they typically treat these paradigms as separate modes, missing the potential benefits of jointly exploring them within a 2-D factorization plane.

To this end, we introduce CausalFusion, a flexible framework that integrates both sequential and noise-level data factorization to unify their advantages. The degree of factorization along these two axes-namely, the AR step and diffusion step-is adjustable, enabling CausalFusion to revert seamlessly to the traditional AR or diffusion paradigms at either extreme. To enhance its generality, CausalFusion is designed to predict any number of tokens at any AR step, with any pre-defined sequence order and any level of inference compute, thereby minimizing the inductive biases presented in existing generative models. As shown in Figure [1](#fig_0), this approach provides a broad spectrum between the AR and diffusion paradigms, allowing smooth interpolation within two endpoints during both training and inference. Specifically, we explore CausalFusion in image generation and multimodal generation scenarios, where we observe that the level of training difficulties significantly influences the overall effectiveness of CausalFusion.

Difficulties of generative tasks in CausalFusion: Both AR and diffusion paradigms present unique challenges based on difficulties of their specific generative stages. In diffusion models, the effectiveness of training depends heavily on proper loss weighting across noise levels [[22,](#b21)[26]](#b25), as higher noise levels are more difficult and usually provide more valuable signals than lower noise levels. Similarly, AR models are susceptible to error accumulation [[3]](#b2) as early-stage predictions are made with limited visible context, making them more error-prone. Optimizing CausalFusion thus requires balancing across these varying task difficulties to optimize training signal impact and ensure sufficient exploration across the entire factorization plane.

In this paper, we formally examine the difficulties of generative tasks within CausalFusion. We show that, in addition to the noise levels in diffusion and the amount of visible context in AR, the total number of AR steps, which controls the interpolation between AR and diffusion, also plays a critical role in shaping training difficulties. Driven by these factors, we develop a scalable and versatile model based on the CausalFusion framework. Starting from the DiT architecture [[44]](#b43), we gradually convert it into a decoderonly transformer compatible with existing AR models like GPT [[5,](#b4)[46,](#b45)[47]](#b46) and LLaMA [[16,](#b15)[61,](#b60)[62]](#b61). We provide insights on how to appropriately choose the number of AR steps during the training of CausalFusion models, and further introduce loss weighing along both the diffusion and AR axis to balance the impact of different generative stages. As shown in Figure [1](#fig_0) and 2, our model achieves state-ofthe-art performance on the ImageNet class-conditional generation benchmark, significantly outperforming DiT [[44]](#b43) and enabling zero-shot image manipulations due to its AR nature. When pretraining on both text-to-image and image-to-text tasks, our model surpasses forced-fusion frameworks such as TransFusion [[75]](#b74), demonstrating the versatility of our CausalFusion framework.

We highlight our main contribution below: • We propose CausalFusion as the AR counterpart to DiT, achieving state-of-the-art results and enabling the unlimited token generation for in-context reasoning. • We systematically study CausalFusion on the dualfactorization plane and identify key factors that improve the effectiveness of CausalFusion models. • Compared with recent studies [[75]](#b74), CausalFusion enables a smooth, cohesive integration with language modeling for cross-modal generation and reasoning.

## Related Works

Diffusion Models. Diffusion models [[26,](#b25)[52,](#b51)[53]](#b52) decompose the image generation task into a sequence of iterative denoising steps, gradually transforming noise into a coherent image. Early diffusion models [[13,](#b12)[43,](#b42)[45,](#b44)[49,](#b48)[50]](#b49) with U-net architectures pioneered denoising techniques for high-quality image synthesis. Later works [[1,](#b0)[44]](#b43) like DiT shift from the U-net to transformer-based architectures, enabling greater compute scalability. Modern methods [[8,](#b7)[33]](#b32) further extend DiT architectures leveraging significantly larger training resources to achieve impressive image generation quality.

Autoregressive Generation. Another popular approach for image generation involves autoregressive (AR) transformers that predict images token by token. Early works [[14,](#b13)[19,](#b18)[48,](#b47)[68]](#b67) generated image tokens in raster order, progressing sequentially across the image grid. This rasterized approach was later identified as inefficient [[6]](#b5), prompting researchers to explore random-order generation methods [[6,](#b5)[7]](#b6). AR methods are further evolved to include new modalities such as video generation [[31]](#b30) and any-to-any generation [[40,](#b39)[70]](#b69).

Combining Diffusion and Autoregressive Models. Recent models explore different methods for integrating AR and diffusion processes. DART [[21]](#b20) unifies AR and diffusion in a non-Markovian framework by conditioning on multiple historical denoising steps instead of only the current one. BiGR [[23]](#b22) generates discrete binary image codes autoregressively using a Bernoulli diffusion process. MAR [[34]](#b33) employs an AR model with a small diffusion head to enable continuous-value generation. Emu2 [[57]](#b56) applies an external diffusion module to decode its AR-based multimodal outputs. Compared to previous methods, CausalFusion focuses on autoregressive sequence factorization and decouples diffusion data processing across both sequential tokens and noise levels, achieving significant performance gains over traditional diffusion frameworks.

## CausalFusion

Preliminaries. We first briefly review the Autoregressive (AR) and Diffusion paradigms in the context of image modeling before introducing our CausalFusion model. Both paradigms factorize the image distribution into a chain of conditional distributions. However, they do so along different axes. Given a sample of training images X, AR models split X along the spatial dimensions into a sequence of tokens, X = {x 1 , . . . , x L }, where L is the number of tokens. The joint distribution of X can be then factorized sequentially:

$q(x 1:L ) = q(x 1 ) L l=2 q(x l |x 1:l-1 ).(1)$During training, a neural network p θ (x l |x 1:l-1 ) is trained to approximate q(x l |x 1:l-1 ) by minimizing the cross-entropy -E q(x 1:L ) log p θ (x 1:L ). At inference time, the image is generated via the next token prediction paradigm.

In contrast, Diffusion models gradually add random noise (typically Gaussian) to X in a so-called forward process. It is a Markov chain along the noise level, where each noisy version x t is conditioned on the previous state x t-1 as q

$(x t |x t-1 ) = N (x t ; √ 1 -β t x t-1 , β t I).$Here, β t is a variance schedule that ensures the forward process starts with a clean image x 0 = X and gradually converges to random noise as t → T . The joint distribution of X is then factorized as: q(x 0:T ) = q(x 0 ) T t=1 q(x t |x t-1 ).

(

$)2$To obtain X from noise, a neural network is trained to approximate the reverse transition of the forward process for t ∈ [1, T ]:

$p θ (x t-1 |x t ) = N (x t-1 ; µ θ (x t ), Σ θ (x t ))(3)$As in AR models, training involves minimizing the crossentropy between q(x 0:T ) and p θ (x 0:T ). In DDPM [[26]](#b25), Σ θ (x t ) is set to a constant value derived from the forward process, and µ θ (x t ) is set to be the linear combination of x t and a noise prediction model ϵ θ that predicts the noise ϵ of the forward process. This parameterization leads to the following training objective:

$min θ E x0,ϵ,t [w(t)∥ϵ -ϵ θ (x t , t)∥ 2 ](4)$where w(t) is derived according to the noise schedule β t , which gradually decays as t grows. This objective is further simplified by setting w(t) = 1 for all t, results in a weighted evidence lower bound that emphasizes more difficult denoising tasks (i.e., larger noise level) at larger t step. This approach enforces the model to reconstruct the image with partial observation, embodying the spirit of masked feature prediction models [[24,](#b23)[35,](#b34)[67]](#b66).

Our approach. From the above formulations, the AR and diffusion paradigms naturally support the scaling of sequence length and denoising steps, respectively, offering complementary advantages for image generation. To unify their advantages, we present CausalFusion, a general paradigm that scales effectively in both directions. We start by directly extending Eqn. (2) to encompass the AR factorization: q(x 0:T,κs |x 0,κ1:s-1 ) = q(x 0,κs ) T t=1 q(x t,κs |x t-1,κs , x 0,κ1:s-1 )

(5)

$for s ∈ [1, S].$Here, S denotes the total number of AR steps, while κ s is an index set that identifies the subset of image tokens to be processed at the s-th AR step, with |κ s | representing the number of tokens in this subset. Each AR step processes only the tokens indicated by κ s , isolating specific portions of the image, as shown in the top row of Figure [1](#fig_0). The term x t,κs represents the dual-factorized image tokens at the s-th AR step and t-th diffusion step.

During training, the objective of our CausalFusion model is to approximate p θ (x t-1,κs |x t,κs , x 0,κ1:s-1 ) for all t and s. Compared with the formulation in Eqn. (3), Causal-Fusion requires the training sequence to contain not only noised image tokens at the current AR step x t,κs , but also the clean image tokens from all previous AR steps x 0,κ1:s-1 , allowing the model to leverage information from earlier AR steps to refine the current tokens effectively. A generalized version of causal attention mask is also required to prevent x 0,κ1:s-1 from observing x t,κs . During inference, the dualfactroization in Eqn. [(5)](#b4)  an unlimited sequence of image tokens through a next token(s) diffusion approach while enhancing the quality of each token by applying larger numbers of diffusion steps. See Figure [3](#fig_3)(b) for an illustration of CausalFusion model architecture. Further details on the generalized causal attention mask can be found in Appendix A. Notably, adhering to the principle of minimal inductive bias, CausalFusion imposes no restrictions on the number of AR steps S, the number of tokens processed at each AR step |κ s |, or the specific token indices within each AR step. This flexibility enables a broad exploration space for generative modeling in both training and inference stages.

## Initial studies on CausalFusion

To systematically study the design space of CausalFusion, we conduct experiments on the ImageNet dataset [[12]](#b11), where we train class-conditional image generation models at 256×256 resolution. We use the DiT-L/2 model as our base configuration. All models are trained with 240 epochs and evaluated using FID-10k (unless specified, we use FID-10K and FID interchangeably) and the ADM [[13]](#b12) codebase.

Detailed training recipes and model configurations are provided in Appendix C.

Baseline setup: In-context DiT. As our target is to unify the AR and Diffusion paradigms, we need to unify their architectures first. To this end, we begin with the Transformer-based DiT [[44]](#b43) model. Following the DiT design, the 256×256 images are encoded into 32×32 latent representations [[50]](#b49) using a pretrained VAE model, followed by a 2×2 patchify layer that produces a sequence of L = 256 latent tokens. In original DiT, conditional information (e.g., label class) and the diffusion time step are incorporated through AdaLN-zero components, which are incompatible with decoder-only LLMs. To address this limitation, we adopt the in-context design of DiT from [[44]](#b43), treating the class and time step conditions as tokens and directly append them to the image token sequence. By default, we use 4 class tokens and one time step token. As a byproduct, this modification reduces the model size of incontext DiT to approximately To accelerate training, we use large batch sizes (e.g., 2048) and implement several improvements to stabilize training: (1) injecting the diffusion time step by adding a time step embedding to the image token embeddings instead of using a time step token; (2) applying head-wise QK layer normalization within the self-attention layers, following the practices in [[11]](#b10); and (3) incorporating a learning rate warmup stage during training.

The impact of our new designs is shown in Table [1](#tab_0). Initially, the native In-context DiT from [[44]](#b43) performs significantly worse than the AdaLN-zero version. Our revised training recipe improves performance to 21.94 FID. Incorporating the time step embedding and head-wise QK norm further enhances performance, achieving an FID of 18.66. Adding the learning rate warmup yields an additional improvement. Overall, our final in-context DiT-L/2 model, though conceptually simple, reaches an FID-10k of 13.78-competitive to the best-performing DiT-XL/2 model (12.92 FID-10k) in [[44]](#b43)-and serves as a robust baseline that can be steadily trained with large batch sizes.

CausalFusion with fixed number of AR steps. Building on the In-context DiT baseline, we begin with a simplified version of CausalFusion that uses a fixed number of AR steps S during both training and inference, with the number of tokens predicted at each AR step fixed to |κ s | = L S . Specifically, we modify the input sequence to include clean image tokens and use generalized causal attention masks within the attention modules. Figure [3](#fig_3) illustrates the architectural differences between DiT and CausalFusion. We train several CausalFusion models with different numbers of AR steps, i.e., S = 1, 2, 4, 8, and 256. Here, S = 1 indicates the In-context DiT, while S = 256, equivalent to L, represents the pure AR training mode. To evaluate these models, we first use the same number of AR steps as in training, and further study their generalization performance to other number of AR steps.

As shown in Table [2](#tab_1), CausalFusion trained with fixed AR steps cannot be robustly transferred to other inference settings. E.g., all models yield substantially worse performance when their inference settings are not aligned with training. By comparing the best evaluation result of each training setting, we observe that increasing the number of AR steps leads to a huge decline in performance. Specifically, the 8-step CausalFusion yields an FID of 20.01, clearly lagging behind the 13.78 FID achieved by In-context DiT. However, from the loss curves in Figure [4](#fig_4)(a), an opposite trend is observed, where models trained with more AR steps consistently exhibit lower loss values than those with fewer AR steps. This suggests that the learning tasks become over-simplified as the number of AR steps increases.

CausalFusion with random number of AR steps. Additionally, we train a CausalFusion model where the number of AR steps S are uniformly sampled from 1 to L, with |κ s | also randomly set at each AR step. We evaluate this model across various inference settings, same as above. As shown in Table [2](#tab_1), this training setting performs relatively consistent under different inference AR steps compared to those trained with fixed AR steps, demonstrating its greater flexibility during training and versatility during inference. However, this setting still leads to inferior results compared to the In-context DiT baseline, e.g., an FID of 21.31 when evaluated with a single AR step (S = 1) as diffusion mode. FID10k↓ ratio S eval = 1 S eval = 2 S eval = 4 S eval = 8 1.0 21.31 22.17 23.54 25.05 0.95 14.49 17.78 19.79 23.93 0.9 12.89 15.57 18.83 22.72 0.85 12.94 15.54 19.12 23.46 0.8 12.78 15.42 19.38 23.78 (a) Exponential decay in AR step sampling. A proper decay ratio leads to competitive or better performance across all inference settings than using fixed AR steps. Patch order FID10k↓ raster order 14.46 block-wise raster (8x8) 14.76 block-wise raster (4x4) 14.62 dilated order 15.54 random order 12.89 (b) Token order influences the locality of image tokens and further affects training difficulty. FID10k↓ weight S eval = 1 S eval = 2 S eval = 4 1→1 12.89 15.57 18.83 1.5→1 12.61 15.49 18.32 2→1 12.13 15.15 18.09 2.5→1 12.32 15.22 17.99 3→1 12.50 15.28 17.92 (c) AR loss weighting boosts performance by facilitating better learning from difficult samples.

Table 3. Ablations on AR step decay, ordering, and AR weighting. Baseline settings are marked by underlines and selected settings highlighted in gray . 

## Shaping task difficulties in CausalFusion

Based on the above observations, we aim to adjust the difficulties of the generative tasks in CausalFusion to balance training signal impact and ensure thorough exploration of the factorization space. By default, we use random AR steps during training due to its effectiveness in generalizing across various inference settings. Building on this setup, we identify several straightforward approaches that effectively shape task difficulties within CausalFusion, leading to significant performance improvements. We categorize the discussion into two parts: design choices for AR step sampling and loss weighting along the AR axis.

Random AR steps with decayed sampling. Instead of uniformly sampling the number of AR steps S from [1, L], we propose to exponentially decrease the sampling probability as S increases, which alleviates the problem of imbalanced |κ| s distribution, as shown in Figure [4(b)](#fig_4). As a result, large |κ s | appears more frequently in the training sequence, and more tokens are predicted base on less visible context. We introduce a hyper-parameter γ ≤ 1 to control the exponential decay ratio where γ = 1 denotes the naive CausalFusion model trained with uniformly sampled AR steps, while γ = 0 represents our In-context DiT baseline. From Table [3](#)(a), by decreasing γ from 1.0 to 0.95, we obtain remarkable performance improvements across all inference settings, with gains of nearly 7 and 5 points when S eval = 1 and 2, respectively. Furthermore, when γ = 0.9, CausalFusion surpasses the strong In-context DiT using the pure diffusion inference mode, and performance in other inference settings is further enhanced. While smaller values of γ, such as 0.8, yield even better performance with one AR step evaluation, we set the defualt value to 0.9 as it provides a balanced improvement across all inference settings.

Loss weighting along AR axis. We modify the weighting term w(•) in Eqn. ( [4](#formula_4)) to further consider the AR step s. In practice, we simply set w(s, t) to a pre-defined value λ ≥ 1 at s = 1, and linearly decay it to 1 at s = S, and keep using constant weight for different t. In this way, the model is trained to focus more on the hard generative tasks at early AR steps or larger noise levels. We analysis the impact of λ in Table [3](#)(c). From the table, setting λ to a proper value improves the performance. Intuitively, as the model generates closer to the end of the AR axis, the task becomes easier due to high locality [[65]](#b64) in the visible context, causing some generative tasks to degrade into local feature interpolation. In contrast, predictions made during earlier AR steps facilitate the learning of non-local dependencies within the visual context, which is beneficial to generative modeling.

Difficulty vs. locality. The hypothesis of locality aligns with our observations in Table [3](#)(b), where using random sequence order in CausalFusion during training significantly outperforms manually assigned orders. Specifically, using fixed (block-wise) raster order leads the model to rely excessively on local tokens, which makes the training task easier. In contrast, CausalFusion is trained with a random order by default, following the principle of minimal inductive bias. This design encourages the model to develop robust generative modeling abilities rather than relying on fixed ordering priors, while allowing flexible inference orders.

## Performance comparison

Class-conditional image generation. We evaluate our final method on the ImageNet class-conditional generation benchmark. For system-level comparisons, we use 64 tokens to encode the class condition. The impact of varying the number of class tokens is analyzed in Appendix B. We train three sizes of CausalFusion models: CausalFusion-L (368M), CausalFusion-XL (676M), and CausalFusion-H (1B). All models are trained for 800 epochs with a batch size of 2048. By default, we use a single AR step inference with 250 DDPM steps as in DiT [[44]](#b43), and report FID-50k for benchmarking against existing models. Detailed hyperparameters are provided Table 5. System performance comparison on 256×256 ImageNet generation, compared with previously reported large models.

in Appendix C. As shown in Table [4](#tab_4), on 256×256 image generation, CausalFusion-L achieves an FID-50k of 5.12 without classifier-free guidance [[25]](#b24) (CFG), outperforming DiT-XL/2 by 4.5 points with 50% fewer parameters. CausalFusion-XL further improves this result to 3.61, and when using CFG, achieves a state-of-the-art result of 1.77, significantly outperforming strong baselines like DiT and SiT. Additionally, CausalFusion-XL demonstrates effectiveness in high-resolution generation, achieving an FID of 1.98 on 512×512 images with CFG.

We also provide a system-level comparison with existing methods in Table [5](#). CausalFusion-H achieves an FID of 1.64 using the standard 250-step DDPM sampler, outperforming previous diffusion models with larger model sizes, such as FiTv2-3B [[66]](#b65) and Large-DiT-7B [[20]](#b19), and achieving comparable results to DiMR-G/2R (1.64 vs. 1.63) despite DiMR-G/2R's use of a stronger sampler (DPMsolver [[51]](#b50)). By applying the CFG interval [[32]](#b31) approach, CausalFusion-H further improves to an FID of 1.57, positioning it among the top-performing models on the Ima-geNet 256×256 benchmark.

A cupcake with sprinkles, lounging under a tiny umbrella on a beach.

A cactus wearing a sombrero and sunglasses in the desert.

A bear wearing a chef's hat baking cookies in a log cabin.

A goose in rain boots, stomping through puddles in the park.

(a) Samples on Text-to-Image generation.

A cat sitting on a red and white chair.

A kitchen with a stainless steel dishwasher and a wooden cabinet.  Source Size FID30k↓ CIDEr↑ Transfusion-L [75] IN1KCap 1M 8.1 34.5 CausalFusion-L IN1KCap 1M 7.1 47.9 (a) MSCOCO [36] is used for FID30k and CIDEr evalution. Params Data Size FID10k↓ Acc↑ CIDEr↑ DiT [44] 458M IN1K 1M 18.2 83.5 94.4 CausalFusion 368M IN1K 1M 11.8 84.2 98.0 CausalFusion † 368M IN1K 1M 9.3 84.7 103.2 (b) ImageNet is used for FID10k and Acc, MSCOCO is used for CIDEr evalution.

Table 6. (a) Comparison with Transfusion [75] on perception and generation benchmarks. All models are trained under the same settings using the same pretraining data. (b) Comparison with DiT [44] on perception and generation benchmarks. The model marked with † is trained with a VAE from [[34]](#b33), using a loss function to predict latent variables rather than noise.

Zero-shot image editing. CausalFusion naturally supports zero-shot image editing, as it is trained to predict a random subset of image tokens based on a random subset of visible image tokens. This inherent flexibility enables the model to perform localized edits without requiring task-specific fine-tuning. As shown in Figure [2](#fig_2)(b), our model can generate high-quality editing results even when only pretrained on the ImageNet class-conditional generation task, demonstrating its robustness and adaptability to editing tasks. Moreover, CausalFusion's dual-factorized design allows it to balance contextual coherence with highfidelity updates, ensuring that edited regions blend seamlessly into the surrounding content. See Appendix D for additional visualizations showcasing the model's ability to handle diverse editing scenarios.

Vision-Language joint modeling. CausalFusion can integrate the language modality by applying a separate nexttoken prediction loss on text, similar to GPT [[46]](#b45), enabling it to jointly model both image and text data. In this experiment, CausalFusion was trained on two tasks simultaneously: Text-to-Image (T2I) generation and Image Captioning. During training, text precedes the image in 90% of cases, framing it as a T2I task where only the image loss is applied. In the remaining cases, the text follows the image, and both text loss (for Image Captioning) and image loss (for classifier-free guidance [[25]](#b24) in T2I) are applied, with the text loss weighted at 0.01 relative to the image loss. We follow the configurations and training/inference protocols from previous sections. For language tokenization, we use the LLaMA-3 [[16]](#b15) tokenizer. Models are trained on a re-captioned ImageNet dataset, with each image labeled by 10 captions generated by Qwen2VL-7B [[64]](#b63). T2I and Image Captioning tasks are evaluated using zero-shot MSCOCO-30k FID and zero-shot MSCOCO CIDEr on Karpathy's test split, respectively. We compare Causal-Fusion with a contemporary multimodal model, Transfusion [[75]](#b74), which integrates language and vision modeling using standard diffusion loss for images and next-token prediction loss for text. In Transfusion, language tokens are conditioned on image embeddings with added diffusion noise. As Transfusion is not open-sourced, we implemented it based on the original paper, aligning the model architecture, VAE encoder, and language tokenizer with those used in CausalFusion. Results with 240-epoch training are presented in Table [6](#)(a). Compared to Transfusion, Causal-Fusion demonstrates superior performance in both text-toimage generation and image captioning, highlighting its strong potential as a foundational model for multimodal tasks. In Figure [5](#fig_7), we show a single pretrained CausalFusion XL model performing text-to-image generation at the top and image-to-text generation (image captioning) at the bottom. Further experiment details, including data, model design, and hyperparameters, are provided in Appendix C.

Visual Representation Learning. We further evaluate CausalFusion models from a representation learning perspective. Specifically, we leverage the CausalFusion model pretrained on the 256×256 ImageNet class-conditional generation task and fine-tune it on the ImageNet classification and MSCOCO captioning tasks. For image classification, we use the average-pooled features from the last layer of CausalFusion, followed by a linear classifier. For image captioning, we add a small transformer encoder-decoder module as the language head on top of CausalFusion. The pretrained CausalFusion model follows the default configuration described in previous sections. We compare it to the DiT-L/2 model pretrained for the same number of epochs. Detailed hyperparameters for fine-tuning are provided in Appendix C. As shown in Table [6](#)(b), our CausalFusion model outperforms DiT on all fine-tuning tasks, indicating that CausalFusion learns superior representations compared to DiT. We hypothesize that the random grouped token diffusion mechanism in CausalFusion, which diffuses images with partially observed inputs, implicitly performs as masked representation prediction [[24,](#b23)[67]](#b66), enhancing the model's representation learning ability.

## Conclusion

We propose CausalFusion, a decoder-only transformer that unifies AR and diffusion paradigms through a dualfactorized framework across sequential tokens and diffusion noise levels. This approach achieves state-of-the-art performance on the ImageNet generation benchmark, supports arbitrary-length token generation, and enables smooth transitions between AR and diffusion modes. CausalFusion also demonstrates multimodal capabilities, including joint image generation and captioning, as well as zero-shot image manipulations. Our framework offers a new perspective on unified learning of diffusion and AR models.

## A. Generalized Causal Attention

We design the Generalized Causal Attention for CausalFusion models. The core idea is to maintain causal dependencies across all AR steps while ensuring that each AR step relies only on clean image tokens from preceding AR steps. This design allows CausalFusion to generate images using a next-token(s) diffusion paradigm. We show an example of the generalized causal attention mask in Figure [6](#fig_8), and the PyTorch-style pseudo code for obtaining generalized causal mask in Algorithm 1. 

## B. More Analyses

Diffusion time steps sampling. Following DiT [[44]](#b43), we randomly sample the diffusion time step t during training. By default, the same t is used across all AR steps when training CausalFusion models. Here, we explore the impact of using different t values for different AR steps during training. The training and evaluation settings remain consistent with Section 4. As shown in Table [7](#tab_9), using either shared or random t values results in similar performance, indicating that CausalFusion is robust to this variation. Additionally, we evaluate a setting where multiple diffusion time steps are sampled for each AR step. Specifically, we experiment with sampling 4 and 8 different time steps, reducing the batch size by factors of 4× and 8×, respectively, to ensure the total number of tokens used for loss calculation remains constant. As shown in the table, using multiple diffusion time steps per AR step achieves comparable performance to the default setting, further demonstrating the robustness of CausalFusion to this factor. Notably, in this approach, the clean image tokens x 0,κs at each AR step need FID10k shared t for different AR steps 12.13 random t for different AR steps 12.27 4× t for each AR step 12.19 8× t for each AR step 12.23 Table 8. #Class tokens offers a trade-off between performance and number of parameters. The default setting is underlined.

Class condition tokens. As discussed in Sections 4 and 6, we use 4 class condition tokens for ablation studies and 64 tokens for system-level comparisons. Here, we examine the impact of the number of class tokens in the Causal-Fusion framework. As shown in Table [8](#), increasing the number of class tokens to 64 slightly improves performance (12.13 vs. 11.84 FID). However, this also adds 62.5M parameters, a significant increase (20%) for a CausalFusion-L model with 304M parameters. To address this, we adopt a token-repeating strategy from [[34]](#b33), which achieves comparable performance (11.75 vs. 11.84 FID) without increasing the parameter count. This finding suggests that the computation allocated to class conditioning is more critical than the number of parameters dedicated to it.

## C. Implementation Details

Class-conditional image generation. In   Table 11. Multi-modal experiment configuration for both Causal-Fusion and Transfusion.

Fine-tuning for ImageNet classification. When finetuning our CausalFusion model for ImageNet classification, we adhere to the basic architecture of the Vision Transformer (ViT) [[15]](#b14), with the exception of the class token. We exclude the extra timestep embedding, label embedding, and conditional position embedding. Layer normalization and a linear classification layer are applied to the averaged output tokens. Regarding hyperparameters, we follow the MAE training recipe [[24]](#b23) as detailed in Table [12](#tab_15), but we use BFloat16 precision during training to enhance stability.

Fine-tuning for MSCOCO captioning. We follow the COCO caption fine-tuning setup of FLIP [[35]](#b34), incorporating an additional caption head consisting of a 3-layer transformer encoder and a 3-layer transformer decoder (with a width of 384 and 6 attention heads). This caption head takes RandAug (9, 0.5) [10] label smoothing [[58]](#b57) 0.1 erasing [[74]](#b73) 0.25 mixup [[71]](#b70) 0.8 cutmix [[69]](#b68) 1.0 drop path [[28]](#b27) 0.1 (L) Table [13](#tab_0). MSCOCO captioning end-to-end fine-tuning setting image features from CausalFusion or DiT as input. We evaluate image features from the 14th, 21st, and 24th layers of CausalFusion and DiT, selecting the layer that achieves the highest performance. The models are fine-tuned on the Karpathy training split for 20 epochs.

## D. Additional Samples

We show more zero-shot editing results from our CausalDiffusion models in Figure [7](#fig_12)

![Figure 1. Illustration of Dual-Factorization. The arrow line indicates CausalFusion's generation path, moving from one state to the next by jointly generating along the sequential and noise-level dimension at each step. Compared to DiT, our In-context DiT substantially improves results with fewer parameters. CausalFusion further enhances performance without changing the architecture or parameter count. Results were trained on IN1K for 240 epochs. CausalFusion adopts arbitrary AR steps for image generation, but each step only diffuses partial tokens, resulting in similar (or slightly lower) computational complexity.]()

![(a) Samples generated by CausalFusion-XL/2, ImageNet 512×512, 800 epoch, DDPM 250 steps, CFG=4.0 (b) Zero-shot image editing results generated by CausalFusion-XL/2, ImageNet 512×512, 800 epoch. We first generate the original image (those on the left), then mask out its centre region, top-half, or bottom-half, and regenerate the image with new class conditions. Details are discussed in Sec 6.]()

![Figure 2. Visualization results. All samples are generated by models trained only on ImageNet-1K class-conditional generation task, demonstrating CausalFusion's zero-shot image manipulation ability. See more visualization results in Appendix D.]()

![Figure 3. Conceptual comparison between the DiT and Causal-Fusion architectures. a) DiT incorporates conditioning via adaptive layer normalization, processing a fixed-size set of entire image tokens as input. All the noise tokens xt are fed into DiT with full attention observation, enabling comprehensive modeling of the input during processing. b) CausalFusion treats all input modalities equally in an in-context manner, denoising a random subset of image tokens xt,κ s at each step while causally conditioning on previously denoised tokens x0,1:κ s-1 , and other contextual inputs.This approach enforces the model to reconstruct the image with partial observation, embodying the spirit of masked feature prediction models[24,35,67].]()

![b) offers further insight into this behavior, showing that uniform AR step sampling during training leads to a highly imbalanced |κ s | distribution. As a result, the training signal is dominated by AR steps with very few tokens-over 95% of AR steps have |κ s | ≤ 16. These steps are uniformly distributed along the AR axis, causing the model to overly rely on visible context and thus diminishing the complexity of the training task.]()

![Lastly, given the CausalFusion model trained with random AR steps, we compare the loss values produced by different AR steps on the validation set. As shown in Figure 4(c), later AR steps yield much lower loss values than earlier steps, suggesting a clear trend of vanished training signals along the AR axis.]()

![Samples on Image Caption generation.]()

![Figure 5. Multimodal generation. Results are generated by a single CausalFusion XL model trained on ImageNet recaption data.]()

![Figure 6. Generalized causal mask. In this case, the input sequence is organized to have 3 AR-steps κ1, κ2, and κ3, containing 2, 2, and 3 tokens, respectively. x0,κ 1 and x0,κ 2 are the clean tokens at the first two AR steps, while xt,κ 1 , xt,κ 2 , and xt,κ 3 are noised tokens. White and gray blocks denote the masked and unmasked attention, respectively. Note that, each xt,κ s only attends to itself and the clean tokens from previous AR steps x0,κ 1:s-1 .]()

![Generalized causal maskdef get_attn_mask(ctx_len, x_len, step): # tx_len: the length of clean tokens # x_len: the length of noisy tokens # step: number of AR steps # sample random tokens per AR step sumk = random.sample(range(1, x_len), step -1) sumk = [0] + sorted(sumk) + [x_len] # build 'causal' masks seq_len = ctx_len + x_len attn_mask = torch.ones(size=(seq_len, seq_len)) m1 = torch.ones(size=(ctx_len, ctx_len)) m2 = torch.ones(size=(x_len, ctx_len)) m3 = torch.ones(size=(x_len, x_len)) for i in range(len(sumk) -2): m1[sumk[i]:sumk[i+1], 0:sumk[i+1]] = 0 m2[sumk[i+1]:sumk[i+2], 0:sumk[i+1]] = 0 for i in range(len(sumk) -1): m3[sumk[i]:sumk[i+1], sumk[i]:sumk[i+1]] = 0 attn_mask[:ctx_len, :ctx_len] = m1 attn_mask[ctx_len:, :ctx_len] = m2 attn_mask[ctx_len:, ctx_len:] = m3 return attn_mask # 1 for mask, 0 for unmask to be computed only once and can be shared across multiple x t,κs with different t values. Consequently, the additional computational cost introduced by clean image tokens during training is minimized, amounting to only ∼10%.]()

![The editing results are achieved by first generating the original image using the initial class label, then masking a portion of the image, and regenerating it conditioned on the unmasked region and the new class label. For example, in the first example in Figure7, an image of "volcano" is first generated. Then, the outer region of the image is masked out, and the new images are regenerated with new labels, such as "televison", "sliding door", and "car mirror".We further show uncurated samples from our CausalDiffusion-XL models at 512×512 and 256×256 resolution. Figures 17 through 22 display samples under varying classifier-free guidance scales and class labels.]()

![Figure 7. Zero-shot editing samples. CausalFusion-XL, resolution 512×512, 800 epoch, Classifier-free guidance scale = 3.0.]()

![Figure 8. Zero-shot editing samples. CausalFusion-XL, resolution 256×256, 800 epoch, Classifier-free guidance scale = 1.5.]()

![Figure 9. Uncurated 512 × 512 CausalFusion-XL samples. Classifier-free guidance scale = 4.0 Class label = "otter" (360)]()

![]()

![]()

![]()

![]()

![]()

![]()

![]()

![]()

![]()

![]()

![]()

![]()

![enables CausalFusion to generateIn-context DiT baseline. ImageNet 256×256, 240 epoch. Baseline settings are marked by underlines and selected settings highlighted in gray .]()

![Ablations on AR steps. Strain and Seval indicates the fixed AR steps used during training and inference, respectively. Baseline settings are marked by underlines and selected settings highlighted in gray .]()

![System performance comparison on ImageNet class-conditioned generation. Numbers marked with gray blocks use temperature sampling during inference.]()

![Diffusion time steps sampling strategy does not affect the performance. The default setting is underlined.]()

![we provide the detailed settings of CausalFusion models for classconditional image generation in Section 4 and 5.]()

![Ablation study configuration.]()

![System-level comparison configuration.InTable 11, we provide the detail experiment hyperparameters for both CausalFusion and Transfusion experiments in Section 6. The training dataset is augmented with 10 captions per image from Ima-geNet, generated using Qwen2VL-7B-Instruct[64] with the following prompt:You are an image captioner. You need to describe images in COCO style. COCO style is short. Here are some examples of COCO style descriptions: 'A car seems to be parked illegally behind a legally parked car' 'This is a blue and white bathroom with a wall sink and a lifesaver on the wall.' 'Meat with vegetable and fruit displayed in roasting pan.' 'Group of men playing baseball on an open makeshift field.']()

![ImageNet classification end-to-end fine-tuning setting.]()

