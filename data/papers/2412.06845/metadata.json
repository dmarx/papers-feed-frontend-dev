{
  "arxivId": "2412.06845",
  "title": "Fully Open Source Moxin-7B Technical Report",
  "authors": "Pu Zhao, Xuan Shen, Zhenglun Kong, Yixin Shen, Sung-En Chang, Timothy Rupprecht, Lei Lu, Enfu Nan, Changdi Yang, Yumei He, Xingchen Xu, Yu Huang, Wei Wang, Yue Chen, Yong He, Yanzhi Wang",
  "abstract": "Recently, Large Language Models (LLMs) have undergone a significant\ntransformation, marked by a rapid rise in both their popularity and\ncapabilities. Leading this evolution are proprietary LLMs like GPT-4 and\nGPT-o1, which have captured widespread attention in the AI community due to\ntheir remarkable performance and versatility. Simultaneously, open-source LLMs,\nsuch as LLaMA and Mistral, have made great contributions to the ever-increasing\npopularity of LLMs due to the ease to customize and deploy the models across\ndiverse applications. Although open-source LLMs present unprecedented\nopportunities for innovation and research, the commercialization of LLMs has\nraised concerns about transparency, reproducibility, and safety. Many\nopen-source LLMs fail to meet fundamental transparency requirements by\nwithholding essential components like training code and data, and some use\nrestrictive licenses whilst claiming to be \"open-source,\" which may hinder\nfurther innovations on LLMs. To mitigate this issue, we introduce Moxin 7B, a\nfully open-source LLM developed in accordance with the Model Openness Framework\n(MOF), a ranked classification system that evaluates AI models based on model\ncompleteness and openness, adhering to principles of open science, open source,\nopen data, and open access. Our model achieves the highest MOF classification\nlevel of \"open science\" through the comprehensive release of pre-training code\nand configurations, training and fine-tuning datasets, and intermediate and\nfinal checkpoints. Experiments show that our model achieves superior\nperformance in zero-shot evaluation compared with popular 7B models and\nperforms competitively in few-shot evaluation.",
  "url": "https://arxiv.org/abs/2412.06845",
  "issue_number": 639,
  "issue_url": "https://github.com/dmarx/papers-feed/issues/639",
  "created_at": "2025-01-04T06:52:51.688399",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 53,
  "last_read": "2025-01-04T06:53:18.651113",
  "last_visited": "2024-12-30T20:04:21.071Z",
  "main_tex_file": null,
  "published_date": "2024-12-08T02:01:46Z",
  "arxiv_tags": [
    "cs.CL",
    "cs.AI",
    "cs.LG"
  ]
}