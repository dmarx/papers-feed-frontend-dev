<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fully Open Source Moxin-LLM Technical Report</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-12-11">11 Dec 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Pu</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Northeastern University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xuan</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Northeastern University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhenglun</forename><surname>Kong</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yixin</forename><surname>Shen</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sung-En</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Northeastern University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Timothy</forename><surname>Rupprecht</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Northeastern University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lei</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Northeastern University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Enfu</forename><surname>Nan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Northeastern University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Changdi</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Northeastern University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yumei</forename><surname>He</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Tulane University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xingchen</forename><surname>Xu</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">Futurewei Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yue</forename><surname>Chen</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">Futurewei Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yong</forename><surname>He</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">Futurewei Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yanzhi</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Northeastern University</orgName>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="laboratory">AIBAO LLC</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Fully Open Source Moxin-LLM Technical Report</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-12-11">11 Dec 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">205955254EC5D3621689D96FB1E04164</idno>
					<idno type="arXiv">arXiv:2412.06845v2[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, Large Language Models (LLMs) have undergone a significant transformation, marked by a rapid rise in both their popularity and capabilities. Leading this evolution are proprietary LLMs like GPT-4 and GPT-o1, which have captured widespread attention in the AI community due to their remarkable performance and versatility. Simultaneously, open-source LLMs, such as LLaMA and Mistral, have made great contributions to the ever-increasing popularity of LLMs due to the ease to customize and deploy the models across diverse applications. Although open-source LLMs present unprecedented opportunities for innovation and research, the commercialization of LLMs has raised concerns about transparency, reproducibility, and safety. Many open-source LLMs fail to meet fundamental transparency requirements by withholding essential components like training code and data, and some use restrictive licenses whilst claiming to be "open-source," which may hinder further innovations on LLMs. To mitigate this issue, we introduce Moxin 7B, a fully open-source LLM developed in accordance with the Model Openness Framework (MOF), a ranked classification system that evaluates AI models based on model completeness and openness, adhering to principles of open science, open source, open data, and open access. Our model achieves the highest MOF classification level of "open science" through the comprehensive release of pre-training code and configurations, training and fine-tuning datasets, and intermediate and final checkpoints. Experiments show that our model achieves superior performance in zero-shot evaluation compared with popular 7B models and performs competitively in few-shot evaluation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The field of natural language processing has witnessed the most exciting discoveries of the last ten years with the emergence of large language models (LLMs). At the forefront of this evolution are LLMs such as GPT-4 <ref type="bibr" target="#b0">[1]</ref>, Claude <ref type="bibr" target="#b1">[2]</ref>, and Gemini <ref type="bibr" target="#b2">[3]</ref>, which have captured the attention of the AI community due to their performance and versatility. Meanwhile, the recent emergence of openly accessible yet highly capable LLMs such as LLaMA <ref type="bibr" target="#b3">[4]</ref>, Falcon <ref type="bibr" target="#b4">[5]</ref>, and Mistral <ref type="bibr" target="#b5">[6]</ref> allow researchers and practitioners to easily obtain, customize, and deploy LLMs in more various environments and for more diverse use cases. The trends have made people eagerly asking about what's next and some suggest "a general intelligence" is right around the corner.</p><p>Despite the growing influence and accessibility of open-source LLMs, a notable challenge emerged: many model producers restrict visibility and access to their training, fine-tuning, and evaluation processes, including crucial components such as their training code and data <ref type="bibr" target="#b6">[7]</ref>. Some model producers even use restrictive licenses whilst claiming to be "open-source." This practice creates barriers for the broader AI research community to study, replicate, and innovate upon advanced LLMs. In parallel, it prevents businesses from fully leveraging open-source models for innovative industrial applications, as its commercialization has raised concerns about transparency, reproducibility, and safety.</p><p>To unlock the full potential of LLMs and open innovation, we must return to democratize this research by putting the model into the hands of more researchers and making the datasets the models train on fully open-source. This requires moving beyond the simple sharing of model weights to embrace complete transparency in training, datasets, and implementation detail, which is crucial for fostering a more inclusive and collaborative research environment that can sustain a healthy open-source ecosystem <ref type="bibr" target="#b7">[8]</ref>.</p><p>To achieve this goal, we introduce Moxin 7B, a fully open-source LLM developed by complying with the Model Openness Framework (MOF) introduced by <ref type="bibr" target="#b8">[9]</ref>. The MOF provides a systematic ranking classification system to rate AI models based on their completeness and openness, incorporating the principles of open science, open source, open data, and open access. By promoting transparency and reproducibility, the MOF serves as a crucial tool to combat "openwashing" practices and establishes completeness and openness as primary criteria alongside the core tenets of responsible AI. Wide adoption of the MOF will cultivate a more open AI ecosystem, benefiting research, innovation, and adoption of state-of-the-art models.</p><p>Our open-source LLM has released pre-training code and configurations, training and fine-tuning data, and intermediate and final checkpoints, aiming to make continuous commitments to fully open-source LLMs. Our model achieves the highest MOF classification level of "open science." It is noteworthy that this commitment to openness has not compromised performance: our base model achieves superior performance in zero-shot evaluation compared with popular 7B models and performs competitively in few-shot evaluation. Remarkably, our chat model can outperform 7B baselines like Llama2-7B-chat. Our homepage is <ref type="url" target="https://github.com/moxin-org/Moxin-LLM">https://github.com/moxin-org/Moxin-LLM</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Models, Tokenizers, and Training</head><p>Models. State-of-the-art large language models (LLMs) typically comprise a substantial number of parameters, often approaching or exceeding 100 billion <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b2">3]</ref>. To facilitate broader accessibility, smaller models with fewer than 20 billion parameters, and even those around 7 billion parameters, have been developed <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>. In addition, efficiency-enhancing techniques, such as implementing MAMBA-based architectures in Jamba, have been employed to optimize performance <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>.</p><p>Tokenizers. Tokenizers are essential to convert raw data into a suitable format for model processing. Many contemporary models employ Byte-Pair Encoding (BPE) <ref type="bibr" target="#b13">[14]</ref>, with OpenAI's tiktoken tokenizer <ref type="bibr" target="#b14">[15]</ref> being a notable implementation. However, for languages that handle tokens differently from Romance languages, alternatives such as SentencePiece <ref type="bibr" target="#b15">[16]</ref> are utilized, as seen in XLNet <ref type="bibr" target="#b16">[17]</ref>. Hugging Face offers an excellent summary of state-of-the-art tokenizers with practical examples <ref type="bibr" target="#b17">[18]</ref>. Moreover, tokenization extends beyond text modalities; many foundational models now include multimodal capabilities, processing documents, audio, images, and even videos <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>Training. To enhance the performance of smaller models beyond their inherent limitations, various training strategies can be employed. A notable example is the application of Mixture of Experts (MoE) training, which has achieved significant success in models like Mixtral <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Data curation methods</head><p>Researchers commonly collect large datasets for training language models (LMs) <ref type="bibr" target="#b23">[24]</ref> by performing web crawls. However, these datasets often contain undesirable content, necessitating data curation to improve their quality. To enhance model performance <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27]</ref>, several data curation techniques are widely employed. These include filtering by language <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>, heuristic-based filtering <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref>, quality filtering <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref>, data deduplication <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>, and data mixing <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Open-source datasets</head><p>As the scale of LMs has increased in recent years <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b0">1]</ref>, the community has correspondingly curated larger datasets to support their training. Early datasets include the C4 dataset, containing 160 billion tokens, and The Pile <ref type="bibr" target="#b31">[32]</ref>, which comprises 300 billion tokens. More recently, even larger datasets have been introduced: RefinedWeb <ref type="bibr" target="#b24">[25]</ref> with 600 billion tokens, Dolma <ref type="bibr" target="#b42">[43]</ref> with 3 trillion tokens, FineWeb <ref type="bibr" target="#b43">[44]</ref> with 15 trillion tokens, and RedPajama-v2 <ref type="bibr" target="#b44">[45]</ref> containing 30 trillion tokens. In addition to these general-purpose datasets, large domain-specific datasets have also been developed. For instance, StackV2 <ref type="bibr" target="#b45">[46]</ref>, a code-focused dataset, includes 900 billion tokens, and FineWeb-Edu <ref type="bibr" target="#b43">[44]</ref>, a high-quality filtered educational text dataset, contains 1.3 trillion tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model Training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Architecture</head><p>We opt to extend the Mistral model architecture <ref type="bibr" target="#b5">[6]</ref> due to its ability to achieve high performance while maintaining efficient inference speeds. The original Mistral 7B model demonstrates superior performance compared to multiple 7B language models and even outperforms larger models on various evaluation benchmarks. Notably, it surpasses the LLaMA 34B model <ref type="bibr" target="#b46">[47]</ref> in tasks such as mathematics and code generation. The original Mistral model leverages grouped-query attention (GQA) <ref type="bibr" target="#b47">[48]</ref> and sliding window attention (SWA) <ref type="bibr" target="#b48">[49]</ref>. GQA reduces memory requirements during decoding, allowing for larger batch sizes and higher throughput, and it significantly accelerates inference speed-an essential factor in real-time applications. Meanwhile, SWA effectively handles long sequences without incurring substantial computational overhead. By incorporating these techniques, the model achieves significant improvements in performance and efficiency, which we have adopted in our extended model.</p><p>Building upon the original Mistral model, which consists of 32 blocks, we have extended the architecture to 36 blocks. Furthermore, we also employ GQA to partition the query heads into multiple groups, each sharing a single key head and value head. This approach interpolates between multi-query attention (MQA) and multi-head attention (MHA) in large language models, striking a balance between the computational speed of MQA and the representational quality of MHA, thereby providing a favorable trade-off. Additionally, our model incorporates a rolling buffer cache with a fixed attention span, effectively limiting cache size and preventing excessive memory usage when processing long sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training Data</head><p>Data are fundamental to the pre-training of LLMs. Preparing such training data requires careful consideration of multiple challenges, including handling sensitive information, ensuring comprehensive knowledge coverage, and achieving higher efficiency with improved data quality.</p><p>In this section, we detail the processes of preparing textual data from general domains and coding data related to programming languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Text Data</head><p>We use a mix of data from SlimPajama <ref type="bibr" target="#b49">[50]</ref> and DCLM-BASELINE <ref type="bibr" target="#b37">[38]</ref> as our text training data.</p><p>During the training of LLaMA, it was demonstrated that the performance of a 7B model continues to improve even after being trained on more than 1T tokens <ref type="bibr" target="#b50">[51]</ref>. Given the outstanding performance of LLaMA, its data collection methodology was rapidly replicated, leading to the release of RedPajama, an open-source dataset containing 1.2 trillion tokens <ref type="bibr" target="#b51">[52]</ref>.</p><p>However, subsequent analyses reveal a significant limitation: some corpora within RedPajama contain a large percentage of duplicate content. The deduplication guidelines in RedPajama operate only within individual data sources, leaving inter-source duplicates largely unaddressed. To improve data quality and training efficiency, SlimPajama<ref type="foot" target="#foot_0">foot_0</ref> was developed as a refined iteration of RedPajama, offering a cleaned and extensively deduplicated version <ref type="bibr" target="#b49">[50]</ref>.</p><p>SlimPajama implements a rigorous two-stage preprocessing pipeline to enhance data quality. In the first stage, short and low-quality documents are removed from RedPajama. Specifically, documents that have fewer than 200 characters after removing punctuation, space symbols, newlines, and tabs are filtered out, as these documents typically contain only metadata and lack useful information. As a result of this step, 1.86% of RedPajama documents are eliminated.</p><p>The second step involves removing duplicate data, as deduplication enhances training efficiency and reduces memorization, thereby decreasing the likelihood of generating text solely by recalling training data <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b54">55]</ref>. To perform deduplication, document signatures are created using pre-processed, lower-cased 13-grams. Subsequently, MinHashLSH <ref type="bibr" target="#b55">[56]</ref> is employed to identify and eliminate duplicates based on a Jaccard similarity threshold of 0.8. Deduplication is performed both within and across data sources. Overall, by pruning 49.6% of the bytes from the RedPajama dataset, the 627B-token SlimPajama dataset is obtained.</p><p>Additionally, we utilize the DCLM-BASELINE <ref type="bibr" target="#b37">[38]</ref> dataset<ref type="foot" target="#foot_1">foot_1</ref> , which is derived from CommonCrawl, a web-crawled dataset <ref type="bibr" target="#b56">[57]</ref>. The construction of DCLM-BASELINE involves several steps. First, resiliparse is employed to extract text from CommonCrawl. Second, deduplication is performed using MinHash <ref type="bibr" target="#b57">[58]</ref> within a suffix array pipeline <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b35">36]</ref> and near-duplicate Bloom filtering, which enhances the exact document and paragraph deduplication scheme <ref type="bibr" target="#b42">[43]</ref>. Third, recent studies <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b60">61]</ref> demonstrate that utilizing learnable models as quality filters leads to downstream performance improvements. Consequently, DCLM-BASELINE applies a fastText OH-2.5 combined with an ELI5 classifier score to retain the top 10% of documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Coding Data</head><p>Programming is crucial for LLMs to support various downstream tasks, such as code completion from natural language descriptions, documentation generation for individual functions, and autocompletion of code snippets. Furthermore, as code is generally better structured and organized than natural language, training on code data may improve the LLM reasoning capabilities <ref type="bibr" target="#b61">[62]</ref>. Therefore, We use part of the-stack-dedup <ref type="bibr" target="#b62">[63]</ref> dataset <ref type="foot" target="#foot_2">3</ref> during the pretraining.</p><p>The Stack comprises more than 6TB of permissively-licensed source code files across 358 programming languages <ref type="bibr" target="#b62">[63]</ref>. This carefully curated resource was designed to enhance the code generation capabilities of LLMs. It facilitates the synthesis of programs by code-generating AI systems from both natural language descriptions and existing code snippets.</p><p>To construct the Stack dataset, 220.92 million active GitHub repositories were collected from event archives published between 2015 and 2022 on GHArchive. Of these repositories, only 137.36 million were publicly accessible on GitHub, resulting in 51.76 billion downloaded files. After initial filtering, 5.28 billion unique files were identified, with an uncompressed size of 92.36 TB.</p><p>To ensure data quality, near-deduplication was implemented within the preprocessing pipeline in addition to exact deduplication. Specifically, MinHash with 256 permutations was computed for all documents, and Locality Sensitive Hashing was employed to identify clusters of duplicates. Within these clusters, Jaccard similarities were calculated to detect near-duplicates using a similarity threshold of 0.85. Approximately 40% of permissively licensed files were identified as (near-)duplicates and subsequently removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Capability Enhancement</head><p>LLMs are expected to demonstrate capabilities such as reasoning, mathematical problem-solving, and knowledge memorizing. However, a significant challenge lies in that, in the pre-training process, high-quality capability-related data is sparsely distributed in the entire corpus, and thereby it is difficult for models to be proficient at these above-mentioned capabilities. Previous research, such as work on Qwen <ref type="bibr" target="#b9">[10]</ref>, GLM-130B <ref type="bibr" target="#b63">[64]</ref>, Nemotron-4 <ref type="bibr" target="#b64">[65]</ref>, has tried to incorporate instructionbased or high-quality data during the pre-training stage to enhance these abilities. In our study, we collect open-source data from HuggingFace, primarily utilizing the training datasets of various evaluation benchmarks such as MMLU <ref type="bibr" target="#b65">[66]</ref> and HellaSwag <ref type="bibr" target="#b66">[67]</ref>. These data are used experimentally to investigate the relationship between high-quality, capability-focused training data and model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training Configuration</head><p>The total number of tokens used for pre-training our Moxin-7B model is over 2T, and the pre-training process consists of three phases. In the first phase, we use pre-training corpora with the context length of 2k. In the second phase, we use pre-training corpora with the context length of 4k. In the third phase, we utilize the capability-specific enhancement data. We provide the model performance with only the first two phases and also with all three phases to validate the performance of the third phase.</p><p>We use Colossal-AI <ref type="bibr" target="#b67">[68]</ref> as our training framework. Colossal-AI is a unified deep learning system that provides the fullest set of acceleration techniques for the AI community. With its modular design, ColossalAI allows for a free combination of these techniques to achieve the best training speedup. Colossal-AI's optimized parallelism and heterogeneous training methods are employed to achieve superior system performance compared to baseline systems. These methods are provided through user-friendly APIs, requiring minimal code modifications.</p><p>During training, AdamW <ref type="bibr" target="#b68">[69]</ref> with β 1 = 0.9, β 2 = 0.95, ϵ = 1e -8 and weight decay = 0.1 is used to optimize the model. We use the cosine learning rate decay and the learning rate decays to 10% of its maximum. Learning Rate is set to 2e -6 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Alignment</head><p>Following the pre-training phase, we fine-tune the model into a helpful and harmless AI assistant. In our Alignment stage, we mainly use supervised fine-tuning (SFT), during which we fine-tune the model to follow diverse human instructions by high-quality instruction data. We use the Tulu v2 dataset <ref type="bibr" target="#b69">[70]</ref> for instruction tuning. The dataset consists of a mix of FLAN, Open Assistant 1, ShareGPT, GPT4-Alpaca, LIMA, and so on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Long-Context</head><p>To deal with the long-context problem, our model leverages grouped-query attention (GQA) <ref type="bibr" target="#b47">[48]</ref>, sliding window attention (SWA) <ref type="bibr" target="#b48">[49]</ref>, and Rolling Buffer Cache <ref type="bibr" target="#b5">[6]</ref>. GQA reduces the memory requirement during decoding, allowing for higher batch sizes hence higher throughput.</p><p>Besides, SWA can handle longer sequences more effectively at a reduced computational cost, thereby alleviating a common limitation in LLMs. SWA exploits the stacked layers of a transformer to attend information beyond the window size W . At the last layer, with SWA, using a window size of W = 4096, we have a theoretical attention span of approximately 14K tokens or above.</p><p>Our model adopts Rolling Buffer Cache which limits the cache size using a rolling buffer cache with a fixed attention span. The cache has a fixed size of W , and the keys and values for the timestep i are stored in position i mod W of the cache. As a result, when the position i is larger than W , past values in the cache are overwritten, and the size of the cache stops increasing. On a sequence length of 32k tokens, this reduces the cache memory usage by 8×, without impacting the model quality.</p><p>With the above techniques, our model can support 32K context length with fast inference and low memory cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>We conducted comprehensive performance comparisons against leading language models of comparable scale, including Mistral-7B <ref type="bibr" target="#b5">[6]</ref>, LLaMA 2-7B <ref type="bibr" target="#b50">[51]</ref>, Gemma-7B <ref type="bibr" target="#b40">[41]</ref>, and Qwen v2-7B <ref type="bibr" target="#b10">[11]</ref>. These models were selected based on their demonstrated excellence within the 7B or 8B category and represent diverse development approaches from various research organizations worldwide. To ensure a robust evaluation, we re-run all benchmarks with the same evaluation pipeline for fair comparisons. Specifically, we use lm-evaluation-harness <ref type="bibr" target="#b70">[71]</ref> and opencompass <ref type="bibr" target="#b71">[72]</ref> for evaluation.</p><p>Lm-evaluation-harness provides a unified framework to test generative language models on a large number of different evaluation tasks. It supports over 60 standard academic benchmarks for LLMs, with hundreds of subtasks and variants implemented. This framework is versatile as it extends to models implemented through various architectures, including transformers (including quantization via AutoGPTQ <ref type="bibr" target="#b72">[73]</ref>), GPT-NeoX <ref type="bibr" target="#b73">[74]</ref>, and Megatron-DeepSpeed <ref type="bibr" target="#b74">[75]</ref>, all unified through a flexible, tokenization-agnostic interface. The framework is reliable, as evidenced by serving as the backend for HuggingFace's popular Open LLM Leaderboard and being utilized by dozens of organizations, including NVIDIA, Cohere, BigScience, BigCode, Nous Research, and Mosaic ML.</p><p>To complement, we also employed openCompass. This framework performs an in-depth and holistic assessment of large language models structured around eight fundamental dimensions of language model capabilities: language comprehension, knowledge precision, logical deduction, creative ideation, mathematical problem-solving, programming proficiency, extended text analysis, and intelligent agent engagement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation Tasks</head><p>We evaluate the model performance on various tasks below.</p><p>• AI2 Reasoning Challenge (ARC) <ref type="bibr" target="#b75">[76]</ref> -a set of genuine grade-school level, multiple-choice science questions, assembled to encourage research in advanced question-answering. The dataset is partitioned into a Challenge Set (ARC-C) and an Easy Set (ARC-E), where the former contains only questions answered incorrectly by both a retrieval-based algorithm and a word co-occurrence algorithm.</p><p>• HellaSwag <ref type="bibr" target="#b66">[67]</ref> -a test of commonsense natural language inference, which is easy for humans ( 95%) but challenging for SOTA models. It consists of 70,000 multiple-choice questions. Each question presents a scenario followed by four possible outcomes, asking the model to select the most reasonable conclusion.</p><p>• MMLU <ref type="bibr" target="#b76">[77]</ref> -a test to measure a text model's multitask accuracy. The test covers 57 tasks, including elementary mathematics, US history, computer science, law, etc.</p><p>• Winogrande <ref type="bibr" target="#b77">[78]</ref> -an adversarial and difficult Winograd benchmark at scale, for commonsense reasoning. It contains 44,000 multiple-choice questions with two options each. It requires the model to choose the appropriate entity word for the pronoun in the descriptive text based on the scenario.</p><p>• PIQA <ref type="bibr" target="#b78">[79]</ref> -the task of physical commonsense reasoning and a corresponding benchmark dataset Physical Interaction: Question Answering (PIQA). Physical commonsense knowledge is a major challenge on the road to true AI-completeness, including robots that interact with the world and understand natural language. PIQA focuses on everyday situations with a preference for atypical solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Results</head><p>We name the initial model as Moxin-7B-original, which presents the foundation model before fine-tuning on the training data of the evaluation datasets. After subsequent partial fine-tuning of Moxin-7B-original on the training data of the evaluation datasets, we developed Moxin-7B-finetuned, enabling direct assessment of how targeted fine-tuning affects model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Zero-Shot Evaluation</head><p>We report the result of base models for zero-shot evaluation in Table <ref type="table" target="#tab_1">2</ref>. The tasks are listed below.</p><p>After training with the training data of evaluation tasks, our Moxin-7B-finetuned can achieve superior performance compared with state-of-the-art (SOTA) baselines. This significant increase from the base model demonstrates the effectiveness of our fine-tuning approach. The improved performance is particularly notable on complex reasoning tasks like PIQA, where the score increased from 78.07% to 82.24%, matching or exceeding several leading models. Consequently, our models emerge as an excellent candidate for real-world applications.</p><p>• AI2 Reasoning Challenge (0-shot) • AI2 Reasoning Easy (0-shot)</p><p>• HellaSwag (0-shot)</p><p>• PIQA (0-shot)</p><p>• Winogrande (0-shot) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Few-Shot Evaluation</head><p>Table <ref type="table" target="#tab_2">3</ref> presents our zero-shot evaluation results across multiple benchmark tasks. The tasks and their few-show settings are listed below. Thanks to its rigorous and high-quality training corpus, our model demonstrates a remarkable competitive edge in tasks that involve language understanding and knowledge application. Our Moxin-7B-original achieves superior performance than LLaMA2-7B in this scenario. After training with the training data of evaluation tasks, our Moxin-7B-finetuned can achieve competitive performance compared with SOTA baselines.</p><p>Consequently, our models emerge as an excellent choice for a multitude of real-world applications where the reliance on robust language comprehension and extensive knowledge is paramount.</p><p>• AI2 Reasoning Challenge (25-shot)</p><p>• HellaSwag (10-shot)</p><p>• MMLU (5-shot)</p><p>• Winogrande (5-shot) We evaluate the alignment performance on MTBench <ref type="bibr" target="#b79">[80]</ref>.</p><p>It is a two-round conversation dataset with 80 questions. It covers eight dimensions (reasoning, roleplay, math, coding, writing, humanities, STEM, and information extraction) with 10 questions for each dimension. The model needs to answer the first question and then refine its previous response following additional specific instructions. We use GPT-4 as a model to provide scores (between 1-10) for the quality of responses. Our Moxin-7B-chat achieves superior performance on MTbench compared with baselines, as shown in Table <ref type="table">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Generation Examples</head><p>We demonstrate some text generation examples from our Moxin Chat 7B and Llama2 Chat 7B in Table <ref type="table" target="#tab_3">5</ref> and <ref type="table">6</ref>.</p><p>Example 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Moxin Chat 7B</head><p>Can you explain the concept of regularization in machine learning?</p><p>Regularization is a technique used in machine learning to prevent overfitting by introducing a penalty term in the loss function. The idea behind regularization is to encourage simpler models that are easier to interpret and generalize better to new data.</p><p>There are two main types of regularization: 1. L1 regularization: also known as Lasso regularization, this method adds a penalty term to the loss function that is proportional to the absolute value of the coefficients in the model. This encourages sparsity, meaning that some coefficients may be exactly zero, which simplifies the model and makes it easier to interpret. 2. L2 regularization: also known as Ridge regularization, this method adds a penalty term to the loss function that is proportional to the square of the coefficients in the model. This encourages shrinkage, meaning that the coefficients are pulled closer to zero, which simplifies the model and reduces the variance of the model estimates.</p><p>Regularization can be used in various machine learning models, including linear regression, logistic regression, and neural networks. It is often used in conjunction with other techniques such as crossvalidation to find the optimal hyperparameters that balance the bias-variance tradeoff and prevent overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Llama2 Chat 7B</head><p>Can you explain the concept of regularization in machine learning?</p><p>Regularization is a technique used in machine learning to prevent overfitting. It involves adding a penalty term to the loss function to discourage large values of the model's weights.</p><p>There are several types of regularization, including: 1. L1 regularization (Lasso): This adds a penalty term to the loss function that is proportional to the absolute value of the model's weights. This encourages the model to have smaller weights, which can help prevent overfitting. 2. L2 regularization (Ridge): This adds a penalty term to the loss function that is proportional to the square of the model's weights. This also encourages the model to have smaller weights, but it can lead to some weights being set to zero, which can result in a simpler model. Regularization can be applied to different layers of a neural network, such as the input layer, hidden layers, or output layer. It can also be applied to different parts of a model, such as the weights, biases, or activation functions. The main idea behind regularization is to add a penalty term to the loss function that discourages large values of the model's weights. This helps to prevent overfitting by encouraging the model to find a simpler solution that generalizes better to new data.</p><p>Regularization can be used in conjunction with other techniques, such as cross-validation and grid search, to improve the performance of a machine learning model. In summary, regularization is a technique used in machine learning to prevent overfitting by adding a penalty term to the loss function. There are several types of regularization, including L1, L2, dropout, and early stopping, and it can be applied to different layers and parts of a model. Regularization helps to find a simpler solution that generalizes better to new data and can be used in conjunction with other techniques to improve the performance of a machine learning model. Normalization is a technique used to scale the data before using it for training a machine learning model. It is used to make the data more uniform, and to prevent it from being skewed by large or small values.</p><p>There are several ways to normalize data, including min-max scaling, z-score normalization, and logarithmic scaling. In min-max scaling, the values are scaled to a range between 0 and 1, by dividing them by the maximum in the dataset. In z-score normalization, the values are scaled to have a mean of 0 and a standard deviation of 1, by subtracting the mean from each value and dividing by the standard deviation. In logarithmic scaling, the values are transformed using the logarithm function, which can be useful for normalizing data with a wide range of values.</p><p>Normalization is an important step in preparing data for machine learning, as it can help to improve the performance of the model by making the data more uniform and easier to analyze. It can also help to prevent overfitting and improve generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Llama2 Chat 7B</head><p>Can you explain the concept of normalization in machine learning?</p><p>Normalization is a technique used in machine learning to transform input data into a range of values that are suitable for use in a model. The goal of normalization is to ensure that the data is on the same scale, so that the model can learn meaningful relationships between the features.</p><p>There are several types of normalization techniques, including: 1. Min-max normalization: This involves scaling the data so that it falls within a specific range, usually between 0 and 1. 2. Standardization: This involves subtracting the mean and dividing by the standard deviation of the data, so that the data has a mean of 0 and a standard deviation of 1.</p><p>3. Z-score normalization: This involves subtracting the mean and dividing by the standard deviation of the data, so that the data has a mean of 0 and a standard deviation of 1. 4. Log transformation: This involves taking the logarithm of the data, which can help to reduce the effect of extreme values in the data. 5. Square root transformation: This involves taking the square root of the data, which can help to reduce the effect of outliers in the data.</p><p>Normalization is an important preprocessing step in machine learning, as it can help to improve the performance of the model by reducing the effect of irrelevant features and improving the interpretability of the results.</p><p>Here are some key points to keep in mind when normalizing data for machine learning: 1. Normalization can improve the performance of the model by reducing the effect of irrelevant features and improving the interpretability of the results. 8. Normalization can be applied to different features of the data, such as the input features, the target variable, or the output features. 9. Normalization can be used in combination with other preprocessing techniques, such as feature scaling, feature selection, and data augmentation. 10. Normalization is a powerful tool for improving the performance of machine learning models, but it is important to carefully evaluate the performance of the normalized data and consider the trade-offs between different normalization techniques.</p><p>Table 6: Generation Examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>The field of Large Language Models has witnessed a significant shift toward open-source development, fostering innovation within the AI community. However, a critical challenge emerges: many purportedly open-source models withhold essential components necessary for full understanding and reproducibility, creating barriers that limit both academic advancement and commercial adoption. This does not not only hamper scientific progress but also prevent businesses from fully leveraging these models for innovative applications, ultimately diminishing potential societal benefits and economic value creation. To address these limitations, we introduce Moxin 7B, a fully open-source language model developed in accordance with the Model Openness Framework (MOF), providing comprehensive access to pre-training code, configurations, training and fine-tuning datasets, and all intermediate checkpoints. Our evaluation results demonstrate that the Moxin 7B achieves superior zero-shot evaluation results compared to popular 7B models while maintaining competitive few-shot capabilities. We wish to see more work that establishes new standard for reproducible research in language model development, fostering a more inclusive and economically vibrant AI ecosystem.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>3 .</head><label>3</label><figDesc>Dropout regularization: This is a type of regularization that is applied during training. It involves randomly setting a fraction of the model's neurons to zero during each training iteration. This helps to prevent overfitting by making the model less reliant on any single neuron. 4. Early stopping: This is a type of regularization that is applied during training. It involves monitoring the validation loss during training and stopping the training process when the validation loss stops improving. This helps to prevent overfitting by stopping the training process before the model has a chance to fit the noise in the training data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>2 .</head><label>2</label><figDesc>There are several types of normalization techniques, including min-max normalization, standardization, z-score normalization, log transformation, and square root transformation. 3. The choice of normalization technique will depend on the nature of the data and the specific requirements of the model. 4. Normalization can be applied to both continuous and categorical data. 5. It is important to normalize the data before training the model, as normalization can improve the convergence of the model and reduce the risk of overfitting. 6. Normalization can also be used to improve the interpretability of the results by reducing the effect of irrelevant features and improving the scalability of the model. 7. It is important to carefully evaluate the performance of the normalized data and consider the trade-offs between different normalization techniques.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Parameter setting.</figDesc><table><row><cell>Parameter</cell><cell>Value</cell></row><row><cell>n_layers</cell><cell>36</cell></row><row><cell>dim</cell><cell>4096</cell></row><row><cell>head_dim</cell><cell>128</cell></row><row><cell cols="2">hidden_dim 14336</cell></row><row><cell>n_heads</cell><cell>32</cell></row><row><cell cols="2">n_kv_heads 8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison for various models in zero-shot evaluation.</figDesc><table><row><cell>Models</cell><cell cols="5">HellaSwag WinoGrade PIQA ARC-E ARC-C</cell><cell>Ave</cell></row><row><cell>Mistral -7B</cell><cell>80.39</cell><cell>73.4</cell><cell>82.15</cell><cell>78.28</cell><cell>52.22</cell><cell>73.29</cell></row><row><cell>LLaMA 2 -7B</cell><cell>75.99</cell><cell>69.06</cell><cell>79.11</cell><cell>74.54</cell><cell>46.42</cell><cell>69.02</cell></row><row><cell>LLaMA 2 -13B</cell><cell>79.37</cell><cell>72.22</cell><cell>80.52</cell><cell>77.4</cell><cell>49.06</cell><cell>71.71</cell></row><row><cell>LLaMA 3.1 -8B</cell><cell>78.92</cell><cell>74.19</cell><cell>81.12</cell><cell>81.06</cell><cell>53.67</cell><cell>73.79</cell></row><row><cell>gemma -7b</cell><cell>80.45</cell><cell>73.72</cell><cell>80.9</cell><cell>79.97</cell><cell>54.1</cell><cell>73.83</cell></row><row><cell>Qwen v2 -7B</cell><cell>78.9</cell><cell>72.38</cell><cell>79.98</cell><cell>74.71</cell><cell>50.09</cell><cell>71.21</cell></row><row><cell>internlm2.5 -7b</cell><cell>79.14</cell><cell>77.9</cell><cell>80.52</cell><cell>76.16</cell><cell>51.37</cell><cell>73.02</cell></row><row><cell>Baichuan2 -7B</cell><cell>72.25</cell><cell>67.17</cell><cell>77.26</cell><cell>72.98</cell><cell>42.15</cell><cell>66.36</cell></row><row><cell>Yi-1.5-9B</cell><cell>77.86</cell><cell>73.01</cell><cell>80.74</cell><cell>79.04</cell><cell>55.03</cell><cell>73.14</cell></row><row><cell>deepseek -7B</cell><cell>76.13</cell><cell>69.77</cell><cell>79.76</cell><cell>71.04</cell><cell>44.8</cell><cell>68.3</cell></row><row><cell>Moxin -7B -original</cell><cell>72.06</cell><cell>66.31</cell><cell>78.07</cell><cell>71.47</cell><cell>48.15</cell><cell>67.21</cell></row><row><cell>Moxin -7B -finetune</cell><cell>80.03</cell><cell>75.17</cell><cell>82.24</cell><cell>81.12</cell><cell>58.64</cell><cell>75.44</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison for various models in few-shot evaluation.</figDesc><table><row><cell>model</cell><cell cols="4">ARC-C hellaswag mmlu WinoGrade</cell><cell>Ave</cell></row><row><cell>Mistral -7B</cell><cell>57.59</cell><cell>83.25</cell><cell>62.42</cell><cell>78.77</cell><cell>70.51</cell></row><row><cell>LLaMA 3.1 -8B</cell><cell>54.61</cell><cell>81.95</cell><cell>65.16</cell><cell>77.35</cell><cell>69.77</cell></row><row><cell>LLaMA 3 -8B</cell><cell>55.46</cell><cell>82.09</cell><cell>65.29</cell><cell>77.82</cell><cell>70.17</cell></row><row><cell>LLaMA 2 -7B</cell><cell>49.74</cell><cell>78.94</cell><cell>45.89</cell><cell>74.27</cell><cell>62.21</cell></row><row><cell>Qwen 2 -7B</cell><cell>57.68</cell><cell>80.76</cell><cell>70.42</cell><cell>77.43</cell><cell>71.57</cell></row><row><cell>gemma -7B</cell><cell>56.48</cell><cell>82.31</cell><cell>63.02</cell><cell>78.3</cell><cell>70.03</cell></row><row><cell>internlm2.5 -7B</cell><cell>54.78</cell><cell>79.7</cell><cell>68.17</cell><cell>80.9</cell><cell>70.89</cell></row><row><cell>Baichuan2 -7B</cell><cell>47.87</cell><cell>73.89</cell><cell>54.13</cell><cell>70.8</cell><cell>61.67</cell></row><row><cell>Yi-1.5-9B</cell><cell>58.36</cell><cell>80.36</cell><cell>69.54</cell><cell>77.53</cell><cell>71.48</cell></row><row><cell>Moxin -7B -original</cell><cell>53.75</cell><cell>75.46</cell><cell>59.43</cell><cell>70.32</cell><cell>64.74</cell></row><row><cell>Moxin -7B -finetuned</cell><cell>59.47</cell><cell>83.08</cell><cell>60.97</cell><cell>78.69</cell><cell>70.55</cell></row><row><cell>4.3 Alignment Evaluation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Generation Examples.</figDesc><table><row><cell>Example 2</cell></row><row><cell>Can you explain the concept of normalization in machine learning?</cell></row><row><cell>Moxin Chat 7B</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://huggingface.co/datasets/cerebras/SlimPajama-627B</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://huggingface.co/datasets/mlfoundations/dclm-baseline-1.0</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://huggingface.co/datasets/bigcode/the-stack-dedup</p></note>
		</body>
		<back>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p><ref type="url" target="https://github.com/moxin-org/Moxin-LLM">https://github.com/moxin-org/Moxin-LLM</ref> Base model: <ref type="url" target="https://huggingface.co/moxin-org/moxin-llm-7b">https://huggingface.co/moxin-org/moxin-llm-7b</ref> Chat model: <ref type="url" target="https://huggingface.co/moxin-org/moxin-chat-7b">https://huggingface.co/moxin-org/moxin-chat-7b</ref> </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Josh</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lama</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilge</forename><surname>Akkaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florencia</forename><surname>Leoni Aleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janko</forename><surname>Altenschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Altman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.08774</idno>
		<title level="m">Shyamal Anadkat, et al. Gpt-4 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The claude 3 model family: Opus, sonnet</title>
		<author>
			<persName><surname>Anthropic</surname></persName>
		</author>
		<ptr target="https://www-cdn.anthropic.com/de" />
		<imprint>
			<date>8ba9b01c9ab7cbabf5c33b80b7bbc618857627/ Model_Card_Claude_3.pdf</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Gemini</forename><surname>Team</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johan</forename><surname>Schalkwyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anja</forename><surname>Hauth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.11805</idno>
		<title level="m">a family of highly capable multimodal models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Abhimanyu</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Jauhri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kadian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Al-Dahle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aiesha</forename><surname>Letman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akhil</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Schelten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.21783</idno>
		<title level="m">The llama 3 herd of models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Prest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Alain</forename><surname>Fouque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Hoffstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Kirchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vadim</forename><surname>Lyubashevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Pornin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Ricosset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregor</forename><surname>Seiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Whyte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Falcon</surname></persName>
		</author>
		<title level="m">Post-Quantum Cryptography Project of NIST</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Albert Q Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devendra</forename><surname>Bamford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Singh Chaplot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>De Las Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gianna</forename><surname>Bressand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lengyel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucile</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><surname>Saulnier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.06825</idno>
		<title level="m">Mistral 7b</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Klyman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sayash</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nestor</forename><surname>Maslej</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Betty</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.12941</idno>
		<title level="m">The foundation model transparency index</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">On the societal impact of open foundation models</title>
		<author>
			<persName><forename type="first">Sayash</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Klyman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashwin</forename><surname>Ramaswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Cihon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aspen</forename><surname>Hopkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Bankston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miranda</forename><surname>Bogen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.07918</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The model openness framework: Promoting completeness and openness for reproducibility, transparency and usability in ai</title>
		<author>
			<persName><forename type="first">Matt</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ibrahim</forename><surname>Haddad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cailean</forename><surname>Osborne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Abdelmonsef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sachin</forename><surname>Varghese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.13784</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Jinze</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfei</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbin</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.16609</idno>
		<title level="m">Qwen technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">An</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baosong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binyuan</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dayiheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.10671</idno>
		<title level="m">Qwen2 technical report</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Opher</forename><surname>Lieber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barak</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hofit</forename><surname>Bata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gal</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jhonathan</forename><surname>Osin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itay</forename><surname>Dalmedigos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erez</forename><surname>Safahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaked</forename><surname>Meirom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shai</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.19887</idno>
		<title level="m">A hybrid transformer-mamba language model</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Jamba</forename><surname>Team</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barak</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Arazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avshalom</forename><surname>Manevich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barak</forename><surname>Peleg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Aviram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Almagor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Fridman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Padnos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.12570</idno>
		<title level="m">Jamba-1.5: Hybrid transformermamba models at scale</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.07909</idno>
		<title level="m">Neural machine translation of rare words with subword units</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">OpenAI Team. tiktoken</orgName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</title>
		<author>
			<persName><surname>Kudo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06226</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08237</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Hugging Face Team. Summary of the tokenizers</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context</title>
		<author>
			<persName><forename type="first">Machel</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Savinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Teplyashin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeanbaptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.05530</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Video-chatgpt: Towards detailed video understanding via large vision and language models</title>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Maaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanoona</forename><surname>Rasheed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.05424</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Video-llama: An instruction-tuned audio-visual language model for video understanding</title>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.02858</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Mmllms: Recent advances in multimodal large language models</title>
		<author>
			<persName><forename type="first">Duzhen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yahan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenxing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahua</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenhui</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.13601</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Albert Q Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blanche</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Savary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devendra</forename><surname>Bamford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Singh Chaplot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emma</forename><forename type="middle">Bou</forename><surname>De Las Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName><surname>Bressand</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.04088</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">Mixtral of experts. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><surname>Agarwal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Guilherme</forename><surname>Penedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Malartic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Hesslow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruxandra</forename><surname>Cojocaru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Cappelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamza</forename><surname>Alobeidli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Pannier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ebtesam</forename><surname>Almazrouei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Launay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.01116</idno>
		<title level="m">The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Jack W Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Aslanides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susannah</forename><surname>Ring</surname></persName>
		</author>
		<author>
			<persName><surname>Young</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.11446</idno>
		<title level="m">Scaling language models: Methods, analysis &amp; insights from training gopher</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><surname>Ccnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.00359</idno>
		<title level="m">Extracting high quality monolingual datasets from web crawl data</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<idno type="arXiv">arXiv:2010.11934</idno>
		<title level="m">L Xue. mt5: A massively multilingual pre-trained text-to-text transformer</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
	<note>Cross-lingual language model pretraining</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiming</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrique</forename><surname>Ponde De Oliveira Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harri</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Brockman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.03374</idno>
		<title level="m">Evaluating large language models trained on code</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Travis</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anish</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noa</forename><surname>Nabeshima</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00027</idno>
		<title level="m">The pile: An 800gb dataset of diverse text for language modeling</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Noveen</forename><surname>Sachdeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Coleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang-Cheng</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmo</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Caverlee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><forename type="middle">Zhiyuan</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.09668</idno>
		<title level="m">How to train data-efficient llms</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">A pretrainer&apos;s guide to training data: Measuring the effects of data age, domain coverage, quality, &amp; toxicity</title>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Yauney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Reif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.13169</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Glam: Efficient scaling of language models with mixture-of-experts</title>
		<author>
			<persName><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5547" to="5569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Ippolito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Nystrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.06499</idno>
		<title level="m">Deduplicating training data makes language models better</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Url normalization for de-duplication of web pages</title>
		<author>
			<persName><forename type="first">Amit</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hema</forename><surname>Swetha Koppula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><forename type="middle">P</forename><surname>Leela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><surname>Prasad Chitrapura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sachin</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavan</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Chittaranjan</forename><surname>Haty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirban</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Sasturkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM conference on information and knowledge management</title>
		<meeting>the 18th ACM conference on information and knowledge management</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1987" to="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Datacomp-lm: In search of the next generation of training sets for language models</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Smyrnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maor</forename><surname>Ivgi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samir</forename><surname>Gadre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hritik</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etash</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sedrick</forename><surname>Keh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kushal</forename><surname>Arora</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.11794</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Efficient online data mixing for language model pre-training</title>
		<author>
			<persName><forename type="first">Alon</forename><surname>Albalak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangming</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">R0-FoMo: Robustness of Few-shot and Zero-shot Learning in Large Foundation Models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianhua</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Willie</forename><surname>Neiswanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengzhong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Hestness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Vassilieva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daria</forename><surname>Soboleva</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.10818</idno>
		<title level="m">Slimpajama-dc: Understanding data combinations for llm training</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Gemma</forename><surname>Team</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Mesnard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cassidy</forename><surname>Hardin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Dadashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Bhupatiraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shreya</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgane</forename><surname>Rivière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihir</forename><surname>Sanjay Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juliette</forename><surname>Love</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.08295</idno>
		<title level="m">Open models based on gemini research and technology</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Palm: Scaling language modeling with pathways</title>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">240</biblScope>
			<biblScope unit="page" from="1" to="113" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Luca</forename><surname>Soldaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodney</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshita</forename><surname>Bhagia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><surname>Authur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Bogin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khyathi</forename><surname>Chandu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Dumas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanai</forename><surname>Elazar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.00159</idno>
		<title level="m">An open corpus of three trillion tokens for language model pretraining research</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Guilherme</forename><surname>Penedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hynek</forename><surname>Kydlíček</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Lozhkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leandro</forename><surname>Von Werra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.17557</idno>
		<title level="m">The fineweb datasets: Decanting the web for the finest text data at scale</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Llm-datasets: An open framework for pretraining datasets of large language models</title>
		<author>
			<persName><forename type="first">Malte</forename><surname>Ostendorff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><forename type="middle">Ortiz</forename><surname>Suarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Fonseca Lage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Rehm</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Anton</forename><surname>Lozhkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loubna</forename><surname>Ben Allal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Cassano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Lamy-Poirier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nouamane</forename><surname>Tazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmytro</forename><surname>Pykhtar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.19173</idno>
		<title level="m">Starcoder 2 and the stack v2: The next generation</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Code llama: Open foundation models for code</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Baptiste Roziere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sten</forename><surname>Gloeckle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itai</forename><surname>Sootla</surname></persName>
		</author>
		<author>
			<persName><surname>Gat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellen</forename><surname>Xiaoqing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yossi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingyu</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romain</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Sauvestre</surname></persName>
		</author>
		<author>
			<persName><surname>Remez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.12950</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Gqa: Training generalized multi-query transformer models from multi-head checkpoints</title>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Lee-Thorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michiel</forename><surname>De Jong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yury</forename><surname>Zemlyanskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Lebrón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Sanghai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.13245</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><surname>Longformer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<title level="m">The long-document transformer</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">SlimPajama: A 627B token cleaned and deduplicated version of RedPajama</title>
		<author>
			<persName><forename type="first">Daria</forename><surname>Soboleva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Al-Khateeb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><forename type="middle">R</forename><surname>Steeves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Hestness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nolan</forename><surname>Dey</surname></persName>
		</author>
		<ptr target="https://cerebras.ai/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Llama 2: Open foundation and fine-tuned chat models</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasmine</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prajjwal</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.09288</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">Maurice</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Oren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Alexandrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaozhong</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huu</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaozhe</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Virginia</forename><surname>Adams</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2411.12372</idno>
		<title level="m">Redpajama: an open dataset for training large language models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Semdedup: Data-efficient learning at web-scale through semantic deduplication</title>
		<author>
			<persName><forename type="first">Amro</forename><surname>Abbas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kushal</forename><surname>Tirumala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dániel</forename><surname>Simig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><forename type="middle">S</forename><surname>Morcos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.09540</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Large-scale near-deduplication behind bigcode</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">The curious case of neural text degeneration</title>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09751</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Mining of massive datasets</title>
		<author>
			<persName><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><surname>Rajaraman</surname></persName>
		</author>
		<author>
			<persName><surname>Ullman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>cambridge university press</publisher>
			<pubPlace>cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Getting structured data from the internet: running web crawlers/scrapers on a big data production scale</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><surname>Patel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="277" to="324" />
		</imprint>
	</monogr>
	<note>Introduction to common crawl datasets</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">On the resemblance and containment of documents</title>
		<author>
			<persName><forename type="first">Andrei</forename><forename type="middle">Z</forename><surname>Broder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. Compression and Complexity of SEQUENCES 1997 (Cat. No. 97TB100171)</title>
		<meeting>Compression and Complexity of SEQUENCES 1997 (Cat. No. 97TB100171)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title/>
		<author>
			<persName><surname>Fineweb</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Color-filter: Conditional loss reduction filtering for targeted language model pretraining</title>
		<author>
			<persName><forename type="first">David</forename><surname>Brandfonbrener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanlin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Kirsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">Richard</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sham</forename><surname>Kakade</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.10670</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<author>
			<persName><forename type="first">Alex</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albin Madappally</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.17425</idno>
		<title level="m">Data filtering networks</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Groeneveld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pete</forename><surname>Walsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshita</forename><surname>Bhagia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodney</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananya</forename><surname>Harsh Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamish</forename><surname>Ivison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Magnusson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.00838</idno>
		<title level="m">Accelerating the science of language models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">The stack: 3 tb of permissively licensed source code</title>
		<author>
			<persName><forename type="first">Denis</forename><surname>Kocetkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loubna</forename><surname>Ben Allal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenghao</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Muñoz Ferrandis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.15533</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">GLM-130b: An open bilingual pre-trained model</title>
		<author>
			<persName><forename type="first">Aohan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanyu</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wendi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weng</forename><surname>Lam Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zixuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jidong</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenguang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<author>
			<persName><forename type="first">Jupinder</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shrimai</forename><surname>Prabhumoye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Jennings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aastha</forename><surname>Jhunjhunwala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.16819</idno>
		<title level="m">Ayush Dattagupta, et al. Nemotron-4 15b technical report</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Measuring massive multitask language understanding</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Collin</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Hellaswag: Can a machine really finish your sentence</title>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.07830</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Colossal-ai: A unified deep learning system for large-scale parallel training</title>
		<author>
			<persName><forename type="first">Shenggui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengda</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiarui</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haichen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boxiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd International Conference on Parallel Processing</title>
		<meeting>the 52nd International Conference on Parallel Processing</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="766" to="775" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<author>
			<persName><surname>Loshchilov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m">Decoupled weight decay regularization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Camels in a changing climate: Enhancing lm adaptation with tulu 2</title>
		<author>
			<persName><forename type="first">Hamish</forename><surname>Ivison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentina</forename><surname>Pyatkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.10702</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Lm evaluation harness</title>
		<imprint>
			<date type="published" when="2024">2024. Summer 2024</date>
			<publisher>LM Evaluation Harness Team</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Open Compass Team. Open compass</title>
		<imprint>
			<date type="published" when="2024">2024. Summer 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Autogptq: An user-friendly llms quantization package</title>
		<author>
			<persName><forename type="first">Autogptq</forename><surname>Team</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024. Spring 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hallahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Leahy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Mcdonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.06745</idno>
		<title level="m">Gpt-neox-20b: An open-source autoregressive language model</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Deepspeed4science initiative: Enabling large-scale scientific discovery through sophisticated ai system technologies</title>
		<author>
			<persName><forename type="first">Leon</forename><surname>Shuaiwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bonnie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjia</forename><surname>Kruft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Conglong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masahiro</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxia</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ammar</forename><surname>Rasley</surname></persName>
		</author>
		<author>
			<persName><surname>Ahmad Awan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.04610</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Think you have solved question answering? try arc, the ai2 reasoning challenge</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isaac</forename><surname>Cowhey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carissa</forename><surname>Schoenick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05457v1</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<author>
			<persName><forename type="first">Teun</forename><surname>Van Der Weij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hofstätter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ollie</forename><surname>Jaffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Samuel F Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ward</forename><surname>Rhys</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.07358</idno>
		<title level="m">Ai sandbagging: Language models can strategically underperform on evaluations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Winogrande: An adversarial winograd schema challenge at scale</title>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="99" to="106" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><surname>Piqa</surname></persName>
		</author>
		<title level="m">Reasoning about physical commonsense in natural language</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Judging llm-as-a-judge with mt-bench and chatbot arena</title>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghao</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="46595" to="46623" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
