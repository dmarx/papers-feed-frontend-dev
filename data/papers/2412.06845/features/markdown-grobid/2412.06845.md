# Fully Open Source Moxin-LLM Technical Report

## Abstract

## 

Recently, Large Language Models (LLMs) have undergone a significant transformation, marked by a rapid rise in both their popularity and capabilities. Leading this evolution are proprietary LLMs like GPT-4 and GPT-o1, which have captured widespread attention in the AI community due to their remarkable performance and versatility. Simultaneously, open-source LLMs, such as LLaMA and Mistral, have made great contributions to the ever-increasing popularity of LLMs due to the ease to customize and deploy the models across diverse applications. Although open-source LLMs present unprecedented opportunities for innovation and research, the commercialization of LLMs has raised concerns about transparency, reproducibility, and safety. Many open-source LLMs fail to meet fundamental transparency requirements by withholding essential components like training code and data, and some use restrictive licenses whilst claiming to be "open-source," which may hinder further innovations on LLMs. To mitigate this issue, we introduce Moxin 7B, a fully open-source LLM developed in accordance with the Model Openness Framework (MOF), a ranked classification system that evaluates AI models based on model completeness and openness, adhering to principles of open science, open source, open data, and open access. Our model achieves the highest MOF classification level of "open science" through the comprehensive release of pre-training code and configurations, training and fine-tuning datasets, and intermediate and final checkpoints. Experiments show that our model achieves superior performance in zero-shot evaluation compared with popular 7B models and performs competitively in few-shot evaluation.

## Introduction

The field of natural language processing has witnessed the most exciting discoveries of the last ten years with the emergence of large language models (LLMs). At the forefront of this evolution are LLMs such as GPT-4 [[1]](#b0), Claude [[2]](#b1), and Gemini [[3]](#b2), which have captured the attention of the AI community due to their performance and versatility. Meanwhile, the recent emergence of openly accessible yet highly capable LLMs such as LLaMA [[4]](#b3), Falcon [[5]](#b4), and Mistral [[6]](#b5) allow researchers and practitioners to easily obtain, customize, and deploy LLMs in more various environments and for more diverse use cases. The trends have made people eagerly asking about what's next and some suggest "a general intelligence" is right around the corner.

Despite the growing influence and accessibility of open-source LLMs, a notable challenge emerged: many model producers restrict visibility and access to their training, fine-tuning, and evaluation processes, including crucial components such as their training code and data [[7]](#b6). Some model producers even use restrictive licenses whilst claiming to be "open-source." This practice creates barriers for the broader AI research community to study, replicate, and innovate upon advanced LLMs. In parallel, it prevents businesses from fully leveraging open-source models for innovative industrial applications, as its commercialization has raised concerns about transparency, reproducibility, and safety.

To unlock the full potential of LLMs and open innovation, we must return to democratize this research by putting the model into the hands of more researchers and making the datasets the models train on fully open-source. This requires moving beyond the simple sharing of model weights to embrace complete transparency in training, datasets, and implementation detail, which is crucial for fostering a more inclusive and collaborative research environment that can sustain a healthy open-source ecosystem [[8]](#b7).

To achieve this goal, we introduce Moxin 7B, a fully open-source LLM developed by complying with the Model Openness Framework (MOF) introduced by [[9]](#b8). The MOF provides a systematic ranking classification system to rate AI models based on their completeness and openness, incorporating the principles of open science, open source, open data, and open access. By promoting transparency and reproducibility, the MOF serves as a crucial tool to combat "openwashing" practices and establishes completeness and openness as primary criteria alongside the core tenets of responsible AI. Wide adoption of the MOF will cultivate a more open AI ecosystem, benefiting research, innovation, and adoption of state-of-the-art models.

Our open-source LLM has released pre-training code and configurations, training and fine-tuning data, and intermediate and final checkpoints, aiming to make continuous commitments to fully open-source LLMs. Our model achieves the highest MOF classification level of "open science." It is noteworthy that this commitment to openness has not compromised performance: our base model achieves superior performance in zero-shot evaluation compared with popular 7B models and performs competitively in few-shot evaluation. Remarkably, our chat model can outperform 7B baselines like Llama2-7B-chat. Our homepage is [https://github.com/moxin-org/Moxin-LLM](https://github.com/moxin-org/Moxin-LLM).

## Related Work

## Models, Tokenizers, and Training

Models. State-of-the-art large language models (LLMs) typically comprise a substantial number of parameters, often approaching or exceeding 100 billion [[4,](#b3)[1,](#b0)[3]](#b2). To facilitate broader accessibility, smaller models with fewer than 20 billion parameters, and even those around 7 billion parameters, have been developed [[10,](#b9)[11,](#b10)[4,](#b3)[6,](#b5)[12,](#b11)[13]](#b12). In addition, efficiency-enhancing techniques, such as implementing MAMBA-based architectures in Jamba, have been employed to optimize performance [[12,](#b11)[13]](#b12).

Tokenizers. Tokenizers are essential to convert raw data into a suitable format for model processing. Many contemporary models employ Byte-Pair Encoding (BPE) [[14]](#b13), with OpenAI's tiktoken tokenizer [[15]](#b14) being a notable implementation. However, for languages that handle tokens differently from Romance languages, alternatives such as SentencePiece [[16]](#b15) are utilized, as seen in XLNet [[17]](#b16). Hugging Face offers an excellent summary of state-of-the-art tokenizers with practical examples [[18]](#b17). Moreover, tokenization extends beyond text modalities; many foundational models now include multimodal capabilities, processing documents, audio, images, and even videos [[19,](#b18)[20,](#b19)[21,](#b20)[22]](#b21).

Training. To enhance the performance of smaller models beyond their inherent limitations, various training strategies can be employed. A notable example is the application of Mixture of Experts (MoE) training, which has achieved significant success in models like Mixtral [[23]](#b22).

## Data curation methods

Researchers commonly collect large datasets for training language models (LMs) [[24]](#b23) by performing web crawls. However, these datasets often contain undesirable content, necessitating data curation to improve their quality. To enhance model performance [[25,](#b24)[26,](#b25)[24,](#b23)[27]](#b26), several data curation techniques are widely employed. These include filtering by language [[28,](#b27)[29,](#b28)[30]](#b29), heuristic-based filtering [[25,](#b24)[31,](#b30)[32]](#b31), quality filtering [[33,](#b32)[34,](#b33)[35]](#b34), data deduplication [[36,](#b35)[37]](#b36), and data mixing [[38,](#b37)[39,](#b38)[40]](#b39).

## Open-source datasets

As the scale of LMs has increased in recent years [[4,](#b3)[41,](#b40)[42,](#b41)[1]](#b0), the community has correspondingly curated larger datasets to support their training. Early datasets include the C4 dataset, containing 160 billion tokens, and The Pile [[32]](#b31), which comprises 300 billion tokens. More recently, even larger datasets have been introduced: RefinedWeb [[25]](#b24) with 600 billion tokens, Dolma [[43]](#b42) with 3 trillion tokens, FineWeb [[44]](#b43) with 15 trillion tokens, and RedPajama-v2 [[45]](#b44) containing 30 trillion tokens. In addition to these general-purpose datasets, large domain-specific datasets have also been developed. For instance, StackV2 [[46]](#b45), a code-focused dataset, includes 900 billion tokens, and FineWeb-Edu [[44]](#b43), a high-quality filtered educational text dataset, contains 1.3 trillion tokens.

## Model Training

## Model Architecture

We opt to extend the Mistral model architecture [[6]](#b5) due to its ability to achieve high performance while maintaining efficient inference speeds. The original Mistral 7B model demonstrates superior performance compared to multiple 7B language models and even outperforms larger models on various evaluation benchmarks. Notably, it surpasses the LLaMA 34B model [[47]](#b46) in tasks such as mathematics and code generation. The original Mistral model leverages grouped-query attention (GQA) [[48]](#b47) and sliding window attention (SWA) [[49]](#b48). GQA reduces memory requirements during decoding, allowing for larger batch sizes and higher throughput, and it significantly accelerates inference speed-an essential factor in real-time applications. Meanwhile, SWA effectively handles long sequences without incurring substantial computational overhead. By incorporating these techniques, the model achieves significant improvements in performance and efficiency, which we have adopted in our extended model.

Building upon the original Mistral model, which consists of 32 blocks, we have extended the architecture to 36 blocks. Furthermore, we also employ GQA to partition the query heads into multiple groups, each sharing a single key head and value head. This approach interpolates between multi-query attention (MQA) and multi-head attention (MHA) in large language models, striking a balance between the computational speed of MQA and the representational quality of MHA, thereby providing a favorable trade-off. Additionally, our model incorporates a rolling buffer cache with a fixed attention span, effectively limiting cache size and preventing excessive memory usage when processing long sequences.

## Training Data

Data are fundamental to the pre-training of LLMs. Preparing such training data requires careful consideration of multiple challenges, including handling sensitive information, ensuring comprehensive knowledge coverage, and achieving higher efficiency with improved data quality.

In this section, we detail the processes of preparing textual data from general domains and coding data related to programming languages.

## Text Data

We use a mix of data from SlimPajama [[50]](#b49) and DCLM-BASELINE [[38]](#b37) as our text training data.

During the training of LLaMA, it was demonstrated that the performance of a 7B model continues to improve even after being trained on more than 1T tokens [[51]](#b50). Given the outstanding performance of LLaMA, its data collection methodology was rapidly replicated, leading to the release of RedPajama, an open-source dataset containing 1.2 trillion tokens [[52]](#b51).

However, subsequent analyses reveal a significant limitation: some corpora within RedPajama contain a large percentage of duplicate content. The deduplication guidelines in RedPajama operate only within individual data sources, leaving inter-source duplicates largely unaddressed. To improve data quality and training efficiency, SlimPajama[foot_0](#foot_0) was developed as a refined iteration of RedPajama, offering a cleaned and extensively deduplicated version [[50]](#b49).

SlimPajama implements a rigorous two-stage preprocessing pipeline to enhance data quality. In the first stage, short and low-quality documents are removed from RedPajama. Specifically, documents that have fewer than 200 characters after removing punctuation, space symbols, newlines, and tabs are filtered out, as these documents typically contain only metadata and lack useful information. As a result of this step, 1.86% of RedPajama documents are eliminated.

The second step involves removing duplicate data, as deduplication enhances training efficiency and reduces memorization, thereby decreasing the likelihood of generating text solely by recalling training data [[25,](#b24)[53,](#b52)[54,](#b53)[36,](#b35)[55]](#b54). To perform deduplication, document signatures are created using pre-processed, lower-cased 13-grams. Subsequently, MinHashLSH [[56]](#b55) is employed to identify and eliminate duplicates based on a Jaccard similarity threshold of 0.8. Deduplication is performed both within and across data sources. Overall, by pruning 49.6% of the bytes from the RedPajama dataset, the 627B-token SlimPajama dataset is obtained.

Additionally, we utilize the DCLM-BASELINE [[38]](#b37) dataset[foot_1](#foot_1) , which is derived from CommonCrawl, a web-crawled dataset [[57]](#b56). The construction of DCLM-BASELINE involves several steps. First, resiliparse is employed to extract text from CommonCrawl. Second, deduplication is performed using MinHash [[58]](#b57) within a suffix array pipeline [[59,](#b58)[36]](#b35) and near-duplicate Bloom filtering, which enhances the exact document and paragraph deduplication scheme [[43]](#b42). Third, recent studies [[60,](#b59)[43,](#b42)[61]](#b60) demonstrate that utilizing learnable models as quality filters leads to downstream performance improvements. Consequently, DCLM-BASELINE applies a fastText OH-2.5 combined with an ELI5 classifier score to retain the top 10% of documents.

## Coding Data

Programming is crucial for LLMs to support various downstream tasks, such as code completion from natural language descriptions, documentation generation for individual functions, and autocompletion of code snippets. Furthermore, as code is generally better structured and organized than natural language, training on code data may improve the LLM reasoning capabilities [[62]](#b61). Therefore, We use part of the-stack-dedup [[63]](#b62) dataset [3](#foot_2) during the pretraining.

The Stack comprises more than 6TB of permissively-licensed source code files across 358 programming languages [[63]](#b62). This carefully curated resource was designed to enhance the code generation capabilities of LLMs. It facilitates the synthesis of programs by code-generating AI systems from both natural language descriptions and existing code snippets.

To construct the Stack dataset, 220.92 million active GitHub repositories were collected from event archives published between 2015 and 2022 on GHArchive. Of these repositories, only 137.36 million were publicly accessible on GitHub, resulting in 51.76 billion downloaded files. After initial filtering, 5.28 billion unique files were identified, with an uncompressed size of 92.36 TB.

To ensure data quality, near-deduplication was implemented within the preprocessing pipeline in addition to exact deduplication. Specifically, MinHash with 256 permutations was computed for all documents, and Locality Sensitive Hashing was employed to identify clusters of duplicates. Within these clusters, Jaccard similarities were calculated to detect near-duplicates using a similarity threshold of 0.85. Approximately 40% of permissively licensed files were identified as (near-)duplicates and subsequently removed.

## Capability Enhancement

LLMs are expected to demonstrate capabilities such as reasoning, mathematical problem-solving, and knowledge memorizing. However, a significant challenge lies in that, in the pre-training process, high-quality capability-related data is sparsely distributed in the entire corpus, and thereby it is difficult for models to be proficient at these above-mentioned capabilities. Previous research, such as work on Qwen [[10]](#b9), GLM-130B [[64]](#b63), Nemotron-4 [[65]](#b64), has tried to incorporate instructionbased or high-quality data during the pre-training stage to enhance these abilities. In our study, we collect open-source data from HuggingFace, primarily utilizing the training datasets of various evaluation benchmarks such as MMLU [[66]](#b65) and HellaSwag [[67]](#b66). These data are used experimentally to investigate the relationship between high-quality, capability-focused training data and model performance.

## Training Configuration

The total number of tokens used for pre-training our Moxin-7B model is over 2T, and the pre-training process consists of three phases. In the first phase, we use pre-training corpora with the context length of 2k. In the second phase, we use pre-training corpora with the context length of 4k. In the third phase, we utilize the capability-specific enhancement data. We provide the model performance with only the first two phases and also with all three phases to validate the performance of the third phase.

We use Colossal-AI [[68]](#b67) as our training framework. Colossal-AI is a unified deep learning system that provides the fullest set of acceleration techniques for the AI community. With its modular design, ColossalAI allows for a free combination of these techniques to achieve the best training speedup. Colossal-AI's optimized parallelism and heterogeneous training methods are employed to achieve superior system performance compared to baseline systems. These methods are provided through user-friendly APIs, requiring minimal code modifications.

During training, AdamW [[69]](#b68) with β 1 = 0.9, β 2 = 0.95, ϵ = 1e -8 and weight decay = 0.1 is used to optimize the model. We use the cosine learning rate decay and the learning rate decays to 10% of its maximum. Learning Rate is set to 2e -6 .

## Alignment

Following the pre-training phase, we fine-tune the model into a helpful and harmless AI assistant. In our Alignment stage, we mainly use supervised fine-tuning (SFT), during which we fine-tune the model to follow diverse human instructions by high-quality instruction data. We use the Tulu v2 dataset [[70]](#b69) for instruction tuning. The dataset consists of a mix of FLAN, Open Assistant 1, ShareGPT, GPT4-Alpaca, LIMA, and so on.

## Long-Context

To deal with the long-context problem, our model leverages grouped-query attention (GQA) [[48]](#b47), sliding window attention (SWA) [[49]](#b48), and Rolling Buffer Cache [[6]](#b5). GQA reduces the memory requirement during decoding, allowing for higher batch sizes hence higher throughput.

Besides, SWA can handle longer sequences more effectively at a reduced computational cost, thereby alleviating a common limitation in LLMs. SWA exploits the stacked layers of a transformer to attend information beyond the window size W . At the last layer, with SWA, using a window size of W = 4096, we have a theoretical attention span of approximately 14K tokens or above.

Our model adopts Rolling Buffer Cache which limits the cache size using a rolling buffer cache with a fixed attention span. The cache has a fixed size of W , and the keys and values for the timestep i are stored in position i mod W of the cache. As a result, when the position i is larger than W , past values in the cache are overwritten, and the size of the cache stops increasing. On a sequence length of 32k tokens, this reduces the cache memory usage by 8×, without impacting the model quality.

With the above techniques, our model can support 32K context length with fast inference and low memory cost.

## Evaluation

We conducted comprehensive performance comparisons against leading language models of comparable scale, including Mistral-7B [[6]](#b5), LLaMA 2-7B [[51]](#b50), Gemma-7B [[41]](#b40), and Qwen v2-7B [[11]](#b10). These models were selected based on their demonstrated excellence within the 7B or 8B category and represent diverse development approaches from various research organizations worldwide. To ensure a robust evaluation, we re-run all benchmarks with the same evaluation pipeline for fair comparisons. Specifically, we use lm-evaluation-harness [[71]](#b70) and opencompass [[72]](#b71) for evaluation.

Lm-evaluation-harness provides a unified framework to test generative language models on a large number of different evaluation tasks. It supports over 60 standard academic benchmarks for LLMs, with hundreds of subtasks and variants implemented. This framework is versatile as it extends to models implemented through various architectures, including transformers (including quantization via AutoGPTQ [[73]](#b72)), GPT-NeoX [[74]](#b73), and Megatron-DeepSpeed [[75]](#b74), all unified through a flexible, tokenization-agnostic interface. The framework is reliable, as evidenced by serving as the backend for HuggingFace's popular Open LLM Leaderboard and being utilized by dozens of organizations, including NVIDIA, Cohere, BigScience, BigCode, Nous Research, and Mosaic ML.

To complement, we also employed openCompass. This framework performs an in-depth and holistic assessment of large language models structured around eight fundamental dimensions of language model capabilities: language comprehension, knowledge precision, logical deduction, creative ideation, mathematical problem-solving, programming proficiency, extended text analysis, and intelligent agent engagement.

## Evaluation Tasks

We evaluate the model performance on various tasks below.

• AI2 Reasoning Challenge (ARC) [[76]](#b75) -a set of genuine grade-school level, multiple-choice science questions, assembled to encourage research in advanced question-answering. The dataset is partitioned into a Challenge Set (ARC-C) and an Easy Set (ARC-E), where the former contains only questions answered incorrectly by both a retrieval-based algorithm and a word co-occurrence algorithm.

• HellaSwag [[67]](#b66) -a test of commonsense natural language inference, which is easy for humans ( 95%) but challenging for SOTA models. It consists of 70,000 multiple-choice questions. Each question presents a scenario followed by four possible outcomes, asking the model to select the most reasonable conclusion.

• MMLU [[77]](#b76) -a test to measure a text model's multitask accuracy. The test covers 57 tasks, including elementary mathematics, US history, computer science, law, etc.

• Winogrande [[78]](#b77) -an adversarial and difficult Winograd benchmark at scale, for commonsense reasoning. It contains 44,000 multiple-choice questions with two options each. It requires the model to choose the appropriate entity word for the pronoun in the descriptive text based on the scenario.

• PIQA [[79]](#b78) -the task of physical commonsense reasoning and a corresponding benchmark dataset Physical Interaction: Question Answering (PIQA). Physical commonsense knowledge is a major challenge on the road to true AI-completeness, including robots that interact with the world and understand natural language. PIQA focuses on everyday situations with a preference for atypical solutions.

## Evaluation Results

We name the initial model as Moxin-7B-original, which presents the foundation model before fine-tuning on the training data of the evaluation datasets. After subsequent partial fine-tuning of Moxin-7B-original on the training data of the evaluation datasets, we developed Moxin-7B-finetuned, enabling direct assessment of how targeted fine-tuning affects model performance.

## Zero-Shot Evaluation

We report the result of base models for zero-shot evaluation in Table [2](#tab_1). The tasks are listed below.

After training with the training data of evaluation tasks, our Moxin-7B-finetuned can achieve superior performance compared with state-of-the-art (SOTA) baselines. This significant increase from the base model demonstrates the effectiveness of our fine-tuning approach. The improved performance is particularly notable on complex reasoning tasks like PIQA, where the score increased from 78.07% to 82.24%, matching or exceeding several leading models. Consequently, our models emerge as an excellent candidate for real-world applications.

• AI2 Reasoning Challenge (0-shot) • AI2 Reasoning Easy (0-shot)

• HellaSwag (0-shot)

• PIQA (0-shot)

• Winogrande (0-shot) 

## Few-Shot Evaluation

Table [3](#tab_2) presents our zero-shot evaluation results across multiple benchmark tasks. The tasks and their few-show settings are listed below. Thanks to its rigorous and high-quality training corpus, our model demonstrates a remarkable competitive edge in tasks that involve language understanding and knowledge application. Our Moxin-7B-original achieves superior performance than LLaMA2-7B in this scenario. After training with the training data of evaluation tasks, our Moxin-7B-finetuned can achieve competitive performance compared with SOTA baselines.

Consequently, our models emerge as an excellent choice for a multitude of real-world applications where the reliance on robust language comprehension and extensive knowledge is paramount.

• AI2 Reasoning Challenge (25-shot)

• HellaSwag (10-shot)

• MMLU (5-shot)

• Winogrande (5-shot) We evaluate the alignment performance on MTBench [[80]](#b79).

It is a two-round conversation dataset with 80 questions. It covers eight dimensions (reasoning, roleplay, math, coding, writing, humanities, STEM, and information extraction) with 10 questions for each dimension. The model needs to answer the first question and then refine its previous response following additional specific instructions. We use GPT-4 as a model to provide scores (between 1-10) for the quality of responses. Our Moxin-7B-chat achieves superior performance on MTbench compared with baselines, as shown in Table [4](#).

## Generation Examples

We demonstrate some text generation examples from our Moxin Chat 7B and Llama2 Chat 7B in Table [5](#tab_3) and [6](#).

Example 1

## Moxin Chat 7B

Can you explain the concept of regularization in machine learning?

Regularization is a technique used in machine learning to prevent overfitting by introducing a penalty term in the loss function. The idea behind regularization is to encourage simpler models that are easier to interpret and generalize better to new data.

There are two main types of regularization: 1. L1 regularization: also known as Lasso regularization, this method adds a penalty term to the loss function that is proportional to the absolute value of the coefficients in the model. This encourages sparsity, meaning that some coefficients may be exactly zero, which simplifies the model and makes it easier to interpret. 2. L2 regularization: also known as Ridge regularization, this method adds a penalty term to the loss function that is proportional to the square of the coefficients in the model. This encourages shrinkage, meaning that the coefficients are pulled closer to zero, which simplifies the model and reduces the variance of the model estimates.

Regularization can be used in various machine learning models, including linear regression, logistic regression, and neural networks. It is often used in conjunction with other techniques such as crossvalidation to find the optimal hyperparameters that balance the bias-variance tradeoff and prevent overfitting.

## Llama2 Chat 7B

Can you explain the concept of regularization in machine learning?

Regularization is a technique used in machine learning to prevent overfitting. It involves adding a penalty term to the loss function to discourage large values of the model's weights.

There are several types of regularization, including: 1. L1 regularization (Lasso): This adds a penalty term to the loss function that is proportional to the absolute value of the model's weights. This encourages the model to have smaller weights, which can help prevent overfitting. 2. L2 regularization (Ridge): This adds a penalty term to the loss function that is proportional to the square of the model's weights. This also encourages the model to have smaller weights, but it can lead to some weights being set to zero, which can result in a simpler model. Regularization can be applied to different layers of a neural network, such as the input layer, hidden layers, or output layer. It can also be applied to different parts of a model, such as the weights, biases, or activation functions. The main idea behind regularization is to add a penalty term to the loss function that discourages large values of the model's weights. This helps to prevent overfitting by encouraging the model to find a simpler solution that generalizes better to new data.

Regularization can be used in conjunction with other techniques, such as cross-validation and grid search, to improve the performance of a machine learning model. In summary, regularization is a technique used in machine learning to prevent overfitting by adding a penalty term to the loss function. There are several types of regularization, including L1, L2, dropout, and early stopping, and it can be applied to different layers and parts of a model. Regularization helps to find a simpler solution that generalizes better to new data and can be used in conjunction with other techniques to improve the performance of a machine learning model. Normalization is a technique used to scale the data before using it for training a machine learning model. It is used to make the data more uniform, and to prevent it from being skewed by large or small values.

There are several ways to normalize data, including min-max scaling, z-score normalization, and logarithmic scaling. In min-max scaling, the values are scaled to a range between 0 and 1, by dividing them by the maximum in the dataset. In z-score normalization, the values are scaled to have a mean of 0 and a standard deviation of 1, by subtracting the mean from each value and dividing by the standard deviation. In logarithmic scaling, the values are transformed using the logarithm function, which can be useful for normalizing data with a wide range of values.

Normalization is an important step in preparing data for machine learning, as it can help to improve the performance of the model by making the data more uniform and easier to analyze. It can also help to prevent overfitting and improve generalization.

## Llama2 Chat 7B

Can you explain the concept of normalization in machine learning?

Normalization is a technique used in machine learning to transform input data into a range of values that are suitable for use in a model. The goal of normalization is to ensure that the data is on the same scale, so that the model can learn meaningful relationships between the features.

There are several types of normalization techniques, including: 1. Min-max normalization: This involves scaling the data so that it falls within a specific range, usually between 0 and 1. 2. Standardization: This involves subtracting the mean and dividing by the standard deviation of the data, so that the data has a mean of 0 and a standard deviation of 1.

3. Z-score normalization: This involves subtracting the mean and dividing by the standard deviation of the data, so that the data has a mean of 0 and a standard deviation of 1. 4. Log transformation: This involves taking the logarithm of the data, which can help to reduce the effect of extreme values in the data. 5. Square root transformation: This involves taking the square root of the data, which can help to reduce the effect of outliers in the data.

Normalization is an important preprocessing step in machine learning, as it can help to improve the performance of the model by reducing the effect of irrelevant features and improving the interpretability of the results.

Here are some key points to keep in mind when normalizing data for machine learning: 1. Normalization can improve the performance of the model by reducing the effect of irrelevant features and improving the interpretability of the results. 8. Normalization can be applied to different features of the data, such as the input features, the target variable, or the output features. 9. Normalization can be used in combination with other preprocessing techniques, such as feature scaling, feature selection, and data augmentation. 10. Normalization is a powerful tool for improving the performance of machine learning models, but it is important to carefully evaluate the performance of the normalized data and consider the trade-offs between different normalization techniques.

Table 6: Generation Examples.

## Conclusion

The field of Large Language Models has witnessed a significant shift toward open-source development, fostering innovation within the AI community. However, a critical challenge emerges: many purportedly open-source models withhold essential components necessary for full understanding and reproducibility, creating barriers that limit both academic advancement and commercial adoption. This does not not only hamper scientific progress but also prevent businesses from fully leveraging these models for innovative applications, ultimately diminishing potential societal benefits and economic value creation. To address these limitations, we introduce Moxin 7B, a fully open-source language model developed in accordance with the Model Openness Framework (MOF), providing comprehensive access to pre-training code, configurations, training and fine-tuning datasets, and all intermediate checkpoints. Our evaluation results demonstrate that the Moxin 7B achieves superior zero-shot evaluation results compared to popular 7B models while maintaining competitive few-shot capabilities. We wish to see more work that establishes new standard for reproducible research in language model development, fostering a more inclusive and economically vibrant AI ecosystem.

![Dropout regularization: This is a type of regularization that is applied during training. It involves randomly setting a fraction of the model's neurons to zero during each training iteration. This helps to prevent overfitting by making the model less reliant on any single neuron. 4. Early stopping: This is a type of regularization that is applied during training. It involves monitoring the validation loss during training and stopping the training process when the validation loss stops improving. This helps to prevent overfitting by stopping the training process before the model has a chance to fit the noise in the training data.]()

![There are several types of normalization techniques, including min-max normalization, standardization, z-score normalization, log transformation, and square root transformation. 3. The choice of normalization technique will depend on the nature of the data and the specific requirements of the model. 4. Normalization can be applied to both continuous and categorical data. 5. It is important to normalize the data before training the model, as normalization can improve the convergence of the model and reduce the risk of overfitting. 6. Normalization can also be used to improve the interpretability of the results by reducing the effect of irrelevant features and improving the scalability of the model. 7. It is important to carefully evaluate the performance of the normalized data and consider the trade-offs between different normalization techniques.]()

![Parameter setting.]()

![Performance comparison for various models in zero-shot evaluation.]()

![Performance comparison for various models in few-shot evaluation.]()

![Generation Examples.]()

https://huggingface.co/datasets/cerebras/SlimPajama-627B

https://huggingface.co/datasets/mlfoundations/dclm-baseline-1.0

https://huggingface.co/datasets/bigcode/the-stack-dedup

