- Decision to use gradient noise scale as a predictor for batch size
- Choice of datasets for empirical testing (MNIST, SVHN, CIFAR-10, ImageNet, Billion Word, Atari, Dota)
- Selection of optimization algorithms (SGD, Adam, RMSProp)
- Assumptions regarding the relationship between model performance and noise scale
- Decision to analyze compute-efficiency vs. time-efficiency tradeoffs
- Choice to implement dynamic batch size tuning during training
- Framework for predicting optimal batch sizes based on noise scale
- Methodology for measuring gradient noise scale
- Decision to focus on empirical validation of theoretical predictions
- Choice of performance metrics for evaluating model training efficiency
- Assumptions about the impact of model complexity on noise scale
- Decision to explore the effects of batch size on convergence rates
- Choice to include both supervised learning and reinforcement learning tasks
- Decision to document the relationship between batch size and training stability
- Assumptions regarding the variance of gradient estimates across different tasks
- Decision to analyze the implications of findings for future experiments and research directions