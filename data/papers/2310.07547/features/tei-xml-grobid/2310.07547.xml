<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Entropy estimators for Markovian sequences: A comparative analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-01-18">January 18, 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Juan</forename><surname>De Gregorio</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Institute for Cross-Disciplinary Physics and Complex Systems IFISC (UIB-CSIC)</orgName>
								<address>
									<addrLine>Campus Universitat de les Illes Balears</addrLine>
									<postCode>E-07122</postCode>
									<settlement>Palma de Mallorca</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Sánchez</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Institute for Cross-Disciplinary Physics and Complex Systems IFISC (UIB-CSIC)</orgName>
								<address>
									<addrLine>Campus Universitat de les Illes Balears</addrLine>
									<postCode>E-07122</postCode>
									<settlement>Palma de Mallorca</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Raúl</forename><surname>Toral</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Institute for Cross-Disciplinary Physics and Complex Systems IFISC (UIB-CSIC)</orgName>
								<address>
									<addrLine>Campus Universitat de les Illes Balears</addrLine>
									<postCode>E-07122</postCode>
									<settlement>Palma de Mallorca</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Entropy estimators for Markovian sequences: A comparative analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-01-18">January 18, 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">845F8E697FCF0DF234A60D1F4594B0EE</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Entropy estimation is a fundamental problem in information theory that has applications in various fields, including physics, biology, and computer science. Estimating the entropy of discrete sequences can be challenging due to limited data and the lack of unbiased estimators. Most existing entropy estimators are designed for sequences of independent events and their performances vary depending on the system being studied and the available data size. In this work, we compare different entropy estimators and their performance when applied to Markovian sequences. Specifically, we analyze both binary Markovian sequences and Markovian systems in the undersampled regime. We calculate the bias, standard deviation, and mean squared error for some of the most widely employed estimators. We discuss the limitations of entropy estimation as a function of the transition probabilities of the Markov processes and the sample size. Overall, this paper provides a comprehensive comparison of entropy estimators and their performance in estimating entropy for systems with memory, which can be useful for researchers and practitioners in various fields.</p><p>I.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>The entropy associated with a random variable is a measure of its uncertainty or diversity, taking large values for a highly unpredictable random variable (i.e., all outcomes equally probable) and low values for a highly predictable one (i.e., one or few outcomes much more probable than the others). As such, the concept has found multiple applications in a variety of fields including but not limited to nonlinear dynamics, statistical physics, information theory, biology, neuroscience, cryptography, and linguistics <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>.</p><p>Due to its mathematical simplicity and clear interpretation, Shannon's definition is the most widely used measure of entropy <ref type="bibr" target="#b13">[14]</ref>. For a discrete random variable X with L distinct possible outcomes x 1 , . . . , x L , the Shannon entropy reads</p><formula xml:id="formula_0">H[X] = - L i=1 p(x i ) ln(p(x i )),<label>(1)</label></formula><p>where p(x i ) denotes the probability that the random variable X takes the value x i . It often occurs in practice that the probability distribution of the variable X is unknown, either due to mathematical difficulties or to the lack of deep knowledge of the details of the underlying experiment described by the random variable X. In those situations, it is not possible to compute the entropy using Equation (1) directly. In general, our information is restricted to a finite set of ordered data resulting from the observation of the outcomes obtained by repeating a large number of times, N , the experiment. Hence, the goal is to estimate H from the ordered sequence S = X 1 , . . . , X N , where each X j ∈ {x i } L i=1 with j = 1, . . . , N . A numerical procedure that provides an approximation to the true value of H based on the sequence S is called an entropy estimator. As the sequence S is random, it is clear that an entropy estimator is itself a random variable, taking different values for different realizations of the sequence of N outcomes. It would be highly desirable to have an unbiased entropy estimator, i.e., an estimator whose average value coincides with the true result H for all values of the sequence length N . However, it can be proven that such an estimator does not exist <ref type="bibr" target="#b14">[15]</ref> and that, apart from the unavoidable statistical errors due to the finite number N of data of the sample (and which typically scale as N -1/2 ), all estimators present systematic errors which are in general difficult to evaluate properly. Therefore, a large effort has been devoted to the development of entropy estimators that, although necessarily biased, provide a good value for H with small statistical and systematic errors <ref type="bibr" target="#b15">[16]</ref>.</p><p>The problem of finding a good estimator with small errors becomes more serious when the number of data N is relatively small. Indeed, when the sizes of available data are much larger than the possible outcomes (N ≫ L), it is not difficult to estimate H accurately, and all of the most popular estimators are naturally satisfactory in this regime. The task becomes much harder as the numbers L and N come closer to each other. It is particularly difficult in the undersampled regime (N ≲ L) <ref type="bibr" target="#b16">[17]</ref>, where some, or potentially many, possible outcomes may not be observed in the sequence. It is in this regime where the difference in accuracy among the available estimators is more significant.</p><p>We emphasize that the discussed difficulties already appear for independent identically distributed (i.i.d.) random variables. Precisely, the previous literature has largely dealt with entropy estimators proposed for sequences of i.i.d. random variables <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref>. However, it is not clear that real data arising from experimental observation can be described with i.i.d. random variables due to the ubiquitous presence of data correlations. The minimal correlations in discrete sequences are of a Markovian nature. Then, how do the main entropy estimators behave for Markovian sequences? arXiv:2310.07547v2 [cond-mat.stat-mech] 17 Jan 2024</p><p>The purpose of this work is to make a detailed comparison of some of the most widely used entropy estimators in systems whose future is conditionally independent of the past (Markovian). In Markovian sequences, correlations stem from the fundamental principle that the probability of a data value appearing at a specific time depends on the value observed in the preceding time step. Markov chains have been used to model systems in a large variety of fields such as statistical physics <ref type="bibr" target="#b21">[22]</ref>, molecular biology <ref type="bibr" target="#b22">[23]</ref>, weather forecast <ref type="bibr" target="#b23">[24]</ref>, and linguistics <ref type="bibr" target="#b24">[25]</ref>, just to mention a few. Below, we analyze the strengths and weaknesses of estimators tested in a correlated series of numerically generated data. We compare the performances for the estimators that have shown to give good results for independent sequences <ref type="bibr" target="#b15">[16]</ref>. For definiteness, we below consider Markovian sequences of binary data. Furthermore, the calculation of relevant quantities in information theory, such as entropy rate and predictability gain <ref type="bibr" target="#b25">[26]</ref>, requires estimating the block entropy of a sequence, obtained from the estimation of the entropy associated not to a single result, but to a block of consecutive results. As we will argue in the following sections, the construction of overlapping blocks induces correlations amongst them, even if the original sequence is not correlated. The calculation of the block entropy is also a tool that can be used to estimate the memory of a given sequence <ref type="bibr" target="#b26">[27]</ref>, which is of utmost importance when dealing with strongly correlated systems <ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref>.</p><p>The rest of the paper is organized as follows. In Section II, we make a brief overview of the ten entropy estimators being considered in this study, nine of which are already known in the literature and an additional estimator built from results presented in ref. <ref type="bibr" target="#b33">[34]</ref>, which is further developed in this work. In Section III, we present the results of our comparative analysis of these estimators in two Markovian cases: (A) binary sequences; and (B) in an undersampled regime. Section IV contains the conclusions and an outlook. Finally, in Appendix A we provide a new interpretation in terms of geometric distributions of an estimator which is widely used as the starting point to construct others, and in Appendix B we prove the equivalence between a dynamics of block sequences and a Markovian random variable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. MATERIALS AND METHODS</head><p>In the following, we will use the notation â to refer to a numerical estimator of the quantity a. The bias of â is defined as</p><formula xml:id="formula_1">B[â] = ⟨â⟩ -a,<label>(2)</label></formula><p>where ⟨â⟩ represents the expected value of â. The estimator â is said to be unbiased if B[â] = 0. The dispersion of â is given by the standard deviation</p><formula xml:id="formula_2">σ[â] = ⟨â 2 ⟩ -⟨â⟩ 2 .<label>(3)</label></formula><p>Ideally, â should be as close to the true value a as possible. Therefore, it is desirable that â has both low bias and low standard deviation. With this in mind, it is natural to consider the mean squared error of an estimator, given by</p><formula xml:id="formula_3">MSE[â] = B[â] 2 + σ[â] 2 ,<label>(4)</label></formula><p>to assess its quality. Hence, when comparing estimators of the same variable, the one with the lowest mean squared error is preferable.</p><p>Given an estimator Ĥ of the entropy, its k-th moment can be computed as</p><formula xml:id="formula_4">⟨ Ĥk ⟩ = S P (S) Ĥ(S) k ,<label>(5)</label></formula><p>where the sum runs over all possible sequences S = X 1 , . . . , X N of length N and Ĥ(S) is the value that the estimator takes on in this sequence. The probability P (S) of observing the sequence S depends on whether S is correlated or not. For example, if S is an independent sequence, P (S) can be calculated as</p><formula xml:id="formula_5">P (S) = N i=1 p(X i ).<label>(6)</label></formula><p>For correlated sequences, Equation ( <ref type="formula" target="#formula_5">6</ref>) no longer holds. Consider a Markovian system, in which the probability of the next event only depends on the current state. In other words, the transition probabilities satisfy</p><formula xml:id="formula_6">P (X s = x j |X s-1 = x ℓ , . . . , X 1 = x k ) = P (X s = x j |X s-1 = x ℓ ),<label>(7)</label></formula><p>with s the position in the series. A homogeneous Markov chain is one in which the transition probabilities are independent of the time step s. Therefore, a homogeneous Markov chain is completely specified given the L × L matrix of transition probabilities p(x j |x ℓ ) = P (X s = x j |X s-1 = x ℓ ), j, ℓ = 1, . . . , L. In this case, the probability of observing the sequence S can be calculated as</p><formula xml:id="formula_7">P (S) = p(X 1 ) N -1 i=1 p(X i+1 |X i ). (<label>8</label></formula><formula xml:id="formula_8">)</formula><p>where we have applied Equation <ref type="bibr" target="#b6">(7)</ref> successively. The calculation of P (S) can be generalized to an morder Markov chain defined by the transition probabilities:</p><formula xml:id="formula_9">P (X s = x j |X s-1 = x ℓ , . . . , X 1 = x k ) = P (X s = x j |X s-1 = x ℓ , . . . , X s-m = x u ),<label>(9)</label></formula><p>that depend on the m previous results of the random variable.</p><p>It is clear that the moments of the estimator Ĥ, and consequently its performance given by its mean squared error, depend on the correlations of the system being analyzed.</p><p>Most of the entropy estimators considered in this work only depend on the number of times each outcome occurs in the sequence. In this case, the calculation of the moments of the estimator can be simplified for independent and Markovian systems considering the corresponding multinomial distributions <ref type="bibr" target="#b34">[35]</ref>.</p><p>Several entropy estimators were developed with the explicit assumption that the sequences being analyzed are uncorrelated <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>. The main assumption is that the probability of the number of times n i that the outcome x i occurs in a sequence of length N follows a binomial distribution,</p><formula xml:id="formula_10">P (n i ) = N n i p(x i ) ni (1 -p(x i )) N -ni . (<label>10</label></formula><formula xml:id="formula_11">)</formula><p>This approach is not valid when dealing with general Markovian sequences because Equation <ref type="bibr" target="#b9">(10)</ref> no longer holds. Instead, the Markovian binomial distribution <ref type="bibr" target="#b37">[38]</ref> should be used, or more generally, the Markovian multinomial distribution <ref type="bibr" target="#b34">[35]</ref>. Even for entropy estimators that were not developed directly using Equation <ref type="bibr" target="#b9">(10)</ref>, their performance is usually only analyzed for independent sequences <ref type="bibr" target="#b15">[16]</ref>. Hence, the need to compare and evaluate the different estimators in Markov chains.</p><p>Even though there exists a plethora of entropy estimators in the literature <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref><ref type="bibr" target="#b44">[45]</ref><ref type="bibr" target="#b45">[46]</ref><ref type="bibr" target="#b46">[47]</ref>, we here focus on nine of the most commonly employed estimators, and we also propose a new estimator, constructed from known results <ref type="bibr" target="#b33">[34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Maximum Likelihood Estimator</head><p>The maximum likelihood estimator (MLE) (also known as plug-in estimator) simply consists of replacing the exact probabilities in Equation ( <ref type="formula" target="#formula_0">1</ref>) for the estimated frequencies,</p><formula xml:id="formula_12">p(x i ) = ni N , (<label>11</label></formula><formula xml:id="formula_13">)</formula><p>where ni is the number of times that the outcome x i is observed in the given sequence. It is well known that Equation ( <ref type="formula" target="#formula_12">11</ref>) is an unbiased estimator of p(x i ), but the MLE estimator, given by</p><formula xml:id="formula_14">ĤMLE = - L i=1 p(x i ) ln(p(x i )),<label>(12)</label></formula><p>is negatively biased <ref type="bibr" target="#b14">[15]</ref>, i.e., ⟨ ĤMLE ⟩ -H &lt; 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Miller-Madow Estimator</head><p>The idea behind the Miller-Madow estimator (MM) <ref type="bibr" target="#b47">[48]</ref> is to correct the bias of ĤMLE up to the first order in 1/N , resulting in</p><formula xml:id="formula_15">ĤMM = ĤMLE + N 0 -1 2N ,<label>(13)</label></formula><p>where N 0 is the number of different elements present in the sequence. Corrections of higher order are not considered because they include the unknown probabilities p(x i ) <ref type="bibr" target="#b48">[49]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Nemenman-Shafee-Bialek Estimator</head><p>A large family of entropy estimators are derived by estimating the probabilities using a Bayesian framework <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b49">[50]</ref><ref type="bibr" target="#b50">[51]</ref><ref type="bibr" target="#b51">[52]</ref><ref type="bibr" target="#b52">[53]</ref>. The Nemenman-Shafee-Bialek estimator (NSB) <ref type="bibr" target="#b53">[54]</ref><ref type="bibr" target="#b54">[55]</ref><ref type="bibr" target="#b55">[56]</ref> provides a novel Bayesian approach that, unlike traditional methods, does not rely on strong prior assumptions on the probability distribution. Instead, this method uses a mixture of Dirichlet priors, designed to produce an approximately uniform distribution of the expected entropy value. This ensures that the entropy estimate is not exceedingly biased by prior assumptions.</p><p>The Python implementation developed in ref. <ref type="bibr" target="#b56">[57]</ref> was used in this paper for the calculations of the NSB estimator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Chao-Shen Estimator</head><p>The Chao-Shen estimator (CS) <ref type="bibr" target="#b17">[18]</ref> takes into account two corrections to Equation <ref type="bibr" target="#b11">(12)</ref> to reduce its bias: first, a Horvitz-Thompson adjustment <ref type="bibr" target="#b57">[58]</ref> to account for missing elements in a finite sequence; second, a correction to the estimated probabilities, pCS (x i ) = ĈCS p(x i ), leading to</p><formula xml:id="formula_16">ĈCS = 1 - N 1 N ,<label>(14)</label></formula><p>where N 1 is the number of elements that appear only once in the sequence. The Chao-Shen entropy estimator is then</p><formula xml:id="formula_17">ĤCS = - xi∈S pCS (x i ) ln(p CS (x i )) 1 -(1 -pCS (x i )) N .<label>(15)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Grassberger Estimator</head><p>Assuming that all p(x i ) ≪ 1, the probability distribution of each n i can be approximated by a Poisson distribution. Following this idea, Grassberger (G) derived the estimator presented in ref. <ref type="bibr" target="#b35">[36]</ref> by first considering Rényi entropies of order q [59]:</p><formula xml:id="formula_18">H(q) = 1 q -1 ln L i=1 p(x i ) q . (<label>16</label></formula><formula xml:id="formula_19">)</formula><p>Taking into account that the Shannon case can be recovered by taking the limit q → 1, the author proposed a low bias estimator for the quantity p q , for an arbitrary q. This approach led to the estimator given by</p><formula xml:id="formula_20">ĤG = ln(N ) - 1 N L i=1 ni G ni ,<label>(17)</label></formula><p>with</p><formula xml:id="formula_21">G 1 = -γ -ln 2, G 2 = 2 -γ -ln 2,</formula><p>and the different values of G ni computed using the recurrence relation</p><formula xml:id="formula_22">G 2n+1 = G 2n<label>(18)</label></formula><formula xml:id="formula_23">G 2n+2 = G 2n + 2 2n + 1 ,<label>(19)</label></formula><p>where γ = 0.57721 . . . is Euler's constant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Bonachela-Hinrichsen-Muñoz Estimator</head><p>The idea behind the Bonachela-Hinrichsen-Muñoz estimator (BHM) <ref type="bibr" target="#b36">[37]</ref> is to make use of Equation ( <ref type="formula" target="#formula_10">10</ref>) to find a balanced estimator of the entropy that, on average, minimizes the mean squared error. The resulting estimator is given by</p><formula xml:id="formula_24">ĤBHM = 1 N + 2 L i=1 (n i + 1) N +2 j=ni+2 1 j .<label>(20)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Shrinkage Estimator</head><p>The estimator proposed by Hausser and Strimmer <ref type="bibr" target="#b19">[20]</ref> (HS) is a shrinkage-type estimator <ref type="bibr" target="#b59">[60]</ref>, in which the probabilities are estimated as an average of two models:</p><formula xml:id="formula_25">pHS (x i ) = α 1 L + (1 -α)p(x i ),<label>(21)</label></formula><p>where the weight α is chosen so that the resulting estimator pHS has lower mean squared error than p and is calculated by <ref type="bibr" target="#b60">[61]</ref> </p><formula xml:id="formula_26">α = min 1, 1 - L i=1 (p(x i )) 2 (N -1) L i=1 (1/L -p(x i )) 2 . (<label>22</label></formula><formula xml:id="formula_27">)</formula><p>Hence, the shrinkage estimator is</p><formula xml:id="formula_28">ĤHS = - L i=1 pHS (x i ) ln(p HS (x i )).<label>(23)</label></formula><p>H. Chao-Wang-Jost Estimator</p><p>The Chao-Wang-Jost estimator (CWJ) <ref type="bibr" target="#b61">[62]</ref> uses the series expansion of the logarithm function, as well as a correction to account for the missing elements in the sequence. This estimator is given by</p><formula xml:id="formula_29">ĤCWJ = L i=1 ni N (ψ(N ) -ψ(n i )) (24) + N 1 N (1 -A) 1-N   -ln(A) - N -1 j=1 1 j (1 -A) j   ,<label>(25)</label></formula><p>where ψ(z) is the digamma function and A is given by</p><formula xml:id="formula_30">A =                2N 2 (N -1)N 1 + 2N 2 , if N 2 &gt; 0, 2 (N -1)(N 1 -1) + 2 , if N 2 = 0, N 1 &gt; 0, 1, if N 1 = N 2 = 0,<label>(26)</label></formula><p>with N 1 and N 2 the number of elements that appear once and twice, respectively, in the sequence.</p><p>In the supplementary material of ref. <ref type="bibr" target="#b61">[62]</ref>, it is proven that the first sum in Equation ( <ref type="formula" target="#formula_29">25</ref>) is the same as the leading terms of the estimators developed in refs. <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref>. In Appendix A, we show that each term in this sum is also equivalent to an estimator that takes into account the number of observations made prior to the occurrence of the element x i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Correlation Coverage-Adjusted Estimator</head><p>The correlation coverage-adjusted estimator (CC) <ref type="bibr" target="#b26">[27]</ref> uses the same ideas that support Equation (15) but considers a different correction to the probabilities, pCC (x i ) = ĈCC p(x i ), where now ĈCC is calculated sequentially taking into account previously observed data,</p><formula xml:id="formula_31">ĈCC = 1 - N ′ j=1 1 N ′ + j I(X N ′ +j / ∈ (X 1 , . . . , X N ′ +j-1 )),<label>(27)</label></formula><p>where N ′ ≡ N/2 and the function I(Z) yields 1 if the event Z is true and 0 otherwise. By construction, this probability estimator considers possible correlations in the sequence.</p><p>Then, the CC estimator is given by</p><formula xml:id="formula_32">ĤCC = - xi∈S pCC (x i ) ln(p CC (x i )) 1 -(1 -pCC (x i )) N .<label>(28)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J. Corrected Miller-Madow Estimator</head><p>In ref. <ref type="bibr" target="#b33">[34]</ref> it is shown that the bias of the MLE estimator can be approximated based on a Taylor expansion as</p><formula xml:id="formula_33">B[ ĤMLE ] ≈ - N 0 -1 2N - 1 N ∞ l=1 K(l),<label>(29)</label></formula><p>where</p><formula xml:id="formula_34">K(l) = L i=1 P (X s+l = x i |X s = x i ) -1. (<label>30</label></formula><formula xml:id="formula_35">)</formula><p>Notice that the first term in Equation ( <ref type="formula" target="#formula_33">29</ref>) is simply the Miller-Madow correction shown in Section II B, whereas the second term involves the unknown conditional probabilities with a lag l that tends to infinity. These quantities can be hard to estimate directly from observations, especially if dealing with short sequences. However, the calculation of K(l) can be simplified. Assuming that the sequence is independent, it can easily be seen that K(l) = 0 for all l and one recovers the Miller-Madow correction.</p><p>Considering that the sequence is Markovian, then K(l) can be written in a simpler way by first noticing that</p><formula xml:id="formula_36">P (X s+l = x j |X s = x i ) = (T l ) ij ,</formula><p>where T is the L × L transition probability matrix given by (T) ij = p(x j |x i ). Hence,</p><formula xml:id="formula_37">K(l) = L i=1 (T l ) ii -1 = Tr(T l ) -1 = L i=1 λ l i -1,<label>(31)</label></formula><p>where Tr(T l ) is the trace of the matrix T l and λ i are the eigenvalues of T. The last equality of Equation ( <ref type="formula" target="#formula_37">31</ref>) is a well-known result in linear algebra. Given that T is a stochastic matrix, then all eigenvalues fulfil that |λ| ≤ 1, and at least one eigenvalue is equal to 1. We will assume that only λ 1 = 1 and we will discuss later on the case where more than one eigenvalue is equal to 1. We can write Equation <ref type="bibr" target="#b28">(29)</ref> as</p><formula xml:id="formula_38">B[ ĤMLE ] ≈ - N 0 -1 2N - 1 N ∞ l=1 L i=2 λ l i .<label>(32)</label></formula><p>Using the well-known result for the sum of the geometric series, then,</p><formula xml:id="formula_39">B[ ĤMLE ] ≈ - N 0 -1 2N - 1 N L i=2 λ i 1 -λ i . (<label>33</label></formula><formula xml:id="formula_40">)</formula><p>Notice that the convergence of the series of Equation <ref type="bibr" target="#b31">(32)</ref> requires that none of the eigenvalues λ 2 , . . . , λ L has an absolute value equal to 1. Given a finite sequence, we need to estimate the transition matrix T as</p><formula xml:id="formula_41">( T) ij = p(x j |x i ) = nij L k=1 nik ,<label>(34)</label></formula><p>with nik the number of times the block (x i , x k ) is observed in the sequence. We can then calculate the eigenvalues λ1 , . . . , λL of the matrix T, which is also stochastic, and hence, one of its eigenvalues, λ1 , is equal to 1.</p><p>Therefore, the proposed corrected Miller-Madow estimator (CMM) is</p><formula xml:id="formula_42">ĤCMM = ĤMM + 1 N L i=2 λi 1 -λi . (<label>35</label></formula><formula xml:id="formula_43">)</formula><p>The correction to the MM estimator should only be used when the absolute value of all eigenvalues but λ1 of the stochastic matrix T are not equal to 1. Otherwise, it is recommended to avoid that correction and simply use ĤMM as the estimator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. RESULTS</head><p>We now proceed to compare the performance of the different estimators defined in the previous Section II. Let us note first that, given a particular sequence, all entropy estimators, with the exception of the CC and CMM estimators, will yield exactly the same value if we permute arbitrarily all numbers in the sequence. The reason behind this difference is that although the CC estimator takes into account the order in which the different elements appear in the sequence, and the CMM estimator considers the transition probabilities of the outcomes, all other estimators are based solely on the knowledge of the number of times that each possible outcome appears, and this number is invariant under permutations.</p><p>Certain estimators, such as CS or CC, can be calculated without any prior knowledge of the possible number of outcomes, L. This feature is particularly advantageous in fields like ecology, where the number of species in a given area may not be accurately known. Conversely, estimators like HS and NSB require an accurate estimate of L for their computation.</p><p>As mentioned before, when analyzing an estimator, there are two important statistics to consider: the bias and the standard deviation. Ideally, we would like an estimator with zero bias and low standard deviation. For the entropy, we have already argued that such an unbiased estimator does not exist. Hence, in this case, the "best" estimator (if it exists) would be the one that has the best balance between bias and standard deviation, i.e., the one with the lowest mean squared error given by Equation <ref type="bibr" target="#b3">(4)</ref>.</p><p>In this section, we will analyze and compare these three statistics-bias, standard deviation, and mean squared error-for the ten entropy estimators reviewed in Section II in two main Markovian cases: (A) binary sequences; and (B) in an undersampled regime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Binary Sequences</head><p>First, we consider homogeneous Markovian binary (L = 2) random variables, with possible outcomes x i = 0, 1. One advantage of discussing this system is that it is uniquely defined by a pair of independent transition probabilities, p(0|0) and p(1|1), where p(x i |x j ) ≡ P (X s+1 = x i |X s = x j ). Then, p(1|0) = 1 -p(0|0) and p(0|1) = 1-p(1|1). To shorten the notation, we hereafter write p 00 for p(0|0) and p 11 for p(1|1).</p><p>It is possible to compute the Shannon entropy of this random variable using the general definition given by Equation ( <ref type="formula" target="#formula_0">1</ref>).</p><formula xml:id="formula_44">H = -p(0) ln p(0) -p(1) ln p(1)<label>(36)</label></formula><p>with the stationary values <ref type="bibr" target="#b4">[5]</ref>:</p><formula xml:id="formula_45">p(0) = 1 -p 11 2 -p 00 -p 11 , p(1) = 1 -p(0).<label>(37)</label></formula><p>The average value and standard deviation of the different entropy estimators were computed using Equation ( <ref type="formula" target="#formula_4">5</ref>) for k = 1, 2 by generating all 2 N possible sequences S and computing the probability of each one using Equation <ref type="bibr" target="#b7">(8)</ref>, where p(X 1 ) are the stationary values given by Equation <ref type="bibr" target="#b36">(37)</ref>. We have followed this approach to compute the estimator bias B = ⟨ Ĥ⟩ -H and its standard deviation σ = ⟨ Ĥ2 ⟩ -⟨ Ĥ⟩ 2 . As an example, we plot the absolute value of the bias for sequences of length N = 4 in the colour map of Figure <ref type="figure" target="#fig_0">1</ref>, for the ten entropy estimators presented in Section II, as a function of the transition probabilities p 00 and p 11 .</p><p>In Figure <ref type="figure" target="#fig_0">1</ref>, we can see that, for all ten estimators, the bias is larger in the region around the values p 00 ≃ p 11 ≃ 1. The reason is that, in this region, the stationary probabilities of 0 and 1 are very similar, but given these particular values of the transition probabilities, a short sequence will most likely feature only one of these values, which makes it very hard to correctly estimate the entropy in those cases. Apart for this common characteristic, the performance of the estimators when considering only the bias is quite diverse, all of them having different regions where the bias is lowest (darker areas in the panels).</p><p>In order to quantitatively compare the performance of the different estimators, we have aggregated all values in the (p 00 , p 11 ) plane. We define the aggregated bias of an estimator,</p><formula xml:id="formula_46">B = (∆p) 2 p00,p11 |B(p 00 , p 11 )|, (<label>38</label></formula><formula xml:id="formula_47">)</formula><p>where the sum runs over all values of the transition probabilities used to produce Figure <ref type="figure" target="#fig_0">1</ref>, ∆p = 0.02 is the step value used for the grid of the figure, and B(p 00 , p 11 ) is the bias for the particular values of the transition probabilities. The aggregated bias given by Equation <ref type="bibr" target="#b37">(38)</ref> depends only on the sequence length N . We conduct the previous analysis for different values of N . The resulting plot of the aggregate bias B of the entropy estimator as a function of the sequence length is shown in Figure <ref type="figure">2</ref>. In this figure, we can see that the CC estimator gives the best performance for small values of N , except for N = 2, where the CWJ estimator has the lowest aggregated bias. However, from N = 7 it is the CMM estimator which outperforms the rest. The poor performance of this estimator for low values of N is due to the fact that this estimator, in contrast to the others, requires estimating the transition probabilities, as well as the stationary probabilities, and therefore more data are needed. As expected, all the estimators yield an aggregated bias that vanishes as N increases.</p><p>In the colour map of Figure <ref type="figure" target="#fig_3">3</ref>, we perform a similar analysis for the standard deviation σ. In the figure, we find that all ten estimators show a similar structure in the sense that the regions of lowest and highest σ are alike. The smallest deviation is mostly located near the left bottom corner of the colour maps and the largest deviation occurs around the regions (0.65 ≲ p 00 ≲ 0.9, 0 ≲ p 11 ≲ 1) and (0 ≲ p 00 ≲ 1, 0.65 ≲ p 11 ≲ 0.9) (green areas in the figures). Of course, the values of σ inside these regions vary for each estimator but they all share this similar feature. In this case, by just looking at the colour maps, it is easy to see that BHM (panel f) and NSB (panel c) estimators are the ones with the lowest standard deviation.</p><p>The aggregated standard deviation σ, defined in a similar way to the aggregated bias, σ = (∆p) 2 p00,p11 σ(p 00 , p 11 ), <ref type="bibr" target="#b38">(39)</ref> is plotted in Figure <ref type="figure">4</ref> as a function of the sequence size N .</p><p>In agreement with the previous visual test, the BHM and NSB estimators clearly outperform the rest, even though their advantage is less significant as N increases.</p><p>Finally, for every particular N , we compute the mean squared error of the entropy estimators, Equation ( <ref type="formula" target="#formula_3">4</ref>), as a function of p 00 and p 11 . Its aggregated value MSE = (∆p) 2 p00,p11 MSE(p 00 , p 11 ), <ref type="bibr" target="#b39">(40)</ref> is plotted as a function of N in Figure <ref type="figure" target="#fig_2">5</ref>. Even though the CC and CMM estimators outperform the others when considering only the bias, their large dispersion dominates the mean squared error. Overall, it can be seen that the BHM and NSB estimators surpass the rest when both the bias and standard deviation are considered although, again, their advantage becomes less significant as N increases.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Undersampled Regime: Block Entropy</head><p>Consider a sequence S = X 1 , . . . , X N , where each X i = 0, 1 is a binary variable, with probabilities P (X i = 1) = p, P (X i = 0) = 1-p. We group the sequence in blocks of size n, such that the jth-block is B j = (X j , . . . , X j+n-1 ). We denote by {b i } i=1,...,2 n the set of all possible blocks. The total number of (overlapping) blocks that can be constructed out of a series of N elements is N n = Nn + 1, whereas the total number of possible blocks is L = 2 n . Hence, depending on the values of n and N , the sequence formed by the N n blocks, S n = B 1 , . . . , B Nn , will be in an undersampled regime whenever N n ≪ 2 n .</p><p>The block entropy H n is defined by</p><formula xml:id="formula_48">H n = - 2 n i=1 p(b i ) ln(p(b i )),<label>(41)</label></formula><p>where p(b i ) is the probability of observing the block b i .</p><p>The important thing to notice here is that, even if the different outcomes X 1 , . . . , X N of the binary variable X are independent, the block sequence B 1 , . . . , B Nn obeys a Markov process for n ≥ 2. This Markovian property can be easily established by noticing that the block B j = (X j , . . . , X j+n-1 ) can only be followed by the block B j+1 = (X j+1 , . . . , X j+n-1 , 1) with probability p or by the block B j+1 = (X j+1 , . . . , X j+n-1 , 0) with probability 1 -p. Therefore, the probability of B j+1 depends only on the value of block B j . In Appendix B we show that the dynamics of block sequences in the case that X i are i.i.d. is equivalent to that of a new stochastic variable Z that can take any of L = 2 n possible outcomes, z i = 0, 1, . . . , 2 n -1, with the following transition probabilities for each state z:</p><formula xml:id="formula_49">p(z k |z i ) =      1 -p, if z k = 2z i (mod 2 n ), p, if z k = 2z i (mod 2 n ) + 1, 0, otherwise.<label>(42)</label></formula><p>These types of Markovian systems have been related to Linguistics and Zipf's law <ref type="bibr" target="#b24">[25]</ref>.</p><p>The previous result can be generalized. If the original sequence X 1 , . . . , X N is Markovian of order m ≥ 1, then the dynamics of the block sequences B 1 , . . . , B Nn are also Markovian of order 1, for n ≥ m.</p><p>It is well known <ref type="bibr" target="#b4">[5]</ref> that the block entropy, when the original sequence S is constructed out of i.i.d. binary variables, obeys</p><formula xml:id="formula_50">H n = nH 1 ,<label>(43)</label></formula><p>where H 1 can be calculated using Equation <ref type="bibr" target="#b35">(36)</ref> with p(1) = p and p(0) = 1 -p. Therefore, the entropy rate is constant. We want to compare now the performance of the different estimators defined before when computing the block entropy. In this case, we cannot use an expression equivalent to Equation ( <ref type="formula" target="#formula_4">5</ref>), summing over all sequences S n , since the number of possible sequences is (2 n ) Nn , and it is not possible to enumerate all the sequences even for relatively small values of n and N n . As an example, we employ in our numerical study N n = 20 and n = 6, for which the total number of possible sequences is 2 120 . Therefore, we use the sample mean µ M [ Ĥn ] and the sample variance s 2 M [ Ĥn ] as unbiased estimators to the expected value ⟨ Ĥn ⟩ and the variance σ 2 [ Ĥn ], respectively. After generating a sample of M independent sequences S i n , i = 1, . . . , M , and computing the estimator Ĥn (S i n ) for each of the sequences, those statistics are computed as</p><formula xml:id="formula_51">µ M [ Ĥn ] = 1 M M i=1</formula><p>Ĥn (S i n ),</p><formula xml:id="formula_52">s 2 M [ Ĥn ] = 1 M -1 M i=1 ( Ĥn (S i n ) -µ M [ Ĥn ]) 2 . (<label>44</label></formula><formula xml:id="formula_53">)</formula><p>Using Equations ( <ref type="formula" target="#formula_50">43</ref>) and ( <ref type="formula" target="#formula_52">44</ref>) we can calculate the bias</p><formula xml:id="formula_54">B n = µ M [ Ĥn ] -H n , the standard deviation s M [ Ĥn ]</formula><p>, and the mean squared error s 2 M [ Ĥn ] + B 2 n . In the following, we set M = 10 4 for our simulations.</p><p>In Figure <ref type="figure" target="#fig_4">6</ref>, we show plots of B n and s M [ Ĥn ] as a function of p ranging from 0.02 to 0.5 with step ∆p = 0.02, for N n = 20. We find that the CC estimator performs remarkably well in terms of bias and we highlight its robustness. Unlike the other estimators, which display significant variations in their bias as p changes, the CC estimator remains approximately constant at a low value. However, the CC estimator presents a high standard deviation, whereas the MLE and MM exhibit the lowest standard deviation. For the majority of estimators considered, we observe that the ones with higher bias are the ones with lower deviation. An exception is the HS estimator.</p><p>To analyze the changes in the overall performances of the estimators with different values of N , we calculated the aggregated bias as</p><formula xml:id="formula_55">B n = ∆p p |B n (p)|.<label>(45)</label></formula><p>Similarly, we calculated the aggregated standard deviation as</p><formula xml:id="formula_56">s n = ∆p p s M [ Ĥn ](p),<label>(46)</label></formula><p>and the aggregated mean squared error as</p><formula xml:id="formula_57">MSE n = ∆p p (s 2 M [ Ĥn ](p) + B n (p) 2 ).<label>(47)</label></formula><p>The resulting plots are shown in Figures <ref type="figure" target="#fig_5">7</ref><ref type="figure">8</ref><ref type="figure">9</ref>, respectively.   It was expected that the total bias of the estimators would decrease by increasing N , and in Figure <ref type="figure" target="#fig_5">7</ref> it can be seen that this is indeed the case for all estimators except for the BHM estimator. Surprisingly, the bias of this estimator follows a typical pattern of decreasing as the sample size increases, just like the other estimators. However, it takes an unexpected turn starting at N = 20, as it begins to increase once more. A possible reason for this behaviour is that the BHM estimator is designed to minimize the MSE.</p><p>Similarly to the results obtained for the binary Markovian case, the CC estimator demonstrates in Figure <ref type="figure" target="#fig_5">7</ref> excellent performance when solely evaluating bias. Even though its performance for a data size of N = 5 is not outstanding, it begins to outperform all but the CS, CWJ, and HS estimators starting at N = 10, and from that point onward, the CC estimator consistently ranks among the top-performing estimators, together with the NSB and CWJ estimators.</p><p>By comparing Figures <ref type="figure" target="#fig_5">7</ref> and <ref type="figure">8</ref>, it can be seen that there is a certain balance: an estimator with a higher bias usually has a lower deviation when compared to others. This is clearly the case for the MLE and MM estimators, as they are the two with the worst performances in terms of bias, but they have the lowest aggregated standard deviation for most of the data sizes considered.</p><p>In this interplay between bias and standard deviation observed for most of the entropy estimators considered here, the NSB estimator is the one that presents the best performance when considering both statistics. From Figure <ref type="figure">9</ref>, it is clear that this estimator shows the lowest aggregated mean squared error, although just from N = 20 the difference with other estimators like the CC or the G becomes vanishingly small.</p><p>It can be seen in Figures <ref type="figure" target="#fig_5">7</ref><ref type="figure">8</ref><ref type="figure">9</ref>that the performance of the CMM estimator is very similar to MM's performance, especially for large values of N . This suggests that for Markovian systems defined by the transition probabilities given by Equation ( <ref type="formula" target="#formula_49">42</ref>), the correction introduced in Equation ( <ref type="formula" target="#formula_42">35</ref>) is not significant, particularly in the limit of large N .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DISCUSSION</head><p>We have made a detailed comparison of nine of the most widely used entropy estimators when applied to Markovian sequences. We have also included in this analysis a new proposed estimator, motivated by the results presented in ref. <ref type="bibr" target="#b33">[34]</ref>. One crucial difference in the way these estimators are constructed is that only the correlation coverage-adjusted estimator <ref type="bibr" target="#b26">[27]</ref> and the corrected Miller-Madow estimator take into account the order in which the elements appear in the sequence. To calculate the CC estimator, it is necessary to know the entire history of the sequence , and the computation of the CMM estimator requires the calculation of the transition probabilities. On the contrary, for all other estimators, it is sufficient to know the number of times that each element is present in the sequence, independently of the position in which they appear. Remarkably, this novel approach to the issue of entropy estimation allows us to reduce the bias, even in undersampled regimes. Unfortunately, both of these estimators present large dispersion, which reduces their overall quality.</p><p>We have found that, when dealing with Markovian sequences, on average, the Nemenman-Shafee-Bialek estimator <ref type="bibr" target="#b53">[54]</ref><ref type="bibr" target="#b54">[55]</ref><ref type="bibr" target="#b55">[56]</ref> outperforms the rest when taking into account both the bias and the standard deviation for both analyzed cases, namely, binary sequences and an undersampled regime. Ref. <ref type="bibr" target="#b15">[16]</ref> presented a similar analysis but for uniformly distributed sequences of bytes and bites, and concluded that the estimator with the lowest mean squared error was the Shrinkage estimator <ref type="bibr" target="#b19">[20]</ref>. Hence, when choosing a reliable estimator, it is not only important to consider the amount of data available, but also whether correlations might be present in the sequence.</p><p>Further analyses should consider Markovian sequences of higher order <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b63">64]</ref>. Another interesting topic would be systems described with continuous variables <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b65">66]</ref>, where the presence of noise is particularly important. Finally, we stress that there are alternative entropies not considered here <ref type="bibr" target="#b66">[67]</ref>, for which the existence of accurate estimators is still an open question. Finally, an exciting possibility would be a comparative study of estimators valid for more than one random variable or probability distributions, leading, respectively, to mutual informa-tion <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b68">69]</ref> and relative entropy <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b70">71]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. ACKNOWLEDGMENTS</head><p>Partial financial support has been received from the Agencia Estatal de Investigación (AEI, MCI, Spain) MCIN/AEI/10.13039/501100011033 and Fondo Europeo de Desarrollo Regional (FEDER, UE) under Project APASOS (PID2021-122256NB-C21) and the María de Maeztu Program for units of Excellence in R&amp;D, grant CEX2021-001164-M.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FIG. 1 .</head><label>1</label><figDesc>FIG. 1. Colour maps representing the bias of the nine entropy estimators reviewed in Section II for Markovian binary sequences of length N = 4. The values of the transition probabilities p(0|0) and p(1|1) vary from 0.01 to 0.99 with step ∆p = 0.02. (a) MLE [Eq. (12)], (b) Miller-Madow [48], (c) Nemenman et al. [54], (d) Chao-Shen [18], (e) Grassberger [36], (f ) Bonachela et al. [37], (g) Shrinkage [20], (h) Chao et al. [62], (i) correlation coverage-adjusted [27], (j) corrected Miller-Madow [Eq. (35)].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FIG. 2 .FIG. 4 .</head><label>24</label><figDesc>FIG. 2. Aggregated bias of the entropy estimators for Markovian binary sequences as a function of the sequence size N .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FIG. 5 .</head><label>5</label><figDesc>FIG.5. Aggregated mean squared error of the entropy estimators for Markovian binary sequences as a function of the sequence size N .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>FIG. 3 .</head><label>3</label><figDesc>FIG. 3. Colour maps representing the standard deviation of the nine entropy estimators reviewed in Section II for Markovian binary sequences of length N = 4. The values of the transition probabilities p(0|0) and p(1|1) vary from to 0.99 with step ∆p = 0.02. (a) MLE [Eq. (12)], (b) Miller-Madow [48], (c) Nemenman et al. [54], (d) Chao-Shen [18], (e) Grassberger [36], (f ) Bonachela et al. [37], (g) Shrinkage [20], (h) Chao et al. [62], (i) correlation coverage-adjusted [27], (j) corrected Miller-Madow [Eq. (35)].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>FIG. 6 .</head><label>6</label><figDesc>FIG.<ref type="bibr" target="#b5">6</ref>. Bias (top) and standard deviation (bottom) of the entropy estimators, when applied to Markovian sequences of length N = 20 and L = 26 , generated from the transition probabilities given by Equation<ref type="bibr" target="#b41">(42)</ref>, as functions of p, which vary from 0.02 to 0.5 with step ∆p = 0.02. By construction, the plot is symmetric around p = 0.5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>FIG. 7 .</head><label>7</label><figDesc>FIG. 7. Aggregated bias of the entropy estimators for Markovian sequences in the undersampled regime with L = 26 , generated from the transition probabilities given by Equation<ref type="bibr" target="#b41">(42)</ref>, as a function of the sequence size N .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>FIG. 8 .FIG. 9 .</head><label>89</label><figDesc>FIG.8. Aggregated standard deviation of the entropy estimators for Markovian sequences in the undersampled regime with L = 26 , generated from the transition probabilities given by Equation<ref type="bibr" target="#b41">(42)</ref>, as a function of the sequence size N .</figDesc></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A</head><p>In Appendix A, we introduce a new estimator ĥ0 of -p ln(p) based on the number of observations made prior to the occurrence of the result x with probability p. We improve this estimator by including all contributions resulting from the shuffling of the original series. Additionally, we show that this improved estimator ĥ has been used as a starting point to construct different estimators proposed in the literature.</p><p>Let x be a possible value, with probability p, of a random variable X. We make independent repetitions of X and define a new random variable K as the number of repetitions until the result x occurs for the first time. This random variable follows a geometric distribution:</p><p>The average value of R is</p><p>where we have used a known series expansion of the logarithm function. Hence, R is an unbiased estimator of -p ln(p) <ref type="bibr" target="#b71">[72]</ref>. By adding similar random variables R i for each possible result x i , i = 1, . . . , L, we can obtain a random variable whose average value is Shannon's entropy. This is not a contradiction with the statement that there is no known unbiased estimator of the entropy for a series of finite length, as a proper evaluation of this estimator requires the possibility of repeating infinite times the random variable. If the maximum allowed number of repetitions is N , we must modify the definition of the random variable as</p><p>It turns out that R N is negatively biased because</p><p>where</p><p>Based on this result, we introduce the following estimator ĥ0 for -p log(p): given a series S = X 1 , . . . , X N in which the symbol x appears n times, we count the set of distances (k 1 , k 2 , . . . , k n ) between successive appearances of the symbol x and then define:</p><p>The Θ function implements the condition k j ≥ 2 and the condition k j ≤ N appears naturally because of the number of data in the series. As the different points in the series are the results of independent repetitions of the random variable X, it is possible to reshuffle all points and still obtain a representative series of the process, whereas the usual MLE estimator is insensitive to this reshuffling, as it only depends on the number of appearances n, the estimator ĥ0 (S) does depend on the order of the sequence. Therefore, it is possible to improve the statistics of this estimator by including all contributions of the N ! possible permutations of the N terms of the original series. If (k</p><p>n ) is the set of distances between successive appearances of the x symbol in the i-th permutation, then we define the improved estimator ĥ</p><p>where the sum over i runs over all possible permutations of the original sequence. Our main result is to prove that this estimator can be written in terms only of n and N , namely</p><p>where ψ(z) is the digamma function, the logarithmic derivative of the gamma function. See proof in Appendix A 1 of Appendix A.</p><p>The average value of ĥ(S) is given by</p><p>where</p><p>is the probability that the element x appears n times in a sequence of length N . As proven in Appendix A 2, the average value is</p><p>which proves that ĥ is an unbiased estimator of ⟨R N ⟩.</p><p>Repeating this same procedure for every x i in the sequence with n i &gt; 0, we arrive at the entropy estimator</p><p>whose bias is the sum of the biases associated with each value of the random variable</p><p>As proven in the supplementary material of <ref type="bibr" target="#b61">[62]</ref>, ĥ(S) has been used as a starting point to construct the estimators CWJ amongst others <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b61">62]</ref>. For example, ref. <ref type="bibr" target="#b41">[42]</ref> proposes to correct ĤR in Equation (A11) by subtracting to this definition the bias in Equation (A12), replacing the values of the unknown probabilities by their estimated frequencies, p(x i ) → n i N . In <ref type="bibr" target="#b40">[41]</ref>, the correcting bias subtraction is estimated using a Bayesian approach. Finally, in <ref type="bibr" target="#b61">[62]</ref>, the authors recognized that the greatest contribution to the bias must come from the outcomes that do not appear in the sequence. Hence, they propose to correct ĤR by using an improved Good-Turing formula <ref type="bibr" target="#b72">[73]</ref> to account for the missing elements in the sequence, leading to the estimator given by Equation <ref type="bibr" target="#b24">(25)</ref>.</p><p>The novel strategy presented here to introduce the estimator ĤR emphasizes its relation with the geometric distribution and provides further insight into its significance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Proof of Equation (A7)</head><p>We prove it in three steps:</p><p>Step 1: Note that not all permutations give a different set (k</p><p>n ). There are, in fact, only N  n permutations that differ in the value of the sequence (k</p><p>n ), corresponding to the selection of the n locations of the x symbol in the sequence. Therefore, we can simplify the expression for the estimator as ĥ(S) = 1 n</p><p>where the sum over i now runs over the permutations that give rise to a different set of numbers (k</p><p>Step 2: We show that the double sum in Equation (A13) can be written as a function of n and N only,</p><p>where</p><p>We prove this relation by mathematical induction. Consider the case n = 1. The N permutations that differ in the value of k correspond to the appearance of the symbol x in the first term of the series (k = 1), the second term of the series (k = 2), and so on up to the N -th term (k = N ). The sum in the left-hand-side of Equation (A14) is</p><p>which coincides with R(1, N ), defined in Equation (A15). Assume now that Equation (A15) is valid up to 1 ≤ n ≤ N -1, and let us evaluate R(n + 1, N ). Consider all possible permutations in a sequence of length N that start with (x, . . .). The total contribution of these sequences to the value of R(n + 1, N ) is the same as having all permutations of a sequence of length N -1 with n occurrences of x (notice that the contribution of the first appearance of x is equal to 0).</p><p>We then consider all N -2 n permutations that start with (0, x, . . .), where with "0" we indicate any value which is not equal to x. That first appearance of x will contribute with a term equal to 1 for each of the permutations, and the rest will contribute the same as having all permutations of a sequence of length N -2 with n occurrences of x. Following this procedure, we have that</p><p>where the last two terms correspond to the contribution of the permutation that has all n + 1 occurrences of x at the end.</p><p>Given that we are assuming that</p><p>(A18) Changing the order of summation of the last term in Equation (A18), we can write it as</p><p>where the last equality is due to Fermat's combinatorial identity (mostly known as the hockey-stick identity). Hence, Equation (A18) can be written as</p><p>Using Pascal's identity</p><p>we obtain,</p><p>which proves Equation (A15) for 1 ≤ n ≤ N .</p><p>Step 3: We show that ĥ can finally be written as ĥ</p><p>where ψ is the digamma function.</p><p>The proof again uses mathematical induction. Consider first the case n = 1. From Equations (A6)-(A14), we derive</p><p>where the last equality is a known identity of the Harmonic numbers. Consider now that Equation (A23) holds for 1 ≤ n ≤ N -1. Let us evaluate the case n + 1:</p><p>Notice that, given our induction hypothesis,</p><p>Hence,</p><p>where for the last equality we have used the known property of the digamma function: ψ(z + 1) = ψ(z) + 1/z.</p><p>2. Calculation of the Average ⟨ ĥ(S)⟩</p><p>The average value of the estimator ĥ is</p><p>where P (n) is given in Equation (A9) and we will use the expression given in Equation (A15) for R(n, N ). Hence,</p><p>changing the order of summation we have,</p><p>the second sum of the equation above is just the binomial expansion of (p + 1 -p) N -k which is equal to 1. Then,</p><p>Consider a Markovian sequence with L = 2 n possible outcomes, z i = 0, 1, . . . , 2 n -1, defined by the following transition probabilities:</p><p>otherwise.</p><p>(B1)</p><p>We can write any z i in base 2 as</p><p>where each X j is either 0 or 1. Then, we can represent the state z i as a binary string of size n: z i ≡ (X 1 , . . . , X n ). Hence,</p><p>Reducing the modulo 2 n Equation (B3), we have 2z i (mod 2 n ) = X 2 2 n-1 + . . . + X n 2 + 0 ≡ (X 2 , . . . , X n , 0) (B4) and 2z i (mod 2 n ) + 1 = X 2 2 n-1 + . . . + X n 2 + 1 ≡ (X 2 , . . . , X n , 1) (B5)</p><p>Hence, the dynamics of this system are equivalent to a block sequence in which the block (X 1 , . . . , X n ) can only be followed by the block (X 2 , . . . , X n , 0) with probability 1 -p or by (X 2 , . . . , X n , 1) with probability p, coincident with Equation (B1).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The Apportionment of Human Diversity</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Lewontin</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-1-4684-9063-3_14</idno>
		<ptr target="https://doi.org/10.1007/978-1-4684-9063-3_14" />
	</analytic>
	<monogr>
		<title level="m">Evolutionary Biology</title>
		<editor>
			<persName><forename type="first">T</forename><surname>Dobzhansky</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Hecht</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>Steere</surname></persName>
		</editor>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1972">1972</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="381" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Cryptography: Theory and Practice, 1st</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Stinson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>CRC Press Inc</publisher>
			<pubPlace>Boca Raton, FL, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Entropy and Information in Neural Spike Trains</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Strong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Koberle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>De Ruyter Van Steveninck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bialek</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevLett.80.197</idno>
		<ptr target="https://doi.org/10.1103/PhysRevLett.80.197" />
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="197" to="200" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Maximum Entropy Modeling of Short Sequence Motifs with Applications to RNA Splicing Signals</title>
		<author>
			<persName><forename type="first">G</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Burge</surname></persName>
		</author>
		<idno type="DOI">10.1089/1066527041410418</idno>
		<ptr target="https://doi.org/10.1089/1066527041410418" />
	</analytic>
	<monogr>
		<title level="j">J. Comput. Biol. J. Comput. Mol. Cell Biol</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="377" to="394" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thomas</surname></persName>
		</author>
		<title level="m">Elements of Information Theory</title>
		<meeting><address><addrLine>Hoboken, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley and Sons</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Estimating the Shannon Entropy: Recurrence Plots versus Symbolic Dynamics</title>
		<author>
			<persName><forename type="first">C</forename><surname>Letellier</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevLett.96.254102</idno>
		<ptr target="https://doi.org/10.1103/PhysRevLett.96.254102" />
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page">254102</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Approaches to Information-Theoretic Analysis of Neural Activity</title>
		<author>
			<persName><forename type="first">J</forename><surname>Victor</surname></persName>
		</author>
		<idno type="DOI">10.1162/biot.2006.1.3.302</idno>
		<ptr target="https://doi.org/10.1162/biot.2006.1.3.302" />
	</analytic>
	<monogr>
		<title level="j">Biol. Theory</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="302" to="316" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Causality detection based on information-theoretic approaches in time series analysis</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hlaváčková-Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paluš</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vejmelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bhattacharya</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.physrep.2006.12.004</idno>
		<ptr target="https://doi.org/10.1016/j.physrep.2006.12.004" />
	</analytic>
	<monogr>
		<title level="j">Phys. Rep</title>
		<imprint>
			<biblScope unit="volume">441</biblScope>
			<biblScope unit="page" from="1" to="46" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distinguishing Noise from Chaos</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">A</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Larrondo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Plastino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Fuentes</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevLett.99.154102</idno>
		<ptr target="https://doi.org/10.1103/PhysRevLett.99.154102" />
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page">154102</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Entropy and Information Approaches to Genetic Diversity and its Expression: Genomic Geography</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Sherwin</surname></persName>
		</author>
		<idno type="DOI">10.3390/e12071765</idno>
		<ptr target="https://doi.org/10.3390/e12071765" />
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1765" to="1798" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Zanin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zunino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">A</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Papo</surname></persName>
		</author>
		<idno type="DOI">10.3390/e14081553</idno>
		<ptr target="https://doi.org/10.3390/e14081553" />
	</analytic>
	<monogr>
		<title level="j">Permutation Entropy and Its Main Biomedical and Econophysics Applications: A Review. Entropy</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1553" to="1577" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The Entropy of Words-Learnability and Expressivity across More than 1000 Languages</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bentz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Alikaniotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cysouw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ferrer-I Cancho</surname></persName>
		</author>
		<idno type="DOI">10.3390/e19060275</idno>
		<ptr target="https://doi.org/10.3390/e19060275" />
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">275</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Entropy Estimators in SAR Image Classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cassetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Delgadino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Frery</surname></persName>
		</author>
		<idno type="DOI">10.3390/e24040509</idno>
		<ptr target="https://doi.org/10.3390/e24040509" />
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page">509</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A Mathematical Theory of Communication</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
		<idno type="DOI">10.1002/j.1538-7305.1948.tb01338.x</idno>
		<ptr target="https://doi.org/10.1002/j.1538-7305.1948.tb01338.x" />
	</analytic>
	<monogr>
		<title level="j">Bell Syst. Tech. J</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="379" to="423" />
			<date type="published" when="1948">1948</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Estimation of Entropy and Mutual Information</title>
		<author>
			<persName><forename type="first">L</forename><surname>Paninski</surname></persName>
		</author>
		<idno type="DOI">10.1162/089976603321780272</idno>
		<ptr target="https://doi.org/10.1162/089976603321780272" />
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1191" to="1253" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Selecting an Effective Entropy Estimator for Short Sequences of Bits and Bytes with Maximum Entropy</title>
		<author>
			<persName><forename type="first">L</forename><surname>Contreras Rodríguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Madarro-Capó</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Legón-Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Rojas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sosa-Gómez</surname></persName>
		</author>
		<idno type="DOI">10.3390/e23050561</idno>
		<ptr target="https://doi.org/10.3390/e23050561" />
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">561</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Tackling the subsampling problem to infer collective properties from limited data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Levina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Priesemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zierenberg</surname></persName>
		</author>
		<idno type="DOI">10.1038/s42254-022-00532-5</idno>
		<ptr target="https://doi.org/10.1038/s42254-022-00532-5" />
	</analytic>
	<monogr>
		<title level="j">Nat. Rev. Phys</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="770" to="784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Nonparametric estimation of Shannon&apos;s diversity index when there are unseen species in sample</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Shen</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1026096204727</idno>
		<ptr target="https://doi.org/10.1023/A:1026096204727" />
	</analytic>
	<monogr>
		<title level="j">Environ. Ecol. Stat</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="429" to="443" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Coverage-adjusted entropy estimation</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">Q</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Kass</surname></persName>
		</author>
		<idno type="DOI">10.1002/sim.2942</idno>
		<ptr target="https://doi.org/10.1002/sim.2942" />
	</analytic>
	<monogr>
		<title level="m">Statistics in Medicine</title>
		<meeting><address><addrLine>Hoboken, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley &amp; Sons, Ltd</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="4039" to="4060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Entropy Inference and the James-Stein Estimator, with Application to Nonlinear Gene Association Networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Strimmer</surname></persName>
		</author>
		<idno type="DOI">10.5555/1577069.1755833</idno>
		<ptr target="https://doi.org/10.5555/1577069.1755833" />
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1469" to="1484" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Estimating the Entropy of Linguistic Distributions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cotterell</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-short.20</idno>
		<ptr target="https://doi.org/10.18653/v1/2022.acl-short.20" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland; Cedarville, OH, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-05">May 2022. 2022</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="175" to="195" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Handbook of Stochastic Methods for Physics</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Gardiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemistry and the Natural Sciences</title>
		<imprint>
			<date type="published" when="1965">1965</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin/Heidelberg, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Stochastic models for heterogeneous DNA sequences</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Churchill</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0092-8240(89)80049-7</idno>
		<ptr target="https://doi.org/10.1016/S0092-8240(89)80049-7" />
	</analytic>
	<monogr>
		<title level="j">Bull. Math. Biol</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="79" to="94" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The weather generation game: a review of stochastic weather models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Wilks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Wilby</surname></persName>
		</author>
		<idno type="DOI">10.1177/030913339902300302</idno>
		<ptr target="https://doi.org/10.1177/030913339902300302" />
	</analytic>
	<monogr>
		<title level="j">Prog. Phys. Geogr. Earth Environ</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="329" to="357" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Markov Processes: Linguistics and Zipf&apos;s Law</title>
		<author>
			<persName><forename type="first">I</forename><surname>Kanter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Kessler</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevLett.74.4559</idno>
		<ptr target="https://doi.org/10.1103/PhysRevLett.74.4559" />
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="4559" to="4562" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Regularities unseen, randomness observed: Levels of entropy convergence</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Crutchfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Feldman</surname></persName>
		</author>
		<idno type="DOI">10.1063/1.1530990</idno>
		<ptr target="https://doi.org/10.1063/1.1530990" />
	</analytic>
	<monogr>
		<title level="j">Chaos Interdiscip. J. Nonlinear Sci</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="25" to="54" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An improved estimator of Shannon entropy with applications to systems with memory</title>
		<author>
			<persName><forename type="first">J</forename><surname>De Gregorio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Toral</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.chaos.2022.112797</idno>
		<ptr target="https://doi.org/10.1016/j.chaos.2022.112797" />
	</analytic>
	<monogr>
		<title level="j">Chaos Solitons Fractals</title>
		<imprint>
			<biblScope unit="volume">165</biblScope>
			<biblScope unit="page">112797</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Regular and stochastic behavior of Parkinsonian pathological tremor signals</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Yulmetyev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Demin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">Y</forename><surname>Panischev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hänggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Timashev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">V</forename><surname>Vstovsky</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.physa.2006.01.077</idno>
		<ptr target="https://doi.org/10.1016/j.physa.2006.01.077" />
	</analytic>
	<monogr>
		<title level="j">Phys. Stat. Mech. Appl</title>
		<imprint>
			<biblScope unit="volume">369</biblScope>
			<biblScope unit="page" from="655" to="678" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A high-order hidden Markov model for emotion detection from textual data. In Pacific Rim Knowledge Acquisition Workshop</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Cao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Berlin/Heidelberg</publisher>
			<biblScope unit="page" from="94" to="105" />
			<pubPlace>Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Parsimonious higher-order hidden Markov models for improved array-CGH analysis with applications to Arabidopsis Thaliana</title>
		<author>
			<persName><forename type="first">M</forename><surname>Seifert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gohr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Strickert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Grosse</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pcbi.1002286</idno>
		<ptr target="https://doi.org/10.1371/journal.pcbi.1002286" />
	</analytic>
	<monogr>
		<title level="j">PLoS Comput. Biol</title>
		<imprint>
			<biblScope unit="page" from="8" to="e1002286" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Detecting memory and structure in human navigation patterns using Markov chain models of varying order</title>
		<author>
			<persName><forename type="first">P</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Helic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Taraghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Strohmaier</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0114952</idno>
		<ptr target="https://doi.org/10.1371/journal.pone.0114952" />
	</analytic>
	<monogr>
		<title level="j">PloS ONE</title>
		<imprint>
			<biblScope unit="volume">2014</biblScope>
			<biblScope unit="page" from="9" to="e102070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Optimal Non-Markovian Search Strategies with n-Step Memory</title>
		<author>
			<persName><forename type="first">H</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rieger</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevLett.127.070601</idno>
		<ptr target="https://doi.org/10.1103/PhysRevLett.127.070601" />
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="page">70601</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Selecting Markov chain orders for generating daily precipitation series across different Köppen climate regimes</title>
		<author>
			<persName><forename type="first">Wilson</forename><surname>Kemsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Dorling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<idno type="DOI">10.1002/joc.7175</idno>
		<ptr target="https://doi.org/10.1002/joc.7175" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Climatol</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="6223" to="6237" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Measures of Dispersion and Serial Dependence in Categorical Time Series</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Weiß</surname></persName>
		</author>
		<idno type="DOI">10.3390/econometrics7020017</idno>
		<ptr target="https://doi.org/10.3390/econometrics7020017" />
	</analytic>
	<monogr>
		<title level="j">Econometrics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">On a Markov multinomial distribution</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Sci</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="40" to="49" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Grassberger</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.physics/0307138</idno>
		<idno type="arXiv">arXiv:2301.13647</idno>
		<ptr target="https://doi.org/10.48550/arXiv.physics/0307138" />
		<title level="m">Entropy Estimates from Insufficient Samplings. arXiv</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Entropy estimates of small data sets</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Bonachela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hinrichsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Muñoz</surname></persName>
		</author>
		<idno type="DOI">10.1088/1751-8113/41/20/202001</idno>
		<ptr target="https://doi.org/10.1088/1751-8113/41/20/202001" />
	</analytic>
	<monogr>
		<title level="j">J. Phys. Math. Theor</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<date type="published" when="2008">2008. 202001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Number of successes in Markov trials</title>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">N</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lal</surname></persName>
		</author>
		<idno type="DOI">10.2307/1427041</idno>
		<ptr target="https://doi.org/10.2307/1427041" />
	</analytic>
	<monogr>
		<title level="j">Adv. Appl. Probab</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="677" to="680" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Estimation of the size of a closed population when capture probabilities vary among animals</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Burnham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Overton</surname></persName>
		</author>
		<idno type="DOI">10.1093/biomet/65.3.625</idno>
		<ptr target="https://doi.org/10.1093/biomet/65.3.625" />
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="625" to="633" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Estimating functions of probability distributions from a finite set of samples</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Wolpert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevE.52.6841</idno>
		<ptr target="https://doi.org/10.1103/PhysRevE.52.6841" />
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. E</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="6841" to="6854" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Estimation of the entropy based on its polynomial representation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Vinck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">B</forename><surname>Balakirsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J H</forename><surname>Vinck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M A</forename><surname>Pennartz</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevE.85.051139</idno>
		<ptr target="https://doi.org/10.1103/PhysRevE.85.051139" />
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. E</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page">51139</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Entropy Estimation in Turing&apos;s Perspective</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1162/NECO_a_00266</idno>
		<ptr target="https://doi.org/10.1162/NECO_a_00266" />
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1368" to="1389" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Bayesian entropy estimation for binary spike train data using parametric prior knowledge</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Archer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Pillow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems; Burges</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</editor>
		<meeting><address><addrLine>NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates, Inc.: Red Hook</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Estimating Functions of Distributions Defined over Spaces of Unknown Size. Entropy 2013</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Wolpert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dedeo</surname></persName>
		</author>
		<idno type="DOI">10.3390/e15114668</idno>
		<ptr target="https://doi.org/10.3390/e15114668" />
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="4668" to="4699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Estimating the Unseen: Improved Estimators for Entropy and Other Properties</title>
		<author>
			<persName><forename type="first">G</forename><surname>Valiant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valiant</surname></persName>
		</author>
		<idno type="DOI">10.1145/3125643</idno>
		<ptr target="https://doi.org/10.1145/3125643" />
	</analytic>
	<monogr>
		<title level="j">Assoc. Comput. Mach</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page">41</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">On Generalized Schürmann Entropy Estimators</title>
		<author>
			<persName><forename type="first">P</forename><surname>Grassberger</surname></persName>
		</author>
		<idno type="DOI">10.3390/e24050680</idno>
		<ptr target="https://doi.org/10.3390/e24050680" />
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page">680</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Bayesian estimation of information-theoretic metrics for sparsely sampled distributions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Piga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Font-Pomarol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sales-Pardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Guimerà</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2301.13647</idno>
		<idno type="arXiv">arXiv:2301.13647</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2301.13647" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Note on the bias of information estimates</title>
		<author>
			<persName><forename type="first">G</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Theory Psychol. Probl. Methods</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page">108</biblScope>
			<date type="published" when="1955">1955</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Bias analysis in entropy estimation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schürmann</surname></persName>
		</author>
		<idno type="DOI">10.1088/0305-4470/37/27/l02</idno>
		<ptr target="https://doi.org/10.1088/0305-4470/37/27/l02" />
	</analytic>
	<monogr>
		<title level="j">J. Phys. Math. Gen</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="295" to="L301" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Some Problems of Simultaneous Minimax Estimation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Trybula</surname></persName>
		</author>
		<idno type="DOI">10.1214/aoms/1177706722</idno>
		<ptr target="https://doi.org/10.1214/aoms/1177706722" />
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Stat</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="245" to="253" />
			<date type="published" when="1958">1958</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The performance of universal encoding</title>
		<author>
			<persName><forename type="first">R</forename><surname>Krichevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Trofimov</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIT.1981.1056331</idno>
		<ptr target="https://doi.org/10.1109/TIT.1981.1056331" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="199" to="207" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Entropy estimation of symbol sequences</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schürmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Grassberger</surname></persName>
		</author>
		<idno type="DOI">10.1063/1.166191</idno>
		<ptr target="https://doi.org/10.1063/1.166191" />
	</analytic>
	<monogr>
		<title level="j">Chaos Interdiscip. J. Nonlinear Sci</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="414" to="427" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Bayes&apos; estimators of generalized entropies</title>
		<author>
			<persName><forename type="first">D</forename><surname>Holste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Große</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Herzel</surname></persName>
		</author>
		<idno type="DOI">10.1088/0305-4470/31/11/007</idno>
		<ptr target="https://doi.org/10.1088/0305-4470/31/11/007" />
	</analytic>
	<monogr>
		<title level="j">J. Phys. Math. Gen</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">2551</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Entropy and Inference, Revisited</title>
		<author>
			<persName><forename type="first">I</forename><surname>Nemenman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shafee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bialek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">T</forename><surname>Dietterich</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Becker</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Entropy and information in neural spike trains: Progress on the sampling problem</title>
		<author>
			<persName><forename type="first">I</forename><surname>Nemenman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bialek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>De Ruyter Van Steveninck</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevE.69.056111</idno>
		<ptr target="https://doi.org/10.1103/PhysRevE.69.056111" />
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. E</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page">56111</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Coincidences and Estimation of Entropies of Random Variables with Large Cardinalities. Entropy</title>
		<author>
			<persName><forename type="first">I</forename><surname>Nemenman</surname></persName>
		</author>
		<idno type="DOI">10.3390/e13122013</idno>
		<ptr target="https://doi.org/10.3390/e13122013" />
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="2013" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<ptr target="https://github.com/simomarsili/ndd" />
		<title level="m">Bayesian Entropy Estimation from Discrete Data. 2021. Available online</title>
		<imprint>
			<date type="published" when="2024-01-17">17 January 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A Generalization of Sampling without Replacement from a Finite Universe</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Horvitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Thompson</surname></persName>
		</author>
		<idno type="DOI">10.1080/01621459.1952.10483446</idno>
		<ptr target="https://doi.org/10.1080/01621459.1952.10483446" />
	</analytic>
	<monogr>
		<title level="j">J. Am. Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="663" to="685" />
			<date type="published" when="1952">1952</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">On measures of entropy and information</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rényi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics Probability</title>
		<meeting>the Fourth Berkeley Symposium on Mathematical Statistics Probability<address><addrLine>Oakland, CA, USA; Oakland, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>University of California Press</publisher>
			<date type="published" when="1961-07-30">20-30 July 1961. 1961</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="547" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Improving Efficiency by Shrinkage: The James-Stein and Ridge Regression Estimators</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H J</forename><surname>Gruber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<pubPlace>London, UK</pubPlace>
		</imprint>
	</monogr>
	<note>Routledge</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A Shrinkage Approach to Large-Scale Covariance Matrix Estimation and Implications for Functional Genomics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Strimmer</surname></persName>
		</author>
		<idno type="DOI">10.2202/1544-6115.1175</idno>
		<ptr target="https://doi.org/10.2202/1544-6115.1175" />
	</analytic>
	<monogr>
		<title level="j">Stat. Appl. Genet. Mol. Biol</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Entropy and the species accumulation curve: a novel entropy estimator via discovery rates of new species</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jost</surname></persName>
		</author>
		<idno type="DOI">10.1111/2041-210X.12108</idno>
		<ptr target="https://doi.org/10.1111/2041-210X.12108" />
	</analytic>
	<monogr>
		<title level="j">Methods Ecol. Evol</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1091" to="1100" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">A model for high-order Markov chains</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Raftery</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.2517-6161.1985.tb01383.x</idno>
		<ptr target="https://doi.org/10.1111/j.2517-6161.1985.tb01383.x" />
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. Ser. Stat. Methodol</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="528" to="539" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Inferring Markov chains: Bayesian estimation, model comparison, entropy rate, and out-of-class modeling</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Strelioff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Crutchfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Hübler</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevE.76.011106</idno>
		<ptr target="https://doi.org/10.1103/PhysRevE.76.011106" />
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. E</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page">11106</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Estimating the entropy of a signal with applications</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Bercher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Vignat</surname></persName>
		</author>
		<idno type="DOI">10.1109/78.845926</idno>
		<ptr target="https://doi.org/10.1109/78.845926" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1687" to="1694" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">A review of Shannon and differential entropy rate estimation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Feutrill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Roughan</surname></persName>
		</author>
		<idno type="DOI">10.3390/e23081046</idno>
		<ptr target="https://doi.org/10.3390/e23081046" />
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">1046</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Generalised information and entropy measures in physics</title>
		<author>
			<persName><forename type="first">C</forename><surname>Beck</surname></persName>
		</author>
		<idno type="DOI">10.1080/00107510902823517</idno>
		<ptr target="https://doi.org/10.1080/00107510902823517" />
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="495" to="510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Estimating mutual information</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kraskov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Stögbauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Grassberger</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevE.69.066138</idno>
		<ptr target="https://doi.org/10.1103/PhysRevE.69.066138" />
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. E</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page">66138</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Estimation of mutual information: A survey</title>
		<author>
			<persName><forename type="first">J</forename><surname>Walters-Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Rough Sets and Knowledge Technology: 4th International Conference</title>
		<meeting>the Rough Sets and Knowledge Technology: 4th International Conference<address><addrLine>Gold Coast, Australia; Berlin/Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Germany</publisher>
			<date type="published" when="2009-07-16">2009. 14-16 July 2009. 2009</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="389" to="396" />
		</imprint>
	</monogr>
	<note>RSKT</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Some properties of a type of the entropy of an ideal and the divergence of two ideals</title>
		<author>
			<persName><forename type="first">N</forename><surname>Minculete</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Savin</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2305.07975</idno>
		<idno type="arXiv">arXiv:2305.07975</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2305.07975" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Bayesian estimation of the Kullback-Leibler divergence for categorical sytems using mixtures of Dirichlet priors</title>
		<author>
			<persName><forename type="first">F</forename><surname>Camaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Nemenman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Walczak</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2307.04201</idno>
		<idno type="arXiv">arXiv:2307.04201</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2307.04201" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Montgomery-Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schürmann</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1410.5002</idno>
		<idno type="arXiv">arXiv:1410.5002</idno>
		<ptr target="https://doi.org/10.48550/arXiv.1410.5002" />
		<title level="m">Unbiased Estimators for Entropy and Class Number. arXiv</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">The population frequencies of species and the estimation of population parameters</title>
		<author>
			<persName><forename type="first">I</forename><surname>Good</surname></persName>
		</author>
		<idno type="DOI">10.2307/2333344</idno>
		<ptr target="https://doi.org/10.2307/2333344" />
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="237" to="264" />
			<date type="published" when="1953">1953</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
