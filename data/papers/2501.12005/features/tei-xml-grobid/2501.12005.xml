<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A note on the relations between mixture models, maximum-likelihood and entropic optimal transport</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-01-24">January 24, 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Titouan</forename><surname>Vayer</surname></persName>
							<email>titouan.vayer@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">UMR 5668</orgName>
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">ENS de Lyon</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">UCBL, LIP</orgName>
								<orgName type="institution" key="instit5">ENS de Lyon</orgName>
								<orgName type="institution" key="instit6">CNRS</orgName>
								<orgName type="institution" key="instit7">UCBL</orgName>
								<address>
									<region>Inria</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Etienne</forename><surname>Lasalle</surname></persName>
							<email>etienne.lasalle@ens-lyon.fr</email>
							<affiliation key="aff1">
								<orgName type="laboratory" key="lab1">LIP</orgName>
								<orgName type="laboratory" key="lab2">UMR 5668</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A note on the relations between mixture models, maximum-likelihood and entropic optimal transport</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-01-24">January 24, 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">E60CE91277E8645700FCCB08BCCE51A9</idno>
					<idno type="arXiv">arXiv:2501.12005v2[stat.ML]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This note aims to demonstrate that performing maximum-likelihood estimation for a mixture model is equivalent to minimizing over the parameters an optimal transport problem with entropic regularization. The objective is pedagogical: we seek to present this already known result in a concise and hopefully simple manner. We give an illustration with Gaussian mixture models by showing that the standard EM algorithm is a specific block-coordinate descent on an optimal transport loss.</p><p>Notations. Any vector x ∈ R K is treated as column matrix. The discrete probability simplex with K bins is noted as ∆ K = {a ∈ R K + : K j=1 a j = 1}. The vector of K ones is denoted as 1 K . δ x is the dirac mass supported at x. For simplicity in this note, we will deliberately remain vague in certain places regarding "edge cases" like 0 log 0. To be entirely rigorous, we would need to reason on the supports of the matrices and define objects that can take ∞ as a value.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction and preliminaries on optimal transport</head><p>The relations between maximum-likelihood and optimal transport (OT) have already been discussed in multiple works <ref type="bibr" target="#b0">(Rigollet and Weed, 2018;</ref><ref type="bibr" target="#b1">Mena et al., 2020;</ref><ref type="bibr" target="#b2">Diebold et al., 2024)</ref>. The purpose of this brief note is to provide the key tools used to establish these connections. The primary aim is pedagogical: we will focus on the (discrete) mixtures case, adopting a "computational OT" perspective. Hopefully, readers will find this exercise insightful. Our analysis will largely rely on the approach described in <ref type="bibr" target="#b0">Rigollet and Weed (2018)</ref>, though adapted to a different formalism and applied to a slightly different problem (mixture estimation rather than Gaussian deconvolution).</p><p>To fix the notations, we first briefly recall the fundamentals of (entropic) discrete optimal transport (OT) and readers seeking more details can refer to <ref type="bibr" target="#b3">Peyré et al. (2019)</ref>. Let C ∈ R n×K be a cost matrix, representing, for example, the distances between points from two distributions. Let a ∈ ∆ n , b ∈ ∆ K be two probability vectors, encoding for instance the quantities to be transported and supplied, respectively. The goal of OT is to determine a way to move these quantities while respecting supply constraints, such that the total transportation cost is minimized (defined by C). This is formalized through the set of couplings, or transport plans, with marginals a, b, which is defined by</p><formula xml:id="formula_0">U(a, b) {P ∈ R n×K + : P1 K = a, P ⊤ 1 n = b} .</formula><p>(1.1)</p><formula xml:id="formula_1">When P ∈ U(a, b), P ij ∈ [0, 1]</formula><p>represents the of probability mass transported from the i-th point to the j-th. When we only care about transporting the mass of the input measure without constraint on the supply we can consider semi-relaxed transport plans as</p><formula xml:id="formula_2">U K (a) {P ∈ R n×K + : P1 K = a} . (1.2)</formula><p>Given a transport plan P, the total cost of displacement is given by C, P and the goal of standard OT is to find the transport plan that minimizes this cost. Entropic regularization, introduced in Cuturi ( <ref type="formula">2013</ref>), was proposed to accelerate the computation of the optimal transport plan. It relies on the Kullback-Leibler divergence between two matrices P, Q ∈ R n×K + which is defined as</p><formula xml:id="formula_3">KL(P|Q) n i=1 K j=1 P ij log P ij Q ij . (1.3)</formula><p>The entropic-regularized optimal transport problem (EOT) is expressed as</p><formula xml:id="formula_4">OT ε (a, b, C) min P∈U (a,b) C, P + εKL(P|ab ⊤ ) ,<label>(1.4)</label></formula><p>In (1.4), the goal is to find the transport plan that minimizes a trade-off between the transport cost and a measure of "distance" to the uniform coupling ab ⊤ , which distributes every source point to every target point uniformly. When P is a semi-relaxed transport plan the problem (1.4) will be called a semi-relaxed entropic OT problem.</p><p>2 Maximum-likelihood for mixture models is minimization of EOT</p><p>In this note, we consider a mixture model as described below.</p><p>Definition 2.1 ((Discrete) mixture model). The generative process of a discrete mixture model consists in</p><p>• Y ∼ P Y where P Y = K j=1 π ⋆ j δ j with π ⋆ ∈ ∆ K represents the discrete distribution on K labels/classes. In other words, P Y is the distribution of the "latent variables".</p><p>• X|y = j ∼ P X|Y (•|j, θ ⋆ ), where θ ⋆ ∈ Θ and P X|Y is the parametrized distribution of the data given the label.</p><p>We note P X,Y the corresponding joint distribution. A simple example of this generative process is the Gaussian mixture model where the parametrized distribution has density</p><formula xml:id="formula_5">P X|Y (x|j, θ ⋆ ) ∝ exp(-1 2 (x -µ ⋆ j ) ⊤ Σ ⋆-1 (x -µ ⋆ j ))</formula><p>where µ ⋆ j ∈ R d is the true mean associated to the j-th class and Σ ⋆ ≻ 0 the true covariance (assumed to be identical for each class). In this case θ</p><formula xml:id="formula_6">⋆ = (µ ⋆ 1 , • • • , µ ⋆ K , Σ ⋆</formula><p>). Now suppose that we observe some samples x 1 , • • • , x n ∼ P X i.i.d. where P X is the distribution of the data, according to the generative model above. The goal of maximum-likelihood estimation is to infer the parameters π ⋆ , θ ⋆ from these observations. By independence, the negative log-likelihood for a parameter β = (θ, π) writes</p><formula xml:id="formula_7">L(β) = - n i=1 log P X (x i |β) ⋆ = - n i=1 log K j=1 P X,Y (x i , j|β) ⋆⋆ = - n i=1 log K j=1 π j P X|Y (x i |j, θ) = - n i=1 log K j=1 π j exp(log(P X|Y (x i |j, θ)) .</formula><p>(2.1)</p><p>In ⋆ we used the law of total probability and in ⋆⋆ we used the Bayes' formula. In the last line we use a (at first glance) stupid reparametrization.</p><p>We will prove three facts: first, the negative log-likelihood (2.1) can be rewritten as a certain semi-relaxed entropic OT problem; second, there is an entropic OT problem that is an upper bound for the negative log-likelihood; and third, minimizing L(β) with respect to π results in equality with this upper bound.</p><p>The key result to make the connections between OT and log-likelihood is to rewrite the "logsumexp" term as a minimization problem over the probability simplex. This is next in the following lemma, which is sometimes referred to as the Gibbs variational principle or the dual formulation of the KL divergence.</p><formula xml:id="formula_8">Lemma 2.2. Let π 1 , • • • , π K be positive real numbers and h 1 , • • • , h K ∈ R. Then log K j=1 π j exp(h j ) = max p∈∆ K K j=1 h j p j - K j=1 p j log( p j π j ) .</formula><p>The optimal solution is given by ∀k</p><formula xml:id="formula_9">∈ [[K]], p k = π k exp(h k ) K j=1 π j exp(h j )</formula><p>. For now, we postpone the proof of this result (see Section 5) but we can use it to rewrite the negative log-likelihood. Combining (2.1) and Lemma 2.2 with log(P X|Y (x i |j, β)) in the role of h j we obtain</p><formula xml:id="formula_10">L(β) = - n i=1 max p∈∆ K K j=1 log(P X|Y (x i |j, θ))p j - K j=1 p j log( p j π j ) = - max p (1) ,••• ,p (n) ∈∆ K n i=1 K j=1 log(P X|Y (x i |j, θ))p (i) j - n i=1 K j=1 p (i) j log( p (i) j π j ) .</formula><p>Equivalently,</p><formula xml:id="formula_11">1 n L(β) = min p (1) ,••• ,p (n) ∈∆ K i,j C ij (θ) p (i) j n + i,j p (i) j n log( p (i) j /n π j /n ) ,</formula><p>where we introduced C ij (θ) -log(P X|Y (x i |j, θ)). Now suppose that we have solutions p (1) , • • • , p (n) of the minimization problem above and that we consider</p><formula xml:id="formula_12">P = 1 n (p (1) , • • • , p (n) ) ⊤ ∈ R n×K + . Then obviously P1 K = 1 n 1 n . Conversely, any matrix P ∈ R n×K + with P1 K = 1 n 1 n can be written as P = 1 n (p (1) , • • • , p (n)</formula><p>) ⊤ for some probability vectors p (1) , • • • , p (n) ∈ ∆ K (which are simply the rows of P). In other words, this proves that</p><formula xml:id="formula_13">1 n L(β) = min P∈R n×K + P1 K = 1 n 1n i,j C ij (θ)P ij + i,j P ij log( P ij π j /n ) = min P∈U K ( 1n n ) C(θ), P + KL(P| 1 n n π ⊤ ) .</formula><p>(2.2)</p><p>We almost have the desired entropic OT problem, albeit with a "semi-relaxed" constraint instead of a standard coupling constraint: if P were in U( 1n n , π) we would be done. To obtain an entropic OT problem we only need to rewrite a bit the quantity above: we show that minimizing the negative log-likelihood with respect to π leads to a coupling constraint rather than a semi-relaxed one. First, for any admissible P ∈ U K ( 1n n ) of the minimization problem (2.2), we have (see Lemma 5.2) KL(P|</p><formula xml:id="formula_14">1 n n π ⊤ ) = KL P| 1 n n (P ⊤ 1 n ) ⊤ + KL(P ⊤ 1 n |π) . (2.3) This implies that 1 n L(β) = min P∈ U K ( 1n n ) C(θ), P + KL P| 1 n n (P ⊤ 1 n ) ⊤ + KL(P ⊤ 1 n |π) .</formula><p>(2.4)</p><p>Now consider P ⋆ the solution of the entropic OT problem min P∈U ( 1 n 1n,π) C(θ), P +KL(P| 1n n π ⊤ ). By the constraints, P ⋆ ∈ U K ( 1 n 1 n ) and KL(P ⋆⊤ 1 n |π) = 0. Thus, by suboptimality in (2.4)</p><formula xml:id="formula_15">1 n L(β) ≤ C(θ), P ⋆ + KL P ⋆ | 1n n (P ⋆⊤ 1 n ) ⊤ .</formula><p>Hence we first obtain an upper-bound on the negative log-likelihood:</p><formula xml:id="formula_16">1 n L(β) ≤ OT ε=1 ( 1 n n , π, C(θ)) . (2.5)</formula><p>To obtain an equality we will minimize with respect to π. Using that min b∈∆ K KL(a|b) = KL(a|a) = 0, if we minimize the RHS in (2.4) over π we get,</p><formula xml:id="formula_17">min π∈∆ K min P∈ U K ( 1n n ) C(θ), P + KL P| 1 n n (P ⊤ 1 n ) ⊤ + KL(P ⊤ 1 n |π) = min P∈ U K ( 1n n ) C(θ), P + KL P| 1 n n (P ⊤ 1 n ) ⊤ ⋆ = min π∈∆ K min P∈U ( 1 n 1n,π) C(θ), P + KL(P| 1 n n π ⊤ ) = min π∈∆ K OT ε=1 ( 1 n n , π, C(θ)) .</formula><p>For the equality in ⋆ we used that the second marginal in the optimization problem is redundant in the RHS. Precisely, if P 0 is a solution of the LHS then b 0</p><formula xml:id="formula_18">= P ⊤ 0 1 n ∈ ∆ K thus P 0 ∈ U( 1n n , b 0 ) and KL P 0 | 1n n (P ⊤ 0 1 n ) ⊤ = KL P 0 | 1n n b ⊤ 0 which implies that LHS ≥ RHS. Conversely, if (π 1 , P 1 ) is a solution of the RHS problem then P 1 ∈ U K ( 1n n ) and π 1 = P ⊤ 1 1 n , hence LHS ≤ RHS.</formula><p>In particular for any θ ∈ Θ, min π</p><formula xml:id="formula_19">1 n L(π, θ) = min π OT ε=1 ( 1n n , π, C(θ)</formula><p>). This gives the final result written below.</p><p>Proposition 2.3 (MLE for mixture models is minimization of an EOT problem). Consider a mixture model as in Definition 2.1 and L the negative log-likelihood on n i.i.d. samples. First, we have the identity</p><formula xml:id="formula_20">∀(π, θ) ∈ ∆ K × Θ, 1 n θ) = min P∈U K ( 1n n ) C(θ), P + KL(P| 1 n n π ⊤ ) , (2.6)</formula><p>where the cost matrix is C(θ) -log P X|Y (x i |j, θ) ij . Second, the EOT problem is a upperbound of the negative log-likelihood:</p><formula xml:id="formula_21">∀(π, θ) ∈ ∆ K × Θ, 1 n L(π, θ) ≤ OT ε=1 ( 1 n n , π, C(θ)) .</formula><p>(2.7)</p><p>Third, minimizing the negative log-likelihood with respect to the parameters is equivalent to minimizing over the parameters an EOT problem. Precisely,</p><formula xml:id="formula_22">min (π,θ)∈∆ K ×Θ 1 n L(π, θ) = min (π,θ)∈∆ K ×Θ OT ε=1 ( 1 n n , π, C(θ)) .</formula><p>(2.8)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Illustration with Gaussian Mixture Models</head><p>To illustrate the previous results we will show that the updates of the Expectation-Maximization algorithm (EM) for the Gaussian mixture model (GMM) can be interpreted as a block-coordinate descent (BCD) on the EOT loss (see e.g. <ref type="bibr">Murphy 2012, Section 11.4</ref> for a description of the EM algorithm). According to Proposition 2.3 minimizing the negative log-likelihood is equivalent to solving min</p><formula xml:id="formula_23">(π,θ)∈∆ K ×Θ min P∈U K ( 1n n ) C(θ), P + KL(P| 1 n n π ⊤ ) . (3.1)</formula><p>The BCD strategy consists in alternating between minimizing (3.1) in π, θ, P while keeping the other variables fixed.</p><p>The update of P with π, θ fixed consists in solving a semi-relaxed entropic OT problem. As described e.g. in <ref type="bibr" target="#b6">Flamary et al. (2016)</ref> (or also in the proof of Proposition 2.3) the problem decouples with respect to the rows of P and the solution is given by</p><formula xml:id="formula_24">∀(i, j) ∈ [[n]] × [[K]], P ij = 1 n π j exp(-C ij (θ)) K k=1 π k exp(-C ik (θ)) = 1 n π j P X|Y (x|j, θ) K k=1 π k P X|Y (x|k, θ) . (3.2)</formula><p>This step actually corresponds to finding the conditional distribution in the "E step" of the EM algorithm. The update for π, with P held fixed, can be found using (2.3) and is simply</p><formula xml:id="formula_25">π = P ⊤ 1 n i.e. ∀j ∈ [[K]], π j = n i=1 P ij . (3.3)</formula><p>Finally we derive the update of θ in the GMM case. We consider</p><formula xml:id="formula_26">P X|Y (x|j, θ) = (2π) -d/2 det Σ -1/2 exp - 1 2 (x -µ j ) ⊤ Σ -1 (x -µ j ) ,</formula><p>and the goal is to update θ = (µ 1 , • • • , µ K , Σ) with µ j ∈ R d and Σ ≻ 0. With other variables fixed this boils down to solving min</p><formula xml:id="formula_27">(µ 1 ,••• ,µ K ,Σ) 1 2 ij (x i -µ j ) ⊤ Σ -1 (x i -µ j )P ij + n 2 log det Σ .</formula><p>Setting the gradient of this loss to zero, one can show that the update of Σ (with (µ <ref type="bibr">(3.4)</ref> and the update of the means (with Σ fixed) are</p><formula xml:id="formula_28">1 , • • • , µ K ) fixed) reads Σ = 1 n ij P ij (x i -µ j )(x i -µ j ) ⊤ ,</formula><formula xml:id="formula_29">∀j ∈ [[K]], µ j = 1 i P ij i P ij x i . (3.5)</formula><p>These updates exactly corresponds to the updates of the EM algorithm apply to a GMM: first update P according to (3.2) which is the "E step", then the proportion of the classes with (3.3) and the means and covariance with (3.4) and (3.5), which is the "M step".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussions</head><p>To finish we make a few comments. First, the proof described above can be easily generalized to infinite mixtures, with appropriate assumptions on P Y , see e.g. <ref type="bibr">Rigollet and Weed (2018, Definition 2)</ref>. With these assumptions we would obtain that minimizing the negative loglikelihood is equivalent to solving a problem of the form inf (P Y ,θ) OT ε=1 (P Y , 1 n n i=1 δ x i ; θ) where the cost of the OT is c(x, y; θ) = -log(P X|Y (x|y, θ)).</p><p>Also, as discussed in <ref type="bibr" target="#b1">Mena et al. (2020)</ref>, the same relations between negative log-likelihood and entropic OT can be obtained for more general generative models where P Y are P X are coupled via a joint distribution</p><formula xml:id="formula_30">Q θ ⋆ X,Y (such as dQ θ ⋆ X,Y (x, y) = exp(-g θ ⋆ (x, y))dP X (x)dP Y (y)</formula><p>). This setting encompasses the GMM case and the principle of the proof remains similar to the one described here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Postponed proofs</head><formula xml:id="formula_31">Lemma 5.1. Let π 1 , • • • , π K be positive real numbers and h 1 , • • • , h K ∈ R. Then log K j=1 π j exp(h j ) = max p∈∆ K K j=1 h j p j - K j=1 p j log( p j π j ) .</formula><p>The optimal solution is given by ∀k</p><formula xml:id="formula_32">∈ [[K]], p k = π k exp(h k ) K j=1 π j exp(h j ) .</formula><p>Proof. The optimization problem above is a maximization of a strictly concave function. Consider the Lagrangian L(p, λ) = j h j p jj p j log( p j π j ) + λ( j p j -1). Then ∂ p k L(p, λ) = h k -log(p k /π k ) -1 + λ thus ∂ p k L(p, λ) = 0 ⇐⇒ p k = exp(λ -1)π k exp(h k ). By primal constraints j p j = 1 =⇒ exp(λ -1) = 1 j exp(h j )π j . This gives the desired optimal solution. Also ∀k ∈ [[K]], log(p k /π k ) = h k -log( K j=1 π j exp(h j )) hence j p j log(p j /π j ) = j h j p j -log( K j=1 π j exp(h j )) j p j . Using that j p j = 1 gives the result. We also used the following result about the KL divergence.</p><p>Lemma 5.2. Let P ∈ R n×K + be a matrix and a ∈ R n + , b ∈ R K + then KL(P|ab ⊤ ) = KL P|(P1 K )(P ⊤ 1 n ) ⊤ + KL(P1 K |a) + KL(P ⊤ 1 n |b) .</p><p>(5.1)</p><formula xml:id="formula_33">Proof. By definition KL(P|ab ⊤ ) = ij P ij log( P ij a i b j ) = ij P ij log( P ij (P1 k ) i (P ⊤ 1 n ) j (P1 k ) i (P ⊤ 1 n ) j a i b j ) = KL P|(P1 K )(P ⊤ 1 n ) ⊤ + ij P ij log( (P1 k ) i a i ) + ij P ij log( (P ⊤ 1 n ) j b j ) = KL P|(P1 K )(P ⊤ 1 n ) ⊤ + i (P1 k ) i log( (P1 k ) i a i ) + j (P ⊤ 1 n ) j log( (P ⊤ 1 n ) j b j</formula><p>) .</p></div>		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Entropic optimal transport is maximum-likelihood deconvolution</title>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Rigollet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Weed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comptes Rendus. Mathématique</title>
		<imprint>
			<biblScope unit="volume">356</biblScope>
			<biblScope unit="issue">11-12</biblScope>
			<biblScope unit="page" from="1228" to="1235" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Sinkhorn em: an expectation-maximization algorithm based on entropic optimal transport</title>
		<author>
			<persName><forename type="first">Gonzalo</forename><surname>Mena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amin</forename><surname>Nejatbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erdem</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Niles-Weed</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.16548</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A unified framework for hard and soft clustering with regularized optimal transport</title>
		<author>
			<persName><forename type="first">Jean-Frédéric</forename><surname>Diebold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papadakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnaud</forename><surname>Dessein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles-Alban</forename><surname>Deledalle</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1711.04366" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Computational optimal transport: With applications to data science</title>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Peyré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends® in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="355" to="607" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sinkhorn distances: Lightspeed computation of optimal transport</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Machine learning: a probabilistic perspective</title>
		<author>
			<persName><forename type="first">Kevin P</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Optimal spectral transportation with application to music transcription</title>
		<author>
			<persName><forename type="first">Remi</forename><surname>Flamary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cédric Févotte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Courty</surname></persName>
		</author>
		<author>
			<persName><surname>Emyia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
