# A note on the relations between mixture models, maximum-likelihood and entropic optimal transport

## Abstract

## 

This note aims to demonstrate that performing maximum-likelihood estimation for a mixture model is equivalent to minimizing over the parameters an optimal transport problem with entropic regularization. The objective is pedagogical: we seek to present this already known result in a concise and hopefully simple manner. We give an illustration with Gaussian mixture models by showing that the standard EM algorithm is a specific block-coordinate descent on an optimal transport loss.

Notations. Any vector x ∈ R K is treated as column matrix. The discrete probability simplex with K bins is noted as ∆ K = {a ∈ R K + : K j=1 a j = 1}. The vector of K ones is denoted as 1 K . δ x is the dirac mass supported at x. For simplicity in this note, we will deliberately remain vague in certain places regarding "edge cases" like 0 log 0. To be entirely rigorous, we would need to reason on the supports of the matrices and define objects that can take ∞ as a value.

## Introduction and preliminaries on optimal transport

The relations between maximum-likelihood and optimal transport (OT) have already been discussed in multiple works [(Rigollet and Weed, 2018;](#b0)[Mena et al., 2020;](#b1)[Diebold et al., 2024)](#b2). The purpose of this brief note is to provide the key tools used to establish these connections. The primary aim is pedagogical: we will focus on the (discrete) mixtures case, adopting a "computational OT" perspective. Hopefully, readers will find this exercise insightful. Our analysis will largely rely on the approach described in [Rigollet and Weed (2018)](#b0), though adapted to a different formalism and applied to a slightly different problem (mixture estimation rather than Gaussian deconvolution).

To fix the notations, we first briefly recall the fundamentals of (entropic) discrete optimal transport (OT) and readers seeking more details can refer to [Peyré et al. (2019)](#b3). Let C ∈ R n×K be a cost matrix, representing, for example, the distances between points from two distributions. Let a ∈ ∆ n , b ∈ ∆ K be two probability vectors, encoding for instance the quantities to be transported and supplied, respectively. The goal of OT is to determine a way to move these quantities while respecting supply constraints, such that the total transportation cost is minimized (defined by C). This is formalized through the set of couplings, or transport plans, with marginals a, b, which is defined by

$U(a, b) {P ∈ R n×K + : P1 K = a, P ⊤ 1 n = b} .$(1.1)

$When P ∈ U(a, b), P ij ∈ [0, 1]$represents the of probability mass transported from the i-th point to the j-th. When we only care about transporting the mass of the input measure without constraint on the supply we can consider semi-relaxed transport plans as

$U K (a) {P ∈ R n×K + : P1 K = a} . (1.2)$Given a transport plan P, the total cost of displacement is given by C, P and the goal of standard OT is to find the transport plan that minimizes this cost. Entropic regularization, introduced in Cuturi ( [2013](#)), was proposed to accelerate the computation of the optimal transport plan. It relies on the Kullback-Leibler divergence between two matrices P, Q ∈ R n×K + which is defined as

$KL(P|Q) n i=1 K j=1 P ij log P ij Q ij . (1.3)$The entropic-regularized optimal transport problem (EOT) is expressed as

$OT ε (a, b, C) min P∈U (a,b) C, P + εKL(P|ab ⊤ ) ,(1.4)$In (1.4), the goal is to find the transport plan that minimizes a trade-off between the transport cost and a measure of "distance" to the uniform coupling ab ⊤ , which distributes every source point to every target point uniformly. When P is a semi-relaxed transport plan the problem (1.4) will be called a semi-relaxed entropic OT problem.

2 Maximum-likelihood for mixture models is minimization of EOT

In this note, we consider a mixture model as described below.

Definition 2.1 ((Discrete) mixture model). The generative process of a discrete mixture model consists in

• Y ∼ P Y where P Y = K j=1 π ⋆ j δ j with π ⋆ ∈ ∆ K represents the discrete distribution on K labels/classes. In other words, P Y is the distribution of the "latent variables".

• X|y = j ∼ P X|Y (•|j, θ ⋆ ), where θ ⋆ ∈ Θ and P X|Y is the parametrized distribution of the data given the label.

We note P X,Y the corresponding joint distribution. A simple example of this generative process is the Gaussian mixture model where the parametrized distribution has density

$P X|Y (x|j, θ ⋆ ) ∝ exp(-1 2 (x -µ ⋆ j ) ⊤ Σ ⋆-1 (x -µ ⋆ j ))$where µ ⋆ j ∈ R d is the true mean associated to the j-th class and Σ ⋆ ≻ 0 the true covariance (assumed to be identical for each class). In this case θ

$⋆ = (µ ⋆ 1 , • • • , µ ⋆ K , Σ ⋆$). Now suppose that we observe some samples x 1 , • • • , x n ∼ P X i.i.d. where P X is the distribution of the data, according to the generative model above. The goal of maximum-likelihood estimation is to infer the parameters π ⋆ , θ ⋆ from these observations. By independence, the negative log-likelihood for a parameter β = (θ, π) writes

$L(β) = - n i=1 log P X (x i |β) ⋆ = - n i=1 log K j=1 P X,Y (x i , j|β) ⋆⋆ = - n i=1 log K j=1 π j P X|Y (x i |j, θ) = - n i=1 log K j=1 π j exp(log(P X|Y (x i |j, θ)) .$(2.1)

In ⋆ we used the law of total probability and in ⋆⋆ we used the Bayes' formula. In the last line we use a (at first glance) stupid reparametrization.

We will prove three facts: first, the negative log-likelihood (2.1) can be rewritten as a certain semi-relaxed entropic OT problem; second, there is an entropic OT problem that is an upper bound for the negative log-likelihood; and third, minimizing L(β) with respect to π results in equality with this upper bound.

The key result to make the connections between OT and log-likelihood is to rewrite the "logsumexp" term as a minimization problem over the probability simplex. This is next in the following lemma, which is sometimes referred to as the Gibbs variational principle or the dual formulation of the KL divergence.

$Lemma 2.2. Let π 1 , • • • , π K be positive real numbers and h 1 , • • • , h K ∈ R. Then log K j=1 π j exp(h j ) = max p∈∆ K K j=1 h j p j - K j=1 p j log( p j π j ) .$The optimal solution is given by ∀k

$∈ [[K]], p k = π k exp(h k ) K j=1 π j exp(h j )$. For now, we postpone the proof of this result (see Section 5) but we can use it to rewrite the negative log-likelihood. Combining (2.1) and Lemma 2.2 with log(P X|Y (x i |j, β)) in the role of h j we obtain

$L(β) = - n i=1 max p∈∆ K K j=1 log(P X|Y (x i |j, θ))p j - K j=1 p j log( p j π j ) = - max p (1) ,••• ,p (n) ∈∆ K n i=1 K j=1 log(P X|Y (x i |j, θ))p (i) j - n i=1 K j=1 p (i) j log( p (i) j π j ) .$Equivalently,

$1 n L(β) = min p (1) ,••• ,p (n) ∈∆ K i,j C ij (θ) p (i) j n + i,j p (i) j n log( p (i) j /n π j /n ) ,$where we introduced C ij (θ) -log(P X|Y (x i |j, θ)). Now suppose that we have solutions p (1) , • • • , p (n) of the minimization problem above and that we consider

$P = 1 n (p (1) , • • • , p (n) ) ⊤ ∈ R n×K + . Then obviously P1 K = 1 n 1 n . Conversely, any matrix P ∈ R n×K + with P1 K = 1 n 1 n can be written as P = 1 n (p (1) , • • • , p (n)$) ⊤ for some probability vectors p (1) , • • • , p (n) ∈ ∆ K (which are simply the rows of P). In other words, this proves that

$1 n L(β) = min P∈R n×K + P1 K = 1 n 1n i,j C ij (θ)P ij + i,j P ij log( P ij π j /n ) = min P∈U K ( 1n n ) C(θ), P + KL(P| 1 n n π ⊤ ) .$(2.2)

We almost have the desired entropic OT problem, albeit with a "semi-relaxed" constraint instead of a standard coupling constraint: if P were in U( 1n n , π) we would be done. To obtain an entropic OT problem we only need to rewrite a bit the quantity above: we show that minimizing the negative log-likelihood with respect to π leads to a coupling constraint rather than a semi-relaxed one. First, for any admissible P ∈ U K ( 1n n ) of the minimization problem (2.2), we have (see Lemma 5.2) KL(P|

$1 n n π ⊤ ) = KL P| 1 n n (P ⊤ 1 n ) ⊤ + KL(P ⊤ 1 n |π) . (2.3) This implies that 1 n L(β) = min P∈ U K ( 1n n ) C(θ), P + KL P| 1 n n (P ⊤ 1 n ) ⊤ + KL(P ⊤ 1 n |π) .$(2.4)

Now consider P ⋆ the solution of the entropic OT problem min P∈U ( 1 n 1n,π) C(θ), P +KL(P| 1n n π ⊤ ). By the constraints, P ⋆ ∈ U K ( 1 n 1 n ) and KL(P ⋆⊤ 1 n |π) = 0. Thus, by suboptimality in (2.4)

$1 n L(β) ≤ C(θ), P ⋆ + KL P ⋆ | 1n n (P ⋆⊤ 1 n ) ⊤ .$Hence we first obtain an upper-bound on the negative log-likelihood:

$1 n L(β) ≤ OT ε=1 ( 1 n n , π, C(θ)) . (2.5)$To obtain an equality we will minimize with respect to π. Using that min b∈∆ K KL(a|b) = KL(a|a) = 0, if we minimize the RHS in (2.4) over π we get,

$min π∈∆ K min P∈ U K ( 1n n ) C(θ), P + KL P| 1 n n (P ⊤ 1 n ) ⊤ + KL(P ⊤ 1 n |π) = min P∈ U K ( 1n n ) C(θ), P + KL P| 1 n n (P ⊤ 1 n ) ⊤ ⋆ = min π∈∆ K min P∈U ( 1 n 1n,π) C(θ), P + KL(P| 1 n n π ⊤ ) = min π∈∆ K OT ε=1 ( 1 n n , π, C(θ)) .$For the equality in ⋆ we used that the second marginal in the optimization problem is redundant in the RHS. Precisely, if P 0 is a solution of the LHS then b 0

$= P ⊤ 0 1 n ∈ ∆ K thus P 0 ∈ U( 1n n , b 0 ) and KL P 0 | 1n n (P ⊤ 0 1 n ) ⊤ = KL P 0 | 1n n b ⊤ 0 which implies that LHS ≥ RHS. Conversely, if (π 1 , P 1 ) is a solution of the RHS problem then P 1 ∈ U K ( 1n n ) and π 1 = P ⊤ 1 1 n , hence LHS ≤ RHS.$In particular for any θ ∈ Θ, min π

$1 n L(π, θ) = min π OT ε=1 ( 1n n , π, C(θ)$). This gives the final result written below.

Proposition 2.3 (MLE for mixture models is minimization of an EOT problem). Consider a mixture model as in Definition 2.1 and L the negative log-likelihood on n i.i.d. samples. First, we have the identity

$∀(π, θ) ∈ ∆ K × Θ, 1 n θ) = min P∈U K ( 1n n ) C(θ), P + KL(P| 1 n n π ⊤ ) , (2.6)$where the cost matrix is C(θ) -log P X|Y (x i |j, θ) ij . Second, the EOT problem is a upperbound of the negative log-likelihood:

$∀(π, θ) ∈ ∆ K × Θ, 1 n L(π, θ) ≤ OT ε=1 ( 1 n n , π, C(θ)) .$(2.7)

Third, minimizing the negative log-likelihood with respect to the parameters is equivalent to minimizing over the parameters an EOT problem. Precisely,

$min (π,θ)∈∆ K ×Θ 1 n L(π, θ) = min (π,θ)∈∆ K ×Θ OT ε=1 ( 1 n n , π, C(θ)) .$(2.8)

## Illustration with Gaussian Mixture Models

To illustrate the previous results we will show that the updates of the Expectation-Maximization algorithm (EM) for the Gaussian mixture model (GMM) can be interpreted as a block-coordinate descent (BCD) on the EOT loss (see e.g. [Murphy 2012, Section 11.4](#) for a description of the EM algorithm). According to Proposition 2.3 minimizing the negative log-likelihood is equivalent to solving min

$(π,θ)∈∆ K ×Θ min P∈U K ( 1n n ) C(θ), P + KL(P| 1 n n π ⊤ ) . (3.1)$The BCD strategy consists in alternating between minimizing (3.1) in π, θ, P while keeping the other variables fixed.

The update of P with π, θ fixed consists in solving a semi-relaxed entropic OT problem. As described e.g. in [Flamary et al. (2016)](#b6) (or also in the proof of Proposition 2.3) the problem decouples with respect to the rows of P and the solution is given by

$∀(i, j) ∈ [[n]] × [[K]], P ij = 1 n π j exp(-C ij (θ)) K k=1 π k exp(-C ik (θ)) = 1 n π j P X|Y (x|j, θ) K k=1 π k P X|Y (x|k, θ) . (3.2)$This step actually corresponds to finding the conditional distribution in the "E step" of the EM algorithm. The update for π, with P held fixed, can be found using (2.3) and is simply

$π = P ⊤ 1 n i.e. ∀j ∈ [[K]], π j = n i=1 P ij . (3.3)$Finally we derive the update of θ in the GMM case. We consider

$P X|Y (x|j, θ) = (2π) -d/2 det Σ -1/2 exp - 1 2 (x -µ j ) ⊤ Σ -1 (x -µ j ) ,$and the goal is to update θ = (µ 1 , • • • , µ K , Σ) with µ j ∈ R d and Σ ≻ 0. With other variables fixed this boils down to solving min

$(µ 1 ,••• ,µ K ,Σ) 1 2 ij (x i -µ j ) ⊤ Σ -1 (x i -µ j )P ij + n 2 log det Σ .$Setting the gradient of this loss to zero, one can show that the update of Σ (with (µ [(3.4)](#) and the update of the means (with Σ fixed) are

$1 , • • • , µ K ) fixed) reads Σ = 1 n ij P ij (x i -µ j )(x i -µ j ) ⊤ ,$$∀j ∈ [[K]], µ j = 1 i P ij i P ij x i . (3.5)$These updates exactly corresponds to the updates of the EM algorithm apply to a GMM: first update P according to (3.2) which is the "E step", then the proportion of the classes with (3.3) and the means and covariance with (3.4) and (3.5), which is the "M step".

## Discussions

To finish we make a few comments. First, the proof described above can be easily generalized to infinite mixtures, with appropriate assumptions on P Y , see e.g. [Rigollet and Weed (2018, Definition 2)](#). With these assumptions we would obtain that minimizing the negative loglikelihood is equivalent to solving a problem of the form inf (P Y ,θ) OT ε=1 (P Y , 1 n n i=1 δ x i ; θ) where the cost of the OT is c(x, y; θ) = -log(P X|Y (x|y, θ)).

Also, as discussed in [Mena et al. (2020)](#b1), the same relations between negative log-likelihood and entropic OT can be obtained for more general generative models where P Y are P X are coupled via a joint distribution

$Q θ ⋆ X,Y (such as dQ θ ⋆ X,Y (x, y) = exp(-g θ ⋆ (x, y))dP X (x)dP Y (y)$). This setting encompasses the GMM case and the principle of the proof remains similar to the one described here.

## Postponed proofs

$Lemma 5.1. Let π 1 , • • • , π K be positive real numbers and h 1 , • • • , h K ∈ R. Then log K j=1 π j exp(h j ) = max p∈∆ K K j=1 h j p j - K j=1 p j log( p j π j ) .$The optimal solution is given by ∀k

$∈ [[K]], p k = π k exp(h k ) K j=1 π j exp(h j ) .$Proof. The optimization problem above is a maximization of a strictly concave function. Consider the Lagrangian L(p, λ) = j h j p jj p j log( p j π j ) + λ( j p j -1). Then ∂ p k L(p, λ) = h k -log(p k /π k ) -1 + λ thus ∂ p k L(p, λ) = 0 ⇐⇒ p k = exp(λ -1)π k exp(h k ). By primal constraints j p j = 1 =⇒ exp(λ -1) = 1 j exp(h j )π j . This gives the desired optimal solution. Also ∀k ∈ [[K]], log(p k /π k ) = h k -log( K j=1 π j exp(h j )) hence j p j log(p j /π j ) = j h j p j -log( K j=1 π j exp(h j )) j p j . Using that j p j = 1 gives the result. We also used the following result about the KL divergence.

Lemma 5.2. Let P ∈ R n×K + be a matrix and a ∈ R n + , b ∈ R K + then KL(P|ab ⊤ ) = KL P|(P1 K )(P ⊤ 1 n ) ⊤ + KL(P1 K |a) + KL(P ⊤ 1 n |b) .

(5.1)

$Proof. By definition KL(P|ab ⊤ ) = ij P ij log( P ij a i b j ) = ij P ij log( P ij (P1 k ) i (P ⊤ 1 n ) j (P1 k ) i (P ⊤ 1 n ) j a i b j ) = KL P|(P1 K )(P ⊤ 1 n ) ⊤ + ij P ij log( (P1 k ) i a i ) + ij P ij log( (P ⊤ 1 n ) j b j ) = KL P|(P1 K )(P ⊤ 1 n ) ⊤ + i (P1 k ) i log( (P1 k ) i a i ) + j (P ⊤ 1 n ) j log( (P ⊤ 1 n ) j b j$) .

