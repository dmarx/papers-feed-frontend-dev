# Maximum Likelihood Estimation of Functionals of Discrete Distributions

## Abstract

## 

We consider the problem of estimating functionals of discrete distributions, and focus on tight (up to universal multiplicative constants for each specific functional) nonasymptotic analysis of the worst case squared error risk of widely used estimators. We apply concentration inequalities to analyze the random fluctuation of these estimators around their expectations, and the theory of approximation using positive linear operators to analyze the deviation of their expectations from the true functional, namely their bias.

We explicitly characterize the worst case squared error risk incurred by the Maximum Likelihood Estimator (MLE) in estimating the Shannon entropy H(P ) = S i=1 -pi ln pi, and the power sum Fα(P ) = S i=1 p α i , α > 0, up to universal multiplicative constants for each fixed functional, for any alphabet size S ≤ ∞ and sample size n for which the risk may vanish. As a corollary, for Shannon entropy estimation, we show that it is necessary and sufficient to have n ≫ S observations for the MLE to be consistent. In addition, we establish that it is necessary and sufficient to consider n ≫ S 1/α samples for the MLE to consistently estimate Fα(P ), 0 < α < 1. The minimax rate-optimal estimators for both problems require S/ ln S and S 1/α / ln S samples, which implies that the MLE has a strictly sub-optimal sample complexity. When 1 < α < 3/2, we show that the worst-case squared error rate of convergence for the MLE is n -2(α-1) for infinite alphabet size, while the minimax squared error rate is (n ln n) -2(α-1) . When α ≥ 3/2, the MLE achieves the minimax optimal rate n -1 regardless of the alphabet size.

As an application of the general theory, we analyze the Dirichlet prior smoothing techniques for Shannon entropy estimation. In this context, one approach is to plug-in the Dirichlet prior smoothed distribution into the entropy functional, while the other one is to calculate the Bayes estimator for entropy under the Dirichlet prior for squared error, which is the conditional expectation. We show that in general such estimators do not improve over the maximum likelihood estimator. No matter how we tune the parameters in the Dirichlet prior, this approach cannot achieve the minimax rates in entropy estimation. The performance of the minimax rate-optimal estimator with n samples is essentially at least as good as that of Dirichlet smoothed entropy estimators with n ln n samples.

## I. INTRODUCTION

Entropy and related information measures arise in information theory, statistics, machine learning, biology, neuroscience, image processing, linguistics, secrecy, ecology, physics, and finance, among other fields. Numerous inferential tasks rely on data driven procedures to estimate these quantities (see, e.g. [[1]](#b0)- [[6]](#b5)). We focus on two concrete and well-motivated examples of information measures, namely the Shannon entropy [[7]](#b6)

$H(P ) S i=1 -p i ln p i ,(1)$and the power sum F α (P ), α > 0:

$F α (P ) S i=1 p α i , α > 0.(2)$The power sum F α (P ) functional often emerges in various operational problems [[8]](#b7). It also has connections to the Rényi entropy [[9]](#b8) H α (P ) via the formula H α (P ) = ln Fα(P ) 1-α . Consider estimating the Shannon entropy H(P ) based on n i.i.d. samples following unknown discrete distribution P with unknown alphabet size S. This problem has a rich history with extensive study in various fields ranging from information theory, statistics, neuroscience, physics, psychology, medicine, etc. We refer the reader to [[10]](#b9) for a review. One of the most widely used estimators for this purpose is the Maximum Likelihood Estimator (MLE), which is simply the empirical entropy. The empirical entropy is an instantiation of the plugin principle in functional estimation, where a point estimate of the parameter (distribution P in this case) is used to construct an estimator for a functional of the parameter via the plug-in approach. The idea of using the MLE for estimating information measures of interest (in this case entropy), is not only intuitive, but has sound justification: asymptotic efficiency.

The beautiful theory of Hájek and Le Cam [[11]](#b10)- [[13]](#b12) shows that, as the number of observed samples grows without bound while the finite parameter dimension (e.g., alphabet size) remains fixed, the MLE performs optimally in estimating any differentiable functional when the statistical model complies with the benign LAN (Local Asymptotic Normality) condition [[13]](#b12). Thus, for finite dimensional problems, the problems of parameter and functional estimation are well understood in an asymptotic sense, and the MLE appears to be not only natural but also theoretically justified. But does it make sense to employ the MLE to estimate the entropy in most practical applications?

As it turns out, while asymptotically optimal in entropy estimation, the MLE is by no means sacrosanct in many real applications, especially in regimes where the alphabet size is comparable to, or even larger than the number of observations. It was shown that the MLE for entropy is strictly sub-optimal in the large alphabet regime [[14]](#b13), [[15]](#b14). Therefore, classical asymptotic theory does not satisfactorily address high dimensional settings, which are becoming increasingly important in the modern era of high dimensional statistics.

There has been a wave of recent research activities focusing on analyzing existing approaches of functional estimation, as well as proposing new estimators that are provably near optimal in the large alphabet regime. Paninski [[14]](#b13) showed that the MLE needs n ≫ S samples to consistently estimate the Shannon entropy, and Paninski [[15]](#b14) established the existence of a (non-explicit) estimator that only required n ≪ S samples. It implies that the MLE is strictly sub-optimal in terms of sample complexity. It was Valiant and Valiant [[16]](#b15) who first explicitly constructed a linear programming based estimator (later modified in [[17]](#b16)) that achieves consistency in entropy estimation with n ≫ S/ ln S samples, which they also proved to be necessary. [Valiant and Valiant [18]](#) constructed another approximation based estimator that achieved better theoretical properties than the linear programming ones, which was not yet shown to be minimax rate-optimal for all ranges of S and n. The authors [[10]](#b9) constructed the first minimax rate-optimal estimators for H(P ) and F α (P ), α > 0 based on best polynomial approximation, which are agnostic to the alphabet size S. Utilizing the released MATLAB and Python packages of the estimators in [[10]](#b9), [[19]](#b18), [[20]](#b19) demonstrated that these minimax rate-optimal estimators can lead to significant performance boosts in various machine learning tasks. Wu and Yang [[21]](#b20) independently applied the best polynomial approximation idea to entropy estimation and obtained the minimax rates. However, their estimator requires the knowledge of the alphabet size S. The approximation ideas proved to be very fruitful in Acharya et al. [[22]](#b21), Wu and Yang [[23]](#b22), Han, Jiao, and Weissman [[24]](#b23), Jiao, Han, and Weissman [[25]](#b24), Bu et al. [[26]](#b25), Orlitsky, Suresh, and Wu [[27]](#b26), Wu and Yang [[28]](#b27).

The main contribution of this paper is an explicit characterization of the worst case squared error risk of estimating H(P ) and F α (P ) using the MLE up to a universal multiplicative constant for each specific functional, for all ranges of S and n in which the risk may vanish. Understanding the benefits and limitations of the MLE in a nonasymptotic setting serves two key purposes. First, the approach is a natural benchmark for comparing other more nuanced procedures for estimation of functionals. Second, performance analysis for the MLE reveals regimes where the problem is difficult, and motivates the development of improvements, which have been validated in [[10]](#b9), [[14]](#b13)- [[18]](#b17), [[21]](#b20), [[22]](#b21). As a byproduct of the analysis, we explicitly point out an equivalence between bias analysis of functional estimators using plug-in rules and approximation theory using positive linear operators. We believe these powerful tools introduced from approximation theory may have far reaching impacts in various applications in the information theory community.

We mention that there exist numerous other approaches proposed in various disciplines to estimate entropy, many among which are difficult to analyze theoretically. Among them we mention the Miller-Madow bias-corrected estimator and its variants [[29]](#b28)- [[31]](#b30), the jackknife estimator [[32]](#b31), the shrinkage estimator [[33]](#b32), the coverage adjusted estimator [[34]](#b33), the Best Upper Bound (BUB) estimator [[14]](#b13), the B-Splines estimator [[35]](#b34), and [[36]](#b35), [[37]](#b36) etc. For a Bayesian statistician, a natural approach is to first impose a prior on the unknown discrete distribution before considering estimating entropy. The Dirichlet prior, being the conjugate prior to the multinomial distribution, appears to be particularly popular in the Bayesian approach to entropy estimation. Dirichlet smoothing may have two connotations in the context of entropy estimation:

• [[38]](#b37), [[39]](#b38) One first obtains a Bayes estimate for the discrete distribution P , which we denote by PB , and then plugs it in the entropy functional to obtain the entropy estimate H( PB ).

• [[40]](#b39)[[41]](#b40) One calculates the Bayes estimate for entropy H(P ) under Dirichlet prior for squared error. The estimator is the conditional expectation E[H(P )|X], where X represents the samples.

Nemenman, Shafee, and Bialek [[42]](#b41) argued in an intuitive way why Dirichlet prior is bad for entropy estimation and proposed to use mixtures of Dirichlet priors. Archer, Park, and Pillow [[43]](#b42) have come up with priors that perform better than the Dirichlet prior. Also see [[44]](#b43), [[45]](#b44).

Another contribution of this paper is an explicit characterization of the worst case squared error risk of estimating H(P ) using the Dirichlet prior plug-in approach up to a universal multiplicative constant, for all ranges of S and n in which the risk may vanish. We show rigorously that neither of the two approaches utilizing the Dirichlet prior result in improvements over the MLE in the large alphabet regime. Specifically, these approaches require at least n ≫ S to be consistent, while the minimax rate-optimal estimators such as the ones in [[10]](#b9)[[21]](#b20) only need n ≫ S ln S to achieve consistency. The rest of the paper is organized as follows. We present the main results in Section III, discuss the fundamental ideas behind the proofs in Section IV, and detail the proofs in Section V and VI. Proofs of auxiliary lemmas are deferred to the appendices.

## II. PRELIMINARIES

The Dirichlet distribution with order S ≥ 2 with parameters α 1 , . . . , α S > 0 has a probability density function with respect to Lebesgue measure on the Euclidean space R S-1 given by

$f (x 1 , • • • , x S ; α 1 , • • • , α S ) = 1 B(α) S i=1 x αi-1 i (3)$on the open S -1-dimensional simplex defined by:

$x 1 , • • • , x S-1 > 0 (4) x 1 + • • • + x S-1 < 1 (5) x S = 1 -x 1 -• • • -x S-1(6)$and zero elsewhere. The normalizing constant is the multinomial Beta function, which can be expressed in terms of the Gamma function:

$B(α) = S i=1 Γ(α i ) Γ S i=1 α i , α = (α 1 , • • • , α S ).(7)$Assuming the unknown discrete distribution P follows prior distribution P ∼ Dir(α), and we observe a vector X = (X 1 , X 2 , . . . , X S ) with multinomial distribution multi(n; p 1 , p 2 , . . . , p S ), then one can show that the posterior distribution P P |X is also a Dirichlet distribution with parameters

$α + X = (α 1 + X 1 , α 2 + X 2 , . . . , α S + X S ) .(8)$Furthermore, the posterior mean (conditional expectation) of p i given X is given by [[46,](#b45)[Example 5.4.4]](#)

$δ i (X) E[p i |X] = α i + X i n + S i=1 α i .(9)$The estimator δ i (X) is widely used in practice for various choices of α. For example, if α i = √ n S , then the corresponding (δ 1 (X), δ 2 (X), . . . , δ S (X)) is the minimax estimator for P under squared loss [[46,](#b45)[Example 5.4.5]](#). However, it is no longer minimax under other loss functions such as ℓ 1 loss, which was investigated in [[47]](#b46).

Note that the estimator δ i (X) subsumes the MLE pi = Xi n as a special case, since we can take the limit α → 0 for δ i (X) to obtain MLE. We denote the empirical distribution by P n = (p 1 , p2 , . . . , pS ). The Dirichlet prior smoothed distribution estimate is denoted as PB , where

$PB = n n + S i=1 α i P n + S i=1 α i n + S i=1 α i α S i=1 α i . (10$$)$Note that the smoothed distribution PB can be viewed as a convex combination of the empirical distribution P n and the prior distribution α S i=1 αi . We call the estimator H( PB ) the Dirichlet prior smoothed plug-in estimator.

Another way to apply Dirichlet prior in entropy estimation is to compute the Bayes estimator for H(P ) under squared error, given that P follows Dirichlet prior. It is well known that the Bayes estimator under squared error is the conditional expectation. It was shown in Wolpert and Wolf [[40]](#b39) that [(11)](#b10) where ψ(z)

$ĤBayes E[H(P )|X] = ψ 1 + S i=1 (α i + X i ) - S i=1 α i + X i S i=1 (α i + X i ) ψ(α i + X i + 1),$$Γ ′ (z) Γ(z)$is the digamma function. We call the estimator ĤBayes the Bayes estimator under Dirichlet prior.

Throughout this paper, we observe n i.i.d. samples from an unknown discrete distribution P = (p 1 , p 2 , . . . , p S ). We denote the n samples as n i.i.d. random variables {Z i } 1≤i≤n taking values in Z = {1, 2, . . . , S} with probability (p 1 , p 2 , . . . , p S ). Defining

$X i n j=1 ½(Z j = i), 1 ≤ i ≤ S,(12)$we know that (X 1 , X 2 , . . . , X S ) follows a multinomial distribution with parameter (n; p 1 , p 2 , . . . , p S ). Denote h j S i=1 ½(X i = j), 0 ≤ j ≤ n. The Maximum Likelihood Estimator (MLE) for H(P ) and F α (P ) are defined, respectively, as H(P n ) and F α (P n ), with P n being the empirical distribution. We assume the functional F (P ) takes the form

$F (P ) = S i=1 f (p i ). (13$$)$Then it is evident that the MLE F (P n ) for estimating functional F (P ) in ( [13](#formula_12)) can be alternatively represented as the following linear function of (h 0 , h 1 , . . . , h n ):

$F (P n ) = n j=0 f j n h j .(14)$Recall that the risk function under squared error for any estimator F in estimating functional F (P ) may be decomposed as

$E P (F (P ) -F ) 2 = (E P F -F (P )) 2 + E P F -E P F 2 ,(15)$where (E P F -F (P )) 2 represents the squared bias, and

$E P F -E P F 2$represents the variance. The subscript P means that the expectation is taken with respect to the distribution P that generates the i.i.d. observations. We omit the subscript for the expectation operator E if the meaning of the expectation is clear from the context.

Notation: a ∧ b denotes min{a, b}, a ∨ b denotes max{a, b}. For two non-negative series {a n }, {b n }, notation a n b n means that there exists a positive universal constant C < ∞ such that an bn ≤ C, for all n. The notation a n ≍ b n is equivalent to a n b n and b n a n . Notation a n ≫ b n means that lim inf n→∞ an bn = ∞. Throughout this paper, the notations , , ≪, ≫ involve absolute constants that may only depend on α but not S or n. We denote by M S the space of discrete distributions with alphabet size S.

## III. MAIN RESULTS

## A. Estimating F α (P )

We split the upper bounds and the lower bounds into two theorems, and present their succinct summaries in Corollary 1 and 2.

Theorem 1 (Upper bounds). We have the following upper bounds on the worst case squared error risk of MLE in estimating F α (P ):

$1) α ≥ 2: sup P ∈MS E P (F α (P n ) -F α (P )) 2 ≤ α(α -1) 2n 2 + α 2 4n .(16)$2) 1 < α < 2:

sup

$P ∈MS E P (F α (P n ) -F α (P )) 2 ≤ 4 n α-1 ∧ 3S 1-α/2 n α/2 ∧ C α,n 5S 2n 2 + α 2 4n ,(17)$where and [ω 2](#) ϕ is the second-order Ditzian-Totik modulus of smoothness introduced in Section IV-B.

$C α,n nω 2 ϕ (x α , n -1/2 ) > 0 satisfies lim sup n→∞ C α,n < ∞ for 1 < α < 2,$$3) 1/2 ≤ α < 1: sup P ∈MS E P (F α (P n ) -F α (P )) 2 ≤ 3S 1-α/2 2n α/2 ∧ 5S 2n α 2 + 10S 2-2α n + 120 α 2 S n 2α ∧ 1 n 2α-1 . (18) 4) 0 < α < 1/2: sup P ∈MS E P (F α (P n ) -F α (P )) 2 ≤ 3S 1-α/2 2n α/2 ∧ 5S 2n α 2 + 10S n 2α + 120 α 2 S n 2α ∧ 1 n 2α-1 . (19$$)$Moreover, in all the bounds presented above, the first term bounds the square of the bias, and the second term bounds the variance.

Theorem 2 (Lower bounds). We have the following lower bounds on the worst case squared error risk of MLE in estimating F α (P ):

1) α ≥ 3/2: there exists a constant C α > 0 such that for all n,

$sup P ∈MS E P (F α (P n ) -F α (P )) 2 ≥ C α n .(20)$2) 1 < α < 3/2: if S = cn, for any c > 0, then

$lim inf n→∞ n 2(α-1) • sup P ∈MS E P (F α (P n ) -F α (P )) 2 > 0.(21)$3)

$1/2 ≤ α < 1: if n ≥ S, then sup P ∈MS E P (F α (P n ) -F α (P )) 2 ≥ α 2 (1 -α) 2 72n 2α (S -1) 2 1 - 1 n 2 + α 2 64en (2(S -1)) 1-α -2 -α - 1 -α 4n (2(S -1)) 1-α + 2 -α 2 - 1 2 e -n/4 S 2(1-α) ,(22) 4$$) 0 < α < 1/2: if n ≥ S, then sup P ∈MS E P (F α (P n ) -F α (P )) 2 ≥ α 2 (1 -α) 2 36n 2α (S -1) 2 1 - 1 n 2 . (23$$)$There are several interesting implications of this result, highlighted in the following corollaries.

Corollary 1. For any fixed α > 1, there exist universal convergence rates for F α (P ):

sup

$S∈N+ sup P ∈MS E P (F α (P n ) -F α (P )) 2 ≍ n -2(α-1) 1 < α < 3/2 n -1 α ≥ 3/2(24)$Corollary 1 implies that, when α ≥ 3/2, estimation of F α (P ) is extremely simple in terms of convergence rate: plugin estimation achieves the best possible rate n -1 (as shown in the theory of regular statistical experiments of classical asymptotic theory, see [[48,](#b47)[Chap. 1.7.]](#)). Results of this form have appeared in the literature, for example, Antos and Kontoyiannis [[49]](#b48) showed that it suffices to take n ≫ 1 samples to consistently estimate F α (P ), α ≥ 2, α ∈ Z. However, when 1 < α < 3/2, the rate n -2(α-1) is considerably slower. Interestingly, there exist estimators that demonstrate better convergence rates for estimating F α (P ), 1 < α < 3/2. Jiao et al. [[10]](#b9) showed that the minimax rate in estimating F α (P ), 1 < α < 3/2, is (n ln n) -2(α-1) as long as S n ln n, which is achieved using the general methodology developed therein for constructing minimax rate-optimal estimators for nonsmooth functionals.

Let us now examine the case 0 < α < 1, another interesting regime that has not been characterized before. In this regime, we observe significant increase in the difficulty of the estimation problem. In particular, the relative scaling between the number of observations n and the alphabet size S for consistent estimation of F α (P ) exhibits a phase transition, encapsulated in the following.

Corollary 2. Fix α ∈ (0, 1). The worst case squared error risk of the MLE F α (P n ) in estimating F α (P ) is characterized as follows when n ≥ S:

$sup P ∈MS E P (F α (P n ) -F α (P )) 2 ≍ S 2 n 2α + S 2-2α n 1/2 < α < 1 S 2 n 2α 0 < α ≤ 1/2(25)$Corollary 2 follows directly from Theorem 1 and Theorem 2. In particular, it implies that it is necessary and sufficient to take n ≫ S 1/α samples to consistently estimate F α (P ), 0 < α < 1 using MLE. Thus, as one might expect, the scale of the number of measurements required for consistent estimation increases as α decreases. When α → 0, the number of samples required for the MLE grows super-polynomially in S, which is consistent with the intuition that F α (P ), α → 0 is essentially equivalent to the alphabet size of a distribution, whose estimation is known to be very hard when there may exist symbols with very small probabilities [[50]](#b49).

We exhibit some of our findings by plotting the value required of ln n/ ln S for consistent estimation of F α (P ) using the MLE F α (P n ), as a function of α, in Figure [1](#).

$not achievable via MLE (Theorem 2) 1 0 1 2 α ln n ln S 1/α achievable via MLE (Theorem 1)$Fig. [1](#): For any fixed point above the thick curve, consistent estimation of F α (P ) is achieved using MLE F α (P n ) as shown in Theorem 1. For any fixed point below the thick curve in the regime 0 < α < 1, Theorem 2 shows that the MLE does not have vanishing maximum squared error risk.

It turns out that one can construct estimators that are better than the MLE in terms of required sample complexity for consistent estimation for the regime 0 < α < 1. Indeed, Jiao et al. [[10]](#b9) showed that the minimax rate-optimal estimator requires n ≫ S 1 α ln S samples to achieve consistency, which attains a logarithmic improvement in the sample complexity over the MLE.

## B. Estimating H(P )

We not only consider H(P n ), but also the so-called Miller-Madow bias-corrected estimator [[29]](#b28) defined as

$H MM (P n ) = H(P n ) + S -1 2n . (26$$)$Theorem 3. The worst case squared error risk of H(P n ) admits the following upper bound for all S, n:

$sup P ∈MS E P (H(P n ) -H(P )) 2 ≤ ln 1 + S -1 n 2 + (ln n) 2 n ∧ 2(ln S + 3) 2 n . (27$$)$If n ≥ 15S, then

$sup P ∈MS E P (H(P n ) -H(P )) 2 ≥ 1 2 S -1 2n + S 2 20n 2 - 1 12n 2 2 + c ln 2 S n . (28$$)$Moreover, if n ≥ 15S, the Miller-Madow bias-corrected estimator satisfies

$sup P ∈MS E P H MM (P n ) -H(P ) 2 ≥ 1 2 S 2 20n 2 - 1 12n 2 2 + c ln 2 S n ,(29)$where the positive constant c > 0 in both expressions does not depend on S or n.

Theorem 3 implies the following corollary.

Corollary 3. The worst case squared error risk of the MLE H(P n ) in estimating H(P ) is characterized as follows when n ≥ 15S:

sup

$P ∈MS E P (H(P n ) -H(P )) 2 ≍ S 2 n 2 + ln 2 S n . (30$$)$Here the first term corresponds to the squared bias, and the second term corresponds to the variance.

Paninski [[14]](#b13) showed that if n = cS, where c > 0 is a constant, the maximum squared error risk of H(P n ), and the Miller-Madow bias-corrected estimator H MM (P n ), would be bounded from zero. Paninski [[14]](#b13) also showed that when n ≫ S, n → ∞, the MLE is consistent for estimating entropy. Corollary 3 implies that it is necessary and sufficient to take n ≫ S samples for the MLE to be consistent for estimating entropy. Comparing the results for H(P ) with those for F α (P ), we see that the intuition that H(P ) being viewed close to F α (P ) when α → 1 -1 is indeed approximately correct as H(P ) coincides with α → 1 -on the phase transition curve shown in Figure [1](#).

Table [I](#) summarizes the minimax squared error rates and the worst case squared error rates of the MLE in estimating H(P ) and F α (P ), α > 0. It is clear that the MLE cannot achieve the minimax rates for estimation of H(P ), and F α (P ) when 0 < α < 3/2. In these cases, there exist strictly better estimators whose performance with n samples is roughly the same as Minimax squared error rates Maximum squared error rates of MLE H(P ) S 2

(n ln n) 2 + ln 2 S n (n S/ ln S) ( [10], [[16]](#b15), [[18]](#b17), [[21]](#b20)

$) S 2 n 2 + ln 2 S n (n S) (Corollary 3) F α (P ), 0 < α ≤ 1 2 S 2 (n ln n) 2α n S 1/α / ln S, ln n ln S ( [10]) S 2 n 2α n S 1/α (Corollary 2) F α (P ), 1 2 < α < 1 S 2 (n ln n) 2α + S 2-2α n n S 1/α / ln S ( [10]) S 2 n 2α + S 2-2α n n S 1/α (Corollary 2) F α (P ), 1 < α < 3 2 (n ln n) -2(α-1) (S n ln n) ( [10]) n -2(α-1) (S n) (Corollary 1) F α (P ), α ≥ 3 2 n -1 (Theorem 1) n -1$TABLE I: Summary of results in this paper and the companion [[10]](#b9) that of the MLE with n ln n samples. This phenomenon was termed effective sample size enlargement in [[10]](#b9).

## C. Dirichlet prior techniques applying to entropy estimation

For symmetry, we restrict attention to the case where the parameter α in the Dirichlet distribution takes the form (a, a, . . . , a).

In comparison to MLE H(P n ), where P n is the empirical distribution, the Dirichlet smoothing scheme H( PB ) has a disadvantage: it requires the knowledge of the alphabet size S in general. We define

$pB,i = np i + a n + Sa ,(31)$and

$p B,i = E[p B,i ] = np i + a n + Sa . (32$$)$It is clear that

$PB = n n + Sa P n + Sa n + Sa U S(33)$$P B = n n + Sa P + Sa n + Sa U S ,(34)$where P n stands for the empirical distribution, P is the true distribution, and U S denotes the uniform distribution on the same alphabet with size S.

Theorem 4. If n ≥ max{Sa, 2ea}, then the maximum squared error risk of H( PB ) in estimating H(P ) is upper bounded as

$sup P ∈MS E P H( PB ) -H(P ) 2 ≤ ln 1 + S -1 n + Sa ∨ 2Sa n + Sa ln n + Sa 2a 2 + 2n (n + Sa) 2 3 + ln n + Sa a + 1 ∧ S 2 . (35$$)$Here the first term bounds the squared bias, and the second term bounds the variance.

Theorem 5. If n ≥ max{15S, Sa, 2ea}, then the maximum

$L 2 risk of H( PB ) in estimating H(P ) is lower bounded as sup P ∈MS E P H( PB ) -H(P ) 2 ≥ 1 2 (S -1)a 4(n + Sa) ln n + Sa a + S -1 8n + S 2 80n 2 - 1 48n 2 2 + c ln 2 S n , (36$$)$where c > 0 is a universal constant that does not depend on a, S, or n.

If n < Sa, then we have

$sup P ∈MS E P H( PB ) -H(P ) 2 ≥ S -1 2S 2 ln 2 S. (37$$)$If n < 2ea, then we have

$sup P ∈MS E P H( PB ) -H(P ) 2 ≥ S -1 S + 2e 2 ln 2 S. (38$$)$If n < 15S, n ≥ 2ea, then we have

$sup P ∈MS E P H( PB ) -H(P ) 2 ≥ (S -1)a 4(n + Sa) ln n + Sa a + ⌊n/15⌋ 8n - 1 16n + 2 , (39$$)$where ⌊x⌋ is the largest integer that does not exceed x, and (x) + = max{x, 0} represents the positive part of x.

The following corollary immediately follows from Theorem 4 and Theorem 5. The next theorem presents a lower bound on the maximum risk of the Bayes estimator under Dirichlet prior. Since we have assumed that all α i = a, 1 ≤ i ≤ S, the Bayes estimator under Dirichlet prior is

$ĤBayes = ψ(Sa + n + 1) - S i=1 a + X i Sa + n ψ(a + X i + 1). (40$$) Theorem 6. If S ≥ 2(n + 1), then sup P ∈MS E P ĤBayes -H(P ) 2 ≥ ln Sa + S/2 Sa + n + e -γ 2 , (41$$)$where γ ≈ 0.5772 is the Euler-Mascheroni constant.

Evident from Theorem 4, 5, and 6 is the fact that in the best situation (i.e. a not too large), both the Dirichlet prior smoothed plug-in estimator and the Bayes estimator under Dirichlet prior still require at least n ≫ S samples to be consistent, which is the same as MLE. In contrast, the estimators in Valiant and Valiant [[16]](#b15)- [[18]](#b17), Jiao et al. [[10]](#b9), Wu and Yang [[21]](#b20) are consistent if n ≫ S ln S , which is the optimal sample complexity. Thus, we can conclude that the Dirichlet smoothing technique does not solve the entropy estimation problem.

## IV. FUNDAMENTAL IDEAS OF OUR ANALYSIS

In this section, we discuss the fundamental tools we employed to obtain the results in Section III, as well as general recipes we suggest for analyzing performances of functional estimators.

## A. Variance

The variance characterizes the degree to which the random variable F ( P ) is fluctuating around its expectation, and the field of concentration inequalities perfectly fits our glove to give the desired results. For all the functionals we consider, it turns out that the Efron-Stein inequality [[51]](#b50) and the bounded differences inequality give very tight bounds. For completeness we state them below.

## Lemma 1.

[52, Efron-Stein inequality, Theorem 3.1] Let Z 1 , . . . , Z n be independent random variables and let

$f (Z 1 , Z 2 , . . . , Z n ) be a square integrable function. Moreover, if Z ′ 1 , Z ′ 2 , . . . , Z ′ n are independent copies of Z 1 , Z 2 , .$. . , Z n and if we define, for every i = 1, 2, . . . , n,

$f ′ i = f (Z 1 , Z 2 , . . . , Z i-1 , Z ′ i , Z i+1 , . . . , Z n ),(42)$then

$Var(f ) ≤ 1 2 n i=1 E (f -f ′ i ) 2 . (43$$)$The following inequality, which is called the bounded differences inequality, is a useful corollary of the Efron-Stein inequality.

## Lemma 2. [52, Bounded differences inequality, Corollary 3.2] If function

$f : Z n → R has the bounded differences property, i.e., for some nonnegative constants c 1 , c 2 , . . . , c n , sup z1,...,zn,z ′ i ∈Z |f (z 1 , . . . , z n ) -f (z 1 , . . . , z i-1 , z ′ i , z i+1 , . . . , z n )| ≤ c i ,(44)$for every

$1 ≤ i ≤ n, then Var(f (Z 1 , Z 2 , . . . , Z n )) ≤ 1 4 n i=1 c 2 i ,(45)$given that Z 1 , Z 2 , . . . , Z n are independent random variables.

We refer the readers to Boucheron et al. [[52]](#b51) for a modern exposition of the concentration inequality toolbox.

## B. Bias

It turns out that the bias analysis in estimation, albeit widely studied in statistics, seems to still largely bear an asymptotic and expansion nature in the mainstream statistical literature [[53]](#b52), [[54]](#b53). In particular, the bootstrap [[55]](#b54) as a method for estimating functionals was essentially only analyzed in an asymptotic setting [[56]](#b55). Among asymptotic analysis techniques, probably the most popular one is the Taylor expansion. We will show that the Taylor expansion may encounter great difficulties in analyzing the bias of MLE in information measure estimation. Then, we will introduce the field of approximation theory using positive linear operators and demonstrate that it is essentially equivalent to nonasymptotic bias analysis for plug-in functional estimators. In doing so, we present the readers with abundant handy tools from approximation theory, which could be readily applicable to many problems that may seem highly intractable with standard expansion methods.

We start from entropy estimation. In the literature, considerable effort has been devoted to understanding the nonasymptotic performance of the MLE H(P n ) in estimating H(P ). One of the earliest investigations in this direction is due to Miller [[29]](#b28) in 1955, who showed that, for any fixed distribution P ,

$EH(P n ) = H(P ) - S -1 2n + O 1 n 2 . (46$$)$Equation ( [46](#formula_64)) was later refined by Harris [[57]](#b56) using higher order Taylor series expansions to yield

$EH(P n ) = H(P )- S -1 2n + 1 12n 2 1 - S i=1 1 p i +O 1 n 3 . (47$) Harris's result reveals an undesirable consequence of the Taylor expansion method: one cannot obtain uniform bounds on the bias of the MLE. Indeed, the term

$S i=11$pi can be arbitrarily large for some distribution P . However, it is evident that both H(P n ) and H(P ) are bounded above by ln S, since the maximum entropy of any distribution supported on S elements is ln S. Conceivably, for such a distribution P that would make

$S i=11$pi very large, we need to compute even higher order Taylor expansions to obtain more accuracy, but even with such efforts we cannot obtain a uniform bias bound for all P .

We gain one of our key insights into the bias of the MLE by relating it to the approximation error induced by the Bernstein polynomial approximation of the function f , which was first observed in Paninski [[14]](#b13). To see this, we first compute the bias of F (P n ) in estimating the functional F (P ) in [(13)](#b12).

Lemma 3. The bias of the estimator F (P n ) is given by

$Bias(F (P n )) EF (P n ) -F (P ) = S i=1   n j=0 f j n n j p j i (1 -p i ) n-j -f (p i )   .(48)$The bias term in ( [48](#formula_69)) can be equivalently expressed as [1](#foot_0)Bias(F

$(P n )) = S i=1   n j=0 f j n B j,n (p i ) -f (p i )   (49) = S i=1 (B n [f ](p i ) -f (p i )) ,(50)$where B j,n (x) n j x j (1-x) n-j is the well-known Bernstein polynomial basis, and B n [f ](x) is the so-called Bernstein polynomial for function f (x). Bernstein in 1912 [[61]](#b60) provided an insightful constructive proof of the Weierstrass theorem on approximation of continuous functions using polynomials, by showing that the Bernstein polynomial of any continuous function converges uniformly to that function. From a functional analytic viewpoint, the Bernstein polynomial is an operator that maps a continuous function

$f ∈ C[0, 1] to another continuous function B n [f ] ∈ C[0, 1]$. This operator is linear in f , and is positive because B n [f ] is also pointwise nonnegative if f is pointwise non-negative. Evidently, bounding the approximation error incurred by the Bernstein polynomial is equivalent to bounding the bias of the MLE f (X/n), where X ∼ B(n, x). Fortunately, the theory of approximation using positive linear operators [[62]](#b61) provides us with advanced tools that are very effective for the bias analysis our problem calls for. A century ago, probability theory served Bernstein in breaking new ground in function approximation. It is therefore very satisfying that advancements in the latter have come full circle to help us better understand probability theory and statistics. We briefly review the general theory of approximation using positive linear operators below.

1) Approximation theory using positive linear operators: Generally speaking, for any estimator θ of a parametric model indexed by θ, the expectation f → E θ f ( θ) is a positive linear operator for f , and analyzing the bias E θ f ( θ) -f (θ) is equivalent to analyzing the approximation properties of the positive linear operator E θ f ( θ) in approximating f (θ). Hence, analyzing the bias of any plug-in estimator for functionals of parameters from any parametric families can be recast as a problem of approximation theory using positive linear operators [[62]](#b61).

Conversely, given a positive linear operator L(f )(x) that operates on the space of continuous functions, the Riesz-Markov-Kakutani theorem implies that under mild conditions the operator may be written as

$L(f )(x) = I f dµ x = E µx f (Z), Z ∼ µ x ,(51)$where {µ x } is a set of probability measures parametrized by x, which may be viewed as a parameter. If we view the random variable Z as a summary statistics to plug-in the functional f (•), the positive linear operator L(f )(x) is nothing but the expectation of the plug-in estimator f (Z). In this sense, there exists a one-to-one correspondence between essentially the most general bias analysis problem in statistics, and the most general positive linear operator approximation problem in approximation theory.

After more than a century's active research on approximation using positive linear operators, we now have highly nontrivial tools for positive linear operators of functions on one dimensional compact sets, but the general theory for vector valued multivariate functions on non-compact sets is still far from complete [[62]](#b61). In the next subsection, we present a sample of existing results in approximation using positive linear operators, corollaries of which will be used to analyze the bias of the MLE for two examples: F α (P ) and H(P ).

2) Some general results in bias analysis: First, some elementary approximation theoretic concepts need to be introduced in order to characterize the degree of smoothness of functions. For I ⊂ R an interval, the first-order modulus of smoothness ω 1 (f, t), t ≥ 0 is defined as [[62](#b61)]

$ω 1 (f, t) sup{|f (u) -f (v)| : u, v ∈ I, |u -v| ≤ t}. (52)$The second-order modulus of smoothness ω 2 (f, t), t ≥ 0 [[62]](#b61) is defined as

$ω 2 (f, t) sup f (u) -2f u + v 2 + f (v) : u, v ∈ I, |u -v| ≤ 2t . (53$$)$Ditzian and Totik [[63]](#b62) introduced a class of moduli of smoothness, which proves to be extremely useful in characterizing the incurred approximation errors. For simplicity, for functions defined on [0, 1], ϕ(x) = x(1 -x), the first-order Ditzian-Totik modulus of smoothness is defined as

$ω 1 ϕ (f, t) sup |f (u) -f (v)| : u, v ∈ [0, 1], |u -v| ≤ tϕ u + v 2 ,(54)$and the second-order Ditzian-Totik modulus of smoothness is defined as

$ω 2 ϕ (f, t) sup f (u) -2f u + v 2 + f (v) : u, v ∈ [0, 1], |u -v| ≤ 2tϕ u + v 2 . (55$$)$Recall that we denote by e j , j ∈ N + ∪ {0}, the monomial functions e j (y) = y j , y ∈ I. The first estimate for general positive linear operators, using modulus ω 2 and with precise constants, was given by Gonska [[64]](#b63). We rephrase Paltanea [[62,](#) Cor. 2.2.1.] as follows. Note that notation e 1 -xe 0 denotes a continuous function on I which is the difference of a linear function y and a constant function with constant value x over I. In other words, it is an abbreviation of e 1 (y)-xe 0 (y), y ∈ I, which is a function of y rather than x.

For a positive linear functional F , we adopt the following notation

$B F (x) = |F (e 1 ) -xF (e 0 )| , V F = F (e 1 -F (e 1 )e 0 ) 2 , (56$$)$which represent the "bias" and "variance" of a positive linear functional F . 

$I ⊂ R is an interval. Suppose that F (e 0 ) = 1, t > 0, length(I) ≥ 2t, s ≥ 2. Then, |F (f ) -f (x)| ≤ B F (x) ω 1 (f, t) t + 1 + F (|e 1 -xe 0 | s ) 2t s ω 2 (f, t). (57$$)$We remark that Lemma 4 can be applied to bound the bias of plug-in estimators in very general models. For example, consider an arbitrary statistical experiment {P θ , θ ∈ I}, from which we obtain n i.i.d. samples X 1 , X 2 , . . . , X n ∼ P θ . For any estimator θn , we would like to analyze the bias of the plug-in estimator f ( θn ) for functional f (θ).

Suppose length(I) ≥ 2t, s ≥ 2, then Lemma 4 implies that

$|E θ f ( θn ) -f (θ)| ≤ |E θ θn -θ| ω 1 (f, t) t + 1 + E| θn -θ| s 2t s ω 2 (f, t). (58$$)$If we further assume that θn is an unbiased estimator for θ, i.e., E θ θn = θ holds for all θ ∈ I, then we have

$|E θ f ( θn ) -f (θ)| ≤ 1 + E| θn -θ| s 2t s ω 2 (f, t).(59)$Taking s = 2 and assuming Var( θn ) ≤ length(I)/2, we have

$|E θ f ( θn ) -f (θ)| ≤ 3 2 ω 2 (f, Var( θn )),(60)$after we take t = E| θn -θ| 2 . We remark that Lemma 4 is only one way to analyze the bias, which is by no means always tight. For example, the following estimate using Ditzian-Totik modulus is significantly better than Lemma 4 for certain functions such as the entropy. 

$|F (f ) -f (x)| ≤ B F (x) 2h 1 ϕ(x) • ω 1 ϕ (f, 2h 1 ) + 5 2 ω 2 ϕ (f, h 1 ), (61$$)$$for all f ∈ C[0, 1] and 0 < h 1 ≤ 1 2 , where ϕ(x) = x(1 -x) and h 1 = F ((e 1 -xe 0 ) 2 )/ϕ(x) = V F + (B F (x)) 2 /ϕ(x).$The "bias" B F (x) and "variance" V F (x) are defined in [(56)](#b55).

Considering the same statistical experiment {P θ , θ ∈ I}, and the plug-in estimator f ( θn ) for f (θ), if θn is unbiased for θ and Var( θn ) ≤ ϕ(θ) 2  4 , then it follows from Lemma 5 that

$|E θ f ( θn ) -f (θ)| ≤ 5 2 ω 2 ϕ   f, Var( θn ) ϕ(θ)   ,(62)$after we take t = √ Var( θn) ϕ(θ) . For certain functions f (θ) and statistical models Lemma 5 is stronger than Lemma 4. For example, if f (θ) = -θ ln θ, θ ∈ [0, 1], and we have n• θn ∼ B(n, θ). We will show in Lemma 8 that ω 2 ϕ (f, t) = t 2 ln 4 1+t 2 , and ω 2 (f, t) = t ln 4. We also have Var( θn ) = θ(1-θ) n . Hence, Lemma 4 gives the upper bound

$|E θ f ( θn ) -f (θ)| ≤ 3 ln 4 2 θ(1 -θ) n ,(63)$whereas Lemma 5 gives

$|E θ f ( θn ) -f (θ)| ≤ 5 ln 4 2n • 1 1 + 1/n ,(64)$which is much stronger when n is large and θ not too close to the endpoints of [0, 1].

There also exist various estimates for the bias when the parameter lies in sets other than an interval in R. However, the bounds we presented are in general not optimal for specific functionals, thereby leaving ample room for future development. For example, note that ( [63](#formula_91)) is stronger than [(64)](#b63) when θ ≤ 1/n, but Han, Jiao, and Weissman [[65]](#b64) showed that when θ ≤ 1/n the pointwise bound in ( [63](#formula_91)) is still strictly suboptimal for the entropy functional. Unsurprisingly, to obtain the results in Section III, we need to go beyond the general results in approximation theory, and incorporate the structure of specific functions.

Note: In approximation theory literature, researchers have explored the interactions between general positive linear operator approximation and its probabilistic counterpart decades ago [[66]](#b65)- [[68]](#b67). However, in statistics literature related to positive linear approximation, usually only specific operators are used, such as the Bernstein operator [[69]](#b68), and the focus may not be on obtaining the tightest bound on bias [[70]](#b69), [[71]](#b70).

## C. Lower bounds

To lower bound the worst case performance of a specific estimator, we have essentially two approaches: first, to analyze the bias or the variance of the specific estimator carefully; second, to prove a lower bound that is satisfied by all the estimators, which naturally include the specific estimator we need to analyze. These two approaches have different relative advantages and disadvantages, so we utilize them together in the lower bound construction.

We refer the readers to Tsybakov [[72]](#b71) for a nice collection of techniques to prove minimax lower bounds. One specific approach we use is the van Trees inequality, which we quote below.

Let (X , F , P θ ; θ ∈ Θ) be a dominated family of distributions on some sample space X ; denote the dominating measure by µ. Assume Θ is a closed interval on the real line. Let f (x|θ) denote the density of P θ with respect to µ. Let π be some probability distribution on Θ with a density λ(θ) with respect to Lebesgue measure. Suppose that λ and f (x|•) are both absolutely continuous (µ-almost surely), and that λ converges to zero at the endpoints of the interval Θ. We define

$I(θ) = E θ ∂ log f (X|θ) ∂θ 2(65)$$I(λ) = E d log λ(θ) dθ 2(66)$the Fisher information for θ and for a location parameter in λ, respectively. We assume I(θ) is continuous in θ. We have the following inequality.

Lemma 6 (van Trees inequality). [[73]](#b72) Under assumptions above, the average risk of an arbitrary estimator ψ(X) in estimating an absolutely continuous functional ψ(θ) under squared error loss satisfies the following inequality:

$E ψ(X) -ψ(θ) 2 ≥ (Eψ ′ (θ)) 2 E[I(θ)] + I(λ)(67)$
## V. PROOFS OF THE UPPER BOUNDS

In order to upper bound the maximum squared error risk of any estimator, a natural approach would be to analyze the squared bias term and the variance term separately. Then, it suffices to find proper tools to give nonasymptotic analysis of the bias and variance.

## A. Bounding the bias

We first work to bound the bias. Lemma 3 shows that the bias of F (P n ) could be represented as

$Bias(F (P n )) = S i=1 (B n [f ](p i ) -f (p i )) ,(68)$where

$B n [f ](x)$is the Bernstein polynomial corresponding to f (x). The following lemma summarizes some state-of-theart bounds for approximation error of Bernstein polynomials. Lemma 7 can be derived easily from the general theory we presented in Section IV-B2. We emphasize that one cannot expect the bounds in Lemma 7 to be tight for any f ∈ C[0, 1], since the Bernstein approximation error itself could be a very complicated function in C[0, 1], and Lemma 7 is using relatively simple functions to upper bound it. Lemma 7. The following bounds are valid for function approximation error incurred by Bernstein polynomials: 1) Pointwise estimate: [62, Cor. 2.2.1] [74] for all continuous functions f on [0, 1],

$|f (x) -B n [f ](x)| ≤ 3 2 ω 2 f, x(1 -x) n ,(69)$and the constant 3/2 is shown by [[74]](#b73) to be the best constant;

2) Norm estimate: [[62,](#b61)[Cor. 4.1.10]](#) for ϕ(x) = x(1 -x) and all continuous functions f on [0, 1], we have

$B n [f ] -f ∞ ≤ 5 2 ω 2 ϕ (f, n -1/2 );(70)$3) [[75,](#b74)[Eqn. 10.3.4]](#) for f ∈ C[foot_1](#foot_1) [0, 1], i.e., twice continuously differentiable,

$|f (x) -B n [f ](x)| ≤ f ′′ ∞ x(1 -x) 2n ;(71)$Proof. The pointwise estimate of Lemma 7 follows from Lemma 4. The norm estimate of Lemma 7 follows from Lemma 5. Regarding the third part, suppose random variable X ∼ B(n, x). We have

$|f (x) -B n [f ](x)| = |E x f (X/n) -f (x)| (72) = |E x [f ′ (x)(X/n -x) + 1 2 f ′′ (ξ X )(X/n -x) 2 ]| (73) = 1 2 |E x f ′′ (ξ X )(X/n -x) 2 | (74) ≤ f ′′ ∞ 2 |E x (X/n -x) 2 | (75) = f ′′ ∞ 2 x(1 -x) n ,(76)$where we used Taylor expansion for f (X/n) at point x with the Lagrange remainder. The proof is complete.

Remark 1. Note that although [(70)](#b69) is in the form of an upper bound, it has been shown to be a lower bound as well.

Totik [[76]](#b75) showed the following equivalence property on the norm estimate of Bernstein approximation errors

$B n [f ](x) -f (x) ∞ ≍ ω 2 ϕ (f, n -1/2 ). 2(77)$It is easy to calculate the second-order modulus of smoothness and the Ditzian-Totik second-order modulus of smoothness for functions x α and -x ln x. The results are presented in the following lemma. Lemma 8. We have

$x α , 0 < α < 1 x α , 1 < α < 2 -x ln x ω 2 (f, t) |2 -2 α |t α |2 -2 α |t α t ln 4 ω 2 ϕ (f, t) |2 -2 α | t 2α (1+t 2 ) α ≍ t 2 t 2 ln 4 1+t 2$where the second-order modulus results hold for 0 < t ≤ 1/2, and the Ditizan-Totik second-order modulus results hold for 0 < t ≤ 1.

## 1) Bias of F α (P n ):

We first bound the bias incurred by F α (P n ).

## 1) α ≥ 2:

In this case, f ∈ C 2 [0, 1], applying the third part of Lemma 7,

$|f (x) -B n [f ](x)| ≤ α(α -1)x(1 -x) 2n .(78)$Thus, we have

$|Bias(F α (P n ))| ≤ S i=1 α(α-1) p i (1 -p i ) 2n ≤ α(α -1) 2n . (79) 2) 1 < α < 2$The following lemma presents a bound on the bias of F α (P n ), which does not depend on the alphabet size S. We note that the proof of Lemma 9 heavily utilizes the special properties of function x α and the fact that

$S i=1 p i = 1.$Lemma 9. The bias of F α (P n ) for estimating F α (P ), 1 < α < 2, is upper bounded by the following:

$|Bias(F α (P n ))| ≤ 4 n α-1 . (80$$)$We also present two additional bounds involving the alphabet size S. Using the pointwise estimate in Lemma 7, the bias term of the MLE is upper bounded as follows for all 0 < α < 2, α = 1:

$S i=1 3 2 |2 -2 α | p i (1 -p i ) n α/2 ≤ 3 2 |2 -2 α | 1 n α/2 S i=1 p α/2 i (81) ≤ 3 2 |2 -2 α | 1 n α/2 S 1 S α/2 (82) = 3 2 |2 -2 α | S 1-α/2 n α/2 . (83$$)$Using the norm estimate in Lemma 7, when 1 < α < 2, the bias would be upper bounded by C α,n

## 5S

2n , where

$C α,n = nω 2 ϕ (x α , n -1/2 ) is a finite positive constant such that lim sup n→∞ C α,n < ∞ for 1 < α < 2.$Combining Lemma 9, the pointwise estimate, and the norm estimate in Lemma 7, we know that the bias of

$F α (P n ) for 1 < α < 2 is upper bounded as |Bias(F α (P n ))| ≤ 4 n α-1 ∧ 3 2 |2 -2 α | S 1-α/2 n α/2 ∧ C α,n 5S 2n . (84) 3) 0 < α < 1:$The pointwise estimate from Lemma 7 is worked out in (83). Using the norm estimate in Lemma 7, the bias would be upper bounded by |2-2 α | 5S 2n α . Combining the pointwise estimate and the norm estimate, we know that the bias of F α (P n ) for 0 < α < 1 is upper bounded as

$|Bias(F α (P n ))| ≤ 3 2 |2 -2 α | S 1-α/2 n α/2 ∧ |2 -2 α | 5S 2n α . (85$) 2) Bias of H(P n ): We then bound the bias incurred by H(P n ). Using the norm estimate in Lemma 7, we know |Bias(H(P n ))| ≤ 5S ln 4 2n . (86) Using the pointwise estimate in Lemma 7, we obtain |Bias(H(P n ))| ≤ 3 2 S n ln 4. (87) It was shown by Paninski [14, Prop. 1] that the squared bias of MLE H(P n ) is upper bounded as

$(Bias(H(P n ))) 2 ≤ ln 1 + S -1 n 2 ,(88)$which is better than the two bounds we obtained using Bernstein polynomial results. However, we remark that (88) is obtained using special properties of the entropy function and connections between KL-divergence and χ 2 -divergence [[72]](#b71), which cannot be applied to general functions. Strukov and Timan [[66]](#b65) also heavily exploited the structure of function x α and -x ln x in order to analyze the Bernstein approximation error for these functions, and obtained tight-in-order results.

## 3) Bias of H( PB ):

We apply the general theory of positive linear operator approximation. The following lemma is a strengthened version of Lemma 5.

## Lemma 10. If

$F : C[0, 1] → R is a linear positive functional and F (e 0 ) = 1, then |F (f ) -f (x)| ≤ ω 1 (f, B F (x); x) + 5 2 ω 2 ϕ (f, h 2 )(89)$for all f ∈ C[0, 1] and 0 < h 2 ≤ 1 2 , where ϕ(x) = x(1 -x) and h 2 = √ V F /ϕ(x), and

$ω 1 (f, h; x) sup {|f (u) -f (x)| : u ∈ [0, 1], |u -x| ≤ h} . (90$$)$The "bias" B F (x) and "variance" V F (x) are defined in [(56)](#b55).

Proof. Applying Lemma 5 to x = F (e 1 ) we have

$|F (f ) -f (F (e 1 ))| ≤ 5 2 ω 2 ϕ (f, h 2 )(91)$and then (89) is the direct result of the triangle inequality

$|F (f ) -f (x)| ≤ |F (f ) -f (F (e 1 ))| + |f (F (e 1 )) -f (x)|.$We show that Lemma 10 is indeed stronger than Lemma 5. Firstly, due to 

$h 1 ≥ h 2 , we have ω 2 ϕ (f, h 2 ) ≤ ω 2 ϕ (f, h 1 ). Second, for x ≤ 1/2, we have B F (x) 2h 1 ϕ(x) • ω 1 ϕ (f, 2h 1 ) ≈ B F (x) 2h 1 ϕ(x) • sup 0≤s≤1 2h 1 ϕ(s)f ′ (s) (92) ≥ B F (x) • sup x≤s≤1-x f ′ (s) (93) ≈ sup x≤s≤1-x ω 1 (f, B F (x); s)(94$Note that Lemma 11 implies a slightly weaker bias bound than Theorem 4, but it is only sub-optimal up to a multiplicative constant. The bias bound in Theorem 4 is obtained using the following lemma, whose proof only applies to the entropy function.

$Lemma 12. If n ≥ max{2ea, Sa}, sup P ∈MS |E P H( PB ) -H(P )| ≤ ln 1 + S -1 n + Sa ∨ 2Sa n + Sa ln n + Sa 2a . (96$$)$
## B. Bounding the variance

The next lemma follows from an application of bounded difference inequality presented in Lemma 2.

Lemma 13. The variance of F (P n ) satisfies the following upper bound:

$Var(F (P n )) ≤ n • max 0≤j<n (f ((j + 1)/n) -f (j/n)) 2 . (97)$If f is monotone, then we can strengthen the bound to be

$Var(F (P n )) ≤ n 4 • max 0≤j<n (f ((j + 1)/n) -f (j/n)) 2 . (98$$)$We first bound the variance for F α (P n ), α > 1. We have

$max 0≤j<n (((j + 1)/n) α -(j/n) α ) 2 ≤ 1 -1 - 1 n α 2 (99) ≤ α n 2 ,(100)$where in the last step we used Bernoulli's inequality:

$(1 + x) r ≥ 1 + rx, ∀r ≥ 1, x > -1, x ∈ R.$Using Lemma 2, we know the variance is upper bounded by

$Var(F α (P n )) ≤ α 2 4n .(101)$We bound the variance of F α (P n ), 0 < α < 1 in the following lemma. Lemma 14. For 0 < α < 1/2, we have

$sup P ∈MS Var(F α (P n )) ≤ 10S n 2α + 3α • 2 3+2α + 1 8α 2 8α e 2α + 4 S n 2α ∧ 1 n 2α-1 (102) S n 2α . (103$$)$For 1/2 ≤ α < 1, we have

$sup P ∈MS Var(F α (P n )) ≤ 10S 2-2α n + 3α • 2 3+2α + 1 8α 2 8α e 2α + 4 S n 2α ∧ 1 n 2α-1 (104) S 2-2α n + S n 2α ∧ 1 n 2α-1 .(105)$Further, one can show that for all α ∈ (0, 1),

$3α • 2 3+2α + 1 8α 2 8α e 2α + 4 ≤ 120 α 2 ,(106)$which is used in Theorem 1.

Regarding the variance of H(P n ), we have Lemma 15.

sup

$P ∈MS Var(H(P n )) ≤ (ln n) 2 n ∧ 2(ln S + 3) 2 n (107$$)$$(ln S) 2 ∧ (ln n) 2 n .(108)$The variance of H( PB ) is upper bounded by the following lemma.

## Lemma 16. The variance of H( PB ) is upper bounded as follows:

$Var H( PB ) ≤ 2n (n + Sa) 2 3 + ln n + Sa a + 1 ∧ S 2 .(109)$
## VI. PROOFS OF THE LOWER BOUNDS

A. Lower bounds for estimation of F α (P ) when α ≥ 3/2

We apply the van Trees inequality as presented in Lemma 6. It suffices to consider the restricted case of S = 2 and prove the n -1 lower bound. Thus, the model is equivalent to observing a Binomial random variable X ∼ B(n, p), and one aims to estimate the functional ψ α (p) = p α + (1 -p) α . We have

$ψ ′ α (p) = αp α-1 -α(1 -p) α-1 . (110$$)$The Fisher information for parameter p under the Binomial model is I(p) = n p(1-p) . Suppose we impose prior λ(p) on parameter p. The van Trees inequality implies

$sup P ∈MS E P (F α (P n ) -F α (P )) 2 ≥ inf Fα sup P ∈MS E P Fα -F α (P ) 2 (111) ≥ E E[F α (P )|X S 1 ] -F α (P ) 2 (Bayes risk) (112) ≥ ( αp α-1 -α(1 -p) α-1 λ(p)dp) 2 E λ n p(1-p) + I(λ) (113) = ( αp α-1 -α(1 -p) α-1 λ(p)dp) 2 n • E λ 1 p(1-p) + I(λ)(114)$where the second inequality follows from the fact that the Bayes risk under any prior is upper bounded by the minimax risk [[78]](#b77).

Taking λ(p) to be the Dirichlet prior with parameter (a, b), i.e.,

$λ(p) = 1 B(a, b) p a-1 (1 -p) b-1 , a > 2, b > 2, (115$$)$we can explicitly evaluate the integrals above. Here B(a, b) is the Beta function.

Taking a = 4, b = 3, we have

$sup P ∈MS E P (F α (P n ) -F α (P )) 2 ≥ (60α(B(α + 3, 3) -B(α + 2, 4))) 2 5n + 45 . (116$$)$Taking C α = 72α 2 (B(α + 3, 3) -B(α + 2, 4)) 2 , we have

$sup P ∈MS E P (F α (P n ) -F α (P )) 2 ≥ C α n , for all n ≥ 1.(117)$Note that C α > 0 for all α ≥ 3/2.

## B. Lower bounds for estimation of

$F α (P ) when 1 < α < 3/2$The following lemma was proved in [[69]](#b68).

Lemma 17. Let k ≥ 4 be an even number. Suppose that the k-th derivative of f satisfies f (k) ≤ 0 in (0, 1), Q k-1 is the Taylor polynomial of order k -1 to f at some x 1 in (0, 1).

Then for x ∈ [0, 1],

$f (x) -B n [f ](x) ≥ Q k-1 -B n [Q k-1 ](x). (118$$) Consider f α (x) = -x α , 1 < α < 2, x ∈ [0, 1]$. Applying Lemma 17 to f α , taking k = 6, we have the following result.

$Lemma 18. Suppose f α (x) = -x α , 1 < α < 2 on [0, 1]. For all x ∈ (0, 1), we have f α (x) -B n [f α ](x) ≥ α(α -1)x α-2 (1 -x) 2n x + (2 -α)(3α -1)x 12n + (2 -α)(5 -3α) 12n + R 1 (x) n 3 + R 2 (x) n 4 , (119$$)$where

$R 1 (x) = α(α -1)(α -2)(α -3)x α-3 (1 -x)24$$× 1 + 2(1 -x)((5 -2α)x + α -4) , (120) R 2 (x) = α(α -1)(α -2)(α -3)(α -4) 120 × x α-4 (1 -x)(1 -2x)(1 -12x(1 -x)). (121)$Note that we have assumed S = cn, c > 0. If c ≤ 1, we take a uniform distribution on S elements P = (1/S, 1/S, . . . , 1/S), otherwise we take distribution P = (n -1 -ǫ, n -1 -ǫ, . . . , n -1 -ǫ, nǫ S-n , . . . , nǫ S-n ), where ǫ will be taken to be arbitrarily small. We first analyze the c ≤ 1 case. Applying Lemma 18, we have

$S i=1 f α (1/S) -B n [f α ](1/S) (Note that f α (x) = -x α ) = EF α (P n ) -F α (P ) ≥ S • α(α -1) 2S α-2 n 1 S + (2 -α)(5 -3α) 12n + α(α -1)(α -2)(α -3) 24S α-3 n 3 (1 + 2(α -4)) + α(α -1)(α -2)(α -3)(α -4) 120S α-4 n 4 + o(n -α ) = α(α -1) n α-1 1 2c α-3 1 c + (2 -α)(5 -3α)12$$+ (α -2)(α -3)(1 + 2(α -4)) 24c α-4 + (α -2)(α -3)(α -4) 120c α-5 + o(n -(α-1) ) = α(α -1)c 2-α n α-1 1 2 + (2 -α)(5 -3α)c24$$+ (α -2)(α -3)(1 + 2(α -4))c 224$$+ (α -2)(α -3)(α -4)c 3 120 + o(n -(α-1) ) ≥ αc 2-α (124 -330α + 285α 2 -90α 3 + 11α 4 ) 120n α-1 + o(n -(α-1) ),$where the first inequality follows from Lemma 18, and in the last step we have taken c = 1 in the following expression

$1 2 + (2 -α)(5 -3α)c 24 + (α -2)(α -3)(1 + 2(α -4))c 2 24 + (α -2)(α -3)(α -4)c 3 120 ,(122)$and considered the fact that it is a monotonically decreasing function with respect to c on (0, 1] for any α ∈ (1, 3/2).

For cases when c > 1, since we take P = (n -1 -ǫ, n -1ǫ, . . . , n -1 -ǫ, nǫ S-n , . . . , nǫ S-n ), by a continuity argument, the analysis is exactly the same as that above when we set c = 1 as we can take ǫ as small as possible. One can verify that the function α(124

$-330α + 285α 2 -90α 3 + 11α 4 )/120 is positive on interval (1, 3/2). Defining √ c α = αc 2-α (124 - 330α + 285α 2 -90α 3 + 11α 4 )/120 > 0 when c ≤ 1, and √ c α = α(124 -330α + 285α 2 -90α 3 + 11α 4 )/120 > 0 when c > 1, the proof is completed.$C. Lower bounds for estimation of F α (P ) when 0 < α < 1 Applying Lemma 17 to function f α (x) = x α , α ∈ (0, 1), taking k = 4, we have the following result: Lemma 19. For f α (x) = x α on [0, 1], α ∈ (0, 1), x ∈ (0, 1), we have

$f α (x) -B n [f α ](x) ≥ α(1 -α) 2n x α-2 (1 -x) x - 2 -α 3n . (123$$)$Suppose n ≥ S. Define distribution W = (w 1 , w 2 , . . . , w S ) ∈ M S such that

$1 ≤ i ≤ S -1, w i = 1 n ; w S = 1 - S -1 n . (124$$) Note that w i ≥ n -1 , 1 ≤ i ≤ S. It follows from Lemma 19 that F α (W ) -E W F α (P n ) ≥ S-1 i=1 α(1 -α) 6n 2 1 n α-2 1 - 1 n (125) = α(1 -α)(S -1) 6n α n -1 n . (126$$)$Thus, we know for all 0 < α < 1, sup

$P ∈MS E P (F α (P ) -F α (P n )) 2 ≥ α 2 (1 -α) 2 (S -1) 2 36n 2α 1 - 1 n 2 . (127$$)$It is shown in [[10]](#b9) that the following minimax lower bound holds for estimation of F α (P ), 1/2 ≤ α < 1.

Lemma 20. For 1 2 ≤ α < 1, we have inf

$F sup P ∈MS E P F -F α (P ) 2 ≥ α 2 32en (2(S -1)) 1-α -2 -α - 1 -α 4n (2(S -1)) 1-α + 2 -α 2 -e -n/4 S 2(1-α) S 2-2α n , (128$$)$where the infimum is taken over all possible estimators.

Since this lower bound holds for all possible estimators, it also holds for the MLE F α (P n ). Since max{a, b} ≥ 1 2 (a + b), we have the desired lower bound.

## D. Lower bounds for estimation of H(P )

Braess and Sauer [[69]](#b68) derived the following lower bound for the approximation error of Bernstein polynomials for the function g(x) = -x ln x:

$Lemma 21. Define g(x) = -x ln x on [0, 1]. For x ≥ 15 n , x ∈ [0, 1], we have g(x) -B n [g](x) ≥ 1 -x 2n + 1 20n 2 x - x 12n 2 . (129$$)$Applying Lemma 21 to the estimation of H(P ), we know that if ∀1 ≤ i ≤ S, p i ≥ 15  n ,

$H(P )-EH(P n ) ≥ S -1 2n + 1 20n 2 S i=1$1 p i -1 12n 2 . (130) Consider the uniform distribution P with n ≥ 15S, which guarantees p i ≥ 15 n . Since S i=1 1 p i ≥ S 2 , (131) we have sup P ∈MS (H(P ) -EH(P n )) ≥ S -1 2n + S 2 20n 2 -1 12n 2 . (132) Thus, when n ≥ 15S, sup P ∈MS E P (H(P ) -H(P n )) 2 ≥ S -1 2n + S 2 20n 2 -1 12n 2 2 . (133) It was shown in [21, Prop. 1] that the following minimax lower bound holds. Lemma 22. There exists a universal constant c > 0 such that inf Ĥ sup P ∈MS

$E P Ĥ -H(P ) 2 ≥ c ln 2 S n , (134$$)$where the infimum is taken over all possible estimators Ĥ.

Hence, we have

$sup P ∈MS E P (H(P ) -H(P n )) 2 ≥ max S -1 2n + S 2 20n 2 - 1 12n 2 2 , c ln 2 S n (135) ≥ 1 2 S -1 2n + S 2 20n 2 - 1 12n 2 2 + c 2 ln 2 S n . (136$$)$Similar arguments can be applied to the Miller-Madow estimator.

## E. Lower bounds for entropy estimation using H( PB )

Since H( PB ) is a specific estimator for entropy, the following lemma is proved via considering several specific distributions.

$Lemma 23. If n ≥ max{15S, Sa, 2ea}, sup P ∈MS E P H( PB ) -H(P ) ≥ (S -1)a 4(n + Sa) ln n + Sa a + S -1 8n + S 2 80n 2 - 1 48n 2 (137) If n < Sa, then sup P ∈MS E P H( PB ) -H(P ) ≥ S -1 2S ln S.(138)$If n < 2ea, then sup

$P ∈MS E P H( PB ) -H(P ) ≥ S -1 2e + S ln S.(139)$If n < 15S, n ≥ 2ea, then sup

$P ∈MS E P H( PB ) -H(P ) ≥ (S -1)a 4(n + Sa) ln n + Sa a + ⌊n/15⌋ 8n - 1 16n .(140)$The corresponding results in Theorem 5 follow from Lemma 23, Lemma 22, and the inequality max{a, b} ≥ a+b 2 .

## F. Lower bounds for entropy estimation using ĤBayes

We prove Theorem 6 below. Applying Lemma 25, we have

$ĤBayes ≤ ψ(Sa + n + 1) - S i=1 a + X i Sa + n ψ(a + 1) (141) = ψ(Sa + n + 1) -ψ(a + 1)(142)$$≤ ln Sa + n + e -γ a + 1 2 . (143$$)$Since ĤBayes is upper bounded by ln Sa+n+e -γ a+ 1 2 for any empirical observations, the squared error it incurs in Shannon entropy estimation when the true distribution is the uniform distribution is at least

$ln Sa + S/2 Sa + n + e -γ 2 (144) if S ≥ 2(n + 1).$
## APPENDIX A AUXILIARY LEMMAS

We begin with the definition of the negative association property, which allows us to upper bound the variance by treating each component of the empirical distribution P n (i) as "independent" random variables.

## Definition 1.

[79, Def.

$2.1] Random variables X 1 , X 2 , • • • , X S are$
## said to be negatively associated if for any pair of disjoint subsets

$A 1 , A 2 of {1, 2, • • • , S}, and any component-wise increasing functions f 1 , f 2 , Cov (f 1 (X i , i ∈ A 1 ), f 2 (X j , j ∈ A 2 )) ≤ 0.(145)$To verify whether random variables X 1 , X 2 , • • • , X S are negatively associated or not, the following lemma presents a useful criterion.

Lemma 24. [79, Thm. 2.9] Let X 1 , X 2 , • • • , X S be S independent random variables with log-concave densities. Then the joint conditional distribution of X 1 , X 2 , • • • , X S given S i=1 X i is negatively associated. In light of the preceding lemma, we can obtain the following corollary.

## Corollary 5. For any discrete probability distribution vector

$P ∈ M S , the random variables X = (X 1 , X 2 , • • • , X S ) drawn from the multinomial distribution X ∼ multi(n; P ) are negatively associated. Proof. Consider the Poissonized model Y i ∼ Poi(np i ), 1 ≤ i ≤ S with all Y i independent, it is straightforward to verify that each Y i possesses a log-concave distribu- tion. Then conditioning on S i=1 Y i = n, we know that (Y 1 , Y 2 , • • • , Y S )|( S i=1 Y i = n) ∼ multi(n; P ), hence$Lemma 24 yields the desired result.

The next lemma gives bounds on the digamma functions ψ(z) = Γ ′ (z) Γ(z) . Lemma 25. [80, [Lemma 1.7]](#) The digamma function ψ(z) is the only solution of the functional equation F (x + 1) = F (x)+ 1

x that is monotone, strictly concave on R + and satisfies F (1) = -γ, where γ ≈ 0.5772 is the Euler-Mascheroni constant.

Let x be a positive real number. Then, ln(x + 1/2) < ψ(x + 1) ≤ ln(x + e -γ ).

(146)

$If x ≥ 1, then ln(x + 1/2) < ψ(x + 1) ≤ ln(x + e 1-γ -1).(147)$The following lemma gives some tail bounds for Poisson or Binomial random variables.

$Lemma 26. [81, Exercise 4.7] If X ∼ Poi(λ) or X ∼ B(n, λ n )$, then for any δ > 0, we have

$P(X ≥ (1 + δ)λ) ≤ e δ (1 + δ) 1+δ λ ,(148)$$P(X ≤ (1 -δ)λ) ≤ e -δ (1 -δ) 1-δ λ ≤ e -δ 2 λ/2 . (149$$)$To establish the upper bound of the variance obtained by the plug-in estimator F α (P n ), we split p into two different regimes p ≤ 1/n or p > 1/n, and the following lemmas give the corresponding variance bounds.

Lemma 27. For nX ∼ B(n, p), p ≤ 1/n, we have

$Var(X α ) ≤ 2 n 2α ∧ 2p n 2α-1 0 < α < 1.(150)$Lemma 28. For nX ∼ B(n, p), p ≥ 1/n, 0 < α < 1, we have

$Var(X α ) ≤ 10p 2α-1 n + 3 2α 16α en 2α + 2 n 2α + 1 8α 2 8α en 2α .(151)$
## APPENDIX B PROOFS OF MAIN LEMMAS

## A. Proof of Lemma 3

We compute the first moment of F (P n ).

$EF (P n ) = n j=0 f j n Eh j ,(152)$and

$Eh j = E S i=1 ½(X i = j) (153) = S i=1 P(X i = j) (154) = S i=1 n j p j i (1 -p i ) n-j .(155)$Thus, we have

$EF (P n ) = n j=0 f j n S i=1 n j p j i (1 -p i ) n-j (156) = n j=0 S i=1 f j n n j p j i (1 -p i ) n-j .(157)$The bias of

$F (P n ) is Bias(F (P n )) = EF (P n ) -F (P ) (158) = S i=1   n j=0 f j n n j p j i (1 -p i ) n-j -f (p i )   .(159)$
## B. Proof of Lemma 8

We first compute the second-order modulus. Fix t,

$0 < t ≤ 1/2. Defining M u+v 2 , then the computation of second- order modulus is equivalent to maximization of |f (M -t) - 2f (M ) + f (M + t)| over interval M ∈ [t, 1 -t],$since all the functions we consider are strictly convex or concave over

$[0, 1]. For f (x) = x α , 0 < α < 1, f (x) is strictly concave on [0, 1]. It follows from Jensen's inequality that g(M ) = (M -t) α -2M α + (M + t) α ≤ 0,(160)$and it suffices to minimize this function of M in order to obtain the modulus. Taking derivative of g(M ), we have

$g ′ (M ) = α (M -t) α-1 -2M α-1 + (M + t) α-1 ≥ 0,(161)$$since x α-1 is a convex function on [t, 1-t].$It implies that the function g(M ) is non-decreasing, and the minimum of g(M ) over M ∈ [t, 1 -t] is attained at M = t, and the minimum value is g(t) = (2 α -2)t α . Hence, the corresponding secondorder modulus is |2 -2 α |t α .

Analogous procedures computes the second-order modulus for x α , 1 < α < 2 and -x ln x. Now we consider the computation of Ditzian-Totik secondorder modulus. Fix t,

$0 < t ≤ 1. Again denote M u+v 2 ∈ [0, 1]. Then the optimization is over the regime |u -v| ≤ 2tϕ(M ) = 2t M (1 -M ). Equivalently, it is the interval [M -t M (1 -M ), M + t M (1 -M )] ∩ [0, 1]. Since the function f (x) = -x ln x is strictly convex on [0, 1], the maximum of f (u) -2f u+v 2 + f (v) is definitely attained when u and v take the boundary values of the feasible interval [M -t M (1 -M ), M + t M (1 -M )] ∩ [0, 1]. Define ∆ t 1-M M .$The feasible interval can be equivalently written as

$[M -∆M, M + ∆M ] ∩ [0, 1]. We have M -t M (1 -M ) ≥ 0 ⇔ M ≥ t 2 1 + t 2 ,(162)$as well as

$M + t M (1 -M ) ≤ 1 ⇔ M ≤ 1 1 + t 2 . (163$$)$Hence, it is equivalent to maximize over three regimes: 1) Regime A:

$u = 0, v = 2M, 0 ≤ M ≤ t 2 1+t 2 . 2) Regime B: u = M -∆M, v = M + ∆M, M ∈ t 2 1+t 2 , 1 1+t2$3) Regime C:

$u = 2M -1, v = 1, 1 ≥ M ≥ 1 1+t 2 .$Over regime A, we have

$f (u) -2f u + v 2 + f (v) = 2M ln 2.(164)$Maximizing over 0 ≤ M ≤ t 2 1+t 2 , the maximum value is

$t 2 ln 4 1+t 2 , attained at M = t 2 1+t 2 . Over regime C, we have f (u) -2f u + v 2 + f (v) = |2M ln M -(2M -1) ln(2M -1)| . (165) Maximizing over 1 1+t 2 ≤ M ≤ 1, the maximum is attained at M = 1$1+t 2 , and the maximum value is no more than t 2 ln 4 1+t 2 . Now we consider regime B. Since M ∈

$t 2 1+t 2 , 1 1+t 2 in regime B, we know ∆ = t 1-M M ∈ [t 2 , 1]. We have f (u) -2f u + v 2 + f (v) = M |(1 -∆) ln(1 -∆) + (1 + ∆) ln(1 + ∆)| . (166) Since ∆ = t 1-M M implies M = t 2 t 2 +∆ 2 ,$we can recast the corresponding optimization problem as maximizing [1]](#b0). One can show that the maximum is always attained at ∆ = 1, with the maximum value t 2 ln 4 1+t 2 . To sum up, we conclude that when 0 < t ≤ 1, the maximum of the optimization problem defining ω 2 ϕ (-x ln x, t) is always attained at u = 0, v = 2t 2 1+t 2 , with the resulting modulus t 2 ln 4 1+t 2 . Analogous computation can also be done for function x α , 0 < α < 1. For the function x α , 1 < α < 2, it is hard to compute the modulus exactly, but it is easy to show that it is of order t 2 .

$t 2 ∆ 2 + t 2 |(1 -∆) ln(1 -∆) + (1 + ∆) ln(1 + ∆)| (167) subject to constraint ∆ ∈ [t 2 ,$
## C. Proof of Lemma 9

The bias of F α (P n ), 1 < α < 2 can be expressed as follows:

$|Bias(F α (P n ))| = |E S i=1 P α n (i) -p α i | (168) ≤ E i:pi≤ 1 n P α n (i) -p α i + E i:pi> 1 n P α n (i) -p α i (169) B 1 + B 2 . (170$$)$Now we bound B 1 and B 2 separately. It follows from Jensen's inequality that for any i,

$EP α n (i) ≥ p α i , 1 ≤ i ≤ S.(171)$Hence, we have

$B 1 = E i:pi≤ 1 n P α n (i) -p α i (172) = E i:pi≤ 1 n P α n (i) - (np i ) α n α (173) ≤ E i:pi≤ 1 n P α n (i) - (np i ) 2 n α (174) = E i:pi≤ 1 n (nP n (i)) α n α - (np i ) 2 n α (175) ≤ E i:pi≤ 1 n (nP n (i)) 2 n α - (np i ) 2 n α (176) = i:pi≤ 1 n E(nP n (i)) 2 n α - (np i ) 2 n α (177) = i:pi≤ 1 n (np i ) 2 + np i (1 -p i ) n α - (np i ) 2 n α(178)$$≤ i:pi≤ 1 n np i n α (179) = 1 n α-1 ,(180)$where we have used the fact that nP n (i) ≥ 1 for any P n (i) = 0.

Regarding B 2 , we have the following bounds:

$B 2 = E i:pi> 1 n P α n (i) -p α i (181) ≤ i:pi> 1 n E|P α n (i) -p α i |(182)$$≤ i:pi> 1 n 3 2 |2 -2 α | p i (1 -p i ) n α/2 (183) ≤ 3|2 -2 α | 2n α/2 i:pi> 1 n p α/2 i ,(184)$where the second inequality follows from the pointwise estimate in Lemma 7.

Denoting |{i :

$p i > 1 n }| = K ≤ n, we know i:pi> 1 n p α/2 i ≤ K 1-α/2 ≤ n 1-α/2 ,(185)$which implies that

$B 2 ≤ 3|2 -2 α | 2n α/2 n 1-α/2 = 3|2 -2 α | 2n α-1 .(186)$Therefore, we have

$|Bias(F α (P n ))| ≤ B 1 + B 2 (187) ≤ 1 n α-1 + 3|2 -2 α | 2n α-1 (188) ≤ 3|2 -2 α | + 2 2n α-1 (189) ≤ 4 n α-1 .$(190)

## D. Proof of Lemma 11

We apply Lemma 10. Note that h 2 = √ n n+Sa . In order to ensure that h 2 ≤ 1/2, it suffices to take n ≥ 4. Also, since n ≥ Sa, for any i,

$1 ≤ i ≤ S, |1 -p i S|a n + Sa ≤ Sa n + Sa ≤ 1 2 . (191$$)$Meanwhile, since the function

$S i=1$|1-piS|a n+Sa is a convex function of P = (p 1 , p 2 , . . . , p S ), it attains its maximum at one of the corner points of the simplex. Hence,

$S i=1 |1 -p i S|a n + Sa ≤ |1 -S|a n + Sa + (S -1) • a n + Sa (192) = 2(S -1)a n + Sa . (193$$)$In light of Lemma

10, we have |E P H( PB ) -H(P )| ≤ S i=1 ω 1 f, |1 -p i S|a n + Sa ; p i + 5n ln 2 (n + Sa) 2 (194) (a) ≤ -S i=1 |1 -p i S|a n + Sa ln 1 S S i=1 |1 -p i S|a n + Sa + 5nS ln 2 (n + Sa) 2 (195) (b) ≤ 2Sa n + Sa ln n + Sa 2a + 5nS ln 2 (n + Sa) 2 , (196) where (a) follows from the fact that if |x -y| ≤ 1/2, x, y ∈ [0, 1], then |x ln x -y ln y| ≤ -|x -y| ln |x -y| [82, Thm. 17.3.3] and Jensen's inequality. Step (b) follows from the fact that the function -y ln y is monotonically increasing on the interval [0, e -1 ], and 1 S S i=1 |1 -p i S|a n + Sa ≤ 2a n + Sa (197)

$≤ 2a n (198) ≤ e -1 ,(199)$where in the last step we used the assumption that n ≥ 2ea.

## E. Proof of Lemma 12

We have

$H( PB ) = S i=1 -p B,i ln pB,i(200)$= H(P B ) + S i=1 (p B,i -pB,i ) ln p B,i -S i=1 pB,i ln pB,i p B,i . (201) Taking expectations on both sides, we have EH( PB ) -H(P ) = H(P B ) -H(P ) -ED( PB P B ), (202) where D(P Q) = S i=1 p i ln pi qi is the KL divergence between distributions P and Q. Since H(P B ) = H n n+Sa P + Sa n+Sa U S , where U S denotes the uniform distribution with alphabet size S, it follows from Jensen's inequality and the concavity of the entropy function that H(P B ) ≥ n n + Sa H(P ) + Sa n + Sa H(U S ) (203) ≥ H(P ). (204) Hence, EH( PB ) -H(P ) ≤ max{H(P B ) -H(P ), ED( PB P B )}.

In order to analyze the bias, it suffices to analyze the two terms separately. We first analyze ED( PB P B ).

It follows from Jensen's inequality that

$D(P Q) = S i=1 p i ln p i q i ≤ ln S i=1 p 2 i q i ,(206)$whose derivation here follows from Tsybakov [[72,](#b71)[Lemma 2.7]](#). By Jensen's inequality, we have

$ED( PB P B ) ≤ E ln S i=1 p2 B,i p B,i ≤ ln S i=1 Ep 2 B,i p B,i.$(207) We also have

$S i=1 p 2 i q i = 1 + S i=1 (p i -q i ) 2 q i ,(208)$and that

$E(p B,i -p B,i ) 2 = n 2 (n + Sa) 2 E(p i -p i ) 2 = np i (1 -p i ) (n + Sa) 2 . (209) Hence, ED( PB P B ) ≤ ln 1 + S i=1 np i (1 -p i ) (n + Sa)(np i + a) (210) = ln 1 + S i=1 np i (1 -p i ) (n + Sa)np i np i np i + a (211) ≤ ln 1 + S i=1 1 -p i (n + Sa) ,(212)$which implies that

$ED( PB P B ) ≤ ln 1 + S -1 n + Sa . (213$$)$Now we consider the deterministic gap H(P B ) -H(P ). It follows from a refinement result of Cover and Thomas [[82,](#b81)[Thm. 17.3.3](#)] that when |p B,i -p i | ≤ 1/2 for all i, we have

$|H(P B ) -H(P )| ≤ -P B -P 1 ln P B -P 1 S (214) = S • f P B -P 1 S ,(215)$where

$f (x) = -x ln x, x ∈ [0, 1]. Note that the condition n ≥ Sa ensures that |p B,i -p i | ≤ 1/2.$We have

$1 S P B -P 1 = 1 S S i=1 Sa n + Sa |p i -1/S| (216) = 1 S S i=1 |1 -p i S|a n + Sa (217) ≤ 2a n + Sa ,(218)$where the last step follows from (192). Since we have assumed n ≥ 2ea, we have 2a n+Sa ≤ 2a n ≤ e -1 . Since the function f (x) = -x ln x is monotonically increasing on the interval [0, e -1 ], we know

$|H(P B ) -H(P )| ≤ 2Sa n + Sa ln n + Sa 2a .(219)$F. Proof of [Lemma 13](#) In our case, apparently F (P n ) is a function of n independent random variables {Z i } 1≤i≤n taking values in Z = {1, 2, . . . , S}. Changing one location of the sample would make some symbol with count j to have count j + 1, and another symbol with count i to have count i -1. Then the total change in the functional estimator is

$f j + 1 n -f j n -f i n + f i -1 n . (220$$)$If f is monotone, then the total change would be upper bounded by max 0≤j<n |f ((j + 1)/n) -f (j/n)|. If f is not monotone, the total change can be upper bounded by 2 • max 0≤j<n |f ((j + 1)/n) -f (j/n)|. Applying Lemma 2, we have the desired bounds.

## G. Proof of Lemma 14

In light of Lemma 27 and 28, we have

$S i=1 Var(P n (i) α ) = i:pi≤1/n Var(P n (i) α ) + i:pi>1/n Var(P n (i) α ) (221) ≤ i:pi≤1/n 2 n 2α ∧ 2p i n 2α-1 + i:pi>1/n 10p 2α-1 i n + 3 2α 16α en 2α + 2 n 2α + 1 8α 2 8α en 2α .(222)$We obtain the desired bounds after using the concavity of x 2α-1 when 1/2 ≤ α < 1.

Now we exploit the negative association property of all random variables P n (i), 1 ≤ i ≤ S. Corollary 5 and the monotonically increasing property of x α yield

$Var(F α (P n )) = S i=1 Var(P n (i) α ) + 2 1≤i<j≤S Cov(P n (i) α , P n (j) α ) (223) ≤ S i=1 Var(P n (i) α ),(224)$which finishes the proof of Lemma 14.

## H. Proof of Lemma 15

The upper bound (ln n) 2 /n follows from Lemma 13. We apply the Efron-Stein inequality (Lemma 1) to obtain the other bound. Denote the n i.i.d. samples from distribution P as Z 1 , Z 2 , . . . , Z n ∈ Z. Denoting the MLE H(P n ) as Ĥ(Z 1 , Z 2 , . . . , Z n ), since it is invariant to any permutation of {Z 1 , Z 2 , . . . , Z n }, we know that the Efron-Stein inequality implies

$Var(H(P n )) ≤ n 2 E Ĥ(Z 1 , Z 2 , . . . , Z n ) -Ĥ(Z ′ 1 , Z 2 , . . . , Z n ) 2 , (225) where Z ′ 1 is an i.i.d. copy of Z 1 . Recall that X i = n j=1 ½(Z j = i), 1 ≤ i ≤ S.(226)$For notional brevity, we denote the S-tuple (X 1 , X 2 , . . . , X S ) as X S 1 , and the n-tuple (Z 1 , Z 2 , . . . , Z n ) as Z n 1 . A specific realization of (X 1 , X 2 , . . . , X S ) is denoted by x S 1 = (x 1 , x 2 , . . . , x S ), and a specific realization of

$(Z 1 , Z 2 , . . . , Z n ) is denoted by z n 1 = (z 1 , z 2 , . . . , z n ).$In order to upper bound the right hand side of (225), we first condition on {X 1 , X 2 , . . . , X S }. In other words, we use

$E Ĥ(Z 1 , Z 2 , . . . , Z n ) -Ĥ(Z ′ 1 , Z 2 , . . . , Z n ) 2 (227) = E E Ĥ(Z 1 , Z 2 , . . . , Z n ) -Ĥ(Z ′ 1 , Z 2 , . . . , Z n ) 2 X S 1 .(228)$The following lemma calculates the conditional distribution of Z 1 conditioned on (X 1 , X 2 , . . . , X S ).

Lemma 29. The conditional distribution of Z 1 conditioned on (X 1 , X 2 , . . . , X S ) is given by the following discrete distribution:

(X 1 /n, X 2 /n, . . . , X S /n).

Proof. By definition of conditional distribution, for any k, 1 ≤ k ≤ S, we have

$P(Z 1 = k|X S 1 = x S 1 ) = P(Z 1 = k, X S 1 = x S 1 ) P(X S 1 = x S 1 )(230)$$= P(Z 1 = k)P(X S 1 = x S 1 |Z 1 = k) P(X S 1 = x S 1 )(231)$$= p k n-1 x1,x2,...,x k -1,...,xS p x k -1 k i =k p xi i n x1,x2,...,xS 1≤i≤S p xi i (232) = n-1 x1,x2,...,x k -1,...,xS n x1,x2,...,xS(233)$$= x k n ,(234)$where the multinomial coefficient n x1,x2,...,xS is defined as n

$x 1 , x 2 , . . . , x S = n! S i=1 x i ! . (235$$)$Denoting r(p) = -p ln p, we have r(j/n) -j n ln j n . We rewrite

$Ĥ(Z ′ 1 , Z 2 , . . . , Z n )-Ĥ(Z 1 , Z 2 , . . . , Z n ) = D -+D + ,(236)$where

$D -= r X Z1 -1 n -r X Z1 n(237)$$D + =    r X Z ′ 1 +1 n -r X Z ′ 1 n Z 1 = Z ′ 1 r X Z ′ 1 n -r X Z ′ 1 -1 n Z 1 = Z ′ 1 (238)$Here, D -is the change in Ĥ that occurs when Z 1 is removed according to the distribution specified in Lemma 29, and D + is the change in Ĥ that occurs when Z ′ 1 is added back according to the true distribution P .

## Now we compute

$E[D 2 -|X S 1 ] and E[D 2 + |X S 1 ]. We have E[D 2 -|X S 1 ] = 1≤i≤S X i n r X i -1 n -r X i n 2 ,(239)$and

$E[D 2 + |X S 1 ] = 1≤i≤S p i X i n r X i n -r X i -1 n 2 + 1≤i≤S p i 1 - X i n r X i + 1 n -r X i n 2 .(240)$Note that we interpret Xi n r Xi n -r Xi-1 n 2 as 0 when

$X i = 0. Taking expectations of E[D 2 -|X S 1 ] and E[D 2 + |X S 1 ] with respect to X S 1 , we have E[D 2 -] = 1≤i≤S 1≤j≤n j n r j -1 n -r j n 2 × P(B(n, p i ) = j)(241)$and

$E[D 2 + ] = 1≤i≤S 0≤j≤n j n r j -1 n -r j n 2 + 1 - j n r j n -r j + 1 n 2 × p i P(B(n, p i ) = j).(242)$After some algebra, one can show that E

$[D 2 + ] = E[D 2 -]. It then follows from (225) that Var(H(P n )) ≤ n 2 • E Ĥ(Z 1 , Z 2 , . . . , Z n ) -Ĥ(Z ′ 1 , Z 2 , . . . , Z n ) 2 (243) = n 2 • E (D -+ D + ) 2 (244) ≤ n • E(D 2 -+ D 2 + ) (245) ≤ 2nED 2 - (246) = 2n • 1≤i≤S EP n (i) r(P n (i)) -r(P n (i) - 1 n ) 2(247)$The proof above is an elaborate version of that in [14, App. B.3]. Now we proceed to obtain non-asymptotic upper bounds of (247). For x ≥ 1/n, it follows from Taylor expansion with integral form residue that

$(x - 1 n ) ln(x - 1 n ) = x ln x + (ln x + 1)(- 1 n ) + x-1 n x (x - 1 n -u) 1 u du. (248)$Then, we have

$r(P n (i)) -r P n (i) - 1 n ≤ | ln P n (i) + 1| n + Pn(i)-1 n Pn(i) P n (i) -1 n u du + 1 n (249) ≤ | ln P n (i) + 1| + 2 n .(250)$Hence, we have

$Var(H(P n )) ≤ 2n • 1≤i≤S EP n (i) | ln P n (i) + 1| + 2 n 2 .$(251) Noting that ln P n (i) ≤ 0, hence 0 ≤ | ln P n (i) + 1| ≤ 1ln P n (i). We have

$Var(H(P n )) ≤ 2 n • 1≤i≤S EP n (i) (ln P n (i) -3) 2 (252) ≤ 2 n 1≤i≤S p i (ln p i -3) 2 (253) ≤ 2 n S • 1 S (-ln S -3) 2 (254) = 2(ln S + 3) 2 n ,(255)$where we have used the fact that x(ln x -3) 2 is a concave function on [0, 1].

## I. Proof of Lemma 16

We apply the bounded differences inequality (Lemma 2). In our case, F ( PB ) is a function of n independent random variables {Z i } 1≤i≤n taking values in Z = {1, 2, • • • , S}. Changing one location of the sample would make some symbol with count j to have count j + 1, and another symbol with count i to have count i -1. Then the absolute value of the total change in the functional estimator is

$f j + 1 + a n + Sa -f j + a n + Sa -f i + a n + Sa + f i -1 + a n + Sa (256) ≤ 2 max 1≤k≤n f k + a n + Sa -f k -1 + a n + Sa .(257)$In light of the Taylor expansion with integral form residue, we have that for 1 ≥ x ≥ t > 0, (x -t) ln(x -t) = x ln x -t(ln x + 1) +

$x-t x x -t -u u du (258) so |(x -t) ln(x -t) -x ln x| ≤ t| ln x + 1| + x-t x x -t u du + t (259) ≤ t| ln x + 1| + 2t (260) ≤ t(3 -ln x).(261)$As a result,

$max 1≤k≤n f k + a n + Sa -f k -1 + a n + Sa ≤ max 1≤k≤n 1 n + Sa 3 -ln k + a n + Sa (262) ≤ 1 n + Sa 3 + ln n + Sa a + 1 .(263)$Hence, the bounded differences inequality shows that

$Var H( PB ) ≤ n max 2≤k≤n f k + a n + Sa -f k -1 + a n + Sa 2 (264) ≤ n (n + Sa) 2 3 + ln n + Sa a + 1 2 ,(265)$which completes the proof of the first part.

To prove the second part, we use the Efron-Stein inequality (Lemma 1). Since H

$( PB ) = ĤB (Z 1 , • • • , Z n ) is invariant to any permutation of (Z 1 , Z 2 , • • • , Z n ), we know that the Efron-Stein inequality implies Var H( PB ) ≤ n 2 E ĤB (Z ′ 1 , Z 2 , • • • , Z n ) -ĤB (Z 1 , Z 2 , • • • , Z n ) 2 ,(266)$where Z ′ 1 is an i.i.d. copy of Z 1 . Recall that

$X i = n j=1 ½(Z j = i), 1 ≤ i ≤ S.(267)$For brevity, we denote the S-tuple (X 1 , • • • , X S ) as X S 1 , and the n-tuple

$(Z 1 , • • • , Z n ) as Z n 1 . A specific realization of (X 1 , • • • , X S ) is denoted by x S 1 = (x 1 , • • • , x S )$, and a specific realization of (Z 1 ,

$• • • , Z n ) is denoted by z n 1 = (z 1 , • • • , z n ). Then we have E ĤB (Z ′ 1 , Z 2 , • • • , Z n ) -ĤB (Z 1 , Z 2 , • • • , Z n ) 2 (268) = x S 1 P(X S 1 = x S 1 ) × E ĤB (Z ′ 1 , Z 2 , • • • , Z n ) -ĤB (Z 1 , Z 2 , • • • , Z n ) 2 X S 1 = x S 1 .(269)$In light of Lemma 29, we know that the conditional distribution of Z 1 conditioned on (X 1 , • • • , X S ) is the discrete distribution (X 1 /n, X 2 /n, • • • , X S /n). Denoting r(p) = f ( np+a n+Sa ),

we can rewrite

$ĤB (Z ′ 1 , Z 2 , • • • , Z n ) -ĤB (Z 1 , Z 2 , • • • , Z n ) = D -+ D +(270)$where

$D -= r X Z1 -1 n -r X Z1 n(271)$$D + =    r X Z ′ 1 +1 n -r X Z ′ 1 n Z 1 = Z ′ 1 r X Z ′ 1 n -r X Z ′ 1 -1 n Z 1 = Z ′ 1 .(272)$Here, D -is the change in ĤB that occurs when Z 1 is removed according to the distribution (X 1 /n, X 2 /n, • • • , X S /n), and D + is the change in Ĥ that occurs when Z ′ 1 is added back according to the true distribution P . Now we have 

$E[D 2 -|X S 1 ] = S i=1 X i n r X i -1 n -r X i n 2(273)$$E[D 2 + |X S 1 ] = S i=1 p i X i n r X i n -r X i -1 n 2 + S i=1 p i 1 - X i n r X i + 1 n -r X i n 2($After some algebra we can show that E[D

2 -] = E[D 2 + ]. Hence, we have Var H( PB ) ≤ n 2 E (D -+ D + ) 2 ≤ nE D 2 -+ D 2 + = 2nED 2 -(277) = 2n S i=1 EP n (i) r(P n (i) -1 n ) -r(P n (i)) 2 (278) ≤ 2n S i=1 EP n (i) 1 n + Sa 3 -ln nP n (i) + a n + Sa 2 (279) = 2n (n + Sa) 2 S i=1 EP n (i) 3 -ln nP n (i) + a n + Sa 2 (280) ≤ 2n (n + Sa) 2 S i=1 p i 3 -ln np i + a n + Sa 2 (281) ≤ 2n (n + Sa) 2 S • 1 S 3 -ln n/S + a n + Sa 2 (282) = 2n(3 + ln S) 2 (n + Sa) 2 (283) where we have used the inequality (261) and Jensen's inequality due to d 2 dx 2 x ln nx + a n + Sa -3 2 = n nx + a 3 ln nx + a n + Sa -3 + nx nx + a 4 -ln nx + a n + Sa (284) < 0. (285) J. Proof of Lemma 18 It is well known (see, e.g. [75, Cor. 10.4.2]

) that if f is concave in (0, 1), then

$f (x) -B n [f ](x) ≥ 0, 0 ≤ x ≤ 1.(286)$Hence we focus on deriving the other bound. For concave function f α (x) = -x α , α ∈ (1, 2), Taylor's polynomial of degree 5 at x = x 0 takes the form 

$B n [(x -x 0 ) 3 ](x 0 ) = x 0 (1 -x 0 ) n 2 (1 -2x 0 )(288)$B n [(x -x 0 ) 4 ](x 0 ) = 3

$x 2 0 (1 -x 0 ) 2 n 2 + x 0 (1 -x 0 ) n 3 [1 -6x 0 (1 -x 0 )](289)$B n [(x -x 0 ) 5 ](x 0 ) = 10

$x 2 0 (1 -x 0 ) 2 n 3 + x 0 (1 -x 0 ) n 4 [1 -12x 0 (1 -x 0 )] × (1 -2x 0 )(290)$Applying Lemma 17 and Lemma 30, taking x 0 = x, we have the desired bound.

## K. Proof of Lemma 19

It is well known (see, e.g. [[75,](#b74)[Cor. 10.4.2]](#)) that if f is concave in (0, 1), then f (x) -B n [f ](x) ≥ 0, 0 ≤ x ≤ 1.

(291)

Hence we focus on deriving the other bound. For function f α (x) = x α , Taylor's polynomial of degree 3 at x = x 0 takes the form

$Q 3 (x) = α(α -1) 2 x α-2 0 (x -x 0 ) 2 + α(α -1)(α -2) 6 x α-3 0 (x -x 0 ) 3 + affine terms of x.(292)$Applying Lemma 17 and Lemma 30, taking x 0 = x, we have

$Q 3 (x) -B n [Q 3 ](x) = - α(α -1) 2 x α-2 x(1 -x) n - α(α -1)(α -2) 6 x α-3 x(1 -x) n 2 (1 -2x) (293) = α(1 -α) 2n x α-2 (1 -x) x - 2 -α 3n (1 -2x) (294) ≥ α(1 -α) 2n x α-2 (1 -x) x - 2 -α 3n .(295)$Hence, we have

$f α (x) -B n [f α ](x) ≥ Q 3 (x) -B n [Q 3 ](x)(296)$≥ α(1 -α) 2n

x α-2 (1 -x) x -2 -α 3n .

(297)

## L. Proof of Lemma 23

By setting P = (1, 0, 0,

• • • , 0), we have H(P ) = 0 and H( PB ) = -(S -1)a n + Sa ln a n + Sa -n + a n + Sa ln n + a n + Sa (298) ≥ (S -1)a n + Sa ln n + Sa a , (299) hence we have obtained the first lower bound sup P ∈MS E P H( PB ) -H(P ) ≥ (S -1)a n + Sa ln n + Sa a . (300) If n < Sa, then sup P ∈MS E P H( PB ) -H(P ) ≥ (S -1)a 2Sa ln S (301) ≥ S -1 2S ln S. (302) If n > 2ea, then sup P ∈MS E P H( PB ) -H(P ) ≥ (S -1)a Sa + 2ea ln S (303) = S -1 S + 2e ln S. (304) From now on we assume n ≥ Sa, n ≥ 2ea. For n ≥ 15S, it follows from applying Lemma 21 that sup P ∈MS E P H( P ) -H(P ) ≥ S -1 2n + S 2 20n 2 -1 12n 2 . (305) If n < 15S, then it follows from applying Lemma 21 that one can essentially take S = ⌊n/15⌋ in (305), and obtain sup P ∈MS E P H( P ) -H(P ) ≥ ⌊n/15⌋ 2n -1 4n . (306) It follows from a refinement result of Cover and Thomas [82, Thm. 17.3.3] that when |p B,i -pi | ≤ 1/2 for all i (which is ensured by condition n ≥ Sa), we have |H( PB ) -H( P )| ≤ Sf PB -P 1 S , (307) where f (x) = -x ln x, x ∈ [0, 1]. We have 1 S PB -P 1 = 1 S S i=1 |S pi -1|a n + Sa (308) ≤ 2(S -1)a S(n + Sa) , (309) where the last step follows from (192). Since we have assumed n ≥ 2ea, we have 2a n+Sa ≤ 2a n ≤ e -1 . Since the function f (x) = -x ln x, x ∈ [0, 1] is monotonically increasing when x ∈ [0, e -1 ], we have |H( PB ) -H( P )| ≤ 2(S -1)a n + Sa ln n + Sa a . (310) A combination of these two inequalities yield the second lower bound sup P ∈MS E P H( PB ) -H(P ) ≥ S -1 2n + S 2 20n 2 -1 12n 2 -2(S -1)a n + Sa ln n + Sa a (311) when n ≥ 15S, and the second lower bound sup P ∈MS E P H( PB ) -H(P ) ≥ ⌊n/15⌋ 2n -1 4n -2(S -1)a n + Sa ln n + Sa a (312)

when n < 15S.

Hence we are done by using these two lower bounds and the inequality max{a, b} ≥ 3a+b 4 .

Tsachy's research is focused on information theory, statistical signal processing, the interplay between them, and their applications.

He is recipient of several best paper awards, and prizes for excellence in research.

He served on the editorial board of the IEEE TRANSACTIONS ON INFORMATION THEORY from Sept. 2010 to Aug. 2013, and currently serves on the editorial board of Foundations and Trends in Communications and Information Theory.

![If n ≫ S and a is upper bounded by a constant, then the maximum squared error risk of H( PB ) vanishes. Conversely, if n S, then the maximum squared error risk of H( PB ) is bounded away from zero.]()

![[62, Cor. 2.2.1.] Let F : C(I) → R be a positive linear functional, where]()

![[62, Thm. 2.5.1.] If F : C[0, 1] → R is a linear positive functional and F (e 0 ) = 1, then we have]()

![) which is almost the supremum of ω 1 (f, |F (e 1 -xe 0 )|; s) over s ∈ [x, 1 -x] and is no less than the pointwise result ω 1 (f, |F (e 1 -xe 0 )|; x), and here we have used the inequality ϕ(s) ≥ ϕ(x) for x ≤ s ≤ 1-x. A similar argument also holds for x > 1/2. Hence, Lemma 10 transforms the first order term from the norm result in Lemma 5 to a pointwise result.Applying Lemma 10 to the function f (p) = -p ln p andF (f ) = E f n p+a n+Sa , where n • p ∼ B(n, p),we have the following lemma. Lemma 11. If n ≥ max{Sa, 2ea, 4}, then sup P ∈MS |E P H( PB ) -H(P )| ≤ 5nS ln 2 (n + Sa) 2 + 2Sa n + Sa ln n + Sa 2a .]()

![274)where we define r(x) = 0 when x / ∈ [0, 1]. Then, by the law of iterated expectation, we know thatE[D 2 -P(B(n, p i ) = j) p i P(B(n, p i ) = j).]()

![α(α -1)(α -2)(α -3)(α -4) 120 x α-affine terms of xWe know that the Bernstein polynomial of any affine function on [0, 1] is the affine function itself, hence it suffices to consider the non-affine part of Q 5 (x). [69, Prop. 4] showed the following results for Bernstein polynomials:Lemma 30. Let 0 ≤ x 0 ≤ 1. Then we have B n [(x -x 0 ) 2 ](x 0 ) = x 0 (1 -x 0 ) n (287)]()

In the literature of combinatorics, the sum n j=0 a j,n B j,n (x) is called the Bernoulli sum, and various approaches have been proposed to evaluate its asymptotics[[58]](#b57),[[59]](#b58),[[60]](#b59).

Note that it is a remarkable fact that (77) holds for any continuous function f (x). The lower bound proof of (77) is considered one of the remarkable results in approximation theory, and currently there are no "short" proofs of this fact. Indeed, Ditzian[[77,](#b76) Section 8]  mentioned that "I still would like to see a new simple proof of (8.4) (Equation (77)) which I am sure will have implications for other operators."

