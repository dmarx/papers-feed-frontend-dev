<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Maximum Likelihood Estimation of Functionals of Discrete Distributions</title>
				<funder ref="#_gZ8fVUg">
					<orgName type="full">Center for Science of Information (CSoI)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2017-08-10">10 Aug 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiantao</forename><surname>Jiao</surname></persName>
							<email>jiantao@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yanjun</forename><surname>Han</surname></persName>
							<email>yjhan@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tsachy</forename><surname>Weissman</surname></persName>
							<email>tsachy@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Maximum Likelihood Estimation of Functionals of Discrete Distributions</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-08-10">10 Aug 2017</date>
						</imprint>
					</monogr>
					<idno type="MD5">A55A896BFA8A8AF788A0D697405FA6E8</idno>
					<idno type="arXiv">arXiv:1406.6959v7[cs.IT]</idno>
					<note type="submission">received Month 00, 0000; revised Month 00, 0000; accepted Month 00, 0000. Date of current version Month 00, 0000.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-entropy estimation</term>
					<term>maximum likelihood estimator</term>
					<term>Dirichlet prior smoothing</term>
					<term>approximation theory</term>
					<term>high dimensional statistics</term>
					<term>Rényi entropy</term>
					<term>approximation using positive linear operators</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider the problem of estimating functionals of discrete distributions, and focus on tight (up to universal multiplicative constants for each specific functional) nonasymptotic analysis of the worst case squared error risk of widely used estimators. We apply concentration inequalities to analyze the random fluctuation of these estimators around their expectations, and the theory of approximation using positive linear operators to analyze the deviation of their expectations from the true functional, namely their bias.</p><p>We explicitly characterize the worst case squared error risk incurred by the Maximum Likelihood Estimator (MLE) in estimating the Shannon entropy H(P ) = S i=1 -pi ln pi, and the power sum Fα(P ) = S i=1 p α i , α &gt; 0, up to universal multiplicative constants for each fixed functional, for any alphabet size S ≤ ∞ and sample size n for which the risk may vanish. As a corollary, for Shannon entropy estimation, we show that it is necessary and sufficient to have n ≫ S observations for the MLE to be consistent. In addition, we establish that it is necessary and sufficient to consider n ≫ S 1/α samples for the MLE to consistently estimate Fα(P ), 0 &lt; α &lt; 1. The minimax rate-optimal estimators for both problems require S/ ln S and S 1/α / ln S samples, which implies that the MLE has a strictly sub-optimal sample complexity. When 1 &lt; α &lt; 3/2, we show that the worst-case squared error rate of convergence for the MLE is n -2(α-1) for infinite alphabet size, while the minimax squared error rate is (n ln n) -2(α-1) . When α ≥ 3/2, the MLE achieves the minimax optimal rate n -1 regardless of the alphabet size.</p><p>As an application of the general theory, we analyze the Dirichlet prior smoothing techniques for Shannon entropy estimation. In this context, one approach is to plug-in the Dirichlet prior smoothed distribution into the entropy functional, while the other one is to calculate the Bayes estimator for entropy under the Dirichlet prior for squared error, which is the conditional expectation. We show that in general such estimators do not improve over the maximum likelihood estimator. No matter how we tune the parameters in the Dirichlet prior, this approach cannot achieve the minimax rates in entropy estimation. The performance of the minimax rate-optimal estimator with n samples is essentially at least as good as that of Dirichlet smoothed entropy estimators with n ln n samples.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Entropy and related information measures arise in information theory, statistics, machine learning, biology, neuroscience, image processing, linguistics, secrecy, ecology, physics, and finance, among other fields. Numerous inferential tasks rely on data driven procedures to estimate these quantities (see, e.g. <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b5">[6]</ref>). We focus on two concrete and well-motivated examples of information measures, namely the Shannon entropy <ref type="bibr" target="#b6">[7]</ref> </p><formula xml:id="formula_0">H(P ) S i=1 -p i ln p i ,<label>(1)</label></formula><p>and the power sum F α (P ), α &gt; 0:</p><formula xml:id="formula_1">F α (P ) S i=1 p α i , α &gt; 0.<label>(2)</label></formula><p>The power sum F α (P ) functional often emerges in various operational problems <ref type="bibr" target="#b7">[8]</ref>. It also has connections to the Rényi entropy <ref type="bibr" target="#b8">[9]</ref> H α (P ) via the formula H α (P ) = ln Fα(P ) 1-α . Consider estimating the Shannon entropy H(P ) based on n i.i.d. samples following unknown discrete distribution P with unknown alphabet size S. This problem has a rich history with extensive study in various fields ranging from information theory, statistics, neuroscience, physics, psychology, medicine, etc. We refer the reader to <ref type="bibr" target="#b9">[10]</ref> for a review. One of the most widely used estimators for this purpose is the Maximum Likelihood Estimator (MLE), which is simply the empirical entropy. The empirical entropy is an instantiation of the plugin principle in functional estimation, where a point estimate of the parameter (distribution P in this case) is used to construct an estimator for a functional of the parameter via the plug-in approach. The idea of using the MLE for estimating information measures of interest (in this case entropy), is not only intuitive, but has sound justification: asymptotic efficiency.</p><p>The beautiful theory of Hájek and Le Cam <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b12">[13]</ref> shows that, as the number of observed samples grows without bound while the finite parameter dimension (e.g., alphabet size) remains fixed, the MLE performs optimally in estimating any differentiable functional when the statistical model complies with the benign LAN (Local Asymptotic Normality) condition <ref type="bibr" target="#b12">[13]</ref>. Thus, for finite dimensional problems, the problems of parameter and functional estimation are well understood in an asymptotic sense, and the MLE appears to be not only natural but also theoretically justified. But does it make sense to employ the MLE to estimate the entropy in most practical applications?</p><p>As it turns out, while asymptotically optimal in entropy estimation, the MLE is by no means sacrosanct in many real applications, especially in regimes where the alphabet size is comparable to, or even larger than the number of observations. It was shown that the MLE for entropy is strictly sub-optimal in the large alphabet regime <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. Therefore, classical asymptotic theory does not satisfactorily address high dimensional settings, which are becoming increasingly important in the modern era of high dimensional statistics.</p><p>There has been a wave of recent research activities focusing on analyzing existing approaches of functional estimation, as well as proposing new estimators that are provably near optimal in the large alphabet regime. Paninski <ref type="bibr" target="#b13">[14]</ref> showed that the MLE needs n ≫ S samples to consistently estimate the Shannon entropy, and Paninski <ref type="bibr" target="#b14">[15]</ref> established the existence of a (non-explicit) estimator that only required n ≪ S samples. It implies that the MLE is strictly sub-optimal in terms of sample complexity. It was Valiant and Valiant <ref type="bibr" target="#b15">[16]</ref> who first explicitly constructed a linear programming based estimator (later modified in <ref type="bibr" target="#b16">[17]</ref>) that achieves consistency in entropy estimation with n ≫ S/ ln S samples, which they also proved to be necessary. <ref type="bibr">Valiant and Valiant [18]</ref> constructed another approximation based estimator that achieved better theoretical properties than the linear programming ones, which was not yet shown to be minimax rate-optimal for all ranges of S and n. The authors <ref type="bibr" target="#b9">[10]</ref> constructed the first minimax rate-optimal estimators for H(P ) and F α (P ), α &gt; 0 based on best polynomial approximation, which are agnostic to the alphabet size S. Utilizing the released MATLAB and Python packages of the estimators in <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref> demonstrated that these minimax rate-optimal estimators can lead to significant performance boosts in various machine learning tasks. Wu and Yang <ref type="bibr" target="#b20">[21]</ref> independently applied the best polynomial approximation idea to entropy estimation and obtained the minimax rates. However, their estimator requires the knowledge of the alphabet size S. The approximation ideas proved to be very fruitful in Acharya et al. <ref type="bibr" target="#b21">[22]</ref>, Wu and Yang <ref type="bibr" target="#b22">[23]</ref>, Han, Jiao, and Weissman <ref type="bibr" target="#b23">[24]</ref>, Jiao, Han, and Weissman <ref type="bibr" target="#b24">[25]</ref>, Bu et al. <ref type="bibr" target="#b25">[26]</ref>, Orlitsky, Suresh, and Wu <ref type="bibr" target="#b26">[27]</ref>, Wu and Yang <ref type="bibr" target="#b27">[28]</ref>.</p><p>The main contribution of this paper is an explicit characterization of the worst case squared error risk of estimating H(P ) and F α (P ) using the MLE up to a universal multiplicative constant for each specific functional, for all ranges of S and n in which the risk may vanish. Understanding the benefits and limitations of the MLE in a nonasymptotic setting serves two key purposes. First, the approach is a natural benchmark for comparing other more nuanced procedures for estimation of functionals. Second, performance analysis for the MLE reveals regimes where the problem is difficult, and motivates the development of improvements, which have been validated in <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>. As a byproduct of the analysis, we explicitly point out an equivalence between bias analysis of functional estimators using plug-in rules and approximation theory using positive linear operators. We believe these powerful tools introduced from approximation theory may have far reaching impacts in various applications in the information theory community.</p><p>We mention that there exist numerous other approaches proposed in various disciplines to estimate entropy, many among which are difficult to analyze theoretically. Among them we mention the Miller-Madow bias-corrected estimator and its variants <ref type="bibr" target="#b28">[29]</ref>- <ref type="bibr" target="#b30">[31]</ref>, the jackknife estimator <ref type="bibr" target="#b31">[32]</ref>, the shrinkage estimator <ref type="bibr" target="#b32">[33]</ref>, the coverage adjusted estimator <ref type="bibr" target="#b33">[34]</ref>, the Best Upper Bound (BUB) estimator <ref type="bibr" target="#b13">[14]</ref>, the B-Splines estimator <ref type="bibr" target="#b34">[35]</ref>, and <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref> etc. For a Bayesian statistician, a natural approach is to first impose a prior on the unknown discrete distribution before considering estimating entropy. The Dirichlet prior, being the conjugate prior to the multinomial distribution, appears to be particularly popular in the Bayesian approach to entropy estimation. Dirichlet smoothing may have two connotations in the context of entropy estimation:</p><p>• <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref> One first obtains a Bayes estimate for the discrete distribution P , which we denote by PB , and then plugs it in the entropy functional to obtain the entropy estimate H( PB ).</p><p>• <ref type="bibr" target="#b39">[40]</ref>  <ref type="bibr" target="#b40">[41]</ref> One calculates the Bayes estimate for entropy H(P ) under Dirichlet prior for squared error. The estimator is the conditional expectation E[H(P )|X], where X represents the samples.</p><p>Nemenman, Shafee, and Bialek <ref type="bibr" target="#b41">[42]</ref> argued in an intuitive way why Dirichlet prior is bad for entropy estimation and proposed to use mixtures of Dirichlet priors. Archer, Park, and Pillow <ref type="bibr" target="#b42">[43]</ref> have come up with priors that perform better than the Dirichlet prior. Also see <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>.</p><p>Another contribution of this paper is an explicit characterization of the worst case squared error risk of estimating H(P ) using the Dirichlet prior plug-in approach up to a universal multiplicative constant, for all ranges of S and n in which the risk may vanish. We show rigorously that neither of the two approaches utilizing the Dirichlet prior result in improvements over the MLE in the large alphabet regime. Specifically, these approaches require at least n ≫ S to be consistent, while the minimax rate-optimal estimators such as the ones in <ref type="bibr" target="#b9">[10]</ref>  <ref type="bibr" target="#b20">[21]</ref> only need n ≫ S ln S to achieve consistency. The rest of the paper is organized as follows. We present the main results in Section III, discuss the fundamental ideas behind the proofs in Section IV, and detail the proofs in Section V and VI. Proofs of auxiliary lemmas are deferred to the appendices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PRELIMINARIES</head><p>The Dirichlet distribution with order S ≥ 2 with parameters α 1 , . . . , α S &gt; 0 has a probability density function with respect to Lebesgue measure on the Euclidean space R S-1 given by</p><formula xml:id="formula_2">f (x 1 , • • • , x S ; α 1 , • • • , α S ) = 1 B(α) S i=1 x αi-1 i (3)</formula><p>on the open S -1-dimensional simplex defined by:</p><formula xml:id="formula_3">x 1 , • • • , x S-1 &gt; 0 (4) x 1 + • • • + x S-1 &lt; 1 (5) x S = 1 -x 1 -• • • -x S-1<label>(6)</label></formula><p>and zero elsewhere. The normalizing constant is the multinomial Beta function, which can be expressed in terms of the Gamma function:</p><formula xml:id="formula_4">B(α) = S i=1 Γ(α i ) Γ S i=1 α i , α = (α 1 , • • • , α S ).<label>(7)</label></formula><p>Assuming the unknown discrete distribution P follows prior distribution P ∼ Dir(α), and we observe a vector X = (X 1 , X 2 , . . . , X S ) with multinomial distribution multi(n; p 1 , p 2 , . . . , p S ), then one can show that the posterior distribution P P |X is also a Dirichlet distribution with parameters</p><formula xml:id="formula_5">α + X = (α 1 + X 1 , α 2 + X 2 , . . . , α S + X S ) .<label>(8)</label></formula><p>Furthermore, the posterior mean (conditional expectation) of p i given X is given by <ref type="bibr" target="#b45">[46,</ref><ref type="bibr">Example 5.4.4]</ref> </p><formula xml:id="formula_6">δ i (X) E[p i |X] = α i + X i n + S i=1 α i .<label>(9)</label></formula><p>The estimator δ i (X) is widely used in practice for various choices of α. For example, if α i = √ n S , then the corresponding (δ 1 (X), δ 2 (X), . . . , δ S (X)) is the minimax estimator for P under squared loss <ref type="bibr" target="#b45">[46,</ref><ref type="bibr">Example 5.4.5]</ref>. However, it is no longer minimax under other loss functions such as ℓ 1 loss, which was investigated in <ref type="bibr" target="#b46">[47]</ref>.</p><p>Note that the estimator δ i (X) subsumes the MLE pi = Xi n as a special case, since we can take the limit α → 0 for δ i (X) to obtain MLE. We denote the empirical distribution by P n = (p 1 , p2 , . . . , pS ). The Dirichlet prior smoothed distribution estimate is denoted as PB , where</p><formula xml:id="formula_7">PB = n n + S i=1 α i P n + S i=1 α i n + S i=1 α i α S i=1 α i . (<label>10</label></formula><formula xml:id="formula_8">)</formula><p>Note that the smoothed distribution PB can be viewed as a convex combination of the empirical distribution P n and the prior distribution α S i=1 αi . We call the estimator H( PB ) the Dirichlet prior smoothed plug-in estimator.</p><p>Another way to apply Dirichlet prior in entropy estimation is to compute the Bayes estimator for H(P ) under squared error, given that P follows Dirichlet prior. It is well known that the Bayes estimator under squared error is the conditional expectation. It was shown in Wolpert and Wolf <ref type="bibr" target="#b39">[40]</ref> that <ref type="bibr" target="#b10">(11)</ref> where ψ(z)</p><formula xml:id="formula_9">ĤBayes E[H(P )|X] = ψ 1 + S i=1 (α i + X i ) - S i=1 α i + X i S i=1 (α i + X i ) ψ(α i + X i + 1),</formula><formula xml:id="formula_10">Γ ′ (z) Γ(z)</formula><p>is the digamma function. We call the estimator ĤBayes the Bayes estimator under Dirichlet prior.</p><p>Throughout this paper, we observe n i.i.d. samples from an unknown discrete distribution P = (p 1 , p 2 , . . . , p S ). We denote the n samples as n i.i.d. random variables {Z i } 1≤i≤n taking values in Z = {1, 2, . . . , S} with probability (p 1 , p 2 , . . . , p S ). Defining</p><formula xml:id="formula_11">X i n j=1 ½(Z j = i), 1 ≤ i ≤ S,<label>(12)</label></formula><p>we know that (X 1 , X 2 , . . . , X S ) follows a multinomial distribution with parameter (n; p 1 , p 2 , . . . , p S ). Denote h j S i=1 ½(X i = j), 0 ≤ j ≤ n. The Maximum Likelihood Estimator (MLE) for H(P ) and F α (P ) are defined, respectively, as H(P n ) and F α (P n ), with P n being the empirical distribution. We assume the functional F (P ) takes the form</p><formula xml:id="formula_12">F (P ) = S i=1 f (p i ). (<label>13</label></formula><formula xml:id="formula_13">)</formula><p>Then it is evident that the MLE F (P n ) for estimating functional F (P ) in ( <ref type="formula" target="#formula_12">13</ref>) can be alternatively represented as the following linear function of (h 0 , h 1 , . . . , h n ):</p><formula xml:id="formula_14">F (P n ) = n j=0 f j n h j .<label>(14)</label></formula><p>Recall that the risk function under squared error for any estimator F in estimating functional F (P ) may be decomposed as</p><formula xml:id="formula_15">E P (F (P ) -F ) 2 = (E P F -F (P )) 2 + E P F -E P F 2 ,<label>(15)</label></formula><p>where (E P F -F (P )) 2 represents the squared bias, and</p><formula xml:id="formula_16">E P F -E P F 2</formula><p>represents the variance. The subscript P means that the expectation is taken with respect to the distribution P that generates the i.i.d. observations. We omit the subscript for the expectation operator E if the meaning of the expectation is clear from the context.</p><p>Notation: a ∧ b denotes min{a, b}, a ∨ b denotes max{a, b}. For two non-negative series {a n }, {b n }, notation a n b n means that there exists a positive universal constant C &lt; ∞ such that an bn ≤ C, for all n. The notation a n ≍ b n is equivalent to a n b n and b n a n . Notation a n ≫ b n means that lim inf n→∞ an bn = ∞. Throughout this paper, the notations , , ≪, ≫ involve absolute constants that may only depend on α but not S or n. We denote by M S the space of discrete distributions with alphabet size S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MAIN RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Estimating F α (P )</head><p>We split the upper bounds and the lower bounds into two theorems, and present their succinct summaries in Corollary 1 and 2.</p><p>Theorem 1 (Upper bounds). We have the following upper bounds on the worst case squared error risk of MLE in estimating F α (P ):</p><formula xml:id="formula_17">1) α ≥ 2: sup P ∈MS E P (F α (P n ) -F α (P )) 2 ≤ α(α -1) 2n 2 + α 2 4n .<label>(16)</label></formula><p>2) 1 &lt; α &lt; 2:</p><p>sup</p><formula xml:id="formula_18">P ∈MS E P (F α (P n ) -F α (P )) 2 ≤ 4 n α-1 ∧ 3S 1-α/2 n α/2 ∧ C α,n 5S 2n 2 + α 2 4n ,<label>(17)</label></formula><p>where and <ref type="figure">ω 2</ref> ϕ is the second-order Ditzian-Totik modulus of smoothness introduced in Section IV-B.</p><formula xml:id="formula_19">C α,n nω 2 ϕ (x α , n -1/2 ) &gt; 0 satisfies lim sup n→∞ C α,n &lt; ∞ for 1 &lt; α &lt; 2,</formula><formula xml:id="formula_20">3) 1/2 ≤ α &lt; 1: sup P ∈MS E P (F α (P n ) -F α (P )) 2 ≤ 3S 1-α/2 2n α/2 ∧ 5S 2n α 2 + 10S 2-2α n + 120 α 2 S n 2α ∧ 1 n 2α-1 . (18) 4) 0 &lt; α &lt; 1/2: sup P ∈MS E P (F α (P n ) -F α (P )) 2 ≤ 3S 1-α/2 2n α/2 ∧ 5S 2n α 2 + 10S n 2α + 120 α 2 S n 2α ∧ 1 n 2α-1 . (<label>19</label></formula><formula xml:id="formula_21">)</formula><p>Moreover, in all the bounds presented above, the first term bounds the square of the bias, and the second term bounds the variance.</p><p>Theorem 2 (Lower bounds). We have the following lower bounds on the worst case squared error risk of MLE in estimating F α (P ):</p><p>1) α ≥ 3/2: there exists a constant C α &gt; 0 such that for all n,</p><formula xml:id="formula_22">sup P ∈MS E P (F α (P n ) -F α (P )) 2 ≥ C α n .<label>(20)</label></formula><p>2) 1 &lt; α &lt; 3/2: if S = cn, for any c &gt; 0, then</p><formula xml:id="formula_23">lim inf n→∞ n 2(α-1) • sup P ∈MS E P (F α (P n ) -F α (P )) 2 &gt; 0.<label>(21)</label></formula><p>3)</p><formula xml:id="formula_24">1/2 ≤ α &lt; 1: if n ≥ S, then sup P ∈MS E P (F α (P n ) -F α (P )) 2 ≥ α 2 (1 -α) 2 72n 2α (S -1) 2 1 - 1 n 2 + α 2 64en (2(S -1)) 1-α -2 -α - 1 -α 4n (2(S -1)) 1-α + 2 -α 2 - 1 2 e -n/4 S 2(1-α) ,<label>(22) 4</label></formula><formula xml:id="formula_25">) 0 &lt; α &lt; 1/2: if n ≥ S, then sup P ∈MS E P (F α (P n ) -F α (P )) 2 ≥ α 2 (1 -α) 2 36n 2α (S -1) 2 1 - 1 n 2 . (<label>23</label></formula><formula xml:id="formula_26">)</formula><p>There are several interesting implications of this result, highlighted in the following corollaries.</p><p>Corollary 1. For any fixed α &gt; 1, there exist universal convergence rates for F α (P ):</p><p>sup</p><formula xml:id="formula_27">S∈N+ sup P ∈MS E P (F α (P n ) -F α (P )) 2 ≍ n -2(α-1) 1 &lt; α &lt; 3/2 n -1 α ≥ 3/2<label>(24)</label></formula><p>Corollary 1 implies that, when α ≥ 3/2, estimation of F α (P ) is extremely simple in terms of convergence rate: plugin estimation achieves the best possible rate n -1 (as shown in the theory of regular statistical experiments of classical asymptotic theory, see <ref type="bibr" target="#b47">[48,</ref><ref type="bibr">Chap. 1.7.]</ref>). Results of this form have appeared in the literature, for example, Antos and Kontoyiannis <ref type="bibr" target="#b48">[49]</ref> showed that it suffices to take n ≫ 1 samples to consistently estimate F α (P ), α ≥ 2, α ∈ Z. However, when 1 &lt; α &lt; 3/2, the rate n -2(α-1) is considerably slower. Interestingly, there exist estimators that demonstrate better convergence rates for estimating F α (P ), 1 &lt; α &lt; 3/2. Jiao et al. <ref type="bibr" target="#b9">[10]</ref> showed that the minimax rate in estimating F α (P ), 1 &lt; α &lt; 3/2, is (n ln n) -2(α-1) as long as S n ln n, which is achieved using the general methodology developed therein for constructing minimax rate-optimal estimators for nonsmooth functionals.</p><p>Let us now examine the case 0 &lt; α &lt; 1, another interesting regime that has not been characterized before. In this regime, we observe significant increase in the difficulty of the estimation problem. In particular, the relative scaling between the number of observations n and the alphabet size S for consistent estimation of F α (P ) exhibits a phase transition, encapsulated in the following.</p><p>Corollary 2. Fix α ∈ (0, 1). The worst case squared error risk of the MLE F α (P n ) in estimating F α (P ) is characterized as follows when n ≥ S:</p><formula xml:id="formula_28">sup P ∈MS E P (F α (P n ) -F α (P )) 2 ≍ S 2 n 2α + S 2-2α n 1/2 &lt; α &lt; 1 S 2 n 2α 0 &lt; α ≤ 1/2<label>(25)</label></formula><p>Corollary 2 follows directly from Theorem 1 and Theorem 2. In particular, it implies that it is necessary and sufficient to take n ≫ S 1/α samples to consistently estimate F α (P ), 0 &lt; α &lt; 1 using MLE. Thus, as one might expect, the scale of the number of measurements required for consistent estimation increases as α decreases. When α → 0, the number of samples required for the MLE grows super-polynomially in S, which is consistent with the intuition that F α (P ), α → 0 is essentially equivalent to the alphabet size of a distribution, whose estimation is known to be very hard when there may exist symbols with very small probabilities <ref type="bibr" target="#b49">[50]</ref>.</p><p>We exhibit some of our findings by plotting the value required of ln n/ ln S for consistent estimation of F α (P ) using the MLE F α (P n ), as a function of α, in Figure <ref type="figure">1</ref>.</p><formula xml:id="formula_29">not achievable via MLE (Theorem 2) 1 0 1 2 α ln n ln S 1/α achievable via MLE (Theorem 1)</formula><p>Fig. <ref type="figure">1</ref>: For any fixed point above the thick curve, consistent estimation of F α (P ) is achieved using MLE F α (P n ) as shown in Theorem 1. For any fixed point below the thick curve in the regime 0 &lt; α &lt; 1, Theorem 2 shows that the MLE does not have vanishing maximum squared error risk.</p><p>It turns out that one can construct estimators that are better than the MLE in terms of required sample complexity for consistent estimation for the regime 0 &lt; α &lt; 1. Indeed, Jiao et al. <ref type="bibr" target="#b9">[10]</ref> showed that the minimax rate-optimal estimator requires n ≫ S 1 α ln S samples to achieve consistency, which attains a logarithmic improvement in the sample complexity over the MLE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Estimating H(P )</head><p>We not only consider H(P n ), but also the so-called Miller-Madow bias-corrected estimator <ref type="bibr" target="#b28">[29]</ref> defined as</p><formula xml:id="formula_30">H MM (P n ) = H(P n ) + S -1 2n . (<label>26</label></formula><formula xml:id="formula_31">)</formula><p>Theorem 3. The worst case squared error risk of H(P n ) admits the following upper bound for all S, n:</p><formula xml:id="formula_32">sup P ∈MS E P (H(P n ) -H(P )) 2 ≤ ln 1 + S -1 n 2 + (ln n) 2 n ∧ 2(ln S + 3) 2 n . (<label>27</label></formula><formula xml:id="formula_33">)</formula><p>If n ≥ 15S, then</p><formula xml:id="formula_34">sup P ∈MS E P (H(P n ) -H(P )) 2 ≥ 1 2 S -1 2n + S 2 20n 2 - 1 12n 2 2 + c ln 2 S n . (<label>28</label></formula><formula xml:id="formula_35">)</formula><p>Moreover, if n ≥ 15S, the Miller-Madow bias-corrected estimator satisfies</p><formula xml:id="formula_36">sup P ∈MS E P H MM (P n ) -H(P ) 2 ≥ 1 2 S 2 20n 2 - 1 12n 2 2 + c ln 2 S n ,<label>(29)</label></formula><p>where the positive constant c &gt; 0 in both expressions does not depend on S or n.</p><p>Theorem 3 implies the following corollary.</p><p>Corollary 3. The worst case squared error risk of the MLE H(P n ) in estimating H(P ) is characterized as follows when n ≥ 15S:</p><p>sup</p><formula xml:id="formula_37">P ∈MS E P (H(P n ) -H(P )) 2 ≍ S 2 n 2 + ln 2 S n . (<label>30</label></formula><formula xml:id="formula_38">)</formula><p>Here the first term corresponds to the squared bias, and the second term corresponds to the variance.</p><p>Paninski <ref type="bibr" target="#b13">[14]</ref> showed that if n = cS, where c &gt; 0 is a constant, the maximum squared error risk of H(P n ), and the Miller-Madow bias-corrected estimator H MM (P n ), would be bounded from zero. Paninski <ref type="bibr" target="#b13">[14]</ref> also showed that when n ≫ S, n → ∞, the MLE is consistent for estimating entropy. Corollary 3 implies that it is necessary and sufficient to take n ≫ S samples for the MLE to be consistent for estimating entropy. Comparing the results for H(P ) with those for F α (P ), we see that the intuition that H(P ) being viewed close to F α (P ) when α → 1 -1 is indeed approximately correct as H(P ) coincides with α → 1 -on the phase transition curve shown in Figure <ref type="figure">1</ref>.</p><p>Table <ref type="table">I</ref> summarizes the minimax squared error rates and the worst case squared error rates of the MLE in estimating H(P ) and F α (P ), α &gt; 0. It is clear that the MLE cannot achieve the minimax rates for estimation of H(P ), and F α (P ) when 0 &lt; α &lt; 3/2. In these cases, there exist strictly better estimators whose performance with n samples is roughly the same as Minimax squared error rates Maximum squared error rates of MLE H(P ) S 2</p><p>(n ln n) 2 + ln 2 S n (n S/ ln S) ( [10], <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b20">[21]</ref></p><formula xml:id="formula_39">) S 2 n 2 + ln 2 S n (n S) (Corollary 3) F α (P ), 0 &lt; α ≤ 1 2 S 2 (n ln n) 2α n S 1/α / ln S, ln n ln S ( [10]) S 2 n 2α n S 1/α (Corollary 2) F α (P ), 1 2 &lt; α &lt; 1 S 2 (n ln n) 2α + S 2-2α n n S 1/α / ln S ( [10]) S 2 n 2α + S 2-2α n n S 1/α (Corollary 2) F α (P ), 1 &lt; α &lt; 3 2 (n ln n) -2(α-1) (S n ln n) ( [10]) n -2(α-1) (S n) (Corollary 1) F α (P ), α ≥ 3 2 n -1 (Theorem 1) n -1</formula><p>TABLE I: Summary of results in this paper and the companion <ref type="bibr" target="#b9">[10]</ref> that of the MLE with n ln n samples. This phenomenon was termed effective sample size enlargement in <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Dirichlet prior techniques applying to entropy estimation</head><p>For symmetry, we restrict attention to the case where the parameter α in the Dirichlet distribution takes the form (a, a, . . . , a).</p><p>In comparison to MLE H(P n ), where P n is the empirical distribution, the Dirichlet smoothing scheme H( PB ) has a disadvantage: it requires the knowledge of the alphabet size S in general. We define</p><formula xml:id="formula_40">pB,i = np i + a n + Sa ,<label>(31)</label></formula><p>and</p><formula xml:id="formula_41">p B,i = E[p B,i ] = np i + a n + Sa . (<label>32</label></formula><formula xml:id="formula_42">)</formula><p>It is clear that</p><formula xml:id="formula_43">PB = n n + Sa P n + Sa n + Sa U S<label>(33)</label></formula><formula xml:id="formula_44">P B = n n + Sa P + Sa n + Sa U S ,<label>(34)</label></formula><p>where P n stands for the empirical distribution, P is the true distribution, and U S denotes the uniform distribution on the same alphabet with size S.</p><p>Theorem 4. If n ≥ max{Sa, 2ea}, then the maximum squared error risk of H( PB ) in estimating H(P ) is upper bounded as</p><formula xml:id="formula_45">sup P ∈MS E P H( PB ) -H(P ) 2 ≤ ln 1 + S -1 n + Sa ∨ 2Sa n + Sa ln n + Sa 2a 2 + 2n (n + Sa) 2 3 + ln n + Sa a + 1 ∧ S 2 . (<label>35</label></formula><formula xml:id="formula_46">)</formula><p>Here the first term bounds the squared bias, and the second term bounds the variance.</p><p>Theorem 5. If n ≥ max{15S, Sa, 2ea}, then the maximum</p><formula xml:id="formula_47">L 2 risk of H( PB ) in estimating H(P ) is lower bounded as sup P ∈MS E P H( PB ) -H(P ) 2 ≥ 1 2 (S -1)a 4(n + Sa) ln n + Sa a + S -1 8n + S 2 80n 2 - 1 48n 2 2 + c ln 2 S n , (<label>36</label></formula><formula xml:id="formula_48">)</formula><p>where c &gt; 0 is a universal constant that does not depend on a, S, or n.</p><p>If n &lt; Sa, then we have</p><formula xml:id="formula_49">sup P ∈MS E P H( PB ) -H(P ) 2 ≥ S -1 2S 2 ln 2 S. (<label>37</label></formula><formula xml:id="formula_50">)</formula><p>If n &lt; 2ea, then we have</p><formula xml:id="formula_51">sup P ∈MS E P H( PB ) -H(P ) 2 ≥ S -1 S + 2e 2 ln 2 S. (<label>38</label></formula><formula xml:id="formula_52">)</formula><p>If n &lt; 15S, n ≥ 2ea, then we have</p><formula xml:id="formula_53">sup P ∈MS E P H( PB ) -H(P ) 2 ≥ (S -1)a 4(n + Sa) ln n + Sa a + ⌊n/15⌋ 8n - 1 16n + 2 , (<label>39</label></formula><formula xml:id="formula_54">)</formula><p>where ⌊x⌋ is the largest integer that does not exceed x, and (x) + = max{x, 0} represents the positive part of x.</p><p>The following corollary immediately follows from Theorem 4 and Theorem 5. The next theorem presents a lower bound on the maximum risk of the Bayes estimator under Dirichlet prior. Since we have assumed that all α i = a, 1 ≤ i ≤ S, the Bayes estimator under Dirichlet prior is</p><formula xml:id="formula_55">ĤBayes = ψ(Sa + n + 1) - S i=1 a + X i Sa + n ψ(a + X i + 1). (<label>40</label></formula><formula xml:id="formula_56">) Theorem 6. If S ≥ 2(n + 1), then sup P ∈MS E P ĤBayes -H(P ) 2 ≥ ln Sa + S/2 Sa + n + e -γ 2 , (<label>41</label></formula><formula xml:id="formula_57">)</formula><p>where γ ≈ 0.5772 is the Euler-Mascheroni constant.</p><p>Evident from Theorem 4, 5, and 6 is the fact that in the best situation (i.e. a not too large), both the Dirichlet prior smoothed plug-in estimator and the Bayes estimator under Dirichlet prior still require at least n ≫ S samples to be consistent, which is the same as MLE. In contrast, the estimators in Valiant and Valiant <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b17">[18]</ref>, Jiao et al. <ref type="bibr" target="#b9">[10]</ref>, Wu and Yang <ref type="bibr" target="#b20">[21]</ref> are consistent if n ≫ S ln S , which is the optimal sample complexity. Thus, we can conclude that the Dirichlet smoothing technique does not solve the entropy estimation problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. FUNDAMENTAL IDEAS OF OUR ANALYSIS</head><p>In this section, we discuss the fundamental tools we employed to obtain the results in Section III, as well as general recipes we suggest for analyzing performances of functional estimators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Variance</head><p>The variance characterizes the degree to which the random variable F ( P ) is fluctuating around its expectation, and the field of concentration inequalities perfectly fits our glove to give the desired results. For all the functionals we consider, it turns out that the Efron-Stein inequality <ref type="bibr" target="#b50">[51]</ref> and the bounded differences inequality give very tight bounds. For completeness we state them below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 1.</head><p>[52, Efron-Stein inequality, Theorem 3.1] Let Z 1 , . . . , Z n be independent random variables and let</p><formula xml:id="formula_58">f (Z 1 , Z 2 , . . . , Z n ) be a square integrable function. Moreover, if Z ′ 1 , Z ′ 2 , . . . , Z ′ n are independent copies of Z 1 , Z 2 , .</formula><p>. . , Z n and if we define, for every i = 1, 2, . . . , n,</p><formula xml:id="formula_59">f ′ i = f (Z 1 , Z 2 , . . . , Z i-1 , Z ′ i , Z i+1 , . . . , Z n ),<label>(42)</label></formula><p>then</p><formula xml:id="formula_60">Var(f ) ≤ 1 2 n i=1 E (f -f ′ i ) 2 . (<label>43</label></formula><formula xml:id="formula_61">)</formula><p>The following inequality, which is called the bounded differences inequality, is a useful corollary of the Efron-Stein inequality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 2. [52, Bounded differences inequality, Corollary 3.2] If function</head><formula xml:id="formula_62">f : Z n → R has the bounded differences property, i.e., for some nonnegative constants c 1 , c 2 , . . . , c n , sup z1,...,zn,z ′ i ∈Z |f (z 1 , . . . , z n ) -f (z 1 , . . . , z i-1 , z ′ i , z i+1 , . . . , z n )| ≤ c i ,<label>(44)</label></formula><p>for every</p><formula xml:id="formula_63">1 ≤ i ≤ n, then Var(f (Z 1 , Z 2 , . . . , Z n )) ≤ 1 4 n i=1 c 2 i ,<label>(45)</label></formula><p>given that Z 1 , Z 2 , . . . , Z n are independent random variables.</p><p>We refer the readers to Boucheron et al. <ref type="bibr" target="#b51">[52]</ref> for a modern exposition of the concentration inequality toolbox.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Bias</head><p>It turns out that the bias analysis in estimation, albeit widely studied in statistics, seems to still largely bear an asymptotic and expansion nature in the mainstream statistical literature <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>. In particular, the bootstrap <ref type="bibr" target="#b54">[55]</ref> as a method for estimating functionals was essentially only analyzed in an asymptotic setting <ref type="bibr" target="#b55">[56]</ref>. Among asymptotic analysis techniques, probably the most popular one is the Taylor expansion. We will show that the Taylor expansion may encounter great difficulties in analyzing the bias of MLE in information measure estimation. Then, we will introduce the field of approximation theory using positive linear operators and demonstrate that it is essentially equivalent to nonasymptotic bias analysis for plug-in functional estimators. In doing so, we present the readers with abundant handy tools from approximation theory, which could be readily applicable to many problems that may seem highly intractable with standard expansion methods.</p><p>We start from entropy estimation. In the literature, considerable effort has been devoted to understanding the nonasymptotic performance of the MLE H(P n ) in estimating H(P ). One of the earliest investigations in this direction is due to Miller <ref type="bibr" target="#b28">[29]</ref> in 1955, who showed that, for any fixed distribution P ,</p><formula xml:id="formula_64">EH(P n ) = H(P ) - S -1 2n + O 1 n 2 . (<label>46</label></formula><formula xml:id="formula_65">)</formula><p>Equation ( <ref type="formula" target="#formula_64">46</ref>) was later refined by Harris <ref type="bibr" target="#b56">[57]</ref> using higher order Taylor series expansions to yield</p><formula xml:id="formula_66">EH(P n ) = H(P )- S -1 2n + 1 12n 2 1 - S i=1 1 p i +O 1 n 3 . (<label>47</label></formula><p>) Harris's result reveals an undesirable consequence of the Taylor expansion method: one cannot obtain uniform bounds on the bias of the MLE. Indeed, the term</p><formula xml:id="formula_67">S i=1<label>1</label></formula><p>pi can be arbitrarily large for some distribution P . However, it is evident that both H(P n ) and H(P ) are bounded above by ln S, since the maximum entropy of any distribution supported on S elements is ln S. Conceivably, for such a distribution P that would make</p><formula xml:id="formula_68">S i=1<label>1</label></formula><p>pi very large, we need to compute even higher order Taylor expansions to obtain more accuracy, but even with such efforts we cannot obtain a uniform bias bound for all P .</p><p>We gain one of our key insights into the bias of the MLE by relating it to the approximation error induced by the Bernstein polynomial approximation of the function f , which was first observed in Paninski <ref type="bibr" target="#b13">[14]</ref>. To see this, we first compute the bias of F (P n ) in estimating the functional F (P ) in <ref type="bibr" target="#b12">(13)</ref>.</p><p>Lemma 3. The bias of the estimator F (P n ) is given by</p><formula xml:id="formula_69">Bias(F (P n )) EF (P n ) -F (P ) = S i=1   n j=0 f j n n j p j i (1 -p i ) n-j -f (p i )   .<label>(48)</label></formula><p>The bias term in ( <ref type="formula" target="#formula_69">48</ref>) can be equivalently expressed as <ref type="foot" target="#foot_0">1</ref>Bias(F</p><formula xml:id="formula_70">(P n )) = S i=1   n j=0 f j n B j,n (p i ) -f (p i )   (49) = S i=1 (B n [f ](p i ) -f (p i )) ,<label>(50)</label></formula><p>where B j,n (x) n j x j (1-x) n-j is the well-known Bernstein polynomial basis, and B n [f ](x) is the so-called Bernstein polynomial for function f (x). Bernstein in 1912 <ref type="bibr" target="#b60">[61]</ref> provided an insightful constructive proof of the Weierstrass theorem on approximation of continuous functions using polynomials, by showing that the Bernstein polynomial of any continuous function converges uniformly to that function. From a functional analytic viewpoint, the Bernstein polynomial is an operator that maps a continuous function</p><formula xml:id="formula_71">f ∈ C[0, 1] to another continuous function B n [f ] ∈ C[0, 1]</formula><p>. This operator is linear in f , and is positive because B n [f ] is also pointwise nonnegative if f is pointwise non-negative. Evidently, bounding the approximation error incurred by the Bernstein polynomial is equivalent to bounding the bias of the MLE f (X/n), where X ∼ B(n, x). Fortunately, the theory of approximation using positive linear operators <ref type="bibr" target="#b61">[62]</ref> provides us with advanced tools that are very effective for the bias analysis our problem calls for. A century ago, probability theory served Bernstein in breaking new ground in function approximation. It is therefore very satisfying that advancements in the latter have come full circle to help us better understand probability theory and statistics. We briefly review the general theory of approximation using positive linear operators below.</p><p>1) Approximation theory using positive linear operators: Generally speaking, for any estimator θ of a parametric model indexed by θ, the expectation f → E θ f ( θ) is a positive linear operator for f , and analyzing the bias E θ f ( θ) -f (θ) is equivalent to analyzing the approximation properties of the positive linear operator E θ f ( θ) in approximating f (θ). Hence, analyzing the bias of any plug-in estimator for functionals of parameters from any parametric families can be recast as a problem of approximation theory using positive linear operators <ref type="bibr" target="#b61">[62]</ref>.</p><p>Conversely, given a positive linear operator L(f )(x) that operates on the space of continuous functions, the Riesz-Markov-Kakutani theorem implies that under mild conditions the operator may be written as</p><formula xml:id="formula_72">L(f )(x) = I f dµ x = E µx f (Z), Z ∼ µ x ,<label>(51)</label></formula><p>where {µ x } is a set of probability measures parametrized by x, which may be viewed as a parameter. If we view the random variable Z as a summary statistics to plug-in the functional f (•), the positive linear operator L(f )(x) is nothing but the expectation of the plug-in estimator f (Z). In this sense, there exists a one-to-one correspondence between essentially the most general bias analysis problem in statistics, and the most general positive linear operator approximation problem in approximation theory.</p><p>After more than a century's active research on approximation using positive linear operators, we now have highly nontrivial tools for positive linear operators of functions on one dimensional compact sets, but the general theory for vector valued multivariate functions on non-compact sets is still far from complete <ref type="bibr" target="#b61">[62]</ref>. In the next subsection, we present a sample of existing results in approximation using positive linear operators, corollaries of which will be used to analyze the bias of the MLE for two examples: F α (P ) and H(P ).</p><p>2) Some general results in bias analysis: First, some elementary approximation theoretic concepts need to be introduced in order to characterize the degree of smoothness of functions. For I ⊂ R an interval, the first-order modulus of smoothness ω 1 (f, t), t ≥ 0 is defined as <ref type="bibr" target="#b61">[62</ref>]</p><formula xml:id="formula_73">ω 1 (f, t) sup{|f (u) -f (v)| : u, v ∈ I, |u -v| ≤ t}. (52)</formula><p>The second-order modulus of smoothness ω 2 (f, t), t ≥ 0 <ref type="bibr" target="#b61">[62]</ref> is defined as</p><formula xml:id="formula_74">ω 2 (f, t) sup f (u) -2f u + v 2 + f (v) : u, v ∈ I, |u -v| ≤ 2t . (<label>53</label></formula><formula xml:id="formula_75">)</formula><p>Ditzian and Totik <ref type="bibr" target="#b62">[63]</ref> introduced a class of moduli of smoothness, which proves to be extremely useful in characterizing the incurred approximation errors. For simplicity, for functions defined on [0, 1], ϕ(x) = x(1 -x), the first-order Ditzian-Totik modulus of smoothness is defined as</p><formula xml:id="formula_76">ω 1 ϕ (f, t) sup |f (u) -f (v)| : u, v ∈ [0, 1], |u -v| ≤ tϕ u + v 2 ,<label>(54)</label></formula><p>and the second-order Ditzian-Totik modulus of smoothness is defined as</p><formula xml:id="formula_77">ω 2 ϕ (f, t) sup f (u) -2f u + v 2 + f (v) : u, v ∈ [0, 1], |u -v| ≤ 2tϕ u + v 2 . (<label>55</label></formula><formula xml:id="formula_78">)</formula><p>Recall that we denote by e j , j ∈ N + ∪ {0}, the monomial functions e j (y) = y j , y ∈ I. The first estimate for general positive linear operators, using modulus ω 2 and with precise constants, was given by Gonska <ref type="bibr" target="#b63">[64]</ref>. We rephrase Paltanea <ref type="bibr">[62,</ref> Cor. 2.2.1.] as follows. Note that notation e 1 -xe 0 denotes a continuous function on I which is the difference of a linear function y and a constant function with constant value x over I. In other words, it is an abbreviation of e 1 (y)-xe 0 (y), y ∈ I, which is a function of y rather than x.</p><p>For a positive linear functional F , we adopt the following notation</p><formula xml:id="formula_79">B F (x) = |F (e 1 ) -xF (e 0 )| , V F = F (e 1 -F (e 1 )e 0 ) 2 , (<label>56</label></formula><formula xml:id="formula_80">)</formula><p>which represent the "bias" and "variance" of a positive linear functional F . </p><formula xml:id="formula_81">I ⊂ R is an interval. Suppose that F (e 0 ) = 1, t &gt; 0, length(I) ≥ 2t, s ≥ 2. Then, |F (f ) -f (x)| ≤ B F (x) ω 1 (f, t) t + 1 + F (|e 1 -xe 0 | s ) 2t s ω 2 (f, t). (<label>57</label></formula><formula xml:id="formula_82">)</formula><p>We remark that Lemma 4 can be applied to bound the bias of plug-in estimators in very general models. For example, consider an arbitrary statistical experiment {P θ , θ ∈ I}, from which we obtain n i.i.d. samples X 1 , X 2 , . . . , X n ∼ P θ . For any estimator θn , we would like to analyze the bias of the plug-in estimator f ( θn ) for functional f (θ).</p><p>Suppose length(I) ≥ 2t, s ≥ 2, then Lemma 4 implies that</p><formula xml:id="formula_83">|E θ f ( θn ) -f (θ)| ≤ |E θ θn -θ| ω 1 (f, t) t + 1 + E| θn -θ| s 2t s ω 2 (f, t). (<label>58</label></formula><formula xml:id="formula_84">)</formula><p>If we further assume that θn is an unbiased estimator for θ, i.e., E θ θn = θ holds for all θ ∈ I, then we have</p><formula xml:id="formula_85">|E θ f ( θn ) -f (θ)| ≤ 1 + E| θn -θ| s 2t s ω 2 (f, t).<label>(59)</label></formula><p>Taking s = 2 and assuming Var( θn ) ≤ length(I)/2, we have</p><formula xml:id="formula_86">|E θ f ( θn ) -f (θ)| ≤ 3 2 ω 2 (f, Var( θn )),<label>(60)</label></formula><p>after we take t = E| θn -θ| 2 . We remark that Lemma 4 is only one way to analyze the bias, which is by no means always tight. For example, the following estimate using Ditzian-Totik modulus is significantly better than Lemma 4 for certain functions such as the entropy. </p><formula xml:id="formula_87">|F (f ) -f (x)| ≤ B F (x) 2h 1 ϕ(x) • ω 1 ϕ (f, 2h 1 ) + 5 2 ω 2 ϕ (f, h 1 ), (<label>61</label></formula><formula xml:id="formula_88">)</formula><formula xml:id="formula_89">for all f ∈ C[0, 1] and 0 &lt; h 1 ≤ 1 2 , where ϕ(x) = x(1 -x) and h 1 = F ((e 1 -xe 0 ) 2 )/ϕ(x) = V F + (B F (x)) 2 /ϕ(x).</formula><p>The "bias" B F (x) and "variance" V F (x) are defined in <ref type="bibr" target="#b55">(56)</ref>.</p><p>Considering the same statistical experiment {P θ , θ ∈ I}, and the plug-in estimator f ( θn ) for f (θ), if θn is unbiased for θ and Var( θn ) ≤ ϕ(θ) 2  4 , then it follows from Lemma 5 that</p><formula xml:id="formula_90">|E θ f ( θn ) -f (θ)| ≤ 5 2 ω 2 ϕ   f, Var( θn ) ϕ(θ)   ,<label>(62)</label></formula><p>after we take t = √ Var( θn) ϕ(θ) . For certain functions f (θ) and statistical models Lemma 5 is stronger than Lemma 4. For example, if f (θ) = -θ ln θ, θ ∈ [0, 1], and we have n• θn ∼ B(n, θ). We will show in Lemma 8 that ω 2 ϕ (f, t) = t 2 ln 4 1+t 2 , and ω 2 (f, t) = t ln 4. We also have Var( θn ) = θ(1-θ) n . Hence, Lemma 4 gives the upper bound</p><formula xml:id="formula_91">|E θ f ( θn ) -f (θ)| ≤ 3 ln 4 2 θ(1 -θ) n ,<label>(63)</label></formula><p>whereas Lemma 5 gives</p><formula xml:id="formula_92">|E θ f ( θn ) -f (θ)| ≤ 5 ln 4 2n • 1 1 + 1/n ,<label>(64)</label></formula><p>which is much stronger when n is large and θ not too close to the endpoints of [0, 1].</p><p>There also exist various estimates for the bias when the parameter lies in sets other than an interval in R. However, the bounds we presented are in general not optimal for specific functionals, thereby leaving ample room for future development. For example, note that ( <ref type="formula" target="#formula_91">63</ref>) is stronger than <ref type="bibr" target="#b63">(64)</ref> when θ ≤ 1/n, but Han, Jiao, and Weissman <ref type="bibr" target="#b64">[65]</ref> showed that when θ ≤ 1/n the pointwise bound in ( <ref type="formula" target="#formula_91">63</ref>) is still strictly suboptimal for the entropy functional. Unsurprisingly, to obtain the results in Section III, we need to go beyond the general results in approximation theory, and incorporate the structure of specific functions.</p><p>Note: In approximation theory literature, researchers have explored the interactions between general positive linear operator approximation and its probabilistic counterpart decades ago <ref type="bibr" target="#b65">[66]</ref>- <ref type="bibr" target="#b67">[68]</ref>. However, in statistics literature related to positive linear approximation, usually only specific operators are used, such as the Bernstein operator <ref type="bibr" target="#b68">[69]</ref>, and the focus may not be on obtaining the tightest bound on bias <ref type="bibr" target="#b69">[70]</ref>, <ref type="bibr" target="#b70">[71]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Lower bounds</head><p>To lower bound the worst case performance of a specific estimator, we have essentially two approaches: first, to analyze the bias or the variance of the specific estimator carefully; second, to prove a lower bound that is satisfied by all the estimators, which naturally include the specific estimator we need to analyze. These two approaches have different relative advantages and disadvantages, so we utilize them together in the lower bound construction.</p><p>We refer the readers to Tsybakov <ref type="bibr" target="#b71">[72]</ref> for a nice collection of techniques to prove minimax lower bounds. One specific approach we use is the van Trees inequality, which we quote below.</p><p>Let (X , F , P θ ; θ ∈ Θ) be a dominated family of distributions on some sample space X ; denote the dominating measure by µ. Assume Θ is a closed interval on the real line. Let f (x|θ) denote the density of P θ with respect to µ. Let π be some probability distribution on Θ with a density λ(θ) with respect to Lebesgue measure. Suppose that λ and f (x|•) are both absolutely continuous (µ-almost surely), and that λ converges to zero at the endpoints of the interval Θ. We define</p><formula xml:id="formula_93">I(θ) = E θ ∂ log f (X|θ) ∂θ 2<label>(65)</label></formula><formula xml:id="formula_94">I(λ) = E d log λ(θ) dθ 2<label>(66)</label></formula><p>the Fisher information for θ and for a location parameter in λ, respectively. We assume I(θ) is continuous in θ. We have the following inequality.</p><p>Lemma 6 (van Trees inequality). <ref type="bibr" target="#b72">[73]</ref> Under assumptions above, the average risk of an arbitrary estimator ψ(X) in estimating an absolutely continuous functional ψ(θ) under squared error loss satisfies the following inequality:</p><formula xml:id="formula_95">E ψ(X) -ψ(θ) 2 ≥ (Eψ ′ (θ)) 2 E[I(θ)] + I(λ)<label>(67)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. PROOFS OF THE UPPER BOUNDS</head><p>In order to upper bound the maximum squared error risk of any estimator, a natural approach would be to analyze the squared bias term and the variance term separately. Then, it suffices to find proper tools to give nonasymptotic analysis of the bias and variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Bounding the bias</head><p>We first work to bound the bias. Lemma 3 shows that the bias of F (P n ) could be represented as</p><formula xml:id="formula_96">Bias(F (P n )) = S i=1 (B n [f ](p i ) -f (p i )) ,<label>(68)</label></formula><p>where</p><formula xml:id="formula_97">B n [f ](x)</formula><p>is the Bernstein polynomial corresponding to f (x). The following lemma summarizes some state-of-theart bounds for approximation error of Bernstein polynomials. Lemma 7 can be derived easily from the general theory we presented in Section IV-B2. We emphasize that one cannot expect the bounds in Lemma 7 to be tight for any f ∈ C[0, 1], since the Bernstein approximation error itself could be a very complicated function in C[0, 1], and Lemma 7 is using relatively simple functions to upper bound it. Lemma 7. The following bounds are valid for function approximation error incurred by Bernstein polynomials: 1) Pointwise estimate: [62, Cor. 2.2.1] [74] for all continuous functions f on [0, 1],</p><formula xml:id="formula_98">|f (x) -B n [f ](x)| ≤ 3 2 ω 2 f, x(1 -x) n ,<label>(69)</label></formula><p>and the constant 3/2 is shown by <ref type="bibr" target="#b73">[74]</ref> to be the best constant;</p><p>2) Norm estimate: <ref type="bibr" target="#b61">[62,</ref><ref type="bibr">Cor. 4.1.10]</ref> for ϕ(x) = x(1 -x) and all continuous functions f on [0, 1], we have</p><formula xml:id="formula_99">B n [f ] -f ∞ ≤ 5 2 ω 2 ϕ (f, n -1/2 );<label>(70)</label></formula><p>3) <ref type="bibr" target="#b74">[75,</ref><ref type="bibr">Eqn. 10.3.4]</ref> for f ∈ C<ref type="foot" target="#foot_1">foot_1</ref> [0, 1], i.e., twice continuously differentiable,</p><formula xml:id="formula_100">|f (x) -B n [f ](x)| ≤ f ′′ ∞ x(1 -x) 2n ;<label>(71)</label></formula><p>Proof. The pointwise estimate of Lemma 7 follows from Lemma 4. The norm estimate of Lemma 7 follows from Lemma 5. Regarding the third part, suppose random variable X ∼ B(n, x). We have</p><formula xml:id="formula_101">|f (x) -B n [f ](x)| = |E x f (X/n) -f (x)| (72) = |E x [f ′ (x)(X/n -x) + 1 2 f ′′ (ξ X )(X/n -x) 2 ]| (73) = 1 2 |E x f ′′ (ξ X )(X/n -x) 2 | (74) ≤ f ′′ ∞ 2 |E x (X/n -x) 2 | (75) = f ′′ ∞ 2 x(1 -x) n ,<label>(76)</label></formula><p>where we used Taylor expansion for f (X/n) at point x with the Lagrange remainder. The proof is complete.</p><p>Remark 1. Note that although <ref type="bibr" target="#b69">(70)</ref> is in the form of an upper bound, it has been shown to be a lower bound as well.</p><p>Totik <ref type="bibr" target="#b75">[76]</ref> showed the following equivalence property on the norm estimate of Bernstein approximation errors</p><formula xml:id="formula_102">B n [f ](x) -f (x) ∞ ≍ ω 2 ϕ (f, n -1/2 ). 2<label>(77)</label></formula><p>It is easy to calculate the second-order modulus of smoothness and the Ditzian-Totik second-order modulus of smoothness for functions x α and -x ln x. The results are presented in the following lemma. Lemma 8. We have</p><formula xml:id="formula_103">x α , 0 &lt; α &lt; 1 x α , 1 &lt; α &lt; 2 -x ln x ω 2 (f, t) |2 -2 α |t α |2 -2 α |t α t ln 4 ω 2 ϕ (f, t) |2 -2 α | t 2α (1+t 2 ) α ≍ t 2 t 2 ln 4 1+t 2</formula><p>where the second-order modulus results hold for 0 &lt; t ≤ 1/2, and the Ditizan-Totik second-order modulus results hold for 0 &lt; t ≤ 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Bias of F α (P n ):</head><p>We first bound the bias incurred by F α (P n ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) α ≥ 2:</head><p>In this case, f ∈ C 2 [0, 1], applying the third part of Lemma 7,</p><formula xml:id="formula_104">|f (x) -B n [f ](x)| ≤ α(α -1)x(1 -x) 2n .<label>(78)</label></formula><p>Thus, we have</p><formula xml:id="formula_105">|Bias(F α (P n ))| ≤ S i=1 α(α-1) p i (1 -p i ) 2n ≤ α(α -1) 2n . (79) 2) 1 &lt; α &lt; 2</formula><p>The following lemma presents a bound on the bias of F α (P n ), which does not depend on the alphabet size S. We note that the proof of Lemma 9 heavily utilizes the special properties of function x α and the fact that</p><formula xml:id="formula_106">S i=1 p i = 1.</formula><p>Lemma 9. The bias of F α (P n ) for estimating F α (P ), 1 &lt; α &lt; 2, is upper bounded by the following:</p><formula xml:id="formula_107">|Bias(F α (P n ))| ≤ 4 n α-1 . (<label>80</label></formula><formula xml:id="formula_108">)</formula><p>We also present two additional bounds involving the alphabet size S. Using the pointwise estimate in Lemma 7, the bias term of the MLE is upper bounded as follows for all 0 &lt; α &lt; 2, α = 1:</p><formula xml:id="formula_109">S i=1 3 2 |2 -2 α | p i (1 -p i ) n α/2 ≤ 3 2 |2 -2 α | 1 n α/2 S i=1 p α/2 i (81) ≤ 3 2 |2 -2 α | 1 n α/2 S 1 S α/2 (82) = 3 2 |2 -2 α | S 1-α/2 n α/2 . (<label>83</label></formula><formula xml:id="formula_110">)</formula><p>Using the norm estimate in Lemma 7, when 1 &lt; α &lt; 2, the bias would be upper bounded by C α,n</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5S</head><p>2n , where</p><formula xml:id="formula_111">C α,n = nω 2 ϕ (x α , n -1/2 ) is a finite positive constant such that lim sup n→∞ C α,n &lt; ∞ for 1 &lt; α &lt; 2.</formula><p>Combining Lemma 9, the pointwise estimate, and the norm estimate in Lemma 7, we know that the bias of</p><formula xml:id="formula_112">F α (P n ) for 1 &lt; α &lt; 2 is upper bounded as |Bias(F α (P n ))| ≤ 4 n α-1 ∧ 3 2 |2 -2 α | S 1-α/2 n α/2 ∧ C α,n 5S 2n . (84) 3) 0 &lt; α &lt; 1:</formula><p>The pointwise estimate from Lemma 7 is worked out in (83). Using the norm estimate in Lemma 7, the bias would be upper bounded by |2-2 α | 5S 2n α . Combining the pointwise estimate and the norm estimate, we know that the bias of F α (P n ) for 0 &lt; α &lt; 1 is upper bounded as</p><formula xml:id="formula_113">|Bias(F α (P n ))| ≤ 3 2 |2 -2 α | S 1-α/2 n α/2 ∧ |2 -2 α | 5S 2n α . (<label>85</label></formula><p>) 2) Bias of H(P n ): We then bound the bias incurred by H(P n ). Using the norm estimate in Lemma 7, we know |Bias(H(P n ))| ≤ 5S ln 4 2n . (86) Using the pointwise estimate in Lemma 7, we obtain |Bias(H(P n ))| ≤ 3 2 S n ln 4. (87) It was shown by Paninski [14, Prop. 1] that the squared bias of MLE H(P n ) is upper bounded as</p><formula xml:id="formula_114">(Bias(H(P n ))) 2 ≤ ln 1 + S -1 n 2 ,<label>(88)</label></formula><p>which is better than the two bounds we obtained using Bernstein polynomial results. However, we remark that (88) is obtained using special properties of the entropy function and connections between KL-divergence and χ 2 -divergence <ref type="bibr" target="#b71">[72]</ref>, which cannot be applied to general functions. Strukov and Timan <ref type="bibr" target="#b65">[66]</ref> also heavily exploited the structure of function x α and -x ln x in order to analyze the Bernstein approximation error for these functions, and obtained tight-in-order results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Bias of H( PB ):</head><p>We apply the general theory of positive linear operator approximation. The following lemma is a strengthened version of Lemma 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 10. If</head><formula xml:id="formula_115">F : C[0, 1] → R is a linear positive functional and F (e 0 ) = 1, then |F (f ) -f (x)| ≤ ω 1 (f, B F (x); x) + 5 2 ω 2 ϕ (f, h 2 )<label>(89)</label></formula><p>for all f ∈ C[0, 1] and 0 &lt; h 2 ≤ 1 2 , where ϕ(x) = x(1 -x) and h 2 = √ V F /ϕ(x), and</p><formula xml:id="formula_116">ω 1 (f, h; x) sup {|f (u) -f (x)| : u ∈ [0, 1], |u -x| ≤ h} . (<label>90</label></formula><formula xml:id="formula_117">)</formula><p>The "bias" B F (x) and "variance" V F (x) are defined in <ref type="bibr" target="#b55">(56)</ref>.</p><p>Proof. Applying Lemma 5 to x = F (e 1 ) we have</p><formula xml:id="formula_118">|F (f ) -f (F (e 1 ))| ≤ 5 2 ω 2 ϕ (f, h 2 )<label>(91)</label></formula><p>and then (89) is the direct result of the triangle inequality</p><formula xml:id="formula_119">|F (f ) -f (x)| ≤ |F (f ) -f (F (e 1 ))| + |f (F (e 1 )) -f (x)|.</formula><p>We show that Lemma 10 is indeed stronger than Lemma 5. Firstly, due to </p><formula xml:id="formula_120">h 1 ≥ h 2 , we have ω 2 ϕ (f, h 2 ) ≤ ω 2 ϕ (f, h 1 ). Second, for x ≤ 1/2, we have B F (x) 2h 1 ϕ(x) • ω 1 ϕ (f, 2h 1 ) ≈ B F (x) 2h 1 ϕ(x) • sup 0≤s≤1 2h 1 ϕ(s)f ′ (s) (92) ≥ B F (x) • sup x≤s≤1-x f ′ (s) (93) ≈ sup x≤s≤1-x ω 1 (f, B F (x); s)<label>(94</label></formula><p>Note that Lemma 11 implies a slightly weaker bias bound than Theorem 4, but it is only sub-optimal up to a multiplicative constant. The bias bound in Theorem 4 is obtained using the following lemma, whose proof only applies to the entropy function.</p><formula xml:id="formula_122">Lemma 12. If n ≥ max{2ea, Sa}, sup P ∈MS |E P H( PB ) -H(P )| ≤ ln 1 + S -1 n + Sa ∨ 2Sa n + Sa ln n + Sa 2a . (<label>96</label></formula><formula xml:id="formula_123">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Bounding the variance</head><p>The next lemma follows from an application of bounded difference inequality presented in Lemma 2.</p><p>Lemma 13. The variance of F (P n ) satisfies the following upper bound:</p><formula xml:id="formula_124">Var(F (P n )) ≤ n • max 0≤j&lt;n (f ((j + 1)/n) -f (j/n)) 2 . (97)</formula><p>If f is monotone, then we can strengthen the bound to be</p><formula xml:id="formula_125">Var(F (P n )) ≤ n 4 • max 0≤j&lt;n (f ((j + 1)/n) -f (j/n)) 2 . (<label>98</label></formula><formula xml:id="formula_126">)</formula><p>We first bound the variance for F α (P n ), α &gt; 1. We have</p><formula xml:id="formula_127">max 0≤j&lt;n (((j + 1)/n) α -(j/n) α ) 2 ≤ 1 -1 - 1 n α 2 (99) ≤ α n 2 ,<label>(100)</label></formula><p>where in the last step we used Bernoulli's inequality:</p><formula xml:id="formula_128">(1 + x) r ≥ 1 + rx, ∀r ≥ 1, x &gt; -1, x ∈ R.</formula><p>Using Lemma 2, we know the variance is upper bounded by</p><formula xml:id="formula_129">Var(F α (P n )) ≤ α 2 4n .<label>(101)</label></formula><p>We bound the variance of F α (P n ), 0 &lt; α &lt; 1 in the following lemma. Lemma 14. For 0 &lt; α &lt; 1/2, we have</p><formula xml:id="formula_130">sup P ∈MS Var(F α (P n )) ≤ 10S n 2α + 3α • 2 3+2α + 1 8α 2 8α e 2α + 4 S n 2α ∧ 1 n 2α-1 (102) S n 2α . (<label>103</label></formula><formula xml:id="formula_131">)</formula><p>For 1/2 ≤ α &lt; 1, we have</p><formula xml:id="formula_132">sup P ∈MS Var(F α (P n )) ≤ 10S 2-2α n + 3α • 2 3+2α + 1 8α 2 8α e 2α + 4 S n 2α ∧ 1 n 2α-1 (104) S 2-2α n + S n 2α ∧ 1 n 2α-1 .<label>(105)</label></formula><p>Further, one can show that for all α ∈ (0, 1),</p><formula xml:id="formula_133">3α • 2 3+2α + 1 8α 2 8α e 2α + 4 ≤ 120 α 2 ,<label>(106)</label></formula><p>which is used in Theorem 1.</p><p>Regarding the variance of H(P n ), we have Lemma 15.</p><p>sup</p><formula xml:id="formula_134">P ∈MS Var(H(P n )) ≤ (ln n) 2 n ∧ 2(ln S + 3) 2 n (<label>107</label></formula><formula xml:id="formula_135">)</formula><formula xml:id="formula_136">(ln S) 2 ∧ (ln n) 2 n .<label>(108)</label></formula><p>The variance of H( PB ) is upper bounded by the following lemma.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 16. The variance of H( PB ) is upper bounded as follows:</head><formula xml:id="formula_137">Var H( PB ) ≤ 2n (n + Sa) 2 3 + ln n + Sa a + 1 ∧ S 2 .<label>(109)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. PROOFS OF THE LOWER BOUNDS</head><p>A. Lower bounds for estimation of F α (P ) when α ≥ 3/2</p><p>We apply the van Trees inequality as presented in Lemma 6. It suffices to consider the restricted case of S = 2 and prove the n -1 lower bound. Thus, the model is equivalent to observing a Binomial random variable X ∼ B(n, p), and one aims to estimate the functional ψ α (p) = p α + (1 -p) α . We have</p><formula xml:id="formula_138">ψ ′ α (p) = αp α-1 -α(1 -p) α-1 . (<label>110</label></formula><formula xml:id="formula_139">)</formula><p>The Fisher information for parameter p under the Binomial model is I(p) = n p(1-p) . Suppose we impose prior λ(p) on parameter p. The van Trees inequality implies</p><formula xml:id="formula_140">sup P ∈MS E P (F α (P n ) -F α (P )) 2 ≥ inf Fα sup P ∈MS E P Fα -F α (P ) 2 (111) ≥ E E[F α (P )|X S 1 ] -F α (P ) 2 (Bayes risk) (112) ≥ ( αp α-1 -α(1 -p) α-1 λ(p)dp) 2 E λ n p(1-p) + I(λ) (113) = ( αp α-1 -α(1 -p) α-1 λ(p)dp) 2 n • E λ 1 p(1-p) + I(λ)<label>(114)</label></formula><p>where the second inequality follows from the fact that the Bayes risk under any prior is upper bounded by the minimax risk <ref type="bibr" target="#b77">[78]</ref>.</p><p>Taking λ(p) to be the Dirichlet prior with parameter (a, b), i.e.,</p><formula xml:id="formula_141">λ(p) = 1 B(a, b) p a-1 (1 -p) b-1 , a &gt; 2, b &gt; 2, (<label>115</label></formula><formula xml:id="formula_142">)</formula><p>we can explicitly evaluate the integrals above. Here B(a, b) is the Beta function.</p><p>Taking a = 4, b = 3, we have</p><formula xml:id="formula_143">sup P ∈MS E P (F α (P n ) -F α (P )) 2 ≥ (60α(B(α + 3, 3) -B(α + 2, 4))) 2 5n + 45 . (<label>116</label></formula><formula xml:id="formula_144">)</formula><p>Taking C α = 72α 2 (B(α + 3, 3) -B(α + 2, 4)) 2 , we have</p><formula xml:id="formula_145">sup P ∈MS E P (F α (P n ) -F α (P )) 2 ≥ C α n , for all n ≥ 1.<label>(117)</label></formula><p>Note that C α &gt; 0 for all α ≥ 3/2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Lower bounds for estimation of</head><formula xml:id="formula_146">F α (P ) when 1 &lt; α &lt; 3/2</formula><p>The following lemma was proved in <ref type="bibr" target="#b68">[69]</ref>.</p><p>Lemma 17. Let k ≥ 4 be an even number. Suppose that the k-th derivative of f satisfies f (k) ≤ 0 in (0, 1), Q k-1 is the Taylor polynomial of order k -1 to f at some x 1 in (0, 1).</p><p>Then for x ∈ [0, 1],</p><formula xml:id="formula_147">f (x) -B n [f ](x) ≥ Q k-1 -B n [Q k-1 ](x). (<label>118</label></formula><formula xml:id="formula_148">) Consider f α (x) = -x α , 1 &lt; α &lt; 2, x ∈ [0, 1]</formula><p>. Applying Lemma 17 to f α , taking k = 6, we have the following result.</p><formula xml:id="formula_149">Lemma 18. Suppose f α (x) = -x α , 1 &lt; α &lt; 2 on [0, 1]. For all x ∈ (0, 1), we have f α (x) -B n [f α ](x) ≥ α(α -1)x α-2 (1 -x) 2n x + (2 -α)(3α -1)x 12n + (2 -α)(5 -3α) 12n + R 1 (x) n 3 + R 2 (x) n 4 , (<label>119</label></formula><formula xml:id="formula_150">)</formula><p>where</p><formula xml:id="formula_151">R 1 (x) = α(α -1)(α -2)(α -3)x α-3 (1 -x)<label>24</label></formula><formula xml:id="formula_152">× 1 + 2(1 -x)((5 -2α)x + α -4) , (120) R 2 (x) = α(α -1)(α -2)(α -3)(α -4) 120 × x α-4 (1 -x)(1 -2x)(1 -12x(1 -x)). (121)</formula><p>Note that we have assumed S = cn, c &gt; 0. If c ≤ 1, we take a uniform distribution on S elements P = (1/S, 1/S, . . . , 1/S), otherwise we take distribution P = (n -1 -ǫ, n -1 -ǫ, . . . , n -1 -ǫ, nǫ S-n , . . . , nǫ S-n ), where ǫ will be taken to be arbitrarily small. We first analyze the c ≤ 1 case. Applying Lemma 18, we have</p><formula xml:id="formula_153">S i=1 f α (1/S) -B n [f α ](1/S) (Note that f α (x) = -x α ) = EF α (P n ) -F α (P ) ≥ S • α(α -1) 2S α-2 n 1 S + (2 -α)(5 -3α) 12n + α(α -1)(α -2)(α -3) 24S α-3 n 3 (1 + 2(α -4)) + α(α -1)(α -2)(α -3)(α -4) 120S α-4 n 4 + o(n -α ) = α(α -1) n α-1 1 2c α-3 1 c + (2 -α)(5 -3α)<label>12</label></formula><formula xml:id="formula_154">+ (α -2)(α -3)(1 + 2(α -4)) 24c α-4 + (α -2)(α -3)(α -4) 120c α-5 + o(n -(α-1) ) = α(α -1)c 2-α n α-1 1 2 + (2 -α)(5 -3α)c<label>24</label></formula><formula xml:id="formula_155">+ (α -2)(α -3)(1 + 2(α -4))c 2<label>24</label></formula><formula xml:id="formula_156">+ (α -2)(α -3)(α -4)c 3 120 + o(n -(α-1) ) ≥ αc 2-α (124 -330α + 285α 2 -90α 3 + 11α 4 ) 120n α-1 + o(n -(α-1) ),</formula><p>where the first inequality follows from Lemma 18, and in the last step we have taken c = 1 in the following expression</p><formula xml:id="formula_157">1 2 + (2 -α)(5 -3α)c 24 + (α -2)(α -3)(1 + 2(α -4))c 2 24 + (α -2)(α -3)(α -4)c 3 120 ,<label>(122)</label></formula><p>and considered the fact that it is a monotonically decreasing function with respect to c on (0, 1] for any α ∈ (1, 3/2).</p><p>For cases when c &gt; 1, since we take P = (n -1 -ǫ, n -1ǫ, . . . , n -1 -ǫ, nǫ S-n , . . . , nǫ S-n ), by a continuity argument, the analysis is exactly the same as that above when we set c = 1 as we can take ǫ as small as possible. One can verify that the function α(124</p><formula xml:id="formula_158">-330α + 285α 2 -90α 3 + 11α 4 )/120 is positive on interval (1, 3/2). Defining √ c α = αc 2-α (124 - 330α + 285α 2 -90α 3 + 11α 4 )/120 &gt; 0 when c ≤ 1, and √ c α = α(124 -330α + 285α 2 -90α 3 + 11α 4 )/120 &gt; 0 when c &gt; 1, the proof is completed.</formula><p>C. Lower bounds for estimation of F α (P ) when 0 &lt; α &lt; 1 Applying Lemma 17 to function f α (x) = x α , α ∈ (0, 1), taking k = 4, we have the following result: Lemma 19. For f α (x) = x α on [0, 1], α ∈ (0, 1), x ∈ (0, 1), we have</p><formula xml:id="formula_159">f α (x) -B n [f α ](x) ≥ α(1 -α) 2n x α-2 (1 -x) x - 2 -α 3n . (<label>123</label></formula><formula xml:id="formula_160">)</formula><p>Suppose n ≥ S. Define distribution W = (w 1 , w 2 , . . . , w S ) ∈ M S such that</p><formula xml:id="formula_161">1 ≤ i ≤ S -1, w i = 1 n ; w S = 1 - S -1 n . (<label>124</label></formula><formula xml:id="formula_162">) Note that w i ≥ n -1 , 1 ≤ i ≤ S. It follows from Lemma 19 that F α (W ) -E W F α (P n ) ≥ S-1 i=1 α(1 -α) 6n 2 1 n α-2 1 - 1 n (125) = α(1 -α)(S -1) 6n α n -1 n . (<label>126</label></formula><formula xml:id="formula_163">)</formula><p>Thus, we know for all 0 &lt; α &lt; 1, sup</p><formula xml:id="formula_164">P ∈MS E P (F α (P ) -F α (P n )) 2 ≥ α 2 (1 -α) 2 (S -1) 2 36n 2α 1 - 1 n 2 . (<label>127</label></formula><formula xml:id="formula_165">)</formula><p>It is shown in <ref type="bibr" target="#b9">[10]</ref> that the following minimax lower bound holds for estimation of F α (P ), 1/2 ≤ α &lt; 1.</p><p>Lemma 20. For 1 2 ≤ α &lt; 1, we have inf</p><formula xml:id="formula_166">F sup P ∈MS E P F -F α (P ) 2 ≥ α 2 32en (2(S -1)) 1-α -2 -α - 1 -α 4n (2(S -1)) 1-α + 2 -α 2 -e -n/4 S 2(1-α) S 2-2α n , (<label>128</label></formula><formula xml:id="formula_167">)</formula><p>where the infimum is taken over all possible estimators.</p><p>Since this lower bound holds for all possible estimators, it also holds for the MLE F α (P n ). Since max{a, b} ≥ 1 2 (a + b), we have the desired lower bound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Lower bounds for estimation of H(P )</head><p>Braess and Sauer <ref type="bibr" target="#b68">[69]</ref> derived the following lower bound for the approximation error of Bernstein polynomials for the function g(x) = -x ln x:</p><formula xml:id="formula_168">Lemma 21. Define g(x) = -x ln x on [0, 1]. For x ≥ 15 n , x ∈ [0, 1], we have g(x) -B n [g](x) ≥ 1 -x 2n + 1 20n 2 x - x 12n 2 . (<label>129</label></formula><formula xml:id="formula_169">)</formula><p>Applying Lemma 21 to the estimation of H(P ), we know that if ∀1 ≤ i ≤ S, p i ≥ 15  n ,</p><formula xml:id="formula_170">H(P )-EH(P n ) ≥ S -1 2n + 1 20n 2 S i=1</formula><p>1 p i -1 12n 2 . (130) Consider the uniform distribution P with n ≥ 15S, which guarantees p i ≥ 15 n . Since S i=1 1 p i ≥ S 2 , (131) we have sup P ∈MS (H(P ) -EH(P n )) ≥ S -1 2n + S 2 20n 2 -1 12n 2 . (132) Thus, when n ≥ 15S, sup P ∈MS E P (H(P ) -H(P n )) 2 ≥ S -1 2n + S 2 20n 2 -1 12n 2 2 . (133) It was shown in [21, Prop. 1] that the following minimax lower bound holds. Lemma 22. There exists a universal constant c &gt; 0 such that inf Ĥ sup P ∈MS</p><formula xml:id="formula_171">E P Ĥ -H(P ) 2 ≥ c ln 2 S n , (<label>134</label></formula><formula xml:id="formula_172">)</formula><p>where the infimum is taken over all possible estimators Ĥ.</p><p>Hence, we have</p><formula xml:id="formula_173">sup P ∈MS E P (H(P ) -H(P n )) 2 ≥ max S -1 2n + S 2 20n 2 - 1 12n 2 2 , c ln 2 S n (135) ≥ 1 2 S -1 2n + S 2 20n 2 - 1 12n 2 2 + c 2 ln 2 S n . (<label>136</label></formula><formula xml:id="formula_174">)</formula><p>Similar arguments can be applied to the Miller-Madow estimator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Lower bounds for entropy estimation using H( PB )</head><p>Since H( PB ) is a specific estimator for entropy, the following lemma is proved via considering several specific distributions.</p><formula xml:id="formula_175">Lemma 23. If n ≥ max{15S, Sa, 2ea}, sup P ∈MS E P H( PB ) -H(P ) ≥ (S -1)a 4(n + Sa) ln n + Sa a + S -1 8n + S 2 80n 2 - 1 48n 2 (137) If n &lt; Sa, then sup P ∈MS E P H( PB ) -H(P ) ≥ S -1 2S ln S.<label>(138)</label></formula><p>If n &lt; 2ea, then sup</p><formula xml:id="formula_176">P ∈MS E P H( PB ) -H(P ) ≥ S -1 2e + S ln S.<label>(139)</label></formula><p>If n &lt; 15S, n ≥ 2ea, then sup</p><formula xml:id="formula_177">P ∈MS E P H( PB ) -H(P ) ≥ (S -1)a 4(n + Sa) ln n + Sa a + ⌊n/15⌋ 8n - 1 16n .<label>(140)</label></formula><p>The corresponding results in Theorem 5 follow from Lemma 23, Lemma 22, and the inequality max{a, b} ≥ a+b 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Lower bounds for entropy estimation using ĤBayes</head><p>We prove Theorem 6 below. Applying Lemma 25, we have</p><formula xml:id="formula_178">ĤBayes ≤ ψ(Sa + n + 1) - S i=1 a + X i Sa + n ψ(a + 1) (141) = ψ(Sa + n + 1) -ψ(a + 1)<label>(142)</label></formula><formula xml:id="formula_179">≤ ln Sa + n + e -γ a + 1 2 . (<label>143</label></formula><formula xml:id="formula_180">)</formula><p>Since ĤBayes is upper bounded by ln Sa+n+e -γ a+ 1 2 for any empirical observations, the squared error it incurs in Shannon entropy estimation when the true distribution is the uniform distribution is at least</p><formula xml:id="formula_181">ln Sa + S/2 Sa + n + e -γ 2 (144) if S ≥ 2(n + 1).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A AUXILIARY LEMMAS</head><p>We begin with the definition of the negative association property, which allows us to upper bound the variance by treating each component of the empirical distribution P n (i) as "independent" random variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 1.</head><p>[79, Def.</p><formula xml:id="formula_182">2.1] Random variables X 1 , X 2 , • • • , X S are</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>said to be negatively associated if for any pair of disjoint subsets</head><formula xml:id="formula_183">A 1 , A 2 of {1, 2, • • • , S}, and any component-wise increasing functions f 1 , f 2 , Cov (f 1 (X i , i ∈ A 1 ), f 2 (X j , j ∈ A 2 )) ≤ 0.<label>(145)</label></formula><p>To verify whether random variables X 1 , X 2 , • • • , X S are negatively associated or not, the following lemma presents a useful criterion.</p><p>Lemma 24. [79, Thm. 2.9] Let X 1 , X 2 , • • • , X S be S independent random variables with log-concave densities. Then the joint conditional distribution of X 1 , X 2 , • • • , X S given S i=1 X i is negatively associated. In light of the preceding lemma, we can obtain the following corollary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corollary 5. For any discrete probability distribution vector</head><formula xml:id="formula_184">P ∈ M S , the random variables X = (X 1 , X 2 , • • • , X S ) drawn from the multinomial distribution X ∼ multi(n; P ) are negatively associated. Proof. Consider the Poissonized model Y i ∼ Poi(np i ), 1 ≤ i ≤ S with all Y i independent, it is straightforward to verify that each Y i possesses a log-concave distribu- tion. Then conditioning on S i=1 Y i = n, we know that (Y 1 , Y 2 , • • • , Y S )|( S i=1 Y i = n) ∼ multi(n; P ), hence</formula><p>Lemma 24 yields the desired result.</p><p>The next lemma gives bounds on the digamma functions ψ(z) = Γ ′ (z) Γ(z) . Lemma 25. [80, <ref type="bibr">Lemma 1.7]</ref> The digamma function ψ(z) is the only solution of the functional equation F (x + 1) = F (x)+ 1</p><p>x that is monotone, strictly concave on R + and satisfies F (1) = -γ, where γ ≈ 0.5772 is the Euler-Mascheroni constant.</p><p>Let x be a positive real number. Then, ln(x + 1/2) &lt; ψ(x + 1) ≤ ln(x + e -γ ).</p><p>(146)</p><formula xml:id="formula_185">If x ≥ 1, then ln(x + 1/2) &lt; ψ(x + 1) ≤ ln(x + e 1-γ -1).<label>(147)</label></formula><p>The following lemma gives some tail bounds for Poisson or Binomial random variables.</p><formula xml:id="formula_186">Lemma 26. [81, Exercise 4.7] If X ∼ Poi(λ) or X ∼ B(n, λ n )</formula><p>, then for any δ &gt; 0, we have</p><formula xml:id="formula_187">P(X ≥ (1 + δ)λ) ≤ e δ (1 + δ) 1+δ λ ,<label>(148)</label></formula><formula xml:id="formula_188">P(X ≤ (1 -δ)λ) ≤ e -δ (1 -δ) 1-δ λ ≤ e -δ 2 λ/2 . (<label>149</label></formula><formula xml:id="formula_189">)</formula><p>To establish the upper bound of the variance obtained by the plug-in estimator F α (P n ), we split p into two different regimes p ≤ 1/n or p &gt; 1/n, and the following lemmas give the corresponding variance bounds.</p><p>Lemma 27. For nX ∼ B(n, p), p ≤ 1/n, we have</p><formula xml:id="formula_190">Var(X α ) ≤ 2 n 2α ∧ 2p n 2α-1 0 &lt; α &lt; 1.<label>(150)</label></formula><p>Lemma 28. For nX ∼ B(n, p), p ≥ 1/n, 0 &lt; α &lt; 1, we have</p><formula xml:id="formula_191">Var(X α ) ≤ 10p 2α-1 n + 3 2α 16α en 2α + 2 n 2α + 1 8α 2 8α en 2α .<label>(151)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B PROOFS OF MAIN LEMMAS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Proof of Lemma 3</head><p>We compute the first moment of F (P n ).</p><formula xml:id="formula_192">EF (P n ) = n j=0 f j n Eh j ,<label>(152)</label></formula><p>and</p><formula xml:id="formula_193">Eh j = E S i=1 ½(X i = j) (153) = S i=1 P(X i = j) (154) = S i=1 n j p j i (1 -p i ) n-j .<label>(155)</label></formula><p>Thus, we have</p><formula xml:id="formula_194">EF (P n ) = n j=0 f j n S i=1 n j p j i (1 -p i ) n-j (156) = n j=0 S i=1 f j n n j p j i (1 -p i ) n-j .<label>(157)</label></formula><p>The bias of</p><formula xml:id="formula_195">F (P n ) is Bias(F (P n )) = EF (P n ) -F (P ) (158) = S i=1   n j=0 f j n n j p j i (1 -p i ) n-j -f (p i )   .<label>(159)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Proof of Lemma 8</head><p>We first compute the second-order modulus. Fix t,</p><formula xml:id="formula_196">0 &lt; t ≤ 1/2. Defining M u+v 2 , then the computation of second- order modulus is equivalent to maximization of |f (M -t) - 2f (M ) + f (M + t)| over interval M ∈ [t, 1 -t],</formula><p>since all the functions we consider are strictly convex or concave over</p><formula xml:id="formula_197">[0, 1]. For f (x) = x α , 0 &lt; α &lt; 1, f (x) is strictly concave on [0, 1]. It follows from Jensen's inequality that g(M ) = (M -t) α -2M α + (M + t) α ≤ 0,<label>(160)</label></formula><p>and it suffices to minimize this function of M in order to obtain the modulus. Taking derivative of g(M ), we have</p><formula xml:id="formula_198">g ′ (M ) = α (M -t) α-1 -2M α-1 + (M + t) α-1 ≥ 0,<label>(161)</label></formula><formula xml:id="formula_199">since x α-1 is a convex function on [t, 1-t].</formula><p>It implies that the function g(M ) is non-decreasing, and the minimum of g(M ) over M ∈ [t, 1 -t] is attained at M = t, and the minimum value is g(t) = (2 α -2)t α . Hence, the corresponding secondorder modulus is |2 -2 α |t α .</p><p>Analogous procedures computes the second-order modulus for x α , 1 &lt; α &lt; 2 and -x ln x. Now we consider the computation of Ditzian-Totik secondorder modulus. Fix t,</p><formula xml:id="formula_200">0 &lt; t ≤ 1. Again denote M u+v 2 ∈ [0, 1]. Then the optimization is over the regime |u -v| ≤ 2tϕ(M ) = 2t M (1 -M ). Equivalently, it is the interval [M -t M (1 -M ), M + t M (1 -M )] ∩ [0, 1]. Since the function f (x) = -x ln x is strictly convex on [0, 1], the maximum of f (u) -2f u+v 2 + f (v) is definitely attained when u and v take the boundary values of the feasible interval [M -t M (1 -M ), M + t M (1 -M )] ∩ [0, 1]. Define ∆ t 1-M M .</formula><p>The feasible interval can be equivalently written as</p><formula xml:id="formula_201">[M -∆M, M + ∆M ] ∩ [0, 1]. We have M -t M (1 -M ) ≥ 0 ⇔ M ≥ t 2 1 + t 2 ,<label>(162)</label></formula><p>as well as</p><formula xml:id="formula_202">M + t M (1 -M ) ≤ 1 ⇔ M ≤ 1 1 + t 2 . (<label>163</label></formula><formula xml:id="formula_203">)</formula><p>Hence, it is equivalent to maximize over three regimes: 1) Regime A:</p><formula xml:id="formula_204">u = 0, v = 2M, 0 ≤ M ≤ t 2 1+t 2 . 2) Regime B: u = M -∆M, v = M + ∆M, M ∈ t 2 1+t 2 , 1 1+t<label>2</label></formula><p>3) Regime C:</p><formula xml:id="formula_205">u = 2M -1, v = 1, 1 ≥ M ≥ 1 1+t 2 .</formula><p>Over regime A, we have</p><formula xml:id="formula_206">f (u) -2f u + v 2 + f (v) = 2M ln 2.<label>(164)</label></formula><p>Maximizing over 0 ≤ M ≤ t 2 1+t 2 , the maximum value is</p><formula xml:id="formula_207">t 2 ln 4 1+t 2 , attained at M = t 2 1+t 2 . Over regime C, we have f (u) -2f u + v 2 + f (v) = |2M ln M -(2M -1) ln(2M -1)| . (165) Maximizing over 1 1+t 2 ≤ M ≤ 1, the maximum is attained at M = 1</formula><p>1+t 2 , and the maximum value is no more than t 2 ln 4 1+t 2 . Now we consider regime B. Since M ∈</p><formula xml:id="formula_208">t 2 1+t 2 , 1 1+t 2 in regime B, we know ∆ = t 1-M M ∈ [t 2 , 1]. We have f (u) -2f u + v 2 + f (v) = M |(1 -∆) ln(1 -∆) + (1 + ∆) ln(1 + ∆)| . (166) Since ∆ = t 1-M M implies M = t 2 t 2 +∆ 2 ,</formula><p>we can recast the corresponding optimization problem as maximizing <ref type="bibr" target="#b0">1]</ref>. One can show that the maximum is always attained at ∆ = 1, with the maximum value t 2 ln 4 1+t 2 . To sum up, we conclude that when 0 &lt; t ≤ 1, the maximum of the optimization problem defining ω 2 ϕ (-x ln x, t) is always attained at u = 0, v = 2t 2 1+t 2 , with the resulting modulus t 2 ln 4 1+t 2 . Analogous computation can also be done for function x α , 0 &lt; α &lt; 1. For the function x α , 1 &lt; α &lt; 2, it is hard to compute the modulus exactly, but it is easy to show that it is of order t 2 .</p><formula xml:id="formula_209">t 2 ∆ 2 + t 2 |(1 -∆) ln(1 -∆) + (1 + ∆) ln(1 + ∆)| (167) subject to constraint ∆ ∈ [t 2 ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Proof of Lemma 9</head><p>The bias of F α (P n ), 1 &lt; α &lt; 2 can be expressed as follows:</p><formula xml:id="formula_210">|Bias(F α (P n ))| = |E S i=1 P α n (i) -p α i | (168) ≤ E i:pi≤ 1 n P α n (i) -p α i + E i:pi&gt; 1 n P α n (i) -p α i (169) B 1 + B 2 . (<label>170</label></formula><formula xml:id="formula_211">)</formula><p>Now we bound B 1 and B 2 separately. It follows from Jensen's inequality that for any i,</p><formula xml:id="formula_212">EP α n (i) ≥ p α i , 1 ≤ i ≤ S.<label>(171)</label></formula><p>Hence, we have</p><formula xml:id="formula_213">B 1 = E i:pi≤ 1 n P α n (i) -p α i (172) = E i:pi≤ 1 n P α n (i) - (np i ) α n α (173) ≤ E i:pi≤ 1 n P α n (i) - (np i ) 2 n α (174) = E i:pi≤ 1 n (nP n (i)) α n α - (np i ) 2 n α (175) ≤ E i:pi≤ 1 n (nP n (i)) 2 n α - (np i ) 2 n α (176) = i:pi≤ 1 n E(nP n (i)) 2 n α - (np i ) 2 n α (177) = i:pi≤ 1 n (np i ) 2 + np i (1 -p i ) n α - (np i ) 2 n α<label>(178)</label></formula><formula xml:id="formula_214">≤ i:pi≤ 1 n np i n α (179) = 1 n α-1 ,<label>(180)</label></formula><p>where we have used the fact that nP n (i) ≥ 1 for any P n (i) = 0.</p><p>Regarding B 2 , we have the following bounds:</p><formula xml:id="formula_215">B 2 = E i:pi&gt; 1 n P α n (i) -p α i (181) ≤ i:pi&gt; 1 n E|P α n (i) -p α i |<label>(182)</label></formula><formula xml:id="formula_216">≤ i:pi&gt; 1 n 3 2 |2 -2 α | p i (1 -p i ) n α/2 (183) ≤ 3|2 -2 α | 2n α/2 i:pi&gt; 1 n p α/2 i ,<label>(184)</label></formula><p>where the second inequality follows from the pointwise estimate in Lemma 7.</p><p>Denoting |{i :</p><formula xml:id="formula_217">p i &gt; 1 n }| = K ≤ n, we know i:pi&gt; 1 n p α/2 i ≤ K 1-α/2 ≤ n 1-α/2 ,<label>(185)</label></formula><p>which implies that</p><formula xml:id="formula_218">B 2 ≤ 3|2 -2 α | 2n α/2 n 1-α/2 = 3|2 -2 α | 2n α-1 .<label>(186)</label></formula><p>Therefore, we have</p><formula xml:id="formula_219">|Bias(F α (P n ))| ≤ B 1 + B 2 (187) ≤ 1 n α-1 + 3|2 -2 α | 2n α-1 (188) ≤ 3|2 -2 α | + 2 2n α-1 (189) ≤ 4 n α-1 .</formula><p>(190)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Proof of Lemma 11</head><p>We apply Lemma 10. Note that h 2 = √ n n+Sa . In order to ensure that h 2 ≤ 1/2, it suffices to take n ≥ 4. Also, since n ≥ Sa, for any i,</p><formula xml:id="formula_220">1 ≤ i ≤ S, |1 -p i S|a n + Sa ≤ Sa n + Sa ≤ 1 2 . (<label>191</label></formula><formula xml:id="formula_221">)</formula><p>Meanwhile, since the function</p><formula xml:id="formula_222">S i=1</formula><p>|1-piS|a n+Sa is a convex function of P = (p 1 , p 2 , . . . , p S ), it attains its maximum at one of the corner points of the simplex. Hence,</p><formula xml:id="formula_223">S i=1 |1 -p i S|a n + Sa ≤ |1 -S|a n + Sa + (S -1) • a n + Sa (192) = 2(S -1)a n + Sa . (<label>193</label></formula><formula xml:id="formula_224">)</formula><p>In light of Lemma</p><p>10, we have |E P H( PB ) -H(P )| ≤ S i=1 ω 1 f, |1 -p i S|a n + Sa ; p i + 5n ln 2 (n + Sa) 2 (194) (a) ≤ -S i=1 |1 -p i S|a n + Sa ln 1 S S i=1 |1 -p i S|a n + Sa + 5nS ln 2 (n + Sa) 2 (195) (b) ≤ 2Sa n + Sa ln n + Sa 2a + 5nS ln 2 (n + Sa) 2 , (196) where (a) follows from the fact that if |x -y| ≤ 1/2, x, y ∈ [0, 1], then |x ln x -y ln y| ≤ -|x -y| ln |x -y| [82, Thm. 17.3.3] and Jensen's inequality. Step (b) follows from the fact that the function -y ln y is monotonically increasing on the interval [0, e -1 ], and 1 S S i=1 |1 -p i S|a n + Sa ≤ 2a n + Sa (197)</p><formula xml:id="formula_225">≤ 2a n (198) ≤ e -1 ,<label>(199)</label></formula><p>where in the last step we used the assumption that n ≥ 2ea.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Proof of Lemma 12</head><p>We have</p><formula xml:id="formula_226">H( PB ) = S i=1 -p B,i ln pB,i<label>(200)</label></formula><p>= H(P B ) + S i=1 (p B,i -pB,i ) ln p B,i -S i=1 pB,i ln pB,i p B,i . (201) Taking expectations on both sides, we have EH( PB ) -H(P ) = H(P B ) -H(P ) -ED( PB P B ), (202) where D(P Q) = S i=1 p i ln pi qi is the KL divergence between distributions P and Q. Since H(P B ) = H n n+Sa P + Sa n+Sa U S , where U S denotes the uniform distribution with alphabet size S, it follows from Jensen's inequality and the concavity of the entropy function that H(P B ) ≥ n n + Sa H(P ) + Sa n + Sa H(U S ) (203) ≥ H(P ). (204) Hence, EH( PB ) -H(P ) ≤ max{H(P B ) -H(P ), ED( PB P B )}.</p><p>In order to analyze the bias, it suffices to analyze the two terms separately. We first analyze ED( PB P B ).</p><p>It follows from Jensen's inequality that</p><formula xml:id="formula_228">D(P Q) = S i=1 p i ln p i q i ≤ ln S i=1 p 2 i q i ,<label>(206)</label></formula><p>whose derivation here follows from Tsybakov <ref type="bibr" target="#b71">[72,</ref><ref type="bibr">Lemma 2.7]</ref>. By Jensen's inequality, we have</p><formula xml:id="formula_229">ED( PB P B ) ≤ E ln S i=1 p2 B,i p B,i ≤ ln S i=1 Ep 2 B,i p B,i<label>.</label></formula><p>(207) We also have</p><formula xml:id="formula_230">S i=1 p 2 i q i = 1 + S i=1 (p i -q i ) 2 q i ,<label>(208)</label></formula><p>and that</p><formula xml:id="formula_231">E(p B,i -p B,i ) 2 = n 2 (n + Sa) 2 E(p i -p i ) 2 = np i (1 -p i ) (n + Sa) 2 . (209) Hence, ED( PB P B ) ≤ ln 1 + S i=1 np i (1 -p i ) (n + Sa)(np i + a) (210) = ln 1 + S i=1 np i (1 -p i ) (n + Sa)np i np i np i + a (211) ≤ ln 1 + S i=1 1 -p i (n + Sa) ,<label>(212)</label></formula><p>which implies that</p><formula xml:id="formula_232">ED( PB P B ) ≤ ln 1 + S -1 n + Sa . (<label>213</label></formula><formula xml:id="formula_233">)</formula><p>Now we consider the deterministic gap H(P B ) -H(P ). It follows from a refinement result of Cover and Thomas <ref type="bibr" target="#b81">[82,</ref><ref type="bibr">Thm. 17.3.3</ref>] that when |p B,i -p i | ≤ 1/2 for all i, we have</p><formula xml:id="formula_234">|H(P B ) -H(P )| ≤ -P B -P 1 ln P B -P 1 S (214) = S • f P B -P 1 S ,<label>(215)</label></formula><p>where</p><formula xml:id="formula_235">f (x) = -x ln x, x ∈ [0, 1]. Note that the condition n ≥ Sa ensures that |p B,i -p i | ≤ 1/2.</formula><p>We have</p><formula xml:id="formula_236">1 S P B -P 1 = 1 S S i=1 Sa n + Sa |p i -1/S| (216) = 1 S S i=1 |1 -p i S|a n + Sa (217) ≤ 2a n + Sa ,<label>(218)</label></formula><p>where the last step follows from (192). Since we have assumed n ≥ 2ea, we have 2a n+Sa ≤ 2a n ≤ e -1 . Since the function f (x) = -x ln x is monotonically increasing on the interval [0, e -1 ], we know</p><formula xml:id="formula_237">|H(P B ) -H(P )| ≤ 2Sa n + Sa ln n + Sa 2a .<label>(219)</label></formula><p>F. Proof of <ref type="bibr">Lemma 13</ref> In our case, apparently F (P n ) is a function of n independent random variables {Z i } 1≤i≤n taking values in Z = {1, 2, . . . , S}. Changing one location of the sample would make some symbol with count j to have count j + 1, and another symbol with count i to have count i -1. Then the total change in the functional estimator is</p><formula xml:id="formula_238">f j + 1 n -f j n -f i n + f i -1 n . (<label>220</label></formula><formula xml:id="formula_239">)</formula><p>If f is monotone, then the total change would be upper bounded by max 0≤j&lt;n |f ((j + 1)/n) -f (j/n)|. If f is not monotone, the total change can be upper bounded by 2 • max 0≤j&lt;n |f ((j + 1)/n) -f (j/n)|. Applying Lemma 2, we have the desired bounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Proof of Lemma 14</head><p>In light of Lemma 27 and 28, we have</p><formula xml:id="formula_240">S i=1 Var(P n (i) α ) = i:pi≤1/n Var(P n (i) α ) + i:pi&gt;1/n Var(P n (i) α ) (221) ≤ i:pi≤1/n 2 n 2α ∧ 2p i n 2α-1 + i:pi&gt;1/n 10p 2α-1 i n + 3 2α 16α en 2α + 2 n 2α + 1 8α 2 8α en 2α .<label>(222)</label></formula><p>We obtain the desired bounds after using the concavity of x 2α-1 when 1/2 ≤ α &lt; 1.</p><p>Now we exploit the negative association property of all random variables P n (i), 1 ≤ i ≤ S. Corollary 5 and the monotonically increasing property of x α yield</p><formula xml:id="formula_241">Var(F α (P n )) = S i=1 Var(P n (i) α ) + 2 1≤i&lt;j≤S Cov(P n (i) α , P n (j) α ) (223) ≤ S i=1 Var(P n (i) α ),<label>(224)</label></formula><p>which finishes the proof of Lemma 14.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Proof of Lemma 15</head><p>The upper bound (ln n) 2 /n follows from Lemma 13. We apply the Efron-Stein inequality (Lemma 1) to obtain the other bound. Denote the n i.i.d. samples from distribution P as Z 1 , Z 2 , . . . , Z n ∈ Z. Denoting the MLE H(P n ) as Ĥ(Z 1 , Z 2 , . . . , Z n ), since it is invariant to any permutation of {Z 1 , Z 2 , . . . , Z n }, we know that the Efron-Stein inequality implies</p><formula xml:id="formula_242">Var(H(P n )) ≤ n 2 E Ĥ(Z 1 , Z 2 , . . . , Z n ) -Ĥ(Z ′ 1 , Z 2 , . . . , Z n ) 2 , (225) where Z ′ 1 is an i.i.d. copy of Z 1 . Recall that X i = n j=1 ½(Z j = i), 1 ≤ i ≤ S.<label>(226)</label></formula><p>For notional brevity, we denote the S-tuple (X 1 , X 2 , . . . , X S ) as X S 1 , and the n-tuple (Z 1 , Z 2 , . . . , Z n ) as Z n 1 . A specific realization of (X 1 , X 2 , . . . , X S ) is denoted by x S 1 = (x 1 , x 2 , . . . , x S ), and a specific realization of</p><formula xml:id="formula_243">(Z 1 , Z 2 , . . . , Z n ) is denoted by z n 1 = (z 1 , z 2 , . . . , z n ).</formula><p>In order to upper bound the right hand side of (225), we first condition on {X 1 , X 2 , . . . , X S }. In other words, we use</p><formula xml:id="formula_244">E Ĥ(Z 1 , Z 2 , . . . , Z n ) -Ĥ(Z ′ 1 , Z 2 , . . . , Z n ) 2 (227) = E E Ĥ(Z 1 , Z 2 , . . . , Z n ) -Ĥ(Z ′ 1 , Z 2 , . . . , Z n ) 2 X S 1 .<label>(228)</label></formula><p>The following lemma calculates the conditional distribution of Z 1 conditioned on (X 1 , X 2 , . . . , X S ).</p><p>Lemma 29. The conditional distribution of Z 1 conditioned on (X 1 , X 2 , . . . , X S ) is given by the following discrete distribution:</p><p>(X 1 /n, X 2 /n, . . . , X S /n).</p><p>Proof. By definition of conditional distribution, for any k, 1 ≤ k ≤ S, we have</p><formula xml:id="formula_246">P(Z 1 = k|X S 1 = x S 1 ) = P(Z 1 = k, X S 1 = x S 1 ) P(X S 1 = x S 1 )<label>(230)</label></formula><formula xml:id="formula_247">= P(Z 1 = k)P(X S 1 = x S 1 |Z 1 = k) P(X S 1 = x S 1 )<label>(231)</label></formula><formula xml:id="formula_248">= p k n-1 x1,x2,...,x k -1,...,xS p x k -1 k i =k p xi i n x1,x2,...,xS 1≤i≤S p xi i (232) = n-1 x1,x2,...,x k -1,...,xS n x1,x2,...,xS<label>(233)</label></formula><formula xml:id="formula_249">= x k n ,<label>(234)</label></formula><p>where the multinomial coefficient n x1,x2,...,xS is defined as n</p><formula xml:id="formula_250">x 1 , x 2 , . . . , x S = n! S i=1 x i ! . (<label>235</label></formula><formula xml:id="formula_251">)</formula><p>Denoting r(p) = -p ln p, we have r(j/n) -j n ln j n . We rewrite</p><formula xml:id="formula_252">Ĥ(Z ′ 1 , Z 2 , . . . , Z n )-Ĥ(Z 1 , Z 2 , . . . , Z n ) = D -+D + ,<label>(236)</label></formula><p>where</p><formula xml:id="formula_253">D -= r X Z1 -1 n -r X Z1 n<label>(237)</label></formula><formula xml:id="formula_254">D + =    r X Z ′ 1 +1 n -r X Z ′ 1 n Z 1 = Z ′ 1 r X Z ′ 1 n -r X Z ′ 1 -1 n Z 1 = Z ′ 1 (238)</formula><p>Here, D -is the change in Ĥ that occurs when Z 1 is removed according to the distribution specified in Lemma 29, and D + is the change in Ĥ that occurs when Z ′ 1 is added back according to the true distribution P .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Now we compute</head><formula xml:id="formula_255">E[D 2 -|X S 1 ] and E[D 2 + |X S 1 ]. We have E[D 2 -|X S 1 ] = 1≤i≤S X i n r X i -1 n -r X i n 2 ,<label>(239)</label></formula><p>and</p><formula xml:id="formula_256">E[D 2 + |X S 1 ] = 1≤i≤S p i X i n r X i n -r X i -1 n 2 + 1≤i≤S p i 1 - X i n r X i + 1 n -r X i n 2 .<label>(240)</label></formula><p>Note that we interpret Xi n r Xi n -r Xi-1 n 2 as 0 when</p><formula xml:id="formula_257">X i = 0. Taking expectations of E[D 2 -|X S 1 ] and E[D 2 + |X S 1 ] with respect to X S 1 , we have E[D 2 -] = 1≤i≤S 1≤j≤n j n r j -1 n -r j n 2 × P(B(n, p i ) = j)<label>(241)</label></formula><p>and</p><formula xml:id="formula_258">E[D 2 + ] = 1≤i≤S 0≤j≤n j n r j -1 n -r j n 2 + 1 - j n r j n -r j + 1 n 2 × p i P(B(n, p i ) = j).<label>(242)</label></formula><p>After some algebra, one can show that E</p><formula xml:id="formula_259">[D 2 + ] = E[D 2 -]. It then follows from (225) that Var(H(P n )) ≤ n 2 • E Ĥ(Z 1 , Z 2 , . . . , Z n ) -Ĥ(Z ′ 1 , Z 2 , . . . , Z n ) 2 (243) = n 2 • E (D -+ D + ) 2 (244) ≤ n • E(D 2 -+ D 2 + ) (245) ≤ 2nED 2 - (246) = 2n • 1≤i≤S EP n (i) r(P n (i)) -r(P n (i) - 1 n ) 2<label>(247)</label></formula><p>The proof above is an elaborate version of that in [14, App. B.3]. Now we proceed to obtain non-asymptotic upper bounds of (247). For x ≥ 1/n, it follows from Taylor expansion with integral form residue that</p><formula xml:id="formula_260">(x - 1 n ) ln(x - 1 n ) = x ln x + (ln x + 1)(- 1 n ) + x-1 n x (x - 1 n -u) 1 u du. (248)</formula><p>Then, we have</p><formula xml:id="formula_261">r(P n (i)) -r P n (i) - 1 n ≤ | ln P n (i) + 1| n + Pn(i)-1 n Pn(i) P n (i) -1 n u du + 1 n (249) ≤ | ln P n (i) + 1| + 2 n .<label>(250)</label></formula><p>Hence, we have</p><formula xml:id="formula_262">Var(H(P n )) ≤ 2n • 1≤i≤S EP n (i) | ln P n (i) + 1| + 2 n 2 .</formula><p>(251) Noting that ln P n (i) ≤ 0, hence 0 ≤ | ln P n (i) + 1| ≤ 1ln P n (i). We have</p><formula xml:id="formula_263">Var(H(P n )) ≤ 2 n • 1≤i≤S EP n (i) (ln P n (i) -3) 2 (252) ≤ 2 n 1≤i≤S p i (ln p i -3) 2 (253) ≤ 2 n S • 1 S (-ln S -3) 2 (254) = 2(ln S + 3) 2 n ,<label>(255)</label></formula><p>where we have used the fact that x(ln x -3) 2 is a concave function on [0, 1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Proof of Lemma 16</head><p>We apply the bounded differences inequality (Lemma 2). In our case, F ( PB ) is a function of n independent random variables {Z i } 1≤i≤n taking values in Z = {1, 2, • • • , S}. Changing one location of the sample would make some symbol with count j to have count j + 1, and another symbol with count i to have count i -1. Then the absolute value of the total change in the functional estimator is</p><formula xml:id="formula_264">f j + 1 + a n + Sa -f j + a n + Sa -f i + a n + Sa + f i -1 + a n + Sa (256) ≤ 2 max 1≤k≤n f k + a n + Sa -f k -1 + a n + Sa .<label>(257)</label></formula><p>In light of the Taylor expansion with integral form residue, we have that for 1 ≥ x ≥ t &gt; 0, (x -t) ln(x -t) = x ln x -t(ln x + 1) +</p><formula xml:id="formula_265">x-t x x -t -u u du (258) so |(x -t) ln(x -t) -x ln x| ≤ t| ln x + 1| + x-t x x -t u du + t (259) ≤ t| ln x + 1| + 2t (260) ≤ t(3 -ln x).<label>(261)</label></formula><p>As a result,</p><formula xml:id="formula_266">max 1≤k≤n f k + a n + Sa -f k -1 + a n + Sa ≤ max 1≤k≤n 1 n + Sa 3 -ln k + a n + Sa (262) ≤ 1 n + Sa 3 + ln n + Sa a + 1 .<label>(263)</label></formula><p>Hence, the bounded differences inequality shows that</p><formula xml:id="formula_267">Var H( PB ) ≤ n max 2≤k≤n f k + a n + Sa -f k -1 + a n + Sa 2 (264) ≤ n (n + Sa) 2 3 + ln n + Sa a + 1 2 ,<label>(265)</label></formula><p>which completes the proof of the first part.</p><p>To prove the second part, we use the Efron-Stein inequality (Lemma 1). Since H</p><formula xml:id="formula_268">( PB ) = ĤB (Z 1 , • • • , Z n ) is invariant to any permutation of (Z 1 , Z 2 , • • • , Z n ), we know that the Efron-Stein inequality implies Var H( PB ) ≤ n 2 E ĤB (Z ′ 1 , Z 2 , • • • , Z n ) -ĤB (Z 1 , Z 2 , • • • , Z n ) 2 ,<label>(266)</label></formula><p>where Z ′ 1 is an i.i.d. copy of Z 1 . Recall that</p><formula xml:id="formula_269">X i = n j=1 ½(Z j = i), 1 ≤ i ≤ S.<label>(267)</label></formula><p>For brevity, we denote the S-tuple (X 1 , • • • , X S ) as X S 1 , and the n-tuple</p><formula xml:id="formula_270">(Z 1 , • • • , Z n ) as Z n 1 . A specific realization of (X 1 , • • • , X S ) is denoted by x S 1 = (x 1 , • • • , x S )</formula><p>, and a specific realization of (Z 1 ,</p><formula xml:id="formula_271">• • • , Z n ) is denoted by z n 1 = (z 1 , • • • , z n ). Then we have E ĤB (Z ′ 1 , Z 2 , • • • , Z n ) -ĤB (Z 1 , Z 2 , • • • , Z n ) 2 (268) = x S 1 P(X S 1 = x S 1 ) × E ĤB (Z ′ 1 , Z 2 , • • • , Z n ) -ĤB (Z 1 , Z 2 , • • • , Z n ) 2 X S 1 = x S 1 .<label>(269)</label></formula><p>In light of Lemma 29, we know that the conditional distribution of Z 1 conditioned on (X 1 , • • • , X S ) is the discrete distribution (X 1 /n, X 2 /n, • • • , X S /n). Denoting r(p) = f ( np+a n+Sa ),</p><p>we can rewrite</p><formula xml:id="formula_272">ĤB (Z ′ 1 , Z 2 , • • • , Z n ) -ĤB (Z 1 , Z 2 , • • • , Z n ) = D -+ D +<label>(270)</label></formula><p>where</p><formula xml:id="formula_273">D -= r X Z1 -1 n -r X Z1 n<label>(271)</label></formula><formula xml:id="formula_274">D + =    r X Z ′ 1 +1 n -r X Z ′ 1 n Z 1 = Z ′ 1 r X Z ′ 1 n -r X Z ′ 1 -1 n Z 1 = Z ′ 1 .<label>(272)</label></formula><p>Here, D -is the change in ĤB that occurs when Z 1 is removed according to the distribution (X 1 /n, X 2 /n, • • • , X S /n), and D + is the change in Ĥ that occurs when Z ′ 1 is added back according to the true distribution P . Now we have </p><formula xml:id="formula_275">E[D 2 -|X S 1 ] = S i=1 X i n r X i -1 n -r X i n 2<label>(273)</label></formula><formula xml:id="formula_276">E[D 2 + |X S 1 ] = S i=1 p i X i n r X i n -r X i -1 n 2 + S i=1 p i 1 - X i n r X i + 1 n -r X i n 2<label>(</label></formula><p>After some algebra we can show that E[D</p><p>2 -] = E[D 2 + ]. Hence, we have Var H( PB ) ≤ n 2 E (D -+ D + ) 2 ≤ nE D 2 -+ D 2 + = 2nED 2 -(277) = 2n S i=1 EP n (i) r(P n (i) -1 n ) -r(P n (i)) 2 (278) ≤ 2n S i=1 EP n (i) 1 n + Sa 3 -ln nP n (i) + a n + Sa 2 (279) = 2n (n + Sa) 2 S i=1 EP n (i) 3 -ln nP n (i) + a n + Sa 2 (280) ≤ 2n (n + Sa) 2 S i=1 p i 3 -ln np i + a n + Sa 2 (281) ≤ 2n (n + Sa) 2 S • 1 S 3 -ln n/S + a n + Sa 2 (282) = 2n(3 + ln S) 2 (n + Sa) 2 (283) where we have used the inequality (261) and Jensen's inequality due to d 2 dx 2 x ln nx + a n + Sa -3 2 = n nx + a 3 ln nx + a n + Sa -3 + nx nx + a 4 -ln nx + a n + Sa (284) &lt; 0. (285) J. Proof of Lemma 18 It is well known (see, e.g. [75, Cor. 10.4.2]</p><p>) that if f is concave in (0, 1), then</p><formula xml:id="formula_278">f (x) -B n [f ](x) ≥ 0, 0 ≤ x ≤ 1.<label>(286)</label></formula><p>Hence we focus on deriving the other bound. For concave function f α (x) = -x α , α ∈ (1, 2), Taylor's polynomial of degree 5 at x = x 0 takes the form </p><formula xml:id="formula_279">B n [(x -x 0 ) 3 ](x 0 ) = x 0 (1 -x 0 ) n 2 (1 -2x 0 )<label>(288)</label></formula><p>B n [(x -x 0 ) 4 ](x 0 ) = 3</p><formula xml:id="formula_280">x 2 0 (1 -x 0 ) 2 n 2 + x 0 (1 -x 0 ) n 3 [1 -6x 0 (1 -x 0 )]<label>(289)</label></formula><p>B n [(x -x 0 ) 5 ](x 0 ) = 10</p><formula xml:id="formula_281">x 2 0 (1 -x 0 ) 2 n 3 + x 0 (1 -x 0 ) n 4 [1 -12x 0 (1 -x 0 )] × (1 -2x 0 )<label>(290)</label></formula><p>Applying Lemma 17 and Lemma 30, taking x 0 = x, we have the desired bound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K. Proof of Lemma 19</head><p>It is well known (see, e.g. <ref type="bibr" target="#b74">[75,</ref><ref type="bibr">Cor. 10.4.2]</ref>) that if f is concave in (0, 1), then f (x) -B n [f ](x) ≥ 0, 0 ≤ x ≤ 1.</p><p>(291)</p><p>Hence we focus on deriving the other bound. For function f α (x) = x α , Taylor's polynomial of degree 3 at x = x 0 takes the form</p><formula xml:id="formula_282">Q 3 (x) = α(α -1) 2 x α-2 0 (x -x 0 ) 2 + α(α -1)(α -2) 6 x α-3 0 (x -x 0 ) 3 + affine terms of x.<label>(292)</label></formula><p>Applying Lemma 17 and Lemma 30, taking x 0 = x, we have</p><formula xml:id="formula_283">Q 3 (x) -B n [Q 3 ](x) = - α(α -1) 2 x α-2 x(1 -x) n - α(α -1)(α -2) 6 x α-3 x(1 -x) n 2 (1 -2x) (293) = α(1 -α) 2n x α-2 (1 -x) x - 2 -α 3n (1 -2x) (294) ≥ α(1 -α) 2n x α-2 (1 -x) x - 2 -α 3n .<label>(295)</label></formula><p>Hence, we have</p><formula xml:id="formula_284">f α (x) -B n [f α ](x) ≥ Q 3 (x) -B n [Q 3 ](x)<label>(296)</label></formula><p>≥ α(1 -α) 2n</p><p>x α-2 (1 -x) x -2 -α 3n .</p><p>(297)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L. Proof of Lemma 23</head><p>By setting P = (1, 0, 0,</p><p>• • • , 0), we have H(P ) = 0 and H( PB ) = -(S -1)a n + Sa ln a n + Sa -n + a n + Sa ln n + a n + Sa (298) ≥ (S -1)a n + Sa ln n + Sa a , (299) hence we have obtained the first lower bound sup P ∈MS E P H( PB ) -H(P ) ≥ (S -1)a n + Sa ln n + Sa a . (300) If n &lt; Sa, then sup P ∈MS E P H( PB ) -H(P ) ≥ (S -1)a 2Sa ln S (301) ≥ S -1 2S ln S. (302) If n &gt; 2ea, then sup P ∈MS E P H( PB ) -H(P ) ≥ (S -1)a Sa + 2ea ln S (303) = S -1 S + 2e ln S. (304) From now on we assume n ≥ Sa, n ≥ 2ea. For n ≥ 15S, it follows from applying Lemma 21 that sup P ∈MS E P H( P ) -H(P ) ≥ S -1 2n + S 2 20n 2 -1 12n 2 . (305) If n &lt; 15S, then it follows from applying Lemma 21 that one can essentially take S = ⌊n/15⌋ in (305), and obtain sup P ∈MS E P H( P ) -H(P ) ≥ ⌊n/15⌋ 2n -1 4n . (306) It follows from a refinement result of Cover and Thomas [82, Thm. 17.3.3] that when |p B,i -pi | ≤ 1/2 for all i (which is ensured by condition n ≥ Sa), we have |H( PB ) -H( P )| ≤ Sf PB -P 1 S , (307) where f (x) = -x ln x, x ∈ [0, 1]. We have 1 S PB -P 1 = 1 S S i=1 |S pi -1|a n + Sa (308) ≤ 2(S -1)a S(n + Sa) , (309) where the last step follows from (192). Since we have assumed n ≥ 2ea, we have 2a n+Sa ≤ 2a n ≤ e -1 . Since the function f (x) = -x ln x, x ∈ [0, 1] is monotonically increasing when x ∈ [0, e -1 ], we have |H( PB ) -H( P )| ≤ 2(S -1)a n + Sa ln n + Sa a . (310) A combination of these two inequalities yield the second lower bound sup P ∈MS E P H( PB ) -H(P ) ≥ S -1 2n + S 2 20n 2 -1 12n 2 -2(S -1)a n + Sa ln n + Sa a (311) when n ≥ 15S, and the second lower bound sup P ∈MS E P H( PB ) -H(P ) ≥ ⌊n/15⌋ 2n -1 4n -2(S -1)a n + Sa ln n + Sa a (312)</p><p>when n &lt; 15S.</p><p>Hence we are done by using these two lower bounds and the inequality max{a, b} ≥ 3a+b 4 .</p><p>Tsachy's research is focused on information theory, statistical signal processing, the interplay between them, and their applications.</p><p>He is recipient of several best paper awards, and prizes for excellence in research.</p><p>He served on the editorial board of the IEEE TRANSACTIONS ON INFORMATION THEORY from Sept. 2010 to Aug. 2013, and currently serves on the editorial board of Foundations and Trends in Communications and Information Theory.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Corollary 4 .</head><label>4</label><figDesc>If n ≫ S and a is upper bounded by a constant, then the maximum squared error risk of H( PB ) vanishes. Conversely, if n S, then the maximum squared error risk of H( PB ) is bounded away from zero.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Lemma 4 .</head><label>4</label><figDesc><ref type="bibr" target="#b61">[62,</ref> Cor. 2.2.1.]  Let F : C(I) → R be a positive linear functional, where</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Lemma 5 .</head><label>5</label><figDesc><ref type="bibr" target="#b61">[62,</ref> Thm. 2.5.1.]  If F : C[0, 1] → R is a linear positive functional and F (e 0 ) = 1, then we have</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><figDesc>) which is almost the supremum of ω 1 (f, |F (e 1 -xe 0 )|; s) over s ∈ [x, 1 -x] and is no less than the pointwise result ω 1 (f, |F (e 1 -xe 0 )|; x), and here we have used the inequality ϕ(s) ≥ ϕ(x) for x ≤ s ≤ 1-x. A similar argument also holds for x &gt; 1/2. Hence, Lemma 10 transforms the first order term from the norm result in Lemma 5 to a pointwise result.Applying Lemma 10 to the function f (p) = -p ln p andF (f ) = E f n p+a n+Sa , where n • p ∼ B(n, p),we have the following lemma. Lemma 11. If n ≥ max{Sa, 2ea, 4}, then sup P ∈MS |E P H( PB ) -H(P )| ≤ 5nS ln 2 (n + Sa) 2 + 2Sa n + Sa ln n + Sa 2a .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>2 × 2 ×</head><label>22</label><figDesc>274)where we define r(x) = 0 when x / ∈ [0, 1]. Then, by the law of iterated expectation, we know thatE[D 2 -P(B(n, p i ) = j) p i P(B(n, p i ) = j).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Q 5 ((x -x 0 ) 4 - 5 0(x -x 0 ) 5 +</head><label>5455</label><figDesc>α(α -1)(α -2)(α -3)(α -4) 120 x α-affine terms of xWe know that the Bernstein polynomial of any affine function on [0, 1] is the affine function itself, hence it suffices to consider the non-affine part of Q 5 (x). [69, Prop. 4] showed the following results for Bernstein polynomials:Lemma 30. Let 0 ≤ x 0 ≤ 1. Then we have B n [(x -x 0 ) 2 ](x 0 ) = x 0 (1 -x 0 ) n (287)</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>In the literature of combinatorics, the sum n j=0 a j,n B j,n (x) is called the Bernoulli sum, and various approaches have been proposed to evaluate its asymptotics<ref type="bibr" target="#b57">[58]</ref>,<ref type="bibr" target="#b58">[59]</ref>,<ref type="bibr" target="#b59">[60]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Note that it is a remarkable fact that (77) holds for any continuous function f (x). The lower bound proof of (77) is considered one of the remarkable results in approximation theory, and currently there are no "short" proofs of this fact. Indeed, Ditzian<ref type="bibr" target="#b76">[77,</ref> Section 8]  mentioned that "I still would like to see a new simple proof of (8.4) (Equation (77)) which I am sure will have implications for other operators."</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>We thank <rs type="person">Dany Leviatan</rs>, <rs type="person">Gancho Tachev</rs>, and <rs type="person">Radu Paltanea</rs> for very helpful discussions regarding the literature on approximation theory using positive linear operators. We thank <rs type="person">Jayadev Acharya</rs>, <rs type="person">Alon Orlitsky</rs>, <rs type="person">Ananda Theertha Suresh</rs>, and <rs type="person">Himanshu Tyagi</rs> for communicating to us the independent discovery that it suffices to take n ≫ 1 samples to consistently estimate F α (P ), when α &gt; 1. We thank <rs type="person">Maya Gupta</rs> for raising the question on the optimality of the Dirichlet prior smoothing techniques applying to entropy estimation. We thank the anonymous reviewers and the associate editor for very helpful comments that significantly improved the presentation of the paper.</p></div>
			</div>
			<div type="funding">
<div><p>This work was supported in part by the <rs type="funder">Center for Science of Information (CSoI)</rs> under grant agreement <rs type="grantNumber">CCF-0939370</rs>. Materials of this paper were presented in part at the 2015 <rs type="institution">International Symposium on Information Theory, Hong Kong, China</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_gZ8fVUg">
					<idno type="grant-number">CCF-0939370</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PROOFS OF AUXILIARY LEMMAS</head><p>A. Proof of <ref type="bibr">Lemma 27</ref> Since nX is an integer, we have (nX) 2 ≥ (nX) 2α , 0 &lt; α &lt; 1. Hence, for p ≤ 1/n,</p><p>which completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Proof of Lemma 28</head><p>Denoting f (p) = p α , 0 &lt; α &lt; 1, we have</p><p>Hence, it suffices to obtain bounds on |Ef 2 (X) -f 2 (p)| and |Ef (X) -f (p)|. Denoting r(x) = f 2 (x), it follows from Taylor's formula and the integral representation of the remainder term that</p><p>where η X ∈ [min{X, p}, max{X, p}].</p><p>Similarly, we have</p><p>where ν X ∈ [min{X, p}, max{X, p}].</p><p>Taking expectation on both sides with respect to X, where nX ∼ B(n, p), we have</p><p>Similarly, we have</p><p>It is straightforward to show that</p><p>Now we are in the position to bound |ER 1 (X; p)| and</p><p>where in the last step we have used Lemma 26. Regarding sup x≤p/2 |R 1 (x; p)|, for any x ≤ p/2, we have</p><p>Hence, we have</p><p>Analogously, we obtain the following bound for |ER 2 (X; p)|: </p><p>where we have used the following inequality in the last step: for x ∈ (0, 1) and any c &gt; 0,</p><p>Note that if 0 &lt; α &lt; 1/2, we can upper bound </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On the impact of entropy estimation on transcriptional regulatory network inference based on mutual information</title>
		<author>
			<persName><forename type="first">C</forename><surname>Olsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bontempi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Bioinformatics and Systems Biology</title>
		<imprint>
			<biblScope unit="volume">2009</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">308959</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mutual-informationbased registration of medical images: a survey</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Pluim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Maintz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Viergever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="986" to="1004" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note>Medical Imaging</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Alignment by maximization of mutual information</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mutual information analysis: a comprehensive study</title>
		<author>
			<persName><forename type="first">L</forename><surname>Batina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gierlichs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Prouff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rivain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F.-X</forename><surname>Standaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Veyrat-Charvillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cryptology</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="269" to="291" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Diversity and evenness: a unifying notation and its consequences</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">O</forename><surname>Hill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecology</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="427" to="432" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rényi entropy of the XY spin chain</title>
		<author>
			<persName><forename type="first">F</forename><surname>Franchini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Its</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Korepin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Physics A: Mathematical and Theoretical</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">25302</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A mathematical theory of communication</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Bell System Technical Journal</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="623" to="656" />
			<date type="published" when="1948">1948</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Olshen</surname></persName>
		</author>
		<title level="m">Classification and regression trees</title>
		<imprint>
			<publisher>CRC press</publisher>
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On measures of entropy and information</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rényi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth Berkeley Symposium on Mathematical Statistics and Probability</title>
		<imprint>
			<date type="published" when="1961">1961</date>
			<biblScope unit="page" from="547" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Minimax estimation of functionals of discrete distributions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Venkat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Weissman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2835" to="2885" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Information Theory</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A characterization of limiting distributions of regular estimates</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hájek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Zeitschrift für Wahrscheinlichkeitstheorie und verwandte Gebiete</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="323" to="330" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Local asymptotic minimax and admissibility in estimation</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixth Berkeley symposium on mathematical statistics and probability</title>
		<meeting>the sixth Berkeley symposium on mathematical statistics and probability</meeting>
		<imprint>
			<date type="published" when="1972">1972</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="175" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Asymptotic methods in statistical decision theory</title>
		<author>
			<persName><forename type="first">L</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Cam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Estimation of entropy and mutual information</title>
		<author>
			<persName><forename type="first">L</forename><surname>Paninski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1191" to="1253" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Estimating entropy on m bins given fewer than m samples</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2200" to="2203" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>Information Theory</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Estimating the unseen: an n/ log n-sample estimator for entropy and support size, shown optimal via new CLTs</title>
		<author>
			<persName><forename type="first">G</forename><surname>Valiant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valiant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd annual ACM symposium on Theory of computing</title>
		<meeting>the 43rd annual ACM symposium on Theory of computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="685" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Estimating the unseen: improved estimators for entropy and other properties</title>
		<author>
			<persName><forename type="first">P</forename><surname>Valiant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Valiant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2157" to="2165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The power of linear estimators</title>
		<author>
			<persName><forename type="first">G</forename><surname>Valiant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valiant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations of Computer Science (FOCS), 2011 IEEE 52nd Annual Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="403" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Beyond maximum likelihood: from theory to practice</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Venkat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Weissman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.7458</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Beyond maximum likelihood: Boosting the Chow-Liu algorithm for large alphabets</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Weissman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signals, Systems and Computers, 2016 50th Asilomar Conference</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="321" to="325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Minimax rates of entropy estimation on large alphabets via best polynomial approximation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3702" to="3720" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Estimating Rényi entropy of discrete distributions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Acharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Orlitsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tyagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="38" to="56" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Chebyshev polynomials, moment matching, and optimal estimation of the unseen</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.01227</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Minimax rate-optimal estimation of divergences between discrete distributions</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Weissman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09124</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Minimax estimation of the L 1 distance</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Weissman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Theory (ISIT), 2016 IEEE International Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="750" to="754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Estimation of KL divergence: Optimal minimax rate</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">V</forename><surname>Veeravalli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.02653</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Optimal prediction of the number of unseen species</title>
		<author>
			<persName><forename type="first">A</forename><surname>Orlitsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="page">201607774</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Sample complexity of the distinct elements problem</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03375</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Note on the bias of information estimates</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Theory in Psychology: Problems and Methods</title>
		<imprint>
			<date type="published" when="1955">1955</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="95" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">On the bias of information estimates</title>
		<author>
			<persName><forename type="first">A</forename><surname>Carlton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">108</biblScope>
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Finite sample corrections to entropy and dimension estimates</title>
		<author>
			<persName><forename type="first">P</forename><surname>Grassberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics Letters A</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="369" to="373" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Jackknifing an index of diversity</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecology</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="907" to="913" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Entropy inference and the James-Stein estimator, with application to nonlinear gene association networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Strimmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1469" to="1484" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Nonparametric estimation of Shannon&apos;s index of diversity when there are unseen species in sample</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Environmental and ecological statistics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="429" to="443" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Estimating mutual information using B-spline functions-an improved similarity measure for analysing gene expression data</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">O</forename><surname>Daub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Steuer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Selbig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kloska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">118</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Entropy estimates from insufficient samplings</title>
		<author>
			<persName><forename type="first">P</forename><surname>Grassberger</surname></persName>
		</author>
		<idno>arXiv preprint physics/0307138</idno>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Estimation of the entropy based on its polynomial representation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Vinck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">B</forename><surname>Balakirsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Vinck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Pennartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">51139</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Entropy estimation of symbol sequences</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schürmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Grassberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chaos: An Interdisciplinary Journal of Nonlinear Science</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="414" to="427" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Some worst-case bounds for Bayesian estimators of discrete distributions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Schober</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Theory Proceedings (ISIT), 2013 IEEE International Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2194" to="2198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Estimating functions of probability distributions from a finite set of samples</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Wolpert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">6841</biblScope>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Bayes&apos; estimators of generalized entropies</title>
		<author>
			<persName><forename type="first">D</forename><surname>Holste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Herzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Physics A: Mathematical and General</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">2551</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Entropy and inference, revisited</title>
		<author>
			<persName><forename type="first">I</forename><surname>Nemenman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shafee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bialek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="471" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Bayesian estimation of discrete entropy with mixtures of stick-breaking priors</title>
		<author>
			<persName><forename type="first">E</forename><surname>Archer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Pillow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2015" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Entropy and information in neural spike trains: Progress on the sampling problem</title>
		<author>
			<persName><forename type="first">I</forename><surname>Nemenman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bialek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D R</forename><surname>Van Steveninck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">56111</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Coincidences and estimation of entropies of random variables with large cardinalities</title>
		<author>
			<persName><forename type="first">I</forename><surname>Nemenman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2013" to="2023" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Theory of point estimation</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Casella</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Minimax estimation of discrete distributions under ℓ 1 loss</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Weissman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="6343" to="6354" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Has&apos; minskii, Statistical estimation: asymptotic theory</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Ibragimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Z</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1981">1981</date>
			<publisher>Springer-Verlag</publisher>
			<biblScope unit="volume">2</biblScope>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Convergence properties of functional estimates for discrete distributions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Antos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kontoyiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Random Structures &amp; Algorithms</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="163" to="193" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Estimating the number of unsen species: How many words did Shakespeare know?</title>
		<author>
			<persName><forename type="first">B</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Thisted</surname></persName>
		</author>
		<ptr target="http://www.jstor.org/stable/2335721" />
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="435" to="447" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The jackknife estimate of variance</title>
		<author>
			<persName><forename type="first">B</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="page" from="586" to="596" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Boucheron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lugosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Massart</surname></persName>
		</author>
		<title level="m">Concentration inequalities: A nonasymptotic theory of independence</title>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Asymptotic techniques for use in statistics</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">E</forename><surname>Barndorff-Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Cox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Chapman &amp; Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Expansions and asymptotics for statistics</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Small</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>CRC Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Bootstrap methods: another look at the Jackknife</title>
		<author>
			<persName><forename type="first">B</forename><surname>Efron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">The bootstrap and Edgeworth expansion</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">The statistical estimation of entropy in the non-parametric case</title>
		<author>
			<persName><forename type="first">B</forename><surname>Harris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DTIC Document, Tech. Rep</title>
		<imprint>
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Entropy computations via analytic depoissonization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Jacquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Szpankowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1072" to="1081" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note>Information Theory</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Singularity analysis and asymptotics of Bernoulli sums</title>
		<author>
			<persName><forename type="first">P</forename><surname>Flajolet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">215</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="371" to="381" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">On deltamethod of moments and probabilistic sums</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cichoń</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Golkebiewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kardas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Klonowski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Collected works: Vol 1. constructive theory of functions (1905-1930), English translation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Atomic Energy Commission, Springfield</title>
		<imprint>
			<date type="published" when="1958">1958</date>
			<pubPlace>Va</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Approximation theory using positive linear operators</title>
		<author>
			<persName><forename type="first">R</forename><surname>Paltanea</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Moduli of smoothness</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ditzian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Totik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Quantitative Aussagen zur Approximation durch positive lineare Operatoren</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Gonska</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979">1979</date>
			<publisher>Gesamthochschule Duisburg</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Adaptive estimation of Shannon entropy</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Weissman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Theory (ISIT), 2015 IEEE International Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1372" to="1376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Mathematical expectation of continuous functions of random variables. smoothness and variance</title>
		<author>
			<persName><forename type="first">L</forename><surname>Strukov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Timan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Siberian Mathematical Journal</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="469" to="474" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Probabilistic methods in the approximation by linear positive operators</title>
		<author>
			<persName><forename type="first">H</forename><surname>Walk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Indagationes Mathematicae (Proceedings)</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1980">1980</date>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="445" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">A note on stochastic methods in connection with approximation theorems for positive linear operators</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pacific J. Math</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="307" to="319" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Bernstein polynomials and learning theory</title>
		<author>
			<persName><forename type="first">D</forename><surname>Braess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Approximation Theory</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="187" to="206" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Closed form summation for classical distributions: variations on a theme of de moivre</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diaconis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zabell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical Science</title>
		<imprint>
			<biblScope unit="page" from="284" to="302" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Feller</surname></persName>
		</author>
		<title level="m">An introduction to probability theory and its applications</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Introduction to Nonparametric Estimation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tsybakov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Springer-Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Applications of the van Trees inequality: a Bayesian Cramér-Rao bound</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Gill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Y</forename><surname>Levit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bernoulli</title>
		<imprint>
			<biblScope unit="page" from="59" to="79" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">On some constants in approximation by Bernstein operators</title>
		<author>
			<persName><forename type="first">R</forename><surname>Paltanea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">General Mathematics</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">137148</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Constructive approximation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Devore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Lorentz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">303</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Approximation by Bernstein polynomials</title>
		<author>
			<persName><forename type="first">V</forename><surname>Totik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Mathematics</title>
		<imprint>
			<biblScope unit="page" from="995" to="1018" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Polynomial approximation and ω r ϕ (f, t) twenty years later</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ditzian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Surveys in Approximation Theory</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="106" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Statistical decision functions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wald</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1950">1950</date>
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Negative association of random variables with applications</title>
		<author>
			<persName><forename type="first">K</forename><surname>Joag-Dev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Proschan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="286" to="295" />
			<date type="published" when="1983-03">March 1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Inequalities for the gamma function</title>
		<author>
			<persName><forename type="first">N</forename><surname>Batir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Archiv der Mathematik</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="554" to="563" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Probability and computing: Randomized algorithms and probabilistic analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mitzenmacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
		<title level="m">Elements of Information Theory</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
