<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TransPixar: Advancing Text-to-Video Generation with Transparency</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-01-06">6 Jan 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Luozhou</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yijun</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhifei</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jui-Hsien</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhifei</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">He</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhe</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yingcong</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Adobe</forename><surname>Research</surname></persName>
						</author>
						<title level="a" type="main">TransPixar: Advancing Text-to-Video Generation with Transparency</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-01-06">6 Jan 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">572B7231C2FF22CAE86B8D87F74B8235</idno>
					<idno type="arXiv">arXiv:2501.03006v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A statue crumbling to dust as cracks spread across its surface" "A squirrel's bushy tail flicking quickly" "A portal crackling with arcane magic as it opens" "A small explosion rapidly expanding and contracting" "A massive storm forming, with swirling clouds and thunderbolts" "A motorcycle drifting and swerving in an enchanted forest" "A white dandelion shifting as seen through a magnifying glass" "A transparent glass of water with ice cubes gently swirling" Figure 1. RGBA Video Generation with TransPixar. By introducing LoRA layers into DiT-based text-to-video model with a novel alpha channel adaptive attention mechanism, our method enables RGBA video generation from text while preserving Text-to-Video quality.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Text-to-Video generative models have quickly advanced, achieving impressive results <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b14">16,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b24">26,</ref><ref type="bibr" target="#b45">47,</ref><ref type="bibr" target="#b48">50,</ref><ref type="bibr" target="#b55">57,</ref><ref type="bibr" target="#b60">62,</ref><ref type="bibr" target="#b63">65]</ref>. This progress has enabled various applications, such as video editing <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b11">13,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b51">53,</ref><ref type="bibr" target="#b54">56]</ref>, image animation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">14,</ref><ref type="bibr" target="#b13">15,</ref><ref type="bibr" target="#b36">38]</ref>, and motion customization <ref type="bibr" target="#b16">[18,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" target="#b46">48,</ref><ref type="bibr" target="#b50">52,</ref><ref type="bibr" target="#b58">60]</ref>. Diffusion Transformers (DiT) enhance these models by using self-attention to capture long-range dependencies <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b24">26,</ref><ref type="bibr" target="#b55">57,</ref><ref type="bibr" target="#b63">65]</ref>. These models are now widely used in entertainment, advertising, and education, meeting the demand for customizable, dynamic content. Notably, Text-to-RGBA (A denotes Alpha channel) video generation is invaluable for VFX and creative industries. The inclusion of an alpha channel in RGBA formats allows for transparent effects, enabling seamless blending of elements like smoke and reflections (see Fig. <ref type="figure">1</ref>). This transparency creates realistic visuals that can integrate smoothly into scenes without modifying the background. Such flexibility is crucial in gaming, virtual reality (VR), and augmented reality (AR), where dynamic and interactive content is in high demand.</p><p>Currently, no direct solutions exist for RGBA video generation, which remains a challenging task due to the scarcity of RGBA video data, with only around 484 videos available in <ref type="bibr" target="#b27">[29]</ref>. This scarcity will significantly limit the diversity of generated content, resulting in a constrained set of object types and motion patterns. One feasible solution is to use video matting <ref type="bibr" target="#b26">[28,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" target="#b38">40]</ref> to obtain alpha channels from generated videos. However, these methods are still limited by the scarcity of RGBA video data and struggle to generalize to a wider range of objects, as shown in Fig. <ref type="figure" target="#fig_1">2 (b)</ref>. Other video segmentation methods, such as SAM-2 <ref type="bibr" target="#b39">[41]</ref>, may generalize well to different tasks. However, they cannot generate alpha channels and are therefore unsuitable for direct compositing. There have been attempts to generate RGBA at the image level, such as LayerDiffusion <ref type="bibr" target="#b62">[64]</ref>. However, adapting its concept directly to a temporal VAE used in video generative models remains challenging.</p><p>In this paper, we explore how to extend pretrained video models to generate corresponding alpha channels while retaining the original capabilities of pretrained models. Our goal is to generate content beyond the limitations of the current RGBA training set. Existing works such as Lotus <ref type="bibr" target="#b17">[19]</ref> and Marigold <ref type="bibr" target="#b23">[25]</ref> demonstrate that leveraging pretrained generation model weights significantly enhances out-of-distribution in dense prediction, hinting at the potential for predicting alpha channels. However, in the context of RGBA video generation, these approaches typically require generating RGB channels first, followed by separate alpha channel prediction. Consequently, information flows unidirectionally from RGB to alpha, keeping the two processes largely disconnected. Given the limited availability of RGBA video data, this imbalance results in insufficient alpha prediction when challenging objects are generated, as shown in Fig. <ref type="figure" target="#fig_1">2 (c)</ref>.</p><p>In this work, we propose TransPixar, which effectively adapts the pretrained RGB video models to generate RGB channels and the alpha channel simultaneously. We leverage state-of-the-art DiT-like video generation models <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b55">57]</ref> , and additionally introduce new tokens appended after text and RGB tokens for generating the alpha channels. To facilitate convergence, we reinitialize the positional embeddings for the alpha tokens and introduce a zero-initialized, learnable domain embedding to distinguish alpha tokens from RGB tokens. Furthermore, we employ a LoRA-based fine-tuning scheme <ref type="bibr" target="#b21">[23]</ref>, applied exclusively to project alpha tokens into the qkv space, thereby maintaining RGB generation quality. With the proposed approach, we extend the modality while preserving the original input-  output structure and relying on the existing attention mechanism through LoRA adaptation.</p><p>The extended sequence contains text, RGB, and alpha tokens, with self-attention divided into a 3x3 grouped attention matrix involving interactions like Text-attend-to-RGB (Text as query, RGB as key) and others. We also systematically analyze the attention mechanisms for RGBA generation: 1) Text-attend-to-RGB and RGB-attend-to-Text. The interaction between text and RGB tokens represents original model's generation capabilities. Minimizing the impact on text and RGB tokens during these attention computation processes can better retain the original model's performance; 2) RGB-attend-to-Alpha. We reveals a fundamental limitation in conventional methods is the lack of RGB-attend-to-Alpha attention. This attention is necessary to refine RGB tokens based on alpha information, improving RGB-alpha alignment; 3) Text-attend-to-Alpha. We remove this attention mechanism to reduce the risk caused by limited training data, which could degrade the model's performance. This removal also enhances the retention of the model's original capabilities.</p><p>By integrating these techniques, our method achieves diverse RGBA generation with limited training data while maintaining strong RGB-alpha alignment. To summarize, our contributions are as follows:</p><p>‚Ä¢ We propose an RGBA video generation framework using DiT models that requires limited data and training parameters, achieving diverse generation with strong alignment. ‚Ä¢ We analyze the role of each attention component in the generation process, optimize their interactions, and introduce necessary modifications to improve RGBA generation quality. ‚Ä¢ Our method is validated through extensive experiments, demonstrating its effectiveness across a variety of challenging scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Text-to-Video Generation. Early video generation models were primarily based on Unet-based latent diffusion models (LDMs) extended from text-to-image models like Stable Diffusion <ref type="bibr" target="#b40">[42]</ref>. For example, AnimateDiff <ref type="bibr" target="#b14">[16]</ref> introduced a temporal attention module to improve temporal consistency across frames. Subsequent video generation models [4, <ref type="bibr" target="#b4">6,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target="#b45">47,</ref><ref type="bibr" target="#b60">62,</ref><ref type="bibr" target="#b61">63]</ref> adopted an alternating approach with 2D spatial and 1D temporal attention, including works like ModelScope, VideoCrafter, Moonshot, and Show-1.</p><p>With advancements in large language models (LLMs) and the introduction of Sora <ref type="bibr" target="#b2">[3]</ref>, attention shifted from Unet architectures to transformer-based architectures (DiT). DiT-based video generation models, such as Latte <ref type="bibr" target="#b35">[37]</ref> and OpenSora <ref type="bibr" target="#b63">[65]</ref>, extended the DiT text-to-image (T2I) model <ref type="bibr" target="#b6">[8]</ref> and maintained the 2D and 1D alternating attention approach, achieving promising results. Recently, DiTbased video generation has rapidly progressed, achieving further improvements in quality. Several methods <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b55">57]</ref> have moved away from the 2D and 1D alternating approach, instead treating video frames as a single long sequence with 3D positional embeddings for encoding. These approaches also prepend text tokens-processed through a text encoder-to the video sequence, creating a streamlined network that relies solely on full self-attention and feedforward layers. Our method builds upon these recent opensource transformer-based video generation models.</p><p>Video Matting. A straightforward approach for RGBA video generation is to extract the alpha channel from generated RGB content, as done with traditional green screen keying or learning-based video matting expert models <ref type="bibr" target="#b26">[28]</ref><ref type="bibr" target="#b27">[29]</ref><ref type="bibr" target="#b28">[30]</ref>. OmnimatteRF <ref type="bibr" target="#b26">[28]</ref> introduces a video matting method that combines dynamic 2D foreground layers with a 3D background model, enabling more realistic scene reconstruction for real-world videos. Robust Video Matting (RVM) <ref type="bibr" target="#b28">[30]</ref> proposes a real-time, high-quality human video matting method with a recurrent architecture for improved temporal coherence, achieving state-of-the-art results without auxiliary inputs. Another work presents a high-speed, high-resolution background replacement technique with precise alpha matte extraction, supported by the Video-Matte240K and PhotoMatte13K/85 datasets <ref type="bibr" target="#b27">[29]</ref>. Additionally, many image matting methods <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b49">51,</ref><ref type="bibr" target="#b56">58]</ref> can be applied for frame-by-frame matting.</p><p>Further, several works <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b52">54]</ref> in image depth estimation adapt pretrained generation models for prediction tasks, achieving strong performance that often surpasses traditional, scratch-trained expert models. Marigold <ref type="bibr" target="#b23">[25]</ref> modifies architectures to create image-conditioned generation models, while Lotus <ref type="bibr" target="#b17">[19]</ref> explores the role of the diffusion process in this context. Although there is currently no dedicated approach for video matting within video gen-eration models, we replicate and extend these methods to evaluate their performance, allowing us to highlight the limitations of prediction-based pipelines for RGBA generation. <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">17,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" target="#b53">55,</ref><ref type="bibr" target="#b59">61,</ref><ref type="bibr" target="#b62">64]</ref> explores expanding generation models to simultaneously generate additional channels, though they are not specifically designed for RGBA video generation. For instance, LayerDiffusion <ref type="bibr" target="#b62">[64]</ref> modifies the VAE in latent diffusion models to decode alpha channels. However, VAEs typically lack the semantic understanding required for precise alpha generation, limiting their effectiveness in complex visual scenarios where texture and contour details are critical. In contrast, other approaches <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" target="#b59">61]</ref> modify the denoising model directly to enable joint generation. Wonder3D <ref type="bibr" target="#b32">[34]</ref> uses a domain embedding to control the model's generation modality, while methods like Intrin-sicDiffusion <ref type="bibr" target="#b33">[35]</ref> and RGB‚ÜîX <ref type="bibr" target="#b59">[61]</ref> adapt the UNet's input and output layers to jointly produce intrinsic modalities. However, all these methods are designed for image tasks and rely on UNet architectures. When applied to video generation, they face limitations in quality and diversity due to the scarcity of RGBA video data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generation beyond RGB. Another category of methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminary</head><p>We first introduce the open-sourced state-of-the-art DiTbased video generation models <ref type="bibr" target="#b42">[44,</ref><ref type="bibr" target="#b55">57]</ref>. The core components of DiT-based video models are attention modules, and there are two primary distinctions between these models and previous approaches. On one hand, unlike previous models that alternate between 1D temporal attention and 2D spatial attention <ref type="bibr">[4,</ref><ref type="bibr" target="#b4">6,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target="#b63">65]</ref>, current methods typically employ 3D spatio-temporal attention, allowing them to capture spatio-temporal dependencies more effectively. On the other hand, instead of using cross-attention for text conditioning, these models concatenate text tokens x text with visual tokens x video into a single long sequence. The shape of video tokens and text tokens are B√óL√óD and B√óL text √óD, wher B equals to batch size, L text equals to the length of text tokens, L equals to the length of video tokens and D equals to the latent dimension of transformer. Full self-attention is then applied across the combined sequence:</p><formula xml:id="formula_0">Attention(Q, K, V) = softmax QK T ‚àö d k V,<label>where</label></formula><formula xml:id="formula_1">Z : Z ‚àà {Q, K, V} = [W z:z‚àà{q,k,v} (x text ); f z:z‚àà{q,k,v} (x video )]<label>(1</label></formula><p>) Here W t (for t ‚àà {q, k, v}) represents the projection matrixs in the transformer model, and f t (for t ‚àà {q, k, v}) represents a combined operation that incorporates both the projection and positional encoding for visual tokens. There DiT Block DiT Block Self-Attention Feed Forward ‚Ä¶ Text Tokens QKV Proj. Key Text RGB Alpha Query Text RGB Alpha LoRA Attention Rectification Copy Positional Embedding Domain Embedding RGB Tokens Alpha Tokens Frozen Learnable ‚Ä¶ Positional Encoding Legends ùëßùëßùëßùëßùëßùëßùëßùëß ùëñùëñùëñùëñùëñùëñùëñùëñ. are two commonly used types of positional encoding. One is absolute positional encoding formulated as follows:</p><formula xml:id="formula_2">f z:z‚àà{q,k,v} (x video ) := W z:z‚àà{q,k,v} (x m video + p m ), (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>where p is the positional embedding (e.g., a sinusoidal function) and m denotes the position of each RGB video token. Another approach is the Rotary Position Embedding (RoPE) <ref type="bibr" target="#b41">[43]</ref>, often used by <ref type="bibr" target="#b42">[44,</ref><ref type="bibr" target="#b55">57]</ref>. This is expressed as</p><formula xml:id="formula_4">f z:z‚àà{q,k} (x video ) := W z:z‚àà{q,k} (x m video ) ‚Ä¢ e imŒ∏ ,<label>(3)</label></formula><p>where m is the positional index, i is the imaginary unit for rotation, and Œ∏ is the rotation angle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Our Approach</head><p>To jointly generate RGB and alpha videos, we adapt a pretrained RGB video generation model through several modifications. The whole pipeline is visualized in Fig. <ref type="figure" target="#fig_2">3</ref>. Firstly, we double the sequence length of noisy input tokens to enable the model to generate videos of double length, from x 1:L video to x 1:2 * L video . Here, x 1:L video will be decoded into the RGB video, while x L+1:2 * L video will be decoded into the corresponding alpha video. The Query(Q), Key(K), Value(V) representations are formulated as:</p><formula xml:id="formula_5">Z : Z ‚àà {Q, K, V} = [W z:z‚àà{q,k,v} (x text ); f z:z‚àà{q,k,v} (x 1:2 * L video )]<label>(4)</label></formula><p>In addition to sequence doubling, we explored increasing batch size or latent dimensions and splitting output into two domains; however, these approaches showed limited effectiveness under constrained datasets, which we discuss later.</p><p>Secondly, we modify the positional encoding function f t:t‚àà{q,k,v} (‚Ä¢), as shown in Fig. <ref type="figure" target="#fig_3">4</ref>. Instead of continuously numbering indices, we allow RGB and alpha tokens to share the same positional encoding. Taking absolute positional encoding as an example:</p><formula xml:id="formula_6">f * z:z‚àà{q,k,v} (x video ) := W z:z‚àà{q,k,v} (x m video + p m ), if m ‚â§ L, W * z:z‚àà{q,k,v} (x m video + p m-L + d), if m &gt; L.<label>(5)</label></formula><p>Here we introduce a domain embedding d, initialized to zero. We make it learnable to help the model adaptively differentiate between RGB (m ‚â§ L) and alpha tokens (m &gt; L). The motivation behind this design is we observe that with same postional encoding, even initializing with different noises, the tokens from two domains tend to generate same results. It minimizes spatial-temporal alignment challenges at the very beginning of training and thus accelerates convergence.</p><p>Next we propose a fine-tuning scheme using LoRA <ref type="bibr" target="#b21">[23]</ref>, in which the LoRA layer is applied only to alpha domain tokens: where Œ≥ controls the residual strength. Additionally, we design an attention mask to block unwanted attention computation. Given a text-video token sequence length L text + 2L, where L text represents text token length, the mask is defined as:</p><formula xml:id="formula_7">W * z:z‚àà{q,k,v} (x m video + p m-L + d) = W z:z‚àà{q,k,v} (x m video + p m-L + d) + Œ≥ ‚Ä¢ LoRA(x m video + p m-L + d), if m &gt; L,<label>(6)</label></formula><formula xml:id="formula_8">M * mn = -‚àû, if m ‚â§ L text and n &gt; L text + L, 0, otherwise.<label>(7)</label></formula><p>Combining these modifications, inference with our method is expressed as:</p><formula xml:id="formula_9">Attention(Q, K, V) = softmax QK T ‚àö d k + M * V,<label>where</label></formula><formula xml:id="formula_10">Z : Z ‚àà {Q, K, V} = [W z:z‚àà{q,k,v} (x text ); f * z:z‚àà{q,k,v} (x video )]<label>(8)</label></formula><p>Training is carried out using flow matching <ref type="bibr" target="#b31">[33]</ref> or a traditional diffusion process <ref type="bibr" target="#b19">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Analysis</head><p>Given our goal of maximizing the inherited capabilities of the pretrained video model, enabling it to generate beyond the existing RGBA training set, we analyze the most critical component within our current 3D full attention DiT video generation model: the attention mechanism. The attention matrix, QK T , has dimensions (L text +2 * L)√ó(L text +2 * L), which we simplify by organizing it into a 3x3 grouped attention matrix-including Text-attend-to-RGB, RGBattend-to-Text, and so forth, as illustrated in Fig. <ref type="figure" target="#fig_2">3</ref>.</p><p>Text-Attend-to-RGB and RGB-Attend-to-Text. These represent the upper-left 2x2 section of and are computations that exist solely in the original RGB generation model. If we ensure that this part of the computation remains unaffected, we can replicate the original RGB generation performance. Therefore, we limit the scope of LoRA's influence, as defined in Eq. ( <ref type="formula" target="#formula_8">7</ref>), by retaining the original QKV values for both text and RGB tokens, thus preserving the pretrained model's behavior in these domains.</p><p>Besides the partial LoRA, the added alpha tokens requires the text and RGB tokens to also act as queries and interact with the alpha tokens as keys, which alters the computation in this 2x2 attention matrix. Therefore, we further analyze two additional attention computations that impact RGB generation, as shown in Fig. <ref type="figure" target="#fig_4">5</ref>.</p><p>Text-Attend-to-Alpha. We find that this attention is detrimental to the generation quality. Since the model was originally trained with text and RGB data, introducing attention from text to alpha causes interference due to the domain gap between alpha and RGB. Specifically, the alpha modality provides only contour information and lacks the rich texture, color, and semantic details associated with the text prompt, thereby degrading generation quality. To mitigate this, we design the attention mask (Eq. ( <ref type="formula" target="#formula_8">7</ref>)) that blocks this computation.</p><p>RGB-Attend-to-Alpha. In contrast, we identify RGB-to-Alpha as essential for successful joint generation. This attention allows the model to refine RGB tokens by considering alpha information, facilitating alignment between generated RGB and alpha channels. This refinement process is a critical component missing in previous generation-thenprediction pipelines, which lacked a feedback mechanism for RGB refinement based on alpha guidance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head><p>Training Dataset. We utilize the public VideoMatte240K dataset <ref type="bibr" target="#b27">[29]</ref>, a comprehensive collection of 484 highresolution green screen videos consists of 240,709 unique frames of alpha mattes and foregrounds. These frames pro-"A coin spinning" "A forest floor being consumed by spreading magical fire" "A cloud of dust erupting and dispersing like an explosion" "An asteroid belt swirling chaotically through space" "A parrot flying" "An astronaut running down an alley, spacesuit flapping" "Water splattering in mid-air" "a woman's long black hair streaming as she runs"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Image</head><p>Generated RGBA Video Input Image Generated RGBA Video vide a diverse range of human subjects, clothing styles, and poses. We apply fundamental preprocessing steps for them, including color decontamination and background blurring. Prompts are extracted using ShareGPT4V <ref type="bibr" target="#b7">[9]</ref>.</p><p>Model. Our RGBA video diffusion models are developed by fine-tuning pre-trained diffusion models. Specifically, we employ two models based on the diffusion transformer architecture: the open-source model CogVideoX <ref type="bibr" target="#b55">[57]</ref> and a modified variant of CogVideoX denoted as J. CogVideoX generates RGB videos at a resolution of 480x720 with 49 frames at 8 FPS, using 50 sampling steps. In contrast, the modified version produces videos at a resolution of 176x320 with 64 frames at 24 FPS, while also using 50 sampling steps. Additionally, we integrate our method with CogVideoX-I2V (Image-to-Video) to support imageto-video generation with transparency. We set the LoRA rank to 128. For domain embedding, we initialize it with an original shape of 1 √ó D and zero values, then expand it to L √ó D through repetition during training. We train these parameters over 5,000 iterations with a batch size of 8 in total, utilizing 8 NVIDIA A100 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Applications</head><p>We mainly demonstrate two applications shown in Fig. <ref type="figure" target="#fig_5">6</ref>:</p><p>Text-to-Video with Transparency. Our method is capable of generating moving objects with various types of motion, such as spinning, running, and flying, while also handling transparent properties of bottles and glasses. Additionally, it can produce complex visual effects, including fire, explosions, cracking, and lightning, as well as creative examples.</p><p>Image-to-Video with Transparency. Our method can also be integrated with an I2V video generation model-CogVideoX-I2V. Users can provide a single image along with an alpha channel (optional), and then we generate subsequent frames with dynamic effects and automatically propagate or generate alpha channels for these frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparisons</head><p>Generation-then-Prediction Pipeline. As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, video matting methods <ref type="bibr" target="#b27">[29,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b57">59</ref>] struggle with matting non-human objects (see supplementary materials for additional results). Therefore, we selected Lotus <ref type="bibr" target="#b17">[19]</ref> and SAM-2 <ref type="bibr" target="#b39">[41]</ref> as baselines due to their stronger generalization: Lotus uses pretrained generative models, and SAM-2 is trained on large datasets. Since Lotus was originally designed for single-image depth estimation, we extended it for RGBA videos, denoted as Lotus + RGBA in our comparisons. Qualitative results are shown in Fig. <ref type="figure" target="#fig_6">7</ref>. Since groundtruth alpha channels are not available for generated videos, we focus on qualitative comparison.</p><p>Joint Generation Pipeline. Since there are currently no existing RGBA video generation models, we integrate Ani-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB SAM-2</head><p>Lotus + RGBA Ours "Turning Head" "Running" "Flickering" "Swaying" mateDiff <ref type="bibr" target="#b14">[16]</ref> with LayerDiffusion <ref type="bibr" target="#b62">[64]</ref> to generate RGBA videos. We use the open-source video generation model CogVideoX <ref type="bibr" target="#b55">[57]</ref> as the base model for fair comparison. The qualitative results are illustrated in Fig. <ref type="figure" target="#fig_7">8</ref>.</p><p>User Study. We also conduct a user study with Amazon Mechanical Turk to compare two joint generation methods, as shown in Table . 1. Participants are asked to evaluate two key aspects: 1) whether the RGB and alpha align correctly; and 2) whether the motion in the generated video matches</p><p>Table 1. User Study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGBA Alignment Motion Quality</head><p>AnimateDiff <ref type="bibr" target="#b14">[16]</ref>+LayerDiff <ref type="bibr" target="#b62">[64]</ref> 6.7% 21.7% Ours + CogVideoX <ref type="bibr" target="#b55">[57]</ref> 93.3% 78.3%</p><p>the corresponding text description. A total of 30 videos are generated from distinct text prompts, and 87 users participated in the evaluation. The study shows that our method is obviously favored more by users with higher votes. As shown in Fig. <ref type="figure" target="#fig_9">10</ref>, we conduct the ablation study across two dimensions: attention rectification and network design. Attention Rectification. By blocking RGB-to-Alpha attention, we first validate the importance of RGB-to-Alpha attention for aligning RGB and alpha channels, a feature lacking in most prediction-based methods. We also examine the effect of removing unnecessary attention to preserve the model's generative capacity, by learning Text-to-Alpha attention only. Without RGB-to-Alpha attention, the alpha channel misaligns with RGB content and the RGB output loses motion quality (e.g., reverse rocket).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><note type="other">DiT Block</note><p>Alternative Designs For Joint Generation. Given the transformer's input dimensions B √ó L √ó D, we extend the sequence dimension L to produce RGB and alpha channels, but alternative extensions are possible at the Batch B or Latent Dimension D levels (see Fig. <ref type="figure" target="#fig_8">9</ref>). In the Batch Extension approach, a new module enables inter-batch communication, similar to the technique in <ref type="bibr" target="#b44">[46]</ref>. For Latent Dimension Extension, we merge video and alpha tokens, project them into the DiT model's latent space, and unmerge post-generation, using learnable linear layers with fine-tuning. Batch Extension shows weaker RGB-alpha alignment, while Latent Dimension Extension, though akin to training from scratch, significantly reduces diversity.</p><p>Evaluation. In addition to the qualitative comparisons shown in Fig. <ref type="figure" target="#fig_9">10</ref>, we also generated a total of 80 videos, each consisting of 64 frames, and evaluated them using two primary metrics: Flow Difference. To measure alignment between the generated RGB and Alpha videos, we use optical flow <ref type="bibr" target="#b20">[22]</ref> to focus on motion consistency while ignoring appearance. Specifically, we calculate optical flow with Farneback method <ref type="bibr" target="#b10">[12]</ref> and compute the flow difference as the average Euclidean distance between RGB and Alpha flow fields. Frech√©t Video Distance (FVD). We use FVD <ref type="bibr" target="#b43">[45]</ref> to compare the RGB videos generated by each RGBA method against those from the original RGB model, evaluating how well each method preserves the model's original generative quality. A lower FVD indicates that the generated results are closer to the original RGB model in terms of motion coherence and diversity, thus demonstrating a high fidelity to the model's intended generative quality. Results are shown in Fig. <ref type="figure" target="#fig_10">11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we present a novel approach for Text-to-RGBA video generation, extending RGB generation models to support RGBA output with minimal modification and high fidelity. By leveraging transformer-based DiT models and optimizing attention mechanisms specific to RGBA generation, our method effectively balances the preservation of RGB quality with the accurate generation of alpha channels. Our approach demonstrates that targeted modifications-such as the addition of alpha tokens, reinitialization of positional embeddings, and selective LoRA fine-tuning-can yield complex and high-quality RGBA outputs even with limited data. Extensive experimental results validate our framework, showing its versatility and robustness across diverse scenarios. Looking forward, we aim to explore further optimizations to reduce computational costs and enhance model scalability.</p><p>TransPixar: Advancing Text-to-Video Generation with Transparency Supplementary Material</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Limitations</head><p>Our DiT-based method for RGBA generation incurs quadratic computational costs due to sequence expansion. However, our method achieves an optimal balance between generation and alignment when trained with a limited dataset. Numerous studies <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b47">49,</ref><ref type="bibr" target="#b64">66]</ref> have addressed the computational overhead of long sequences, with many optimizations reducing complexity to a linear scale. To enhance the efficiency of our method, we plan to incorporate these optimizations in future work. Additionally, our performance is influenced by the generative priors provided by the chosen T2V model, which affects the quality and consistency of our outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Comparisons with Video Matting</head><p>We compare our method with video matting methods Bi-Matting <ref type="bibr" target="#b38">[40]</ref> and Robust Video Matting (RVM) <ref type="bibr" target="#b28">[30]</ref>, as well as the image matting method Matte-Anything <ref type="bibr" target="#b57">[59]</ref>. From the results, it is evident that most methods, trained on the VideoMatte240k <ref type="bibr" target="#b27">[29]</ref> dataset, struggle to produce valid outputs for non-human objects, often resulting in empty results.</p><p>Even image matting methods trained on large-scale datasets fail to handle certain visual effects correctly. Results are shown in the attached HTML source files.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Data Preprocessing</head><p>Color Decontamination. In our method, we preprocess the training data by applying a color decontamination step to enhance the quality of the RGBA video generation.</p><p>Color contamination typically occurs when there is an undesired blending of foreground and background colors, especially along the edges of an object, due to imperfect alpha masks. This blending causes color bleeding, where the foreground and background colors mix, resulting in lower quality RGBA frames with inaccurate color representation.</p><p>To address this issue, we refine the alpha mask using parameters such as gain (Œ≥ = 1.1) and choke (œá = 0.5) to adjust the sharpness and influence of the mask edges. The decontaminated RGB values are then computed as follows:</p><p>RGB decon = RGB√ó(1-mask refined )+mask refined √óBackground</p><p>This equation ensures that unwanted color contamination is minimized, providing a more precise distinction between foreground and background regions. By performing this preprocessing step, we generate high-quality train-ing data that significantly improves the performance of our RGBA video generation model. Background Blurring. Unlike typical training strategies in video matting methods, where objects are composited with complex backgrounds to increase the difficulty of the task, our goal is to support joint generation of alpha and RGB channels while ensuring alignment between them. Instead of emphasizing complex matting, we focus on generating consistent and high-quality output by compositing objects with simple, static backgrounds that match the black areas in the alpha channel. Specifically, we apply a large Gaussian blur kernel of size 201 to the first frame to create a blurred background and blend each subsequent frame with this static background. This approach helps simplify the training conditions, allowing the model to better align the RGB and alpha components while maintaining high-quality output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Optical Flow Difference</head><p>To evaluate the alignment between the RGB and alpha channels in generated videos, we introduce a metric based on optical flow difference. Optical flow measures the apparent motion of objects between consecutive frames, and comparing the optical flow fields of RGB and alpha frames provides insight into the consistency of motion across these modalities. Specifically, we use the Farneback method (cv::calcOpticalFlowFarneback) to compute the optical flow for both RGB and alpha frames, and then calculate the average Euclidean distance between their flow vectors as a measure of misalignment. This approach quantifies the degree to which the RGB and alpha channels align in terms of motion. Pseudo Code Overview: 1. Load consecutive RGB and alpha frames from the input video. 2. Convert the frames to grayscale for optical flow computation, as optical flow is typically calculated on intensity values. 3. Compute optical flow using the Farneback method (cv::calcOpticalFlowFarneback) for both the RGB and alpha frames. 4. Calculate the Euclidean distance between the RGB and alpha flow vectors for each pixel. 5. Average the differences across all pixels and frames to obtain the final optical flow difference. The average optical flow difference provides a quantitative metric for evaluating the alignment between RGB and alpha channels, helping to ensure that both modalities ex-hibit consistent motion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Video Results</head><p>For all video results shown in the main paper, please see the attached HTML source files.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">Additional Visual Results</head><p>In addition to the video results in the main paper, we provide more generated results in the supplementary files, including various objects and visual effects. Please find the corresponding results in the supplementary files.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>"</head><figDesc>A dust cloud expanding after an explosion, covering the area"(c) with generative prior (MariGold, Lotus) (d) Ours (b) w/o generative prior (Video Matting) (a) Generated RGB</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Comparison between Generation-Then-Prediction and our Joint Generation approach. Given the generated RGB in (a), (b) and (c) show the predicted alpha (top) and the composited result (bottom). In (d), the top shows the jointly generated alpha.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Pipeline of TransPixar. Our method is organized as follows: (1) Left: we extend the input of DiT to include new alpha tokens; (2) Top Center: we initialize alpha tokens with our positional encoding; (3) Bottom Center: we insert a partial LoRA and adjust attention computation during training and inference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Positional Encoding Design for RGBA Generation.Assigning alpha tokens the same positional encoding as RGB yields similar results, resulting in faster convergence after 1000 iterations compared to standard encoding strategies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Attention Rectification. (a) Eliminating all attention from alpha as a key preserves 100% RGB generation but leads to poor alignment. (b) Retaining all attention significantly degrades quality, causing a lack of motion in bicycles. (c) Our method achieves an effective balance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Applications. Top: Text-to-Video with Transparency. Bottom: Image-to-Video generation with transparency. .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Comparison with Generation-then-Prediction Pipelines. Our method demonstrates superior alignment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Comparison with Joint Generation Pipelines. Top: LayerDiffusion + AnimateDiff; Bottom: Ours. Our method achieves better alignment and generates corresponding motion described by prompts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Alternative Designs for Joint Generation with DiT. Sequence extension (b) represents our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>"Figure 10 .</head><label>10</label><figDesc>Figure 10. Ablation Study. (a) Ours; (b) Ours without RGBattend-to-Alpha; (c) Ours with Text-attend-to-alpha; (d) Batch Extension Strategy; (e) Latent Dimension Extension Strategy. Our method maintains high-quality motion generation (e.g., butterflies waving their wings) while achieving good alignment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Quantitative Evaluation. Our approach achieves a good balance between alignment (low flow difference) and preserving generative quality (low FVD).</figDesc><graphic coords="8,325.44,69.33,216.92,107.28" type="bitmap" /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">One transformer fits all distributions in multi-modal diffusion at scale</title>
		<author>
			<persName><forename type="first">Fan</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shen</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongxuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaole</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<idno>PMLR, 2023. 3</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="1692" to="1717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Stable video diffusion: Scaling latent video diffusion models to large datasets</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dockhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumith</forename><surname>Kulal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Mendelevitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maciej</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yam</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zion</forename><surname>English</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikram</forename><surname>Voleti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Letts</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.15127</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Video generation models as world simulators</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Depue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Schnurr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Troy</forename><surname>Luhman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Luhman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clarence</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricky</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Guowei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juncai</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuying</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lutao</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zewu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiliang</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.09433</idno>
		<title level="m">Pp-matting: high-accuracy natural image matting</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Videocrafter1: Open diffusion models for high-quality video generation</title>
		<author>
			<persName><forename type="first">Haoxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menghan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingqing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoshu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinbo</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaofang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.19512</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Videocrafter2: Overcoming data limitations for high-quality video diffusion models</title>
		<author>
			<persName><forename type="first">Haoxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menghan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.09047</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis</title>
		<author>
			<persName><forename type="first">Junsong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jincheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongjian</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongdao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.00426</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Lin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jisong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Conghui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.12793</idno>
		<title level="m">Sharegpt4v: Improving large multi-modal models with better captions</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Control-a-video: Controllable text-to-video generation with diffusion models</title>
		<author>
			<persName><forename type="first">Weifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yatai</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hefeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuefeng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.13840</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Jiayu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.02486</idno>
		<title level="m">Longnet: Scaling transformers to 1,000,000,000 tokens 2023</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Two-frame motion estimation based on polynomial expansion</title>
		<author>
			<persName><forename type="first">Gunnar</forename><surname>Farneb√§ck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Scandinavian Conference on Image Analysis (SCIA)</title>
		<meeting>the Scandinavian Conference on Image Analysis (SCIA)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Michal</forename><surname>Geyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Bar-Tal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shai</forename><surname>Bagon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tali</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName><surname>Tokenflow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.10373</idno>
		<title level="m">Consistent diffusion features for consistent video editing</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Jianzhu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingyun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhizhou</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.03168</idno>
		<title level="m">Liveportrait: Efficient portrait animation with stitching and retargeting control</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">I2v-adapter: A general imageto-video adapter for video diffusion models</title>
		<author>
			<persName><forename type="first">Xun</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingwu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongyang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengjun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Wan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.16693</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Animatediff: Animate your personalized text-to-image diffusion models without specific tuning</title>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anyi</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaohui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.04725</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Lucidfusion: Generating 3d gaussians with arbitrary unposed images</title>
		<author>
			<persName><forename type="first">Yixun</forename><surname>Hao He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luozhou</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanhao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinli</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao-Xiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingcong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Cameractrl: Enabling camera control for text-to-video generation</title>
		<author>
			<persName><forename type="first">Yinghao</forename><surname>Hao He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gordon</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wetzstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsheng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ceyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.02101</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Jing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haodong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiqiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongbo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingbing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying-Cong</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.18124</idno>
		<title level="m">Diffusion-based visual foundation model for high-quality dense prediction</title>
		<meeting><address><addrLine>Lotus</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Latent video diffusion models for high-fidelity video generation with arbitrary lengths</title>
		<author>
			<persName><forename type="first">Yingqing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.13221</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Determining optical flow</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Berthold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">G</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><surname>Schunck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="185" to="203" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shean</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Lora</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09685</idno>
		<title level="m">Low-rank adaptation of large language models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Dreammotion: Space-time self-similarity score distillation for zero-shot video editing</title>
		<author>
			<persName><forename type="first">Hyeonho</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinho</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geon Yeong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><surname>Chul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2403.12002</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Repurposing diffusion-based image generators for monocular depth estimation</title>
		<author>
			<persName><forename type="first">Bingxin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Obukhov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nando</forename><surname>Metzger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodrigo</forename><forename type="middle">Caye</forename><surname>Daudt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Open-sora-plan</title>
		<author>
			<persName><forename type="first">Pku-Yuan</forename><surname>Lab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Tuzhan</surname></persName>
		</author>
		<author>
			<persName><surname>Etc</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024. 1, 2, 3</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Matting anything</title>
		<author>
			<persName><forename type="first">Jiachen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitesh</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Humphrey</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="1775" to="1785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Omnimatterf: Robust omnimatte with 3d background modeling</title>
		<author>
			<persName><forename type="first">Geng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changil</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yipeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Zwicker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayush</forename><surname>Saraf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Real-time high-resolution background matting</title>
		<author>
			<persName><forename type="first">Shanchuan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Ryabtsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumyadip</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">L</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ira</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021. 2, 3, 5, 6, 1</date>
			<biblScope unit="page" from="8762" to="8771" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Robust high-resolution video matting with temporal guidance</title>
		<author>
			<persName><forename type="first">Shanchuan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Imran</forename><surname>Saleemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumyadip</forename><surname>Sengupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Pengyang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiazi</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhang</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.05338</idno>
		<title level="m">Motionclone: Training-free motion cloning for controllable video generation</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Video-p2p: Video editing with cross-attention control</title>
		<author>
			<persName><forename type="first">Shaoteng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuechen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="8599" to="8608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Flow straight and fast: Learning to generate and transfer data with rectified flow</title>
		<author>
			<persName><forename type="first">Xingchao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengyue</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.03003</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Wonder3d: Single image to 3d using cross-domain diffusion</title>
		<author>
			<persName><forename type="first">Xiaoxiao</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><surname>Yuan-Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingjie</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuexin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song-Hai</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Habermann</surname></persName>
		</author>
		<author>
			<persName><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="9970" to="9980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Intrinsicdiffusion: Joint intrinsic layers from latent diffusion models</title>
		<author>
			<persName><forename type="first">Jundan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duygu</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jae</forename><surname>Shin Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanxuan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Fr√ºhst√ºck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuanfeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIG-GRAPH 2024 Conference Papers</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Trailblazer: Trajectory control for diffusion-based video generation</title>
		<author>
			<persName><forename type="first">Wan-Duo Kurt</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bastiaan Kleijn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.00896</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Latte: Latent diffusion transformer for video generation</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaohui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gengyun</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan-Fang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cunjian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.03048</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Mofa-video: Controllable image animation via generative motion field adaptions in frozen image-to-video diffusion model</title>
		<author>
			<persName><forename type="first">Muyao</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinqiang</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.20222</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fatezero: Fusing attentions for zero-shot text-based video editing</title>
		<author>
			<persName><forename type="first">Chenyang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyang</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="15932" to="15942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Bimatting: Efficient video matting via binarization</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Haotong Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xudong</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Wing</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi-Keung</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianglong</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fisher</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Nikhila</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Gabeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan-Ting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaitanya</forename><surname>Ryali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haitham</forename><surname>Khedr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>R√§dle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chloe</forename><surname>Rolland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Gustafson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Mintun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junting</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalyan</forename><surname>Vasudev Alwala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Doll√°r</surname></persName>
		</author>
		<author>
			<persName><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><surname>Sam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.00714</idno>
		<title level="m">Segment anything in images and videos</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bj√∂rn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Roformer: Enhanced transformer with rotary position embedding</title>
		<author>
			<persName><forename type="first">Jianlin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murtadha</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengfeng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfeng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">568</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">127063</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Genmo</forename><surname>Team</surname></persName>
		</author>
		<author>
			<persName><surname>Mochi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2024. 3, 4</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Fvd: A new metric for video generation</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karol</forename><surname>Sjoerd Van Steenkiste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rapha√´l</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Marinier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName><surname>Gelly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Dante De Nigris, Ciara Rowles, Nicolas Perony, and Simon Donn√©. Collaborative control for geometry-conditioned pbr image generation</title>
		<author>
			<persName><forename type="first">Shimon</forename><surname>Vainer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Boss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Parger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Kutsy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.05919</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Jiuniu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hangjie</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dayou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiwei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.06571</idno>
		<title level="m">Modelscope text-to-video technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">Luozhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guibao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingcong</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.20193</idno>
		<title level="m">Motion inversion for video customization</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Linformer: Self-attention with linear complexity</title>
		<author>
			<persName><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Belinda</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04768</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Compositional video synthesis with motion controllability</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hangjie</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dayou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiuniu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><surname>Videocomposer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Matting by generation</title>
		<author>
			<persName><forename type="first">Zhixiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Lun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinwei</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yung-Yu</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shin'ichi</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2024 Conference Papers</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Motionctrl: A unified and flexible motion controller for video generation</title>
		<author>
			<persName><forename type="first">Zhouxia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyang</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaowei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianshui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menghan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2024 Conference Papers</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation</title>
		<author>
			<persName><forename type="first">Jay Zhangjie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><forename type="middle">Weixian</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wynne</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohu</forename><surname>Qie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><forename type="middle">Zheng</forename><surname>Shou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="7623" to="7633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Depth anything: Unleashing the power of large-scale unlabeled data</title>
		<author>
			<persName><forename type="first">Lihe</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="10371" to="10381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Defect spectrum: A granular look of large-scale defect datasets with rich semantics</title>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengguang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingcong</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Rerender a video: Zero-shot text-guided video-to-video translation</title>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH Asia 2023 Conference Papers</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Cogvideox: Text-to-video diffusion models with an expert transformer</title>
		<author>
			<persName><forename type="first">Zhuoyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayan</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wendi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiazheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyi</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanyu</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.06072</idno>
		<imprint>
			<date type="published" when="2007">2024. 1, 2, 3, 4, 6, 7</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Vitmatte: Boosting image matting with pretrained plain vision transformers</title>
		<author>
			<persName><forename type="first">Jingfeng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shusheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baoyuan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">102091</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Matte anything: Interactive natural image matting with segment anything model</title>
		<author>
			<persName><forename type="first">Jingfeng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">147</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Dragnuwa: Fine-grained control in video generation by integrating text, image, and trajectory</title>
		<author>
			<persName><forename type="first">Shengming</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gong</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.08089</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Rgb‚Üîx: Image decomposition and synthesis using material-and lighting-aware diffusion models</title>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Deschaintre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iliyan</forename><surname>Georgiev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannick</forename><surname>Hold-Geoffroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiwei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fujun</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling-Qi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milo≈°</forename><surname>Ha≈°an</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2024 Conference Papers</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Show-1: Marrying pixel and latent diffusion models for text-to-video generation</title>
		<author>
			<persName><forename type="first">David Junhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><forename type="middle">Zhangjie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia-Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingmin</forename><surname>Ran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Difei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><forename type="middle">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.15818</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Moonshot: Towards controllable video generation and editing with multimodal conditions</title>
		<author>
			<persName><forename type="first">David Junhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><forename type="middle">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doyen</forename><surname>Sahoo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.01827</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Transparent image layer diffusion using latent transparency</title>
		<author>
			<persName><forename type="first">Lvmin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maneesh</forename><surname>Agrawala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.17113</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Open-sora: Democratizing efficient video production for all</title>
		<author>
			<persName><forename type="first">Zangwei</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianji</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenhui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenggui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Long-short transformer: Efficient transformers for language and vision</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaowei</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="17723" to="17736" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
