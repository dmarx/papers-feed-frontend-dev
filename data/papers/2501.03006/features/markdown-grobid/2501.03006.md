# TransPixar: Advancing Text-to-Video Generation with Transparency

## Abstract

## 

A statue crumbling to dust as cracks spread across its surface" "A squirrel's bushy tail flicking quickly" "A portal crackling with arcane magic as it opens" "A small explosion rapidly expanding and contracting" "A massive storm forming, with swirling clouds and thunderbolts" "A motorcycle drifting and swerving in an enchanted forest" "A white dandelion shifting as seen through a magnifying glass" "A transparent glass of water with ice cubes gently swirling" Figure 1. RGBA Video Generation with TransPixar. By introducing LoRA layers into DiT-based text-to-video model with a novel alpha channel adaptive attention mechanism, our method enables RGBA video generation from text while preserving Text-to-Video quality.

## Introduction

Text-to-Video generative models have quickly advanced, achieving impressive results [[6,](#b4)[16,](#b14)[20,](#b18)[26,](#b24)[47,](#b45)[50,](#b48)[57,](#b55)[62,](#b60)[65]](#b63). This progress has enabled various applications, such as video editing [[10,](#b8)[13,](#b11)[32,](#b30)[39,](#b37)[53,](#b51)[56]](#b54), image animation [[2,](#b1)[14,](#b12)[15,](#b13)[38]](#b36), and motion customization [[18,](#b16)[24,](#b22)[31,](#b29)[36,](#b34)[48,](#b46)[52,](#b50)[60]](#b58). Diffusion Transformers (DiT) enhance these models by using self-attention to capture long-range dependencies [[3,](#b2)[26,](#b24)[57,](#b55)[65]](#b63). These models are now widely used in entertainment, advertising, and education, meeting the demand for customizable, dynamic content. Notably, Text-to-RGBA (A denotes Alpha channel) video generation is invaluable for VFX and creative industries. The inclusion of an alpha channel in RGBA formats allows for transparent effects, enabling seamless blending of elements like smoke and reflections (see Fig. [1](#)). This transparency creates realistic visuals that can integrate smoothly into scenes without modifying the background. Such flexibility is crucial in gaming, virtual reality (VR), and augmented reality (AR), where dynamic and interactive content is in high demand.

Currently, no direct solutions exist for RGBA video generation, which remains a challenging task due to the scarcity of RGBA video data, with only around 484 videos available in [[29]](#b27). This scarcity will significantly limit the diversity of generated content, resulting in a constrained set of object types and motion patterns. One feasible solution is to use video matting [[28,](#b26)[30,](#b28)[40]](#b38) to obtain alpha channels from generated videos. However, these methods are still limited by the scarcity of RGBA video data and struggle to generalize to a wider range of objects, as shown in Fig. [2 (b)](#fig_1). Other video segmentation methods, such as SAM-2 [[41]](#b39), may generalize well to different tasks. However, they cannot generate alpha channels and are therefore unsuitable for direct compositing. There have been attempts to generate RGBA at the image level, such as LayerDiffusion [[64]](#b62). However, adapting its concept directly to a temporal VAE used in video generative models remains challenging.

In this paper, we explore how to extend pretrained video models to generate corresponding alpha channels while retaining the original capabilities of pretrained models. Our goal is to generate content beyond the limitations of the current RGBA training set. Existing works such as Lotus [[19]](#b17) and Marigold [[25]](#b23) demonstrate that leveraging pretrained generation model weights significantly enhances out-of-distribution in dense prediction, hinting at the potential for predicting alpha channels. However, in the context of RGBA video generation, these approaches typically require generating RGB channels first, followed by separate alpha channel prediction. Consequently, information flows unidirectionally from RGB to alpha, keeping the two processes largely disconnected. Given the limited availability of RGBA video data, this imbalance results in insufficient alpha prediction when challenging objects are generated, as shown in Fig. [2 (c)](#fig_1).

In this work, we propose TransPixar, which effectively adapts the pretrained RGB video models to generate RGB channels and the alpha channel simultaneously. We leverage state-of-the-art DiT-like video generation models [[26,](#b24)[57]](#b55) , and additionally introduce new tokens appended after text and RGB tokens for generating the alpha channels. To facilitate convergence, we reinitialize the positional embeddings for the alpha tokens and introduce a zero-initialized, learnable domain embedding to distinguish alpha tokens from RGB tokens. Furthermore, we employ a LoRA-based fine-tuning scheme [[23]](#b21), applied exclusively to project alpha tokens into the qkv space, thereby maintaining RGB generation quality. With the proposed approach, we extend the modality while preserving the original input-  output structure and relying on the existing attention mechanism through LoRA adaptation.

The extended sequence contains text, RGB, and alpha tokens, with self-attention divided into a 3x3 grouped attention matrix involving interactions like Text-attend-to-RGB (Text as query, RGB as key) and others. We also systematically analyze the attention mechanisms for RGBA generation: 1) Text-attend-to-RGB and RGB-attend-to-Text. The interaction between text and RGB tokens represents original model's generation capabilities. Minimizing the impact on text and RGB tokens during these attention computation processes can better retain the original model's performance; 2) RGB-attend-to-Alpha. We reveals a fundamental limitation in conventional methods is the lack of RGB-attend-to-Alpha attention. This attention is necessary to refine RGB tokens based on alpha information, improving RGB-alpha alignment; 3) Text-attend-to-Alpha. We remove this attention mechanism to reduce the risk caused by limited training data, which could degrade the model's performance. This removal also enhances the retention of the model's original capabilities.

By integrating these techniques, our method achieves diverse RGBA generation with limited training data while maintaining strong RGB-alpha alignment. To summarize, our contributions are as follows:

â€¢ We propose an RGBA video generation framework using DiT models that requires limited data and training parameters, achieving diverse generation with strong alignment. â€¢ We analyze the role of each attention component in the generation process, optimize their interactions, and introduce necessary modifications to improve RGBA generation quality. â€¢ Our method is validated through extensive experiments, demonstrating its effectiveness across a variety of challenging scenarios.

## Related Work

Text-to-Video Generation. Early video generation models were primarily based on Unet-based latent diffusion models (LDMs) extended from text-to-image models like Stable Diffusion [[42]](#b40). For example, AnimateDiff [[16]](#b14) introduced a temporal attention module to improve temporal consistency across frames. Subsequent video generation models [4, [6,](#b4)[7,](#b5)[47,](#b45)[62,](#b60)[63]](#b61) adopted an alternating approach with 2D spatial and 1D temporal attention, including works like ModelScope, VideoCrafter, Moonshot, and Show-1.

With advancements in large language models (LLMs) and the introduction of Sora [[3]](#b2), attention shifted from Unet architectures to transformer-based architectures (DiT). DiT-based video generation models, such as Latte [[37]](#b35) and OpenSora [[65]](#b63), extended the DiT text-to-image (T2I) model [[8]](#b6) and maintained the 2D and 1D alternating attention approach, achieving promising results. Recently, DiTbased video generation has rapidly progressed, achieving further improvements in quality. Several methods [[26,](#b24)[44,](#b42)[57]](#b55) have moved away from the 2D and 1D alternating approach, instead treating video frames as a single long sequence with 3D positional embeddings for encoding. These approaches also prepend text tokens-processed through a text encoder-to the video sequence, creating a streamlined network that relies solely on full self-attention and feedforward layers. Our method builds upon these recent opensource transformer-based video generation models.

Video Matting. A straightforward approach for RGBA video generation is to extract the alpha channel from generated RGB content, as done with traditional green screen keying or learning-based video matting expert models [[28]](#b26)[[29]](#b27)[[30]](#b28). OmnimatteRF [[28]](#b26) introduces a video matting method that combines dynamic 2D foreground layers with a 3D background model, enabling more realistic scene reconstruction for real-world videos. Robust Video Matting (RVM) [[30]](#b28) proposes a real-time, high-quality human video matting method with a recurrent architecture for improved temporal coherence, achieving state-of-the-art results without auxiliary inputs. Another work presents a high-speed, high-resolution background replacement technique with precise alpha matte extraction, supported by the Video-Matte240K and PhotoMatte13K/85 datasets [[29]](#b27). Additionally, many image matting methods [[5,](#b3)[27,](#b25)[51,](#b49)[58]](#b56) can be applied for frame-by-frame matting.

Further, several works [[19,](#b17)[25,](#b23)[54]](#b52) in image depth estimation adapt pretrained generation models for prediction tasks, achieving strong performance that often surpasses traditional, scratch-trained expert models. Marigold [[25]](#b23) modifies architectures to create image-conditioned generation models, while Lotus [[19]](#b17) explores the role of the diffusion process in this context. Although there is currently no dedicated approach for video matting within video gen-eration models, we replicate and extend these methods to evaluate their performance, allowing us to highlight the limitations of prediction-based pipelines for RGBA generation. [[1,](#b0)[17,](#b15)[34,](#b32)[35,](#b33)[55,](#b53)[61,](#b59)[64]](#b62) explores expanding generation models to simultaneously generate additional channels, though they are not specifically designed for RGBA video generation. For instance, LayerDiffusion [[64]](#b62) modifies the VAE in latent diffusion models to decode alpha channels. However, VAEs typically lack the semantic understanding required for precise alpha generation, limiting their effectiveness in complex visual scenarios where texture and contour details are critical. In contrast, other approaches [[1,](#b0)[34,](#b32)[35,](#b33)[61]](#b59) modify the denoising model directly to enable joint generation. Wonder3D [[34]](#b32) uses a domain embedding to control the model's generation modality, while methods like Intrin-sicDiffusion [[35]](#b33) and RGBâ†”X [[61]](#b59) adapt the UNet's input and output layers to jointly produce intrinsic modalities. However, all these methods are designed for image tasks and rely on UNet architectures. When applied to video generation, they face limitations in quality and diversity due to the scarcity of RGBA video data.

## Generation beyond RGB. Another category of methods

## Method

## Preliminary

We first introduce the open-sourced state-of-the-art DiTbased video generation models [[44,](#b42)[57]](#b55). The core components of DiT-based video models are attention modules, and there are two primary distinctions between these models and previous approaches. On one hand, unlike previous models that alternate between 1D temporal attention and 2D spatial attention [[4,](#)[6,](#b4)[7,](#b5)[65]](#b63), current methods typically employ 3D spatio-temporal attention, allowing them to capture spatio-temporal dependencies more effectively. On the other hand, instead of using cross-attention for text conditioning, these models concatenate text tokens x text with visual tokens x video into a single long sequence. The shape of video tokens and text tokens are BÃ—LÃ—D and BÃ—L text Ã—D, wher B equals to batch size, L text equals to the length of text tokens, L equals to the length of video tokens and D equals to the latent dimension of transformer. Full self-attention is then applied across the combined sequence:

$Attention(Q, K, V) = softmax QK T âˆš d k V,where$$Z : Z âˆˆ {Q, K, V} = [W z:zâˆˆ{q,k,v} (x text ); f z:zâˆˆ{q,k,v} (x video )](1$) Here W t (for t âˆˆ {q, k, v}) represents the projection matrixs in the transformer model, and f t (for t âˆˆ {q, k, v}) represents a combined operation that incorporates both the projection and positional encoding for visual tokens. There DiT Block DiT Block Self-Attention Feed Forward â€¦ Text Tokens QKV Proj. Key Text RGB Alpha Query Text RGB Alpha LoRA Attention Rectification Copy Positional Embedding Domain Embedding RGB Tokens Alpha Tokens Frozen Learnable â€¦ Positional Encoding Legends ð‘§ð‘§ð‘§ð‘§ð‘§ð‘§ð‘§ð‘§ ð‘–ð‘–ð‘–ð‘–ð‘–ð‘–ð‘–ð‘–. are two commonly used types of positional encoding. One is absolute positional encoding formulated as follows:

$f z:zâˆˆ{q,k,v} (x video ) := W z:zâˆˆ{q,k,v} (x m video + p m ), (2$$)$where p is the positional embedding (e.g., a sinusoidal function) and m denotes the position of each RGB video token. Another approach is the Rotary Position Embedding (RoPE) [[43]](#b41), often used by [[44,](#b42)[57]](#b55). This is expressed as

$f z:zâˆˆ{q,k} (x video ) := W z:zâˆˆ{q,k} (x m video ) â€¢ e imÎ¸ ,(3)$where m is the positional index, i is the imaginary unit for rotation, and Î¸ is the rotation angle.

## Our Approach

To jointly generate RGB and alpha videos, we adapt a pretrained RGB video generation model through several modifications. The whole pipeline is visualized in Fig. [3](#fig_2). Firstly, we double the sequence length of noisy input tokens to enable the model to generate videos of double length, from x 1:L video to x 1:2 * L video . Here, x 1:L video will be decoded into the RGB video, while x L+1:2 * L video will be decoded into the corresponding alpha video. The Query(Q), Key(K), Value(V) representations are formulated as:

$Z : Z âˆˆ {Q, K, V} = [W z:zâˆˆ{q,k,v} (x text ); f z:zâˆˆ{q,k,v} (x 1:2 * L video )](4)$In addition to sequence doubling, we explored increasing batch size or latent dimensions and splitting output into two domains; however, these approaches showed limited effectiveness under constrained datasets, which we discuss later.

Secondly, we modify the positional encoding function f t:tâˆˆ{q,k,v} (â€¢), as shown in Fig. [4](#fig_3). Instead of continuously numbering indices, we allow RGB and alpha tokens to share the same positional encoding. Taking absolute positional encoding as an example:

$f * z:zâˆˆ{q,k,v} (x video ) := W z:zâˆˆ{q,k,v} (x m video + p m ), if m â‰¤ L, W * z:zâˆˆ{q,k,v} (x m video + p m-L + d), if m > L.(5)$Here we introduce a domain embedding d, initialized to zero. We make it learnable to help the model adaptively differentiate between RGB (m â‰¤ L) and alpha tokens (m > L). The motivation behind this design is we observe that with same postional encoding, even initializing with different noises, the tokens from two domains tend to generate same results. It minimizes spatial-temporal alignment challenges at the very beginning of training and thus accelerates convergence.

Next we propose a fine-tuning scheme using LoRA [[23]](#b21), in which the LoRA layer is applied only to alpha domain tokens: where Î³ controls the residual strength. Additionally, we design an attention mask to block unwanted attention computation. Given a text-video token sequence length L text + 2L, where L text represents text token length, the mask is defined as:

$W * z:zâˆˆ{q,k,v} (x m video + p m-L + d) = W z:zâˆˆ{q,k,v} (x m video + p m-L + d) + Î³ â€¢ LoRA(x m video + p m-L + d), if m > L,(6)$$M * mn = -âˆž, if m â‰¤ L text and n > L text + L, 0, otherwise.(7)$Combining these modifications, inference with our method is expressed as:

$Attention(Q, K, V) = softmax QK T âˆš d k + M * V,where$$Z : Z âˆˆ {Q, K, V} = [W z:zâˆˆ{q,k,v} (x text ); f * z:zâˆˆ{q,k,v} (x video )](8)$Training is carried out using flow matching [[33]](#b31) or a traditional diffusion process [[21]](#b19).

## Analysis

Given our goal of maximizing the inherited capabilities of the pretrained video model, enabling it to generate beyond the existing RGBA training set, we analyze the most critical component within our current 3D full attention DiT video generation model: the attention mechanism. The attention matrix, QK T , has dimensions (L text +2 * L)Ã—(L text +2 * L), which we simplify by organizing it into a 3x3 grouped attention matrix-including Text-attend-to-RGB, RGBattend-to-Text, and so forth, as illustrated in Fig. [3](#fig_2).

Text-Attend-to-RGB and RGB-Attend-to-Text. These represent the upper-left 2x2 section of and are computations that exist solely in the original RGB generation model. If we ensure that this part of the computation remains unaffected, we can replicate the original RGB generation performance. Therefore, we limit the scope of LoRA's influence, as defined in Eq. ( [7](#formula_8)), by retaining the original QKV values for both text and RGB tokens, thus preserving the pretrained model's behavior in these domains.

Besides the partial LoRA, the added alpha tokens requires the text and RGB tokens to also act as queries and interact with the alpha tokens as keys, which alters the computation in this 2x2 attention matrix. Therefore, we further analyze two additional attention computations that impact RGB generation, as shown in Fig. [5](#fig_4).

Text-Attend-to-Alpha. We find that this attention is detrimental to the generation quality. Since the model was originally trained with text and RGB data, introducing attention from text to alpha causes interference due to the domain gap between alpha and RGB. Specifically, the alpha modality provides only contour information and lacks the rich texture, color, and semantic details associated with the text prompt, thereby degrading generation quality. To mitigate this, we design the attention mask (Eq. ( [7](#formula_8))) that blocks this computation.

RGB-Attend-to-Alpha. In contrast, we identify RGB-to-Alpha as essential for successful joint generation. This attention allows the model to refine RGB tokens by considering alpha information, facilitating alignment between generated RGB and alpha channels. This refinement process is a critical component missing in previous generation-thenprediction pipelines, which lacked a feedback mechanism for RGB refinement based on alpha guidance.

## Experiment

Training Dataset. We utilize the public VideoMatte240K dataset [[29]](#b27), a comprehensive collection of 484 highresolution green screen videos consists of 240,709 unique frames of alpha mattes and foregrounds. These frames pro-"A coin spinning" "A forest floor being consumed by spreading magical fire" "A cloud of dust erupting and dispersing like an explosion" "An asteroid belt swirling chaotically through space" "A parrot flying" "An astronaut running down an alley, spacesuit flapping" "Water splattering in mid-air" "a woman's long black hair streaming as she runs"

## Input Image

Generated RGBA Video Input Image Generated RGBA Video vide a diverse range of human subjects, clothing styles, and poses. We apply fundamental preprocessing steps for them, including color decontamination and background blurring. Prompts are extracted using ShareGPT4V [[9]](#b7).

Model. Our RGBA video diffusion models are developed by fine-tuning pre-trained diffusion models. Specifically, we employ two models based on the diffusion transformer architecture: the open-source model CogVideoX [[57]](#b55) and a modified variant of CogVideoX denoted as J. CogVideoX generates RGB videos at a resolution of 480x720 with 49 frames at 8 FPS, using 50 sampling steps. In contrast, the modified version produces videos at a resolution of 176x320 with 64 frames at 24 FPS, while also using 50 sampling steps. Additionally, we integrate our method with CogVideoX-I2V (Image-to-Video) to support imageto-video generation with transparency. We set the LoRA rank to 128. For domain embedding, we initialize it with an original shape of 1 Ã— D and zero values, then expand it to L Ã— D through repetition during training. We train these parameters over 5,000 iterations with a batch size of 8 in total, utilizing 8 NVIDIA A100 GPUs.

## Applications

We mainly demonstrate two applications shown in Fig. [6](#fig_5):

Text-to-Video with Transparency. Our method is capable of generating moving objects with various types of motion, such as spinning, running, and flying, while also handling transparent properties of bottles and glasses. Additionally, it can produce complex visual effects, including fire, explosions, cracking, and lightning, as well as creative examples.

Image-to-Video with Transparency. Our method can also be integrated with an I2V video generation model-CogVideoX-I2V. Users can provide a single image along with an alpha channel (optional), and then we generate subsequent frames with dynamic effects and automatically propagate or generate alpha channels for these frames.

## Comparisons

Generation-then-Prediction Pipeline. As shown in Fig. [2](#fig_1), video matting methods [[29,](#b27)[40,](#b38)[59](#b57)] struggle with matting non-human objects (see supplementary materials for additional results). Therefore, we selected Lotus [[19]](#b17) and SAM-2 [[41]](#b39) as baselines due to their stronger generalization: Lotus uses pretrained generative models, and SAM-2 is trained on large datasets. Since Lotus was originally designed for single-image depth estimation, we extended it for RGBA videos, denoted as Lotus + RGBA in our comparisons. Qualitative results are shown in Fig. [7](#fig_6). Since groundtruth alpha channels are not available for generated videos, we focus on qualitative comparison.

Joint Generation Pipeline. Since there are currently no existing RGBA video generation models, we integrate Ani-

## RGB SAM-2

Lotus + RGBA Ours "Turning Head" "Running" "Flickering" "Swaying" mateDiff [[16]](#b14) with LayerDiffusion [[64]](#b62) to generate RGBA videos. We use the open-source video generation model CogVideoX [[57]](#b55) as the base model for fair comparison. The qualitative results are illustrated in Fig. [8](#fig_7).

User Study. We also conduct a user study with Amazon Mechanical Turk to compare two joint generation methods, as shown in Table . 1. Participants are asked to evaluate two key aspects: 1) whether the RGB and alpha align correctly; and 2) whether the motion in the generated video matches

Table 1. User Study.

## RGBA Alignment Motion Quality

AnimateDiff [[16]](#b14)+LayerDiff [[64]](#b62) 6.7% 21.7% Ours + CogVideoX [[57]](#b55) 93.3% 78.3%

the corresponding text description. A total of 30 videos are generated from distinct text prompts, and 87 users participated in the evaluation. The study shows that our method is obviously favored more by users with higher votes. As shown in Fig. [10](#fig_9), we conduct the ablation study across two dimensions: attention rectification and network design. Attention Rectification. By blocking RGB-to-Alpha attention, we first validate the importance of RGB-to-Alpha attention for aligning RGB and alpha channels, a feature lacking in most prediction-based methods. We also examine the effect of removing unnecessary attention to preserve the model's generative capacity, by learning Text-to-Alpha attention only. Without RGB-to-Alpha attention, the alpha channel misaligns with RGB content and the RGB output loses motion quality (e.g., reverse rocket).

## Ablation Study

DiT BlockAlternative Designs For Joint Generation. Given the transformer's input dimensions B Ã— L Ã— D, we extend the sequence dimension L to produce RGB and alpha channels, but alternative extensions are possible at the Batch B or Latent Dimension D levels (see Fig. [9](#fig_8)). In the Batch Extension approach, a new module enables inter-batch communication, similar to the technique in [[46]](#b44). For Latent Dimension Extension, we merge video and alpha tokens, project them into the DiT model's latent space, and unmerge post-generation, using learnable linear layers with fine-tuning. Batch Extension shows weaker RGB-alpha alignment, while Latent Dimension Extension, though akin to training from scratch, significantly reduces diversity.

Evaluation. In addition to the qualitative comparisons shown in Fig. [10](#fig_9), we also generated a total of 80 videos, each consisting of 64 frames, and evaluated them using two primary metrics: Flow Difference. To measure alignment between the generated RGB and Alpha videos, we use optical flow [[22]](#b20) to focus on motion consistency while ignoring appearance. Specifically, we calculate optical flow with Farneback method [[12]](#b10) and compute the flow difference as the average Euclidean distance between RGB and Alpha flow fields. FrechÃ©t Video Distance (FVD). We use FVD [[45]](#b43) to compare the RGB videos generated by each RGBA method against those from the original RGB model, evaluating how well each method preserves the model's original generative quality. A lower FVD indicates that the generated results are closer to the original RGB model in terms of motion coherence and diversity, thus demonstrating a high fidelity to the model's intended generative quality. Results are shown in Fig. [11](#fig_10).

## Conclusion

In this work, we present a novel approach for Text-to-RGBA video generation, extending RGB generation models to support RGBA output with minimal modification and high fidelity. By leveraging transformer-based DiT models and optimizing attention mechanisms specific to RGBA generation, our method effectively balances the preservation of RGB quality with the accurate generation of alpha channels. Our approach demonstrates that targeted modifications-such as the addition of alpha tokens, reinitialization of positional embeddings, and selective LoRA fine-tuning-can yield complex and high-quality RGBA outputs even with limited data. Extensive experimental results validate our framework, showing its versatility and robustness across diverse scenarios. Looking forward, we aim to explore further optimizations to reduce computational costs and enhance model scalability.

TransPixar: Advancing Text-to-Video Generation with Transparency Supplementary Material

## Limitations

Our DiT-based method for RGBA generation incurs quadratic computational costs due to sequence expansion. However, our method achieves an optimal balance between generation and alignment when trained with a limited dataset. Numerous studies [[11,](#b9)[49,](#b47)[66]](#b64) have addressed the computational overhead of long sequences, with many optimizations reducing complexity to a linear scale. To enhance the efficiency of our method, we plan to incorporate these optimizations in future work. Additionally, our performance is influenced by the generative priors provided by the chosen T2V model, which affects the quality and consistency of our outputs.

## Comparisons with Video Matting

We compare our method with video matting methods Bi-Matting [[40]](#b38) and Robust Video Matting (RVM) [[30]](#b28), as well as the image matting method Matte-Anything [[59]](#b57). From the results, it is evident that most methods, trained on the VideoMatte240k [[29]](#b27) dataset, struggle to produce valid outputs for non-human objects, often resulting in empty results.

Even image matting methods trained on large-scale datasets fail to handle certain visual effects correctly. Results are shown in the attached HTML source files.

## Data Preprocessing

Color Decontamination. In our method, we preprocess the training data by applying a color decontamination step to enhance the quality of the RGBA video generation.

Color contamination typically occurs when there is an undesired blending of foreground and background colors, especially along the edges of an object, due to imperfect alpha masks. This blending causes color bleeding, where the foreground and background colors mix, resulting in lower quality RGBA frames with inaccurate color representation.

To address this issue, we refine the alpha mask using parameters such as gain (Î³ = 1.1) and choke (Ï‡ = 0.5) to adjust the sharpness and influence of the mask edges. The decontaminated RGB values are then computed as follows:

RGB decon = RGBÃ—(1-mask refined )+mask refined Ã—Background

This equation ensures that unwanted color contamination is minimized, providing a more precise distinction between foreground and background regions. By performing this preprocessing step, we generate high-quality train-ing data that significantly improves the performance of our RGBA video generation model. Background Blurring. Unlike typical training strategies in video matting methods, where objects are composited with complex backgrounds to increase the difficulty of the task, our goal is to support joint generation of alpha and RGB channels while ensuring alignment between them. Instead of emphasizing complex matting, we focus on generating consistent and high-quality output by compositing objects with simple, static backgrounds that match the black areas in the alpha channel. Specifically, we apply a large Gaussian blur kernel of size 201 to the first frame to create a blurred background and blend each subsequent frame with this static background. This approach helps simplify the training conditions, allowing the model to better align the RGB and alpha components while maintaining high-quality output.

## Optical Flow Difference

To evaluate the alignment between the RGB and alpha channels in generated videos, we introduce a metric based on optical flow difference. Optical flow measures the apparent motion of objects between consecutive frames, and comparing the optical flow fields of RGB and alpha frames provides insight into the consistency of motion across these modalities. Specifically, we use the Farneback method (cv::calcOpticalFlowFarneback) to compute the optical flow for both RGB and alpha frames, and then calculate the average Euclidean distance between their flow vectors as a measure of misalignment. This approach quantifies the degree to which the RGB and alpha channels align in terms of motion. Pseudo Code Overview: 1. Load consecutive RGB and alpha frames from the input video. 2. Convert the frames to grayscale for optical flow computation, as optical flow is typically calculated on intensity values. 3. Compute optical flow using the Farneback method (cv::calcOpticalFlowFarneback) for both the RGB and alpha frames. 4. Calculate the Euclidean distance between the RGB and alpha flow vectors for each pixel. 5. Average the differences across all pixels and frames to obtain the final optical flow difference. The average optical flow difference provides a quantitative metric for evaluating the alignment between RGB and alpha channels, helping to ensure that both modalities ex-hibit consistent motion.

## Video Results

For all video results shown in the main paper, please see the attached HTML source files.

## Additional Visual Results

In addition to the video results in the main paper, we provide more generated results in the supplementary files, including various objects and visual effects. Please find the corresponding results in the supplementary files.

![A dust cloud expanding after an explosion, covering the area"(c) with generative prior (MariGold, Lotus) (d) Ours (b) w/o generative prior (Video Matting) (a) Generated RGB]()

![Figure 2. Comparison between Generation-Then-Prediction and our Joint Generation approach. Given the generated RGB in (a), (b) and (c) show the predicted alpha (top) and the composited result (bottom). In (d), the top shows the jointly generated alpha.]()

![Figure 3. Pipeline of TransPixar. Our method is organized as follows: (1) Left: we extend the input of DiT to include new alpha tokens; (2) Top Center: we initialize alpha tokens with our positional encoding; (3) Bottom Center: we insert a partial LoRA and adjust attention computation during training and inference.]()

![Figure 4. Positional Encoding Design for RGBA Generation.Assigning alpha tokens the same positional encoding as RGB yields similar results, resulting in faster convergence after 1000 iterations compared to standard encoding strategies.]()

![Figure 5. Attention Rectification. (a) Eliminating all attention from alpha as a key preserves 100% RGB generation but leads to poor alignment. (b) Retaining all attention significantly degrades quality, causing a lack of motion in bicycles. (c) Our method achieves an effective balance.]()

![Figure 6. Applications. Top: Text-to-Video with Transparency. Bottom: Image-to-Video generation with transparency. .]()

![Figure 7. Comparison with Generation-then-Prediction Pipelines. Our method demonstrates superior alignment.]()

![Figure 8. Comparison with Joint Generation Pipelines. Top: LayerDiffusion + AnimateDiff; Bottom: Ours. Our method achieves better alignment and generates corresponding motion described by prompts.]()

![Figure 9. Alternative Designs for Joint Generation with DiT. Sequence extension (b) represents our method.]()

![Figure 10. Ablation Study. (a) Ours; (b) Ours without RGBattend-to-Alpha; (c) Ours with Text-attend-to-alpha; (d) Batch Extension Strategy; (e) Latent Dimension Extension Strategy. Our method maintains high-quality motion generation (e.g., butterflies waving their wings) while achieving good alignment.]()

![Figure 11. Quantitative Evaluation. Our approach achieves a good balance between alignment (low flow difference) and preserving generative quality (low FVD).]()

