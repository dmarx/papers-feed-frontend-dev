{
  "arxivId": "2501.03006",
  "title": "TransPixar: Advancing Text-to-Video Generation with Transparency",
  "authors": "Luozhou Wang, Yijun Li, Zhifei Chen, Jui-Hsien Wang, Zhifei Zhang, He Zhang, Zhe Lin, Yingcong Chen",
  "abstract": "Text-to-video generative models have made significant strides, enabling\ndiverse applications in entertainment, advertising, and education. However,\ngenerating RGBA video, which includes alpha channels for transparency, remains\na challenge due to limited datasets and the difficulty of adapting existing\nmodels. Alpha channels are crucial for visual effects (VFX), allowing\ntransparent elements like smoke and reflections to blend seamlessly into\nscenes. We introduce TransPixar, a method to extend pretrained video models for\nRGBA generation while retaining the original RGB capabilities. TransPixar\nleverages a diffusion transformer (DiT) architecture, incorporating\nalpha-specific tokens and using LoRA-based fine-tuning to jointly generate RGB\nand alpha channels with high consistency. By optimizing attention mechanisms,\nTransPixar preserves the strengths of the original RGB model and achieves\nstrong alignment between RGB and alpha channels despite limited training data.\nOur approach effectively generates diverse and consistent RGBA videos,\nadvancing the possibilities for VFX and interactive content creation.",
  "url": "https://arxiv.org/abs/2501.03006",
  "issue_number": 885,
  "issue_url": "https://github.com/dmarx/papers-feed/issues/885",
  "created_at": "2025-01-10T05:17:17.451567",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 23,
  "last_read": "2025-01-10T05:24:23.109329",
  "last_visited": "2025-01-10T05:23:00.328000+00:00",
  "main_tex_file": null,
  "published_date": "2025-01-06T13:32:16Z",
  "arxiv_tags": [
    "cs.CV"
  ]
}