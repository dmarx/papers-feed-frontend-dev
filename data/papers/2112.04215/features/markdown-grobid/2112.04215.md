# Self-Supervised Models are Continual Learners

## Abstract

## 

Self-supervised models have been shown to produce comparable or better visual representations than their supervised counterparts when trained offline on unlabeled data at scale. However, their efficacy is catastrophically reduced in a Continual Learning (CL) scenario where data is presented to the model sequentially. In this paper, we show that self-supervised loss functions can be seamlessly converted into distillation mechanisms for CL by adding a predictor network that maps the current state of the representations to their past state. This enables us to devise a framework for Continual self-supervised visual representation Learning that (i) significantly improves the quality of the learned representations, (ii) is compatible with several state-of-the-art self-supervised objectives, and (iii) needs little to no hyperparameter tuning. We demonstrate the effectiveness of our approach empirically by training six popular self-supervised models in various CL settings. Code: github.com/DonkeyShot21/cassle.

## Introduction

During the last few years, self-supervised learning (SSL) has become the most popular paradigm for unsupervised visual representation learning [[3,](#b2)[7,](#b6)[8,](#b7)[13,](#b12)[14,](#b13)[26,](#b25)[28,](#b27)[58]](#b58). Indeed, under certain assumptions (e.g., offline training with large amounts of data and resources), SSL methods are able to extract representations that match the quality of representations obtained with supervised learning, without requiring annotations. However, these assumptions do not always hold in real-world scenarios, e.g., when new unlabeled data are made available progressively over time. In fact, in order to integrate new knowledge into the model, training needs to be repeated on the whole dataset, which is impractical, expensive, and sometimes even impossible when old data is not available. This issue is exacerbated by the fact that SSL models are notoriously computationally expensive to train.

62.2 60.4 59.5 58.3 57.8 53.6 40 45 50 55 60 65 BYOL Barlow Twins MoCoV2+ SimCLR SwAV VICReg Class-incremental CIFAR100 66.4 68.2 68.8 68 66 64.8 58 60 62 64 66 68 70 BYOL Barlow Twins MoCoV2+ SimCLR SwAV VICReg Supervised fine-tuning Ours SSL fine-tuning

## Class-incremental ImageNet100

Figure [1](#). Linear evaluation accuracy of representations learned with different self-supervised methods on class-incremental CI-FAR100 and ImageNet100. In blue the accuracy of SSL finetuning, in green the improvement brought by CaSSLe. The red dashed line is the accuracy attained by supervised fine-tuning.

Continual learning (CL) studies the ability of neural networks to learn tasks sequentially. Prior art in the field focuses on mitigating catastrophic forgetting [[17,](#b16)[23,](#b22)[25,](#b24)[40]](#b39). Common benchmarks in the CL literature evaluate the discriminative performance of classifiers learned with supervision from non-stationary distributions. In this paper, we tackle the same forgetting phenomenon in the context of SSL. Unsupervised representation learning is indeed appealing for sequential learning since it does not require human annotations, which are particularly hard to obtain when new data is generated on-the-fly. This setup, called Continual Self-Supervised Learning (CSSL), is surprisingly underinvestigated in the literature.

In this work, we propose CaSSLe, a simple and effective framework for CSSL of visual representations based on the intuition that SSL models are intrinsically capable of learning continually, and that SSL losses can be seamlessly converted into distillation losses. Our key idea is to train the current model to predict past representations with a prediction head, thus encouraging it to remember past knowledge. CaSSLe has several favourable features: (i) it is compatible with popular state-of-the-art SSL loss functions and architectures, (ii) it is simple to implement, and (iii) it does not require any additional hyperparameter tuning with respect to the original SSL method. Our experiments demonstrate that SSL methods trained continually with CaSSLe significantly outperform all the related methods (CSSL baselines and several methods adapted from supervised CL).

We also perform a comprehensive analysis of the behavior of six popular SSL methods in diverse CL settings (i.e., class, data, and domain incremental). We provide empirical results on small (CIFAR100), medium (ImageNet100), and large (DomainNet) scale datasets. Our study sheds new light on interesting properties of SSL methods that emerge when learning continually. Among other findings, we discover that, in the class-incremental setting, SSL methods typically approach or outperform supervised learning (see Fig. [1](#)), while this is not generally true for other settings (data-incremental and domain-incremental) where supervised learning still shows a sizeable advantage.

## Related Work

Self-Supervised Learning. Recent SSL approaches have shown performance comparable to their supervised learning equivalents [[3,](#b2)[7,](#b6)[8,](#b7)[13,](#b12)[14,](#b13)[26,](#b25)[28,](#b27)[58]](#b58). In a nutshell, most of these methods use image augmentation techniques to generate correlated views (positives) from a sample, and then learn a model that is invariant to these augmentations by enforcing the network to output similar representations for the positives. Initially, contrastive learning, based on instance discrimination [[56]](#b56) using noise-contrastive estimation [[27,](#b26)[41]](#b40), was a popular strategy [[13,](#b12)[28]](#b27). However, this learning paradigm requires large batch sizes or memory banks. A few methods that use a negative-free cosine similarity loss [[15,](#b14)[26]](#b25) have addressed such issues. Concurrently, clustering-based methods (SwAV [[7]](#b6), DeepCluster v2 [[6,](#b5)[7]](#b6) and DINO [[8]](#b7)) have also been proposed. They do not operate on the features directly, and instead compare positives through a cross-entropy loss using cluster prototypes as a proxy. Redundancy reduction-based methods have also been popular [[3,](#b2)[20,](#b19)[58]](#b58). Among them, BarlowTwins [[58]](#b58) considers an objective function measuring the cross-correlation matrix between the features, and VicReg [[3]](#b2) uses a mix of variance, invariance and covariance regularizations. Methods such as [[19]](#b18) have explored the use of nearest-neighbour retrieval and divide and conquer [[53]](#b52). However, none of these works studied the ability of SSL methods to learn continually and adaptively. Continual Learning. A plethora of methods have been developed to counteract catastrophic forgetting [[2, 4, 9-12, 18, 22, 31, 34, 36, 38, 42, 44, 46-50, 55, 59]](#). Following [[17]](#b16), these works can be organized into three macro-categories: replay-based [[4,](#b3)[12,](#b11)[38,](#b37)[42,](#b41)[46,](#b45)[47]](#b46), regularization-based [[2, 9-11, 18, 22, 31, 34, 36, 50, 55, 59]](#), and parameter isolation methods [[48,](#b47)[49]](#b48). All these works evaluate the effectiveness of CL methods using a linear classifier learned sequentially over time. However, this evaluation does not reflect an important aspect, i.e., the internal dynamics of the hidden representations. Moreover, most CL methods tend to rely on supervision in order to mitigate catastrophic forgetting. A few of them can be adapted for the unsupervised setting, although their effectiveness is greatly reduced (see discussion in Sec. 5, Sec. 6 and the supplementary material).

Works such as [[1,](#b0)[45,](#b44)[51]](#b50) laid the foundations of unsupervised CL, but their studies are severely limited to digitlike datasets, e.g., MNIST and Omniglot, and the proposed methods are unfit for large-scale scenarios. Recently, [[5,](#b4)[24]](#b23) explored self-supervised pretraining for supervised continual learning with online and few-shot tasks, and [[10]](#b9) presented a supervised contrastive CL approach. Two concurrent works [[37,](#b36)[39]](#b38) have also attempted to address CSSL recently. The former [[37]](#b36) extends [[10]](#b9) to the unsupervised setting, but is specifically designed for contrastive SSL, such as [[13,](#b12)[28]](#b27), and lacks generalizability to other popular SSL paradigms. The latter [[39]](#b38) is also limited as it only shows small-scale experiments in the class-incremental setting and considers just two SSL methods. In contrast, we present a general framework for CSSL with superior performance, conduct large-scale experiments on three challenging settings, thereby presenting a deeper analysis of CSSL.

## Preliminaries

Self-Supervised Learning. The training procedure of several state-of-the-art SSL methods [[3,](#b2)[7,](#b6)[8,](#b7)[13,](#b12)[19,](#b18)[26,](#b25)[28,](#b27)[58]](#b58) can be summarized as follows. Given an image x in a batch sampled from a distribution D, two correlated views x A and x B are extracted by applying stochastic image augmentations, such as random cropping, color jittering and horizontal flipping. View x A is fed to an encoder f θ = f p • f b , which is parametrized by θ and has a backbone f b and a projection head f p , that extracts feature representations z A = f θ (x A ). Similarly, x B is forwarded into the same networks, or possibly copies thereof, updated with exponential moving average (EMA), to obtain the representation z B . A loss function L SSL is applied to these representations to learn the parameters θ as follows:

$argmin θ E x∼D L SSL z A , z B .$(

$)1$More details on the implementation of L SSL are provided in Sec. 5.1 and Tab. 1. This procedure turns out to be extremely powerful at extracting visual representations from large unlabeled datasets. The intuition behind the success of these models is that they learn to be invariant to augmentations. Importantly, augmentations are hand-crafted in a way that the two views x A and x B contain roughly the same semantics as x, but their overall appearance (geometry, colors, resolution, etc.) is different. This forces the model map images with the same semantics to similar regions of the feature space. Interestingly, these augmentations are much stronger, i.e., they distort the image more, than augmentations commonly used to train supervised models.

Continual Learning. The CL problem focuses on training models such as deep neural networks from non-stationary data distributions. More formally, this involves a network

$f ′ θ ′ = f ′ c • f ′ b with parameters θ ′ , backbone f ′ b and a classi- fier f ′ c$, that learns from an ordered set of tasks {1, . . . , T }, each exhibiting a different data distribution D t . Usually, an image x sampled i.i.d. from D t is processed by f ′ that predicts a probability distribution p over the set of classes Y t . The objective is to find parameters θ ′ such as:

$argmin θ ′ T t=1 E (x,y)∼Dt [L CL (p, y)] ,(2)$where, in most cases, L CL is the cross-entropy loss. However, during task t, the previous data distribution D t-1 is not available and therefore Eq. ( [2](#formula_3)) cannot be minimized directly. Current research focuses on approximating θ ′ using indirect approaches. Some of them [[18,](#b17)[36]](#b35) are based on knowledge distillation [[30]](#b29), i.e., transferring knowledge from one network to another by forcing them to produce the same outputs. We will discuss the applicability of distillation methods in CSSL in Sec. 5.

## Continual Self-Supervised Learning

In this paper, we tackle the problem of Continual Self-Supervised Learning as an extension of both SSL and CL. In practice, a CSSL experiment starts with the first task, where the model is trained as per the specific self-supervised method that it implements, with no difference from offline training. Subsequent tasks are then presented to the model sequentially, and the data from the previous tasks are discarded. No labels are provided during this training phase. For the sake of simplicity and since we are exploring a new, challenging setting, we assume task boundaries to be provided to the model. More formally, the CSSL objective is to learn a strong feature extractor that is invariant to augmentations on all tasks. Following the notation introduced in Sec. 3, we define:

$argmin θ T t=1 E x∼Dt L SSL z A , z B .(3)$Note the absence of labels y when sampling from D t , the summation over the set of tasks inherited from Eq. ( [2](#formula_3)) and the SSL loss function in Eq. ( [1](#formula_1)). The expectation is approximated using stochastic gradient descent on minibatches.

Evaluation. After each task, it is possible (for evaluation purposes) to train a linear classifier on top of the obtained backbone f b . With this linear classifier we report accuracy on the test set. This protocol is compatible with standard CL metrics, as shown in Sec. 6.1. We explore three CSSL settings in our work. ▶ Class-incremental: each task t is represented by a dataset D t ∼ D t containing images that belong to a set of classes Y t such that Y t ∩ Y s = ∅ for each other task s ̸ = t. Note that the class labels are only used for splitting the dataset and they are unknown to the model. In practice, the set of classes in the dataset are shuffled and then partitioned into T tasks. Each task contains the same number of classes. ▶ Data-incremental: each task t contains a set of images D t such that D t ∩ D s = ∅ for each other task s ̸ = t. No additional constraints are imposed on the classes. In practice, the whole dataset is shuffled and then partitioned into T tasks. Each task can potentially contain all the classes. ▶ Domain-incremental: each task t contains a set of images D t drawn from a different domain. We assume that the set of classes Y t in each dataset remains the same for all tasks but the data distribution changes, as if the data were collected from different sources.

## The CaSSLe Framework

We now introduce "CaSSLe", our framework for continual self-supervised learning of visual representations and detail its compatibility with several SSL methods.

## Distillation in CSSL.

From a supervised CL perspective, the concept of invariance is interesting. Here, we would like to learn representations of previously-learned semantic concepts that are invariant to the state of the model's parameters. Indeed, this idea was investigated in prior works [[18,](#b17)[31]](#b30) that leverage knowledge distillation for CL. However, such approaches are only mildly effective in a CSSL scenario, as we show in Sec. 6. We believe this is due to CSSL being fundamentally different from supervised CL. In CSSL, we aim to extract the best possible representations that can be subsequently reused in a variety of tasks, and maximize the linear separability of features at the end of the CL phase. Thus, the linear classifier does not benefit much from the stability of the representations. Also, forcing the representations not to change may prevent the model from learning new concepts. This is especially critical for SSL methods for two reasons: (i) the performance of the models improve substantially with longer training, implying that the representations continue to get refined, and (ii) they exhibit different losses and feature normalizations that might interfere with distillation and vice-versa (e.g., BarlowTwins uses standardization while [[18,](#b17)[31]](#b30) use l2-normalization). Nonetheless, the features still need to be informative of previous tasks to maximize the separability of the old distribution but the current state might be too different from the pre- vious one making comparing representations complicated.

Distillation through SSL losses. Our framework, shown in Fig. [2](#fig_0), is based on the following ideas: (i) a predictor network that maps the current state of the representations to their past state, by leveraging a distillation through time strategy that satisfies both stability and plasticity principles, and (ii) a family of adaptable distillation losses inherited from the SSL literature that solves the issue of having different objectives interfering with each other.

When a new task is received, we start by making a copy of the current model. This copy does not require gradient computation and will not be updated. We call this the frozen encoder f t-1 . As soon as an image x ∈ D t is available we apply our stochastic image augmentations and extract its features z = f t (x). In addition, we also use the frozen encoder to extract another feature vector z = f t-1 (x). Now, our goal is to ensure that z contains at least as much information as (and ideally more than) z. Instead of enforcing the two feature vectors to be similar, and hence discouraging the new model from learning new concepts, we propose to use a predictor network g to project the representations from the new feature space to the old one. If the predictor is able to perfectly map from one space to the other, then it implies that z is at least as powerful as z.

We are now ready to perform distillation, but which is the most appropriate distillation loss? Since we want the representations produced by g to be invariant to the state of the model, we propose to use the same SSL loss used to simulate invariance to augmentations. Empirically, we verify that this choice reduces interference and minimizes the need for hyperparameter tuning. We can hence write a generic distillation loss by reusing the definition of L SSL :

$L D (z, z) = L SSL (g(z), z). (4$$)$Note that z is always detached from the computational graph, such that the frozen encoder does not receive any gradient, and the gradient only flows through the predictor g, as prescribed in [[15]](#b14). On the one hand, if training converges and L D is minimized, the features predicted by g will likely be quasi-invariant to the state of the model, which satisfies the stability principle. On the other hand, the current encoder is less bound to its previous state, hence representations z can be more plastic. The loss can be extended to multiple views by applying it to both representations, i.e., L D (z A , zA ) + L D (z B , zB ), and also swapped distillation, e.g., L D (z A , zB ) and vice-versa (see ablation in Tab. 6).

The final loss of an SSL method trained continually with the CaSSLe framework is given by:

$L = L SSL (z A , z B ) + L D (z A , zA ) = L SSL (z A , z B ) + L SSL (g(z A ), zA ).$(

$)5$This loss can be made symmetric by applying it to both the views (swapping A and B in Eq. ( [5](#formula_8))) and it can also be easily adapted for multi-crop [[7]](#b6). Note that we do not use any hyperparameter to weight the importance of the distillation loss with respect to the SSL loss.

## Compatibility of SSL methods with CaSSLe

The main difference among SSL methods is the loss function that they use. Following the notation defined in Sec. 3, and the loss functions in Tab. 1, we now detail if and how SSL losses can be used in our CaSSLe framework. Full derivation of distillation losses is deferred to the supplementary material.

InfoNCE-based methods [[13,](#b12)[28]](#b27) perform instance discrimination, where positive samples help to build invariance to augmentations. The negatives prevent the model from falling into degenerate solutions. The InfoNCE (a.k.a. contrastive) loss can be written as in Eq. ( [6](#)), where subscript i is the index of a generic sample in the batch, sim is the cosine similarity and η(i) is the set of negatives for sample i in the current batch. Distilling knowledge with this loss is equivalent to performing instance discrimination of current task samples but in the feature space learnt in the past. Thus, the predictor g learns to project samples from the present to the past space to maximize the distance with the negative samples, and the similarity with itself in the past. [[15,](#b14)[26]](#b25) enforce consistency among positive samples and ignore the negatives. BYOL [[26]](#b25) uses a momentum encoder and SimSiam [[15]](#b14) performs a stop gradient operation to avoid degenerate solutions. Since the representations are l2-normalized, their loss (Eq. 7) can be rewritten as the negative cosine similarity:

## MSE-based approaches

$-sim(q A , z B ) = -q A ||q A ||2 • z B ||z B ||2$, where q A = h(z A ) and h is a prediction head. The gradient is backpropagated only Table [1](#). Overview of state-of-the-art SSL methods and losses. In all tables, highlight colors are coded according to the type of loss.

## Methods

## Loss Equation

SimCLR [[13]](#b12) -log

$exp(sim(z A i ,z B i )/τ) z j ∈η(i) exp(sim(z A i ,zj )/τ) (6)$MoCo [[28]](#b27) NNCLR [[19]](#b18) InfoNCE

$BYOL [26] -||q A -z B || 2 2 (7) SimSiam [15] VICReg [3] MSE SwAV [7] -d a B d log exp(sim(z A ,cd)/τ ) k exp(sim(z A ,ck)/τ ) (8)$DCV2 [[7]](#b6) DINO [[8]](#b7) Cross-entropy Barlow Twins [[58]](#b58) u (1

$-C uv ) 2 + λ u v̸ =u C 2 uv (9) VICReg [3]$Cross-correlation through the representations of the first argumentation. A special case of this family of methods is VICReg [[3]](#b2), which uses a combination of multiple losses, where MSE acts as invariance term. Features are not l2-normalized in VICReg and its predictor is the identity function. In our framework, this loss encourages the model to predict the past state of representations without additional regularization.

Cross-entropy-based. Instead of simply enforcing invariance of the representations to augmentations, cluster prototypes C = {c 1 , . . . , c K } are used as a proxy in these approaches, so that the model learns to predict invariant cluster assignments. Slight variations of this idea result in different methods: SwAV [[7]](#b6), DeepClusterV2 [[7]](#b6) and DINO [[8]](#b7). Once a probability distribution over the prototypes is predicted, the cross-entropy loss (Eq. 8) is used to compare the two views. Features and cluster prototypes c are l2-normalized. The assignments a B can be calculated in several ways, e.g., k-means in DeepCluster, Sinkhorn-Knopp in SwAV and EMA in DINO. When employed as a distillation loss, cross-entropy encourages g to predict the assignments generated by the frozen encoder with a set of frozen prototypes:

$a B = exp(sim(z B ,c t-1 d )/τ) k exp(sim(z B ,c t-1 k )/τ)$, where

$C t-1 = c t-1 1 , . . . , c t-1 K .$Cross-correlation-based. These methods use a different approach based on decorrelating the components of the feature space, e.g., Barlow Twins [[58]](#b58), VICReg [[3]](#b2) and W-MSE [[20]](#b19). For our analysis, we will mainly focus on Barlow Twins' implementation of this objective. Extensions to VICReg are left for future work. The crosscorrelation based objective function is shown in Eq. 9, where λ is an hyperparameter to control the importance of the first and the second terms of the loss, and

$C uv = i z A i,u z B i,v i (z A i,u ) 2 . i (z B i,v )$2 is the value of position (u, v) of the cross-correlation matrix computed between the representations of the views along the batch dimension. Note that the representations here are mean centered along the batch dimension, such that each unit has mean output zero over the batch. Performing distillation with this loss has the addi-tional effect of decorrelating the dimensions of the predicted features g(z A ).

## Experiments

## Experimental Protocol

Evaluation Metrics. Following previous work [[38]](#b37), we propose the following metrics to evaluate the quality of the representations extracted by our CSSL model: ▶ Linear Evaluation Accuracy: accuracy of a classifier trained on top of the backbone f b on all tasks (or a subset, e.g., 10% of the data) or a downstream task. For classincremental and data-incremental, we use the task-agnostic setting, meaning that at evaluation time we do not assume to know the task ID. For the domain-incremental setting, we perform both task-aware and task-agnostic evaluations (the latter is discussed in the supplementary material). To calculate the average accuracy we compute A = 1 T T i=1 A T,i , where A j,k is the linear evaluation accuracy of the model on task k after observing the last sample from task j. ▶ Forgetting: a common metric in the CL literature, it quantifies how much information the model has forgotten about previous tasks. It is formally defined as:

$F = 1 T -1 T -1 i=1 max t∈{1,...,T } (A t,i -A T,i ).$▶ Forward Transfer: measures how much the representations that we learned so far are helpful in learning new tasks, namely:

$F T = 1 T -1 T i=2 A i-1,i -R i$where R i is the linear evaluation accuracy of a random network on task i.

Datasets. We perform experiments on 3 datasets: CI-FAR100 [[35]](#b34) (class-incremental), a 100-class dataset with 60k 32x32 colour images; ImageNet100 [[54]](#b53) (class-and data-incremental), 100-class subset of the ILSVRC2012 dataset with ≈130k images in high resolution (resized to 224x224); DomainNet [[43]](#b42) (domain-incremental), a 345class dataset containing roughly 600k high-resolution images (resized to 224x224) divided into 6 domains. We experiment with 5 tasks for the class-and data-incremental settings and with 6 tasks (one for each domain in Domain-Net) in the case of domain-incremental. The supplementary material presents additional results with different number of tasks. For the domain-incremental setting, we order the domains in decreasing number of images.

Implementation details. The SSL methods are adapted from solo-learn [[16]](#b15), an established SSL library, which is the main code base for all our experiments. The number of epochs per task is as follows: 500 for CIFAR100, 400 for ImageNet100, 200 for DomainNet. The backbone f b is a ResNet18 [[29]](#b28), with batch size 256. We use LARS [[57]](#b57) for all our experiments. The offline version of each method, that serves as an upper bound, is trained for the same number of epochs as the continual counterpart for a fair comparison. All the results for offline upper bounds are ob-Table [2](#). Comparison with state-of-the-art CL methods on CI-FAR100 (5 tasks, class-incremental) using linear evaluation top-1 accuracy, forgetting and forward transfer.

## Strategy

## SimCLR Barlow Twins BYOL

## A (↑) F (↓) T (↑) A (↑) F (↓) T (↑) A (↑) F (↓) T (↑)

Fine-tuning 48.9 1.0 33. [5](#b4)  tained using the checkpoints provided in [[16]](#b15). For some SSL methods, it was necessary to slightly increase the learning rate over the values provided by [[16]](#b15) in order for the methods to fully convergence in the CSSL setting. Although tuning the hyperparameters might be beneficial in some settings, we do not perform any hyperparameter tuning for CaSSLe. We also neither change the parameters of the SSL methods, nor use a weight for the distillation loss (as per Eq. ( [5](#formula_8))).

Baselines. Most of the CL methods require labels which makes them unsuitable for CSSL. However, a few works can be adapted for our setting with minimal changes. We choose baselines from three categories [[17]](#b16): priorfocused regularization (EWC [[34]](#b33)), data-focused regularization (POD [[18]](#b17), Less-Forget [[31]](#b30)), and rehearsal-based replay (ER [[47]](#b46), DER [[4]](#b3)) methods. We also compare with two concurrent works that propose approaches for CSSL (LUMP [[39]](#b38), Lin et al. [[37]](#b36)). Finally, we do not consider methods based on VAEs [[1,](#b0)[45]](#b44), since they have been shown to yield poor performance on large scale. Details on how the baselines are selected, implemented and tuned for CSSL can be found in the supplementary material.

## Results

Comparison with the state of the art. In Tab. 2 we report comparison with CL baselines and fine-tuning in composition with three SSL methods: SimCLR, Barlow Twins and BYOL. We select these three methods for the following reasons: (i) they feature different losses (InfoNCE, Crosscorrelation and MSE), (ii) they exhibit different feature normalizations (l2, standardization and mean centering), and (iii) they use different techniques to avoid collapse (neg- atives, redundancy reduction, momentum encoder). The comparison is performed on class-incremental CIFAR100 with 5 tasks. Offline learning results are reported as upper bound.

First, we notice that CaSSLe produces better representations than all the other strategies, outperforming them by large margins with all SSL methods in terms of top-1 accuracy. Moreover, our framework also shows better forward transfer, meaning that its features are easier to generalize to other tasks (also evident in Tab. 8). CaSSLe appears to reduce catastrophic forgetting with respect to fine-tuning, and is comparable to other methods. In general, SSL methods already have low forgetting with respect to supervised learning on CIFAR100 (see Tab. 4) and therefore there is little margin for improvement. However, on higher resolution images (ImageNet100) CaSSLe actually achieves remarkable results in the mitigation of catastrophic forgetting.

Replay-based methods (ER, DER) clearly do not help against forgetting in CSSL. We found two reasons for this failure. First, in supervised CL, replay-based methods benefit from storing labels, which contain a lot of information about previous tasks and enable the retraining of the linear classifier on old classes. This is not the case in CSSL, where labels are unavailable. Second, SSL models need more training epochs to converge, which means that samples in the buffer are also replayed many more times. This causes severe overfitting on these exemplars, defeating the purpose of the replay buffer. LUMP mitigates this effect by augmenting the buffer using mixup but does not reach too far, surpassing other baselines only with Barlow Twins. EWC holds up surprisingly well, outperforming more recent methods, meaning that the importance of the weights can be calculated accurately with the self-supervised loss. Distillation methods (POD, Less-Forget) show good performance. However, they use l2-normalization in their loss, causing loss of information when coupled with Barlow Twins, which decreases accuracy. Fig. [3](#fig_1) shows the evolution of top-1 linear evaluation accuracy over the whole training trajectory on classincremental CIFAR100 with 5 tasks. CaSSLe outperforms the other methods, and keeps improving throughout the sequence. We found BYOL to be unstable when simply finetuning the model. CaSSLe, EWC and Less-Forget mitigate this instability completely. On the other hand, LUMP first drops slightly and then recovers. We believe this is due to some instability introduced by the mixup regularization, to which the model takes time to adapt.

In Tab. 3 we also compare with Lin et al. [[37]](#b36) on classincremental CIFAR100. Although our method is not specifically designed for contrastive learning, it substantially outperforms Lin et al. with 2 and 5 tasks. It is worth nothing that MoCoV2+ is slightly better than MoCoV2 (≈1% difference), whereas our gains are much larger (≈7%).

Ablation study. We ablate the most critical design choices we adopt in CaSSLe: (i) distillation without swapped views, and (ii) the presence of a prediction head g. These results are reported in Tab. 6. Our full framework clearly outperforms its variants with swapped views and without predictor. This validates our hypothesis that a predictor to map new features to the old feature space is crucial. The result that swapping views does not help is likely due to the frozen encoder not being invariant to the current task.

Class-incremental. In Tab. 4 we report a study of CSSL with 6 SSL methods in composition with the CaSSLe framework on class-incremental CIFAR100 and ImageNet100. Fine-tuning and Offline SSL results are reported as lower and upper bounds. The accuracy of supervised learning is also reported. CaSSLe always improves with respect to fine-tuning. In particular, our framework produces higher forward transfer and lower forgetting, especially on ImageNet100, where methods tend to forget more. Notably, CaSSLe outperforms supervised fine-tuning, ex- cept when coupled with VICReg on CIFAR100. On average, SSL methods trained continually with CaSSLe improve by 6.8% on CIFAR100 and 4% on ImageNet100.

Data-incremental. Tab. 7 presents results for linear evaluation top-1 accuracy on ImageNet100 with 5 tasks in a dataincremental scenario. While no SSL method is better than supervised fine-tuning, Barlow Twins coupled with CaSSLe is competitive. CaSSLe improves performance in all cases by 2% on average, except for BYOL. This is likely due to the fact that in the data-incremental scenario remembering past knowledge is less important than in other scenarios, and BYOL already has a momentum encoder that provides some information about the past. This hypothesis is validated by the fact that MoCoV2+ (that uses a momentum encoder) improves less than SimCLR when coupled with CaSSLe. We believe that, by tuning the EMA schedule, improvement could also be achieved for BYOL. In addition, BYOL already shows impressive performance with finetuning, outperforming all the other methods by more than 2%. Interestingly, SwAV comes closest to its offline upper bound, with only a 3% decrease in performance when coupled with CaSSLe.

## Domain-incremental.

We also examine the capability of CaSSLe to learn continually when the domain from which the data is drawn changes. Tab. 7 shows the average top-1 accuracy of a linear classifier trained on top of the frozen feature extractor on all domains separately (domain-aware). Domain-agnostic evaluation and results for each domain are presented in the supplementary material. Again, CaSSLe improves every method by 4.4% on average, showing that our distillation strategy is robust to domain shift, and although the data distribution is really different, information transfer is still performed. Interestingly, most of the methods, when trained with CaSSLe get very close to their offline accuracy.

Long training vs continual training. We also analyze the following question: is it worth training continually or is it better to train for longer on a small dataset? This depends on two factors: (i) the SSL method, and (ii) the CSSL setting. For SimCLR and Barlow Twins in the classincremental setting it seems to be better to train offline on 1/5 of the classes instead of training continually with 5 tasks. In this setting, offline BYOL seems to suffer from instability, ending up lower than fine-tuning. On the other hand, on the data-incremental setting, fine-tuning outperforms longer training, especially for BYOL, which also outperforms CaSSLe (as explained previously). Apart from this exception, CaSSLe always produces better representations than other strategies, making it the go-to option.

Downstream and semi-supervised. In Tab. 8, we present the downstream performance of CaSSLe compared with fine-tuning when trained on ImageNet100 and evaluated on DomainNet (Real). Barlow Twins, SwAV and BYOL show higher performance than the supervised model, even when considering a fine-tuning strategy. This is probably due to the fact that SSL methods tend to learn more general features than their supervised counterparts. CaSSLe improves performance on all the SSL methods, making them surpass the supervised baseline. Lastly, when compared with finetuning, CaSSLe improves the performance of SSL methods by 3.4% on average. Tab. 9 contains the top-1 accuracy on ImageNet100 when training a linear classifier on a frozen backbone with limited amount of labels (10% and 1%). First, we can observe that no SSL method with fine-tune surpasses the performance of supervised learning. When using CaSSLe, MoCoV2+ outperforms supervised with 10% labels and, in general, Barlow Twins and Mo-CoV2+ work best in both semi-supervised settings. CaSSLe improves all SSL methods when compared with fine-tuning.

## Conclusion

In this work, we study Continual Self-Supervised Learning (CSSL), the problem of learning a set of tasks without labels continually. We make two important contributions for the SSL and CL communities: (i) we present CaSSLe, a simple and effective framework for CSSL that shows how SSL methods and losses can be seamlessly reused to learn continually, and (ii) we perform a comprehensive analysis of CSSL, leading to the emergence of interesting properties of SSL methods.

Limitations. Although CaSSLe shows exciting performance, it has some limitations. First, it is applicable in settings where task boundaries are provided. Second, our framework increases the amount of computational resources needed for training by roughly 30%, both in terms of memory and time. Finally, CaSSLe does not perform clustering, meaning that it is unable to directly learn a mapping from data to latent classes, and thus needs either a linear classifier trained with supervision, or some clustering algorithm.

Broader impact. The capabilities of supervised CL agents are bounded by the need for human-produced annotations. CSSL models can potentially improve without the need for human supervision. This facilitates the creation of powerful AIs that may be used for malicious purposes such as discrimination and surveillance. Also, since in CSSL the data is supposed to come from a non-curated stream, the model may be affected by biases in the data. This is problematic because biases are then be transferred to downstream tasks. MSE based. This distillation loss is simply the MSE between the predicted features and the frozen features:

$L(z, z) = -||g(z) -z|| 2 2 .(12)$It can be implemented with the cosine similarity as stated in the main manuscript.

Cross-entropy based. The cross-entropy loss, when used for distillation in an unsupervised setting, makes sure that the current encoder is able to assign samples to the frozen centroids (or prototypes) consistently with the frozen encoder:

$L(z, z) = - d ād log exp sim g(z), c t-1 d /τ k exp sim g(z), c t-1 k /τ(13)$where:

$ā = exp sim z, c t-1 d /τ k exp sim z, c t-1 k /τ ,(14)$and the set of frozen prototypes is denoted as follows:

$C t-1 = c t-1 1 , . . . , c t-1 K .$Cross-correlation based. We consider Barlow Twins' [[58]](#b58) implementation of this objective. For VICReg [[3]](#b2) we only consider the invariance term. As a distillation loss, the crosscorrelation matrix is computed with the predicted and frozen features:

$L(z, z) = u 1 -Cuv 2 + λ u v̸ =u C2 uv ,(15)$where:

$Cuv = i g(zi,u)zi,v i g (zi,u) 2 . i (zi,v) 2 . (16$$)$C. Further discussion and implementation details of the baselines Selection. When evaluating our framework, we try to compare with as many existing related methods as possible. However, given that SSL models are computationally intensive, it was not possible to run all baselines and methods in all the CL settings we considered. As mentioned in the main manuscript, we choose eight baselines (seven related methods + fine-tuning) belonging to three CL macro-categories, and test them on CIFAR100 (class-incremental) in combination with three SSL methods. The selection was based on the ease of adaptation to CSSL and the similarity to our framework.

The most similar to CaSSLe are data-focused regularization methods. Among them, a large majority leverage knowledge distillation using the outputs of a classifier learned with supervision e.g. [[9,](#b8)[22,](#b21)[36]](#b35), while a few works employ feature distillation [[18,](#b17)[31]](#b30) which is viable even without supervision. [[32]](#b31) is also related to CaSSLe, but it focuses on memory efficiency which is less interesting in our setting. Also, [[32]](#b31) explicitly uses the classifier after feature adaptation, hence it is unclear how to adapt it

Table A. Linear evaluation top-1 accuracy on DomainNet (6 tasks, domain-incremental setting) w/ and w/o CaSSLe. The sequence of tasks is Real→Quickdraw→Painting→Sketch→Infograph→Clipart. "Aw." stands for task-aware, "Ag," for task-agnostic. Method Strategy Real Quickdraw Painting Sketch Infograph Clipart Avg.

for CSSL, especially since in SSL positives are generated using image augmentations, which are not applicable to a memory bank of features. On the contrary, augmentations can be used in replay methods, among which we select the most common (ER [[47]](#b46)) and one of the most recent (DER [[4]](#b3)). Regarding prior-focused regularization methods, we choose EWC [[34]](#b33) over others (SI [[59]](#b59), MAS [[2]](#b1), etc.) as it is considered the most influential and it works best with task boundaries. We also consider two CSSL baselines: LUMP [[39]](#b38) and Lin et al. [[37]](#b36). Finally, we do not consider methods based on VAEs [[1,](#b0)[45]](#b44), since they have been shown to yield poor performance in the large and medium scale. For instance, as found by [[21]](#b20), a VAE trained offline on CIFAR10 reaches an accuracy of 57.2%, which is lower than any method (except VICReg) trained continually on CIFAR100 with CaSSLe.

Implementation. For EWC, we use the SSL loss instead of the supervised loss to estimate importance weights. For POD and Less-Forget, we only re-implement the feature distillation without considering the parts of their methods that explicitly use the classifier. For DER, we replace the logits of the classifier with the projected features in the buffer. We re-implement all these baselines by adapting them from the official implementation (POD), or from the Mammoth framework provided with [[4]](#b3) (DER, ER, EWC), or from the paper (Less-Forget). We also compare with two concurrent works that propose approaches for CSSL (LUMP [[39]](#b38), Lin et al. [[37]](#b36)). LUMP uses k-NN evaluation, therefore we adapt the code provided by the authors to run in our code base. For Lin et al., we compare directly with their published results, since they use the same evaluation protocol. We perform hyperparameter tuning for all baselines, searching over 5 values for the distillation loss weights of POD and Less-Forget, 3 values for the weight of the regularization in EWC and 3 replay batch sizes for replay methods. The size of the replay buffer is 500 samples for all replay based methods.

## D. Additional results

Continual supervised contrastive with CaSSLe. After the popularization of contrastive learning [[13,](#b12)[28]](#b27) for unsupervised learning of representations, [[33]](#b32) proposed a supervised version of the contrastive loss. Here, we show that CaSSLe is easily extendable to support supervised contrastive learning. The implementation is basically the same as for our vanilla contrastive-based distillation loss. In Tab. B, we show the improvement that CaSSLe brings with respect to fine-tuning, which is sizeable in the classincremental setting. We also report the same comparison on Do-mainNet in Tab. A, showing interesting results in both task-aware and task-incremental evaluation.

Task-agnostic evaluation and domain-wise accuracy on DomainNet. In the main manuscript, we showed that CaSSLe significantly improved performance in the domain-incremental setting using task-aware evaluation. Here, "task-aware" refers to

Table B. Linear evaluation top-1 accuracy on ImageNet100 (5 tasks, class-and data-incremental). Method Strategy ImageNet100 Class-inc. Data-inc. Supervised Contrastive Fine-tuning 61.6 74.3 CaSSLe 69.6 76.9 Table C. k-NN evaluation on ImageNet100 (5 tasks, classincremental) performed on backbone and projected features. Method Strategy k-NN accuracy (↑) Backbone (f b ) Projector (fp) Barlow Twins Fine-tuning 59.1 34.4 CaSSLe 63.4 53.2 SwAV Fine-tuning 60.0 53.9 CaSSLe 59.7 61.3 BYOL Fine-tuning 57.1 33.0 CaSSLe 61.2 60.8 VICReg Fine-tuning 56.7 35.3 CaSSLe 59.5 43.4 MoCoV2+ Fine-tuning 54.5 39.0 CaSSLe 61.5 53.1 SimCLR Fine-tuning 54.8 40.1 CaSSLe 61.7 53.2

the fact that linear evaluation is performed on each domain separately, i.e. a different linear classifier is learned for each domain. However, it might also be interesting to check the performance of the model when the domain is unknown at test time. For this reason, we report the performance of our model when evaluated in a task-agnostic fashion. In addition, we also show the accuracy on each task (i.e. domain). All this information is presented in Tab. A. CaSSLe always outperforms fine-tuning with both evaluation protocols. The accuracy of CaSSLe on "Clipart" is also higher than offline. This is probably due to a combination of factors: (i) Clipart is the last task, therefore it probably benefits in forward transfer and (ii) a similar effect to the one found in [[53]](#b52), where dividing data in subgroups tends to enable the learning of better representations. Also, we notice that task-agnostic accuracy is lower than the task-aware counterpart. This is expected and means that the class conditional distributions are not perfectly aligned in different domains. As in the main paper, the colors are related to the type of SSL loss.

Additional results with k-NN evaluation. For completeness, in this supplementary material, we also show that CaSSLe yields superior performance when evaluated with a k-NN classifier instead of linear evaluation. We use weighted k-NN with l2normalization (cosine similarity) and temperature scaling as in [[8]](#b7). Since since k-NN is much faster than linear evaluation we could also assess the quality of the projected representations, instead of just using the backbone. The results can be inspected in Tab. C. Three interesting phenomena arise: (i) CaSSLe always improves with respect to fine-tuning, (ii) the features of the backbone f b are usually better than the features of the projector fp and (iii) CaSSLe causes information retention in the projector, which significantly increases the performance of the projected features. An excep-

Table D. Linear evaluation top-1 accuracy on CIFAR100 (10 tasks, class-incremental). Method Strategy A (↑) SimCLR Fine-tuning 39.3 CaSSLe 52.7 Barlow Twins Fine-tuning 49.9 CaSSLe 53.7 Table E. Linear evaluation top-1 accuracy on ImageNet100 (5 tasks, class-and data-incremental) with ResNet50 [29]. Method Strategy A (↑) Class-inc. Data-inc. SimCLR Fine-tuning 70.7 75.6 CaSSLe 74.0 77.2 Barlow Twins Fine-tuning 71.2 75.8 CaSSLe 74.8 78.1

tion is represented by SwAV [[7]](#b6), that seems to behave differently to other methods. First, the accuracy of the projected features in SwAV is much higher than other methods. This might be due to the fact that it uses prototypes, which bring the representations 1 layer away from the loss, making them less specialized in the SSL task. Second, it seems that CaSSLe only improves the projected features when coupled with SwAV. However, this is probably an artifact of the evaluation procedure, as the l2-normalization probably causes loss of information. Indeed, although the overall performance is lower, SwAV + CaSSLe outperforms SwAV + fine-tuning (58.7% vs 56.9%) if the euclidean distance is used in place of the cosine similarity for the backbone features. We leave a deeper investigation of this phenomenon for future work.

Different number of tasks. The analysis of CSSL settings that we show in the main manuscript is limited to the 5 task scenario. However, it is interesting to run the same benchmarks with a longer task sequence. Nonetheless, one should also remember that SSL methods are data hungry, hence the less data is available per task, the higher the instability of the SSL models. In Tab. D, we present additional results with 10 tasks on CIFAR100 (classincremental). Barlow Twins seems to hold up surprisingly well, finishing up at roughly 50% accuracy, while SimCLR suffers in the low data regime. Nonetheless, CaSSLe outperforms fine-tuning with Barlow Twins, and to a very large extent with SimCLR.

## Deeper architectures.

The experiments we propose in the main manuscript feature a ResNet18 network. This is a common choice in CL. However, in SSL, it is more common to use ResNet50. For this reason, in Tab. E we show that the same behavior observed with smaller networks is also obtained with deeper architectures. More specifically, CaSSLe outperforms fine-tuning in both class-and data-incremental settings by large margins.

The role of the predictor. In the main manuscript, we provided an intuitive explanation of the role of the predictor network that maps the current feature space to the frozen feature space.

Table F. Combinations of SSL methods and distillation losses on CIFAR100 (class-incremental, 2 tasks). Distillation Loss SimCLR Barlow Twins BYOL InfoNCE 61.8 64.5 64.8 Cross-correlation 60.1 67.2 65.8 MSE 61.3 64.6 66.7

This intuition is corroborated by extensive experimentation and ablation studies. However, one more thing that is worth mentioning is that the success of the predictor network might also be related to the findings in SimSiam [[15]](#b14), BYOL [[26]](#b25) and Direct-Pred [[52]](#b51). Moreover, we perform additional ablations on the design of CaSSLe's predictor for SimCLR on CIFAR100 (5 tasks): adding BatchNorm after the hidden layer does not make any difference in terms of performance, and removing the non-linearity only causes a 0.3% drop in accuracy.

Combinations of SSL methods and distillation losses.

For computational reasons, it was not feasible to perform experiments combing all SSL methods with all possible distillation losses. However, in Tab. F we provide a subset of the possible combinations to validate our strategy that uses the same SSL loss for distillation.

![Figure 2. Overview of the CaSSLe framework.]()

![Figure 3. Evolution of top-1 linear evaluation accuracy over tasks on CIFAR100 (5 tasks, class-incremental).]()

![Comparison with Lin et al.[37] on CIFAR100 (2 and 5 tasks, class-incremental setting). MoCoV2+ is an updated version of MoCoV2 that uses a symmetric loss. The difference between the two is ≈1% at convergence[15].]()

![Linear evaluation top-1 accuracy on class-incremental CIFAR100 and ImageNet100 with 5 tasks. CaSSLe is compared to fine-tuning, offline and supervised learning.]()

![Training 5 times longer on 1/5 of the data vs. training continually w/ and w/o CaSSLe on ImageNet100 (5 tasks, classand data-incremental). Bold is best, underlined is second best.]()

![Ablation study of design choices in CaSSLe.]()

![Linear evaluation accuracy on ImageNet100 (5 tasks, data-incremental) and DomainNet (6 tasks, domain-incremental).]()

![Downstream performance with different SSL methods trained on Imagenet-100 and evaluated on DomainNet (Real).]()

![Top-1 linear accuracy on Imagenet-100 with different SSL methods, semi-supervised setting with 10% and 1% of labels.]()

