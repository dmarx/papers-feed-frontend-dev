<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Supervised Models are Continual Learners</title>
				<funder ref="#_JKFupK2 #_Fj5fJFz #_VUPfXvQ">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
				<funder ref="#_y36GDJk">
					<orgName type="full">European Research Council</orgName>
					<orgName type="abbreviated">ERC</orgName>
				</funder>
				<funder>
					<orgName type="full">UNITN</orgName>
				</funder>
				<funder ref="#_PNRrp4R #_C3HggMh">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder>
					<orgName type="full">European Institute of Innovation &amp; Technology</orgName>
					<orgName type="abbreviated">EIT</orgName>
				</funder>
				<funder ref="#_9PURANy">
					<orgName type="full">European Commission</orgName>
				</funder>
				<funder ref="#_RYgNq6N">
					<orgName type="full">GENCI</orgName>
				</funder>
				<funder ref="#_UztgsDc">
					<orgName type="full">H2020</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Enrico</forename><surname>Fini</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Trento</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory" key="lab1">Enrico Fini and Victor G. Turrisi da Costa contributed equally. † Univ. Grenoble Alpes</orgName>
								<orgName type="laboratory" key="lab2">LJK</orgName>
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">Grenoble INP</orgName>
								<address>
									<postCode>38000</postCode>
									<settlement>Grenoble</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Victor</forename><forename type="middle">G</forename><surname>Turrisi Da Costa</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Trento</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory" key="lab1">Enrico Fini and Victor G. Turrisi da Costa contributed equally. † Univ. Grenoble Alpes</orgName>
								<orgName type="laboratory" key="lab2">LJK</orgName>
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">Grenoble INP</orgName>
								<address>
									<postCode>38000</postCode>
									<settlement>Grenoble</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xavier</forename><surname>Alameda-Pineda</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Trento</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Julien</forename><surname>Mairal</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Fondazione</forename><forename type="middle">Bruno</forename><surname>Kessler</surname></persName>
						</author>
						<title level="a" type="main">Self-Supervised Models are Continual Learners</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1BC68A683F38772C2A56736365E4C2CC</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Self-supervised models have been shown to produce comparable or better visual representations than their supervised counterparts when trained offline on unlabeled data at scale. However, their efficacy is catastrophically reduced in a Continual Learning (CL) scenario where data is presented to the model sequentially. In this paper, we show that self-supervised loss functions can be seamlessly converted into distillation mechanisms for CL by adding a predictor network that maps the current state of the representations to their past state. This enables us to devise a framework for Continual self-supervised visual representation Learning that (i) significantly improves the quality of the learned representations, (ii) is compatible with several state-of-the-art self-supervised objectives, and (iii) needs little to no hyperparameter tuning. We demonstrate the effectiveness of our approach empirically by training six popular self-supervised models in various CL settings. Code: github.com/DonkeyShot21/cassle.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>During the last few years, self-supervised learning (SSL) has become the most popular paradigm for unsupervised visual representation learning <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b58">58]</ref>. Indeed, under certain assumptions (e.g., offline training with large amounts of data and resources), SSL methods are able to extract representations that match the quality of representations obtained with supervised learning, without requiring annotations. However, these assumptions do not always hold in real-world scenarios, e.g., when new unlabeled data are made available progressively over time. In fact, in order to integrate new knowledge into the model, training needs to be repeated on the whole dataset, which is impractical, expensive, and sometimes even impossible when old data is not available. This issue is exacerbated by the fact that SSL models are notoriously computationally expensive to train.</p><p>62.2 60.4 59.5 58.3 57.8 53.6 40 45 50 55 60 65 BYOL Barlow Twins MoCoV2+ SimCLR SwAV VICReg Class-incremental CIFAR100 66.4 68.2 68.8 68 66 64.8 58 60 62 64 66 68 70 BYOL Barlow Twins MoCoV2+ SimCLR SwAV VICReg Supervised fine-tuning Ours SSL fine-tuning</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class-incremental ImageNet100</head><p>Figure <ref type="figure">1</ref>. Linear evaluation accuracy of representations learned with different self-supervised methods on class-incremental CI-FAR100 and ImageNet100. In blue the accuracy of SSL finetuning, in green the improvement brought by CaSSLe. The red dashed line is the accuracy attained by supervised fine-tuning.</p><p>Continual learning (CL) studies the ability of neural networks to learn tasks sequentially. Prior art in the field focuses on mitigating catastrophic forgetting <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b39">40]</ref>. Common benchmarks in the CL literature evaluate the discriminative performance of classifiers learned with supervision from non-stationary distributions. In this paper, we tackle the same forgetting phenomenon in the context of SSL. Unsupervised representation learning is indeed appealing for sequential learning since it does not require human annotations, which are particularly hard to obtain when new data is generated on-the-fly. This setup, called Continual Self-Supervised Learning (CSSL), is surprisingly underinvestigated in the literature.</p><p>In this work, we propose CaSSLe, a simple and effective framework for CSSL of visual representations based on the intuition that SSL models are intrinsically capable of learning continually, and that SSL losses can be seamlessly converted into distillation losses. Our key idea is to train the current model to predict past representations with a prediction head, thus encouraging it to remember past knowledge. CaSSLe has several favourable features: (i) it is compatible with popular state-of-the-art SSL loss functions and architectures, (ii) it is simple to implement, and (iii) it does not require any additional hyperparameter tuning with respect to the original SSL method. Our experiments demonstrate that SSL methods trained continually with CaSSLe significantly outperform all the related methods (CSSL baselines and several methods adapted from supervised CL).</p><p>We also perform a comprehensive analysis of the behavior of six popular SSL methods in diverse CL settings (i.e., class, data, and domain incremental). We provide empirical results on small (CIFAR100), medium (ImageNet100), and large (DomainNet) scale datasets. Our study sheds new light on interesting properties of SSL methods that emerge when learning continually. Among other findings, we discover that, in the class-incremental setting, SSL methods typically approach or outperform supervised learning (see Fig. <ref type="figure">1</ref>), while this is not generally true for other settings (data-incremental and domain-incremental) where supervised learning still shows a sizeable advantage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Self-Supervised Learning. Recent SSL approaches have shown performance comparable to their supervised learning equivalents <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b58">58]</ref>. In a nutshell, most of these methods use image augmentation techniques to generate correlated views (positives) from a sample, and then learn a model that is invariant to these augmentations by enforcing the network to output similar representations for the positives. Initially, contrastive learning, based on instance discrimination <ref type="bibr" target="#b56">[56]</ref> using noise-contrastive estimation <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b40">41]</ref>, was a popular strategy <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b27">28]</ref>. However, this learning paradigm requires large batch sizes or memory banks. A few methods that use a negative-free cosine similarity loss <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b25">26]</ref> have addressed such issues. Concurrently, clustering-based methods (SwAV <ref type="bibr" target="#b6">[7]</ref>, DeepCluster v2 <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> and DINO <ref type="bibr" target="#b7">[8]</ref>) have also been proposed. They do not operate on the features directly, and instead compare positives through a cross-entropy loss using cluster prototypes as a proxy. Redundancy reduction-based methods have also been popular <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b58">58]</ref>. Among them, BarlowTwins <ref type="bibr" target="#b58">[58]</ref> considers an objective function measuring the cross-correlation matrix between the features, and VicReg <ref type="bibr" target="#b2">[3]</ref> uses a mix of variance, invariance and covariance regularizations. Methods such as <ref type="bibr" target="#b18">[19]</ref> have explored the use of nearest-neighbour retrieval and divide and conquer <ref type="bibr" target="#b52">[53]</ref>. However, none of these works studied the ability of SSL methods to learn continually and adaptively. Continual Learning. A plethora of methods have been developed to counteract catastrophic forgetting <ref type="bibr">[2, 4, 9-12, 18, 22, 31, 34, 36, 38, 42, 44, 46-50, 55, 59]</ref>. Following <ref type="bibr" target="#b16">[17]</ref>, these works can be organized into three macro-categories: replay-based <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47]</ref>, regularization-based <ref type="bibr">[2, 9-11, 18, 22, 31, 34, 36, 50, 55, 59]</ref>, and parameter isolation methods <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49]</ref>. All these works evaluate the effectiveness of CL methods using a linear classifier learned sequentially over time. However, this evaluation does not reflect an important aspect, i.e., the internal dynamics of the hidden representations. Moreover, most CL methods tend to rely on supervision in order to mitigate catastrophic forgetting. A few of them can be adapted for the unsupervised setting, although their effectiveness is greatly reduced (see discussion in Sec. 5, Sec. 6 and the supplementary material).</p><p>Works such as <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b50">51]</ref> laid the foundations of unsupervised CL, but their studies are severely limited to digitlike datasets, e.g., MNIST and Omniglot, and the proposed methods are unfit for large-scale scenarios. Recently, <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b23">24]</ref> explored self-supervised pretraining for supervised continual learning with online and few-shot tasks, and <ref type="bibr" target="#b9">[10]</ref> presented a supervised contrastive CL approach. Two concurrent works <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b38">39]</ref> have also attempted to address CSSL recently. The former <ref type="bibr" target="#b36">[37]</ref> extends <ref type="bibr" target="#b9">[10]</ref> to the unsupervised setting, but is specifically designed for contrastive SSL, such as <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b27">28]</ref>, and lacks generalizability to other popular SSL paradigms. The latter <ref type="bibr" target="#b38">[39]</ref> is also limited as it only shows small-scale experiments in the class-incremental setting and considers just two SSL methods. In contrast, we present a general framework for CSSL with superior performance, conduct large-scale experiments on three challenging settings, thereby presenting a deeper analysis of CSSL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminaries</head><p>Self-Supervised Learning. The training procedure of several state-of-the-art SSL methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b58">58]</ref> can be summarized as follows. Given an image x in a batch sampled from a distribution D, two correlated views x A and x B are extracted by applying stochastic image augmentations, such as random cropping, color jittering and horizontal flipping. View x A is fed to an encoder f θ = f p • f b , which is parametrized by θ and has a backbone f b and a projection head f p , that extracts feature representations z A = f θ (x A ). Similarly, x B is forwarded into the same networks, or possibly copies thereof, updated with exponential moving average (EMA), to obtain the representation z B . A loss function L SSL is applied to these representations to learn the parameters θ as follows:</p><formula xml:id="formula_0">argmin θ E x∼D L SSL z A , z B .</formula><p>(</p><formula xml:id="formula_1">)<label>1</label></formula><p>More details on the implementation of L SSL are provided in Sec. 5.1 and Tab. 1. This procedure turns out to be extremely powerful at extracting visual representations from large unlabeled datasets. The intuition behind the success of these models is that they learn to be invariant to augmentations. Importantly, augmentations are hand-crafted in a way that the two views x A and x B contain roughly the same semantics as x, but their overall appearance (geometry, colors, resolution, etc.) is different. This forces the model map images with the same semantics to similar regions of the feature space. Interestingly, these augmentations are much stronger, i.e., they distort the image more, than augmentations commonly used to train supervised models.</p><p>Continual Learning. The CL problem focuses on training models such as deep neural networks from non-stationary data distributions. More formally, this involves a network</p><formula xml:id="formula_2">f ′ θ ′ = f ′ c • f ′ b with parameters θ ′ , backbone f ′ b and a classi- fier f ′ c</formula><p>, that learns from an ordered set of tasks {1, . . . , T }, each exhibiting a different data distribution D t . Usually, an image x sampled i.i.d. from D t is processed by f ′ that predicts a probability distribution p over the set of classes Y t . The objective is to find parameters θ ′ such as:</p><formula xml:id="formula_3">argmin θ ′ T t=1 E (x,y)∼Dt [L CL (p, y)] ,<label>(2)</label></formula><p>where, in most cases, L CL is the cross-entropy loss. However, during task t, the previous data distribution D t-1 is not available and therefore Eq. ( <ref type="formula" target="#formula_3">2</ref>) cannot be minimized directly. Current research focuses on approximating θ ′ using indirect approaches. Some of them <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b35">36]</ref> are based on knowledge distillation <ref type="bibr" target="#b29">[30]</ref>, i.e., transferring knowledge from one network to another by forcing them to produce the same outputs. We will discuss the applicability of distillation methods in CSSL in Sec. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Continual Self-Supervised Learning</head><p>In this paper, we tackle the problem of Continual Self-Supervised Learning as an extension of both SSL and CL. In practice, a CSSL experiment starts with the first task, where the model is trained as per the specific self-supervised method that it implements, with no difference from offline training. Subsequent tasks are then presented to the model sequentially, and the data from the previous tasks are discarded. No labels are provided during this training phase. For the sake of simplicity and since we are exploring a new, challenging setting, we assume task boundaries to be provided to the model. More formally, the CSSL objective is to learn a strong feature extractor that is invariant to augmentations on all tasks. Following the notation introduced in Sec. 3, we define:</p><formula xml:id="formula_4">argmin θ T t=1 E x∼Dt L SSL z A , z B .<label>(3)</label></formula><p>Note the absence of labels y when sampling from D t , the summation over the set of tasks inherited from Eq. ( <ref type="formula" target="#formula_3">2</ref>) and the SSL loss function in Eq. ( <ref type="formula" target="#formula_1">1</ref>). The expectation is approximated using stochastic gradient descent on minibatches.</p><p>Evaluation. After each task, it is possible (for evaluation purposes) to train a linear classifier on top of the obtained backbone f b . With this linear classifier we report accuracy on the test set. This protocol is compatible with standard CL metrics, as shown in Sec. 6.1. We explore three CSSL settings in our work. ▶ Class-incremental: each task t is represented by a dataset D t ∼ D t containing images that belong to a set of classes Y t such that Y t ∩ Y s = ∅ for each other task s ̸ = t. Note that the class labels are only used for splitting the dataset and they are unknown to the model. In practice, the set of classes in the dataset are shuffled and then partitioned into T tasks. Each task contains the same number of classes. ▶ Data-incremental: each task t contains a set of images D t such that D t ∩ D s = ∅ for each other task s ̸ = t. No additional constraints are imposed on the classes. In practice, the whole dataset is shuffled and then partitioned into T tasks. Each task can potentially contain all the classes. ▶ Domain-incremental: each task t contains a set of images D t drawn from a different domain. We assume that the set of classes Y t in each dataset remains the same for all tasks but the data distribution changes, as if the data were collected from different sources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">The CaSSLe Framework</head><p>We now introduce "CaSSLe", our framework for continual self-supervised learning of visual representations and detail its compatibility with several SSL methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distillation in CSSL.</head><p>From a supervised CL perspective, the concept of invariance is interesting. Here, we would like to learn representations of previously-learned semantic concepts that are invariant to the state of the model's parameters. Indeed, this idea was investigated in prior works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b30">31]</ref> that leverage knowledge distillation for CL. However, such approaches are only mildly effective in a CSSL scenario, as we show in Sec. 6. We believe this is due to CSSL being fundamentally different from supervised CL. In CSSL, we aim to extract the best possible representations that can be subsequently reused in a variety of tasks, and maximize the linear separability of features at the end of the CL phase. Thus, the linear classifier does not benefit much from the stability of the representations. Also, forcing the representations not to change may prevent the model from learning new concepts. This is especially critical for SSL methods for two reasons: (i) the performance of the models improve substantially with longer training, implying that the representations continue to get refined, and (ii) they exhibit different losses and feature normalizations that might interfere with distillation and vice-versa (e.g., BarlowTwins uses standardization while <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b30">31]</ref> use l2-normalization). Nonetheless, the features still need to be informative of previous tasks to maximize the separability of the old distribution but the current state might be too different from the pre- vious one making comparing representations complicated.</p><p>Distillation through SSL losses. Our framework, shown in Fig. <ref type="figure" target="#fig_0">2</ref>, is based on the following ideas: (i) a predictor network that maps the current state of the representations to their past state, by leveraging a distillation through time strategy that satisfies both stability and plasticity principles, and (ii) a family of adaptable distillation losses inherited from the SSL literature that solves the issue of having different objectives interfering with each other.</p><p>When a new task is received, we start by making a copy of the current model. This copy does not require gradient computation and will not be updated. We call this the frozen encoder f t-1 . As soon as an image x ∈ D t is available we apply our stochastic image augmentations and extract its features z = f t (x). In addition, we also use the frozen encoder to extract another feature vector z = f t-1 (x). Now, our goal is to ensure that z contains at least as much information as (and ideally more than) z. Instead of enforcing the two feature vectors to be similar, and hence discouraging the new model from learning new concepts, we propose to use a predictor network g to project the representations from the new feature space to the old one. If the predictor is able to perfectly map from one space to the other, then it implies that z is at least as powerful as z.</p><p>We are now ready to perform distillation, but which is the most appropriate distillation loss? Since we want the representations produced by g to be invariant to the state of the model, we propose to use the same SSL loss used to simulate invariance to augmentations. Empirically, we verify that this choice reduces interference and minimizes the need for hyperparameter tuning. We can hence write a generic distillation loss by reusing the definition of L SSL :</p><formula xml:id="formula_5">L D (z, z) = L SSL (g(z), z). (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>Note that z is always detached from the computational graph, such that the frozen encoder does not receive any gradient, and the gradient only flows through the predictor g, as prescribed in <ref type="bibr" target="#b14">[15]</ref>. On the one hand, if training converges and L D is minimized, the features predicted by g will likely be quasi-invariant to the state of the model, which satisfies the stability principle. On the other hand, the current encoder is less bound to its previous state, hence representations z can be more plastic. The loss can be extended to multiple views by applying it to both representations, i.e., L D (z A , zA ) + L D (z B , zB ), and also swapped distillation, e.g., L D (z A , zB ) and vice-versa (see ablation in Tab. 6).</p><p>The final loss of an SSL method trained continually with the CaSSLe framework is given by:</p><formula xml:id="formula_7">L = L SSL (z A , z B ) + L D (z A , zA ) = L SSL (z A , z B ) + L SSL (g(z A ), zA ).</formula><p>(</p><formula xml:id="formula_8">)<label>5</label></formula><p>This loss can be made symmetric by applying it to both the views (swapping A and B in Eq. ( <ref type="formula" target="#formula_8">5</ref>)) and it can also be easily adapted for multi-crop <ref type="bibr" target="#b6">[7]</ref>. Note that we do not use any hyperparameter to weight the importance of the distillation loss with respect to the SSL loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Compatibility of SSL methods with CaSSLe</head><p>The main difference among SSL methods is the loss function that they use. Following the notation defined in Sec. 3, and the loss functions in Tab. 1, we now detail if and how SSL losses can be used in our CaSSLe framework. Full derivation of distillation losses is deferred to the supplementary material.</p><p>InfoNCE-based methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b27">28]</ref> perform instance discrimination, where positive samples help to build invariance to augmentations. The negatives prevent the model from falling into degenerate solutions. The InfoNCE (a.k.a. contrastive) loss can be written as in Eq. ( <ref type="formula">6</ref>), where subscript i is the index of a generic sample in the batch, sim is the cosine similarity and η(i) is the set of negatives for sample i in the current batch. Distilling knowledge with this loss is equivalent to performing instance discrimination of current task samples but in the feature space learnt in the past. Thus, the predictor g learns to project samples from the present to the past space to maximize the distance with the negative samples, and the similarity with itself in the past. <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b25">26]</ref> enforce consistency among positive samples and ignore the negatives. BYOL <ref type="bibr" target="#b25">[26]</ref> uses a momentum encoder and SimSiam <ref type="bibr" target="#b14">[15]</ref> performs a stop gradient operation to avoid degenerate solutions. Since the representations are l2-normalized, their loss (Eq. 7) can be rewritten as the negative cosine similarity:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MSE-based approaches</head><formula xml:id="formula_9">-sim(q A , z B ) = -q A ||q A ||2 • z B ||z B ||2</formula><p>, where q A = h(z A ) and h is a prediction head. The gradient is backpropagated only Table <ref type="table">1</ref>. Overview of state-of-the-art SSL methods and losses. In all tables, highlight colors are coded according to the type of loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss Equation</head><p>SimCLR <ref type="bibr" target="#b12">[13]</ref> -log</p><formula xml:id="formula_10">exp(sim(z A i ,z B i )/τ) z j ∈η(i) exp(sim(z A i ,zj )/τ) (6)</formula><p>MoCo <ref type="bibr" target="#b27">[28]</ref> NNCLR <ref type="bibr" target="#b18">[19]</ref> InfoNCE</p><formula xml:id="formula_11">BYOL [26] -||q A -z B || 2 2 (7) SimSiam [15] VICReg [3] MSE SwAV [7] -d a B d log exp(sim(z A ,cd)/τ ) k exp(sim(z A ,ck)/τ ) (8)</formula><p>DCV2 <ref type="bibr" target="#b6">[7]</ref> DINO <ref type="bibr" target="#b7">[8]</ref> Cross-entropy Barlow Twins <ref type="bibr" target="#b58">[58]</ref> u (1</p><formula xml:id="formula_12">-C uv ) 2 + λ u v̸ =u C 2 uv (9) VICReg [3]</formula><p>Cross-correlation through the representations of the first argumentation. A special case of this family of methods is VICReg <ref type="bibr" target="#b2">[3]</ref>, which uses a combination of multiple losses, where MSE acts as invariance term. Features are not l2-normalized in VICReg and its predictor is the identity function. In our framework, this loss encourages the model to predict the past state of representations without additional regularization.</p><p>Cross-entropy-based. Instead of simply enforcing invariance of the representations to augmentations, cluster prototypes C = {c 1 , . . . , c K } are used as a proxy in these approaches, so that the model learns to predict invariant cluster assignments. Slight variations of this idea result in different methods: SwAV <ref type="bibr" target="#b6">[7]</ref>, DeepClusterV2 <ref type="bibr" target="#b6">[7]</ref> and DINO <ref type="bibr" target="#b7">[8]</ref>. Once a probability distribution over the prototypes is predicted, the cross-entropy loss (Eq. 8) is used to compare the two views. Features and cluster prototypes c are l2-normalized. The assignments a B can be calculated in several ways, e.g., k-means in DeepCluster, Sinkhorn-Knopp in SwAV and EMA in DINO. When employed as a distillation loss, cross-entropy encourages g to predict the assignments generated by the frozen encoder with a set of frozen prototypes:</p><formula xml:id="formula_13">a B = exp(sim(z B ,c t-1 d )/τ) k exp(sim(z B ,c t-1 k )/τ)</formula><p>, where</p><formula xml:id="formula_14">C t-1 = c t-1 1 , . . . , c t-1 K .</formula><p>Cross-correlation-based. These methods use a different approach based on decorrelating the components of the feature space, e.g., Barlow Twins <ref type="bibr" target="#b58">[58]</ref>, VICReg <ref type="bibr" target="#b2">[3]</ref> and W-MSE <ref type="bibr" target="#b19">[20]</ref>. For our analysis, we will mainly focus on Barlow Twins' implementation of this objective. Extensions to VICReg are left for future work. The crosscorrelation based objective function is shown in Eq. 9, where λ is an hyperparameter to control the importance of the first and the second terms of the loss, and</p><formula xml:id="formula_15">C uv = i z A i,u z B i,v i (z A i,u ) 2 . i (z B i,v )</formula><p>2 is the value of position (u, v) of the cross-correlation matrix computed between the representations of the views along the batch dimension. Note that the representations here are mean centered along the batch dimension, such that each unit has mean output zero over the batch. Performing distillation with this loss has the addi-tional effect of decorrelating the dimensions of the predicted features g(z A ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Experimental Protocol</head><p>Evaluation Metrics. Following previous work <ref type="bibr" target="#b37">[38]</ref>, we propose the following metrics to evaluate the quality of the representations extracted by our CSSL model: ▶ Linear Evaluation Accuracy: accuracy of a classifier trained on top of the backbone f b on all tasks (or a subset, e.g., 10% of the data) or a downstream task. For classincremental and data-incremental, we use the task-agnostic setting, meaning that at evaluation time we do not assume to know the task ID. For the domain-incremental setting, we perform both task-aware and task-agnostic evaluations (the latter is discussed in the supplementary material). To calculate the average accuracy we compute A = 1 T T i=1 A T,i , where A j,k is the linear evaluation accuracy of the model on task k after observing the last sample from task j. ▶ Forgetting: a common metric in the CL literature, it quantifies how much information the model has forgotten about previous tasks. It is formally defined as:</p><formula xml:id="formula_16">F = 1 T -1 T -1 i=1 max t∈{1,...,T } (A t,i -A T,i ).</formula><p>▶ Forward Transfer: measures how much the representations that we learned so far are helpful in learning new tasks, namely:</p><formula xml:id="formula_17">F T = 1 T -1 T i=2 A i-1,i -R i</formula><p>where R i is the linear evaluation accuracy of a random network on task i.</p><p>Datasets. We perform experiments on 3 datasets: CI-FAR100 <ref type="bibr" target="#b34">[35]</ref> (class-incremental), a 100-class dataset with 60k 32x32 colour images; ImageNet100 <ref type="bibr" target="#b53">[54]</ref> (class-and data-incremental), 100-class subset of the ILSVRC2012 dataset with ≈130k images in high resolution (resized to 224x224); DomainNet <ref type="bibr" target="#b42">[43]</ref> (domain-incremental), a 345class dataset containing roughly 600k high-resolution images (resized to 224x224) divided into 6 domains. We experiment with 5 tasks for the class-and data-incremental settings and with 6 tasks (one for each domain in Domain-Net) in the case of domain-incremental. The supplementary material presents additional results with different number of tasks. For the domain-incremental setting, we order the domains in decreasing number of images.</p><p>Implementation details. The SSL methods are adapted from solo-learn <ref type="bibr" target="#b15">[16]</ref>, an established SSL library, which is the main code base for all our experiments. The number of epochs per task is as follows: 500 for CIFAR100, 400 for ImageNet100, 200 for DomainNet. The backbone f b is a ResNet18 <ref type="bibr" target="#b28">[29]</ref>, with batch size 256. We use LARS <ref type="bibr" target="#b57">[57]</ref> for all our experiments. The offline version of each method, that serves as an upper bound, is trained for the same number of epochs as the continual counterpart for a fair comparison. All the results for offline upper bounds are ob-Table <ref type="table">2</ref>. Comparison with state-of-the-art CL methods on CI-FAR100 (5 tasks, class-incremental) using linear evaluation top-1 accuracy, forgetting and forward transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Strategy</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SimCLR Barlow Twins BYOL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A (↑) F (↓) T (↑) A (↑) F (↓) T (↑) A (↑) F (↓) T (↑)</head><p>Fine-tuning 48.9 1.0 33. <ref type="bibr" target="#b4">5</ref>  tained using the checkpoints provided in <ref type="bibr" target="#b15">[16]</ref>. For some SSL methods, it was necessary to slightly increase the learning rate over the values provided by <ref type="bibr" target="#b15">[16]</ref> in order for the methods to fully convergence in the CSSL setting. Although tuning the hyperparameters might be beneficial in some settings, we do not perform any hyperparameter tuning for CaSSLe. We also neither change the parameters of the SSL methods, nor use a weight for the distillation loss (as per Eq. ( <ref type="formula" target="#formula_8">5</ref>)).</p><p>Baselines. Most of the CL methods require labels which makes them unsuitable for CSSL. However, a few works can be adapted for our setting with minimal changes. We choose baselines from three categories <ref type="bibr" target="#b16">[17]</ref>: priorfocused regularization (EWC <ref type="bibr" target="#b33">[34]</ref>), data-focused regularization (POD <ref type="bibr" target="#b17">[18]</ref>, Less-Forget <ref type="bibr" target="#b30">[31]</ref>), and rehearsal-based replay (ER <ref type="bibr" target="#b46">[47]</ref>, DER <ref type="bibr" target="#b3">[4]</ref>) methods. We also compare with two concurrent works that propose approaches for CSSL (LUMP <ref type="bibr" target="#b38">[39]</ref>, Lin et al. <ref type="bibr" target="#b36">[37]</ref>). Finally, we do not consider methods based on VAEs <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b44">45]</ref>, since they have been shown to yield poor performance on large scale. Details on how the baselines are selected, implemented and tuned for CSSL can be found in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Results</head><p>Comparison with the state of the art. In Tab. 2 we report comparison with CL baselines and fine-tuning in composition with three SSL methods: SimCLR, Barlow Twins and BYOL. We select these three methods for the following reasons: (i) they feature different losses (InfoNCE, Crosscorrelation and MSE), (ii) they exhibit different feature normalizations (l2, standardization and mean centering), and (iii) they use different techniques to avoid collapse (neg- atives, redundancy reduction, momentum encoder). The comparison is performed on class-incremental CIFAR100 with 5 tasks. Offline learning results are reported as upper bound.</p><p>First, we notice that CaSSLe produces better representations than all the other strategies, outperforming them by large margins with all SSL methods in terms of top-1 accuracy. Moreover, our framework also shows better forward transfer, meaning that its features are easier to generalize to other tasks (also evident in Tab. 8). CaSSLe appears to reduce catastrophic forgetting with respect to fine-tuning, and is comparable to other methods. In general, SSL methods already have low forgetting with respect to supervised learning on CIFAR100 (see Tab. 4) and therefore there is little margin for improvement. However, on higher resolution images (ImageNet100) CaSSLe actually achieves remarkable results in the mitigation of catastrophic forgetting.</p><p>Replay-based methods (ER, DER) clearly do not help against forgetting in CSSL. We found two reasons for this failure. First, in supervised CL, replay-based methods benefit from storing labels, which contain a lot of information about previous tasks and enable the retraining of the linear classifier on old classes. This is not the case in CSSL, where labels are unavailable. Second, SSL models need more training epochs to converge, which means that samples in the buffer are also replayed many more times. This causes severe overfitting on these exemplars, defeating the purpose of the replay buffer. LUMP mitigates this effect by augmenting the buffer using mixup but does not reach too far, surpassing other baselines only with Barlow Twins. EWC holds up surprisingly well, outperforming more recent methods, meaning that the importance of the weights can be calculated accurately with the self-supervised loss. Distillation methods (POD, Less-Forget) show good performance. However, they use l2-normalization in their loss, causing loss of information when coupled with Barlow Twins, which decreases accuracy. Fig. <ref type="figure" target="#fig_1">3</ref> shows the evolution of top-1 linear evaluation accuracy over the whole training trajectory on classincremental CIFAR100 with 5 tasks. CaSSLe outperforms the other methods, and keeps improving throughout the sequence. We found BYOL to be unstable when simply finetuning the model. CaSSLe, EWC and Less-Forget mitigate this instability completely. On the other hand, LUMP first drops slightly and then recovers. We believe this is due to some instability introduced by the mixup regularization, to which the model takes time to adapt.</p><p>In Tab. 3 we also compare with Lin et al. <ref type="bibr" target="#b36">[37]</ref> on classincremental CIFAR100. Although our method is not specifically designed for contrastive learning, it substantially outperforms Lin et al. with 2 and 5 tasks. It is worth nothing that MoCoV2+ is slightly better than MoCoV2 (≈1% difference), whereas our gains are much larger (≈7%).</p><p>Ablation study. We ablate the most critical design choices we adopt in CaSSLe: (i) distillation without swapped views, and (ii) the presence of a prediction head g. These results are reported in Tab. 6. Our full framework clearly outperforms its variants with swapped views and without predictor. This validates our hypothesis that a predictor to map new features to the old feature space is crucial. The result that swapping views does not help is likely due to the frozen encoder not being invariant to the current task.</p><p>Class-incremental. In Tab. 4 we report a study of CSSL with 6 SSL methods in composition with the CaSSLe framework on class-incremental CIFAR100 and ImageNet100. Fine-tuning and Offline SSL results are reported as lower and upper bounds. The accuracy of supervised learning is also reported. CaSSLe always improves with respect to fine-tuning. In particular, our framework produces higher forward transfer and lower forgetting, especially on ImageNet100, where methods tend to forget more. Notably, CaSSLe outperforms supervised fine-tuning, ex- cept when coupled with VICReg on CIFAR100. On average, SSL methods trained continually with CaSSLe improve by 6.8% on CIFAR100 and 4% on ImageNet100.</p><p>Data-incremental. Tab. 7 presents results for linear evaluation top-1 accuracy on ImageNet100 with 5 tasks in a dataincremental scenario. While no SSL method is better than supervised fine-tuning, Barlow Twins coupled with CaSSLe is competitive. CaSSLe improves performance in all cases by 2% on average, except for BYOL. This is likely due to the fact that in the data-incremental scenario remembering past knowledge is less important than in other scenarios, and BYOL already has a momentum encoder that provides some information about the past. This hypothesis is validated by the fact that MoCoV2+ (that uses a momentum encoder) improves less than SimCLR when coupled with CaSSLe. We believe that, by tuning the EMA schedule, improvement could also be achieved for BYOL. In addition, BYOL already shows impressive performance with finetuning, outperforming all the other methods by more than 2%. Interestingly, SwAV comes closest to its offline upper bound, with only a 3% decrease in performance when coupled with CaSSLe.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Domain-incremental.</head><p>We also examine the capability of CaSSLe to learn continually when the domain from which the data is drawn changes. Tab. 7 shows the average top-1 accuracy of a linear classifier trained on top of the frozen feature extractor on all domains separately (domain-aware). Domain-agnostic evaluation and results for each domain are presented in the supplementary material. Again, CaSSLe improves every method by 4.4% on average, showing that our distillation strategy is robust to domain shift, and although the data distribution is really different, information transfer is still performed. Interestingly, most of the methods, when trained with CaSSLe get very close to their offline accuracy.</p><p>Long training vs continual training. We also analyze the following question: is it worth training continually or is it better to train for longer on a small dataset? This depends on two factors: (i) the SSL method, and (ii) the CSSL setting. For SimCLR and Barlow Twins in the classincremental setting it seems to be better to train offline on 1/5 of the classes instead of training continually with 5 tasks. In this setting, offline BYOL seems to suffer from instability, ending up lower than fine-tuning. On the other hand, on the data-incremental setting, fine-tuning outperforms longer training, especially for BYOL, which also outperforms CaSSLe (as explained previously). Apart from this exception, CaSSLe always produces better representations than other strategies, making it the go-to option.</p><p>Downstream and semi-supervised. In Tab. 8, we present the downstream performance of CaSSLe compared with fine-tuning when trained on ImageNet100 and evaluated on DomainNet (Real). Barlow Twins, SwAV and BYOL show higher performance than the supervised model, even when considering a fine-tuning strategy. This is probably due to the fact that SSL methods tend to learn more general features than their supervised counterparts. CaSSLe improves performance on all the SSL methods, making them surpass the supervised baseline. Lastly, when compared with finetuning, CaSSLe improves the performance of SSL methods by 3.4% on average. Tab. 9 contains the top-1 accuracy on ImageNet100 when training a linear classifier on a frozen backbone with limited amount of labels (10% and 1%). First, we can observe that no SSL method with fine-tune surpasses the performance of supervised learning. When using CaSSLe, MoCoV2+ outperforms supervised with 10% labels and, in general, Barlow Twins and Mo-CoV2+ work best in both semi-supervised settings. CaSSLe improves all SSL methods when compared with fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this work, we study Continual Self-Supervised Learning (CSSL), the problem of learning a set of tasks without labels continually. We make two important contributions for the SSL and CL communities: (i) we present CaSSLe, a simple and effective framework for CSSL that shows how SSL methods and losses can be seamlessly reused to learn continually, and (ii) we perform a comprehensive analysis of CSSL, leading to the emergence of interesting properties of SSL methods.</p><p>Limitations. Although CaSSLe shows exciting performance, it has some limitations. First, it is applicable in settings where task boundaries are provided. Second, our framework increases the amount of computational resources needed for training by roughly 30%, both in terms of memory and time. Finally, CaSSLe does not perform clustering, meaning that it is unable to directly learn a mapping from data to latent classes, and thus needs either a linear classifier trained with supervision, or some clustering algorithm.</p><p>Broader impact. The capabilities of supervised CL agents are bounded by the need for human-produced annotations. CSSL models can potentially improve without the need for human supervision. This facilitates the creation of powerful AIs that may be used for malicious purposes such as discrimination and surveillance. Also, since in CSSL the data is supposed to come from a non-curated stream, the model may be affected by biases in the data. This is problematic because biases are then be transferred to downstream tasks. MSE based. This distillation loss is simply the MSE between the predicted features and the frozen features:</p><formula xml:id="formula_18">L(z, z) = -||g(z) -z|| 2 2 .<label>(12)</label></formula><p>It can be implemented with the cosine similarity as stated in the main manuscript.</p><p>Cross-entropy based. The cross-entropy loss, when used for distillation in an unsupervised setting, makes sure that the current encoder is able to assign samples to the frozen centroids (or prototypes) consistently with the frozen encoder:</p><formula xml:id="formula_19">L(z, z) = - d ād log exp sim g(z), c t-1 d /τ k exp sim g(z), c t-1 k /τ<label>(13)</label></formula><p>where:</p><formula xml:id="formula_20">ā = exp sim z, c t-1 d /τ k exp sim z, c t-1 k /τ ,<label>(14)</label></formula><p>and the set of frozen prototypes is denoted as follows:</p><formula xml:id="formula_21">C t-1 = c t-1 1 , . . . , c t-1 K .</formula><p>Cross-correlation based. We consider Barlow Twins' <ref type="bibr" target="#b58">[58]</ref> implementation of this objective. For VICReg <ref type="bibr" target="#b2">[3]</ref> we only consider the invariance term. As a distillation loss, the crosscorrelation matrix is computed with the predicted and frozen features:</p><formula xml:id="formula_22">L(z, z) = u 1 -Cuv 2 + λ u v̸ =u C2 uv ,<label>(15)</label></formula><p>where:</p><formula xml:id="formula_23">Cuv = i g(zi,u)zi,v i g (zi,u) 2 . i (zi,v) 2 . (<label>16</label></formula><formula xml:id="formula_24">)</formula><p>C. Further discussion and implementation details of the baselines Selection. When evaluating our framework, we try to compare with as many existing related methods as possible. However, given that SSL models are computationally intensive, it was not possible to run all baselines and methods in all the CL settings we considered. As mentioned in the main manuscript, we choose eight baselines (seven related methods + fine-tuning) belonging to three CL macro-categories, and test them on CIFAR100 (class-incremental) in combination with three SSL methods. The selection was based on the ease of adaptation to CSSL and the similarity to our framework.</p><p>The most similar to CaSSLe are data-focused regularization methods. Among them, a large majority leverage knowledge distillation using the outputs of a classifier learned with supervision e.g. <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b35">36]</ref>, while a few works employ feature distillation <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b30">31]</ref> which is viable even without supervision. <ref type="bibr" target="#b31">[32]</ref> is also related to CaSSLe, but it focuses on memory efficiency which is less interesting in our setting. Also, <ref type="bibr" target="#b31">[32]</ref> explicitly uses the classifier after feature adaptation, hence it is unclear how to adapt it</p><p>Table A. Linear evaluation top-1 accuracy on DomainNet (6 tasks, domain-incremental setting) w/ and w/o CaSSLe. The sequence of tasks is Real→Quickdraw→Painting→Sketch→Infograph→Clipart. "Aw." stands for task-aware, "Ag," for task-agnostic. Method Strategy Real Quickdraw Painting Sketch Infograph Clipart Avg.</p><p>for CSSL, especially since in SSL positives are generated using image augmentations, which are not applicable to a memory bank of features. On the contrary, augmentations can be used in replay methods, among which we select the most common (ER <ref type="bibr" target="#b46">[47]</ref>) and one of the most recent (DER <ref type="bibr" target="#b3">[4]</ref>). Regarding prior-focused regularization methods, we choose EWC <ref type="bibr" target="#b33">[34]</ref> over others (SI <ref type="bibr" target="#b59">[59]</ref>, MAS <ref type="bibr" target="#b1">[2]</ref>, etc.) as it is considered the most influential and it works best with task boundaries. We also consider two CSSL baselines: LUMP <ref type="bibr" target="#b38">[39]</ref> and Lin et al. <ref type="bibr" target="#b36">[37]</ref>. Finally, we do not consider methods based on VAEs <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b44">45]</ref>, since they have been shown to yield poor performance in the large and medium scale. For instance, as found by <ref type="bibr" target="#b20">[21]</ref>, a VAE trained offline on CIFAR10 reaches an accuracy of 57.2%, which is lower than any method (except VICReg) trained continually on CIFAR100 with CaSSLe.</p><p>Implementation. For EWC, we use the SSL loss instead of the supervised loss to estimate importance weights. For POD and Less-Forget, we only re-implement the feature distillation without considering the parts of their methods that explicitly use the classifier. For DER, we replace the logits of the classifier with the projected features in the buffer. We re-implement all these baselines by adapting them from the official implementation (POD), or from the Mammoth framework provided with <ref type="bibr" target="#b3">[4]</ref> (DER, ER, EWC), or from the paper (Less-Forget). We also compare with two concurrent works that propose approaches for CSSL (LUMP <ref type="bibr" target="#b38">[39]</ref>, Lin et al. <ref type="bibr" target="#b36">[37]</ref>). LUMP uses k-NN evaluation, therefore we adapt the code provided by the authors to run in our code base. For Lin et al., we compare directly with their published results, since they use the same evaluation protocol. We perform hyperparameter tuning for all baselines, searching over 5 values for the distillation loss weights of POD and Less-Forget, 3 values for the weight of the regularization in EWC and 3 replay batch sizes for replay methods. The size of the replay buffer is 500 samples for all replay based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Additional results</head><p>Continual supervised contrastive with CaSSLe. After the popularization of contrastive learning <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b27">28]</ref> for unsupervised learning of representations, <ref type="bibr" target="#b32">[33]</ref> proposed a supervised version of the contrastive loss. Here, we show that CaSSLe is easily extendable to support supervised contrastive learning. The implementation is basically the same as for our vanilla contrastive-based distillation loss. In Tab. B, we show the improvement that CaSSLe brings with respect to fine-tuning, which is sizeable in the classincremental setting. We also report the same comparison on Do-mainNet in Tab. A, showing interesting results in both task-aware and task-incremental evaluation.</p><p>Task-agnostic evaluation and domain-wise accuracy on DomainNet. In the main manuscript, we showed that CaSSLe significantly improved performance in the domain-incremental setting using task-aware evaluation. Here, "task-aware" refers to</p><p>Table B. Linear evaluation top-1 accuracy on ImageNet100 (5 tasks, class-and data-incremental). Method Strategy ImageNet100 Class-inc. Data-inc. Supervised Contrastive Fine-tuning 61.6 74.3 CaSSLe 69.6 76.9 Table C. k-NN evaluation on ImageNet100 (5 tasks, classincremental) performed on backbone and projected features. Method Strategy k-NN accuracy (↑) Backbone (f b ) Projector (fp) Barlow Twins Fine-tuning 59.1 34.4 CaSSLe 63.4 53.2 SwAV Fine-tuning 60.0 53.9 CaSSLe 59.7 61.3 BYOL Fine-tuning 57.1 33.0 CaSSLe 61.2 60.8 VICReg Fine-tuning 56.7 35.3 CaSSLe 59.5 43.4 MoCoV2+ Fine-tuning 54.5 39.0 CaSSLe 61.5 53.1 SimCLR Fine-tuning 54.8 40.1 CaSSLe 61.7 53.2</p><p>the fact that linear evaluation is performed on each domain separately, i.e. a different linear classifier is learned for each domain. However, it might also be interesting to check the performance of the model when the domain is unknown at test time. For this reason, we report the performance of our model when evaluated in a task-agnostic fashion. In addition, we also show the accuracy on each task (i.e. domain). All this information is presented in Tab. A. CaSSLe always outperforms fine-tuning with both evaluation protocols. The accuracy of CaSSLe on "Clipart" is also higher than offline. This is probably due to a combination of factors: (i) Clipart is the last task, therefore it probably benefits in forward transfer and (ii) a similar effect to the one found in <ref type="bibr" target="#b52">[53]</ref>, where dividing data in subgroups tends to enable the learning of better representations. Also, we notice that task-agnostic accuracy is lower than the task-aware counterpart. This is expected and means that the class conditional distributions are not perfectly aligned in different domains. As in the main paper, the colors are related to the type of SSL loss.</p><p>Additional results with k-NN evaluation. For completeness, in this supplementary material, we also show that CaSSLe yields superior performance when evaluated with a k-NN classifier instead of linear evaluation. We use weighted k-NN with l2normalization (cosine similarity) and temperature scaling as in <ref type="bibr" target="#b7">[8]</ref>. Since since k-NN is much faster than linear evaluation we could also assess the quality of the projected representations, instead of just using the backbone. The results can be inspected in Tab. C. Three interesting phenomena arise: (i) CaSSLe always improves with respect to fine-tuning, (ii) the features of the backbone f b are usually better than the features of the projector fp and (iii) CaSSLe causes information retention in the projector, which significantly increases the performance of the projected features. An excep-</p><p>Table D. Linear evaluation top-1 accuracy on CIFAR100 (10 tasks, class-incremental). Method Strategy A (↑) SimCLR Fine-tuning 39.3 CaSSLe 52.7 Barlow Twins Fine-tuning 49.9 CaSSLe 53.7 Table E. Linear evaluation top-1 accuracy on ImageNet100 (5 tasks, class-and data-incremental) with ResNet50 [29]. Method Strategy A (↑) Class-inc. Data-inc. SimCLR Fine-tuning 70.7 75.6 CaSSLe 74.0 77.2 Barlow Twins Fine-tuning 71.2 75.8 CaSSLe 74.8 78.1</p><p>tion is represented by SwAV <ref type="bibr" target="#b6">[7]</ref>, that seems to behave differently to other methods. First, the accuracy of the projected features in SwAV is much higher than other methods. This might be due to the fact that it uses prototypes, which bring the representations 1 layer away from the loss, making them less specialized in the SSL task. Second, it seems that CaSSLe only improves the projected features when coupled with SwAV. However, this is probably an artifact of the evaluation procedure, as the l2-normalization probably causes loss of information. Indeed, although the overall performance is lower, SwAV + CaSSLe outperforms SwAV + fine-tuning (58.7% vs 56.9%) if the euclidean distance is used in place of the cosine similarity for the backbone features. We leave a deeper investigation of this phenomenon for future work.</p><p>Different number of tasks. The analysis of CSSL settings that we show in the main manuscript is limited to the 5 task scenario. However, it is interesting to run the same benchmarks with a longer task sequence. Nonetheless, one should also remember that SSL methods are data hungry, hence the less data is available per task, the higher the instability of the SSL models. In Tab. D, we present additional results with 10 tasks on CIFAR100 (classincremental). Barlow Twins seems to hold up surprisingly well, finishing up at roughly 50% accuracy, while SimCLR suffers in the low data regime. Nonetheless, CaSSLe outperforms fine-tuning with Barlow Twins, and to a very large extent with SimCLR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deeper architectures.</head><p>The experiments we propose in the main manuscript feature a ResNet18 network. This is a common choice in CL. However, in SSL, it is more common to use ResNet50. For this reason, in Tab. E we show that the same behavior observed with smaller networks is also obtained with deeper architectures. More specifically, CaSSLe outperforms fine-tuning in both class-and data-incremental settings by large margins.</p><p>The role of the predictor. In the main manuscript, we provided an intuitive explanation of the role of the predictor network that maps the current feature space to the frozen feature space.</p><p>Table F. Combinations of SSL methods and distillation losses on CIFAR100 (class-incremental, 2 tasks). Distillation Loss SimCLR Barlow Twins BYOL InfoNCE 61.8 64.5 64.8 Cross-correlation 60.1 67.2 65.8 MSE 61.3 64.6 66.7</p><p>This intuition is corroborated by extensive experimentation and ablation studies. However, one more thing that is worth mentioning is that the success of the predictor network might also be related to the findings in SimSiam <ref type="bibr" target="#b14">[15]</ref>, BYOL <ref type="bibr" target="#b25">[26]</ref> and Direct-Pred <ref type="bibr" target="#b51">[52]</ref>. Moreover, we perform additional ablations on the design of CaSSLe's predictor for SimCLR on CIFAR100 (5 tasks): adding BatchNorm after the hidden layer does not make any difference in terms of performance, and removing the non-linearity only causes a 0.3% drop in accuracy.</p><p>Combinations of SSL methods and distillation losses.</p><p>For computational reasons, it was not feasible to perform experiments combing all SSL methods with all possible distillation losses. However, in Tab. F we provide a subset of the possible combinations to validate our strategy that uses the same SSL loss for distillation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Overview of the CaSSLe framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Evolution of top-1 linear evaluation accuracy over tasks on CIFAR100 (5 tasks, class-incremental).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparison with Lin et al.<ref type="bibr" target="#b36">[37]</ref> on CIFAR100 (2 and 5 tasks, class-incremental setting). MoCoV2+ is an updated version of MoCoV2 that uses a symmetric loss. The difference between the two is ≈1% at convergence<ref type="bibr" target="#b14">[15]</ref>.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">54.3 0.4 39.2 52.7 0.1 35.9</cell></row><row><cell>EWC [34]</cell><cell cols="9">53.6 0.0 33.3 56.7 0.2 39.1 56.4 0.0 39.9</cell></row><row><cell>ER [47]</cell><cell cols="9">50.3 0.1 32.7 54.6 3.0 39.4 54.7 0.4 36.3</cell></row><row><cell>DER [4]</cell><cell cols="9">50.7 0.4 33.2 55.3 2.5 39.6 54.8 1.1 36.7</cell></row><row><cell>LUMP [39]</cell><cell cols="9">52.3 0.3 34.5 57.8 0.3 41.0 56.4 0.2 37.9</cell></row><row><cell cols="10">Less-Forget [31] 52.5 0.2 33.8 56.4 0.2 40.1 58.6 0.2 41.1</cell></row><row><cell>POD [18]</cell><cell cols="9">51.3 0.1 33.8 55.9 0.3 40.3 57.9 0.0 41.1</cell></row><row><cell>CaSSLe</cell><cell cols="9">58.3 0.2 36.4 60.4 0.4 42.2 62.2 0.0 43.6</cell></row><row><cell>Offline</cell><cell>65.8</cell><cell>-</cell><cell>-</cell><cell>70.9</cell><cell>-</cell><cell>-</cell><cell>70.5</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Strategy</cell><cell></cell><cell cols="2">Method</cell><cell>2 Tasks</cell><cell></cell><cell>5 Tasks</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Lin et al. [37]</cell><cell></cell><cell cols="2">SimCLR MoCoV2</cell><cell>55.7 56.1</cell><cell></cell><cell>-53.8</cell><cell></cell><cell></cell></row><row><cell></cell><cell>CaSSLe</cell><cell></cell><cell cols="2">SimCLR MoCoV2+</cell><cell>61.8 63.3</cell><cell></cell><cell>58.3 59.5</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Linear evaluation top-1 accuracy on class-incremental CIFAR100 and ImageNet100 with 5 tasks. CaSSLe is compared to fine-tuning, offline and supervised learning.</figDesc><table><row><cell>Method</cell><cell>Strategy</cell><cell></cell><cell>CIFAR100</cell><cell></cell><cell cols="3">ImageNet100</cell></row><row><cell></cell><cell></cell><cell cols="6">A (↑) F (↓) T (↑) A (↑) F (↓) T (↑)</cell></row><row><cell></cell><cell>Fine-tuning</cell><cell>54.3</cell><cell>0.4</cell><cell>39.2</cell><cell>63.1</cell><cell>10.7</cell><cell>44.4</cell></row><row><cell>Barlow</cell><cell>CaSSLe</cell><cell>60.4</cell><cell>0.4</cell><cell>42.2</cell><cell>68.2</cell><cell>1.3</cell><cell>47.9</cell></row><row><cell>Twins</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Offline</cell><cell>70.9</cell><cell>-</cell><cell>-</cell><cell>80.4</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Fine-tuning</cell><cell>55.5</cell><cell>0.0</cell><cell>32.8</cell><cell>64.4</cell><cell>4.3</cell><cell>42.8</cell></row><row><cell>SwAV</cell><cell>CaSSLe</cell><cell>57.8</cell><cell>0.0</cell><cell>34.5</cell><cell>66.0</cell><cell>0.2</cell><cell>43.6</cell></row><row><cell></cell><cell>Offline</cell><cell>64.9</cell><cell>-</cell><cell>-</cell><cell>74.3</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Fine-tuning</cell><cell>52.7</cell><cell>0.1</cell><cell>35.9</cell><cell>66.0</cell><cell>2.9</cell><cell>43.2</cell></row><row><cell>BYOL</cell><cell>CaSSLe</cell><cell>62.2</cell><cell>0.0</cell><cell>42.2</cell><cell>66.4</cell><cell>1.1</cell><cell>46.6</cell></row><row><cell></cell><cell>Offline</cell><cell>70.5</cell><cell>-</cell><cell>-</cell><cell>80.3</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Fine-tuning</cell><cell>51.5</cell><cell>0.9</cell><cell>36.4</cell><cell>61.3</cell><cell>7.9</cell><cell>42.0</cell></row><row><cell>VICReg</cell><cell>CaSSLe</cell><cell>53.6</cell><cell>0.2</cell><cell>41.1</cell><cell>64.8</cell><cell>4.3</cell><cell>45.3</cell></row><row><cell></cell><cell>Offline</cell><cell>68.5</cell><cell>-</cell><cell>-</cell><cell>79.4</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Fine-tuning</cell><cell>47.3</cell><cell>0.2</cell><cell>33.4</cell><cell>62.0</cell><cell>8.4</cell><cell>41.6</cell></row><row><cell>MoCoV2+</cell><cell>CaSSLe</cell><cell>59.5</cell><cell>0.0</cell><cell>39.6</cell><cell>68.8</cell><cell>1.5</cell><cell>46.8</cell></row><row><cell></cell><cell>Offline</cell><cell>69.9</cell><cell>-</cell><cell>-</cell><cell>79.3</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Fine-tuning</cell><cell>48.9</cell><cell>1.0</cell><cell>33.5</cell><cell>61.5</cell><cell>8.1</cell><cell>40.3</cell></row><row><cell>SimCLR</cell><cell>CaSSLe</cell><cell>58.3</cell><cell>0.2</cell><cell>36.4</cell><cell>68.0</cell><cell>2.2</cell><cell>45.8</cell></row><row><cell></cell><cell>Offline</cell><cell>65.8</cell><cell>-</cell><cell>-</cell><cell>77.5</cell><cell>-</cell><cell>-</cell></row><row><cell>Supervised</cell><cell>Fine-tuning</cell><cell>54.1</cell><cell>6.8</cell><cell>36.5</cell><cell>63.1</cell><cell>5.6</cell><cell>42.5</cell></row><row><cell></cell><cell>Offline</cell><cell>75.6</cell><cell>-</cell><cell>-</cell><cell>81.9</cell><cell>-</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Training 5 times longer on 1/5 of the data vs. training continually w/ and w/o CaSSLe on ImageNet100 (5 tasks, classand data-incremental). Bold is best, underlined is second best.</figDesc><table><row><cell>Setting</cell><cell>Method</cell><cell>Fine-tune</cell><cell>Offline 1/5</cell><cell>CaSSLe</cell></row><row><cell></cell><cell>SimCLR</cell><cell>61.5</cell><cell>63.1</cell><cell>68.0</cell></row><row><cell>Class-inc.</cell><cell>Barlow Twins</cell><cell>63.1</cell><cell>63.5</cell><cell>68.2</cell></row><row><cell></cell><cell>BYOL</cell><cell>66.0</cell><cell>60.6</cell><cell>66.4</cell></row><row><cell></cell><cell>SimCLR</cell><cell>68.9</cell><cell>67.2</cell><cell>72.1</cell></row><row><cell>Data-inc.</cell><cell>Barlow Twins</cell><cell>71.3</cell><cell>70.2</cell><cell>74.9</cell></row><row><cell></cell><cell>BYOL</cell><cell>74.0</cell><cell>66.7</cell><cell>73.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Ablation study of design choices in CaSSLe.</figDesc><table><row><cell>Strategy</cell><cell>Method</cell><cell>Swap</cell><cell>No pred.</cell><cell>Ours</cell></row><row><cell></cell><cell>SimCLR</cell><cell>49.3</cell><cell>52.6</cell><cell>58.3</cell></row><row><cell>CaSSLe</cell><cell>Barlow Twins</cell><cell>57.4</cell><cell>57.3</cell><cell>60.4</cell></row><row><cell></cell><cell>BYOL</cell><cell>52.0</cell><cell>58.6</cell><cell>62.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>Linear evaluation accuracy on ImageNet100 (5 tasks, data-incremental) and DomainNet (6 tasks, domain-incremental).</figDesc><table><row><cell>Method</cell><cell>Strategy</cell><cell>ImageNet100 (Data-inc.)</cell><cell>DomainNet (Domain-inc.)</cell></row><row><cell></cell><cell>Fine-tuning</cell><cell>71.3</cell><cell>50.3</cell></row><row><cell>Barlow</cell><cell>CaSSLe</cell><cell>74.9</cell><cell>55.5</cell></row><row><cell>Twins</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Offline</cell><cell>80.4</cell><cell>57.2</cell></row><row><cell></cell><cell>Fine-tuning</cell><cell>70.8</cell><cell>49.6</cell></row><row><cell>SwAV</cell><cell>Knowledge</cell><cell>71.3</cell><cell>54.3</cell></row><row><cell></cell><cell>Offline</cell><cell>74.3</cell><cell>54.6</cell></row><row><cell></cell><cell>Fine-tuning</cell><cell>74.0</cell><cell>50.6</cell></row><row><cell>BYOL</cell><cell>CaSSLe</cell><cell>73.3</cell><cell>55.1</cell></row><row><cell></cell><cell>Offline</cell><cell>80.3</cell><cell>56.6</cell></row><row><cell></cell><cell>Fine-tuning</cell><cell>70.2</cell><cell>49.3</cell></row><row><cell>VICReg</cell><cell>CaSSLe</cell><cell>72.3</cell><cell>52.9</cell></row><row><cell></cell><cell>Offline</cell><cell>79.4</cell><cell>56.7</cell></row><row><cell></cell><cell>Fine-tuning</cell><cell>69.5</cell><cell>43.2</cell></row><row><cell>MoCoV2+</cell><cell>CaSSLe</cell><cell>71.9</cell><cell>46.7</cell></row><row><cell></cell><cell>Offline</cell><cell>78.2</cell><cell>53.7</cell></row><row><cell></cell><cell>Fine-tuning</cell><cell>68.9</cell><cell>45.1</cell></row><row><cell>SimCLR</cell><cell>CaSSLe</cell><cell>72.1</cell><cell>50.0</cell></row><row><cell></cell><cell>Offline</cell><cell>77.5</cell><cell>52.6</cell></row><row><cell>Supervised</cell><cell>Fine-tuning</cell><cell>75.9</cell><cell>55.9</cell></row><row><cell></cell><cell>Offline</cell><cell>81.9</cell><cell>66.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 .</head><label>8</label><figDesc>Downstream performance with different SSL methods trained on Imagenet-100 and evaluated on DomainNet (Real).</figDesc><table><row><cell>Strategy</cell><cell>Barlow Twins</cell><cell>SwAV</cell><cell>BYOL</cell><cell cols="4">VICReg MoCoV2+ SimCLR Supervised</cell></row><row><cell>Fine-tune CaSSLe</cell><cell>56.2 60.3</cell><cell>55.9 56.9</cell><cell>55.0 56.9</cell><cell>54.0 56.3</cell><cell>52.4 58.7</cell><cell>51.6 56.5</cell><cell>54.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 .</head><label>9</label><figDesc>Top-1 linear accuracy on Imagenet-100 with different SSL methods, semi-supervised setting with 10% and 1% of labels.</figDesc><table><row><cell cols="2">Percentage Strategy</cell><cell>Barlow Twins</cell><cell>SwAV</cell><cell>BYOL</cell><cell cols="4">VICReg MoCoV2+ SimCLR Supervised</cell></row><row><cell>10%</cell><cell>Fine-tune CaSSLe</cell><cell>56.6 60.3</cell><cell>57.6 58.2</cell><cell>55.7 56.5</cell><cell>53.6 56.5</cell><cell>54.9 61.7</cell><cell>52.5 58.9</cell><cell>60.8</cell></row><row><cell>1%</cell><cell>Fine-tune CaSSLe</cell><cell>42.6 47.0</cell><cell>42.5 43.1</cell><cell>42.3 43.4</cell><cell>40.4 43.2</cell><cell>40.9 47.8</cell><cell>39.7 46.8</cell><cell>48.1</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was supported by the <rs type="funder">European Institute of Innovation &amp; Technology (EIT)</rs> and the <rs type="funder">H2020</rs> <rs type="projectName">EU</rs> project <rs type="projectName">SPRING</rs>, funded by the <rs type="funder">European Commission</rs> under GA 871245. It was carried out under the "<rs type="programName">Vision and Learning joint Laboratory" between FBK</rs> and <rs type="funder">UNITN</rs>. <rs type="person">Karteek Alahari</rs> was funded by the <rs type="funder">ANR</rs> grant <rs type="projectName">AVENUE</rs> (<rs type="grantNumber">ANR-18-CE23-0011</rs>). <rs type="person">Julien Mairal</rs> was funded by the <rs type="funder">ERC</rs> grant number <rs type="grantNumber">714381</rs> (<rs type="projectName">SO-LARIS</rs> project) and by <rs type="funder">ANR</rs> <rs type="grantNumber">3IA</rs> <rs type="projectName">MIAI@Grenoble Alpes</rs> (<rs type="grantNumber">ANR-19-P3IA-0003</rs>). Xavier Alameda-Pineda was funded by the <rs type="funder">ARN</rs> grant <rs type="grantNumber">ML3RI</rs> (<rs type="grantNumber">ANR-19-CE33-0008-01</rs>). This project was granted access to the HPC resources of IDRIS under the allocation <rs type="grantNumber">2021-[AD011013084</rs>] made by <rs type="funder">GENCI</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_UztgsDc">
					<orgName type="project" subtype="full">EU</orgName>
				</org>
				<org type="funded-project" xml:id="_9PURANy">
					<orgName type="project" subtype="full">SPRING</orgName>
					<orgName type="program" subtype="full">Vision and Learning joint Laboratory&quot; between FBK</orgName>
				</org>
				<org type="funded-project" xml:id="_JKFupK2">
					<idno type="grant-number">ANR-18-CE23-0011</idno>
					<orgName type="project" subtype="full">AVENUE</orgName>
				</org>
				<org type="funded-project" xml:id="_y36GDJk">
					<idno type="grant-number">714381</idno>
					<orgName type="project" subtype="full">SO-LARIS</orgName>
				</org>
				<org type="funded-project" xml:id="_Fj5fJFz">
					<idno type="grant-number">3IA</idno>
					<orgName type="project" subtype="full">MIAI@Grenoble Alpes</orgName>
				</org>
				<org type="funding" xml:id="_VUPfXvQ">
					<idno type="grant-number">ANR-19-P3IA-0003</idno>
				</org>
				<org type="funding" xml:id="_PNRrp4R">
					<idno type="grant-number">ML3RI</idno>
				</org>
				<org type="funding" xml:id="_C3HggMh">
					<idno type="grant-number">ANR-19-CE33-0008-01</idno>
				</org>
				<org type="funding" xml:id="_RYgNq6N">
					<idno type="grant-number">2021-[AD011013084</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. PyTorch-like pseudo-code</head><p>We provide a PyTorch-like pseudo-code of our method. As you can see, CaSSLe is simple to implement and does not add much complexity to the base SSL method. In this snippet, the losses are made symmetric by summing the two contributions. In some cases, the two losses are averaged instead. In CaSSLe, we symmetrize in the same way as the base SSL method we are considering.</p><p>Algorithm 1 PyTorch-like pseudo-code for CaSSLe. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Derivation of distillation losses</head><p>In this section, we derive distillation losses from the SSL losses in Tab. 1 of the main paper, starting from the definition of our distillation loss:</p><p>where z and z are the representations of the current and frozen encoder, and g is CaSSLe's predictor network implemented as a two layer MLP with 2048 hidden neurons and ReLU activation.</p><p>Contrastive based. Our distillation loss based on contrastive learning is implemented as follows:</p><p>L(zi, zi) = -log exp (sim (zi, zi) /τ )</p><p>where η(i) is the set of negatives for the sample with index i in the batch. Note that the negatives are drawn both from the predicted and frozen features.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Life-long disentangled representation learning with cross-domain latent homologies</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Achille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Eccles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irina</forename><surname>Lerchner</surname></persName>
		</author>
		<author>
			<persName><surname>Higgins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Memory aware synapses: Learning what (not) to forget</title>
		<author>
			<persName><forename type="first">Rahaf</forename><surname>Aljundi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesca</forename><surname>Babiloni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Vicreg: Variance-invariance-covariance regularization for selfsupervised learning</title>
		<author>
			<persName><forename type="first">Adrien</forename><surname>Bardes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.04906</idno>
		<imprint>
			<date type="published" when="2009">2021. 1, 2, 5, 9</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Dark experience for general continual learning: a strong, simple baseline</title>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Buzzega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Boschini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelo</forename><surname>Porrello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Abati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Calderara</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note>In NeurIPS, 2020. 2, 6</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Special: Selfsupervised pretraining for continual learning</title>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Caccia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09065</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="132" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020. 1, 2, 4, 5, 11</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2021">2021. 1, 2, 5, 11</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">End-to-end incremental learning</title>
		<author>
			<persName><forename type="first">Manuel</forename><forename type="middle">J</forename><surname>Francisco M Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Marin-Jimenez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Guil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karteek</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><surname>Alahari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Co2l: Contrastive continual learning</title>
		<author>
			<persName><forename type="first">Hyuntak</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaeho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9516" to="9525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Riemannian walk for incremental learning: Understanding forgetting and intransigence</title>
		<author>
			<persName><forename type="first">Arslan</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Puneet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thalaiyasingam</forename><surname>Dokania</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip Hs</forename><surname>Ajanthan</surname></persName>
		</author>
		<author>
			<persName><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient lifelong learning with agem</title>
		<author>
			<persName><forename type="first">Arslan</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc'aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020">2020. 1, 2, 4, 5, 10</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Improved baselines with momentum contrastive learning</title>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021">2021. 2, 4, 5, 6, 12</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Moin Nabi, Nicu Sebe, and Elisa Ricci. solo-learn: A library of selfsupervised methods for visual representation learning</title>
		<author>
			<persName><forename type="first">Guilherme Turrisi Da</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enrico</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><surname>Fini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">56</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Continual learning: A comparative study on how to defy forgetting in classification tasks</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>De Lange</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahaf</forename><surname>Aljundi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Masana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Parisot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ales</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Slabaugh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2006">2019. 1, 2, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Podnet: Pooled outputs distillation for small-tasks incremental learning</title>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Douillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Ollion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduardo</forename><surname>Valle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2009">2020. 2, 3, 6, 9</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">With a little help from my friends: Nearest-neighbor contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">Debidatta</forename><surname>Dwibedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Whitening for self-supervised representation learning</title>
		<author>
			<persName><forename type="first">Aleksandr</forename><surname>Ermolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aliaksandr</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enver</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">William</forename><surname>Falcon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananya</forename><surname>Harsh Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teddy</forename><surname>Koker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.12329</idno>
		<title level="m">Aavae: Augmentation-augmented variational autoencoders</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Online continual learning under extreme memory constraints</title>
		<author>
			<persName><forename type="first">Enrico</forename><surname>Fini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stèphane</forename><surname>Lathuilière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enver</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moin</forename><surname>Nabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Catastrophic forgetting in connectionist networks</title>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">M</forename><surname>French</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="128" to="135" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Self-supervised training enhances online continual learning</title>
		<author>
			<persName><forename type="first">Jhair</forename><surname>Gallardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyler</forename><forename type="middle">L</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Kanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14010</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">An empirical investigation of catastrophic forgetting in gradient-based neural networks</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6211</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florent</forename><surname>Altché</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Pierre H Richemond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaohan</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Daniel Guo</surname></persName>
		</author>
		<author>
			<persName><surname>Gheshlaghi Azar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020. 1, 2, 4, 5, 12</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020">2020. 1, 2, 4, 5, 10</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning a unified classifier incrementally via rebalancing</title>
		<author>
			<persName><forename type="first">Saihui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zilei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2019. 2, 3, 6, 9</date>
			<biblScope unit="page" from="831" to="839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Memory-efficient incremental learning through feature adaptation</title>
		<author>
			<persName><forename type="first">Ahmet</forename><surname>Iscen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="699" to="715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Prannay</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Supervised contrastive learning. NeurIPS</title>
		<imprint>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kieran</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agnieszka</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the national academy of sciences</title>
		<meeting>of the national academy of sciences</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>Univ. Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning without forgetting</title>
		<author>
			<persName><forename type="first">Zhizhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Continual contrastive self-supervised learning for image classification</title>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongtao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxiang</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.01776</idno>
		<imprint>
			<date type="published" when="2021">2021. 2, 6, 7, 10</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Gradient episodic memory for continual learning</title>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-</forename><surname>Paz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc-Aurelio</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Rethinking the representational continuity: Towards unsupervised continual learning</title>
		<author>
			<persName><forename type="first">Divyam</forename><surname>Madaan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaehong</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanchun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunxin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.06976</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Catastrophic interference in connectionist networks: The sequential learning problem</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neal</forename><forename type="middle">J</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychology of learning and motivation</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="109" to="165" />
			<date type="published" when="1989">1989</date>
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning to remember: A synaptic plasticity driven framework for continual learning</title>
		<author>
			<persName><forename type="first">Oleksiy</forename><surname>Ostapenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Puscas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tassilo</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Jahnichen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moin</forename><surname>Nabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Moment matching for multi-source domain adaptation</title>
		<author>
			<persName><forename type="first">Xingchao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinxun</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xide</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Gdumb: A simple approach that questions our progress in continual learning</title>
		<author>
			<persName><forename type="first">Ameya</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Puneet</forename><forename type="middle">K</forename><surname>Philip Hs Torr</surname></persName>
		</author>
		<author>
			<persName><surname>Dokania</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="524" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Continual unsupervised representation learning</title>
		<author>
			<persName><forename type="first">Dushyant</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Visin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">icarl: Incremental classifier and representation learning</title>
		<author>
			<persName><surname>Sylvestre-Alvise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Sperl</surname></persName>
		</author>
		<author>
			<persName><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Catastrophic forgetting, rehearsal and pseudorehearsal</title>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Robins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Connection Science</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<idno>arXiv 1606.04671</idno>
		<title level="m">Progressive neural networks</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting with hard attention to the task</title>
		<author>
			<persName><forename type="first">Joan</forename><surname>Serra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Didac</forename><surname>Suris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marius</forename><surname>Miron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Continual learning with deep generative replay</title>
		<author>
			<persName><forename type="first">Hanul</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung</forename><surname>Kwon Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaehong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">James</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cameron</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><surname>Baer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Constantine</forename><surname>Dovrolis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.02021</idno>
		<title level="m">Unsupervised progressive learning and the stam architecture</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Understanding self-supervised learning dynamics without contrastive pairs</title>
		<author>
			<persName><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<idno>PMLR, 2021. 12</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="10268" to="10278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Divide and contrast: Self-supervised learning from uncurated data</title>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><forename type="middle">J</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.08054</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Contrastive multiview coding</title>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="page" from="776" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020-05">2020. 5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Large scale incremental learning</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuancheng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yandong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Large batch training of convolutional networks</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Gitman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03888</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Barlow twins: Self-supervised learning via redundancy reduction</title>
		<author>
			<persName><forename type="first">Jure</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stéphane</forename><surname>Deny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2009">2021. 1, 2, 5, 9</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Continual learning through synaptic intelligence</title>
		<author>
			<persName><forename type="first">Friedeman</forename><surname>Zenke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
