{
  "arxivId": "2112.04215",
  "title": "Self-Supervised Models are Continual Learners",
  "authors": "Enrico Fini, Victor G. Turrisi da Costa, Xavier Alameda-Pineda, Elisa Ricci, Karteek Alahari, Julien Mairal",
  "abstract": "Self-supervised models have been shown to produce comparable or better visual\nrepresentations than their supervised counterparts when trained offline on\nunlabeled data at scale. However, their efficacy is catastrophically reduced in\na Continual Learning (CL) scenario where data is presented to the model\nsequentially. In this paper, we show that self-supervised loss functions can be\nseamlessly converted into distillation mechanisms for CL by adding a predictor\nnetwork that maps the current state of the representations to their past state.\nThis enables us to devise a framework for Continual self-supervised visual\nrepresentation Learning that (i) significantly improves the quality of the\nlearned representations, (ii) is compatible with several state-of-the-art\nself-supervised objectives, and (iii) needs little to no hyperparameter tuning.\nWe demonstrate the effectiveness of our approach empirically by training six\npopular self-supervised models in various CL settings.",
  "url": "https://arxiv.org/abs/2112.04215",
  "issue_number": 407,
  "issue_url": "https://github.com/dmarx/papers-feed/issues/407",
  "created_at": "2025-01-04T14:49:51.228547",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 0,
  "last_read": null,
  "last_visited": "2024-12-28T07:29:58.974Z",
  "main_tex_file": null,
  "published_date": "2021-12-08T10:39:13Z",
  "arxiv_tags": [
    "cs.CV",
    "cs.LG"
  ]
}