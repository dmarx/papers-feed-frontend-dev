- Decision to use late-phase weights for improving generalization
- Choice of weight interaction function for base and late-phase weights
- Selection of late-phase weight initialization strategy
- Adoption of stochastic learning algorithm for late-phase weights
- Implementation of multiplicative interactions in late-phase weight models
- Use of batch normalization layers as late-phase weights
- Exploration of rank-1 matrix weights as a late-phase weight model
- Integration of hypernetworks for generating late-phase weights
- Inclusion of late-phase classification layers in the model
- Decision to average late-phase weights at test time
- Choice of hyperparameters for late-phase weight models
- Strategy for gradient accumulation during training
- Selection of optimizers for late-phase and base weights
- Assumption regarding mode coverage in late-phase learning
- Decision to evaluate performance on established benchmarks (CIFAR-10/100, ImageNet, enwik8)
- Theoretical analysis of the noisy quadratic problem as a foundation for late-phase learning