# Bayesian inverse Navier-Stokes problems: joint flow field reconstruction and parameter learning

## Abstract

## 

We formulate and solve a Bayesian inverse Navier-Stokes (N-S) problem that assimilates velocimetry data in order to jointly reconstruct a 3D flow field and learn the unknown N-S parameters, including the boundary position. By hardwiring a generalised N-S problem, and regularising its unknown parameters using Gaussian prior distributions, we learn the most likely parameters in a collapsed search space. The most likely flow field reconstruction is then the N-S solution that corresponds to the learned parameters. We develop the method in the variational setting and use a stabilised Nitsche weak form of the N-S problem that permits the control of all N-S parameters. To regularise the inferred geometry, we use a viscous signed distance field (vSDF) as an auxiliary variable, which is given as the solution of a viscous Eikonal boundary value problem. We devise an algorithm that solves this inverse problem, and numerically implement it using an adjoint-consistent stabilised cut-cell finite element method. We then use this method to reconstruct magnetic resonance velocimetry (flow-MRI) data of a 3D steady laminar flow through a physical model of an aortic arch for two different Reynolds numbers and signal-to-noise ratio (SNR) levels (low/high). We find that the method can accurately i) reconstruct the low SNR data by filtering out the noise/artefacts and recovering flow features that are obscured by noise, and ii) reproduce the high SNR data without overfitting. Although the framework that we develop applies to 3D steady laminar flows in complex geometries, it readily extends to time-dependent laminar and Reynoldsaveraged turbulent flows, as well as non-Newtonian (e.g. viscoelastic) fluids.

## 

3 Bayesian inversion of the Navier-Stokes problem 3.1 Data-model discrepancy: operators S, C u ⋆ . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 Generalised N-S problem: operators G, Q . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2.1 Weak form using Nitsche's method . . . . . . . . . . . . . . . . . . . . . . . . . 3.2.2 Linearised N-S problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2.3 Adjoint N-S problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2.4 Inlet boundary condition g i . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2.5 Outlet boundary condition g o . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2.6 Effective viscosity field ν e . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2.7 Geometry Ω . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2.8 Assembling operator G . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2.9 Solving the (forward) N-S problem: operator Q . . . . . . . . . . . . . . . . . . 3.2.10 Solving the viscous Eikonal problem: operator Q φ ± . . . . . . . . . . . . . . . . 3.3 N-S parameters prior: operator C x . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.4 Inverse problem solution algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

also provides additional physical knowledge (e.g. pressure, wall shear stress, and effective viscosity field/tensor), which is otherwise difficult or impossible to measure. Inverse N-S problems have been intensively studied during the last decade, mainly enabled by the increase of available computing power. Recent applications in fluid dynamics range from the forcing inference problem [[Fou+14;](#b29)[HLS14]](#b30), to the reconstruction of scalar image velocimetry (SIV) [[Gil+18;](#b42)[Sha+19]](#b49) and particle image velocimetry (PIV) [[GBY19]](#b46) data, and the identification of optimal sensor arrangements [[MCS17;](#b40)[Ver+19]](#b50). Regularisation methods that can be used to reduce the search space of model parameters are reviewed in [[Stu10]](#b24) from a Bayesian perspective, and in [[BB18]](#b41) from a variational perspective. The well-posedness of Bayesian inverse N-S problems has been addressed in [[Cot+09]](#b21).

## Flow field reconstruction

In [[KB18]](#b43) the authors treat the reduced inverse N-S problem of finding the Dirichlet boundary condition for the inlet velocity that matches the modelled velocity field to flow-MRI data for a steady 3D flow in a glass replica of the human aorta. They measure the data-model discrepancy using the L 2 -norm and introduce additional variational regularisation terms for the Dirichlet boundary condition. The same formulation is extended to periodic flows in [Kol19; KCH19], using the harmonic balance method for the temporal discretisation of the N-S problem. In [[Fun+19]](#b45) the authors address the problem of inferring both the inlet velocity (Dirichlet) boundary condition and the initial condition, for unsteady blood flows and 4D flow-MRI data in a cerebral aneurysm. Note that the above studies consider rigid boundaries and require a priori an accurate, and time-averaged, geometric representation of the blood vessel, which is a severe constraint on the inverse problem of finding the velocity field.

To find the shape of the flow domain, e.g. the blood vessel boundaries, computed tomography (CT) or magnetic resonance angiography (MRA) is often used. The acquired image is then reconstructed, segmented, and smoothed. This process not only requires substantial effort and the design of an additional experiment (e.g. CT, MRA), but it also introduces geometric uncertainties [[Mor+16;](#b35)[San+16]](#b36), which, in turn, affect the predictive confidence of arterial wall shear stress distributions and their mappings [[Kat+07;](#b19)[Sot+16]](#b37). For example, in [[Fun+19]](#b45) the authors report discrepancies between the modelled and the measured velocity fields near the flow boundaries, and they suspect they are caused by geometric errors that were introduced during the boundary segmentation process. In general, the assumption of rigid boundaries either implies that a time-averaged geometry has to be used, or that an additional experiment (e.g. CT, MRA) has to be conducted to register the moving boundaries to the flow measurements. A more consistent approach to this problem is to treat the blood vessel geometry as an unknown when solving the generalised inverse N-S problem [[Kon+22]](#b60). In this way, the method simultaneously reconstructs and segments the velocity fields and can better adapt to the velocimetry experiment by correcting the geometric errors and improving the reconstruction. In other words, the velocity field is used to find the boundary, and the boundary is used to find the velocity field. This is a neater optimisation problem since it removes the severe constraint of fixing the boundary before assimilating the velocity field.

In this study we build upon the work in [[Kon+22]](#b60). In particular, we i) extend the methodology from 2D to 3D flows, ii) define the data-to-model projection operator for imaging problems, iii) incorporate a stabilised Nitsche cut-cell finite element method for the N-S problem that remains robust for high Re numbers and leads to an adjoint-consistent discrete formulation (i.e. the discretised continuous adjoint operator is equivalent to the discrete adjoint operator), iv) revise and improve the implicit representation of the geometry by incorporating the viscous Eikonal equation as an additional constraint to the problem, and v) describe the numerical problem in detail by defining the discrete function spaces and the stabilised discrete weak forms that make up the inverse problem. We then devise an improved algorithm that solves this Bayesian inverse N-S problem, and demonstrate it on flow-MRI data of a 3D steady laminar flow through a physical model of an aortic arch for low and high Re numbers.

## Deep learning reconstruction algorithms

Artificial intelligence (AI) algorithms, such as deep neural networks (NNs), are versatile, relatively simpler to implement, and have revolutionised the field of computer vision. There are, however, fundamental problems that still need to be addressed for their application to physics-based problems such as flow field reconstruction (e.g. AI-generated hallucinations, and the problem of existence of stable computational algorithms) [[Muc+21;](#b54)[CAH22]](#b58). Physics-informed NNs (PINNs) cannot yet solve boundary value problems, (e.g the N-S problem) more efficiently than finite element methods [[Gro+24]](#b63). Further, PINNs contain orders of magnitude more degrees of freedom than the method in this paper, with a corresponding increase in the required amounts of data and training time. The method in this paper is, in contrast, formulated from a N-S boundary value problem in a variational framework. The physics is hardwired into the model, meaning that the search space is restricted to solutions that satisfy the N-S problem. This is a key-difference between PINNs and the approach in this study: while PINNs solve a minimisation problem[foot_0](#foot_0) , we solve a saddle point problem. In other words, PINNs do not hardwire the N-S problem; they simply penalise its residuals (i.e. they treat the N-S problem as a soft constraint). In our experience, solving the saddle point problem, even though more computationally expensive, leads to superior flow field reconstructions and more accurate inferred parameters. Unlike NNs [Fer+20; RRAJ21] and PINNs [Sai+24; Zhu+24], our model is relatively small, physically interpretable, amenable to mathematical analysis, extrapolatable, and will reconstruct flows that it has not seen before, thus enabling digital twin applications in engineering design and patient-specific cardiovascular modelling.

## Outline

• In section 2 we present the general framework for the Bayesian inversion of nonlinear models.

• In section 3 we formulate the Bayesian inversion of the Navier-Stokes problem, in a continuous, variational setting, and devise an algorithm that solves it.

• In section 4 we formulate the discrete Bayesian inverse N-S problem, and discuss the details of its numerical implementation.

• In section 5 we use the algorithm to reconstruct flow-MRI data of a 3D steady laminar flow through a physical model of an aortic arch.

## Notation

In what follows, L 2 (Ω) denotes the space of square-integrable functions in Ω, with inner product

$•, • and norm ∥•∥ L 2 (Ω)$, and H k (Ω) denotes the space of square-integrable functions with k squareintegrable derivatives. For a given covariance operator, C, we also define the covariance-weighted L 2 (Ω) spaces, endowed with the inner product

$•, • C(Ω) := C -1/2 •, C -1/2 • Ω , which generates the norm ∥•∥ C(Ω)$. The Euclidean norm in the space of real numbers R n is denoted by |•| R n , and the measure (volume) of the domain Ω by |Ω|. The first variation of a functional J : L 2 (Ω) → R is defined by

$δ z J ≡ d dτ J (z + τ z ′ ) τ =0 = D z J , z ′ Ω ,(1)$where

$τ ∈ R, z ∈ L 2 (Ω), z ′ ∈ L 2 (Ω)$is an allowed perturbation of z, and D z J is the generalised gradient. We use the superscript (•) * to denote the adjoint of an operator, (•) ⋆ to denote a measurement, and (•) • to denote a reconstruction. For instance, u ⋆ denotes the measured velocity obtained from a velocimetry experiment, u • denotes the corresponding reconstructed velocity field, and u denotes a (modelled) velocity field obtained from the N-S problem.

## Bayesian inversion of nonlinear models

Given experimental, possibly noisy and sparse, vector field data of a physical quantity, u ⋆ , we assume that we already know a physical model, u = u(x), that can fit the data for suitable, physicallyinterpretable parameters, x. In other words, there exist parameters x • such that

$u ⋆ ≃ Zx • = SQ x • = Su • ,(2)$where '≃' denotes an approximation in some metric space to be defined later, Z := SQ is the operator that maps model parameters to model solutions projected into the data space, S is the model space to data space projection operator, which is linear and non-invertible[foot_1](#foot_1) , and Q is the operator that encapsulates the physical model, which is nonlinear. Under this assumption, the main goal is to find x • , which is of interest because it often pertains to physical quantities that cannot be directly measured, and which are instead inferred from the data. At the same time, we obtain the reconstructed (model-filtered) velocity field, u • ≡ Qx • , which fits (as opposed to overfits) the noisy, sparse, and under-resolved data, u ⋆ . Starting from the assumptions that i) the model explains the data, and ii) the data noise is Gaussian, we write

$u ⋆ -Zx = ε ∼ N (0, C u ⋆ ) ,(3)$where ε ∼ N (0, C u ⋆ ) is Gaussian noise with zero mean and with covariance operator C u ⋆[foot_2](#foot_2) . We also assume that we have prior knowledge of the probability distribution of the model parameters, x. We call this the prior parameter distribution, N (x, C x), which we assume to be Gaussian with prior mean x, and prior covariance operator C x. We then use Bayes' theorem, which states that the posterior probability density function (p.d.f.) of x, given the data u ⋆ , π x u ⋆ , is proportional to the data likelihood, π u ⋆ x , times the prior p.d.f. of x, π x , i.e.

$π x u ⋆ ∝ π u ⋆ x π(x) = exp - 1 2 ∥u ⋆ -Zx∥ 2 C u ⋆ - 1 2 ∥x -x∥ 2 Cx (4)$where π(•) is the Gaussian p.d.f., and

$∥•, •∥ 2 C := •, C -1 • is the covariance-weighted L 2 -norm.$The most likely parameters x • , in the sense that they maximise the posterior p.d.f. (maximum a posteriori probability, or MAP estimator), are given implicitly as the solution of the nonlinear optimisation problem

$x • ≡ argmin x J (x) , where J (x) := 1 2 ∥u ⋆ -Zx∥ 2 C u ⋆ + 1 2 ∥x -x∥ 2 Cx .$(5)

## Nonlinearity and optimisation

In order to solve problem (5) we first linearise u around x k , i.e.

$u = Qx ≃ Qx k + A k x -x k ) ,(6)$where A k ≡ D u x k is the Jacobian of u with respect to x. In the same spirit, we obtain

$Zx ≃ Zx k + G k x -x k ) , G k := SA k .(7)$To find the unknown x that satisfies the optimality conditions of (5), we can use an iteration based on the following update rule [Tar05, Chapter 6.22.6]

$x k+1 → x k -τ k C x k D x J k ,(8)$where R ∋ τ k > 0 is the step size at iteration k, which is determined by a line search algorithm, C x k is the posterior (parameter) covariance operator at iteration k, which is given by

$C x k := G * k C -1 u ⋆ G k + C -1 x -1 ,(9)$where G * k is the adjoint of G k , and D x J k is the gradient of the objective, J , with respect to the parameters, x, which is given by

$D x J k := -G * k C -1 u ⋆ u ⋆ -Zx k model term + C -1 x x k -x prior term . (10$$)$
## Posterior distribution and the Gaussian assumption

For x in a neighbourhood of the MAP point, x • , we can furthermore write

$π x u ⋆ ≃ π x u ⋆ := exp - 1 2 ∥x -x • ∥ 2 C x • -const. ,(11)$which is known as the Laplace approximation [Mac03, Chapter 27], and is exact when Z is a linear operator. Figure [1](#fig_0) illustrates the conceptual difference between linear, weakly nonlinear, and strongly nonlinear operators Z. In this study, Z encodes a Navier-Stokes problem, and the 'strength' of the nonlinearity depends on the flow conditions (e.g. on the geometry and the boundary conditions) and the Reynolds number, Re. Note that, for Re ≪ 1, the N-S problem can be approximated well enough by the Stokes problem, which is linear. At the other end of the spectrum, for sufficiently large Re, the flow becomes turbulent, which is a strongly nonlinear phenomenon. It should be emphasised that what matters is the behaviour of Z around the MAP point, x • , and within the ∼ 3σ support of the joint density, π(x, u ⋆ ). If Z is to be linearised around x • , the greater the spread of π(x, u ⋆ ), the less accurate the approximation will be. Also, the spread of π(x, u ⋆ ) depends on i) the noise level in the data, and ii) the assumed prior distribution of the N-S unknowns, x. This implies that, when Z is strongly nonlinear, we may require higher-quality data and/or more informative priors in order to regularise the inverse problem. Alternatively, we can simplify or reformulate the model such that Z exhibits smoother behaviour. In turbulent flows, for example, the large scales will depend on the geometry and the boundary conditions of the problem, while the small scales of turbulence exhibit a universal behaviour that is not directly linked to these parameters. Hence, inferring the evolution of the averaged velocity and pressure fields using the Reynolds-averaged Navier-Stokes (RANS) equations, instead of the instantaneous fields that are governed by the N-S equations, can lead to a well-posed inverse problem. In this paper, however, we only consider laminar flow with a Newtonian viscosity model.

## Posterior covariance approximation

Instead of using formula (9) to compute the exact posterior parameter covariance operator, C x k , which is computationally expensive, we can approximate it using a quasi-Newton method (e.g. BFGS) [[Fle00]](#b8). When Z is strongly nonlinear (right), π(x|u ⋆ ) can even be multimodal, in which case there are multiple critical points which will yield different approximations π(x|u ⋆ ) (even locally, the approximation may be inaccurate).

Quasi-Newton methods use the states, x k , and the gradients, D x J k , to reconstruct the Hessian operator or its inverse. In the case of the quadratic problem (5), the inverse Hessian operator, H k , is identical to C x k . The reconstructed inverse Hessian, H k , which is equivalent to the reconstructed posterior (parameter) covariance, C k , is then given by the recursive formula [YJH96][Tar05, Chapter 6.22.8]

$C k • = C x • + k-1 j=1 1 + ω j ρ j ρ j s j , • s j + k-1 j=1 -ρ j •, α j s j -ρ j s j , • α j ,(12)$where

$s k → x k -x k-1 and y k → D x J k -D x J k-1 ,(13)$• is a placeholder for the argument, ω j ≡ y j , C j y j , ρ j ≡ 1/ y j , s j , and α j ≡ C j y j , are quantities that are computed only once and stored in memory. Note that we here choose to initialise the posterior covariance using the prior covariance, i.e. C 0 → C x, but the best initialisation strategy depends on the specifics of the problem.

## Uncertainty estimation

To estimate the predicted uncertainty around the MAP solution, we can use Arnoldi iteration to compute the eigendecomposition of either the exact posterior covariance, which is given by (9), or the approximated posterior covariance, which is given by (12). A third option is to reconstruct

$C x • using the eigendecompositions of G x • , G * x • , C -1 u ⋆ , C -1$x , which may be simpler to compute. Once the eigenvalue/eigenvector pairs, (λ, v) ℓ , of the covariance operator are obtained, we find

$C x • • = ∞ ℓ λ ℓ v ℓ v ℓ , • .(14)$The pointwise variance, var(x), which is often used to quantify uncertainty, is then given by

$var(x) = ∞ ℓ λ ℓ v 2 ℓ ,(15)$and the total variance is given by

$∥x -x • ∥ 2 C x • = ∞ ℓ λ ℓ .(16)$Figure [2](#): The Bayesian inverse N-S problem assimilates velocimetry data, u ⋆ , in order to jointly reconstruct the flow field, u, and learn the unknown N-S parameters, x. The MAP estimator of the unknown N-S parameters, x • , is found by solving problem (19), and the reconstructed velocity field is given by u

$• = Qx • .$Samples, x s , can be drawn using the Karhunen-Loève (K-L) expansion

$x s = x • + ∞ ℓ η ℓ λ ℓ v ℓ , with R ∋ η ℓ ∼ N (0, 1) ,(17)$in order to further inspect the posterior distribution around the MAP point. Likewise, the posterior covariance of the model solution at iteration k, which is given by

$C u k := A x k C x k A * x k ,(18)$can be reconstructed from the eigendecompositions of C x k , A k , and A * k .

## Bayesian inversion of the Navier-Stokes problem

In section 2 we discussed the general Bayesian inversion problem and the MAP estimator, given by (5), which can be iteratively solved by using the update rule (8) along with the objective gradients (10) and the posterior covariance (9) (or its BFGS-approximation) when the operators C u ⋆ , C x, S, G, and Q are precisely defined. In order to obtain these operators for the Bayesian inversion of the generalised Navier-Stokes problem, we need to derive the first order optimality conditions of the following saddle point problem find x • ≡ arg min

$x max v,q,r J (u, p, v, q, r; x) ,(19)$where

$J (u, p, v, q, r; x) := U (u) + R x (x) + M (u, p, v, q; x) + D(r, φ ± ) .(20)$The augmented objective functional, J , incorporates the data-model discrepancy, U , the priors of the N-S parameters, R x , and two nonlinear equality constraints: i) the generalised Navier-Stokes boundary value problem, M , for (u, p) (i.e. velocity and pressure), and ii) the viscous Eikonal boundary value problem, D, for the viscous signed distance field (vSDF), φ ± . The vSDF, φ ± , is an auxiliary variable that we use to implicitly define and control the geometry. The Lagrange multipliers, (v, q, r), also known as adjoint variables, are used to enforce the two model constraints: i) (v, q) enforce the N-S b.v.p. for (u, p), and ii) r enforces the vSDF b.v.p. for φ ± . Note that, problem ( [19](#formula_25))-( [20](#formula_26)) is the Lagrangian formulation of the MAP estimation problem (5), where the nonlinear constraint u = Qx, which is now encoded in M , is enforced using Lagrange multipliers. The additional constraint on the vSDF, which is encoded in D, is because we use φ ± as an auxiliary variable to define the geometry, Ω, which belongs to the N-S unknowns, x.

In this section, we first define C u ⋆ and S, then we use variational calculus to construct G and Q, and lastly we define C x. We further devise an algorithm that solves the Bayesian inverse N-S problem, which is presented at the end of this section. 

$I d = N i=1 V i , and int(V i ) ∩ int(V j ) = ∅ for i ̸ = j ,(21)$where int(•) denotes the interior of a set. The data space, D, is the function space on I d that is comprised of all piecewise constant functions

$u d (x) = N i=1 φ i (x) u i d ,(22)$where φ i (x) ≡ (1, 1, 1) for x ∈ V i and φ i (x) ≡ 0 otherwise, and u i d ∈ R 3 . In general, the model space, M , is a subspace of L 2 (I m ), but more can be said about M and I m after discretising the N-S problem, which we will address in section 4.1. The data-model discrepancy is measured on the data space, and is given by

$U (u) := 1 2 u ⋆ -Su 2 C u ⋆ ,(23)$where u = (u x , u y , u z ), u ⋆ = (u ⋆ x , u ⋆ y , u ⋆ z ), and S : M → D is the model-to-data projection operator such that

$Su m := N i=1 χ i 1 |V i | V i u m φ i (x) , u m ∈ M ,(24)$where χ = N i=1 χ i φ i (x) is a mask. Then, by definition, the adjoint operator S * satisfies

$u d , Su m D = S * u d , u m M , for all u m ∈ M , u d ∈ D . (25$$)$We further assume that the noise colour in the velocity images is white, and define the covariance operator

$C u ⋆ = diag σ 2 u ⋆ x I, σ 2 u ⋆ y I, σ 2 u ⋆ z I ,(26)$where

$σ 2 u ⋆ x , σ 2 u ⋆ y , σ 2 u ⋆ z are the variances of u ⋆ x , u ⋆ y , u ⋆ z$, respectively, and

$I : L 2 (I d ) → L 2 (I d )$is the identity operator. The Gaussian white noise assumption is valid for velocity images obtained from phasecontrast MRI when the signal-to-noise ratio (SNR) is above 3 and the frequency (k-) space is fully sampled [[GP95]](#b5). If the noise is Gaussian but coloured (i.e. correlated), C u ⋆ has to be revised [[KJ23]](#b62). Having defined S, S * , and C u ⋆ , the first variation of U with respect to the velocity field is then given by

$δ u U := D u U , u ′ Im , where D u U ≡ -S * C -1 u ⋆ (u ⋆ -Su) .(27)$In what follows we will show that the discrepancy, D u U ∈ M , is the forcing term of the adjoint N-S problem, whose solution plays an important role in obtaining the model term of the objective gradient (10).

## Generalised N-S problem: operators G, Q

The generalised Navier-Stokes boundary value problem (b.v.p.) in

$Ω ⊂ I m ⊂ R 3 is given by                u • ∇u -∇ • 2ν e ∇ s u + ∇p = 0 in Ω ∇ • u = 0 in Ω u = 0 on Γ u = T Γ i g i on Γ i -2ν e ∇ s u • ν + pν = T Γo g o on Γ o , x = (g i , g o , ν e , Ω) unknown parameters , (28$$)$where u is the velocity, p → p/ρ is the reduced pressure, ρ is the density, ν e is the effective (kinematic) viscosity,

$∇ s u ≡ (∇ s u) ij := 1 2 (∂ j u i + ∂ i u j )$is the strain-rate tensor, g i is the Dirichlet boundary condition (b.c.) at the inlet Γ i , g o is the natural b.c. at the outlet Γ o , and ν is the unit normal vector on the boundary ∂Ω = Γ ∪ Γ i ∪ Γ o , where Γ is the part of the boundary where a no-slip b.c. is imposed. The trace operators, T Γ i and T Γo , are defined in appendix A. We call this N-S b.v.p. 'generalised' because all of its input parameters, namely the shape of the domain Ω, the boundary conditions g i , g o , and the viscosity field ν e , are considered to be unknown. Consequently, problem (28) can explain many different wall-bounded incompressible flows for Newtonian or generalised Newtonian fluids (i.e. 'power-law' non-Newtonian fluids).

## Weak form using Nitsche's method

To construct Q, G, we start from the weak form of the Navier-Stokes problem, M , for velocity test functions v ∈ H 1 (Ω), and pressure test functions q ∈ L 2 (Ω). We weakly enforce Dirichlet (essential) velocity boundary conditions on Γ e := Γ ∪ Γ i using Nitsche's method [[Nit71]](#b0), which was initially formulated for the Poisson problem and later on extended to the linearised N-S (Oseen) problem [BFH06; BH07; MSW18]. This is done by augmenting M with the following terms

$N ∆ (v, q, u; g i ) := -2ν e ν • ∇ s v -qν symmetry + η ∆ v penalty , u -χ Γ i g i b.c. Γe ,(29a)$$N ∇ (v, u; g i ) := -v, (u -χ Γ i g i )(u • ν) Γ in + η ∇ v • ν, (u -χ Γ i g i ) • ν Γe ,(29b)$where N ∆ is the Stokes problem Nitsche term, and N ∇ is the extra Nitsche term needed to impose the b.c. in convection-dominant flows. The characteristic function, χ Γ i , takes the value of 1 on Γ i , and 0 otherwise, and the inflow boundary, Γ in , is such that

$Γ in := x ∈ Γ e : u • ν < 0 . (30$$)$The Nitsche penalty parameters, η ∆ and η ∇ , will, in general, scale with the flow, and they will be precisely defined in section 4, where we discuss the discrete form of M . It is also worth noting that the terms N ∆ , N ∇ vanish when u ≡ χ Γ i g i (consistency), and that the Stokes Nitsche term, N ∆ , is designed such that the Stokes b.v.p. produces a symmetric weak form. The weak form of the N-S problem, M , is then given by

$M (u, p, v, q; x) := a(v, u; x) + b(p, v; x) momentum, M I -b(q, u; x) div-free, M II -j u (v; x) -j p (q; x) inhomogeneous b.c. = 0 . (31$$)$The velocity-velocity bilinear form, a, consists of the convective form, a ∇ , the viscous form, a ∆ , and the Nitsche form, a N , which are given by

$a(v, u; x) := a ∇ + a ∆ + a N ,(32) a$$∇ (v, u; x) := v, u • ∇u Ω -v, (u • ν)u Γ in , (32a) a ∆ (v, u; x) := 2ν e ∇ s v, ∇ s u Ω -2ν e v, ν • ∇ s u Γe -2ν e ν • ∇ s v, u Γe , (32b) a N (v, u; x) := η ∆ v, u Γe + η ∇ v, (u • ν)ν Γe . (32c$$)$The velocity-pressure bilinear form, b, which encapsulates the divergence-free constraint on u, is given by b(q, u; x)

$:= -q, ∇ • u Ω + qν, u Γe .(33)$The linear forms j u , j p , are due to the inhomogeneous inlet and outlet b.c., and are given by

$j u (v; x) := -v, (u • ν)T Γ i g i Γ in -2ν e ν • ∇ s v, T Γ i g i Γ i + η ∆ v, T Γ i g i Γ i + η ∇ v, (T Γ i g i • ν)ν Γ i -v, T Γo g o Γo , (34) j p (q; x) := -qν, T Γ i g i Γ i .$(35)

## Linearised N-S problem

The first order Taylor expansion of the weak form of the N-S b.v.p., M , around u k , can be written as[foot_3](#foot_3)

$y ′ = A k x ′ or y ≃ y k + A k x -x k ,(36)$where y ≡ (u, p) is the N-S state, y ′ ≡ (u ′ , p ′ ) is the N-S state perturbation, and x ′ is the N-S parameter perturbation (see also formulas (6), (7) in section 2.1). The operator A k can be split such that

$A k ≡ D y x k := D M y -1 k D M x k . (37$$)$We therefore proceed to define (D M y ) k by linearising M around u k such that

$δ u M := v, (D M I u ) k u ′ Ω + q, (D M II u ) k u ′ Ω = δ u α(v, u ′ ) -δ u b(q, u ′ ) , (38) δ p M := v, (D M I p ) k p ′ Ω = δ p b(p ′ , v) ,(39)$where

$δ u a = δ u a ∇ + δ u a ∆ + δ u a N , (40a) δ u a ∇ = v, u ′ • ∇u k Ω + v, u k • ∇u ′ Ω -v, (u ′ • ν) u k Γ in -v, (u k • ν) u ′ Γ in ,(40b)$$δ u a ∆ = a ∇ (v, u ′ ) , δ u a N = a N (v, u ′ ) , δ u b = b(q, u ′ ) , δ p b = b(p ′ , v) . (40c$$)$The operator (D M y ) k is then given by

$(D M y ) k ≡ (D M (u,p) ) k := D M I u D M I p D M II u 0 k . (41$$)$We proceed to define D M x k for each N-S parameter, but first we must introduce the adjoint N-S problem. Given that the optimality conditions are based on a linearisation around u k , in the following subsections the subscript (.) k will often be omitted for the sake of clarity.

## Adjoint N-S problem

For any N-S parameter, x, we have

$δ x (U + M ) = D u U , u ′ Im + y * , D M y y ′ Ω variations that depend on y ′ + y * , D M x x ′(42)$where u ′ = D u x x ′ , p ′ = D p x x ′ , and y * ≡ (v, q). In order to eliminate the dependence of δ x J on state perturbations, y ′ ≡ (u ′ , p ′ ), we set

$D u U , u ′ Im + y * , D M y y ′ Im = 0 .(43)$In particular, we demand that

$D u U 0 + D M I u D M I p D M II u 0 * v q • y ′ = 0 for all y ′ ∈ H 1 (Ω) × L 2 (Ω) .(44)$The adjoint state, y * , is then obtained by solving the following operator equation

$Ay * = b , where A ≡ D M y * , and b ≡ -D u U 0 T ,(45)$which we call the adjoint N-S problem, and whose strong form is given by

$         -u k • 2∇ s v -∇ • 2ν e ∇ s v + ∇q = -D u U k in Ω ∇ • v = 0 in Ω v = 0 on Γ ∪ Γ i (u k • ν)v + (u k • v)ν + 2ν e ∇ s v • ν -qν = 0 on Γ o . (46$$)$Note that, v ≡ 0 and q ≡ 0 when D u U k ≡ 0. Also, on the boundary Γ e , the inhomogeneous Dirichlet b.c. in the N-S problem produces an homogeneous Dirichlet b.c. in the adjoint N-S problem. By solving equation ( [45](#formula_63)) for y * we eliminate the dependence of δ x J to state perturbations, y ′ . This allows us to define D M x using only the state, y k , and the adjoint state, y * . We therefore proceed to define D M

x for the N-S parameters x ∈ g i , g o , ν e , Ω .

## Inlet boundary condition g i

The generalised gradient, D x M , and the operator D M x of the inlet b.c. are given by

$D g i M , g ′ i Im = δ g i (-j u -j p ) ≃ 2ν e ν • ∇ s v + qν, T Γ i g ′ i Γ i = T * Γ i 2ν e ν • ∇ s v + qν , g ′ i Im (47) ≡ D M g i * y * , g ′ i Im = y * , D M g i g ′ i Im$.

The approximation in (47) is due to the fact that the adjoint N-S b.c. on Γ e , v| Γe ≡ 0, is weakly imposed. Boundary terms involving v on Γ e can be assumed to be negligible, but not exactly zero (because the penalisation parameters η ∇ , η ∆ are finite).

## Outlet boundary condition g o

For the outlet b.c. we find

$D go M , g ′ o Im = δ go (-j u ) = v, T Γo g ′ o Γo = T * Γo v, g ′ o Im (48) ≡ D M go * y * , g ′ o Im = y * , D M go g ′ o Im$.

## Effective viscosity field ν e

For the effective viscosity field we find

$D νe M , ν ′ e Ω = δ νe a ∆ ≃ 2 ∇ s v : ∇ s u, ν ′ e Ω (49) ≡ D M νe * y * , ν ′ e Ω = y * , D M νe ν ′ e Ω$, where a : b = ij a ij b ij , and the approximation in (49) is again due to the fact that v| Γe ≡ 0 is weakly imposed, similarly to equation (47). The generalised gradient of the effective viscosity field, D νe M , which is given by equation (49), is, in general, a function in L 2 (Ω). For Newtonian fluids ν e is constant (i.e. ν e ∈ R) and the generalised gradient is

$D νe M , ν ′ e R = 2 Ω ∇ s v : ∇ s u, ν ′ e R . (50$$)$For 'power-law' non-Newtonian fluids, also known as generalised Newtonian fluids, the effective viscosity field is an explicit function of the shear-strain magnitude, γ := √ 2∇ s u : ∇ s u, and the n rheological parameters of the fluid, p ν ∈ R n , such that ν e = ν e ( γ; p ν ) .

(51)

In that case, the N-S unknown is p ν instead of ν e , and the generalised gradient is thus given by

$D pν M , p ′ ν R n = 2 Ω D pν ν e ∇ s v : ∇ s u , p ′ ν R n , (52$$)$where D pν ν e is obtained by differentiating the algebraic relation ν e ( γ; p ν ). Note that, when ν e is given by the algebraic model ( [51](#)), which depends on γ = γ(u), ν e becomes a function of the velocity field and, consequently, the linearised N-S operator, (D M y ) k , which is given by (41), involves additional terms that depend on the variation of ν e with respect to u-perturbations.

It is worth mentioning that viscoelastic non-Newtonian fluids and Reynolds-averaged turbulent flows can be modelled using an effective viscosity field that is implicitly defined by one or more boundary value problems. The present formulation can therefore be extended to such problems by augmenting the objective functional, J , which is given by equation (20), with the weak form of the model that governs ν e ∈ L 2 (Ω). This is out of the scope of the present study. Here, we assume that the fluid is Newtonian. Nevertheless, the same framework has been applied to infer the rheological parameters of a power-law non-Newtonian fluid in [[KHM24]](#b64).

## Geometry Ω

The geometry, Ω, is implicitly defined by the viscous signed distance field (vSDF), φ ± , such that

$Ω := x ∈ Ω : φ ± (x) < 0 and Γ := x ∈ Ω : φ ± (x) = 0 . (53$$)$The vSDF is a viscosity solution of an Eikonal problem whose boundary value problem is given by

$sgn(φ ± ) |∇φ ± | -1 -ε φ ± ∆φ ± = 0 in I m φ ± = 0 on Γ ,(54)$where sgn(•) is the sign function, |•| is the Euclidean norm, and ε φ ± is the (artificial) viscosity. With the aid of φ ± , we can furthermore define the unit normal vector extension, ν, and the unit normal outward-facing vector extension, νo , such that

$ν := ∇φ ± |∇φ ± | and νo := sgn(φ ± ) ν .(55)$A common approach to modelling geometric (shape) perturbations in shape optimisation [SZ92; Wal15], is by introducing the speed field

$V := ζν , ζ ∈ L 2 (Γ) , (56$$)$where ζ is the shape perturbation magnitude, and ν is the unit normal vector on Γ. Then, the vSDF perturbation problem for φ ′ ± , due to Γ-perturbations induced by V , is given by

$νo • ∇φ ′ ± -ε φ ± ∆φ ′ ± = 0 in I m φ ′ ± + ζ∂ ν φ ± = 0 on Γ . (57$$)$Problem (57) models the convection-diffusion of φ ′ ± with unit velocity, since |ν o | ≡ 1. Thus it makes sense to define the Reynolds number

$Re φ ± := ℓ Γ ε -1 φ ± , (58$$)$where ℓ Γ is a characteristic length scale. The dissipation of small-scale (geometric) features away from Γ can then be explained by the scaling

$δ ∼ ε φ ± ℓ Γ 1 2 = Re -1 2 φ ± ℓ Γ , (59$$)$where δ is the diffusion width of geometric details at distance ℓ Γ from Γ. The use of artificial dissipation, ε φ ± , is thus particularly useful in mitigating non-physical, small-scale geometric perturbations, when the position of Γ is inferred from noisy velocity fields, u ⋆ . We now proceed to define the generalised gradient D x M , and the operator D M

x , for the vSDF. To compute the shape derivatives of the weak form M , due to Γ-perturbations induced by V , we use the Leibniz-Reynolds transport theorem (see appendix B) to obtain

$D ζ M , ζ Γ ≃ ∂ ν u k • -2ν e ν • ∇ s v -qν , ζ Γ (60) ≡ D M ζ * y * , ζ Γ = y * , D M ζ ζ Γ ,$where the approximation in (60) is because the homogeneous Dirichlet boundary conditions u k = v = 0 are weakly satisfied on Γ. The weak form of the vSDF b.v.p. (54), D, for test functions r ∈ H 1 (I m ), is given by

$D(r, φ ± ) := r, sgn(φ ± ) |∇φ ± | -1 Im + ε φ ± ∇r, ∇φ ± Im -ε φ ± r, ∂ ν φ ± ∂Im + η ∆ r, φ ± Γ = 0 ,(61)$where the homogeneous Dirichlet b.c. is imposed using the penalty term on Γ. The shape derivatives of the weak form D, due to Γ-perturbations induced by V , are then given by

$δ φ ± D = r νo , ∇φ ′ ± Im + ε φ ± ∇r, ∇φ ′ ± Im -ε φ ± r, ∂ ν φ ′ ± ∂Im + η ∆ r, φ ′ ± Γ + η ∆ r, ∂ ν φ ± ζ Γ (62) := r, D D φ ± φ ′ ± Im + r, D D ζ ζ Γ ,$for a given ζ ∈ L 2 (Γ), noting that the weak form (62) corresponds to the boundary value problem (57).

We then set i)

$δ φ ± D = 0 for all r ∈ L 2 (I m ) and ii) ζ = D ζ M = ∂ ν u k • -2ν e ν • ∇ s v -qν .(63)$The generalised gradient D φ ± M ∈ L 2 (I m ) is therefore implicitly obtained as the solution of the operator equation

$A D φ ± M = b , where A ≡ D D φ ± , and b ≡ D D ζ D ζ M ,(64)$which is equivalent to solving (62) (or (57

$) if η ∆ → ∞) for ζ = D ζ M , where D ζ M is given by (60).$Note that in [Kon+22, Section 2.4] the geometry is implicitly represented by an approximate SDF, which is obtained using the 'heat method' [[CWW17]](#b38), and that this constraint is not incorporated into the objective functional, J . In this paper, instead of the 'heat method', we use the viscous Eikonal equation, which is incorporated into J as an additional constraint. This new constraint is crucial for the solution of the inverse problem in complicated geometries (e.g. geometries with detailed features and regions of high curvature) because i) the viscous Eikonal equation provides more accurate SDFs, and ii) incorporating this equation as an additional constraint and deriving the respective optimality conditions (see equation ( [64](#formula_89)) for the propagation of the vSDF) ensures numerical consistency and leads to a robust shape inference algorithm.

## Assembling operator G

Using the above results we can precisely define G * k (and thus G k ) for the inverse N-S problem. Since G k := SA k , where S is given by (24), and

$A k ≡ D y$x k is given by (37), we find

$G * k := SD y x * k = S D M y -1 k D M x k * = D M x * k D M y * k -1 S * ,(65)$where (D M y ) k is the Jacobian of the N-S problem around u k , with respect to the flow variables, which is given by (41), and

$(D M x ) * k := diag D M g i , D M go , D M νe , D M φ ± * k (66)$is the Jacobian of the N-S problem around u k , with respect to the N-S parameters, where D M g i is given by (47), D M go is given by ( [48](#)), D M νe is given by either (49), (50), or (52), and D M φ ± is given by (64). It is worth noting that (D M

x ) * k depends on the flow variables at iteration k, y k , which are known, and the adjoint flow variables, y * , which are found by solving the adjoint problem (45).

## Solving the (forward) N-S problem: operator Q

The N-S problem (31) is nonlinear owing to a ∇ and N ∇ being nonlinear. The equivalent Oseen problem, which is linear, is obtained by replacing the nonlinear terms u • ∇u and (u -χ Γ i g i )(u • ν) with β • ∇u and (u -χ Γ i g i )(β • ν), respectively, where β is a known velocity field. Also, the inflow boundary definition (30) becomes β • ν < 0. The Oseen problem can then be recast as the linear system

$A β B -B * 0 u p = J u J p ,(67)$where the operators A β , B and the vectors J u , J p , correspond to the Oseen linearisation of (31).

The N-S problem can be solved using Picard iteration. Starting from an initial guess of the velocity field, u 0 , we solve (67) with β → u 0 in order to obtain a new velocity field, u 1 . This leads to an iterative process that under certain conditions converges to a fixed point such that ∥u k+1 -u k ∥ → 0 when k → ∞. The initial guess, u 0 , can be obtained by solving the Stokes problem that corresponds to (31). Picard iteration is robust and easy to implement, but converges slowly. In contrast, Newton iteration convergences faster, but requires a good initial approximation of the velocity field. Consequently, to solve N-S we first perform a few Picard iterations in order to obtain a good initial approximation of the velocity field, and then switch to Newton iterations. The Newton iteration is defined through the following update rule

$u k+1 → u k + τ u ′ , p k+1 → p k + τ p ′ ,$where u ′ , p ′ are the velocity and pressure perturbations, and τ is a line search step size. The line search step is initially taken as τ = 1 but it can be reduced via a backtracking line search algorithm to facilitate convergence when the initial guess is far away from the solution. The velocity and pressure perturbations are obtained after solving the Newton problem

$D M I u D M I p D M II u 0 k u ′ p ′ = -R u k -R p k ,(68)$where

$R u k := A u k u k + Bp -J u , (69a) R p k := -B * u k -J p ,(69b)$are the momentum and mass conservation residuals of the N-S problem, respectively. The Newton problem for the perturbations y ′ can also be written in compact form as D M y y ′ = -R, where D M y is given by (41).

To simplify the formulation, and to remain consistent with the operator-based approach we have adopted in this study, we use the nonlinear operator Q, which maps N-S parameters, x, to N-S velocity and pressure fields, y, to denote the Picard/Newton iteration approach to solving the N-S problem. When we write

$y = Q x ,(70)$we therefore imply that an iterative approach is used to solve the N-S problem for given parameters, x. Note that, in order to simplify the exposition in section 2, we defined Q as the operator that maps parameters, x, to velocity fields, u, instead of N-S solutions (u, p). This abuse of notation, however, does not introduce any problems or inconsistencies in the formulation because p is simply an (internal) Lagrange multiplier that enforces the divergence-free constraint of the N-S problem.

## Solving the viscous Eikonal problem: operator Q φ ±

The viscous signed distance field, φ ± , is a solution of the viscous Eikonal equation (54), which is nonlinear. We therefore define, similarly to operator Q, an operator Q φ ± that maps functions ψ, whose zeroth level-set is ∂Ω, to viscous signed distance fields (that correspond to ∂Ω), i.e.

$φ ± = Q φ ± ψ .(71)$Note that ψ encodes the position of the boundary Γ, which is the input parameter of (54). Equation (71) implies a standard Newton iteration that solves the weak form of (54), given by (61), starting from an approximation of the vSDF, ψ. If ψ is not readily available, an initial approximation is obtained by using the 'heat method' [CWW17][Kon+22, Section 2.4], which involves the solution of two linear and elliptic boundary value problems.

## N-S parameters prior: operator C x

The N-S parameter prior terms are given by

$R x := 1 2 x∈x x -x 2 Cx(Vx) ,(72)$where x = (g i , g o , ν e , φ ± ), x is the prior mean, V x is the domain, and

$C x : L 2 (V x ) → L 2 (V x )$is the prior covariance operator. For the inlet and outlet boundary conditions, g i and g o , we set [Kon+22]

$C x = σ 2 x I -ℓ 2 x ∆ -1 ,(73)$where σ 2 x is the prior variance, ℓ x is a characteristic length, and ∆ is the Laplacian operator with zero natural (Neumann) boundary conditions on ∂V x . For the effective viscosity, ν e , and the geometry, φ ± , we set C x = σ 2

x I. The domain V x is set to I m if the unknown is a field, and to R n , for n ≥ 1, if the unknown is a parameter vector (e.g. viscosity model parameters, or constant viscosity field).

## Inverse problem solution algorithm

In section 2 we formulated the general Bayesian inversion problem (MAP estimator) (5), which is solved using the update rule (8), alongside (9) and (10). In the start of section 3 we formulated the Bayesian inverse Navier-Stokes problem (MAP estimator) using Lagrange multipliers to impose the model constraints. This produced the saddle point problem (19), whose first order optimality conditions were derived in this section. Algorithm 1 summarises the iterative procedure that solves this saddle point problem by collecting the results obtained in section 3, and closely follows the nonlinear update rule (8).

Algorithm 1: Bayesian inversion of the Navier-Stokes problem 1 Input: data, u ⋆ , data cov., C u ⋆ , prior mean, x, prior cov. C x, projection operator, S 2 Initialisation:

$Solve φ± = Q φ ± ψ, ȳ = Q x, and set x 0 → x, u 0 → ū, C x 0 → C x, k → 0 3 while termination_criterion_is_not_met do 4 y * → ((D M y ) * k ) -1 S * C -1 u ⋆ (u ⋆ -Su k )$(solve adjoint N-S problem, eq. ( [45](#formula_63)))

$5 (D x M ) k → (D M x ) * k y *$(grad. model term, eq. ( [47](#)),( [48](#)),( [50](#formula_69)),(64))

$6 (D x J ) k → (D x M k + C -1 x (x k -x) (total gradient, eq. (10)) 7 C x k+1 → (x k , (D x J ) k ))$(BFGS post. cov. approx., eq. ( [12](#formula_16)))

$8 x ′ → C x k (D x J ) k (parameter perturbation) 9 y ′ → A k x ′$(solve for state perturbation, eq. ( [36](#formula_51)))

$10 τ → f (x ′ , y ′ ) (initial step size) 11 do 12 x k+1 → x k + τ x ′ (update N-S parameters) 13 y k+1 → y k + τ y ′ (update N-S solution) 14 φ ± k+1 → Q φ ± ψ k+1$(reproject to vSDF space, eq. ( [71](#formula_99)))

15 y k+1 → Q x k+1 (reproject to N-S space, eq. ( [70](#formula_98)))

$16 τ → τ /2 (reduce step size) 17 while J k < J k+1 ; 18 k → k + 1 19 Output: parameters, x • → x k , C x • → C x k , and state, y • → y k (MAP estimates)$It is worth noting that in order to compute the model term of the gradient, G * k S * C -1 u ⋆ (u ⋆ -Su k ), which appears in (10), we use the adjoint state, y * , as an auxiliary variable. This is because, if the algorithm is to be discretised and solved by a computer, the operator G * k , as defined by (65), is constructed using the inverse of (D M y ) * k , which is not necessarily sparse and thus prohibitively expensive (or impossible for large-scale problems) to store in computer memory, even with state-of-the-art hardware. The operator (D M y ) * k , however, is known to be sparse and can thus be stored in computer memory. Once the total gradient is computed, gradient information is used to update the posterior parameter covariance approximation, C x k+1 , using the BFGS quasi-Newton method. To accelerate the algorithm we do not impose the Wolfe conditions [[Fle00]](#b8) (since they require additional evaluations of the N-S and the adjoint N-S problems). We, however, use damping [[GRB20]](#b52) to ensure C x k remains positive definite, and its approximation remains numerically stable. The initial step size of the backtracking line search, τ , is obtained by f (x ′ , y ′ ), which is a problem-dependent user-input functional that depends on the parameter and state perturbations norms. During the line search, the perturbed parameters and the state are reprojected to their corresponding constraints (i.e. ψ k+1 ≡ (φ ± ) k+1 must be a vSDF and y must be a N-S solution), and the new value of the objective functional, J k+1 , is computed. The algorithm terminates if one of the following criteria is met: i) the optimality condition is satisfied within a tolerance ε, i.e. δ x J ≤ ε, ii) J reduces below a specified tolerance ε, i.e. J ≤ ε, iii) the line search fails to reduce J further.

The output of algorithm 1 is the posterior (MAP) parameter mean, x • , the approximated posterior parameter covariance, C x • , and the posterior (MAP) state mean y • . The approximated posterior state covariance, C y • , is given by (18). We thus obtain the following approximations for the posterior probability density functions

$π(x|u ⋆ ) ≃ N (x • , C x • ) , π(y|x) ≃ N (y • , C y • ) . (74$$)$The Gaussian approximation of the posterior distribution was discussed in section 2.2, and uncertainty estimation from covariance operators was discussed in section 2.3. Note that the accuracy of the computed uncertainties relies on i) the accuracy of the BFGS approximation of C x • , and ii) the accuracy of the Laplace approximation (11).

## Discretised operators and numerics

We numerically solve the boundary value problems of section 3 using a 'meshless' finite element method (FEM) [[BBO03]](#b11). In particular, we implement the fictitious domain cut-cell finite element method, introduced by [Bur10; BH12] for the Poisson problem, and later on extended to the Stokes and the Oseen problems [SW14; Mas+14; BCM15; MSW18]. This method has certain advantages over traditional mesh-dependent methods when working with complicated, moving geometries, since it does not require a body fitted mesh. Rather, it uses a fixed Cartesian mesh and thus requires special handling of the cut-cell region. We ensure consistency between the continuous and discrete formulations of the inverse problem (19) by discretising the weak forms derived in section 3 and adding symmetric numerical stabilisation terms that vanish as the background mesh size tends to zero. The numerical stabilisation that is introduced, and the weak imposition of the boundary conditions using Nitsche terms, lead to an adjoint-consistent formulation, i.e. the discretised continuous adjoint operator is equivalent to the discrete adjoint operator.

## Stabilised cut-cell finite element method

In the context of cut-cell FEM, we consider a geometry of interest Ω, immersed in a global domain I m ⊂ R 3 . The geometry Ω, and its boundary, Γ, are implicitly defined by a viscous signed distance field, φ ± ∈ L 2 (I m ). We define T h to be a tessellation of I m produced by cuboid cells (voxels), K ∈ T h , having sides of length h. This tessellation is comprised of cut-cells, T ▷ h , which are cells that are cut by the boundary, Γ, and of intact cells, T □ h , which are found either inside or outside of Ω, and are not cut by the boundary, Γ (see figure [2](#)). We assume that the boundary, Γ, is well-resolved, i.e. ℓ/h ≫ 1, where ℓ is the smallest length scale of Γ. Because of this assumption, the intersection T ▷ h ≡ T h ∩Γ is comprised of specific types of polyhedral elements, P, which are shown in figure [5b](#). The discretised model space, M h , is generated by assigning a trilinear finite element, Q 1 , to every cell K ∈ T h , i.e.

$M h = u ∈ C 0 (I m ) : u| K ∈ Q 1 for all K ∈ T h , (75$$)$where C 0 is the space of continuous functions, and

$Q 1 := ℓ c ℓ p ℓ (x)q ℓ (y)r ℓ (z), p ℓ , q ℓ , r ℓ polynomials of deg. ≤ 1, c ℓ ∈ R . (76$$)$The discrete velocity-pressure pair is defined in the same space, i.e. (u h , p h ) ∈ M h × M h , which leads to an inf-sup unstable formulation [[Bre08]](#b20). We therefore use the continuous interior penalty (CIP) method for pressure and velocity (convective) stabilisation [[BFH06]](#b17), and ∇-div stabilisation for both stabilisation and preconditioning of the Schur complement [BO06; Ols+09; HR13]. The behaviour of the numerical solution in the vicinity of the intersection T h ∩ Γ is stabilised using a CIP-like approach, known as the ghost-penalty method [Bur10; BH12; MSW18]. 

## Discrete weak form of the Navier-Stokes problem

The stabilised, discrete weak form of the N-S problem, M h , is given by

$M h := M (u h , p h , v h , q h ; x) + s ∇div u h (v h , u h ) + s CIP u h (v h , u h ) + s CIP p h (q h , p h ) numerical stabilisation terms = 0 ,(77)$where M is given by (31), and the numerical stabilisation terms are given by

$s ∇div u h (v h , u h ) := K∈T h ∩Ω c ∇div,K ∇ • v h , ∇ • u h K∩Ω ,(78a)$$s CIP u h (v h , u h ) := F ∈F i c u,F [∇v h ], [∇u h ] F ,(78b)$$s CIP p h (q h , p h ) := F ∈F i c p,F [∇q h ], [∇p h ] F . (78c$$)$where F i is the set of interior facets, and [•] is the jump of a function along the face F , as defined in [[MSW18]](#b44). For the numerical method to be robust in a variety of flow conditions, and for the discrete, linearised N-S solution to be unique [[MSW18]](#b44), the Nitsche penalty parameters, η ∆ , η ∇ , and the stabilisation coefficients, c ∇div,K , c u,F , and c p,F , must all scale with the flow. Appropriate voxel-wise constant formulas are given by [Cod02; MSW18]

$η ∆ = γ N h -1 ∥ν e ∥ L ∞ (K) , η ∇ = γ N h -1 ϕ u,K ,(79a)$$c ∇div,K := γ ∇div ∥ν e ∥ L ∞ (K) + h∥u k ∥ L ∞ (K) ,(79b)$$c u,F := c u,F i , F ∈ F i \ F Γ γ ν h∥ν e ∥ L ∞ (K) + c u,F i , F ∈ F Γ , c u,F i := γ u h∥u k ∥ 2 L ∞ (F ) ⟨ϕ p ⟩| F , (79c) ϕ p,K := h 2 ϕ -1 u,K , ϕ u,K := ∥ν e ∥ L ∞ (K) + c u h∥u k ∥ L ∞ (K) , c p,F := γ p h⟨ϕ p ⟩| F ,(79d)$where u k is the velocity field at iteration k, F Γ ⊂ F i is the set of boundary facets, and ⟨•⟩| F is the face average value between two neighbouring voxels [[MSW18]](#b44). Typical values for the numerical parameters are γ N γ ∇div γ u γ p γ ν c u (30, 100) (0.1, 1) (0.01, 0.1) (0.01, 0.1) (0.01, 0.1) (0.1, 1)

.

The numerical (Galerkin) problem is thus to find (u h , p h ) ∈ M h × M h such that (77) holds true for all (v h , q h ) ∈ M h × M h . The stabilised cut-cell finite element method is adopted from [[MSW18]](#b44). In this study we slightly modify the formulation by including ∇-div stabilisation in order to improve the condition number of the Schur complement [BO06; Ols+09; HR13].

## Discrete weak form of the viscous Eikonal problem

The discrete weak form of the viscous Eikonal problem, D h , is given by

$D h := D(r h , φ ± h ) + s CIP u h (r h , φ ± h ) = 0 ,(81)$where D is given by (61), and the CIP stabilisation term, s CIP u h (•, •), was defined in section 4.1.1. We further define the smoothed sign function

$sgn h (φ ± h ) := φ ± h φ 2 ± h + h 2 , (82$$)$which replaces the sign function, sgn(x) := x/|x|, of (61) in the case of the discrete problem. As in section 4.1.1, the numerical (Galerkin) problem is to find φ ± h ∈ M h such that (81) holds true for all r h ∈ M h .

## Cut-cell integration

For the numerical evaluation of integrals we use standard Gaussian quadrature for intact cells, K ∈ T □ h . For cut-cells, K ∈ T ▷ h , however, integration must be considered only on the intersection K ∩ Ω for volumetric integrals and on K ∩ Γ for surface integrals. We therefore generalise the approach of [[Mir96]](#b6), which relies on the divergence theorem and replaces the integral over K ∩ Ω with an integral over ∂ K ∩ Ω . The boundary integral on ∂ K ∩ Ω is then easily computed using one-dimensional Gaussian quadrature [[MLL13]](#b28). For instance, let us consider that we want to compute the integral of the Laplacian in a cut-voxel, P, given by

$A pq := P ∇φ p • ∇φ q . (83$$)$If φ is a polynomial test function, we can find coefficients c pqk such that

$A pq = k c pqk P x α k ,(84)$for a multi-index α k . In general, any integral of the weak (Galerkin) formulation can be expressed in the form of (84). The problem is then to i) find the coefficients c pqk , and ii) compute the monomial integrals

$I P k := P x α k .(85)$For polynomial test functions, the coefficients c pqk are easily obtained using symbolic mathematics [[Meu+17]](#b39). They only depend on the integrand, which depends on the boundary value problem at hand, and can thus be stored in a computer file. The monomial integrals, I P k , depend on the geometry, P, and therefore need to be computed as a pre-processing step before assembling the FEM system matrix. For every cut-cell, the monomial integrals that are needed can be computed only once and stored in memory in order to avoid unnecessary extra calculations. The monomial integrals are evaluated only for the cut-cells, K ∈ T ▷ h . Hence, the numerical cost is negligible compared to other bottlenecks in the algorithm, such as the linear system solution. The algorithm to compute I P k , which generalises the methodology presented originally in [[Mir96]](#b6), is described in [Kon23, Appendix D].

## Exact pressure projection via the Schur complement

The (u h ,p h )-system that results after assembling the discrete weak form (31), is given by

$A B -B * D u h p h = f g ,(86)$and has a non-zero pressure-pressure block, i.e. D ̸ = 0 (in contrast to (67)) due to the pressure stabilisation term, s CIP p h (q h , p h ), which has been added in the discrete weak form. Multiplying the first equation of (86) by A -1 , and substituting it into the second equation, we obtain

$Sp h = -B * A -1 f -g ,(87)$where S ≡ -B * A -1 B -D is the Schur complement. After solving (87) for p h , u h can be obtained by solving

$Au h = f -Bp h .(88)$Equivalently, we can write the coupled system (86) in its decoupled form

$A 0 0 S u h p h = f -BS -1 -B * A -1 f -g -B * A -1 f -g . (89$$)$It is worth noting that S is not necessarily sparse, and thus it cannot be assembled. Instead, S is treated as a matrix-vector product operator. This implies that, when an iterative algorithm is used for (87), a linear system with the velocity-velocity subblock, A, is solved at every pressure iteration. For the exact projection method to be efficient we therefore require good preconditioners for both S and A. We also use ∇-div stabilisation for Schur-complement preconditioning, which amounts to adding s ∇div u h (v h , u h ) to the discrete weak form [[HR13]](#b27). We then use the approximation of S,

$S := -B * diag(A) -1 B -D ,(90)$as the preconditioner of the iterative pressure solver. 

## Implementation

The Bayesian inverse Navier-Stokes solver, which incorporates the cut-cell FEM solver, is implemented in Python using MPI (mpi4py, [[DPS05]](#b13)) and PETSc (petsc4py) [[Bal+22a;](#b56)[Bal+22b]](#b57). We solve (87) using FGMRES [[Saa93]](#b4) for the outer iterations (pressure iterations), and GMRES [[SS86]](#b1) for the inner iterations (velocity subblock, A, iterations). We precondition the inner solver using either the overlapping additive Schwarz method (ASM) [[WD87]](#b2), with ILU(0) for each subdomain, or generalised algebraic multigrid (GAMG). In our implementation, ASM is more efficient when run on a workstation, and for relatively small problems, while GAMG performs and scales better in high performance computing clusters. When solving with S, we use GMRES with GAMG. A similar setup is used to solve the viscous Eikonal problem. The uncertainties around the MAP solution can be estimated by computing the eigendecomposition of either the exact or the approximated posterior covariance operators, as was described in section 2.3.1. This requires the integration of a large-scale eigenvalue problem solver, such as SLEPc [[HRV05]](#b14) (slepc4py), into the current software, which is the subject of future work.

## Reconstruction of flow-MRI data in an aortic arch

In this section we demonstrate algorithm 1 by reconstructing flow-MRI data of a steady laminar flow through a physical model of an aortic arch at two different flow conditions: i) low Re and ii) high Re, and for two different signal-to-noise ratios (SNR): i) low SNR (∼ 5) and ii) high SNR (> 200).

## Problem setup

The model geometry, which is shown in figure [6a](#), is taken from a computed tomography scan and was 3D-printed using stereolithography (resin-printing). The characteristic length, based on the inlet diameter, is L = 1.3 cm, and the characteristic velocity is defined as U ≡ Q/A, where Q is the volumetric flow rate, and A is the cross-sectional area of the inlet, Γ i . In the low Re case we have Re = 554 (= U L/ν), where U = 4.26 cm/s, and in the high Re case we have Re = 1526 (= U L/ν), where U = 11.74 cm/s. The field of view (FOV) is 5.39 × 2.53 × 4.51 cm, which is the region of interest (ROI) (shown in figure [6a](#)), and is the same for both the data space and the model space. That is, we reconstruct the flow in the ROI using only the flow-MRI data inside the ROI. The number of voxels and the resolution in the data space and the model space are shown in table 1. Note that, the total number of active voxels in the model space are ∼ 670 × 10 3 , consisting of ∼ 590 × 10 3 interior intact voxels  [6a](#)).

## Prior means for the N-S unknowns

We obtain a level-set function, ψ, of the prior mean geometry, Ω, using Chan-Vese segmentation [CV01; Get12] on the magnitude images (spin density) of the flow-MRI data (see figure [6b](#)). We then solve the vSDF problem (71) to obtain the prior mean of the vSDF, φ± . To regularise the shape of the inferred boundary Γ and avoid oversmoothing, we choose a Re φ ± value of 4. Since the smallest scale of geometric details is controlled by the mesh, i.e. ℓ Γ = h, the diffusion width, δ, of geometric details at distance h from Γ is h/2 (see section 3.2.7). The prior mean of the inlet b.c., ḡi , is obtained by solving

$I -ℓ 2 0 ∆ ḡi = S * u ⋆ Γ i on Γ i ,(91)$where ℓ 0 = 3h, ∆ is the L 2 -extension of the Laplacian that incorporates the zero-Dirichlet (no-slip) b.c. on ∂Γ i , and S * u ⋆ Γ i is the restriction of the model-space projection of the flow-MRI data on the inlet, Γ i . The prior mean of the outlet (natural) b.c., ḡ0 , is set to ḡ0 ≡ (0, 0, 0) due to lack of information, hence the high uncertainty (see table [1](#tab_1)). The characteristic lengths of the covariance operators for the boundary conditions, ℓ g i , ℓ go are set to h for regularisation and numerical stability purposes. Because we used a Newtonian fluid (water) in the flow-MRI experiments, the effective viscosity, ν e , is assumed to be constant in Ω, and is set to νe = U L/Re. The prior covariance operator, C x, is constructed using the values of table 1, as explained in section 3.3.

$data prior reconstruction u ⋆ /u • ū/ u u • /u • •$Table 2: Notation for the low/high SNR reconstruction cases.

It is important to note that the prior means for φ ± and g i are generated from the flow-MRI data that we reconstruct. That is, we do not perform any additional experiment in order to obtain higher-quality priors (e.g. a computed tomography scan for the geometry or a high-resolution flow-MRI scan for the geometry and the inlet b.c.). The prior means for g o and ν e , however, cannot be extracted from the flow-MRI scans, and, therefore, depend on the prior knowledge of the problem.

## Reconstruction results

We demonstrate algorithm 1 at two different flow conditions and two different SNRs for the 3D steady laminar flow in the ROI of the aortic arch (see table 2 for notation). We apply the algorithm on low SNR flow-MRI data, u ⋆ , in order to infer the maximum a posteriori (MAP) estimate, u • , which is the most probable reconstructed velocity field. This highlights the capability of algorithm 1 to accurately reconstruct flow features that are obscured by noise, which relies on the explanatory power of the generalised Navier-Stokes problem. We also apply the algorithm on high SNR flow-MRI data, u • , (i.e. u ⋆ → u • in algorithm 1) in order to infer the MAP estimate, u • • . This highlights the capability of algorithm 1 to reproduce (fit) the high-quality velocity images, and helps us identify data-model discrepancies other than Gaussian white noise (i.e. modelling and/or experimental biases).

## Velocity field reconstruction at low Re

The reconstruction results for the low Re (= 554) case are shown in figure [7](#fig_6). We observe that the low SNR reconstruction, u • , obtained from the low SNR data, u ⋆ , is almost indistinguishable from the high SNR data, u • , and the high SNR reconstruction, u • • , except for minor discrepancies. Also, the high SNR reconstruction, u • • , obtained from the high SNR data, u • , fits the data with visually imperceptible error. We reach the same conclusions by inspecting the data-model discrepancies of figure 8. In the low SNR case, algorithm 1 manages to assimilate physically relevant information, which appears in the form of coherent structures immersed in Gaussian white noise, while filtering out noise and artefacts. The same applies to the high SNR case, the only difference being that model and/or flow-MRI experimental biases dominate the data-MAP residuals over white noise. For this reason, the characteristic scale for the discrepancy is chosen to be U , instead of the noise variance, σ 2 . For this Re number, it is sensible to assume that the bias comes mainly from the flow-MRI experiment because the flow is steady and laminar, and the Newtonian viscosity model describes water to high accuracy. Consequently, the N-S problem (28) can model the velocity field to high accuracy.

## Velocity field reconstruction at high Re

The reconstruction results for the high Re (= 1526) case, shown in figures 9 and 10, can be interpreted in the same way. In the high Re case, however, the velocity field has a richer, finer structure, and sharper gradients compared with the low Re case. This showcases the capability of algorithm 1 to reconstruct details of the flow that are completely obscured by noise. In the low SNR case, we observe that the data-MAP residuals (figure [10](#fig_2)) approximate white noise almost everywhere except for two regions: i) the upper main branch, and ii) the descending aorta. Both regions involve recirculation zones and local flow separation (see u • and u • • in figure [9](#fig_9)). In the high SNR case, we observe that the data-MAP residuals are, in contrast to the low Re case, non-negligible, and that their peak magnitude is at the recirculation region of the descending aorta. For this near-critical Re number, it is possible that the laminar flow transitions to (weakly) turbulent flow in the vicinity of the recirculation bubbles, particularly at the descending aorta[foot_4](#foot_4) [Man+24, Figure [5](#)]. Since flow-MRI data are, to a certain extent, averaged (e.g. due to cross-voxel motion and asynchronous spatial and velocity encoding in the MRI sequence), and given that the high SNR flow-MRI scan acquisition time was ∼ 3.5 hours per velocity component [Kon23, Appendix E.2], it is possible that flow fluctuations during the course of the experiment (e.g. due to temperature and flow rate changes or bubbles) have slightly distorted the velocity images. It is also expected for this averaging bias to increase in magnitude as the Re number increases, since the flow becomes more susceptible to external perturbations.

## Mean reconstruction errors

We quantitatively compare the reconstructions by defining the mean reconstruction error

$E := |Ω| -1 u data -Su model L 2 (I d ) ,(92)$where

$u data ∈ (u ⋆ , u • ), u model ∈ (ū, u, u • , u • •$), and |Ω| is the volume of Ω.

For the low SNR case, we examine the σ u ⋆ -normalised errors, (E/σ u ⋆ ) i , because the noise dominates the residuals (see figures 8a, 10a). Under the assumption of Gaussian white noise, an ideal reconstruction of the low SNR data (i.e. one that perfectly assimilates the data without fitting noise) is expected to have a mean reconstruction error (E/σ u ⋆ ) i = 1. In practice, however, imperfections in the data, such as artefacts, outliers, and interference noise, increase the kurtosis of the distribution (fat-tailed distribution). Thus, values higher than 1 may correspond to an ideal reconstruction. In general, under the assumption of Gaussian white noise, (E/σ u ⋆ ) i ≫ 1 implies that the reconstruction algorithm did not succeed in assimilating all the information (e.g. due to experimental or model biases), while (E/σ u ⋆ ) i ≪ 1 implies that the reconstruction overfits the data. In the extreme case of (E/σ u ⋆ ) i = 0, we have that u data ≡ Su model , i.e. there is no reconstruction. For the low Re case, we find that the MAP reconstruction is ideal, in the sense that the MAP mean errors are near 1.00 (see table [3](#tab_4)). For the high Re case, the mean errors are larger than 1.00, which is due to the model/experimental biases that we have attributed to the Re number being near-critical (see section 5.3.2). However, the MAP mean errors are significantly lower than the prior mean errors, showing that the algorithm has managed to assimilate useful information.

For the high SNR case, we examine the errors (E/U ) i because the experimental/model bias dominates the residuals (see figures 8b, 10b). We find that for both the low and high Re cases there is a significant reduction in the mean error (see table [3](#tab_4)). For the low Re case, the MAP reconstruction fits the high SNR data with high accuracy and the mean errors are negligible. For the high Re case, the mean errors are non-negligible due to the model/experimental biases that we have attributed to the near-critical Re number (see section 5.3.2). For comparison, volumetric flow rates calculated from the flow-MRI images, in the ascending portion of the aorta, agreed with volumetric flow rates measured from the pump outlet within an average error of ±0.04U .

Figure [11](#fig_2) shows the optimisation logs of the four reconstruction problems. Note that the high SNR reconstruction problems converge more slowly than the low SNR reconstruction problems because there is more information to be assimilated.

## Explanatory power and interpretability of the N-S problem

It is worth noting that the N-S unknowns, x, as chosen in this study, correspond to physical quantities that are interpretable. For example, in cardiovascular applications, the fluid domain, Ω, defines      prior velocity field low SNR (E/σ u ⋆ ) high SNR (E/U ) low Re (1.09, 1.04, 1.23) (0.21, 0.14, 0.30) high Re (1.20, 1.21, 1.54) (0.32, 0.34, 0.52) posterior (MAP) velocity field low SNR (E/σ u ⋆ ) high SNR (E/U ) low Re (1.00, 1.00, 1.02) (0.04, 0.04, 0.06) high Re (1.07, 1.06, 1.12) (0.17, 0.15, 0.24) Table 4: Dimensionality of the problem assuming a discrete domain of O(n 3 ) voxels.

the geometry of the blood vessel, the boundary condition g i defines the inlet velocity, the boundary condition g o defines the fluid stresses at the outlets, and the effective kinematic viscosity, ν e , depends on the rheological properties of the fluid [[KHM24]](#b64). Although we have neglected the equations of elastodynamics, we have not fixed the shape of Ω. Therefore, for unsteady flows and compliant walls, we can still infer the movement of the geometry and its influence on the velocity field, but we cannot yet infer the elastic properties of the walls.

Another important aspect regarding the choice of the N-S parameters, x, is that they have limited control over the modelled velocity field, u. More precisely, every N-S unknown is a function defined on a (d -1)-manifold, where d is the dimension of the problem. This means that the inverse N-S problem is a sparsifying transform[foot_5](#foot_5) [KJ23] that maps velocity fields, defined on a subset of R d , to N-S unknowns, defined on a subset of R d-1 (see table [4](#)). In contrast, the inverse, forced N-S problem, which is obtained after adding a volumetric forcing term, f , to the momentum equation in order to enhance control (i.e. increase model flexibility to fit the data), is no longer a sparse transformation. It can be made sparse, however, if f is parametrised such that f = f (z), where z is an unknown defined on a (d -1)-manifold or a subset thereof. In addition, the inverse problem is more interpretable if z has a physical meaning, which is preferable.

Because we limit the control to physically-interpretable parameters in R d-1 , we rely on the explanatory power of the N-S problem, which relies on first principles, such as momentum and mass conservation, and explains much from little. This an advantage because it provides insight into the physical problem.

If the model cannot explain the data then it has to be revised, which leads to an improved physical model and the advancement of fluid dynamics modelling. The process of model selection can be made even more rigorous using Bayesian inference [Mac03, Chapter 28][JY22; YJ24a].

## Summary and conclusions

We have formulated a Bayesian inverse N-S problem that assimilates noisy and sparse velocimetry data in order to jointly reconstruct the flow field and infer the unknown N-S parameters. By hardwiring 6.1 Future work: extensions and other applications

In this paper, we have limited our study to 3D steady laminar flows. The framework that we develop, however, readily extends to time-dependent laminar and turbulent flows, be they periodic or unsteady flows, and compliant walls. In particular, the formulation can be extended to reconstruct Reynoldsaveraged turbulent flows and non-Newtonian fluid flows. This has already been done for power law fluids [[KHM24]](#b64), and it further extends to viscoelastic fluids. Concerning the numerics, i.e. the discrete formulation, the algorithm can greatly benefit from adaptive discretisation and multiresolution methods (in this study we used a uniform Cartesian mesh, which is suboptimal and computationally expensive). Even though the algorithm in this paper reconstructs velocimetry data, it can be extended to reconstruct raw, sparse, and noisy flow-MRI data in frequency space, as in [[KJ23]](#b62). Lastly, the Bayesian inversion framework that we adopt is already set up for model comparison (e.g. ranking different viscoelastic or turbulence models based on their evidence), experimental/model bias learning (i.e. inferring the unknown unknowns) [YJ24a, Section 4.2.3], and optimal experiment design [[YJ24b]](#b68) (e.g. design of optimal k-space sampling patterns in order to accelerate flow-MRI scanning for specific types of flows).

![Figure 1: When Z is linear (left), π(x|u ⋆ ) is Gaussian. When Z is weakly nonlinear (middle), π(x|u ⋆ ) can be approximated reasonably well by a Gaussian p.d.f., π(x|u ⋆ ), around x • (Laplace approximation).When Z is strongly nonlinear (right), π(x|u ⋆ ) can even be multimodal, in which case there are multiple critical points which will yield different approximations π(x|u ⋆ ) (even locally, the approximation may be inaccurate).]()

![Figure3: The projection operator S : M → D is used to project the modelled velocity field, u ∈ M , to Su ∈ D, so that it can be compared with the velocity data, u ⋆ . A mask, χ, is commonly used after pre-processing the data to remove irregularities such as outliers (e.g. defective pixels), or regions with no signal. The adjoint operator S * is used to project from D to M (the inverse of S does not exist).]()

![Data-model discrepancy: operators S, C u ⋆ We define the data space D ≡ L 2 (I d ), and the model space M ≡ L 2 (I m ), where I d , I m are bounded subsets of R 3 . The size of I d depends on the field of view (FOV) of the experiment, and the size of I m depends on the field of view of the model. For problems in velocity imaging, such as flow-MRI, the data domain, I d , is a uniform tessellation consisting of N (disjoint) voxels (or pixels in 2D), {V i } N i=1 , such that]()

![Figure4: The geometry, Ω ⊂ I m , is implicitly defined using a viscous signed distance field (vSDF), φ ± ∈ L 2 (I m ), which generates an extension of the unit normal vector on ∂Ω to the whole domain, I m . The shape gradient, ζ ∈ L 2 (Γ), is then extended to the whole domain, I m , along the ν-streamlines, by solving a convection-diffusion problem. This produces a perturbation of the vSDF, φ ′ ± ∈ L 2 (I m ), which models the flow of geometry. The artificial viscosity coefficient, ε φ ± , controls the regularity of the shape ∂Ω by dissipating small-scale features when assimilating noisy velocimetry data, u ⋆ .]()

![Figure 5: In cut-cell FEM the boundary, Γ, is implicitly defined by the zeroth level-set of φ ± . The discretised boundary, Γ h , consists of the facets of the cut-cells. Under the assumption of a geometryresolving mesh, there are only a few different cut-cell types (polyhedra, P, formed by cuboid-plane intersections).]()

![Figure 6: 3D-printed (rigid) physical model of an aortic arch and boundary condition setup (figure 6a). We have obtained flow-MRI data of the full geometry, but we only reconstruct the flow in the region of interest (ROI), I, which is enclosed in a cuboid in R 3 . To reconstruct the flow in I, we only use the flow-MRI data inside I. That is, I d ≡ I m ≡ I.]()

![Figure 7: We apply algorithm 1 on low/high SNR data u ⋆ /u • for the low Re flow (U = 4.26 cm/s). The algorithm first generates a prior velocity field, ū/ u, and then solves a Bayesian inverse N-S problem in order to find the most likely (MAP) reconstruction, u • /u • • . The prior u, which is generated from the high SNR data, u • , is similar to ū and thus not shown.]()

![velocity residuals at low SNR (b) velocity residuals at high SNR]()

![Figure 8: Data-model discrepancies (reconstruction residuals) projected onto the model space, M , for the low SNR case (figure8a) and the high SNR case (figure8b) of the low Re flow. In the low SNR case, coherent structures in the initial residual (data-prior discrepancy) are assimilated and the final residual (data-MAP discrepancy) approximates Gaussian white noise with variance σ 2 . In the high SNR case the results are similar with the exception that experimental (flow-MRI) biases dominate over noise and the characteristic scale of the error is based on U .]()

![Figure 9: As for figure 7 but for the high Re flow (U = 11.74 cm/s).]()

![Figure 10: As for figure 8 but for the high Re flow.]()

![Figure 11: Optimisation log depicting the evolution of the objective functional, J , the data-model discrepancy, U , and the priors, R x , for the four different inverse problems.]()

![54 × 77 469 × 469 × 586 µm 207 × 97 × 174 260 × 260 × 260 µm Problem setup and input parameters for the Bayesian inverse N-S problem. and ∼ 80 × 10 3 cut voxels. For this geometry we have obtained quantitatively accurate high SNR (> 200) flow-MRI (phase-contrast MRI) data, which we corrupt with Gaussian white noise in order to generate the low SNR (∼ 5) flow-MRI data. (Further details regarding the flow-MRI experiment can be found in [Kon23, Appendix E.2], and the SNR is computed as in [KJ23, Section III.A].) The noise magnitude of the low SNR and the high SNR data is shown in table 1. In the N-S problem (28), we set a no-slip (zero-Dirichlet) b.c. on the aorta walls, Γ, a Dirichlet (velocity) boundary condition, g i , on the ascending aorta inlet, Γ i , and natural boundary conditions, g o , on the aorta outlets, Γ o , which are four in total, consisting of the three upper branches and the descending aorta (shown in figure]()

![Mean reconstruction errors for the prior and the posterior (MAP) velocity fields.) O(n 2 ) O(n 2 ) O(1) O(n 3 ) O(n 3 ) O(n 3 )]()

It is possible to hardwire constraints with PINNs[[Lu+21]](#b53), but then the following question arises: do PINNs make for better PDE-solvers? The answer is no[[Gro+24]](#b63). In fact, for inverse Navier-Stokes problems with hard constraints, the success of the method heavily relies on the robustness of the forward/adjoint solvers.

Except if the two spaces are identical, in which case S ≡ I, where I is the identity operator.

In addition to data noise, it is possible to account for model errors by incorporating an additional term ε M , which can be approximated by another Gaussian distribution.

The first variation of M around u k with respect to v and q is zero because u k is a N-S solution.

Qualitative analysis of the MR signal attenuation with and without flow suggests that, at the high Re case, these regions of the flow field have weakly fluctuating velocities, which is indicative of locally turbulent flow.

In other words, the inverse N-S problem can be used for data compression and storage of flow-MRI data.

