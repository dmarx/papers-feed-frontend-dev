- Decision to use a Transformer-based architecture for multi-view reconstruction
- Choice to process multiple images in parallel instead of sequentially
- Adoption of pointmap representation for 3D structure prediction
- Implementation of all-to-all attention mechanism in the fusion transformer
- Design choice to eliminate global postprocessing
- Decision to use confidence-weighted loss for training
- Selection of CroCo ViT as the image encoder
- Use of position embeddings for image patch features
- Strategy for handling unordered and unposed images
- Decision to allow for scalability to over 1000 images during inference
- Choice to implement image masking during training
- Adoption of generalized pointmap loss from DUSt3R
- Implementation of separate decoder heads for local and global pointmaps
- Decision to use random index embeddings for improved generalization
- Choice to incorporate empirical scaling along the view axis for model performance
- Decision to focus on reducing error accumulation during reconstruction
- Implementation of confidence maps alongside pointmaps
- Choice to utilize a 12-layer transformer architecture
- Decision to allow the model to generalize to more views than seen during training
- Strategy for addressing label noise in training data