<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fast3R: Towards 3D Reconstruction of 1000+ Images in One Forward Pass</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-01-23">23 Jan 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jianing</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexander</forename><surname>Sax</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Liang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hao</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ang</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Joyce</forename><surname>Chai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Franziska</forename><surname>Meier</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Fast3R: Towards 3D Reconstruction of 1000+ Images in One Forward Pass</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-01-23">23 Jan 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">3214123457A12D54E2DEF64DC518649C</idno>
					<idno type="arXiv">arXiv:2501.13928v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>github.io/ 3D Reconstruction FPS 1000+ Unordered, Unposed Images Fast3R 1 Forward Pass 3D Reconstruction Camera Poses 0.78 65.49 251.1 DUSt3R Spann3R Fast3R</p><p>Figure 1. Fast3R is a method towards 3D reconstructing 1000+ unordered, unposed images in a single forward pass.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>3D reconstruction from multiple views has long been a foundational task across applications in autonomous navigation, augmented reality, and robotics <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b52">53]</ref>. Establishing correspondences across images, known as multi-view matching, is central to these applications and enables an accurate scene representation. Traditional reconstruction pipelines, such as those based on Structure-from-Motion (SfM) <ref type="bibr" target="#b43">[44]</ref> and Multi-View Stereo (MVS) <ref type="bibr" target="#b17">[18]</ref>, fundamentally rely on image pairs to reconstruct 3D geometry. While effective in some settings, these methods require extensive engineering to manage the sequential stages of feature extraction, correspondence matching, triangulation, and global alignment, limiting scalability and speed.</p><p>This traditional "pipeline" paradigm has recently been challenged by DUSt3R <ref type="bibr" target="#b60">[61]</ref>, which directly predicts 3D structure from RGB images. It achieves this with a design that "cast[s] the pairwise reconstruction problem as a regression of pointmaps, relaxing the hard constraints of usual projective camera models" <ref type="bibr" target="#b60">[61]</ref>, yielding impressive robustness across challenging viewpoints. This represents a radical shift in 3D reconstruction, as an end-to-end learnable solution is less prone to pipeline error accumulation, while also being dramatically simpler.</p><p>On the other hand, a fundamental limitation of DUSt3R is its restriction to two image inputs. While image pairs are an important use case, often one is interested in reconstructing from more than two views, as when scanning of objects <ref type="bibr" target="#b38">[39]</ref> or scenes <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b66">67]</ref>, e.g. for asset generation or mapping. To process more than two images, DUSt3R computes O(N 2 ) pairs of pointmaps and performs a global alignment optimization procedure. This process can be computationally expensive, scaling poorly as the collection of images grows. For instance, it will lead to OOM with only 48 views on an A100 GPU.</p><p>Moreover, such a process is still fundamentally pairwise, which limits the model's context, both affecting learning during training and ultimate accuracy during inference. In this sense, DUSt3R suffers from the same pair-wise bottleneck as traditional SfM and MVS methods.</p><p>We propose Fast3R, a novel multi-view reconstruction framework designed to overcome these limitations. Building on DUSt3R's foundations, Fast3R leverages a Transformer-based architecture <ref type="bibr" target="#b55">[56]</ref> that processes multiple images in parallel, allowing N images to be reconstructed in a single forward pass. By eliminating the need for sequential or pairwise processing, each frame can simultaneously attend to all other frames in the input set during reconstruction, significantly reduces error accumulation. Perhaps surprisingly, Fast3R also takes significantly less time. Our contributions are threefold. 1. We introduce Fast3R, a Transformer-based model for multi-view pointmap estimation that obviates the need for global postprocessing; resulting in significant improvements in speed, computation overhead and scalability. 2. We show empirically that the model performance improves by scaling along the view axis. For camera pose localization and reconstruction tasks, the model improves when trained on progressively larger sets of views. Per-view accuracy further improves when more views are used during inference, and the model can generalize to significantly more views than seen during training. 3. We demonstrate state-of-the-art performance in camera pose estimation with significant inference time improvements. On CO3Dv2 <ref type="bibr" target="#b38">[39]</ref>, Fast3R gets 99.7% accuracy within 15-degrees for pose estimation, over a 14x error reduction compared to DUSt3R with global alignment. Fast3R offers a scalable and accurate alternative for real-world applications, setting a new standard for efficient multi-view 3D reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Multi-view 3D reconstruction: Almost all modern 3D reconstruction approaches are based on the traditional multiview geometry (MVG) pipeline <ref type="bibr" target="#b20">[21]</ref>. MVG-based methods first identify corresponding pixels between image pairs, and then use camera models and projective multiview geometry to lift these correspondences to 3D points. The process happens in sequential stages: feature extraction, finding pairwise image correspondences, triangulation to 3D and pairwise relative camera pose, and global bundle alignment. However, any pipeline approach is prone to accumulating errors, which are especially common in the hand-crafted components. Moreover, the sequential nature prevents parallelization, which limits speed and scalability. MVG approaches have existed since the early days of computer vision, and are still in use for a reason: they can be highly accurate when they do not catastrophically fail. The latest multi-view geometry pipelines like COLMAP <ref type="bibr" target="#b43">[44]</ref> or OrbSLAM2 <ref type="bibr" target="#b29">[30]</ref> incorporate nearly 60 years of compounding engineering improvements, but these approaches still catastrophically fail &gt;40% of the time on static scenes like ETH-3D <ref type="bibr" target="#b51">[52]</ref>), which can actually be considered an easy case due to dense image coverage of the scene.</p><p>Much recent work has successfully addressed the robustness and speed by replacing increasingly large components of MVG pipelines with end-to-end learned versions that are faster and reduce the rate of catastrophic failures <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b71">72]</ref>. For example, <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b67">68]</ref> improve feature extraction and correspondences, <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b70">71]</ref> learn to estimate camera pose, and <ref type="bibr" target="#b51">[52]</ref> introduce a bundle adjustment layer. <ref type="bibr" target="#b60">[61]</ref> contains an excellent and comprehensive survey of such efforts. Overall, the trend is towards replacing increasingly large components with end-to-end solutions. Pointmap representation: DUSt3R <ref type="bibr" target="#b60">[61]</ref> takes this evolution the furthest by proposing pointmap regression to replace everything in the MVG pipeline up to global pairwise alignment. Rather than first attempting to solve for camera parameters in order to triangulate corresponding pixels, DUSt3R trains a model to directly predict 3D pointmaps for pairs of images in a shared coordinate system. Other MVG component tasks such as relative camera pose estimation and depth estimation can be recovered from the resulting pointmap representation. However, DUSt3R's pairwise assumption is a limitation, as it requires inference on O(N 2 ) image pairs and then a global alignment optimization, which is per-scene and does not improve with more data. Moreover, this process quickly becomes slow or crashes due to exceeded system memory, even for relatively modest numbers of images.</p><p>DUSt3R has inspired several follow-ups. MASt3R <ref type="bibr" target="#b24">[25]</ref> ViT Encoder </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fusion Transformer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Per-View Tokenization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global Head</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global Fusion</head><p>Decode XYZ and Confidence</p><p>… ViT Encoder ViT Encoder ViT Encoder Local Head Global Head Local Head Global Head Local Head Global Head Local Head Random Idx. Emb. Random Idx. Emb. Random Idx. Emb. … … Confidence Point Map RGB Point Map Per-View Point Map Shared Weights Shared Weights Shared Weights Shared Weights Many Views … Figure 2. Model architecture of Fast3R. Built upon a novel Transformer-based architecture which supports bidirectional information flow, Fast3R is able to process dense input views simultaneously.</p><p>adds a local feature head to each decoder's output, while MonST3R <ref type="bibr" target="#b68">[69]</ref> does a data-driven exploration of dynamic scenes, but both are still fundamentally pairwise methods. MASt3R in particular does not make any changes to the global alignment methodology. Concurrently with our work, Spann3R <ref type="bibr" target="#b56">[57]</ref> treats images as an ordered sequence (e.g. from a video) and incrementally reconstructs a scene using a pairwise sliding window network, along with a learned spatial memory system. This extends DUSt3R to handle more images, but Spann3R's incremental pairwise processing cannot fix reconstructions from earlier frames, which can cause errors to accumulate. Crucially, Fast3R's transformer architecture uses all-to-all attention, allowing the model to reason simultaneously and jointly over all frames without any assumption of image order. Fast3R removes sequential dependencies, enabling parallelized inference across many devices in a single forward pass.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Model</head><p>Fast3R is a transformer-based model that predicts a 3D pointmap from a set of unordered and unposed images. The model architecture is designed to be scalable to over 1000 images during inference, though during training we use image masking to train it with far fewer. In this section, we detail our implementation of Fast3R, and discuss the design choices that enable its scalability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem definition</head><p>Taking a set of (N ) unordered and unposed RGB images I ∈ R N ×H×W ×3 as inputs 1 , Fast3R reconstructs the 3D structures of the scene by predicting the corresponding 1 We assume all images are resized to the same H × W for simplicity.</p><p>pointmap X, where X ∈ R N ×H×W ×3 . A pointmap is a set of 3D locations indexed by pixels in an image I, enabling the derivation of camera poses, depths, and 3D structures. Fast3R predicts local and global pointmaps X L and X G , and corresponding confidence maps Σ L and Σ G (of shape Σ ∈ R N ×H×W ). Fast3R maps N RGB images to</p><formula xml:id="formula_0">Fast3R : I → (X L , Σ L , X G , Σ G )</formula><p>Like MASt3R, the global pointmap X G is in the coordinate frame of the first camera and the X L is in the coordinate frame of the viewing camera, as shown in Figure <ref type="figure">2</ref> 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.2. Training Objective</head><p>This section describes the loss, using the notation in Section 3.1 above. Fast3R's predictions of ( XL , ΣL , XG , ΣG ) are trained using generalized versions of the pointmap loss in DUST3R <ref type="bibr" target="#b60">[61]</ref>.</p><p>Our total loss is the combination of pointmap losses for the local and global pointmaps:</p><formula xml:id="formula_1">L total = L XG + L XL<label>(1)</label></formula><p>which are confidence-weighted versions of the normalized 3D pointwise regression loss. Normalized 3D pointwise regression loss: The normalized regression loss for X is a multi-view version of that in DUST3R <ref type="bibr" target="#b65">[66]</ref> or monocular depth estimation <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b65">66]</ref>. It is the L 2 loss between the normalized predicted pointmaps and normalized target pointmaps, rescaled by the mean Euclidean distance to the origin: Note that the predictions and targets are independently normalized by the mean euclidean distance to the origin.</p><formula xml:id="formula_2">ℓ regr ( X, X) = 1 ẑ X - 1 z X 2 , z = 1 |X| x∈X ∥x∥ 2 (2)</formula><p>Pointmap loss: As in <ref type="bibr" target="#b60">[61]</ref>, we use a confidence-adjusted version of the loss above, using the confidence score Σ predicted by the model. The total loss for a pointmap is</p><formula xml:id="formula_3">L X ( Σ, X, X) = 1 |X| Σ+ • ℓ regr ( X, X) + α log( Σ+ )</formula><p>(3) Since the log term requires the confidence scores to be positive, we enforce Σ+ = 1 + exp( Σ). Our intuition is that the confidence weighting helps the model deal with label noise. Like DUST3R, we train on real-world scans typically containing systematic errors in the underlying pointmap labels. For example, glass or thin structures are often not reconstructed properly in the ground-truth laser scans <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b66">67]</ref>, and errors in camera registration will cause misalignments between the images and pointmap labels <ref type="bibr" target="#b65">[66]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Model architecture</head><p>The Fast3R meta-architecture is inspired by DUSt3R, and has three components: image encoding, fusion transformer, and pointmap decoding, as shown in Figure <ref type="figure">2</ref>. We emphasize that Fast3R makes no assumptions on the ordering of images in I, and all output pointmaps and confidence maps (X L , Σ L , X G , Σ G ) are predicted simultaneously, not sequentially. Image encoder: Fast3R encodes each image I i ∈ I to a set of patch features H i , using a feature extractor F. This is done independently per image, yielding a sequence of image patch features H i = {h i,j } HW/P 2 j=1 for each image:</p><formula xml:id="formula_4">H i = F(I i ), i ∈ 1, ..., N<label>(4)</label></formula><p>We follow DUSt3R's design and use CroCo ViT <ref type="bibr" target="#b62">[63]</ref> as our encoder, though we found DINOv2 <ref type="bibr" target="#b32">[33]</ref> works similarly. Before passing image patch features H to the fusion transformer, we add position embeddings with onedimensional image index positional embeddings.</p><p>Index embeddings help the fusion transformer determine which patches come from the same image and are the mechanism for identifying I 1 , which importantly defines the global coordinate frame. This is critical for allowing the model to implicitly reason about camera pose jointly for all images from an otherwise permutationally invariant set of tokens. Fusion transformer: Most of the computation in Fast3R happens in the fusion transformer, which has a generic architecture. We use a 12-layer transformer similar to ViT-B <ref type="bibr" target="#b11">[12]</ref> or BERT <ref type="bibr" target="#b9">[10]</ref>, however this could be scaled up. This fusion transformer takes the concatenated encoded image patches from all views and performs all-to-all self-attention. This operation provides Fast3R with full context, beyond the information provided in pairs alone. Pointmap head: Finally, Fast3R uses separate DPT-L <ref type="bibr" target="#b36">[37]</ref> decoder heads to map these tokens to the local and global pointmaps (X L , X G ), and confidence maps (Σ L , Σ G ). Image index positional embedding generalization: We would like Fast3R to be able to handle many views at inference, more than were used to train a model. A naïve way to embed views during testing would be to embed them in the same way as training: i.e. use the same Spherical Harmonic frequencies <ref type="bibr" target="#b48">[49]</ref> to embed raw indices SH({1, ..., N }) during training, and SH({1, ..., N test }) during inference. In LLMs this causes poor performance, and in preliminary experiments we also found that the resulting model did not work well when the number of input images exceeded that used during training. We therefore adapt Position Interpolation <ref type="bibr" target="#b4">[5]</ref>, a solution from LLMs, where during training we randomly draw N indexes from a larger pool N ′ of possible samples. <ref type="bibr" target="#b4">[5]</ref> draws samples using a regular grid since the LLM inputs form a regular ordered sequence. Our images are unordered, so we draw N ⊂ {1, ..., N ′ } uniformly at random. To the transformer, the strategy looks indistinguishable from masking out images, and N ′ ≫ N controls the masking ratio. <ref type="foot" target="#foot_0">2</ref> This strategy enables Fast3r to handle N = 1000 images during inference, even if only trained with N = 20 images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Memory-Efficient Implementation</head><p>With a standard transformer architecture and a single-pass inference procedure, Fast3R is able to leverage many of the recent advances designed to improve scalability at train and inference time <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b53">54]</ref>.</p><p>For example, model size and throughput can be increased by sharding the model and/or data minibatch across multiple machines, such as through model parallelism <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b44">45]</ref>, data parallelism <ref type="bibr" target="#b25">[26]</ref>, and tensor parallelism <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b45">46]</ref>. During training, optimizer weights, states, and gradients can also be sharded <ref type="bibr" target="#b34">[35]</ref>. Systems-level advances have also been proposed, such as FlashAttention <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>, which uses GPU kernels leveraging the hardware topology to compute attention in a time and memory-efficient way. These are implemented in libraries such as FAIRScale <ref type="bibr" target="#b15">[16]</ref>, Deep-Speed <ref type="bibr" target="#b34">[35]</ref> and Huggingface <ref type="bibr" target="#b63">[64]</ref>, and require significant engineering effort.</p><p>The Fast3R meta-architecture is explicitly designed to take advantage of these efforts. We leverage two different forms of parallelism at training and inference time, as well as FlashAttention, described in more detail in Sec. 4. More broadly, we believe that our approach will continue to benefit in the longer term as transformer-based scaling infrastructure continues to mature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Training Data We train on a mix of real-world objectcentric and scene scan data: CO3D <ref type="bibr" target="#b38">[39]</ref>, ScanNet++ <ref type="bibr" target="#b66">[67]</ref>, ARKitScenes <ref type="bibr" target="#b3">[4]</ref>, and Habitat <ref type="bibr" target="#b42">[43]</ref>. We use a subset of the datasets in DUSt3R, specifically 4 of the 9 datasets, for a total of around 2000 unique scenes scans and 1300 videos of 50 object classes. Note that this is only 7% of CO3D, which is also what the baselines DUSt3R <ref type="bibr" target="#b60">[61]</ref>, Spann3R <ref type="bibr" target="#b56">[57]</ref>, and MASt3R <ref type="bibr" target="#b24">[25]</ref> use. Baselines DUSt3R <ref type="bibr" target="#b60">[61]</ref> is the closest approach to ours, and competitive on visual odometry and reconstruction benchmarks. That paper contains extensive comparisons against other methods, and we adopt it as our main baseline. We additionally consider DUSt3R's follow-up work, MASt3R <ref type="bibr" target="#b24">[25]</ref>, as well as a concurrent work Spann3R <ref type="bibr" target="#b56">[57]</ref>, which also seeks to replace DUSt3R's expensive global alignment stage by sequentially processing frames with a spatial memory. For camera pose estimation and 3D reconstruction, we include comparisons to task-specific methods. Architecture Details In our experiments we use the following components for the meta-architecture: following the DPT-L architecture <ref type="bibr" target="#b36">[37]</ref>. Training Details Our models are trained on 512 × 512 images using AdamW <ref type="bibr" target="#b28">[29]</ref> for 6.5K steps, with a learning rate of 0.0001 and cosine annealing schedule. Unlike DUSt3R, we do not use staged training. For the most part, we use the same dataloaders used in the public baseline implementations. We train with batch size 64, with each sample consisting of a tuple of N = 20 views. This process takes 6.13 days on 64 A100 GPUs. We additionally make use of several strategies to enable efficient training. First, we use the FlashAttention <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> to improve time and memory efficiency. Even so, a naïve implementation runs out of memory even with batch size 1 when N &gt; 16, so we use Deep-Speed ZeRO stage 2 training <ref type="bibr" target="#b34">[35]</ref>, whereby optimizer states, moment estimates, and gradients are partitioned across different machines. This enables us to train with up to N = 28 views per data sample, with a batch size of one per GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Inference Efficiency</head><p>At inference time, we aim to handle 1000+ views compared to 20 during training, which requires additional optimizations. We observe the memory bottleneck at inference is in the DPT heads producing the pointmaps: with 320 views on a single A100 GPU, over 60% of VRAM is consumed by activations from the DPT heads, largely due to each needing to upsample 1024 tokens into a high-resolution 512 × 512 image. To address this, we implement a simple version of tensor parallelism, putting the model on GPU 0 and then Methods Co3Dv2 <ref type="bibr" target="#b38">[39]</ref> FPS RRA@15↑ RRA@5↑ RTA@15↑ RTA@5↑ mAA(30)↑ Colmap+SG <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b40">41]</ref> 36 Table 2. System performance metrics for different view counts on Fast3R and DUSt3R on a single A100. Time is measured in seconds (s), and memory is measured in gibibytes (GiB). Each view is 512x384 in resolution. For DUSt3R, at 48 views the N 2 pairwise reconstructions eventually consume all VRAM at its global alignment stage. Note that Fast3R's reported fastest FPS of 251.1 uses 108 views in 224x224 resolution.</p><p>copying the DPT heads to each of the K -1 other GPUs.</p><p>When processing a batch of N ≈ 1000 images, we pass the entire batch through the ViT encoder and global fusion decoder, and then split the outputs across K machines for parallel DPT head inference. Table <ref type="table">2</ref> shows the inference time and memory usage as we increase the number of views. Fast3R is able to process up to 1500 views in a single pass, whereas DUSt3R runs out of memory past 32. Fast3R also has a significantly faster inference time, with gains that increase with more views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Camera Pose Estimation</head><p>We evaluate camera pose estimation on unseen trajectories from 41 object categories from CO3D <ref type="bibr" target="#b38">[39]</ref>. Following <ref type="bibr" target="#b60">[61]</ref>, we sample 10 random views from each trajectory.</p><p>Inspired by DUSt3R <ref type="bibr" target="#b60">[61]</ref>, we estimate the focal length, camera rotation, and camera translation from the predicted global pointmaps. We begin by initializing a set of random focal length guesses based on the image resolution, then use RANSAC-PnP to estimate the camera's rotation and translation based on the guessed focal lengths and the global pointmap. Table 4. Quantitative results on object-centric DTU [1] dataset. Using a skip=5 on trajectories of 49 frames.</p><p>is used to score each guessed focal length (lower is better), and the best-scoring focal length is selected to compute the intrinsic and extrinsic camera matrices.</p><p>During RANSAC-PnP, we only use points with the top 15% confidence scores predicted by Fast3R, ensuring efficient PnP processing and reducing outliers. If all images are known to originate from the same physical camera, we use the focal length estimated from the first view as the focal length for all cameras, as this initial estimate has been empirically found to be more reliable. Otherwise, we independently estimate the focal length for each input. It is worth noting that the camera pose estimation process is parallelized using multi-threading, ensuring minimal wallclock time. Even for hundreds of views, the process completes in just a few seconds on standard CPUs.  We report Relative Rotation Accuracy (RRA) and Relative Translation Accuracy (RTA) at a threshold of 15 • , mean Average Accuracy (mAA) at threshold 30 • , and model frames per second (FPS) in Table <ref type="table" target="#tab_1">1</ref>. On Co3D, Fast3R surpasses all other methods across the RRA and mAA metrics, achieving near-perfect RRA, while remaining competitive on RTA. Importantly, it is also orders of magnitude faster: 200× faster than DUSt3R and 700× faster than MASt3R.</p><p>Figure <ref type="figure" target="#fig_2">4</ref> and Figure <ref type="figure" target="#fig_4">5</ref> shows that Fast3R's predictions improve with more views, indicating that the model is able to use the additional context from multiple images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">3D Reconstruction</head><p>We evaluate Fast3R's 3D reconstruction on scene-level benchmarks: 7-Scenes <ref type="bibr" target="#b46">[47]</ref> and Neural RGB-D <ref type="bibr" target="#b2">[3]</ref>, and the object-level benchmark DTU <ref type="bibr" target="#b0">[1]</ref>.</p><p>We found that local pointmaps learn finer detail than the global pointmaps. Therefore we use the local pointmaps for detail and the global pointmaps for the high-level structure. Specifically, we independently align each image's local pointmap to the global pointmap using ICP, and use aligned local pointmaps for evaluation.</p><p>Fast3R is competitive with other pointmap reconstruction methods like DUSt3r and MASt3R, while being significantly faster, as shown in Table <ref type="table" target="#tab_3">3</ref> and <ref type="table">Table 4</ref>. We believe that Fast3R will continue to improve with better reconstruc-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">4D Reconstruction: Qualitative Results</head><p>Because Fast3R can handle multiple frames naturally, one may wonder how well Fast3R can handle dynamic scenes. We qualitatively test Fast3R's 4D reconstruction ability, showing examples of dynamic aligned pointmaps at multiple time steps in Figure <ref type="figure" target="#fig_5">6</ref>. Fast3R can be trained to achieve such results by finetuning a 16 static views checkpoint on the PointOdyssey <ref type="bibr" target="#b72">[73]</ref> and TartanAir <ref type="bibr" target="#b61">[62]</ref> datasets, consisting of 110 dynamic and 150 static scenes, respectively. We freeze the ViT encoder, use 224x224 image resolution, and swap in a newly-initialized global DPT head. We fine-tune the model with 15 epochs with a frame length of 16, batch size per GPU of 1, and use the same learning rate schedule as Fast3R. The process takes 45 hours to finetune on 2 Nvidia Quadro RTX A6000 GPUs.</p><p>We see that our approach produces qualitatively reasonable reconstructions with minimal changes. MonST3R <ref type="bibr" target="#b68">[69]</ref> is a concurrent work also tackling dynamic scene reconstruction that builds atop DUSt3R. However, like DUSt3R, it assumes a pairwise architecture and also uses a separate model to predict optical flow. We show that the same Fast3R architecture trained end-to-end with the same many-view pointmap regression (just swapping the data to dynamic scenes), can also work for 4D reconstruction. Importantly, our method remains significantly faster, opening the poten-No rand. emb. tial for real-time applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Ablation Studies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Scaling the number of views</head><p>Fast3R is able to use all-to-all attention during training, which lets it learn from global context. We hypothesize that the additional context provided by more views during training allows the model to learn higher-order correspondences between multiple frames, ultimately increasing model performance and increasing potential for scaling.</p><p>Figures <ref type="figure" target="#fig_6">7</ref> and <ref type="figure" target="#fig_7">8</ref> shows that training on increasingly more views consistently improves RRA and RTA for visual odometry, and reconstruction accuracy-even when the number of views used during evaluation is held constant and the model is ultimately evaluated on fewer views than were seen during training. We further evaluate the model's ability to reason about additional views by increasing the number of images that Fast3R sees during inference. Figure <ref type="figure" target="#fig_2">4</ref> and Figure <ref type="figure" target="#fig_4">5</ref> indicate that as the model uses more views, the average per-view performance improves. This behavior holds for all evaluated metrics in both camera pose estimation and reconstruction. As shown in Figure <ref type="figure" target="#fig_4">5</ref>, the model has a better per-view accuracy using 50 images than it does with 20, even though it was trained with 20. Many applications (e.g. reconstruction, odometry) require inference on many views, which is a major motivation for Fast3R removing the pairwise constraint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Training without position interpolation</head><p>In Section 3.3, we introduced a randomized version of <ref type="bibr" target="#b4">[5]</ref> to enable inference on more views than seen training. Without this technique, model accuracy quickly degrades for pointmap corresponding to image indexes outside the training range, as shown in Figure <ref type="figure" target="#fig_8">9</ref>  ity pointmaps for views in slot 5 to 24 (Figure <ref type="figure" target="#fig_8">9</ref> bottom).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Removing the local decoder head</head><p>Table <ref type="table" target="#tab_5">5</ref> shows that removing the local head learns finer details before the global head does. We hypothesize that this is because the global head needs to first learn the coordinate system and then learn the fine details. As the model improves, the local head may not be necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We introduce Fast3R, a transformer that predicts 3D locations for all pixels in a common frame of reference, directly in a single forward pass. By replacing the whole SfM pipeline with a generic architecture trained end-toend, Fast3R and similar approaches should benefit from the usual scaling rules for transformers: consistent improvement with better data and increased parameters. Since Fast3R uses global attention, it avoids two potentially artificial scaling limits due to bottlenecks in existing systems.</p><p>First, the bottleneck of image pair reconstruction restricts the information available to the model. Second, pairwise global optimization can only make up for this so much and does not improve with more data. With our efficient implementation, Fast3R can operate at &gt; 250 FPS, and process 1500 images in one forward pass, far exceeding other methods while achieving competitive results on 3D reconstruction and camera pose estimation benchmarks. We demonstrate that Fast3R can be finetuned to reconstruct videos by changing the data and without modifying the pointmap regression objective and architecture. In contrast with pipeline approaches bottlenecked by custom and slow operations, Fast3R inherits the benefits of future engineering improvements to efficiently serve and train large transformer-based models. For example, packages like Deepspeed-Inference <ref type="bibr" target="#b37">[38]</ref>, FlashAttention <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> provide fused kernels, model parallelism, and data parallelism. These speed up inference and reduce memory requirements, allowing more images per device, and the number of images scales with the number of devices. Limitations: A current limiting factor for scaling may be data accuracy and quantity. Synthetic data <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b39">40]</ref> could be a solution as, broadly speaking, models trained for geometry estimation seem to generalize well from simulation data.</p><p>Fast3R can successfully use simulated data to train for 4D reconstruction, showing generalization results on DAVIS. Similarly, DepthAnythingV2 <ref type="bibr" target="#b65">[66]</ref> showed the potential of this approach to scale for monocular depth estimation.</p><p>The architecture of Fast3R allows for parallel processing of many views, and its positional embedding design enables "train short, test long" in terms of context length of views. However, we observed that for scenes with very large reconstruction areas, when the number of views becomes extreme (e.g., more than 200), the point map of some views (particularly those with a low confidence score) begins to exhibit drifting behavior. One current way to address this issue is to drop frames with low confidence scores. In dense reconstruction, this approach typically does not hurt reconstruction quality too much. However, to fundamentally address this problem, we hypothesize that future work could explore the following avenues: (1) incorporating more data containing large scenes to improve generalization to such cases; (2) designing better positional embeddings inspired by state-ofthe-art long-context language models <ref type="bibr" target="#b73">[74]</ref>, which can handle very long context lengths and exploit the temporal structure of ordered image sequences (e.g., video).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fast3R: Towards 3D Reconstruction of 1000+ Images in One Forward Pass</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Model Scaling Effect</head><p>We investigate the effect of scaling model size by trying three model sizes for the Fusion Transformer: ViT-base, ViT-large, and ViT-huge, according to the settings in the original ViT paper <ref type="bibr" target="#b11">[12]</ref>. The results are shown in Figure <ref type="figure" target="#fig_11">11</ref>. This experiment demonstrates that larger model size continually benefits 3D tasks including camera pose estimations and 3D reconstruction. Note that the Fusion Transformer size used in the main text for all experiments is a ViT-base.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Data Scaling Effect</head><p>We study the effect of scaling the data using 4 different scales of data, 12.5%, 25%, 50%, and 100%, to train the model. The results are shown in Figure <ref type="figure" target="#fig_14">12</ref>. The training settings for all models are kept the same except for how much data they have access to. The results demonstrate that Fast3R continually benefits from more data, suggesting Fast3R could achieve better results in the future given more data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Gaussian Splatting</head><p>We qualitatively demonstrate the potential of using Fast3R's output for downstream novel view synthesis tasks. A visualization of the Gaussian Splatting generated by adopting the pipeline of InstantSplat <ref type="bibr" target="#b16">[17]</ref> is shown in Figure <ref type="figure" target="#fig_15">13</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Bundle Adjustment (via Gaussian splatting)</head><p>While not necessary, using bundle-adjustment at inference time can also improve Fast3R's performance. We show an example of bundle adjustment using Gaussian Splatting (GS-BA).</p><p>Specifically, we use InstantSplat <ref type="bibr" target="#b16">[17]</ref> to optimize a set of gaussians per-scene, using initializations from a pointcloud, and updates the locations and poses in order to minimize reprojection error. We show an example of the Gaussian reconstruction in Figure <ref type="figure" target="#fig_15">13</ref> shows an example reconstruction on CO3D.</p><p>We can compare against ground-truth trajectories from COLMAP. We found that GS-BA significantly reduces both the pose and translation error. Table <ref type="table">6</ref> quantifies this, showing over a 2.5x reduction in translation error and 4x redution in rotational error on the "Family" scene from Tanks and Temples, which we found to be representative. We show a visualization of the original reconstruction and the poses pre-and post-bundle adjustment. There are only 8 scenes in the evaluation set in InstantSplat. Table <ref type="table">6</ref>. Pose estimation can further improve with Bundle Adjustment. We show an example on the "Family" scene from Tanks and Temples, using InstantSplat <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. More Visualizations</head><p>We show more visualizations of Fast3R's performance on indoor scenes in Figure <ref type="figure" target="#fig_10">10</ref>. Fast3R learns the regularity of indoor rooms (square-like shapes) and demonstrates "loop closure" capabilities.</p><p>In Figure <ref type="figure" target="#fig_4">15</ref>, we visualize the point cloud produced using local vs. global point maps.       </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Qualitative examples of Fast3R's output. The text on the yellow sign says "Caution, cleaning in progress" and is legible if zoomed in.</figDesc><graphic coords="4,192.44,168.02,94.70,97.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Pose accuracy with more views: Fast3R improves with the context from more views. Fast3R saturates the orientation portion of the benchmark, even using 3-5 views.</figDesc><graphic coords="5,314.96,81.01,122.89,73.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1 .</head><label>1</label><figDesc>The Image Encoder uses a CroCo ViT-B<ref type="bibr" target="#b62">[63]</ref> architecture, initialized with DUSt3R pretrained weights<ref type="bibr" target="#b60">[61]</ref>. 2. The Fusion Transformer is a 12-layer ViT-B model using the BERT architecture with 12 layers, 12 heads, embedding dimension size 768 and MLP ratio 4.0. We use a pool size of N ′ = 1000 image index embeddings. 3. The Pointmap Decoder is a dense vision Transformer</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. DTU reconstruction quality vs. test number of views. Accuracy and Completion (lower is better) get better as we inference with more views.</figDesc><graphic coords="7,65.83,71.01,218.57,72.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Qualitative 4D reconstruction results on unseen dynamic scenes in DAVIS. Results are obtained with one forward pass. The tracks are visualized using ground-truth track annotations from TAP-Vid-DAVIS [11].</figDesc><graphic coords="7,58.50,180.16,236.24,144.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Increasing # views during training: camera pose estimation on CO3D. Estimates of both orientation (RRA@5) and translation (RTA@5) improve with more views.</figDesc><graphic coords="7,325.97,69.62,218.20,72.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Increasing # views during training: reconstruction on 7scenes and NRGBD. Accuracy and Completion (lower is better) get better as we train with more views. Normal Consistency (high is better) also gets better as we train with more views.</figDesc><graphic coords="7,334.69,191.69,201.60,115.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Effect of sampling image index PE during training. If we train the model without sampling index embeddings, regression loss spikes (orange) when testing with more views than seen at training (top). Our embedding strategy performs comparably even with 6× the number of views during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Visualizations of results on NRGBD scenes. Fast3R learns the regularity of indoor rooms (square-like shapes) and demonstrates "loop closure" capabilities.</figDesc><graphic coords="14,78.56,280.07,119.49,110.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Model scaling effect. Increasing the size of the Fusion Transformer leads to better camera pose estimation (↑) and 3D reconstruction (↓). All models are trained for 60k steps (equivalent to 60 epochs; the main paper uses 100 epochs).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. Data scaling effect. More training data leads to better camera pose estimation (↑) and 3D reconstruction (↓). All models are trained for 60k steps (equivalent to 60 epochs; the main paper uses 100 epochs).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 13 .</head><label>13</label><figDesc>Figure 13. Visualization of Gaussians from unseen poses. The frames are ordered temporally along the direction of the arrows. The middle frames show poses very different from those used for reconstruction, as is evidenced by the large areas with no Gausisans. The scene is fit from 7 images from CO3D.</figDesc><graphic coords="15,58.50,110.17,494.99,199.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 14 .</head><label>14</label><figDesc>Figure 14. Bundle adjustment further improves pose. Left: reconstruction from Fast3R. Middle: Original poses pre-GS-BA. Right: Poses after GS-BA.</figDesc><graphic coords="15,58.50,436.40,494.99,204.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Multi-view pose regression on the CO3Dv2<ref type="bibr" target="#b38">[39]</ref> dataset with 10 random frames. Parentheses denote methods that do not report results on the 10 views set; we report their best for comparison (8 views). Fast3R does not assume known camera intrinsics.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>.1</cell><cell>24.4</cell><cell>27.3</cell><cell>17.2</cell><cell>25.3</cell><cell>0.056</cell></row><row><cell></cell><cell></cell><cell>PixSfM [28]</cell><cell>33.7</cell><cell>26.1</cell><cell>32.9</cell><cell>17.6</cell><cell>30.1</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>RelPose [70]</cell><cell>57.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.02</cell></row><row><cell></cell><cell></cell><cell>PosReg [59]</cell><cell>53.2</cell><cell>-</cell><cell>49.1</cell><cell>-</cell><cell>45.0</cell><cell>0.015</cell></row><row><cell></cell><cell></cell><cell>PoseDiff [59]</cell><cell>80.5</cell><cell>59.5</cell><cell>79.8</cell><cell>61.7</cell><cell>66.5</cell><cell>0.015</cell></row><row><cell></cell><cell></cell><cell>RelPose++ [27]</cell><cell>(85.5)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.02</cell></row><row><cell></cell><cell></cell><cell>DUSt3R [60]</cell><cell>96.2</cell><cell>-</cell><cell>86.8</cell><cell>-</cell><cell>76.7</cell><cell>0.78</cell></row><row><cell></cell><cell></cell><cell>MASt3R [24]</cell><cell>94.6</cell><cell>93.2</cell><cell>91.9</cell><cell>86.2</cell><cell>81.8</cell><cell>0.23</cell></row><row><cell></cell><cell></cell><cell>Fast3R (Ours)</cell><cell>99.7</cell><cell>97.4</cell><cell>87.1</cell><cell>76.1</cell><cell>82.5</cell><cell>251.1</cell></row><row><cell></cell><cell cols="2">Fast3R</cell><cell cols="2">DUSt3R</cell><cell></cell><cell></cell></row><row><cell># Views</cell><cell>Time (s)</cell><cell>Peak GPU</cell><cell>Time (s)</cell><cell>Peak GPU</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Mem (GiB)</cell><cell></cell><cell>Mem (GiB)</cell><cell></cell><cell></cell></row><row><cell>2</cell><cell>0.065</cell><cell>3.84</cell><cell>0.092</cell><cell>3.52</cell><cell></cell><cell></cell></row><row><cell>8</cell><cell>0.122</cell><cell>6.33</cell><cell>8.386</cell><cell>24.59</cell><cell></cell><cell></cell></row><row><cell>32</cell><cell>0.509</cell><cell>13.25</cell><cell>129.0</cell><cell>67.61</cell><cell></cell><cell></cell></row><row><cell>48</cell><cell>0.84</cell><cell>20.8</cell><cell>OOM</cell><cell>OOM</cell><cell></cell><cell></cell></row><row><cell>320</cell><cell>15.938</cell><cell>41.90</cell><cell>OOM</cell><cell>OOM</cell><cell></cell><cell></cell></row><row><cell>800</cell><cell>89.569</cell><cell>55.97</cell><cell>OOM</cell><cell>OOM</cell><cell></cell><cell></cell></row><row><cell>1000</cell><cell>137.62</cell><cell>63.01</cell><cell>OOM</cell><cell>OOM</cell><cell></cell><cell></cell></row><row><cell>1500</cell><cell>308.85</cell><cell>78.59</cell><cell>OOM</cell><cell>OOM</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>The count of outliers from RANSAC-PnP Quantitative</figDesc><table><row><cell>Method FPS</cell><cell cols="2">7 scenes [47]</cell><cell cols="2">NRGBD [3]</cell></row><row><cell></cell><cell>Acc↓</cell><cell>Comp↓</cell><cell>Acc↓</cell><cell>Comp↓</cell></row><row><cell>F-Recon [65] &lt;0.1</cell><cell>7.62</cell><cell>2.31</cell><cell>20.59</cell><cell>6.31</cell></row><row><cell>DUSt3R  † [61] 0.78</cell><cell>1.23</cell><cell>0.91</cell><cell>2.51</cell><cell>1.03</cell></row><row><cell>Spann3R [57] 65.4</cell><cell>1.48</cell><cell>0.85</cell><cell>3.15</cell><cell>1.10</cell></row><row><cell>Fast3R (Ours) 251.1</cell><cell>1.58</cell><cell>0.93</cell><cell>3.40</cell><cell>1.01</cell></row><row><cell>Method Views</cell><cell></cell><cell>Acc↓</cell><cell cols="2">Comp↓</cell></row><row><cell></cell><cell></cell><cell>Med.</cell><cell></cell><cell>Med.</cell></row><row><cell>DUSt3R [61] all/5</cell><cell></cell><cell>1.159</cell><cell></cell><cell>0.914</cell></row><row><cell>DUSt3R  † [61] all/5</cell><cell></cell><cell>1.297</cell><cell></cell><cell>1.002</cell></row><row><cell>Spann3R [57] all/5</cell><cell></cell><cell>2.268</cell><cell></cell><cell>1.295</cell></row><row><cell>Fast3R (Ours) all/5</cell><cell></cell><cell>1.706</cell><cell></cell><cell>0.857</cell></row></table><note><p>reconstruction results on scene datasets:</p><p>The numbers indicate median distance to GT points on 7-Scenes<ref type="bibr" target="#b46">[47]</ref> </p><p>and NRGBD<ref type="bibr" target="#b2">[3]</ref> </p><p>datasets. These datasets contain video trajectories of 500-1500 frames, and we evaluate using the same frame skip as other baselines. For 7-Scenes this is skip=20, and NRGBD uses skip=40. DUSt3R † indicates using DUSt3R's final weights on 224 × 224 images, to fit within GPU memory. Distances are scaled 100× to remove the leading 0.0. We defer the full table to the appendix.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>(top). A version of Fast3R trained on N = 4 views still produces high qual-Ablation on the effect of local head on 3D reconstruction. Red/green indicate an increase/decrease in error compared to using the local pointmap aligned to the global pointmap.</figDesc><table><row><cell>Method</cell><cell cols="2">7-Scenes [47]</cell><cell cols="2">NRGBD [3]</cell><cell cols="2">DTU</cell></row><row><cell></cell><cell cols="6">Acc↓ Comp↓ Acc↓ Comp↓ Acc↓ Comp↓</cell></row><row><cell cols="2">Fast3R (ours) 2.84</cell><cell>1.37</cell><cell>4.39</cell><cell>1.28</cell><cell>3.91</cell><cell>1.41</cell></row><row><cell cols="2">w/o local head 4.81</cell><cell>1.64</cell><cell>4.85</cell><cell>1.32</cell><cell>3.88</cell><cell>1.41</cell></row><row><cell cols="5">∆ +1.97 +0.27 +0.46 +0.04</cell><cell>-0.03</cell><cell>0.00</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>Patches H 1 from the first image I 1 are always embedded with p 1 , since that image provides the coordinate frame for the global head.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reconstruction using Global Point Map</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reconstruction using Local (aligned to Global) Point Map</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Large-scale data for multiple-view stereopsis</title>
		<author>
			<persName><forename type="first">Henrik</forename><surname>Aanaes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramsbøl</forename><surname>Rasmus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Engin</forename><surname>Vogiatzis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><forename type="middle">Bjorholm</forename><surname>Tola</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Lama Ahmad</title>
		<author>
			<persName><forename type="first">Josh</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal ; Ilge Akkaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florencia</forename><surname>Leoni Aleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janko</forename><surname>Altenschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Altman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Shyamal Anadkat, et al. Gpt-4 technical report</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural rgb-d surface reconstruction</title>
		<author>
			<persName><forename type="first">Dejan</forename><surname>Azinović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Martin-Brualla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Gilad</forename><surname>Baruch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Afshin</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Dimry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuri</forename><surname>Feigin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Gebauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Joffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Kurz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arik</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Shulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.08897</idno>
		<title level="m">Arkitscenes: A diverse real-world dataset for 3d indoor scene understanding using mobile rgb-d data</title>
		<imprint>
			<date type="published" when="2005">2021. 2, 4, 5</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Extending context window of large language models via positional interpolation</title>
		<author>
			<persName><forename type="first">Shouyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherman</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangjian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maciej</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">FlashAttention-2: Faster attention with better par-allelism and work partitioning</title>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">FlashAttention: Fast and memory-efficient exact attention with IO-awareness</title>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Superpoint: Self-supervised interest point detection and description</title>
		<author>
			<persName><forename type="first">Tomasz</forename><surname>Daniel Detone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2017">2018. 2017</date>
			<biblScope unit="page" from="337" to="33712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Tap-vid: A benchmark for tracking any point in a video</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankush</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larisa</forename><surname>Markeeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrià</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">João</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The llama 3 herd of models</title>
		<author>
			<persName><forename type="first">Abhimanyu</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Jauhri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kadian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Al-Dahle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aiesha</forename><surname>Letman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akhil</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Schelten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">D2-net: A trainable cnn for joint description and detection of local features</title>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Dusmanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ignacio</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomás</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akihiko</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torsten</forename><surname>Sattler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="8084" to="8093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Omnidata: A scalable pipeline for making multitask mid-level vision datasets from 3d scans</title>
		<author>
			<persName><forename type="first">Ainaz</forename><surname>Eftekhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Zamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10786" to="10796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Fairscale: A general purpose modular pytorch library for high performance and large scale training</title>
		<author>
			<persName><forename type="first">Fairscale</forename><surname>Authors</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/fairscale,2021.5" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Instantsplat: Unbounded sparse-view pose-free gaussian splatting in 40 seconds</title>
		<author>
			<persName><forename type="first">Zhiwen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyan</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kairun</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinghao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Ivanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Pavone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Massively parallel multiview stereopsis by surface normal diffusion</title>
		<author>
			<persName><forename type="first">Silvano</forename><surname>Galliani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katrin</forename><surname>Lasinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="873" to="881" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Silk: Simple learned keypoints</title>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Gleize</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiyao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="10932" to="10942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ego-exo4d: Understanding skilled human activity from first-and third-person perspectives</title>
		<author>
			<persName><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Westbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Triantafyllos</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kumar</forename><surname>Ashutosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Baiyya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhant</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bikram</forename><surname>Boote</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Multiple View Geometry in Computer Vision</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
	<note>2 edition</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Gpipe: Efficient training of giant neural networks using pipeline parallelism</title>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youlong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019-05">2019. 5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Albert</forename><forename type="middle">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blanche</forename><surname>Savary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Bamford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devendra</forename><surname>Singh Chaplot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>De Las Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emma</forename><forename type="middle">Bou</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Bressand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gianna</forename><surname>Lengyel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Bour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renard</forename><surname>Lélio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucile</forename><surname>Lavaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Saulnier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandeep</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName><surname>Subramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Mixtral of experts</title>
		<editor>
			<persName><forename type="first">Sophia</forename><surname>Yang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Szymon</forename><surname>Antoniak</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Teven</forename><surname>Le Scao</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Théophile</forename><surname>Gervet</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thomas</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Timothée</forename><surname>Lacroix</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">William</forename><forename type="middle">El</forename><surname>Sayed</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Leroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yohann</forename><surname>Cabon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jérôme</forename><surname>Revaud</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.09756</idno>
		<title level="m">Grounding image matching in 3d with mast3r</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Grounding image matching in 3d with mast3r, 2024</title>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Leroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yohann</forename><surname>Cabon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerome</forename><surname>Revaud</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Shen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omkar</forename><surname>Salpekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Vaughan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pritam</forename><surname>Damania</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Pytorch distributed: Experiences on accelerating data parallel training. CoRR, abs/2006.15704, 2020. 5</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Amy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shubham</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName><surname>Relpose++</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.04926</idno>
		<title level="m">Recovering 6d poses from sparse-view observations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pixel-perfect structure-from-motion with featuremetric refinement</title>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Lindenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul-Edouard</forename><surname>Sarlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktor</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="page" from="5967" to="5977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Fixing weight decay regularization in adam</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno>CoRR, abs/1711.05101</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">ORB-SLAM2: an open-source SLAM system for monocular, stereo and RGB-D cameras</title>
		<author>
			<persName><forename type="first">Raúl</forename><surname>Mur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Artal</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">D</forename><surname>Tardós</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1255" to="1262" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Orb-slam: a versatile and accurate monocular slam system</title>
		<author>
			<persName><forename type="first">Raul</forename><surname>Mur-Artal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><forename type="middle">Martinez</forename><surname>Montiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">D</forename><surname>Tardos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on robotics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1147" to="1163" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Efficient large-scale language model training on GPU clusters</title>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Korthikanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitri</forename><surname>Vainbrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prethvi</forename><surname>Kashinkunti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julie</forename><surname>Bernauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amar</forename><surname>Phanishayee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<idno>CoRR, abs/2104.04473, 2021. 5</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning robust visual features without supervision</title>
		<author>
			<persName><forename type="first">Maxime</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothée</forename><surname>Darcet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theo</forename><surname>Moutakanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Huy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vasil</forename><surname>Szafraniec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Khalidov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Haziza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alaaeldin</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Po-Yao</forename><surname>Howes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vasu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shang-Wen</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Galuba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mido</forename><surname>Rabbat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Assran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herve</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Labatut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><surname>Bojanowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dinov</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Infinite photorealistic worlds using procedural generation</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Raistrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lahav</forename><surname>Lipson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingjie</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karhan</forename><surname>Kayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beining</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yihan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankit</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="12630" to="12641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Zero: Memory optimization towards training A trillion parameter models</title>
		<author>
			<persName><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Rasley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olatunji</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
		<idno>CoRR, abs/1910.02054</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer</title>
		<author>
			<persName><forename type="first">René</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katrin</forename><surname>Lasinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Vision transformers for dense prediction</title>
		<author>
			<persName><forename type="first">René</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.13413</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters</title>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Rasley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olatunji</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3505" to="3506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Common objects in 3d: Large-scale learning and evaluation of real-life 3d category reconstruction</title>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Reizenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Shapovalov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Henzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Sbordone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Labatut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Novotny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Hypersim: A photorealistic synthetic dataset for holistic indoor scene understanding</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Ramapuram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atulit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><forename type="middle">Angel</forename><surname>Bautista</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Paczan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">M</forename><surname>Susskind</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV) 2021</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Superglue: Learning feature matching with graph neural networks</title>
		<author>
			<persName><forename type="first">Paul-Edouard</forename><surname>Sarlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomasz</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019">2020. 2019</date>
			<biblScope unit="page" from="4937" to="4946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">SuperGlue: Learning feature matching with graph neural networks</title>
		<author>
			<persName><forename type="first">Paul-Edouard</forename><surname>Sarlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomasz</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Habitat: A platform for embodied AI research</title>
		<author>
			<persName><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kadian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Maksymets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yili</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Wijmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhavana</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<idno>CoRR, abs/1904.01201</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Structure-from-motion revisited</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Lutz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Schönberger</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jan-Michael</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016. 1, 2</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Mesh-tensorflow: Deep learning for supercomputers</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youlong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Penporn</forename><surname>Koanantakool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingsheng</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blake</forename><forename type="middle">A</forename><surname>Hechtman</surname></persName>
		</author>
		<idno>CoRR, abs/1811.02084</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Megatronlm: Training multi-billion parameter language models using model parallelism</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno>CoRR, abs/1909.08053</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Scene coordinate regression forests for camera relocalization in rgb-d images</title>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shahram</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Flowmap: High-quality camera poses, intrinsics, and depth via gradient descent</title>
		<author>
			<persName><forename type="first">Cameron</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Charatan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayush</forename><surname>Kumar Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Sitzmann</surname></persName>
		</author>
		<idno>ArXiv, abs/2404.15259</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Fourier features let networks learn high frequency functions in low dimensional domains</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pratul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nithin</forename><surname>Fridovich-Keil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Utkarsh</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ren</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7537" to="7547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Aden: Adaptive density representations for sparse-view camera pose estimation</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiyao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.09042</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Droid-slam: Deep visual slam for monocular, stereo, and rgb-d cameras</title>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Teed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">DROID-SLAM: Deep Visual SLAM for Monocular, Stereo, and RGB-D Cameras. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Teed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Probabilistic robotics</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="52" to="57" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Llama 2: Open foundation and fine-tuned chat models</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasmine</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prajjwal</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">EPIC Fields: Marrying 3D Geometry and Video Understanding</title>
		<author>
			<persName><forename type="first">Vadim</forename><surname>Tschernezki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Darkhalil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iro</forename><surname>Larina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diane</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Neural Information Processing Systems (NeurIPS)</title>
		<meeting>the Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">3d reconstruction with spatial memory</title>
		<author>
			<persName><forename type="first">Hengyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lourdes</forename><surname>Agapito</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.16061</idno>
		<imprint>
			<date type="published" when="2006">2024. 3, 5, 6</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Vggsfm: Visual geometry grounded deep structure from motion</title>
		<author>
			<persName><forename type="first">Jianyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Karaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Novotny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2023">2024. 2023</date>
			<biblScope unit="page" from="21686" to="21697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Posediffusion: Solving pose estimation via diffusion-aided bundle adjustment</title>
		<author>
			<persName><forename type="first">Jianyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Novotný</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Dust3r: Geometric 3d vision made easy</title>
		<author>
			<persName><forename type="first">Shuzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Leroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yohann</forename><surname>Cabon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Chidlovskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jérôme</forename><surname>Revaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2023">2024. 2023</date>
			<biblScope unit="page" from="20697" to="20709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Dust3r: Geometric 3d vision made easy</title>
		<author>
			<persName><forename type="first">Shuzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Leroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yohann</forename><surname>Cabon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Chidlovskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerome</forename><surname>Revaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006">2024. 1, 2, 3, 4, 5, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Tartanair: A dataset to push the limits of visual slam</title>
		<author>
			<persName><forename type="first">Wenshan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Delong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaoyu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuheng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yafei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Leonid Antsfeld, Boris Chidlovskii, Gabriela Csurka, and Jérôme Revaud. Croco: Self-supervised pre-training for 3d vision tasks by cross-view completion</title>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Leroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romain</forename><surname>Brégier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yohann</forename><surname>Cabon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaibhav</forename><surname>Arora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Brew</surname></persName>
		</author>
		<idno>CoRR, abs/1910.03771</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Frozenrecon: Pose-free 3d scene reconstruction with frozen depth models</title>
		<author>
			<persName><forename type="first">Guangkai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="9310" to="9320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Depth anything: Unleashing the power of large-scale unlabeled data</title>
		<author>
			<persName><forename type="first">Lihe</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Scannet++: A high-fidelity dataset of 3d indoor scenes</title>
		<author>
			<persName><forename type="first">Chandan</forename><surname>Yeshwanth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yueh-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2005">2023. 2, 4, 5</date>
			<biblScope unit="page" from="12" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Lift: Learned invariant feature transform</title>
		<author>
			<persName><forename type="first">Kwang</forename><surname>Moo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><forename type="middle">V</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Monst3r: A simple approach for estimating geometry in the presence of motion</title>
		<author>
			<persName><forename type="first">Junyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Herrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junhwa</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Forrester</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<idno>arxiv:2410.03825</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Relpose: Predicting probabilistic relative rotation for single objects in the wild</title>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shubham</forename><surname>Tulsiani</surname></persName>
		</author>
		<idno>ArXiv, abs/2208.05963</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Cameras as rays: Pose estimation via ray diffusion</title>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moneish</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tzu-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shubham</forename><surname>Tulsiani</surname></persName>
		</author>
		<idno>ArXiv, abs/2402.14817</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Exploiting dense point trajectories for localizing moving cameras in the wild</title>
		<author>
			<persName><forename type="first">Wang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaohui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengkai</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Particlesfm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Pointodyssey: A large-scale synthetic dataset for long-term point tracking</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><forename type="middle">W</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bokui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gordon</forename><surname>Wetzstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="19855" to="19865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Pose: Efficient context window extension of llms via positional skip-wise training</title>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
