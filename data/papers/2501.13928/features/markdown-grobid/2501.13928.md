# Fast3R: Towards 3D Reconstruction of 1000+ Images in One Forward Pass

## Abstract

## 

github.io/ 3D Reconstruction FPS 1000+ Unordered, Unposed Images Fast3R 1 Forward Pass 3D Reconstruction Camera Poses 0.78 65.49 251.1 DUSt3R Spann3R Fast3R

Figure 1. Fast3R is a method towards 3D reconstructing 1000+ unordered, unposed images in a single forward pass.

## Introduction

3D reconstruction from multiple views has long been a foundational task across applications in autonomous navigation, augmented reality, and robotics [[31,](#b30)[53]](#b52). Establishing correspondences across images, known as multi-view matching, is central to these applications and enables an accurate scene representation. Traditional reconstruction pipelines, such as those based on Structure-from-Motion (SfM) [[44]](#b43) and Multi-View Stereo (MVS) [[18]](#b17), fundamentally rely on image pairs to reconstruct 3D geometry. While effective in some settings, these methods require extensive engineering to manage the sequential stages of feature extraction, correspondence matching, triangulation, and global alignment, limiting scalability and speed.

This traditional "pipeline" paradigm has recently been challenged by DUSt3R [[61]](#b60), which directly predicts 3D structure from RGB images. It achieves this with a design that "cast[s] the pairwise reconstruction problem as a regression of pointmaps, relaxing the hard constraints of usual projective camera models" [[61]](#b60), yielding impressive robustness across challenging viewpoints. This represents a radical shift in 3D reconstruction, as an end-to-end learnable solution is less prone to pipeline error accumulation, while also being dramatically simpler.

On the other hand, a fundamental limitation of DUSt3R is its restriction to two image inputs. While image pairs are an important use case, often one is interested in reconstructing from more than two views, as when scanning of objects [[39]](#b38) or scenes [[4,](#b3)[6,](#b5)[20,](#b19)[55,](#b54)[67]](#b66), e.g. for asset generation or mapping. To process more than two images, DUSt3R computes O(N 2 ) pairs of pointmaps and performs a global alignment optimization procedure. This process can be computationally expensive, scaling poorly as the collection of images grows. For instance, it will lead to OOM with only 48 views on an A100 GPU.

Moreover, such a process is still fundamentally pairwise, which limits the model's context, both affecting learning during training and ultimate accuracy during inference. In this sense, DUSt3R suffers from the same pair-wise bottleneck as traditional SfM and MVS methods.

We propose Fast3R, a novel multi-view reconstruction framework designed to overcome these limitations. Building on DUSt3R's foundations, Fast3R leverages a Transformer-based architecture [[56]](#b55) that processes multiple images in parallel, allowing N images to be reconstructed in a single forward pass. By eliminating the need for sequential or pairwise processing, each frame can simultaneously attend to all other frames in the input set during reconstruction, significantly reduces error accumulation. Perhaps surprisingly, Fast3R also takes significantly less time. Our contributions are threefold. 1. We introduce Fast3R, a Transformer-based model for multi-view pointmap estimation that obviates the need for global postprocessing; resulting in significant improvements in speed, computation overhead and scalability. 2. We show empirically that the model performance improves by scaling along the view axis. For camera pose localization and reconstruction tasks, the model improves when trained on progressively larger sets of views. Per-view accuracy further improves when more views are used during inference, and the model can generalize to significantly more views than seen during training. 3. We demonstrate state-of-the-art performance in camera pose estimation with significant inference time improvements. On CO3Dv2 [[39]](#b38), Fast3R gets 99.7% accuracy within 15-degrees for pose estimation, over a 14x error reduction compared to DUSt3R with global alignment. Fast3R offers a scalable and accurate alternative for real-world applications, setting a new standard for efficient multi-view 3D reconstruction.

## Related Work

Multi-view 3D reconstruction: Almost all modern 3D reconstruction approaches are based on the traditional multiview geometry (MVG) pipeline [[21]](#b20). MVG-based methods first identify corresponding pixels between image pairs, and then use camera models and projective multiview geometry to lift these correspondences to 3D points. The process happens in sequential stages: feature extraction, finding pairwise image correspondences, triangulation to 3D and pairwise relative camera pose, and global bundle alignment. However, any pipeline approach is prone to accumulating errors, which are especially common in the hand-crafted components. Moreover, the sequential nature prevents parallelization, which limits speed and scalability. MVG approaches have existed since the early days of computer vision, and are still in use for a reason: they can be highly accurate when they do not catastrophically fail. The latest multi-view geometry pipelines like COLMAP [[44]](#b43) or OrbSLAM2 [[30]](#b29) incorporate nearly 60 years of compounding engineering improvements, but these approaches still catastrophically fail >40% of the time on static scenes like ETH-3D [[52]](#b51)), which can actually be considered an easy case due to dense image coverage of the scene.

Much recent work has successfully addressed the robustness and speed by replacing increasingly large components of MVG pipelines with end-to-end learned versions that are faster and reduce the rate of catastrophic failures [[48,](#b47)[58,](#b57)[72]](#b71). For example, [[14,](#b13)[19,](#b18)[25,](#b24)[42,](#b41)[51,](#b50)[68]](#b67) improve feature extraction and correspondences, [[27,](#b26)[50,](#b49)[59,](#b58)[71]](#b70) learn to estimate camera pose, and [[52]](#b51) introduce a bundle adjustment layer. [[61]](#b60) contains an excellent and comprehensive survey of such efforts. Overall, the trend is towards replacing increasingly large components with end-to-end solutions. Pointmap representation: DUSt3R [[61]](#b60) takes this evolution the furthest by proposing pointmap regression to replace everything in the MVG pipeline up to global pairwise alignment. Rather than first attempting to solve for camera parameters in order to triangulate corresponding pixels, DUSt3R trains a model to directly predict 3D pointmaps for pairs of images in a shared coordinate system. Other MVG component tasks such as relative camera pose estimation and depth estimation can be recovered from the resulting pointmap representation. However, DUSt3R's pairwise assumption is a limitation, as it requires inference on O(N 2 ) image pairs and then a global alignment optimization, which is per-scene and does not improve with more data. Moreover, this process quickly becomes slow or crashes due to exceeded system memory, even for relatively modest numbers of images.

DUSt3R has inspired several follow-ups. MASt3R [[25]](#b24) ViT Encoder 

## Fusion Transformer

## Per-View Tokenization

## Global Head

## Global Fusion

Decode XYZ and Confidence

… ViT Encoder ViT Encoder ViT Encoder Local Head Global Head Local Head Global Head Local Head Global Head Local Head Random Idx. Emb. Random Idx. Emb. Random Idx. Emb. … … Confidence Point Map RGB Point Map Per-View Point Map Shared Weights Shared Weights Shared Weights Shared Weights Many Views … Figure 2. Model architecture of Fast3R. Built upon a novel Transformer-based architecture which supports bidirectional information flow, Fast3R is able to process dense input views simultaneously.

adds a local feature head to each decoder's output, while MonST3R [[69]](#b68) does a data-driven exploration of dynamic scenes, but both are still fundamentally pairwise methods. MASt3R in particular does not make any changes to the global alignment methodology. Concurrently with our work, Spann3R [[57]](#b56) treats images as an ordered sequence (e.g. from a video) and incrementally reconstructs a scene using a pairwise sliding window network, along with a learned spatial memory system. This extends DUSt3R to handle more images, but Spann3R's incremental pairwise processing cannot fix reconstructions from earlier frames, which can cause errors to accumulate. Crucially, Fast3R's transformer architecture uses all-to-all attention, allowing the model to reason simultaneously and jointly over all frames without any assumption of image order. Fast3R removes sequential dependencies, enabling parallelized inference across many devices in a single forward pass.

## Model

Fast3R is a transformer-based model that predicts a 3D pointmap from a set of unordered and unposed images. The model architecture is designed to be scalable to over 1000 images during inference, though during training we use image masking to train it with far fewer. In this section, we detail our implementation of Fast3R, and discuss the design choices that enable its scalability.

## Problem definition

Taking a set of (N ) unordered and unposed RGB images I ∈ R N ×H×W ×3 as inputs 1 , Fast3R reconstructs the 3D structures of the scene by predicting the corresponding 1 We assume all images are resized to the same H × W for simplicity.

pointmap X, where X ∈ R N ×H×W ×3 . A pointmap is a set of 3D locations indexed by pixels in an image I, enabling the derivation of camera poses, depths, and 3D structures. Fast3R predicts local and global pointmaps X L and X G , and corresponding confidence maps Σ L and Σ G (of shape Σ ∈ R N ×H×W ). Fast3R maps N RGB images to

$Fast3R : I → (X L , Σ L , X G , Σ G )$Like MASt3R, the global pointmap X G is in the coordinate frame of the first camera and the X L is in the coordinate frame of the viewing camera, as shown in Figure [2](#) 3

## .2. Training Objective

This section describes the loss, using the notation in Section 3.1 above. Fast3R's predictions of ( XL , ΣL , XG , ΣG ) are trained using generalized versions of the pointmap loss in DUST3R [[61]](#b60).

Our total loss is the combination of pointmap losses for the local and global pointmaps:

$L total = L XG + L XL(1)$which are confidence-weighted versions of the normalized 3D pointwise regression loss. Normalized 3D pointwise regression loss: The normalized regression loss for X is a multi-view version of that in DUST3R [[66]](#b65) or monocular depth estimation [[15,](#b14)[36,](#b35)[66]](#b65). It is the L 2 loss between the normalized predicted pointmaps and normalized target pointmaps, rescaled by the mean Euclidean distance to the origin: Note that the predictions and targets are independently normalized by the mean euclidean distance to the origin.

$ℓ regr ( X, X) = 1 ẑ X - 1 z X 2 , z = 1 |X| x∈X ∥x∥ 2 (2)$Pointmap loss: As in [[61]](#b60), we use a confidence-adjusted version of the loss above, using the confidence score Σ predicted by the model. The total loss for a pointmap is

$L X ( Σ, X, X) = 1 |X| Σ+ • ℓ regr ( X, X) + α log( Σ+ )$(3) Since the log term requires the confidence scores to be positive, we enforce Σ+ = 1 + exp( Σ). Our intuition is that the confidence weighting helps the model deal with label noise. Like DUST3R, we train on real-world scans typically containing systematic errors in the underlying pointmap labels. For example, glass or thin structures are often not reconstructed properly in the ground-truth laser scans [[4,](#b3)[67]](#b66), and errors in camera registration will cause misalignments between the images and pointmap labels [[66]](#b65).

## Model architecture

The Fast3R meta-architecture is inspired by DUSt3R, and has three components: image encoding, fusion transformer, and pointmap decoding, as shown in Figure [2](#). We emphasize that Fast3R makes no assumptions on the ordering of images in I, and all output pointmaps and confidence maps (X L , Σ L , X G , Σ G ) are predicted simultaneously, not sequentially. Image encoder: Fast3R encodes each image I i ∈ I to a set of patch features H i , using a feature extractor F. This is done independently per image, yielding a sequence of image patch features H i = {h i,j } HW/P 2 j=1 for each image:

$H i = F(I i ), i ∈ 1, ..., N(4)$We follow DUSt3R's design and use CroCo ViT [[63]](#b62) as our encoder, though we found DINOv2 [[33]](#b32) works similarly. Before passing image patch features H to the fusion transformer, we add position embeddings with onedimensional image index positional embeddings.

Index embeddings help the fusion transformer determine which patches come from the same image and are the mechanism for identifying I 1 , which importantly defines the global coordinate frame. This is critical for allowing the model to implicitly reason about camera pose jointly for all images from an otherwise permutationally invariant set of tokens. Fusion transformer: Most of the computation in Fast3R happens in the fusion transformer, which has a generic architecture. We use a 12-layer transformer similar to ViT-B [[12]](#b11) or BERT [[10]](#b9), however this could be scaled up. This fusion transformer takes the concatenated encoded image patches from all views and performs all-to-all self-attention. This operation provides Fast3R with full context, beyond the information provided in pairs alone. Pointmap head: Finally, Fast3R uses separate DPT-L [[37]](#b36) decoder heads to map these tokens to the local and global pointmaps (X L , X G ), and confidence maps (Σ L , Σ G ). Image index positional embedding generalization: We would like Fast3R to be able to handle many views at inference, more than were used to train a model. A naïve way to embed views during testing would be to embed them in the same way as training: i.e. use the same Spherical Harmonic frequencies [[49]](#b48) to embed raw indices SH({1, ..., N }) during training, and SH({1, ..., N test }) during inference. In LLMs this causes poor performance, and in preliminary experiments we also found that the resulting model did not work well when the number of input images exceeded that used during training. We therefore adapt Position Interpolation [[5]](#b4), a solution from LLMs, where during training we randomly draw N indexes from a larger pool N ′ of possible samples. [[5]](#b4) draws samples using a regular grid since the LLM inputs form a regular ordered sequence. Our images are unordered, so we draw N ⊂ {1, ..., N ′ } uniformly at random. To the transformer, the strategy looks indistinguishable from masking out images, and N ′ ≫ N controls the masking ratio. [2](#foot_0) This strategy enables Fast3r to handle N = 1000 images during inference, even if only trained with N = 20 images.

## Memory-Efficient Implementation

With a standard transformer architecture and a single-pass inference procedure, Fast3R is able to leverage many of the recent advances designed to improve scalability at train and inference time [[2,](#b1)[13,](#b12)[23,](#b22)[54]](#b53).

For example, model size and throughput can be increased by sharding the model and/or data minibatch across multiple machines, such as through model parallelism [[22,](#b21)[45]](#b44), data parallelism [[26]](#b25), and tensor parallelism [[32,](#b31)[46]](#b45). During training, optimizer weights, states, and gradients can also be sharded [[35]](#b34). Systems-level advances have also been proposed, such as FlashAttention [[7,](#b6)[8]](#b7), which uses GPU kernels leveraging the hardware topology to compute attention in a time and memory-efficient way. These are implemented in libraries such as FAIRScale [[16]](#b15), Deep-Speed [[35]](#b34) and Huggingface [[64]](#b63), and require significant engineering effort.

The Fast3R meta-architecture is explicitly designed to take advantage of these efforts. We leverage two different forms of parallelism at training and inference time, as well as FlashAttention, described in more detail in Sec. 4. More broadly, we believe that our approach will continue to benefit in the longer term as transformer-based scaling infrastructure continues to mature.

## Experiments

Training Data We train on a mix of real-world objectcentric and scene scan data: CO3D [[39]](#b38), ScanNet++ [[67]](#b66), ARKitScenes [[4]](#b3), and Habitat [[43]](#b42). We use a subset of the datasets in DUSt3R, specifically 4 of the 9 datasets, for a total of around 2000 unique scenes scans and 1300 videos of 50 object classes. Note that this is only 7% of CO3D, which is also what the baselines DUSt3R [[61]](#b60), Spann3R [[57]](#b56), and MASt3R [[25]](#b24) use. Baselines DUSt3R [[61]](#b60) is the closest approach to ours, and competitive on visual odometry and reconstruction benchmarks. That paper contains extensive comparisons against other methods, and we adopt it as our main baseline. We additionally consider DUSt3R's follow-up work, MASt3R [[25]](#b24), as well as a concurrent work Spann3R [[57]](#b56), which also seeks to replace DUSt3R's expensive global alignment stage by sequentially processing frames with a spatial memory. For camera pose estimation and 3D reconstruction, we include comparisons to task-specific methods. Architecture Details In our experiments we use the following components for the meta-architecture: following the DPT-L architecture [[37]](#b36). Training Details Our models are trained on 512 × 512 images using AdamW [[29]](#b28) for 6.5K steps, with a learning rate of 0.0001 and cosine annealing schedule. Unlike DUSt3R, we do not use staged training. For the most part, we use the same dataloaders used in the public baseline implementations. We train with batch size 64, with each sample consisting of a tuple of N = 20 views. This process takes 6.13 days on 64 A100 GPUs. We additionally make use of several strategies to enable efficient training. First, we use the FlashAttention [[7,](#b6)[8]](#b7) to improve time and memory efficiency. Even so, a naïve implementation runs out of memory even with batch size 1 when N > 16, so we use Deep-Speed ZeRO stage 2 training [[35]](#b34), whereby optimizer states, moment estimates, and gradients are partitioned across different machines. This enables us to train with up to N = 28 views per data sample, with a batch size of one per GPU.

## Inference Efficiency

At inference time, we aim to handle 1000+ views compared to 20 during training, which requires additional optimizations. We observe the memory bottleneck at inference is in the DPT heads producing the pointmaps: with 320 views on a single A100 GPU, over 60% of VRAM is consumed by activations from the DPT heads, largely due to each needing to upsample 1024 tokens into a high-resolution 512 × 512 image. To address this, we implement a simple version of tensor parallelism, putting the model on GPU 0 and then Methods Co3Dv2 [[39]](#b38) FPS RRA@15↑ RRA@5↑ RTA@15↑ RTA@5↑ mAA(30)↑ Colmap+SG [[9,](#b8)[41]](#b40) 36 Table 2. System performance metrics for different view counts on Fast3R and DUSt3R on a single A100. Time is measured in seconds (s), and memory is measured in gibibytes (GiB). Each view is 512x384 in resolution. For DUSt3R, at 48 views the N 2 pairwise reconstructions eventually consume all VRAM at its global alignment stage. Note that Fast3R's reported fastest FPS of 251.1 uses 108 views in 224x224 resolution.

copying the DPT heads to each of the K -1 other GPUs.

When processing a batch of N ≈ 1000 images, we pass the entire batch through the ViT encoder and global fusion decoder, and then split the outputs across K machines for parallel DPT head inference. Table [2](#) shows the inference time and memory usage as we increase the number of views. Fast3R is able to process up to 1500 views in a single pass, whereas DUSt3R runs out of memory past 32. Fast3R also has a significantly faster inference time, with gains that increase with more views.

## Camera Pose Estimation

We evaluate camera pose estimation on unseen trajectories from 41 object categories from CO3D [[39]](#b38). Following [[61]](#b60), we sample 10 random views from each trajectory.

Inspired by DUSt3R [[61]](#b60), we estimate the focal length, camera rotation, and camera translation from the predicted global pointmaps. We begin by initializing a set of random focal length guesses based on the image resolution, then use RANSAC-PnP to estimate the camera's rotation and translation based on the guessed focal lengths and the global pointmap. Table 4. Quantitative results on object-centric DTU [1] dataset. Using a skip=5 on trajectories of 49 frames.

is used to score each guessed focal length (lower is better), and the best-scoring focal length is selected to compute the intrinsic and extrinsic camera matrices.

During RANSAC-PnP, we only use points with the top 15% confidence scores predicted by Fast3R, ensuring efficient PnP processing and reducing outliers. If all images are known to originate from the same physical camera, we use the focal length estimated from the first view as the focal length for all cameras, as this initial estimate has been empirically found to be more reliable. Otherwise, we independently estimate the focal length for each input. It is worth noting that the camera pose estimation process is parallelized using multi-threading, ensuring minimal wallclock time. Even for hundreds of views, the process completes in just a few seconds on standard CPUs.  We report Relative Rotation Accuracy (RRA) and Relative Translation Accuracy (RTA) at a threshold of 15 • , mean Average Accuracy (mAA) at threshold 30 • , and model frames per second (FPS) in Table [1](#tab_1). On Co3D, Fast3R surpasses all other methods across the RRA and mAA metrics, achieving near-perfect RRA, while remaining competitive on RTA. Importantly, it is also orders of magnitude faster: 200× faster than DUSt3R and 700× faster than MASt3R.

Figure [4](#fig_2) and Figure [5](#fig_4) shows that Fast3R's predictions improve with more views, indicating that the model is able to use the additional context from multiple images.

## 3D Reconstruction

We evaluate Fast3R's 3D reconstruction on scene-level benchmarks: 7-Scenes [[47]](#b46) and Neural RGB-D [[3]](#b2), and the object-level benchmark DTU [[1]](#b0).

We found that local pointmaps learn finer detail than the global pointmaps. Therefore we use the local pointmaps for detail and the global pointmaps for the high-level structure. Specifically, we independently align each image's local pointmap to the global pointmap using ICP, and use aligned local pointmaps for evaluation.

Fast3R is competitive with other pointmap reconstruction methods like DUSt3r and MASt3R, while being significantly faster, as shown in Table [3](#tab_3) and [Table 4](#). We believe that Fast3R will continue to improve with better reconstruc-  

## 4D Reconstruction: Qualitative Results

Because Fast3R can handle multiple frames naturally, one may wonder how well Fast3R can handle dynamic scenes. We qualitatively test Fast3R's 4D reconstruction ability, showing examples of dynamic aligned pointmaps at multiple time steps in Figure [6](#fig_5). Fast3R can be trained to achieve such results by finetuning a 16 static views checkpoint on the PointOdyssey [[73]](#b72) and TartanAir [[62]](#b61) datasets, consisting of 110 dynamic and 150 static scenes, respectively. We freeze the ViT encoder, use 224x224 image resolution, and swap in a newly-initialized global DPT head. We fine-tune the model with 15 epochs with a frame length of 16, batch size per GPU of 1, and use the same learning rate schedule as Fast3R. The process takes 45 hours to finetune on 2 Nvidia Quadro RTX A6000 GPUs.

We see that our approach produces qualitatively reasonable reconstructions with minimal changes. MonST3R [[69]](#b68) is a concurrent work also tackling dynamic scene reconstruction that builds atop DUSt3R. However, like DUSt3R, it assumes a pairwise architecture and also uses a separate model to predict optical flow. We show that the same Fast3R architecture trained end-to-end with the same many-view pointmap regression (just swapping the data to dynamic scenes), can also work for 4D reconstruction. Importantly, our method remains significantly faster, opening the poten-No rand. emb. tial for real-time applications.

## Ablation Studies

## Scaling the number of views

Fast3R is able to use all-to-all attention during training, which lets it learn from global context. We hypothesize that the additional context provided by more views during training allows the model to learn higher-order correspondences between multiple frames, ultimately increasing model performance and increasing potential for scaling.

Figures [7](#fig_6) and [8](#fig_7) shows that training on increasingly more views consistently improves RRA and RTA for visual odometry, and reconstruction accuracy-even when the number of views used during evaluation is held constant and the model is ultimately evaluated on fewer views than were seen during training. We further evaluate the model's ability to reason about additional views by increasing the number of images that Fast3R sees during inference. Figure [4](#fig_2) and Figure [5](#fig_4) indicate that as the model uses more views, the average per-view performance improves. This behavior holds for all evaluated metrics in both camera pose estimation and reconstruction. As shown in Figure [5](#fig_4), the model has a better per-view accuracy using 50 images than it does with 20, even though it was trained with 20. Many applications (e.g. reconstruction, odometry) require inference on many views, which is a major motivation for Fast3R removing the pairwise constraint.

## Training without position interpolation

In Section 3.3, we introduced a randomized version of [[5]](#b4) to enable inference on more views than seen training. Without this technique, model accuracy quickly degrades for pointmap corresponding to image indexes outside the training range, as shown in Figure [9](#fig_8)  ity pointmaps for views in slot 5 to 24 (Figure [9](#fig_8) bottom).

## Removing the local decoder head

Table [5](#tab_5) shows that removing the local head learns finer details before the global head does. We hypothesize that this is because the global head needs to first learn the coordinate system and then learn the fine details. As the model improves, the local head may not be necessary.

## Conclusion

We introduce Fast3R, a transformer that predicts 3D locations for all pixels in a common frame of reference, directly in a single forward pass. By replacing the whole SfM pipeline with a generic architecture trained end-toend, Fast3R and similar approaches should benefit from the usual scaling rules for transformers: consistent improvement with better data and increased parameters. Since Fast3R uses global attention, it avoids two potentially artificial scaling limits due to bottlenecks in existing systems.

First, the bottleneck of image pair reconstruction restricts the information available to the model. Second, pairwise global optimization can only make up for this so much and does not improve with more data. With our efficient implementation, Fast3R can operate at > 250 FPS, and process 1500 images in one forward pass, far exceeding other methods while achieving competitive results on 3D reconstruction and camera pose estimation benchmarks. We demonstrate that Fast3R can be finetuned to reconstruct videos by changing the data and without modifying the pointmap regression objective and architecture. In contrast with pipeline approaches bottlenecked by custom and slow operations, Fast3R inherits the benefits of future engineering improvements to efficiently serve and train large transformer-based models. For example, packages like Deepspeed-Inference [[38]](#b37), FlashAttention [[7,](#b6)[8]](#b7) provide fused kernels, model parallelism, and data parallelism. These speed up inference and reduce memory requirements, allowing more images per device, and the number of images scales with the number of devices. Limitations: A current limiting factor for scaling may be data accuracy and quantity. Synthetic data [[34,](#b33)[40]](#b39) could be a solution as, broadly speaking, models trained for geometry estimation seem to generalize well from simulation data.

Fast3R can successfully use simulated data to train for 4D reconstruction, showing generalization results on DAVIS. Similarly, DepthAnythingV2 [[66]](#b65) showed the potential of this approach to scale for monocular depth estimation.

The architecture of Fast3R allows for parallel processing of many views, and its positional embedding design enables "train short, test long" in terms of context length of views. However, we observed that for scenes with very large reconstruction areas, when the number of views becomes extreme (e.g., more than 200), the point map of some views (particularly those with a low confidence score) begins to exhibit drifting behavior. One current way to address this issue is to drop frames with low confidence scores. In dense reconstruction, this approach typically does not hurt reconstruction quality too much. However, to fundamentally address this problem, we hypothesize that future work could explore the following avenues: (1) incorporating more data containing large scenes to improve generalization to such cases; (2) designing better positional embeddings inspired by state-ofthe-art long-context language models [[74]](#b73), which can handle very long context lengths and exploit the temporal structure of ordered image sequences (e.g., video).

## Fast3R: Towards 3D Reconstruction of 1000+ Images in One Forward Pass

## Supplementary Material

## A. Model Scaling Effect

We investigate the effect of scaling model size by trying three model sizes for the Fusion Transformer: ViT-base, ViT-large, and ViT-huge, according to the settings in the original ViT paper [[12]](#b11). The results are shown in Figure [11](#fig_11). This experiment demonstrates that larger model size continually benefits 3D tasks including camera pose estimations and 3D reconstruction. Note that the Fusion Transformer size used in the main text for all experiments is a ViT-base.

## B. Data Scaling Effect

We study the effect of scaling the data using 4 different scales of data, 12.5%, 25%, 50%, and 100%, to train the model. The results are shown in Figure [12](#fig_14). The training settings for all models are kept the same except for how much data they have access to. The results demonstrate that Fast3R continually benefits from more data, suggesting Fast3R could achieve better results in the future given more data.

## C. Gaussian Splatting

We qualitatively demonstrate the potential of using Fast3R's output for downstream novel view synthesis tasks. A visualization of the Gaussian Splatting generated by adopting the pipeline of InstantSplat [[17]](#b16) is shown in Figure [13](#fig_15).

## D. Bundle Adjustment (via Gaussian splatting)

While not necessary, using bundle-adjustment at inference time can also improve Fast3R's performance. We show an example of bundle adjustment using Gaussian Splatting (GS-BA).

Specifically, we use InstantSplat [[17]](#b16) to optimize a set of gaussians per-scene, using initializations from a pointcloud, and updates the locations and poses in order to minimize reprojection error. We show an example of the Gaussian reconstruction in Figure [13](#fig_15) shows an example reconstruction on CO3D.

We can compare against ground-truth trajectories from COLMAP. We found that GS-BA significantly reduces both the pose and translation error. Table [6](#) quantifies this, showing over a 2.5x reduction in translation error and 4x redution in rotational error on the "Family" scene from Tanks and Temples, which we found to be representative. We show a visualization of the original reconstruction and the poses pre-and post-bundle adjustment. There are only 8 scenes in the evaluation set in InstantSplat. Table [6](#). Pose estimation can further improve with Bundle Adjustment. We show an example on the "Family" scene from Tanks and Temples, using InstantSplat [[17]](#b16).

## E. More Visualizations

We show more visualizations of Fast3R's performance on indoor scenes in Figure [10](#fig_10). Fast3R learns the regularity of indoor rooms (square-like shapes) and demonstrates "loop closure" capabilities.

In Figure [15](#fig_4), we visualize the point cloud produced using local vs. global point maps.       

![Figure 3. Qualitative examples of Fast3R's output. The text on the yellow sign says "Caution, cleaning in progress" and is legible if zoomed in.]()

![Figure 4. Pose accuracy with more views: Fast3R improves with the context from more views. Fast3R saturates the orientation portion of the benchmark, even using 3-5 views.]()

![The Image Encoder uses a CroCo ViT-B[63] architecture, initialized with DUSt3R pretrained weights[61]. 2. The Fusion Transformer is a 12-layer ViT-B model using the BERT architecture with 12 layers, 12 heads, embedding dimension size 768 and MLP ratio 4.0. We use a pool size of N ′ = 1000 image index embeddings. 3. The Pointmap Decoder is a dense vision Transformer]()

![Figure 5. DTU reconstruction quality vs. test number of views. Accuracy and Completion (lower is better) get better as we inference with more views.]()

![Figure 6. Qualitative 4D reconstruction results on unseen dynamic scenes in DAVIS. Results are obtained with one forward pass. The tracks are visualized using ground-truth track annotations from TAP-Vid-DAVIS [11].]()

![Figure 7. Increasing # views during training: camera pose estimation on CO3D. Estimates of both orientation (RRA@5) and translation (RTA@5) improve with more views.]()

![Figure 8. Increasing # views during training: reconstruction on 7scenes and NRGBD. Accuracy and Completion (lower is better) get better as we train with more views. Normal Consistency (high is better) also gets better as we train with more views.]()

![Figure 9. Effect of sampling image index PE during training. If we train the model without sampling index embeddings, regression loss spikes (orange) when testing with more views than seen at training (top). Our embedding strategy performs comparably even with 6× the number of views during training.]()

![Figure 10. Visualizations of results on NRGBD scenes. Fast3R learns the regularity of indoor rooms (square-like shapes) and demonstrates "loop closure" capabilities.]()

![Figure 11. Model scaling effect. Increasing the size of the Fusion Transformer leads to better camera pose estimation (↑) and 3D reconstruction (↓). All models are trained for 60k steps (equivalent to 60 epochs; the main paper uses 100 epochs).]()

![Figure 12. Data scaling effect. More training data leads to better camera pose estimation (↑) and 3D reconstruction (↓). All models are trained for 60k steps (equivalent to 60 epochs; the main paper uses 100 epochs).]()

![Figure 13. Visualization of Gaussians from unseen poses. The frames are ordered temporally along the direction of the arrows. The middle frames show poses very different from those used for reconstruction, as is evidenced by the large areas with no Gausisans. The scene is fit from 7 images from CO3D.]()

![Figure 14. Bundle adjustment further improves pose. Left: reconstruction from Fast3R. Middle: Original poses pre-GS-BA. Right: Poses after GS-BA.]()

![Multi-view pose regression on the CO3Dv2[39] dataset with 10 random frames. Parentheses denote methods that do not report results on the 10 views set; we report their best for comparison (8 views). Fast3R does not assume known camera intrinsics.]()

![The count of outliers from RANSAC-PnP Quantitative]()

![(top). A version of Fast3R trained on N = 4 views still produces high qual-Ablation on the effect of local head on 3D reconstruction. Red/green indicate an increase/decrease in error compared to using the local pointmap aligned to the global pointmap.]()

Patches H 1 from the first image I 1 are always embedded with p 1 , since that image provides the coordinate frame for the global head.

