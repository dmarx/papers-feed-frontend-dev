{
  "arxivId": "2411.12372",
  "title": "RedPajama: an Open Dataset for Training Large Language Models",
  "authors": "Maurice Weber, Daniel Fu, Quentin Anthony, Yonatan Oren, Shane Adams, Anton Alexandrov, Xiaozhong Lyu, Huu Nguyen, Xiaozhe Yao, Virginia Adams, Ben Athiwaratkun, Rahul Chalamala, Kezhen Chen, Max Ryabinin, Tri Dao, Percy Liang, Christopher R\u00e9, Irina Rish, Ce Zhang",
  "abstract": "Large language models are increasingly becoming a cornerstone technology in\nartificial intelligence, the sciences, and society as a whole, yet the optimal\nstrategies for dataset composition and filtering remain largely elusive. Many\nof the top-performing models lack transparency in their dataset curation and\nmodel development processes, posing an obstacle to the development of fully\nopen language models. In this paper, we identify three core data-related\nchallenges that must be addressed to advance open-source language models. These\ninclude (1) transparency in model development, including the data curation\nprocess, (2) access to large quantities of high-quality data, and (3)\navailability of artifacts and metadata for dataset curation and analysis. To\naddress these challenges, we release RedPajama-V1, an open reproduction of the\nLLaMA training dataset. In addition, we release RedPajama-V2, a massive\nweb-only dataset consisting of raw, unfiltered text data together with quality\nsignals and metadata. Together, the RedPajama datasets comprise over 100\ntrillion tokens spanning multiple domains and with their quality signals\nfacilitate the filtering of data, aiming to inspire the development of numerous\nnew datasets. To date, these datasets have already been used in the training of\nstrong language models used in production, such as Snowflake Arctic,\nSalesforce's XGen and AI2's OLMo. To provide insight into the quality of\nRedPajama, we present a series of analyses and ablation studies with\ndecoder-only language models with up to 1.6B parameters. Our findings\ndemonstrate how quality signals for web data can be effectively leveraged to\ncurate high-quality subsets of the dataset, underscoring the potential of\nRedPajama to advance the development of transparent and high-performing\nlanguage models at scale.",
  "url": "https://arxiv.org/abs/2411.12372",
  "issue_number": 656,
  "issue_url": "https://github.com/dmarx/papers-feed/issues/656",
  "created_at": "2025-01-04T06:53:06.611354",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 0,
  "last_read": null,
  "last_visited": "2024-12-30T20:18:27.856Z",
  "main_tex_file": null,
  "published_date": "2024-11-19T09:35:28Z",
  "arxiv_tags": [
    "cs.CL",
    "cs.LG"
  ]
}