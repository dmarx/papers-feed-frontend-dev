<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RedPajama: an Open Dataset for Training Large Language Models</title>
				<funder>
					<orgName type="full">Ericsson</orgName>
				</funder>
				<funder>
					<orgName type="full">VMWare</orgName>
				</funder>
				<funder>
					<orgName type="full">Total</orgName>
				</funder>
				<funder>
					<orgName type="full">Stanford Data Science Initiative</orgName>
					<orgName type="abbreviated">SDSI</orgName>
				</funder>
				<funder>
					<orgName type="full">Salesforce</orgName>
				</funder>
				<funder ref="#_nYF67rw">
					<orgName type="full">Accenture</orgName>
				</funder>
				<funder>
					<orgName type="full">Analog Devices</orgName>
				</funder>
				<funder>
					<orgName type="full">Canada CIFAR AI Chair Program</orgName>
				</funder>
				<funder ref="#_dGquPF7">
					<orgName type="full">Canada Excellence Research Chairs Program</orgName>
				</funder>
				<funder ref="#_AvCtwG4">
					<orgName type="full">Office of Science of the U.S. Department of Energy</orgName>
				</funder>
				<funder>
					<orgName type="full">Qualcomm</orgName>
				</funder>
				<funder ref="#_YUGBQUf">
					<orgName type="full">Stanford HAI</orgName>
				</funder>
				<funder ref="#_nfJnZS5 #_DJFKMsC">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_8YRYZhj">
					<orgName type="full">National Institutes of Health</orgName>
					<orgName type="abbreviated">NIH</orgName>
				</funder>
				<funder ref="#_E33quR5 #_xSnkH2h">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder>
					<orgName type="full">Meta</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-11-19">19 Nov 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Maurice</forename><surname>Weber</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Together AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Together AI</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Quentin</forename><surname>Anthony</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">Ohio State University</orgName>
								<address>
									<addrLine>11 Caltech</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yonatan</forename><surname>Oren</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Together AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shane</forename><surname>Adams</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Together AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anton</forename><surname>Alexandrov</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xiaozhong</forename><surname>Lyu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Huu</forename><surname>Nguyen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xiaozhe</forename><surname>Yao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Virginia</forename><surname>Adams</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Together AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ben</forename><surname>Athiwaratkun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Together AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rahul</forename><surname>Chalamala</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Together AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kezhen</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Together AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Max</forename><surname>Ryabinin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Together AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Together AI</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Princeton University</orgName>
								<address>
									<addrLine>7 ETH Zurich 8 Mila</addrLine>
									<settlement>Montréal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Together AI</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Together AI</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Irina</forename><surname>Rish</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Université de Montréal</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ce</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Together AI</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<persName><surname>Eleutherai</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ontocord</forename><surname>Ai</surname></persName>
						</author>
						<title level="a" type="main">RedPajama: an Open Dataset for Training Large Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-11-19">19 Nov 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">88FA09DBB511107F51C1F2FCBAE0E683</idno>
					<idno type="arXiv">arXiv:2411.12372v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large language models are increasingly becoming a cornerstone technology in artificial intelligence, the sciences, and society as a whole, yet the optimal strategies for dataset composition and filtering remain largely elusive. Many of the top-performing models lack transparency in their dataset curation and model development processes, posing an obstacle to the development of fully open language models. In this paper, we identify three core data-related challenges that must be addressed to advance open-source language models. These include (1) transparency in model development, including the data curation process, (2) access to large quantities of high-quality data, and (3) availability of artifacts and metadata for dataset curation and analysis. To address these challenges, we release RedPajama-V1, an open reproduction of the LLaMA training dataset. In addition, we release RedPajama-V2, a massive web-only dataset consisting of raw, unfiltered text data together with quality signals and metadata. Together, the RedPajama datasets comprise over 100 trillion tokens spanning multiple domains and with their quality signals facilitate the filtering of data, aiming to inspire the development of numerous new datasets. To date, these datasets have already been used in the training of strong language models used in production, such as Snowflake Arctic, Salesforce's XGen and AI2's OLMo. To provide insight into the quality of RedPajama, we present a series of analyses and ablation studies with decoder-only language models with up to 1.6B parameters. Our findings demonstrate how quality signals for web data can be effectively leveraged to curate high-quality subsets of the dataset, underscoring the potential of RedPajama to advance the development of transparent and high-performing language models at scale.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pretraining data is among the most central building blocks that go into the development of modern large language models (LLMs). However, one of the core challenges this field faces is the general lack of transparency regarding the composition and curation strategy of pretraining data <ref type="bibr" target="#b7">[8]</ref>. Indeed, with a few notable exceptions <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b64">65]</ref>, the majority of reports documenting state-of-the-art LLMs <ref type="bibr" target="#b0">[1]</ref> provide scarce details, if any, on their pretraining datasets. Even open-weights models such as LLaMA <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b57">58]</ref> provide little to no details about their training data, let alone release their datasets. Furthermore, the process of studying and building optimal data compositions, along with developing filtering rules and heuristics, is time-consuming as it necessitates running numerous ablations on   <ref type="bibr" target="#b35">[36]</ref>, OLMo <ref type="bibr" target="#b18">[19]</ref>, Snowflake's Arctic <ref type="bibr" target="#b53">[54]</ref> and RedPajama-INCITE. SlimPajama is a cleaned and deduplicated version of RedPajama-V1. The remainder of this paper is organized as follows. In Section 2, we position the RedPajama dataset in the current landscape of open pretraining datasets. Section 3 describes the details of the dataset creation process behind RedPajama-V1, as well as building the RedPajama-INCITE family of models. Section 4 proceeds to RedPajama-V2, our web-only dataset. Next to describing the data processing steps, we present dataset statistics and ablation studies. Finally, we conclude in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Numerous efforts have focused on constructing pretraining datasets for large language models. While some of these datasets are curated from a mix of various sources, others are exclusively derived from web data. In the realm of web-only datasets, the C4 dataset <ref type="bibr" target="#b45">[46]</ref> was one of the first large-scale web datasets, comprising a 175B token web corpus filtered down from CommonCrawl. C4 still remains a benchmark for web dataset quality. More recently, RefinedWeb <ref type="bibr" target="#b43">[44]</ref> and FineWeb <ref type="bibr" target="#b42">[43]</ref> have demonstrated that web-only data can yield strong models without the need for compositing multiple domains, and have, similar to our work, also provide ample details on their data curation techniques. In contrast to these datasets, RedPajama-V2 is composed of 100 trillion tokens of raw, mostly unfiltered text. With its more than 40 quality signals for potential filtering, RedPajama-V2 promotes an entirely different approach and aims to set a new standard for future high-quality web datasets, providing a robust foundation for the next generation of high quality web datasets. Of further great relevance are the Gopher rules proposed in <ref type="bibr" target="#b44">[45]</ref>, which have been central to many of the previously mentioned open pretraining datasets.</p><p>Complementing web-only datasets, composite datasets introduce additional domains and enable broader coverage. Most notably, the Pile <ref type="bibr" target="#b16">[17]</ref> was one of the first fully open datasets. After the release of LLaMA <ref type="bibr" target="#b56">[57]</ref>, which used seven individual subsets and multiple domains, we published RedPajama-V1 as an open source replication of the LLaMA recipe, which gained widespread adoption. Building on this, the SlimPajama dataset <ref type="bibr" target="#b50">[51]</ref> was derived from RedPajama-V1 by further cleaning and deduplication. Similarly, the Dolma <ref type="bibr" target="#b51">[52]</ref> dataset includes other specialized domains, such as cleaned versions of code datasets including The Stack <ref type="bibr" target="#b24">[25]</ref>, StarcoderData <ref type="bibr" target="#b30">[31]</ref> as well as the ArXiv and StackExchange splits of RedPajama-V1. The Zyda <ref type="bibr" target="#b55">[56]</ref> dataset goes in a similar vein and further refines open datasets, including the SlimPajama dataset derived from RedPajama-V1. Finally, the ROOTS corpus <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref> is also among the core open datasets spanning multiple domains and languages. Table <ref type="table" target="#tab_0">1</ref> shows an overview over these open datasets and makes a comparison where each dataset stands in terms of transparency, versatility and scale.  In our first iteration of the RedPajama datasets, our primary goal was to recreate the training data documented in the LLaMA technical report <ref type="bibr" target="#b56">[57]</ref>. To this end, we closely follow the descriptions of the original recipes. In this section, we first document our process for recreating the original LLaMA training corpus (Section 3.1). We highlight gaps in the description of the original dataset collection and describe how we choose to resolve those ambiguities. Next, we report on RedPajama-INCITE, a family of models trained on this corpus in collaboration with Oak Ridge National Lab (ORNL) (Section 3.2). We find that although the resulting models are performant at the 3B scale, there remains a gap at 7B to the original LLaMA-7B model. We hypothesize that this is partly due to the need to train with the FP16 precision.</p><p>In addition, this also suggests the possibility that some salient details that went into the construction of the original LLaMA training corpus may be missing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Processing Steps</head><p>Here we describe our attempt to recreate the training corpus described in the LLaMa technical report <ref type="bibr" target="#b56">[57]</ref>. The pretraining data of the LLaMA training corpus are drawn from seven datasets: English CommonCrawl, C4, GitHub, Wikipedia, Books (Project Gutenberg and Books3), ArXiv, and Stack Exchange. Each of these datasets are given a short (approximately one-paragraph) description in the LLaMA technical report, and there are some gaps in the dataset descriptions. In this section, we detail our process for recreating each of the individual datasets, highlight the gaps in the descriptions from the LLaMA technical report, and describe our choices in resolving those ambiguities. These steps together resulted in a dataset of approximately 1.2 Trillion tokens. Table <ref type="table" target="#tab_1">2</ref> summarizes these datasets and the token counts. In Table <ref type="table" target="#tab_0">10</ref> in the Appendix, we further list all uncertainties encountered during the construction of the dataset.</p><p>CommonCrawl. The LLaMA corpus includes five CommonCrawl snapshots from 2017 to 2020, processed using the CCNet pipeline <ref type="bibr" target="#b60">[61]</ref>. CCNet deduplicates each snapshot in shards and assigns a quality classification to the data in each snapshot. It assigns a "head," "middle," and "tail" classification to each document based on the distribution of the perplexity assigned by a 5-gram Kneser-Ney model trained on Wikipedia. Here we only keep the "head" and "middle" buckets and discard the "tail." In addition, Touvron et. al. <ref type="bibr" target="#b56">[57]</ref> use a linear classifier trained on Wikipedia reference articles to filter out low-quality documents. The LLaMA paper does not specify which snapshots were used in the dataset or give details on the classifier.</p><p>Wikipedia. The LLaMA corpus uses Wikipedia dumps from June to August 2022 across 20 languages, processing the data to remove hyperlinks, comments, and other formatting boilerplate. For RedPajama-V1, we use the Wikipedia dataset available on Hugging Face Hub using the dump from 2023-03-20. This also preprocesses the data to remove hyperlinks, comments, and other boilerplate.</p><p>Gutenberg and Books3. The LLaMA corpus uses book corpora from the Gutenberg Project and Books3 from the Pile. We only use the PG19 subset of Gutenberg and use SimHash to remove near duplicates. We originally included Books3 as well but took it down due to copyright issues.</p><p>ArXiv. The LLaMA corpus processes arXiv LaTeX files and removes everything before the first section, comments, inline-expanded definitions and macros, and the bibliography, following <ref type="bibr" target="#b28">[29]</ref>.</p><p>We downloaded arXiv data from Amazon S3 in the "arXiv" requester pays bucket and implemented a similar postprocessing, keeping only LaTeX source files and removing preambles, comments, bibliographies, and expanding macros.</p><p>Stack Exchange. The LLaMa corpus includes a dump of Stack Exchange. The data is kept from the 28 largest websites, HTML tags are removed from the text, and answers are sorted by score from highest to lowest. Similarly, we download Stack Exchange from the Internet Archive, keep only the posts from the 28 largest sites, and remove HTML tags. In addition, we group the posts into question-answer pairs and order answers by their scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The RedPajama-INCITE family of LLMs</head><p>To evaluate how well RedPajama-V1 matches the original LLaMA corpus, we train a family of LLMs of various sizes in collaboration with the Incite project on the Summit supercomputer at Oak Ridge National Lab. The RedPajama-Incite family of LLMs includes a suite of pretrained and instruction-tuned models at the 3B and 7B model sizes. In this section, we first describe the compute setup of the Summit supercomputer and the implications for the pretraining runs (Section 3.2.1).</p><p>We then describe how we evaluate the model and speculate on differences in quality between these models and the LLaMA family (Section 3.2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Summit Training Setup</head><p>In this section, we describe the Summit supercomputer and the engineering and pretraining challenges to training the RedPajama-Incite family of LLMs. Our language models were trained using the Summit supercomputer at Oak Ridge National Lab, a cluster containing 4608 6xV100 nodes running an IBM Power9 architecture. This setup introduced a few challenges in training modern LLMs. In the following, we discuss these challenges and describe how we overcame them.</p><p>The IBM Power9 architecture uses a different instruction set than most modern chipsets (i.e., Intel-, Arm-, or Apple-based chips). Modern versions of PyTorch and most of the Python stack they depend on are not pre-compiled to support the Power9 architecture (the latest version officially supported was PyTorch 1.9). To support pretraining with modern libraries, members of our team needed to recompile PyTorch from scratch and build a custom training stack for Summit. Some of these efforts are documented in more detail in the GPT-NeoX technical report <ref type="bibr" target="#b5">[6]</ref>.</p><p>As of this writing, the Summit supercomputer runs on V100 GPUs, which are older than A100 or H100 GPUs typically used to train LLMs. Critically, V100s do not support the bf16 data type, which is necessary for modern stable training recipes for LLMs. Instead, we had to train with fp16 and use loss scaling <ref type="bibr" target="#b36">[37]</ref> to allow for stable training runs. We also had to lower the learning rate compared to those reported in the LLaMA training, which may have had an effect on convergence (1.6 • 10 -4 for the 3B model and 1.2 • 10 -4 for the 7B model).</p><p>The IBM Power9 architecture had slow interconnect, limiting the number of nodes we could use for each run. We were also unable to use the entire cluster since other projects were running simultaneously on it. We used 512 nodes in parallel (3072 GPUs) to train the 7B and 256 nodes in parallel (1536 GPUs) to train the 3B, with a global batch size of 4M tokens for each model. In scaling experiments, we found that we could not further increase the amount of parallelism without increasing the global batch size, which would hurt convergence.</p><p>The 6xV100 nodes introduce challenges for training with tensor and pipeline parallelism. We used 12way pipeline parallelism for the 7B and 6-way for the 3B model, as well as 2-way tensor parallelism for both models.</p><p>After accounting for these challenges, we were able to train the 3B model for 800B tokens total and the 7B model for 1.001T tokens total on Summit. We decayed the learning rate linearly following a warmup period, matching those described in the original LLaMA paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Evaluation</head><p>Here, we discuss evaluations for the RedPajama-INCITE-3B and 7B models on common benchmarks. The full results and benchmark scores are provided in Appendix C.2. After training RedPajama-Base-INCITE-3B for 800B tokens, it has better few-shot performance (measured in HELM classic <ref type="bibr" target="#b8">[9]</ref>), as the average score over 16 core scenarios) and better zero-shot performance (measured using Eleuther AI's LM evaluation harness <ref type="bibr" target="#b17">[18]</ref>) compared to other open models of similar size, including the well-regarded GPT-Neo and Pythia-2.8B (trained with 420B and 300B tokens, respectively, on the Pile). On HELM, it outperforms these models by 3-5 points. On a subset of tasks from LMevaluation harness, it outperforms these open models by 2-7 points.</p><p>The RedPajama-INCITE-7B-Base model is 1.0 points behind Falcon-7B and 4.1 points behind Llama-7B on HELM-classic. We further break down the tasks and see that they lag behind only on tasks that require using logprobs, which computes the difference between the probabilities of right and wrong answers. However, the model achieves comparable average HELM scores on tasks that directly generate answers and measure quality. Since all benchmarks in the LM harness use logprobs, we see similarly lower results for this benchmark. We hypothesize this was partly due to training with FP16, which does not allow us to use larger learning rates. Furthermore, as illustrated in the previous section, there were sources of uncertainty in the construction of the training dataset which likely resulted in a slightly different dataset than what was used to train the Llama-1 model. We believe that these two factors have led to the slightly lower performance compared to the Llama models.</p><p>RedPajama-INCITE-7B-Instruct is an instruction-tuned version of the base model optimized for few-shot performance by training on a diverse collection of NLP tasks from both P3 (BigScience) <ref type="bibr" target="#b48">[49]</ref> and Natural Instructions (AI2) <ref type="bibr" target="#b38">[39]</ref>. The Instruct version shows excellent performance on few-shot tasks, outperforming leading open models of similar sizes, including Llama-7B, Falcon-7B (both base and instruct version), and MPT-7B (both base and instruct version) on HELM by 2-8 points. We provide the detailed evaluation scores in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RedPajama-V2</head><p>In contrast to the first iteration of the RedPajama dataset, the second iteration focuses exclusively on web data and, in addition to design principles Transparency and Scale, we also put a higher emphasis on Versatility. Specifically, next to the goals of providing a fully transparent and open dataset, the purpose of the corpus is to serve as a foundation for creating high quality subsets. While the goal of transparency is achieved by making the dataset and its artifacts publicly available, and scale is achieved by processing large parts of the Common Crawl corpus, to follow the design principle Versatility, we release RedPajama V2 as a dataset that is enriched with a set of metadata that enables fast and cheap iteration for creating high quality, diverse and large datasets. In this section, we first present the data processing steps used to create the raw text data, give an overview over the quality signals available for each document, and present statistics on the dataset composition. Finally, we present ablation studies on how the quality signals can be used to create successively better datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data Processing Steps</head><p>RedPajama-V2 is a dataset created by processing web documents provided by the CommonCrawl foundation <ref type="foot" target="#foot_2">3</ref> . As web data is inherently noisy and only available as text embedded in the HTML code, it is necessary to process it to make it suitable for training LLMs. To that end, the raw data used for RedPajama-V2 undergoes a series of basic processing steps, which we explain in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Data Acquisition</head><p>The Common Crawl Archive is a vast repository of web crawl data that is freely available to the public. The corpus contains crawling results since 2013 and is updated regularly on a (bi-) monthly basis. Next to raw web data in HTML (warc) format, the archive also provides metadata (wat) and plain text data in the wet format. It has been the basis for numerous datasets including C4 <ref type="bibr" target="#b45">[46]</ref>, RefinedWeb <ref type="bibr" target="#b43">[44]</ref>, Dolma <ref type="bibr" target="#b51">[52]</ref>, and FineWeb <ref type="bibr" target="#b42">[43]</ref> among others.</p><p>To create the RedPajama-V2 dataset, we used the web-extracted text (i.e., .wet files) from all 84 monthly snapshots between 2014 and April 2023 and passed it through the CCNet pipeline <ref type="bibr" target="#b60">[61]</ref>.</p><p>In contrast to RPv1, here we keep all perplexity buckets, and in addition to the English language, we also keep French, German, Italian, and Spanish data. We chose this pipeline due to its light processing, which aligns with our guiding principle of preserving as much information in the raw dataset as possible and allowing downstream model developers to filter the dataset. This processing step produces over 100 billion individual text documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Quality Signals</head><p>A central ingredient to state-of-the-art open LLMs like Llama <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b57">58]</ref>, Mistral <ref type="bibr" target="#b21">[22]</ref>, Falcon <ref type="bibr" target="#b1">[2]</ref>, MPT <ref type="bibr" target="#b52">[53]</ref>, or the Qwen <ref type="bibr" target="#b2">[3]</ref> models is the large amount of high-quality data that these models are trained on. For example, Llama 3 is trained on 15 trillion carefully curated tokens. The most prominent data sources that provide the necessary scale are the crawls made publicly available by CommonCrawl. However, this raw text, which in our case is additionally processed by the CCNet pipeline, is still not ideal for direct use as LLM training data due to artifacts arising from the conversion from HTML to plain text (e.g., parsing errors, and menus), sources of generally low quality, and biases inherent to the distribution of content on the web. To clean such datasets, the literature has proposed a multitude of heuristics to extract high-quality datasets out of large corpora of heterogeneous web data. However, unlike previous datasets that filter out low-quality content, our approach retains the entire raw text corpus, incorporating quality signals as additional metadata. This strategy allows us to use the full spectrum of data, transforming sections typically discarded into informative attributes that enhance our dataset's utility. This enables the creation of other datasets such as C4 as special cases of the RedPajama-V2 dataset. For each document, we provide the quality signals used in C4 <ref type="bibr" target="#b45">[46]</ref>, Gopher <ref type="bibr" target="#b44">[45]</ref>, RefinedWeb <ref type="bibr" target="#b43">[44]</ref>, the Pretrainer's Guide <ref type="bibr" target="#b33">[34]</ref> and DSIR <ref type="bibr" target="#b61">[62]</ref>. These can roughly be categorized into quality signals which measure natural language, the repetiveness of the text, are based on the content of the text, ML-based heuristics, and deduplication. In the following, we explain each of these categories in detail. A comprehensive list with detailed descriptions encompassing all quality signals, as well as histograms is provided in Appendix D.2.</p><p>Natural Language. Text documents extracted from websites often have content that does not correspond to natural language, such as JavaScript code, menus, and other boilerplate text. To measure how natural a given text document is, we provide simple heuristic measures such as the fraction of all caps words or letters, the fraction of lines that end with an ellipsis, the fraction of unique words, whether or not a line ends in a terminal punctuation mark, and others.</p><p>Repetitiveness. An often observed artifact of web data is repetitive text, which has been linked with uninformative content <ref type="bibr" target="#b44">[45]</ref>. Repetitious generations are also a known failure mode of language models <ref type="bibr" target="#b20">[21]</ref>, and removing excessively repetitive content can potentially contribute to alleviating this behavior <ref type="bibr" target="#b44">[45]</ref>. For each document, we calculate the fraction of characters appearing in the most frequent (word) n-gram for n ∈ {2, 3, 4}. Second, we calculate the fraction of characters appearing in any duplicated n-gram for values of n ∈ {5, . . . , 10}. We ensure not to count characters that appear in overlapping n-grams more than once.</p><p>Content-based. Web documents can contain harmful and offensive content, which needs to be addressed. To that end we provide the signals used in C4 and RefinedWeb, namely, (1) the number of sequences of words that are contained in the LDNOOBW blocklist <ref type="foot" target="#foot_3">4</ref> . In addition, we include a flag which indicates whether the domain of the document appears in the UT1 list of blocked urls <ref type="foot" target="#foot_4">5</ref> . While these quality signals focus on NSFW content, we believe other content-based filters such as domains or embedding clusters <ref type="bibr" target="#b54">[55]</ref> are also promising directions. In Figure <ref type="figure" target="#fig_9">8</ref> in the Appendix, we show the distribution of topics found via clustering of embeddings.</p><p>ML Heuristics. ML-based quality signals revolve around the idea of measuring similarity to a high-quality domain. Here, we use fastText classifiers <ref type="bibr" target="#b23">[24]</ref>, and the importance weights proposed in <ref type="bibr" target="#b61">[62]</ref>. While ML filters have been shown to improve the quality of datasets (e.g., <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b10">11]</ref>), they have also been reported to lead to biases or underrepresent minorities <ref type="bibr" target="#b14">[15]</ref>. The fastText classifier signals provided in RPv2 are unigram bag-of-word models trained to distinguish between unfiltered RPv2 data and a high-quality domain. For English data, we use Wikipedia, websites referenced by Wikipedia, books, and the OpenWebText dataset. For non-English data, we only use Wikipedia.</p><p>The DSIR weights proposed in <ref type="bibr" target="#b61">[62]</ref> estimate the importance of individual samples to a given target domain in a reduced feature space and are based on word unigrams and bigram models. The weights are defined as the log-likelihood ratio between a language model of the target vs. the source domain, where we use the same domains as for the fasttext classifiers.</p><p>Deduplication. Removing duplicated training data has been found to improve model perplexity and reduce the amount of memorization while reducing the training data size and the required compute <ref type="bibr" target="#b27">[28]</ref>. Deduplication is also one of the core building blocks of the most popular datasets <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b43">44]</ref>. In RPv2, we include MinHash signatures for fuzzy deduplication <ref type="bibr" target="#b9">[10]</ref> at different similarity levels, as well as IDs of documents found to be exact duplicates via a Bloom filter <ref type="bibr" target="#b6">[7]</ref> with the error rate set to 1%<ref type="foot" target="#foot_5">foot_5</ref> . For this document-level deduplication, we proceed sequentially, starting with the most recent dump (2023-14) and successively iterating over the following dumps until we reach the oldest one (2014-15). An overview over how many documents were flagged as duplicates in this manner, can be seen in Figure <ref type="figure" target="#fig_4">3</ref> in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Dataset Statistics</head><p>RPv2 consists of 113B documents in five different languages: English, German, French, Spanish and Italian. As mentioned previously, the CCNet pipeline partitions the dataset into the tree buckets "head", "middle", and "tail" corresponding to documents with low, medium, and high Wikipedia perplexity. There are 32.8B documents in the head+middle partition and 80B documents in the tail partition. Documents in the tail are typically shorter (850 tokens) than in the head and middle buckets (∼ 1500 tokens). Token counts were estimated based on an i.i.d. sample of 100M documents using the Mistral <ref type="bibr" target="#b21">[22]</ref> BPE tokenizer. A detailed overview of token counts for each language and partition is given in Table <ref type="table" target="#tab_2">3</ref>. We provide further statistics on the number of documents before and after deduplication, as well as the distribution of quality signals in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Dataset Ablations</head><p>Here we present a series of dataset ablations with the aim of developing a better understanding of how the quality signals introduced in Section 4.1.2 influence the downstream performance of language models trained on data filtered with different heuristics. More specifically, here we ask how do different quality filtering rules affect downstream performance? We strive for a broad evaluation and measure the performance on diverse downstream benchmarks and the language modeling objective on multiple domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Setup</head><p>Models. We adopt decoder-only Llama-2 architectures <ref type="bibr" target="#b57">[58]</ref> with 468M parameters and 1.6B parameters and 2048 sequence length. Both models have 24 layers, 16 attention heads and the MLP expansion ratio set to 4.0. The 468M has 1024 hidden dimension, while for the 1.6B we use 2048.</p><p>For each dataset, we train the 468M model on 100B tokens and the 1.6B model on 350B tokens. We use the AdamW <ref type="bibr" target="#b13">[14]</ref> optimizer with the weight decay of 0.1, a maximum learning rate set to 5e-3 and 5e-4 respectively, and a cosine decay schedule with linear warmup during the first 1% of steps.</p><p>We use relatively small scales, as this enables us to explore a wider range of filters, showing the breadth of the quality filters available in RedPajama.</p><p>Hardware and Training Stack. Due to its ease of setup, use, and high model flops utilization, we use the OLMo framework <ref type="foot" target="#foot_6">7</ref> for distributed training, using FSDP <ref type="bibr" target="#b65">[66]</ref> for parallelization across multiple GPUs and nodes. For evaluation, we use the lm-evaluation-harness. We train our models on up to 5 H100 nodes with Infiniband interconnect.</p><p>Evaluation Metrics. We strive for broad coverage of benchmarks and domains. At the same time, we are operating at a relatively small scale, where many tasks are too hard to provide a high enough signal to distinguish datasets.</p><p>Table <ref type="table">4</ref>: Benchmarks used in our ablations. The column "Agg. BM-Eval" indicates whether the score is used in the aggregate scores reported in Tables <ref type="table" target="#tab_3">5</ref> and <ref type="table" target="#tab_4">6</ref>.</p><p>Task Type Random Metric Agg. BM-Eval ANLI <ref type="bibr" target="#b39">[40]</ref> Natural language inference 25.0 acc ARC-c <ref type="bibr" target="#b12">[13]</ref> Natural language inference 25.0 acc_norm ARC-e <ref type="bibr" target="#b12">[13]</ref> Natural language inference 25.0 acc_norm ✔ Winogrande <ref type="bibr" target="#b47">[48]</ref> Coreference resolution 50.0 acc ✔ Hellaswag <ref type="bibr" target="#b63">[64]</ref> Sentence completion 25.0 acc_norm ✔ LAMBADA <ref type="bibr" target="#b41">[42]</ref> Sentence completion 0.0 acc ✔ CoQA <ref type="bibr" target="#b46">[47]</ref> Conversational QA 0.0 F1 ✔ MMLU <ref type="bibr" target="#b19">[20]</ref> Multiple-choice QA 25.0 acc ✔ OpenbookQA <ref type="bibr" target="#b37">[38]</ref> Multiple-choice QA 25.0 acc_norm ✔ PIQA <ref type="bibr" target="#b4">[5]</ref> Multiple-choice QA 50.0 acc_norm ✔ PubMedQA <ref type="bibr" target="#b22">[23]</ref> Multiple-choice QA 33.3 acc ✔ SciQ <ref type="bibr" target="#b59">[60]</ref> Multiple-choice QA 25.0 acc_norm ✔ SocialIQA <ref type="bibr" target="#b49">[50]</ref> Multiple-choice QA 25.0 acc TruthfulQA <ref type="bibr" target="#b32">[33]</ref> Multiple-choice QA 25.0 acc Similar to the FineWeb <ref type="bibr" target="#b42">[43]</ref> dataset, we look for benchmarks that provide a high enough signalto-noise ratio, even at this small model scale.</p><p>After careful consideration, we settled for the choice of benchmarks in Table <ref type="table">4</ref>. Here we present aggregated scores by (1) computing the average over benchmarks, (2) the normalized average, and (3) the normalized sum of ranks for each data recipe. We chose to include the latter to avoid averaging over scores with different scales. More detailed scores are provided in the supplementary materials. To rank datasets based on the Perplexity of target domains, we follow the approach taken in Dolma <ref type="bibr" target="#b51">[52]</ref> and adopt the Paloma <ref type="bibr" target="#b34">[35]</ref> and Pile <ref type="bibr" target="#b16">[17]</ref> validation sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Results</head><p>We start by using the quality signals to implement some of the most widely used filters in the literature.</p><p>In addition, we also investigate ML heuristics available in RPv2, which are based on fastText bag of n-gram classifiers <ref type="bibr" target="#b23">[24]</ref> and DSIR importance weights <ref type="bibr" target="#b61">[62]</ref>. We run ablations on two subsets of RPv2, namely on the 2023-14 crawl and on the 9 crawls from 2021-49 to 2023-14, which were deduplicated using MinHash LSH on word 13-grams, with 128 hash functions, 9 bands and 13 rows. For the 1.6B ablations, we filter the full RPv2 dataset, then sample roughly 1T tokens and deduplicate it with the same Minhash hyperparameters.</p><p>Filters. We seek to cover a wide range of quality filtering configurations. Rather than optimizing the performance on a particular benchmark, the goal is to show that filtering the RPv2 dataset in different ways can lead to wildly different model performance. We thus experiment with variations of the C4 and Gopher rules and also use the ML-based quality signals in RPv2. We also use a custom configuration custom-rules based on word counts, average line length, Wikipedia Perplexity and the Wikipedia References classifier.</p><p>Results. From Table <ref type="table" target="#tab_3">5</ref>, we can draw a series of conclusions on filtering the RedPajama-V2 dataset.</p><p>First, we can see that the Gopher rules generally improve performance. In particular we see that fuzzy deduplication and filtering with Gopher has the highest aggregated scores across all RPv2 datasets. In addition, both the average and normalized average benchmark score is only second to RefinedWeb, while the rank-score is higher than for RefinedWeb. The per-benchmark tables 18, 19, and 20 in the Appendix, show that the RPv2 dataset filtered with fuzzy deduplication and Gopher is always in the upper middle (minimum rank score 9 of 19), while RefinedWeb is performing worse on Hellaswag, LAMBADA, Winogrande, MMLU and OpenBookQA. This indicates that filtering RPv2 with the full Gopher rules and fuzzy deduplication (Minhash LSH) creates a dataset that performs well across a wider range of tasks than all other datasets. Second, we can see that the Gopher-natlang filters perform better than the Gopher-repetition filters. Third, in the context of model based filtering, we see no significant difference between using a fasttext classifier and DSIR. Fourth, using only the line-level C4 filters appears to reduce perplexity, but has negligible effect on the aggregated benchmark scores. Finally, we notice that the unfiltered RPv2 2023-14 dataset appears to have the lowest perplexity on the Paloma dataset, while other filtering methods lead to models with higher perplexity. We believe that this can (at least in part) be attributed to the wide range of domains covered by Paloma. In addition, Paloma also contains the RPv1 dataset, which can explain the low Perplexity score obtained by the model trained on RPv1-CC. Table <ref type="table" target="#tab_4">6</ref> shows further that the model trained on RPv2 filtered with the full Gopher rules outperforms the model trained on RPv2 filtered with only the Gopher-natlang rules, and comes close to the quality of a model trained on the RefinedWeb dataset. In conclusion, this series of ablation studies shows how the quality signals in the RPv2 dataset can be used to successively filter better datasets. In combination with its vast scale of over 100T tokens, we see that this dataset provides a powerful source for creating high-quality web datasets for LLM pretraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we have presented the RedPajama datasets. With over 100 Trillion tokens, these are the largest, fully open, and transparent datasets for pretraining language models and have been a central building block for many of the strongest open-source LLMs. Next to documentation accompanying the datasets, we have also shown examples of how RedPajama-V2 can be filtered down to successively higher quality subsets, leading to language models of varying levels of quality on a diverse set of benchmark tasks and outperforming models trained on other large-scale pretraining corpora. While the models are relatively small and enabled us to explore a wider variety of filters, it is also a limitation and further, larger-scale explorations are required. We did not explore a thorough decontamination analysis against common benchmarks or an analysis of personally identifiable information present in the dataset, posing another limitation of this work. By publishing the RedPajama-V2 dataset in raw, unfiltered form, but accompanied by a set of quality signals, we hope that future work will continue to build on RedPajama and provide new innovative ways of filtering, curating, and mixing multiple pretraining corpora.</p><p>" pred_label " : ... , " pred_label_prob " : ... , " wiki_prob " : ... , " source " : "..." }</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.2 RedPajama-V2</head><p>The core of the dataset is composed of the text documents, accompanied by the quality annotations, duplicate ids and minhash signatures. For the text documents, the structure largely follows the one defined by CCNet. Specifically, the documents for a given CommonCrawl snapshot are partitioned into 5000 shards, where the filename indicates the shard, language of the document, and the perplexity bucket (partition). The quality annotations, duplicates and minhashes follow the same logic and reflect the filenames of the raw documents.</p><p>File Structure. The files containing the raw text documents are organized according to the following pattern:</p><formula xml:id="formula_0">documents/&lt;snapshot_id&gt;/</formula><p>&lt;shard_id&gt;/&lt;lang&gt;_&lt;ppl_bucket&gt;.json.gz where snapshot_id corresponds to any of the crawls included in RPv2, shard_id ranges from 0000 to 4999, lang is any one of en, de, fr, es or it. Finally, ppl_bucket indicates the partitioning according to Wikipedia perplexity and is either head, middle or tail. Similarly, quality signals, duplicate ids and minhashes follow the patterns quality_signals/&lt;snapshot_id&gt;/&lt;shard_id&gt;/&lt;lang&gt;_&lt;ppl_bucket&gt;.signals.json.gz, duplicates/&lt;snapshot_id&gt;/&lt;shard_id&gt;/&lt;lang&gt;_&lt;ppl_bucket&gt;.duplicates.parquet, and minhashes/&lt;snapshot_id&gt;/&lt;shard_id&gt;/&lt;lang&gt;_&lt;ppl_bucket&gt;.minhash.parquet.</p><p>Documents Structure. The documents are stored as Gzip-compressed JSONL files and follow the schema { " url " : "..." , " date_download " : "..." , " digest " : "..." , " length " : ... , " nlines " : ... , " source_domain " : "..." , " title " : "..." , " raw_content " : "..." , " cc_segment " : "..." , " original_nlines " : ... , " original_length " : .. , " line_ids " : [ ... ] , " language " : "..." , " language_score " : ... , " perplexity " : ... , " bucket " : "..." } Quality Signals Structure. The quality signals are Gzip-compressed JSONL files and follow the schema { " id " : "..." , " id_int " : ... , " metadata " : { " cc_segment " : "..." , " cc_net_source " : "..." , " url " : "..." , " source_domain " : "..." , " language " : "..." , " snapshot_id " : "..." } , " quality_signals " : { " key " : [ [ start , end , score ] ] } }</p><p>The quality_signals field is a dictionary with the name of the quality signal as key and a list of tuples as values. Each tuple consists of the three floats start, end, and score indicating the location where the score in the raw_content string applies. This representation follows the one used in Dolma <ref type="bibr" target="#b51">[52]</ref> and allows a single representation to encode quality signals which apply at different levels of granularity of the text (e.g., line-level and document-level).</p><p>Duplicate IDs. The ids of duplicated documents are stored as parquet files. Each row in the parquet file corresponds to a document which is duplicated at least once across the entire corpus. We emphasize that this does not include the first occurrence of a document which has subsequent duplicates. In other words, if every document appearing in the list of duplicates is dropped, one member of each cluster of documents remains in the dataset.</p><p>Minhashes. The minhash signatures are stored in parquet files and are partitioned into multiple bands and rows, corresponding to different levels of Jaccard similarities in the range {0.7, 0.8, 0.9, 1.0}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C RedPajama-V1</head><p>Here we provide additional details and results for the RedPajama-V1 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Filtering Heuristics for Code obtained from GitHub</head><p>As indicated in the main part of this paper, we filter the raw GitHub dataset by keeping only projects under Apache, BSD and MIT licenses, and additionally apply filtering heuristics similar to the ones used in The Stack dataset <ref type="bibr" target="#b24">[25]</ref>. Specifically, we apply the following set of heuristics, removing any file with the following properties:</p><p>• maximum line length of more than 1000 characters.</p><p>• an average line length of more than 100 characters.</p><p>• a proportion of alphanumeric characters of less than 0.25.</p><p>• a ratio between the number of alphabetical characters and the number of tokens of less than 1.5.</p><p>• extension is not in the following set of whitelisted extensions: .asm, .</p><p>bat, .cmd, .c, .h, .cs, .cpp, .hpp, .c++, .h++, .cc, .hh, .C, .H, .cmake, .css, .dockerfile, .f90, .f, .f03, .f08, .f77, .f95, .for, .fpp, .go, .hs, .html, .java, .js, .jl, .lua, .md, .markdown, .php, .php3, .php4, .php5, .phps, .phpt, .pl, .pm, .pod, .perl, .ps1, .psd1, .psm1, .py, .rb, .rs, .sql, .scala, .sh, .bash, .command, .zsh, .ts, .tsx, .tex, .vb, Dockerfile, Makefile, .xml, .rst, .m, .smali C.2 Detailed Evaluations for the RedPajama-INCITE LLMs Here we provide detailed benchmark scores for the RedPajama-INCITE 3B and 7B LLMs, trained on the RedPajama-V1 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Detailed Sources of Uncertainties in the Construction of the RedPajama-V1 Dataset</head><p>Table <ref type="table" target="#tab_0">10</ref> shows a detailed overview over different sources of uncertainty that arose during the construction of the RedPajama-V1 dataset. These uncertainties mainly stem from a lack of details on the dataset presented in <ref type="bibr" target="#b56">[57]</ref>. From this list, it can be seen that it is likely that there is a mismatch Table 7: Results for RedPajama-INCITE-Base-3B-v1 on a subset of lm-evaluation-harness (Zero-Shot) and HELM, compared to models with similar parameter counts. The top-scoring model for each benchmark is highlighted in bold font. Lambada OpenAi (acc) Hellaswag (acc_norm) Winogrande (acc) Piqa (acc) Avg. HELM avg. GPT-Neo 0.6223 0.5579 0.5769 0.7219 0.6197 0.3570 Pythia-2.8B 0.6466 0.5933 0.6006 0.7399 0.6451 0.3770 Pythia-2.8B-dedup 0.6524 0.5941 0.5848 0.7404 0.6429 -RedPajama-INCITE-Base-3B-v1 0.6541 0.6317 0.6322 0.7470 0.6662 0.4060 between RedPajama-V1 and the dataset used to train the Llama-1 models. We believe this is a significant factor that has contributed to the performance mismatch between RedPajama-INCITE and LLaMA-1.</p><p>Table <ref type="table" target="#tab_0">10</ref>: Overview over the different uncertainties and decisions made during the construction of the RedPajama-V1 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subset Uncertainty Decision</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CommonCrawl</head><p>Which snapshots were used?</p><p>We use the first snapshot from 2019 to 2023.</p><p>What classifier was used, and how was it constructed?</p><p>We use a fasttext classifier with unigram features and use 300k training samples.</p><p>What threshold was used to classify a sample as high quality?</p><p>We set the threshold to match the token count reported in LLama.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GitHub Quality filtering heuristics</head><p>We remove any file • with a maximum line length of more than 1000 characters.</p><p>• with an average line length of more than 100 characters.</p><p>• with a proportion of alphanumeric characters of less than 0.25.</p><p>• with a ratio between the number of alphabetical characters and the number of tokens of less than 1.5.</p><p>• whose extension is not in the following set of whitelisted extensions: .asm, .</p><p>.sh, .bash, .command, .zsh, .ts, .tsx, .tex, .vb, Dockerfile, Makefile, .xml, .rst, .m, .smali Wikipedia Which Wikipedia dump was used? We used the most recent at the time of data curation (2023-03-20).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Books</head><p>How were the books deduplicated? We use SimHash to perform near deduplication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D RedPajama-V2</head><p>In this section we provide additional analyses and statistics of the RedPajama-V2 web dataset, and present detailed results for the ablation models trained on differently filtered subsets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Summary Statistics of our Deduplication Approach</head><p>In Figure <ref type="figure" target="#fig_4">3</ref>, we see how the number of documents in the head+middle partition develops as a function of the point in time of each crawl. What stands out here is that there is a relatively stable number until 2018 and a significantly smaller number of documents between 2014 and 2016 (up to 10x for, e.g., German). It is also worth noting how the number of unique documents over time develops (dashed line). Specifically, since we ran the deduplication from the newest snapshot to the oldest, one expects an increasingly smaller number of unique documents in the corpus, which can be observed from Figure <ref type="figure" target="#fig_4">3</ref> (note the log-scale). However, it is worth pointing out the sudden drop in unique documents occurring for the crawls between 2014 and 2017. We believe that this can be explained by a different list of seeds used by the CommonCrawl web crawler during that period.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Quality Signals</head><p>In this section we provide further details and statistics on the quality signals which are part of the RedPajama-V2 dataset.  The set of quality signals can be grouped into such with measure natural language (Table <ref type="table" target="#tab_11">12</ref>), the repetitiveness of the text (Table <ref type="table" target="#tab_0">14</ref>), are based on the content of the text (Table <ref type="table" target="#tab_3">15</ref>), or which are ML-based heuristics (Table <ref type="table" target="#tab_2">13</ref>). In addition, here we also summarize the quality signals which are computed by the CCNet pipeline in Table <ref type="table" target="#tab_10">11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2.2 Histograms</head><p>Histograms of the distribution of quality signals are shown in Figures <ref type="figure" target="#fig_5">4,</ref><ref type="figure" target="#fig_6">5</ref>,6 and 7. The statistics are obtained from the 2023-06 snapshot and are computed only for English data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Embedding-based Clustering</head><p>To compute clusters based on the semantics of text documents, we randomly sampled 2,000,000 documents from the unfiltered 2021-04 snapshot of the RedPajama-V2 dataset and used the Alibaba-NLP gte-large-en-v1.5 model <ref type="bibr" target="#b31">[32]</ref> to compute embeddings on the middle 8,192 tokens of each document. We used Nomic Atlas <ref type="bibr" target="#b40">[41]</ref> for clustering and topic modeling analysis. An overview of clusters and associated topics is shown in Figure <ref type="figure" target="#fig_9">8</ref>. 6 randomly sampled documents, along with their corresponding cluster topics and a 1000-character substring from each document (starting after a random whitespace character), are shown in Table <ref type="table" target="#tab_4">16</ref> and Table <ref type="table" target="#tab_0">17</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 Data Ablations: Detailed Evaluations</head><p>We have shown aggregated benchmark scores in the main part of this work. Here, we provide more details and report the scores for each task separately. The results are shown in <ref type="bibr">Tables 18,</ref><ref type="bibr" target="#b18">19</ref> and 20. The ratio between the number of occurrences of '{' or '}' and the number of characters in the raw text. <ref type="bibr" target="#b45">[46]</ref> rps_doc_frac_all_caps_words The fraction of words in the content that only consist of uppercase letters. This is based on the raw content. <ref type="bibr" target="#b33">[34]</ref> rps_doc_frac_lines_end_with_ellipsis</p><p>The fraction of lines that end with an ellipsis, where an ellipsis is defined as either "..." or "U+2026".</p><p>[ <ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45]</ref> rps_doc_frac_no_alph_words The fraction of words that contain no alphabetical character. <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45]</ref> rps_doc_lorem_ipsum</p><p>The ratio between the number of occurrences of 'lorem ipsum' and the number of characters in the content after normalisation. <ref type="bibr" target="#b45">[46]</ref> rps_doc_mean_word_length The mean length of words in the content after normalisation. <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45]</ref> rps_doc_stop_word_fraction</p><p>The ratio between the number of stop words and the number of words in the document. Stop words are obtained from <ref type="url" target="https://github.com/6/stopwords-json">https://github.com/6/stopwords-json</ref>.</p><p>[ <ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45]</ref> rps_doc_symbol_to_word_ratio</p><p>The ratio of symbols to words in the content. Symbols are defined as U+0023 (#), "...", and U+2026. <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45]</ref> rps_doc_frac_unique_words</p><p>The fraction of unique words in the content. This is also known as the degeneracy of a text sample. Calculated based on the normalised content.</p><p>[34] rps_doc_unigram_entropy</p><p>The entropy of the unigram distribution of the content. This measures the diversity of the content and is computed using</p><formula xml:id="formula_2">x -x n • log( 1 n</formula><p>)where the sum is taken over counts of unique words in the normalised content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>-rps_doc_word_count</head><p>The number of words in the content after normalisation. <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45]</ref> rps_lines_ending_with_terminal_punctution_mark Indicates whether a line ends with a terminal punctuation mark. A terminal punctuation mark is defined as one of: ".", "!", "?", """. <ref type="bibr" target="#b45">[46]</ref> rps_lines_javascript_counts The number of occurrences of the word "javascript" in each line. <ref type="bibr" target="#b45">[46]</ref> rps_lines_num_words The number of words in each line. This is computed based on the normalised text. <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b43">44]</ref> rps_lines_numerical_chars_fraction</p><p>The ratio between the number of numerical characters and total number of characters in each line. This is based on the normalised content. <ref type="bibr" target="#b43">[44]</ref> rps_lines_start_with_bulletpoint Whether the lines that start with a bullet point symbol. The following set of unicodes are considered a bullet point: U+2022 (bullet point), U+2023 (triangular bullet point), U+25B6 (black right pointing triangle), U+25C0 (black left pointing triangle), U+25E6 (white bullet point), U+2013 (en dash) U+25A0 (black square), U+25A1 (white square), U+25AA (black small square), U+25AB (white small square).</p><p>[ <ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b44">45]</ref> rps_lines_uppercase_letter_fraction</p><p>The ratio between the number of uppercase letters and total number of characters in each line. This is based on the raw text. <ref type="bibr" target="#b43">[44]</ref> rps_doc_num_sentences</p><p>The number of sentences in the content. <ref type="bibr" target="#b45">[46]</ref> Table <ref type="table" target="#tab_0">17</ref>: Examples of documents and corresponding cluster topics from Nomic Atlas <ref type="bibr" target="#b40">[41]</ref>.</p><p>Cluster Topics Document (broad -medium -specific) Online Privacy -Privacy Policy -Contracts shall be governed by the laws of the Federal Republic of Germany under exclusion of the UN Convention on the International Sale of Goods (CISG), without prejudice to any mandatory conflict of laws and consumer protection provisions. 11.2 If the Customer is an entrepreneur according to Sec. 14 German Civil Code ("BGB"), a legal person under public law or a special fund under public law the courts at the place of business of the vendor shall have exclusive jurisdiction in respect of all disputes arising out of or in connection with the relevant contract. 11. <ref type="bibr" target="#b2">3</ref> In the event that one or more provisions of the contract should be or become invalid or unenforceable, the validity of the remaining provisions shall not be affected thereby. The invalid or unenforceable provision shall be deemed to be replaced -as existentwith statutory provisions. In case of an unacceptable rigor to one of the parties, the contract shall be deemed invalid as a whole. 11.</p><p>4 In case of deviations of these General Religion/Spirituality -Film/Movie -Movie Movie of Nelson Mandela's life premieres in South Africa Nov. 04 -Stars Idris Elba and Naomie Harris attend the premiere of "Mandela: Long Walk to Freedom," based on the autobiography of anti-apartheid icon Nelson Mandela. Matthew Stock reports. Election -Election (2) -Healthcare (4)</p><p>McAuliffe revived that language as an amendment to the budget. He also called on the General Assembly to immediately convene a special joint committee that had been created to assess the impact that repealing the ACA would have had on Virginia. The legislature will gather April 5 to consider the governor's amendments and vetoes, but leaders said Monday that McAuliffe's new budget language stands no better chance this time. In a joint statement, the Republican leadership of the House of Delegates said expanding Medicaid would lead to increased costs and eventually blow a hole in the state budget. "The lack of action in Washington has not changed that and in fact, the uncertainty of federal health policy underscores the need to be cautious over the long term," the leaders, including House Speaker William J. Howell (R-Stafford) and the man selected to replace him as speaker when he retires next year, Del. Kirk Cox (R-Colonial Heights), said via email. "Virginians can barely afford our cu         </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Author Responsibility Statement</head><p>This aggregated dataset is licensed to you under the terms of the ODC-By-1.0, as well as any licenses that may apply to its constituent parts.</p><p>While we have made every effort to ensure the accuracy and legality of the data contained within this dataset, we cannot guarantee its absolute completeness or correctness due to its scale. Therefore, in the event that any rights, legal or otherwise, are violated through the use of this dataset, including but not limited to copyright infringement, privacy violations, or misuse of sensitive information, we, the authors, assume no liability for such violations. The dataset is provided to you "as is", without warranty of any kind, express or implied.</p><p>By utilizing this dataset, you agree that any consequences, legal or otherwise, arising from the use of this dataset will be your sole responsibility. You acknowledge that you will exercise due diligence and adhere to all applicable laws, regulations, and ethical guidelines when using the dataset. By accessing, downloading, or using this dataset, you signify your acceptance of this statement and your commitment to abide by the terms and conditions of the licenses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F License</head><p>The code provided in the GitHub repository<ref type="foot" target="#foot_7">foot_7</ref> is distributed under an Apache 2.0 license. For the datasets themselves, we refer to the Common Crawl Foundation Terms of Use<ref type="foot" target="#foot_8">foot_8</ref> for the datasets derived from the Common Crawl Archive. For the other datasets we refer to the license under which the dataset was originally distributed. Specifically,</p><p>• The C4 dataset at <ref type="url" target="https://huggingface.co/datasets/allenai/c4#license">https://huggingface.co/datasets/allenai/c4#license</ref>,</p><p>• The GitHub subset was limited to MIT, BSD, or Apache licenses only,</p><p>• The Arxiv terms of use for the arxiv subset under <ref type="url" target="https://info.arxiv.org/help/api/tou.html">https://info.arxiv.org/help/api/ tou.html</ref>, • The Wikipedia license for any wikipedia derived data <ref type="url" target="https://huggingface.co/datasets/legacy-datasets/wikipedia#licensing-information">https://huggingface.co/ datasets/legacy-datasets/wikipedia#licensing-information</ref>, • The StackExchange license on the Internet Archive for the StackExchange data <ref type="url" target="https://archive.org/details/stackexchange">https: //archive.org/details/stackexchange</ref>.</p><p>We further request from users that they abide by each individual license for the subset they use.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>38th</head><figDesc>Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets and Benchmarks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The ecosystem around the RedPajama datasets. RedPajama has provided pretraining data for multiple open-source LLMs, including OpenELM [36], OLMo [19], Snowflake's Arctic [54] and RedPajama-INCITE. SlimPajama is a cleaned and deduplicated version of RedPajama-V1.</figDesc><graphic coords="2,108.00,72.00,396.01,134.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>3</head><figDesc>RedPajama-V1: An open Reproduction of the LLaMA Training Data</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: RedPajama-INCITE-Base 3B results on a subset of lm-evaluation-harness. The tasks were selected according to the selection made to evaluate Pythia<ref type="bibr" target="#b3">[4]</ref> and GPT-J<ref type="bibr" target="#b58">[59]</ref> </figDesc><graphic coords="19,108.00,79.14,395.98,173.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Chronological count of documents for each CommonCrawl snapshot before and after deduplication. Deduplication is performed sequentially, starting from the most recent snapshot and iterating until the oldest snapshot.</figDesc><graphic coords="21,108.00,72.00,396.00,123.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Histograms for the quality signals computed by the CCNet [61] pipeline.</figDesc><graphic coords="26,108.00,170.60,126.72,88.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Histograms for ML-based quality signals.</figDesc><graphic coords="26,178.03,392.64,126.72,88.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Histograms for Natural language based quality signals.</figDesc><graphic coords="27,178.03,566.96,126.72,88.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Histograms for quality signals measuring the repetitiveness of text.</figDesc><graphic coords="28,108.00,297.98,126.72,88.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Visualization of topical clusters appearing in the RedPajama-V2 dataset. The clusters are computed in Nomic Atlas [41] based on gte-large-en-v1.5 embeddings for 2M documents of the unfiltered 2021-04 snapshot.</figDesc><graphic coords="29,108.00,72.00,395.99,388.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of open pretraining Datasets along the dimensions of transparency, versatility, and scale.C4 Based on the RedPajama-V2 dataset, we present a series of ablation studies on decoder-only Transformer models with 468 million parameters, showing how the quality signals can be used to create models of varying performance on common NLP benchmarks.</figDesc><table><row><cell>Dataset</cell><cell>Transparency</cell><cell></cell><cell></cell><cell>Versatility</cell><cell></cell><cell>Scale (TB)</cell></row><row><cell></cell><cell cols="5">Open Access Open Code Raw Data Composite Multilingual</cell><cell></cell></row><row><cell>Refined Web [44]</cell><cell>✔(subset)</cell><cell>✗</cell><cell>✗</cell><cell>✗</cell><cell>✗</cell><cell>2.8</cell></row><row><cell>FineWeb [43]</cell><cell>✔</cell><cell>✔</cell><cell>✗</cell><cell>✗</cell><cell>✗</cell><cell>93.4</cell></row><row><cell>FineWeb-EDU [43]</cell><cell>✔</cell><cell>✔</cell><cell>✗</cell><cell>✗</cell><cell>✗</cell><cell>8.8</cell></row><row><cell>C4 [46]</cell><cell>✔</cell><cell>✔</cell><cell>✗</cell><cell>✗</cell><cell>✗</cell><cell>0.3</cell></row><row><cell>mC4 [63]</cell><cell>✔</cell><cell>✔</cell><cell>✗</cell><cell>✗</cell><cell>✔</cell><cell>9.7</cell></row><row><cell>DCLM baseline [30]</cell><cell>✔</cell><cell>✔</cell><cell>✗</cell><cell>✗</cell><cell>✗</cell><cell>10.0</cell></row><row><cell>DCLM-Pool [30]</cell><cell>✔</cell><cell>✔</cell><cell>✔</cell><cell>✗</cell><cell>✔</cell><cell>340.0</cell></row><row><cell>Dolma v1.7 [52]</cell><cell>✔</cell><cell>✔</cell><cell>✗</cell><cell>✔</cell><cell>✗</cell><cell>4.5</cell></row><row><cell>Pile [17]</cell><cell>✔</cell><cell>✔</cell><cell>✗</cell><cell>✔</cell><cell>✗</cell><cell>0.8</cell></row><row><cell>SlimPajama [51]</cell><cell>✔</cell><cell>✔</cell><cell>✗</cell><cell>✔</cell><cell>✗</cell><cell>0.9</cell></row><row><cell>ROOTS [26, 27]</cell><cell>✔</cell><cell>✔</cell><cell>✗</cell><cell>✔</cell><cell>✔</cell><cell>1.6</cell></row><row><cell>RedPajama-V1</cell><cell>✔</cell><cell>✔</cell><cell>✗</cell><cell>✔</cell><cell>✗</cell><cell>3.0</cell></row><row><cell>RedPajama-V2</cell><cell>✔</cell><cell>✔</cell><cell>✔</cell><cell>✗</cell><cell>✔</cell><cell>270.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Token counts for the RedPajama-V1 dataset.</figDesc><table><row><cell>Dataset Slice</cell><cell>Token Count</cell></row><row><cell>CommonCrawl</cell><cell>878B</cell></row><row><cell>C4</cell><cell>175B</cell></row><row><cell>GitHub</cell><cell>59B</cell></row><row><cell>Books</cell><cell>26B</cell></row><row><cell>ArXiv</cell><cell>28B</cell></row><row><cell>Wikipedia</cell><cell>24B</cell></row><row><cell>StackExchange</cell><cell>20B</cell></row><row><cell>Total</cell><cell>1.2T</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Document and token counts for each partition and language of the RPv2 dataset.</figDesc><table><row><cell></cell><cell>All</cell><cell></cell><cell>tail</cell><cell></cell><cell cols="2">head+middle</cell><cell cols="2">head+middle (dedupe)</cell></row><row><cell></cell><cell cols="7">docs (B) tokens (T) docs (B) tokens (T) docs (B) tokens (T) docs (B)</cell><cell>tokens (T)</cell></row><row><cell>English</cell><cell>87.5</cell><cell>90.5</cell><cell>63.0</cell><cell>53.6</cell><cell>24.5</cell><cell>37.0</cell><cell>14.5</cell><cell>20.5</cell></row><row><cell>German</cell><cell>8.6</cell><cell>10.3</cell><cell>5.9</cell><cell>6.2</cell><cell>2.7</cell><cell>4.1</cell><cell>1.9</cell><cell>3.0</cell></row><row><cell>French</cell><cell>6.7</cell><cell>8.5</cell><cell>4.5</cell><cell>4.8</cell><cell>2.2</cell><cell>3.7</cell><cell>1.6</cell><cell>2.7</cell></row><row><cell>Spanish</cell><cell>6.9</cell><cell>9.5</cell><cell>4.7</cell><cell>5.6</cell><cell>2.3</cell><cell>3.9</cell><cell>1.8</cell><cell>2.8</cell></row><row><cell>Italian</cell><cell>3.5</cell><cell>4.7</cell><cell>2.4</cell><cell>2.7</cell><cell>1.2</cell><cell>1.9</cell><cell>0.9</cell><cell>1.5</cell></row><row><cell>Total</cell><cell>113.3</cell><cell>123.7</cell><cell>80.5</cell><cell>73.0</cell><cell>32.8</cell><cell>50.7</cell><cell>20.8</cell><cell>30.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Evaluations for the 468M parameter LM for different dataset filters and other SOTA web datasets. The Benchmark scores are aggregated from the benchmarks outlined in Table3, using (1) the average accuracy, (2) the Rank-Score, and (3) the normalized average score. The best score is indicated in bold underlined font, the second-best is bolded, and the third is in italics underlined.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Deduplication</cell><cell cols="2">Rule-based</cell><cell></cell><cell>ML Heuristics</cell><cell></cell><cell></cell><cell cols="2">Agg. BM-Eval (↑)</cell><cell>Val-Perplexity (↓)</cell></row><row><cell></cell><cell>Exact</cell><cell>Fuzzy</cell><cell>C4</cell><cell>Gopher</cell><cell>Classif.</cell><cell>DSIR</cell><cell>PPL</cell><cell cols="3">Avg. Norm. Avg. Rank-Score Pile</cell><cell>Paloma</cell></row><row><cell>C4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>35.8</cell><cell>0.140</cell><cell>0.472</cell><cell>29.5</cell><cell>39.5</cell></row><row><cell>Dolma-v1.7 CC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>36.0</cell><cell>0.140</cell><cell>0.511</cell><cell>21.4</cell><cell>38.3</cell></row><row><cell>FineWeb</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>36.5</cell><cell>0.146</cell><cell>0.644</cell><cell>26.8</cell><cell>33.6</cell></row><row><cell>RefinedWeb</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>37.9</cell><cell>0.165</cell><cell>0.650</cell><cell>19.1</cell><cell>32.8</cell></row><row><cell>RPv1-CC</cell><cell>✔(sharded)</cell><cell></cell><cell></cell><cell></cell><cell>✔ (Wiki-Ref.)</cell><cell></cell><cell></cell><cell>35.6</cell><cell>0.127</cell><cell>0.461</cell><cell>18.7</cell><cell>31.5</cell></row><row><cell>RPv2 (2023-14)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>36.4</cell><cell>0.141</cell><cell>0.594</cell><cell>19.7</cell><cell>31.1</cell></row><row><cell>RPv2 (2023-14)</cell><cell>✔</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>36.2</cell><cell>0.138</cell><cell>0.472</cell><cell>19.5</cell><cell>39.9</cell></row><row><cell>RPv2 (2023-14)</cell><cell></cell><cell>✔</cell><cell></cell><cell>✔ (full)</cell><cell></cell><cell></cell><cell></cell><cell>37.6</cell><cell>0.160</cell><cell>0.700</cell><cell>24.9</cell><cell>34.5</cell></row><row><cell>RPv2 (2023-14)</cell><cell></cell><cell>✔</cell><cell>✔</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>36.8</cell><cell>0.150</cell><cell>0.622</cell><cell>36.3</cell><cell>56.9</cell></row><row><cell>RPv2 (2023-14)</cell><cell></cell><cell>✔</cell><cell></cell><cell>✔ (natlang)</cell><cell></cell><cell></cell><cell cols="2">Wiki-middle 37.2</cell><cell>0.154</cell><cell>0.639</cell><cell>23.6</cell><cell>38.2</cell></row><row><cell>RPv2 (2023-14)</cell><cell></cell><cell>✔</cell><cell></cell><cell>✔ (Rep.)</cell><cell></cell><cell></cell><cell cols="2">Wiki-middle 37.5</cell><cell>0.158</cell><cell>0.633</cell><cell>20.4</cell><cell>36.0</cell></row><row><cell>RPv2 (9 Dumps)</cell><cell></cell><cell>✔</cell><cell>✔</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>35.3</cell><cell>0.128</cell><cell>0.517</cell><cell>35.0</cell><cell>54.2</cell></row><row><cell>RPv2 (9 Dumps)</cell><cell></cell><cell>✔</cell><cell>✔</cell><cell>✔ (full)</cell><cell></cell><cell></cell><cell></cell><cell>36.7</cell><cell>0.149</cell><cell>0.556</cell><cell>43.8</cell><cell>63.9</cell></row><row><cell>RPv2 (9 Dumps)</cell><cell></cell><cell>✔</cell><cell>✔</cell><cell>✔ (Rep.)</cell><cell></cell><cell>✔ (Palm-mix)</cell><cell></cell><cell>35.9</cell><cell>0.138</cell><cell>0.439</cell><cell>44.3</cell><cell>89.9</cell></row><row><cell>RPv2 (9 Dumps)</cell><cell></cell><cell>✔</cell><cell>✔</cell><cell>✔ (Rep.)</cell><cell>✔ (Palm-mix)</cell><cell></cell><cell></cell><cell>35.9</cell><cell>0.139</cell><cell>0.483</cell><cell>43.8</cell><cell>67.1</cell></row><row><cell>RPv2 (9 Dumps)</cell><cell></cell><cell>✔</cell><cell>✔</cell><cell>✔ (natlang)</cell><cell>✔ (Palm-mix)</cell><cell></cell><cell></cell><cell>36.7</cell><cell>0.152</cell><cell>0.550</cell><cell>41.8</cell><cell>67.9</cell></row><row><cell>RPv2 (9 Dumps)</cell><cell></cell><cell>✔</cell><cell>✔ (line-filter)</cell><cell>✔ (natlang)</cell><cell>✔ (Palm-mix)</cell><cell></cell><cell></cell><cell>36.4</cell><cell>0.144</cell><cell>0.539</cell><cell>32.4</cell><cell>52.9</cell></row><row><cell>RPv2 (9 Dumps)</cell><cell></cell><cell>✔</cell><cell cols="2">custom-rules</cell><cell>✔ (Wiki-Ref.)</cell><cell></cell><cell>Pwiki &gt; 30</cell><cell>35.8</cell><cell>0.130</cell><cell>0.467</cell><cell>18.5</cell><cell>39.7</cell></row><row><cell>RPv2 (9 Dumps)</cell><cell></cell><cell>✔</cell><cell cols="3">custom-rules + Gopher-Rep. ✔ (Wiki-Ref.)</cell><cell></cell><cell>Pwiki &gt; 30</cell><cell>35.9</cell><cell>0.133</cell><cell>0.500</cell><cell>19.8</cell><cell>45.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Aggregated evaluations for the 1.6B parameter LM for different datasets. The Benchmark scores are aggregated from the benchmarks outlined in Table4, using (1) the average accuracy, (2) the Rank-Score, and (3) the normalized average score.</figDesc><table><row><cell>Dataset</cell><cell>Fuzzy</cell><cell cols="2">Rule-based</cell><cell>ML Heuristics</cell><cell></cell><cell cols="2">Agg. BM-Eval (↑)</cell><cell cols="2">Val-Perplexity (↓)</cell></row><row><cell></cell><cell>Deduplication</cell><cell>C4</cell><cell>Gopher</cell><cell cols="5">Palm Classif. Wiki-Ref Classif. Avg. Norm. Avg. Rank-Score Pile</cell><cell>Paloma</cell></row><row><cell>RefinedWeb</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>52.0</cell><cell>34.0</cell><cell>0.139</cell><cell>10.7</cell><cell>17.7</cell></row><row><cell>RPv2 (full)</cell><cell>✔</cell><cell></cell><cell>✔</cell><cell>✔</cell><cell>50.0</cell><cell>31.1</cell><cell>0.106</cell><cell>13.6</cell><cell>20.8</cell></row><row><cell>RPv2 (full)</cell><cell>✔</cell><cell cols="2">✔ ✔(natlang)</cell><cell>✔</cell><cell>47.9</cell><cell>29.4</cell><cell>0.089</cell><cell>22.2</cell><cell>30.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>HELM Benchmark results for RedPajama-INCITE-Base-7B-v1 and instruction tuned. The top-scoring model for each benchmark is highlighted in bold font.</figDesc><table><row><cell>Model</cell><cell>RedPajama 7B Instruct</cell><cell cols="3">Llama 7B MPT 7B Falcon 7B</cell><cell>RedPajama 7B Base</cell><cell>GPT J</cell><cell>Falcon 7B Instruct</cell><cell cols="2">Pythia 7B Dolly v2</cell><cell>MPT 7B Instruct</cell><cell>Stablelm Alpha 7B</cell></row><row><cell>HELM-AVG</cell><cell>0.492</cell><cell>0.472</cell><cell>0.444</cell><cell>0.441</cell><cell>0.431</cell><cell>0.417</cell><cell>0.407</cell><cell>0.400</cell><cell>0.396</cell><cell>0.393</cell><cell>0.288</cell></row><row><cell>MMLU -EM</cell><cell>0.366</cell><cell>0.345</cell><cell>0.294</cell><cell>0.285</cell><cell>0.323</cell><cell>0.249</cell><cell>0.271</cell><cell>0.266</cell><cell>0.238</cell><cell>0.349</cell><cell>0.293</cell></row><row><cell>BoolQ -EM</cell><cell>0.697</cell><cell>0.751</cell><cell>0.731</cell><cell>0.770</cell><cell>0.694</cell><cell>0.649</cell><cell>0.708</cell><cell>0.656</cell><cell>0.602</cell><cell>0.442</cell><cell>0.537</cell></row><row><cell>NarrativeQA -F1</cell><cell>0.623</cell><cell>0.524</cell><cell>0.541</cell><cell>0.549</cell><cell>0.512</cell><cell>0.545</cell><cell>0.381</cell><cell>0.427</cell><cell>0.441</cell><cell>0.220</cell><cell>0.218</cell></row><row><cell>NaturalQuestions (closed-book) -F1</cell><cell>0.229</cell><cell>0.297</cell><cell>0.284</cell><cell>0.289</cell><cell>0.258</cell><cell>0.156</cell><cell>0.192</cell><cell>0.141</cell><cell>0.133</cell><cell>0.247</cell><cell>0.077</cell></row><row><cell>NaturalQuestions (open-book) -F1</cell><cell>0.654</cell><cell>0.580</cell><cell>0.603</cell><cell>0.574</cell><cell>0.600</cell><cell>0.559</cell><cell>0.453</cell><cell>0.549</cell><cell>0.535</cell><cell>0.627</cell><cell>0.317</cell></row><row><cell>QuAC -F1</cell><cell>0.252</cell><cell>0.332</cell><cell>0.343</cell><cell>0.322</cell><cell>0.323</cell><cell>0.330</cell><cell>0.300</cell><cell>0.306</cell><cell>0.299</cell><cell>0.352</cell><cell>0.218</cell></row><row><cell>*HellaSwag -EM</cell><cell>0.698</cell><cell>0.747</cell><cell>0.754</cell><cell>0.732</cell><cell>0.702</cell><cell>0.663</cell><cell>0.690</cell><cell>0.653</cell><cell>0.692</cell><cell>0.763</cell><cell>0.421</cell></row><row><cell>*OpenbookQA -EM</cell><cell>0.488</cell><cell>0.574</cell><cell>0.540</cell><cell>0.546</cell><cell>0.504</cell><cell>0.514</cell><cell>0.498</cell><cell>0.496</cell><cell>0.516</cell><cell>0.532</cell><cell>0.394</cell></row><row><cell>TruthfulQA -EM</cell><cell>0.226</cell><cell>0.297</cell><cell>0.186</cell><cell>0.206</cell><cell>0.205</cell><cell>0.199</cell><cell>0.203</cell><cell>0.225</cell><cell>0.250</cell><cell>0.188</cell><cell>0.209</cell></row><row><cell>*MS MARCO (regular) -RR@10</cell><cell>0.391</cell><cell>0.252</cell><cell>0.161</cell><cell>0.169</cell><cell>0.135</cell><cell>0.152</cell><cell>0.225</cell><cell>0.159</cell><cell>0.160</cell><cell>0.161</cell><cell>0.110</cell></row><row><cell>*MS MARCO (TREC) -NDCG@10</cell><cell>0.709</cell><cell>0.482</cell><cell>0.369</cell><cell>0.362</cell><cell>0.322</cell><cell>0.345</cell><cell>0.481</cell><cell>0.342</cell><cell>0.359</cell><cell>0.387</cell><cell>0.253</cell></row><row><cell>CNN/DailyMail -ROUGE-2</cell><cell>0.143</cell><cell>0.149</cell><cell>0.137</cell><cell>0.147</cell><cell>0.137</cell><cell>0.131</cell><cell>0.114</cell><cell>0.101</cell><cell>0.140</cell><cell>0.148</cell><cell>0.045</cell></row><row><cell>XSUM -ROUGE-2</cell><cell>0.101</cell><cell>0.127</cell><cell>0.107</cell><cell>0.116</cell><cell>0.114</cell><cell>0.096</cell><cell>0.071</cell><cell>0.079</cell><cell>0.074</cell><cell>0.101</cell><cell>0.037</cell></row><row><cell>IMDB -EM</cell><cell>0.941</cell><cell>0.933</cell><cell>0.903</cell><cell>0.893</cell><cell>0.916</cell><cell>0.939</cell><cell>0.906</cell><cell>0.930</cell><cell>0.907</cell><cell>0.891</cell><cell>0.627</cell></row><row><cell>CivilComments -EM</cell><cell>0.667</cell><cell>0.578</cell><cell>0.525</cell><cell>0.511</cell><cell>0.536</cell><cell>0.520</cell><cell>0.516</cell><cell>0.527</cell><cell>0.520</cell><cell>0.270</cell><cell>0.490</cell></row><row><cell>RAFT -EM</cell><cell>0.682</cell><cell>0.583</cell><cell>0.618</cell><cell>0.586</cell><cell>0.611</cell><cell>0.619</cell><cell>0.498</cell><cell>0.542</cell><cell>0.466</cell><cell>0.616</cell><cell>0.368</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>LM eval harness results for RedPajama-INCITE-Base-7B-v1 and instruction tuned model. The top-scoring model for each benchmark is highlighted in bold font.</figDesc><table><row><cell></cell><cell>MPT 7B Instruct</cell><cell cols="2">Falcon 7B MPT 7B</cell><cell>RedPajama 7B Base</cell><cell>Llama 7B</cell><cell>RedPajama 7B Instruct</cell><cell>Falcon 7B Instruct</cell><cell cols="3">Dolly v2 GPT-J Pythia 7B</cell><cell>StableLM Alpha 7B</cell></row><row><cell>LM-eval-harness-AVG</cell><cell>0.7195</cell><cell>0.7161</cell><cell>0.7100</cell><cell>0.6882</cell><cell>0.6881</cell><cell>0.6858</cell><cell>0.6813</cell><cell>0.6557</cell><cell>0.6526</cell><cell>0.6392</cell><cell>0.5260</cell></row><row><cell>arc_challenge (acc_norm)</cell><cell>0.4462</cell><cell>0.4326</cell><cell>0.4215</cell><cell>0.3925</cell><cell>0.4147</cell><cell>0.4078</cell><cell>0.4283</cell><cell>0.4027</cell><cell>0.3660</cell><cell>0.3532</cell><cell>0.2705</cell></row><row><cell>arc_easy (acc)</cell><cell>0.7218</cell><cell>0.7096</cell><cell>0.7008</cell><cell>0.6923</cell><cell>0.5253</cell><cell>0.7159</cell><cell>0.6789</cell><cell>0.6423</cell><cell>0.6225</cell><cell>0.6338</cell><cell>0.4487</cell></row><row><cell>boolq (acc)</cell><cell>0.7425</cell><cell>0.7361</cell><cell>0.7486</cell><cell>0.707</cell><cell>0.7315</cell><cell>0.6865</cell><cell>0.7089</cell><cell>0.6502</cell><cell>0.6544</cell><cell>0.6446</cell><cell>0.6006</cell></row><row><cell>copa (acc)</cell><cell>0.9000</cell><cell>0.8600</cell><cell>0.8500</cell><cell>0.880</cell><cell>0.8500</cell><cell>0.850</cell><cell>0.8400</cell><cell>0.8600</cell><cell>0.8300</cell><cell>0.7400</cell><cell>0.7500</cell></row><row><cell>hellaswag (acc_norm)</cell><cell>0.7717</cell><cell>0.7634</cell><cell>0.7626</cell><cell>0.7037</cell><cell>0.7620</cell><cell>0.7103</cell><cell>0.6978</cell><cell>0.6896</cell><cell>0.6625</cell><cell>0.6588</cell><cell>0.4122</cell></row><row><cell>lambada_openai (acc)</cell><cell>0.6918</cell><cell>0.7467</cell><cell>0.7056</cell><cell>0.7143</cell><cell>0.7360</cell><cell>0.6895</cell><cell>0.6831</cell><cell>0.6893</cell><cell>0.6831</cell><cell>0.6441</cell><cell>0.6379</cell></row><row><cell>piqa (acc_norm)</cell><cell>0.8041</cell><cell>0.8069</cell><cell>0.8052</cell><cell>0.7737</cell><cell>0.7810</cell><cell>0.7699</cell><cell>0.7856</cell><cell>0.7486</cell><cell>0.7617</cell><cell>0.7671</cell><cell>0.6736</cell></row><row><cell>winogrande (acc)</cell><cell>0.6780</cell><cell>0.6732</cell><cell>0.6859</cell><cell>0.6417</cell><cell>0.7040</cell><cell>0.6567</cell><cell>0.6669</cell><cell>0.6140</cell><cell>0.6409</cell><cell>0.6267</cell><cell>0.5012</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11 :</head><label>11</label><figDesc>Quality signals originating from the CCNet pipeline<ref type="bibr" target="#b60">[61]</ref>.</figDesc><table><row><cell>Annotation Tag</cell><cell>Description</cell></row><row><cell>ccnet_bucket</cell><cell>head, middle or tail bucket of the perplexity score</cell></row><row><cell cols="2">ccnet_language_score score of the language identification model</cell></row><row><cell>ccnet_length</cell><cell>number of characters</cell></row><row><cell>ccnet_nlines</cell><cell>number of lines</cell></row><row><cell cols="2">ccnet_original_length number of characters before line-level deduplication</cell></row><row><cell cols="2">ccnet_original_nlines number of lines before line-level deduplication</cell></row><row><cell>ccnet_perplexity</cell><cell>perplexity of an LM trained on Wikipedia</cell></row><row><cell cols="2">D.2.1 Overview of Available Quality Signals</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 12 :</head><label>12</label><figDesc>Summary of quality signals which measure how much a document corresponds to natural language.</figDesc><table><row><cell>Annotation Tag</cell><cell>Description</cell><cell>Reference(s)</cell></row><row><cell>rps_doc_curly_bracket</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 18 :</head><label>18</label><figDesc>Evaluations for the 468M parameter LM for different dataset filters and other strong web datasets. The top-scoring dataset for each metric is indicated in bolded underlined, the top-2 is bolded, and the third-scoring dataset is in italics underlined.</figDesc><table><row><cell>Dataset</cell><cell>Deduplication</cell><cell></cell><cell>Rule-based</cell><cell></cell><cell>ML Heuristics</cell><cell></cell><cell cols="4">Natural Language Inference Coref. Res.</cell><cell>Sentence Completion</cell></row><row><cell></cell><cell>Exact Fuzzy</cell><cell>C4</cell><cell>Gopher</cell><cell>Classif.</cell><cell>DSIR</cell><cell>PPL</cell><cell cols="2">ANLI ARC-c</cell><cell>ARC-e</cell><cell cols="2">Winogrande Hellaswag LAMBADA</cell></row><row><cell>C4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>33.8</cell><cell>22.0</cell><cell>37.0</cell><cell>51.9</cell><cell>32.9</cell><cell>15.5</cell></row><row><cell>Dolma-v1.7 CC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>33.5</cell><cell>24.0</cell><cell>38.3</cell><cell>49.6</cell><cell>32.3</cell><cell>17.3</cell></row><row><cell>FineWeb</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>34.0</cell><cell>23.4</cell><cell>37.7</cell><cell>51.8</cell><cell>32.8</cell><cell>18.1</cell></row><row><cell>RefinedWeb</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>32.8</cell><cell>22.6</cell><cell>38.3</cell><cell>51.9</cell><cell>31.6</cell><cell>17.8</cell></row><row><cell>RPv1-CC</cell><cell></cell><cell></cell><cell></cell><cell>✔ (Wiki-Ref.)</cell><cell></cell><cell></cell><cell>33.9</cell><cell>22.4</cell><cell>37.5</cell><cell>52.6</cell><cell>29.7</cell><cell>19.0</cell></row><row><cell>RPv2 (2023-14)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>33.3</cell><cell>22.2</cell><cell>38.5</cell><cell>52.4</cell><cell>31.5</cell><cell>18.2</cell></row><row><cell>RPv2 (2023-14)</cell><cell>✔</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>33.9</cell><cell>22.1</cell><cell>38.1</cell><cell>50.6</cell><cell>31.3</cell><cell>18.0</cell></row><row><cell>RPv2 (2023-14)</cell><cell>✔</cell><cell></cell><cell>✔ (full)</cell><cell></cell><cell></cell><cell></cell><cell>34.1</cell><cell>22.3</cell><cell>38.3</cell><cell>52.2</cell><cell>32.1</cell><cell>18.7</cell></row><row><cell>RPv2 (2023-14)</cell><cell>✔</cell><cell>✔</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>33.4</cell><cell>22.7</cell><cell>38.9</cell><cell>51.1</cell><cell>32.4</cell><cell>17.5</cell></row><row><cell>RPv2 (2023-14)</cell><cell>✔</cell><cell></cell><cell>✔ (natlang)</cell><cell></cell><cell></cell><cell>Wiki-middle</cell><cell>33.4</cell><cell>24.2</cell><cell>37.7</cell><cell>49.8</cell><cell>33.1</cell><cell>19.2</cell></row><row><cell>RPv2 (2023-14)</cell><cell>✔</cell><cell></cell><cell>✔ (Rep.)</cell><cell></cell><cell></cell><cell>Wiki-middle</cell><cell>34.2</cell><cell>23.1</cell><cell>37.4</cell><cell>50.8</cell><cell>32.5</cell><cell>18.5</cell></row><row><cell>RPv2 (9 Dumps)</cell><cell>✔</cell><cell>✔</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>34.3</cell><cell>23.5</cell><cell>38.6</cell><cell>51.5</cell><cell>32.0</cell><cell>17.2</cell></row><row><cell>RPv2 (9 Dumps)</cell><cell>✔</cell><cell>✔</cell><cell>✔ (full)</cell><cell></cell><cell></cell><cell></cell><cell>33.5</cell><cell>23.3</cell><cell>38.4</cell><cell>50.2</cell><cell>32.8</cell><cell>16.8</cell></row><row><cell>RPv2 (9 Dumps)</cell><cell>✔</cell><cell>✔</cell><cell>✔ (Rep.)</cell><cell></cell><cell>✔ (Palm-mix)</cell><cell></cell><cell>33.8</cell><cell>21.9</cell><cell>38.0</cell><cell>52.5</cell><cell>32.0</cell><cell>17.3</cell></row><row><cell>RPv2 (9 Dumps)</cell><cell>✔</cell><cell>✔</cell><cell>✔ (Rep.)</cell><cell>✔ (Palm-mix)</cell><cell></cell><cell></cell><cell>34.6</cell><cell>23.3</cell><cell>38.6</cell><cell>52.2</cell><cell>32.7</cell><cell>16.4</cell></row><row><cell>RPv2 (9 Dumps)</cell><cell>✔</cell><cell>✔</cell><cell cols="2">✔ (natlang) ✔ (Palm-mix)</cell><cell></cell><cell></cell><cell>34.8</cell><cell>23.0</cell><cell>39.2</cell><cell>53.0</cell><cell>32.3</cell><cell>16.9</cell></row><row><cell>RPv2 (9 Dumps)</cell><cell>✔</cell><cell cols="3">✔ (line-filter) ✔ (natlang) ✔ (Palm-mix)</cell><cell></cell><cell></cell><cell>33.7</cell><cell>22.9</cell><cell>38.5</cell><cell>50.9</cell><cell>32.3</cell><cell>19.9</cell></row><row><cell>RPv2 (9 Dumps)</cell><cell>✔</cell><cell cols="2">custom-rules</cell><cell>✔ (Wiki-Ref.)</cell><cell></cell><cell>Pwiki &gt; 30</cell><cell>33.2</cell><cell>23.0</cell><cell>37.9</cell><cell>49.6</cell><cell>30.1</cell><cell>18.7</cell></row><row><cell>RPv2 (9 Dumps)</cell><cell>✔</cell><cell cols="3">custom-rules + Gopher-Rep ✔ (Wiki-Ref.)</cell><cell></cell><cell>Pwiki &gt; 30</cell><cell>33.0</cell><cell>23.8</cell><cell>38.9</cell><cell>50.5</cell><cell>30.0</cell><cell>18.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 19 :</head><label>19</label><figDesc>Evaluations in the 5-shot setting on MMLU and subtasks for the 468M parameter LM. The top-scoring dataset for each metric is indicated in bolded underlined, the top-2 is bolded, and the third-scoring dataset is in italics underlined.</figDesc><table><row><cell>Dataset</cell><cell>Deduplication</cell><cell></cell><cell>Rule-based</cell><cell></cell><cell>ML Heuristics</cell><cell></cell><cell cols="4">MMLU Stem Humanities Other Social Sciences</cell></row><row><cell></cell><cell>Exact Fuzzy</cell><cell>C4</cell><cell>Gopher</cell><cell>Classif.</cell><cell>DSIR</cell><cell>PPL</cell><cell></cell><cell></cell><cell></cell></row><row><cell>C4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>24.9</cell><cell>26.4</cell><cell>24.1</cell><cell>25.8</cell><cell>23.4</cell></row><row><cell>Dolma-v1.7 CC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>26.0</cell><cell>27.8</cell><cell>24.5</cell><cell>26.2</cell><cell>26.1</cell></row><row><cell>FineWeb</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>26.2</cell><cell>25.4</cell><cell>25.1</cell><cell>25.8</cell><cell>29.3</cell></row><row><cell>RefinedWeb</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>24.8</cell><cell>23.9</cell><cell>23.7</cell><cell>26.5</cell><cell>25.6</cell></row><row><cell>RPv1-CC</cell><cell></cell><cell></cell><cell></cell><cell>✔ (Wiki-Ref.)</cell><cell></cell><cell></cell><cell>25.1</cell><cell>25.1</cell><cell>23.7</cell><cell>24.0</cell><cell>28.5</cell></row><row><cell>RPv2 (2023-14)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>26.3</cell><cell>26.7</cell><cell>25.3</cell><cell>24.1</cell><cell>29.6</cell></row><row><cell>RPv2 (2023-14)</cell><cell>✔</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>26.4</cell><cell>26.8</cell><cell>25.3</cell><cell>25.2</cell><cell>28.8</cell></row><row><cell>RPv2 (2023-14)</cell><cell>✔</cell><cell></cell><cell>✔ (full)</cell><cell></cell><cell></cell><cell></cell><cell>27.0</cell><cell>28.8</cell><cell>24.8</cell><cell>25.6</cell><cell>30.0</cell></row><row><cell>RPv2 (2023-14)</cell><cell>✔</cell><cell>✔</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>25.4</cell><cell>27.8</cell><cell>24.1</cell><cell>26.1</cell><cell>24.1</cell></row><row><cell>RPv2 (2023-14)</cell><cell>✔</cell><cell></cell><cell>✔ (natlang)</cell><cell></cell><cell></cell><cell>Wiki-middle</cell><cell>26.1</cell><cell>27.4</cell><cell>25.2</cell><cell>24.6</cell><cell>27.7</cell></row><row><cell>RPv2 (2023-14)</cell><cell>✔</cell><cell></cell><cell>✔ (Rep.)</cell><cell></cell><cell></cell><cell>Wiki-middle</cell><cell>25.5</cell><cell>24.3</cell><cell>25.2</cell><cell>27.8</cell><cell>24.8</cell></row><row><cell>RPv2 (9 Dumps)</cell><cell>✔</cell><cell>✔</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>26.3</cell><cell>28.3</cell><cell>25.3</cell><cell>25.8</cell><cell>26.6</cell></row><row><cell>RPv2 (9 Dumps)</cell><cell>✔</cell><cell>✔</cell><cell>✔ (full)</cell><cell></cell><cell></cell><cell></cell><cell>25.6</cell><cell>28.0</cell><cell>25.1</cell><cell>24.9</cell><cell>24.4</cell></row><row><cell>RPv2 (9 Dumps)</cell><cell>✔</cell><cell>✔</cell><cell>✔ (Rep.)</cell><cell></cell><cell>✔ (Palm-mix)</cell><cell></cell><cell>24.4</cell><cell>26.9</cell><cell>23.7</cell><cell>24.8</cell><cell>22.7</cell></row><row><cell>RPv2 (9 Dumps)</cell><cell>✔</cell><cell>✔</cell><cell>✔ (Rep.)</cell><cell>✔ (Palm-mix)</cell><cell></cell><cell></cell><cell>24.9</cell><cell>26.1</cell><cell>24.0</cell><cell>26.3</cell><cell>23.8</cell></row><row><cell>RPv2 (9 Dumps)</cell><cell>✔</cell><cell>✔</cell><cell cols="2">✔ (natlang) ✔ (Palm-mix)</cell><cell></cell><cell></cell><cell>25.3</cell><cell>27.8</cell><cell>24.2</cell><cell>25.4</cell><cell>24.5</cell></row><row><cell>RPv2 (9 Dumps)</cell><cell>✔</cell><cell cols="3">✔ (line-filter) ✔ (natlang) ✔ (Palm-mix)</cell><cell></cell><cell></cell><cell>25.1</cell><cell>27.5</cell><cell>24.0</cell><cell>25.0</cell><cell>24.4</cell></row><row><cell>RPv2 (9 Dumps)</cell><cell>✔</cell><cell cols="2">custom-rules</cell><cell>✔ (Wiki-Ref.)</cell><cell></cell><cell>Pwiki &gt; 30</cell><cell>27.0</cell><cell>27.9</cell><cell>25.1</cell><cell>26.0</cell><cell>30.0</cell></row><row><cell>RPv2 (9 Dumps)</cell><cell>✔</cell><cell cols="3">custom-rules + Gopher-Rep ✔ (Wiki-Ref.)</cell><cell></cell><cell>Pwiki &gt; 30</cell><cell>25.9</cell><cell>25.8</cell><cell>24.3</cell><cell>27.1</cell><cell>27.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 20 :</head><label>20</label><figDesc>Evaluations on multiple choice tasks for the 468M parameter LM. The top-scoring dataset for each metric is indicated in bolded underlined, the top-2 is bolded, and the third-scoring dataset is in italics underlined.</figDesc><table><row><cell>Dataset</cell><cell>Deduplication</cell><cell></cell><cell>Rule-based</cell><cell></cell><cell>ML Heuristics</cell><cell></cell><cell cols="6">CoQA OpenbookQA PIQA PubMedQA SciQ SocialIQA TruthfulQA</cell></row><row><cell></cell><cell>Exact Fuzzy</cell><cell>C4</cell><cell>Gopher</cell><cell>Classif.</cell><cell>DSIR</cell><cell>PPL</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>C4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>3.8</cell><cell>30.2</cell><cell>64.4</cell><cell>46.0</cell><cell>51.7</cell><cell>33.4</cell><cell>33.3</cell></row><row><cell>Dolma-v1.7 CC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>5.2</cell><cell>28.2</cell><cell>65.3</cell><cell>42.6</cell><cell>55.2</cell><cell>31.6</cell><cell>33.2</cell></row><row><cell>FineWeb</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>9.0</cell><cell>29.4</cell><cell>64.5</cell><cell>41.4</cell><cell>54.3</cell><cell>32.4</cell><cell>33.5</cell></row><row><cell>RefinedWeb</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>13.2</cell><cell>28.6</cell><cell>64.4</cell><cell>52.2</cell><cell>56.4</cell><cell>32.8</cell><cell>33.3</cell></row><row><cell>RPv1-CC</cell><cell></cell><cell></cell><cell></cell><cell>✔ (Wiki-Ref.)</cell><cell></cell><cell></cell><cell>11.6</cell><cell>25.4</cell><cell>57.3</cell><cell>40.6</cell><cell>56.7</cell><cell>33.1</cell><cell>33.9</cell></row><row><cell>RPv2 (2023-14)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>12.5</cell><cell>29.2</cell><cell>61.6</cell><cell>40.8</cell><cell>53.0</cell><cell>32.9</cell><cell>31.4</cell></row><row><cell>RPv2 (2023-14)</cell><cell>✔</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>11.8</cell><cell>27.6</cell><cell>61.1</cell><cell>43.6</cell><cell>53.7</cell><cell>32.5</cell><cell>33.4</cell></row><row><cell>RPv2 (2023-14)</cell><cell>✔</cell><cell></cell><cell>✔ (full)</cell><cell></cell><cell></cell><cell></cell><cell>11.3</cell><cell>28.8</cell><cell>62.8</cell><cell>51.0</cell><cell>53.9</cell><cell>32.6</cell><cell>32.6</cell></row><row><cell>RPv2 (2023-14)</cell><cell>✔</cell><cell>✔</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>5.8</cell><cell>28.8</cell><cell>63.4</cell><cell>49.6</cell><cell>54.7</cell><cell>36.6</cell><cell>33.8</cell></row><row><cell>RPv2 (2023-14)</cell><cell>✔</cell><cell></cell><cell>✔ (natlang)</cell><cell></cell><cell></cell><cell>Wiki-middle</cell><cell>11.3</cell><cell>28.4</cell><cell>63.5</cell><cell>49.6</cell><cell>53.6</cell><cell>32.8</cell><cell>33.4</cell></row><row><cell>RPv2 (2023-14)</cell><cell>✔</cell><cell></cell><cell>✔ (Rep.)</cell><cell></cell><cell></cell><cell>Wiki-middle</cell><cell>11.9</cell><cell>29.4</cell><cell>63.1</cell><cell>52.6</cell><cell>53.4</cell><cell>32.5</cell><cell>31.6</cell></row><row><cell>RPv2 (9 Dumps)</cell><cell>✔</cell><cell>✔</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>6.6</cell><cell>29.0</cell><cell>62.0</cell><cell>36.2</cell><cell>53.7</cell><cell>33.2</cell><cell>34.3</cell></row><row><cell>RPv2 (9 Dumps)</cell><cell>✔</cell><cell>✔</cell><cell>✔ (full)</cell><cell></cell><cell></cell><cell></cell><cell>5.8</cell><cell>28.6</cell><cell>62.8</cell><cell>51.2</cell><cell>54.8</cell><cell>34.4</cell><cell>31.2</cell></row><row><cell>RPv2 (9 Dumps)</cell><cell>✔</cell><cell>✔</cell><cell>✔ (Rep.)</cell><cell></cell><cell>✔ (Palm-mix)</cell><cell></cell><cell>6.0</cell><cell>29.4</cell><cell>61.6</cell><cell>45.4</cell><cell>52.2</cell><cell>33.4</cell><cell>33.1</cell></row><row><cell>RPv2 (9 Dumps)</cell><cell>✔</cell><cell>✔</cell><cell>✔ (Rep.)</cell><cell>✔ (Palm-mix)</cell><cell></cell><cell></cell><cell>5.4</cell><cell>29.4</cell><cell>62.5</cell><cell>45.0</cell><cell>51.7</cell><cell>34.0</cell><cell>33.7</cell></row><row><cell>RPv2 (9 Dumps)</cell><cell>✔</cell><cell>✔</cell><cell cols="2">✔ (natlang) ✔ (Palm-mix)</cell><cell></cell><cell></cell><cell>4.9</cell><cell>28.0</cell><cell>62.9</cell><cell>52.8</cell><cell>52.0</cell><cell>33.0</cell><cell>33.6</cell></row><row><cell>RPv2 (9 Dumps)</cell><cell>✔</cell><cell cols="3">✔ (line-filter) ✔ (natlang) ✔ (Palm-mix)</cell><cell></cell><cell></cell><cell>6.4</cell><cell>27.0</cell><cell>63.2</cell><cell>47.8</cell><cell>52.9</cell><cell>32.8</cell><cell>32.0</cell></row><row><cell>RPv2 (9 Dumps)</cell><cell>✔</cell><cell cols="2">custom-rules</cell><cell>✔ (Wiki-Ref.)</cell><cell></cell><cell>Pwiki &gt; 30</cell><cell>10.0</cell><cell>27.8</cell><cell>59.6</cell><cell>41.2</cell><cell>55.8</cell><cell>33.3</cell><cell>32.0</cell></row><row><cell>RPv2 (9 Dumps)</cell><cell>✔</cell><cell cols="3">custom-rules + Gopher-Rep ✔ (Wiki-Ref.)</cell><cell></cell><cell>Pwiki &gt; 30</cell><cell>9.3</cell><cell>28.0</cell><cell>59.2</cell><cell>43.4</cell><cell>54.9</cell><cell>33.0</cell><cell>33.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 21 :</head><label>21</label><figDesc>Downstream task accuracy for a 1.6B LM trained on different datasets over 350B tokens. Evaluations for the 1.6B Parameter Models Tables 21, 22, and 23 show results for ablations with the 1.6B models. Each model was trained on 350B tokens.</figDesc><table><row><cell>Dataset</cell><cell>Fuzzy</cell><cell cols="2">Rule-based</cell><cell>ML Heuristics</cell><cell cols="4">Natural Language Inference Coref. Res.</cell><cell cols="2">Sentence Completion</cell></row><row><cell></cell><cell>Deduplication</cell><cell>C4</cell><cell>Gopher</cell><cell></cell><cell cols="2">ANLI ARC-c</cell><cell>ARC-e</cell><cell cols="3">Winogrande Hellaswag LAMBADA</cell></row><row><cell>RefinedWeb</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>33.6</cell><cell>26.9</cell><cell>51.7</cell><cell>54.4</cell><cell>55.8</cell><cell>47.9</cell></row><row><cell>RPv2 (full)</cell><cell>✔</cell><cell></cell><cell>✔</cell><cell>WikiRef</cell><cell>32.4</cell><cell>27.9</cell><cell>51.3</cell><cell>56.4</cell><cell>47.4</cell><cell>47.4</cell></row><row><cell>RPv2 (full)</cell><cell>✔</cell><cell cols="2">✔ ✔(natlang)</cell><cell>Palm-Mix</cell><cell>33.6</cell><cell>28.7</cell><cell>52.4</cell><cell>54.5</cell><cell>53.1</cell><cell>42.9</cell></row><row><cell>D.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 22 :</head><label>22</label><figDesc>Evaluations in the 5-shot setting on MMLU and subtasks for the 1.6B parameter LM.</figDesc><table><row><cell>Dataset</cell><cell>Fuzzy</cell><cell cols="2">Rule-based</cell><cell>ML Heuristics</cell><cell></cell><cell></cell><cell>MMLU</cell><cell></cell></row><row><cell></cell><cell>Deduplication</cell><cell>C4</cell><cell>Gopher</cell><cell></cell><cell cols="5">MMLU Stem Humanities Other Social Sciences</cell></row><row><cell>RefinedWeb</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>25.3</cell><cell>24.9</cell><cell>24.9</cell><cell>27.0</cell><cell>24.7</cell></row><row><cell>RPv2 (full)</cell><cell>✔</cell><cell></cell><cell>✔</cell><cell>WikiRef</cell><cell>25.2</cell><cell>26.0</cell><cell>26.7</cell><cell>23.9</cell><cell>23.3</cell></row><row><cell>RPv2 (full)</cell><cell>✔</cell><cell cols="2">✔ ✔(natlang)</cell><cell>Palm-Mix</cell><cell>24.7</cell><cell>25.7</cell><cell>25.4</cell><cell>23.8</cell><cell>23.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 23 :</head><label>23</label><figDesc>Evaluations on multiple choice tasks for the 1.6B parameter LM.</figDesc><table><row><cell>Dataset</cell><cell>Fuzzy</cell><cell cols="2">Rule-based</cell><cell cols="8">ML Heuristics CoQA OpenbookQA PIQA PubMedQA SciQ SocialIQA TruthfulQA</cell></row><row><cell></cell><cell>Deduplication</cell><cell>C4</cell><cell>Gopher</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RefinedWeb</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>47.4</cell><cell>31.6</cell><cell>73.8</cell><cell>57.0</cell><cell>75.3</cell><cell>41.0</cell><cell>36.6</cell></row><row><cell>RPv2 (full)</cell><cell>✔</cell><cell></cell><cell>✔</cell><cell>WikiRef</cell><cell>43.7</cell><cell>32.6</cell><cell>67.4</cell><cell>55.6</cell><cell>72.7</cell><cell>40.4</cell><cell>36.9</cell></row><row><cell>RPv2 (full)</cell><cell>✔</cell><cell cols="2">✔ ✔(natlang)</cell><cell>Palm-Mix</cell><cell>22.1</cell><cell>32.2</cell><cell>71.3</cell><cell>55.2</cell><cell>71.0</cell><cell>42.2</cell><cell>35.7</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The code to reproduce RedPajama is available at github.com/togethercomputer/RedPajama-Data.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://huggingface.co/datasets/allenai/c4</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://commoncrawl.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>https://dsi.ut-capitole.fr/blacklists/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>We remark that exact deduplication was performed based on the hashes of the .wet documents, i.e., prior to processing the data with CCNet.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>https://github.com/allenai/OLMo</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7"><p>https://github.com/togethercomputer/RedPajama-Data</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8"><p>https://commoncrawl.org/terms-of-use</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments and Disclosure of Funding</head><p>We acknowledge support from the <rs type="funder">Canada CIFAR AI Chair Program</rs> [I.R.], and the <rs type="funder">Canada Excellence Research Chairs Program</rs> [I.R.]. This research was made possible thanks to the computing resources on the Summit supercomputer, provided as a part of the <rs type="programName">INCITE 2023 program</rs> award "<rs type="projectName">Scalable Foundation Models for Transferable Generalist AI</rs>". These resources were provided by the <rs type="institution" subtype="infrastructure">Oak Ridge Leadership Computing Facility</rs> at the <rs type="institution">Oak Ridge National Laboratory</rs>, which is supported by the <rs type="funder">Office of Science of the U.S. Department of Energy</rs> under Contract No. <rs type="grantNumber">DE-AC05-00OR22725</rs>. We gratefully acknowledge the support of <rs type="funder">NIH</rs> under No. <rs type="grantNumber">U54EB020405</rs> (Mobilize), <rs type="funder">NSF</rs> under Nos. <rs type="grantNumber">CCF2247015</rs> (<rs type="affiliation">Hardware-Aware</rs>), <rs type="grantNumber">CCF1763315</rs> (<rs type="affiliation">Beyond Sparsity</rs>), CCF1563078 (Volume to Velocity), and 1937301 (RTML); US DEVCOM ARL under Nos. <rs type="grantNumber">W911NF-23-2-0184</rs> (Long-context) and <rs type="grantNumber">W911NF-21-2-0251</rs> (<rs type="projectName">Interactive Human-AI Teaming</rs>); ONR under Nos. <rs type="grantNumber">N000142312633</rs> (<rs type="affiliation">Deep Signal Processing</rs>); <rs type="funder">Stanford HAI</rs> under No. <rs type="grantNumber">247183</rs>; <rs type="affiliation">NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF</rs>, <rs type="funder">Accenture</rs>, <rs type="funder">Ericsson</rs>, <rs type="funder">Qualcomm</rs>, <rs type="funder">Analog Devices</rs>, <rs type="person">Google Cloud</rs>, <rs type="funder">Salesforce</rs>, <rs type="funder">Total</rs>, the <rs type="institution">HAI-GCP Cloud Credits for Research program</rs>, the <rs type="funder">Stanford Data Science Initiative (SDSI)</rs>, and members of the Stanford DAWN project: <rs type="funder">Meta</rs>, <rs type="person">Google</rs>, and <rs type="funder">VMWare</rs>. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of <rs type="funder">NIH</rs>, <rs type="institution">ONR</rs>, or the <rs type="institution">U.S. Government</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_dGquPF7">
					<orgName type="project" subtype="full">Scalable Foundation Models for Transferable Generalist AI</orgName>
					<orgName type="program" subtype="full">INCITE 2023 program</orgName>
				</org>
				<org type="funding" xml:id="_AvCtwG4">
					<idno type="grant-number">DE-AC05-00OR22725</idno>
				</org>
				<org type="funding" xml:id="_8YRYZhj">
					<idno type="grant-number">U54EB020405</idno>
				</org>
				<org type="funding" xml:id="_E33quR5">
					<idno type="grant-number">CCF2247015</idno>
				</org>
				<org type="funding" xml:id="_xSnkH2h">
					<idno type="grant-number">CCF1763315</idno>
				</org>
				<org type="funding" xml:id="_nfJnZS5">
					<idno type="grant-number">W911NF-23-2-0184</idno>
				</org>
				<org type="funded-project" xml:id="_DJFKMsC">
					<idno type="grant-number">W911NF-21-2-0251</idno>
					<orgName type="project" subtype="full">Interactive Human-AI Teaming</orgName>
				</org>
				<org type="funding" xml:id="_YUGBQUf">
					<idno type="grant-number">N000142312633</idno>
				</org>
				<org type="funding" xml:id="_nYF67rw">
					<idno type="grant-number">247183</idno>
				</org>
			</listOrg>

			<listOrg type="infrastructure">
				<org type="infrastructure">					<orgName type="extracted">Oak Ridge Leadership Computing Facility</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Intended Uses</head><p>The RedPajama datasets were created with the primary use case of providing training data for large language models. RedPajama contains data from different sources and domains. RedPajama-V1 contains data obtained from web scrapes, Wikipedia articles, scientific content extracted from articles available on arXiv, as well as code from various programming languages. RedPajama-V2 contains data exclusively based on web scrapes and is accompanied by a series of quality signals that are intended to be used for filtering the raw dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Dataset Accessibility</head><p>Both RedPajama-V1 and RedPajama-V2 are available for download via the Huggingface Hub at <ref type="url" target="https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T">https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T</ref> and <ref type="url" target="https://huggingface.co/datasets/togethercomputer/RedPajama-Data-V2">https: //huggingface.co/datasets/togethercomputer/RedPajama-Data-V2</ref>.</p><p>Access via public HTTP endpoints. We also provide access to the datasets via public HTTPS endpoints. The list of urls for components of the RedPajama-V1 dataset can be obtained from <ref type="url" target="https://data.together.xyz/redpajama-data-1T/v1.0.0/urls.txt">https://data.together.xyz/redpajama-data-1T/v1.0.0/urls.txt</ref>. The list of urls for the different components of RedPajama-V2 can be obtained from the following urls: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. } }</head><p>The meta field varies between the different sources:</p><p>• The Arxiv subset has the meta fields timestamp, yymm, arxiv_id, language and url.</p><p>• The C4 subset has the meta fields timestamp, source, language and url.</p><p>• The Github subset has the meta fields content_hash, timestamp, source, line_count, max_line_length, avg_line_length, alnum_prop, repo_name, id, size, binary, copies, ref, path, mode, license, language. • The Stack Exchange subset has the meta fields timestamp, source, language, question_score and url. • The Wikipedia subset has the meta fields timestamp, title, language, and url.</p><p>The Common Crawl subset follows the structure { " text " : "..." , and a model trained on the source domain q, This is the logarithm of the ratio p/q.</p><p>[62] rps_doc_openwebtext_importance Given a bag of 1,2-wordgram model trained on OpenWebText p, and a model trained on the source domain q, this is the logarithm of the ratio p/q.</p><p>[62]</p><p>rps_doc_wikipedia_importance Given a bag of 1,2-wordgram model trained on Wikipedia articles p, and a model trained on the source domain q, this is the logarithm of the ratio p/q.</p><p>[62] rps_doc_ml_wikiref_score Fasttext classifier prediction for the document being a Wikipedia reference. This is the same fasttext model used in the RedPajama-1T dataset.</p><p>Only applies to English data.</p><p>[57] rps_doc_ml_palm_score Fasttext classifier prediction for the document being a Wikipedia article, OpenWebText sample or a RedPajama-V1 book.</p><p>Only for English data.</p><p>[12], <ref type="bibr" target="#b15">[16]</ref> rps_doc_ml_wikipedia_score Fasttext classifier prediction for the document being a Wikipedia article. This is used for non-English data - rps_doc_frac_chars_dupe_10grams The fraction of characters in duplicate word 10grams. <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b44">45]</ref> rps_doc_frac_chars_dupe_5grams</p><p>The fraction of characters in duplicate word 5grams. <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b44">45]</ref> rps_doc_frac_chars_dupe_6grams</p><p>The fraction of characters in duplicate word 6grams. <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b44">45]</ref> rps_doc_frac_chars_dupe_7grams</p><p>The fraction of characters in duplicate word 7grams. <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b44">45]</ref> rps_doc_frac_chars_dupe_8grams</p><p>The fraction of characters in duplicate word 8grams. <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b44">45]</ref> rps_doc_frac_chars_dupe_9grams</p><p>The fraction of characters in duplicate word 9grams. <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b44">45]</ref> rps_doc_frac_chars_top_2gram</p><p>The fraction of characters in the top word 2gram. <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b44">45]</ref> rps_doc_frac_chars_top_3gram</p><p>The fraction of characters in the top word 3gram. <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b44">45]</ref> rps_doc_frac_chars_top_4gram</p><p>The fraction of characters in the top word 4gram. <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b44">45]</ref>  The number of sequences of words that are contained in the List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words blocklist.</p><p>The blocklist is obtained from <ref type="url" target="https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words">https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words</ref>.</p><p>[46] rps_doc_ut1_blacklist A categorical id corresponding to the list of categories of the domain of the document. Categories are obtained from <ref type="url" target="https://dsi.ut-capitole.fr/blacklists/">https://dsi.ut-capitole.fr/blacklists/</ref>  <ref type="bibr" target="#b43">[44]</ref> Table <ref type="table">16</ref>: Examples of documents and corresponding cluster topics from Nomic Atlas <ref type="bibr" target="#b40">[41]</ref>.</p><p>Cluster Topics Document (broad -medium -specific) Election -Health (2) -COVID Testing immediately moving to the Purple Tier. This is the most restrictive level in the State's effort to control the spread of COVID-19. Businesses and residents must comply with the Purple Tier restrictions by Tuesday, Nov. 17. To determine restrictions by industry, business and activity, visit: <ref type="url" target="https://covid19.ca.gov/safer-economy/">https://covid19.ca.gov/safer-economy/</ref> Read the full news release here:</p><p><ref type="url" target="www.gov.ca.gov/2020/11/16/governor-newsom-announces-new-">www.gov.ca.gov/2020/11/16/governor-newsom-announces-new-</ref>immediate-actions-to-curb-covid-19-transmission/ Watch the Governor's press conference during which he made the announcement today here:</p><p><ref type="url" target="www.facebook.com/CAgovernor/videos/376746553637721">www.facebook.com/CAgovernor/videos/376746553637721</ref> According to County of Orange officials, schools that have not already opened must continue with remote classes and cannot reopen in-person.</p><p>Read the County's release here: <ref type="url" target="https://cms.ocgov.com/civicax/filebank/blobdload.aspx?BlobID=118441">https://cms.ocgov.com/civicax/filebank/blobdload.aspx?BlobID=118441</ref> </p><p>The California Department of Public Health has also issued a travel advisory encouraging Californians to stay home or in their region and avoid non-esse Religion/Spirituality -Gaming -Gaming (3) Top 100 Employers, and one of Canada's Top Employers for Young People multiple years running! At Ubisoft Toronto, we look for people who are excited to create the future of games in one of the most diverse cities in the world. We believe that embracing our differences helps us build stronger creative teams and develop better games for all players. We are an equal-opportunity employer and welcome applications from all interested candidates. We strongly encourage applications from Indigenous people, racialized people, neurodivergent people, people with disabilities, people from gender and sexually diverse communities and/or people with intersectional identities. We are committed to providing reasonable accommodation for people with disability upon request. If this sounds like your kind of studio, what are you waiting for? Apply to join us now! We thank you for your interest, however, only those candidates selected for an interview will be contacted. No agencies please. Senior Game Design Education -Golf -Rotary Meetings what's happening. Conversely, some people rely on the newsletter. Thus, the more avenues to inform people, the better. attendance at many social functions is poor, possibly due to the limited advertising reach. In practical terms, it means that social functions may be advertised in the OOC newsletter (current practice) the schedule, as is done for outdoor activities such as hikes the OOC's Facebook group As when social functions are advertised in the newsletter, the person organizing the social function can choose how much location information to provide, especially if it is to be held at someone's residence. OOC bylaw Article 3, Section 9 (f) states (highlighting added) (f) Social Coordinator: Shall be responsible for coordinating all social events for Club members only, and for preparing a schedule of these outings, not to be advertised to non-members. The executive voted to amend this statement by removing the limitation per Paragraph 3 of "Article 5 -Amending Formula" of the Const</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Josh</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lama</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilge</forename><surname>Akkaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florencia</forename><surname>Leoni Aleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janko</forename><surname>Altenschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Altman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.08774</idno>
		<title level="m">Shyamal Anadkat, et al. Gpt-4 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The falcon series of open language models</title>
		<author>
			<persName><forename type="first">Ebtesam</forename><surname>Almazrouei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamza</forename><surname>Alobeidli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdulaziz</forename><surname>Alshamsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Cappelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruxandra</forename><surname>Cojocaru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mérouane</forename><surname>Debbah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Étienne</forename><surname>Goffinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Hesslow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Launay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Malartic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.16867</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Jinze</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfei</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbin</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.16609</idno>
		<title level="m">Qwen technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pythia: A suite for analyzing large language models across training and scaling</title>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailey</forename><surname>Schoelkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><forename type="middle">Gregory</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herbie</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Kyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Brien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Hallahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivanshu</forename><surname>Aflah Khan</surname></persName>
		</author>
		<author>
			<persName><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Usvsn Sai Prashanth</surname></persName>
		</author>
		<author>
			<persName><surname>Raff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2397" to="2430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Piqa: Reasoning about physical commonsense in natural language</title>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fourth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hallahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Leahy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Mcdonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.06745</idno>
		<title level="m">Gpt-neox-20b: An open-source autoregressive language model</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<author>
			<persName><forename type="first">H</forename><surname>Burton</surname></persName>
		</author>
		<author>
			<persName><surname>Bloom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Space/time trade-offs in hash coding with allowable errors</title>
		<imprint>
			<date type="published" when="1970">1970</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="422" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Klyman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sayash</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nestor</forename><surname>Maslej</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Betty</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.12941</idno>
		<title level="m">The foundation model transparency index</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Holistic evaluation of language models</title>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of the New York Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">1525</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="140" to="146" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On the resemblance and containment of documents</title>
		<author>
			<persName><forename type="first">Andrei</forename><forename type="middle">Z</forename><surname>Broder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. Compression and Complexity of SEQUENCES 1997 (Cat. No. 97TB100171)</title>
		<meeting>Compression and Complexity of SEQUENCES 1997 (Cat. No. 97TB100171)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Palm: Scaling language modeling with pathways</title>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">240</biblScope>
			<biblScope unit="page" from="1" to="113" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Think you have solved question answering? try arc, the ai2 reasoning challenge</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isaac</forename><surname>Cowhey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carissa</forename><surname>Schoenick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<idno>ArXiv, abs/1803.05457</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Documenting large webtext corpora: A case study on the colossal clean crawled corpus</title>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ana</forename><surname>Marasović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Agnew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Groeneveld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08758</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Glam: Efficient scaling of language models with mixture-of-experts</title>
		<author>
			<persName><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5547" to="5569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Travis</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anish</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noa</forename><surname>Nabeshima</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00027</idno>
		<title level="m">The pile: An 800gb dataset of diverse text for language modeling</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A framework for few-shot language model evaluation</title>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Tow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Baber Abbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sid</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Dipofi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alain</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haonan</forename><surname>Le Noac'h</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Mcdonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Ociepa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laria</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailey</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aviya</forename><surname>Schoelkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lintang</forename><surname>Skowron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anish</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Zou</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="7" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Groeneveld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pete</forename><surname>Walsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshita</forename><surname>Bhagia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodney</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananya</forename><surname>Harsh Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamish</forename><surname>Ivison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Magnusson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.00838</idno>
		<title level="m">Accelerating the science of language models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Measuring massive multitask language understanding</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Collin</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The curious case of neural text degeneration</title>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09751</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Albert Q Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devendra</forename><surname>Bamford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Singh Chaplot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>De Las Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gianna</forename><surname>Bressand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lengyel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucile</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><surname>Saulnier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.06825</idno>
		<title level="m">Mistral 7b</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pubmedqa: A dataset for biomedical research question answering</title>
		<author>
			<persName><forename type="first">Qiao</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinghua</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2567" to="2577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-04">April 2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="427" to="431" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Leandro von Werra, and Harm de Vries. The stack: 3 tb of permissively licensed source code</title>
		<author>
			<persName><forename type="first">Denis</forename><surname>Kocetkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loubna</forename><surname>Ben Allal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenghao</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Muñoz Ferrandis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The bigscience roots corpus: A 1.6 tb composite multilingual dataset</title>
		<author>
			<persName><forename type="first">Lucile</forename><surname>Hugo Laurençon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Saulnier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Akiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Villanova Del Moral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leandro</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenghao</forename><surname>Von Werra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduardo</forename><forename type="middle">González</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huu</forename><surname>Ponferrada</surname></persName>
		</author>
		<author>
			<persName><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="31809" to="31826" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Le</forename><surname>Teven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellie</forename><surname>Akiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suzana</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ilić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Hesslow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Castagné</surname></persName>
		</author>
		<author>
			<persName><forename type="first">François</forename><surname>Sasha Luccioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Yvon</surname></persName>
		</author>
		<author>
			<persName><surname>Gallé</surname></persName>
		</author>
		<title level="m">A 176b-parameter open-access multilingual language model</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Ippolito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Nystrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.06499</idno>
		<title level="m">Deduplicating training data makes language models better</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Solving quantitative reasoning problems with language models</title>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Lewkowycz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>Andreassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henryk</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinay</forename><surname>Ramasesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ambrose</forename><surname>Slone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cem</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Imanol</forename><surname>Schlag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theo</forename><surname>Gutman-Solo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="3843" to="3857" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Datacomp-lm: In search of the next generation of training sets for language models</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Smyrnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maor</forename><surname>Ivgi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samir</forename><surname>Gadre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hritik</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etash</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sedrick</forename><surname>Keh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kushal</forename><surname>Arora</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.11794</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Raymond</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loubna</forename><surname>Ben Allal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangtian</forename><surname>Zi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Kocetkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenghao</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Marone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Akiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenny</forename><surname>Chim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.06161</idno>
		<title level="m">Starcoder: may the source be with you! arXiv preprint</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Zehan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanzhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingkun</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengjun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.03281</idno>
		<title level="m">Towards general text embeddings with multi-stage contrastive learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Owain</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><surname>Truthfulqa</surname></persName>
		</author>
		<title level="m">Measuring how models mimic human falsehoods</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">A pretrainer&apos;s guide to training data: Measuring the effects of data age, domain coverage, quality, &amp; toxicity</title>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Yauney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Reif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.13169</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Ian</forename><surname>Magnusson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshita</forename><surname>Bhagia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Soldaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananya</forename><surname>Harsh Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><forename type="middle">Pete</forename><surname>Walsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanai</forename><surname>Elazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.10523</idno>
		<title level="m">A benchmark for evaluating language model fit</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Openelm: An efficient language model family with open-source training and inference framework</title>
		<author>
			<persName><forename type="first">Sachin</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Hossein Sekhavat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingqing</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Horton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanzi</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenfan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iman</forename><surname>Mirzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahyar</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Belenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Zatloukal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.14619</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Paulius</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonah</forename><surname>Alben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksii</forename><surname>Kuchaiev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.03740</idno>
		<title level="m">Ganesh Venkatesh, et al. Mixed precision training</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Can a suit of armor conduct electricity? a new dataset for open book question answering</title>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Cross-task generalization via natural language crowdsourcing instructions</title>
		<author>
			<persName><forename type="first">Swaroop</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chitta</forename><surname>Baral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Adversarial nli: A new benchmark for natural language understanding</title>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Structure unstructured data: Interact, discover insights and build with unstructured text, image and audio data</title>
		<author>
			<persName><surname>Nomic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="2024" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The LAMBADA dataset: Word prediction requiring a broad discourse context</title>
		<author>
			<persName><forename type="first">Denis</forename><surname>Paperno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ngoc</forename><forename type="middle">Quan</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandro</forename><surname>Pezzelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gemma</forename><surname>Boleda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Fernandez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-08">August 2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1525" to="1534" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Guilherme</forename><surname>Penedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hynek</forename><surname>Kydlíček</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Lozhkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leandro</forename><surname>Von Werra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.17557</idno>
		<title level="m">The fineweb datasets: Decanting the web for the finest text data at scale</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The refinedweb dataset for falcon llm: Outperforming curated corpora with web data only</title>
		<author>
			<persName><forename type="first">Guilherme</forename><surname>Penedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Malartic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Hesslow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruxandra</forename><surname>Cojocaru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamza</forename><surname>Alobeidli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Cappelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Pannier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ebtesam</forename><surname>Almazrouei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Launay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Jack W Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Aslanides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susannah</forename><surname>Ring</surname></persName>
		</author>
		<author>
			<persName><surname>Young</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.11446</idno>
		<title level="m">Scaling language models: Methods, analysis &amp; insights from training gopher</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Coqa: A conversational question answering challenge</title>
		<author>
			<persName><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="249" to="266" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Winogrande: an adversarial winograd schema challenge at scale</title>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="99" to="106" />
			<date type="published" when="2021-08">aug 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Multitask prompted training enables zero-shot task generalization</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">H</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lintang</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaid</forename><surname>Alyafeai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Chaffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnaud</forename><surname>Stiegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Raja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manan</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saiful Bari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Urmish</forename><surname>Thakker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanya</forename><surname>Sharma Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Szczechla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taewoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunjan</forename><surname>Chhablani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nihal</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debajyoti</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Tian-Jian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Manica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng Xin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harshit</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Bawden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
		<editor>Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
			<pubPlace>Jason Alan Fries, Ryan Teehan, Stella Biderman, Leo Gao, Tali Bers, Thomas Wolf</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Lebras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><surname>Socialiqa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09728</idno>
		<title level="m">Commonsense reasoning about social interactions</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianhua</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Willie</forename><surname>Neiswanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Hestness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Vassilieva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daria</forename><surname>Soboleva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.10818</idno>
		<title level="m">Slimpajama-dc: Understanding data combinations for llm training</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">Luca</forename><surname>Soldaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodney</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshita</forename><surname>Bhagia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><surname>Authur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Bogin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khyathi</forename><surname>Chandu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Dumas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanai</forename><surname>Elazar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.00159</idno>
		<title level="m">An open corpus of three trillion tokens for language model pretraining research</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Introducing mpt-7b: A new standard for open-source, commercially usable llms</title>
		<author>
			<persName><forename type="first">Nlp</forename><surname>Mosaicml</surname></persName>
		</author>
		<author>
			<persName><surname>Team</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2023" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Snowflake arctic: The best llm for enterprise ai -efficiently intelligent, truly open</title>
		<author>
			<persName><forename type="first">Ai</forename><forename type="middle">Research</forename><surname>Snowflake</surname></persName>
		</author>
		<author>
			<persName><surname>Team</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2024" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Improving llm pretraining via document de-duplication and diversification</title>
		<author>
			<persName><forename type="first">Kushal</forename><surname>Tirumala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Simig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Morcos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Zyda: A 1.3t dataset for open language modeling</title>
		<author>
			<persName><forename type="first">Yury</forename><surname>Tokpanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beren</forename><surname>Millidge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Glorioso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Pilault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Whittington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Anthony</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothée</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Rozière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Azhar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13971</idno>
		<title level="m">Open and efficient foundation language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Llama 2: Open foundation and fine-tuned chat models</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasmine</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prajjwal</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.09288</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aran</forename><surname>Komatsuzaki</surname></persName>
		</author>
		<ptr target="https://github.com/kingoflolz/mesh-transformer-jax" />
		<imprint>
			<date type="published" when="2021-05">May 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Crowdsourcing multiple choice science questions</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nelson</forename><forename type="middle">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06209</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><surname>Ccnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.00359</idno>
		<title level="m">Extracting high quality monolingual datasets from web crawl data</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Data selection for language models via importance resampling</title>
		<author>
			<persName><forename type="first">Sang</forename><surname>Michael Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><forename type="middle">S</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="34201" to="34227" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">mT5: A massively multilingual pre-trained text-to-text transformer</title>
		<author>
			<persName><forename type="first">Linting</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihir</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Barua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-06">June 2021</date>
			<biblScope unit="page" from="483" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Hellaswag: Can a machine really finish your sentence?</title>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<author>
			<persName><forename type="first">Ge</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenchen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenghua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leuang</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danny</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esther</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qunshu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.19327</idno>
		<title level="m">Map-neo: Highly capable and transparent bilingual large language model series</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Pytorch fsdp: experiences on scaling fully sharded data parallel</title>
		<author>
			<persName><forename type="first">Yanli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chien-Chin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Less</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamid</forename><surname>Shojanazeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.11277</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
