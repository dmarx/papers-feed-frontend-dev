<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
							<email>samyamr@microsoft.com</email>
						</author>
						<author>
							<persName><forename type="first">Olatunji</forename><surname>Ruwase</surname></persName>
							<email>olruwase@microsoft.com</email>
						</author>
						<author>
							<persName><forename type="first">Jeff</forename><surname>Rasley</surname></persName>
							<email>jerasley@microsoft.com</email>
						</author>
						<author>
							<persName><forename type="first">Shaden</forename><surname>Smith</surname></persName>
							<email>shsmit@microsoft.com</email>
						</author>
						<author>
							<persName><forename type="first">Yuxiong</forename><surname>He</surname></persName>
						</author>
						<title level="a" type="main">ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">49161FCF5A17A2BA79BB4B428B39F9C1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the last three years, the largest dense deep learning models have grown over 1000x to reach hundreds of billions of parameters, while the GPU memory has only grown by 5x (16 GB to 80 GB). Therefore, the growth in model scale has been supported primarily though system innovations that allow large models to fit in the aggregate GPU memory of multiple GPUs. However, we are getting close to the GPU memory wall. It requires 800 NVIDIA V100 GPUs just to fit a trillion parameter model for training, and such clusters are simply out of reach for most data scientists. In addition, training models at that scale requires complex combinations of parallelism techniques that puts a big burden on the data scientists to refactor their model.</p><p>In this paper we present ZeRO-Infinity, a novel heterogeneous system technology that leverages GPU, CPU, and NVMe memory to allow for unprecedented model scale on limited resources without requiring model code refactoring. At the same time it achieves excellent training throughput and scalability, unencumbered by the limited CPU or NVMe bandwidth. ZeRO-Infinity can fit models with tens and even hundreds of trillions of parameters for training on current generation GPU clusters. It can be used to fine-tune trillion parameter models on a single NVIDIA DGX-2 node, making large models more accessible. In terms of training throughput and scalability, it sustains over 25 petaflops on 512 NVIDIA V100 GPUs (40% of peak), while also demonstrating super linear scalability. An open source implementation of ZeRO-Infinity is available through DeepSpeed 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">EXTENDED INTRODUCTION</head><p>Deep learning (DL) has made tremendous advances in recent years, allowing it to become an integral part of our lives from powering our search engines to our smart home virtual assistants. Increased model size is at the center of these advancements <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>, and multiple studies have shown that this trend will continue <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. As a result, there has been significant investment in training huge models.</p><p>In the last three years, the largest trained dense model in deep learning has grown over 1000x, from one hundred million parameters (ELMo <ref type="bibr" target="#b5">[6]</ref>) to over one hundred billion parameters (GPT-3 <ref type="bibr" target="#b3">[4]</ref>). In comparison, the single GPU memory has increased by a meager 5x (16 GB to 80 GB). Therefore, the growth in model size has been made possible mainly through advances in system technology for training large DL models, with parallelism technologies such as model parallelism <ref type="bibr" target="#b6">[7]</ref>, pipeline parallelism <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref>, and ZeRO <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> creating a path to training larger and more powerful models.</p><p>The current state-of-the-art in large model training technology is three-dimensional parallelism (3D parallelism <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>), which combines model (tensor-slicing) and pipeline parallelism with data parallelism to efficiently scale DL training to trillions of parameters on hundreds or thousands of GPUs. For example, the DeepSpeed implementation of 3D parallelism can scale to over a trillion parameters on 800 NVIDIA V100 GPUs by fully leveraging the aggregate GPU memory of a cluster <ref type="bibr" target="#b14">[15]</ref>.</p><p>Despite the capabilities of 3D parallelism for large model training, we are now arriving at the GPU memory wall <ref type="bibr" target="#b15">[16]</ref>. The aggregate GPU memory is simply not large enough to support the growth in model size. Even with the newest NVIDIA A100 GPUs with 80 GB of memory, 3D parallelism requires 320 GPUs just to fit a trillionparameter model for training, and scaling to a hundred trillion parameter model of the future would require over 6K GPUs even if we assume a 5x increase in GPU memory in the next few years. We can no longer sustain the continuous growth in the model scale with GPU memory as the bottleneck.</p><p>The GPU memory wall also limits data scientists from accessing even the large models of today, especially for fine tuning. Large models are first pretrained on large amounts of generic data, and through fine tuning the same model can be specialized for a wide variety of applications. While pretraining a model with hundreds of billions of parameters can require millions of GPU compute hours, fine-tuning it is much cheaper, requiring significantly fewer GPU compute hours, and could be done on a single compute node with a handful of GPUs. While such compute resources are accessible to many businesses and users, they are unfortunately restricted by the memory available on these compute nodes, which in turn limits the size of the model that can be fine tuned. It makes large model fine tuning inaccessible to most researchers and companies that do not have access to massive GPU clusters. For example fine-tuning GPT-3 would require over 8 DGX-2 nodes(128 GPUs) with 3D parallelism to just fit the model for training, even though a single DGX-2 node (16-GPUs) has enough compute to fine-tune it in a reasonable time.</p><p>In addition to the GPU memory wall, state-of-the-art for training massive models is also limited in terms of usability and flexibility. As discussed above, 3D parallelism requires combining data, model, and pipeline parallelism in sophisticated ways to get to hundreds of billions or trillions of parameters. While such a system can be very efficient, it requires data scientists to perform major model code refactoring, replacing single GPU operators with tensor-sliced versions, and splitting the model into load-balanced pipeline stages. This also makes 3D parallelism inflexible in the types of models that it can support. Models with complex dependencies cannot be easily converted into a load-balanced pipeline.</p><p>Given the landscape of large model training, 3 questions arise:</p><p>• Looking ahead, how do we support the next 1000x growth in model size, going from models like GPT-3 with 175 billion parameters to models with hundreds of trillions of parameters? • How can we make large models of today accessible to more data scientists who don't have access to hundreds to GPUs? • Can we make large model training easier by eliminating the need for model refactoring and multiple forms of parallelism?</p><p>In this paper, we take a leap forward from 3D parallelism and present ZeRO-Infinity, a novel system capable of addressing all the aforementioned challenges of large model training.</p><p>Unprecedented Model Scale ZeRO-Infinity extends the ZeRO family of technology <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> with new innovations in heterogeneous memory access called the infinity offload engine. This allows ZeRO-Infinity to support massive model sizes on limited GPU resources by exploiting CPU and NVMe memory simultaneously. In addition, ZeRO-Infinity also introduces a novel GPU memory optimization technique called memory-centric tiling to support extremely large individual layers that would otherwise not fit in GPU memory even one layer at a time. With the infinity offload engine and memory-centric tiling, ZeRO-Infinity not only supports the next 1000x growth in model size, but also makes large models accessible to data scientists with limited GPU resources.</p><p>Excellent Training Efficiency ZeRO-Infinity introduces a novel data partitioning strategy for leveraging aggregate memory bandwidth across all devices, which we refer to as bandwidth-centric partitioning, and combines it with powerful communication overlapcentric design, as well as optimizations for high performance NVMe access in the infinity offload engine. Together, ZeRO-Infinity offers excellent training efficiency, despite offloading data to CPU or NVMe, unencumbered by their limited bandwidth.</p><p>Ease of Use With ZeRO-Infinity, data scientists no longer have to adapt their model to multiple forms of parallelism like in 3D parallelism. This is possible due to memory-centric tiling in ZeRO-Infinity discussed above aimed at reducing GPU memory requirements of large individual layers that would otherwise require model parallelism (tensor-slicing) to fit the layers in GPU memory. In addition, ZeRO-Infinity eliminates the need for manual model code refactoring, even when scaling to trillions of parameters via an ease inspired implementation that automates all of the communication and data partitioning required for training arbitrary model architectures.</p><p>The main contributions of this paper are as follows: • Memory and performance characterization for large model training that describes the memory requirements (Sec. 3) for different components of a large model training as well as their bandwidth requirements (Sec. 4) for the training to be efficient.</p><p>• ZeRO-Infinity (Sec. 5, 6 &amp; Sec. 7): A novel DL training system technology consisting five innovative technologies to address the memory and bandwidth requirements for offering unprecedented model scale that is accessible and easy to use while achieving excellent training efficiency: i) infinity offload engine to fully leverage heterogeneous architecture on modern clusters by simultaneously exploiting GPU, CPU and NVMe memory, and GPU and CPU compute, ii) memory-centric tiling to handle massive operators without requiring model parallelism, iii) bandwidthcentric partitioning for leveraging aggregate memory band-</p><p>width across all parallel devices, iv) overlap-centric design for overlapping compute and communication, v) ease-inspired implementation to avoid model code refactoring. • An extensive evaluation of ZeRO-Infinity demonstrating: i) unprecedented scale running 32 trillion parameters on 32 NVIDIA DGX-2 nodes (512 V100 GPUs), ii) excellent training efficiency achieving over 25 petaflops in throughput on the same hardware, iii) superlinear scalability of a trillion parameter model, iv) accessibility and ease-of-use: fine-tune up to a trillion parameter model on a single DGX-2 node, without using any model parallelism or model code refactoring, and v) impact of different technologies in ZeRO-Infinity on model-scale and efficiency (Sec. 8). • A discussion of ZeRO-Infinity and its potential implications of future hardware system design (Sec. 9) • An open source implementation of ZeRO-Infinity in DeepSpeed<ref type="foot" target="#foot_1">foot_1</ref> , a deep learning optimization library for making distributed training easy, efficient, and effective training that has been extensively adopted by the DL community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND AND RELATED WORK</head><p>Data, Model, Pipeline and 3D Parallelism Parallelization is an important strategy for training large models at scale. For a model that fits in the device memory for training, data parallelism (DP) can be used to scale training to multiple devices. When models do not fit in device memory, model parallelism<ref type="foot" target="#foot_2">foot_2</ref> (MP) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref> and pipeline parallelism (PP) <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref> can split the model among processes, vertically and horizontally, respectively. 3D parallelism <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref> combines data, model, and pipeline parallelism to leverage the merits of each, allowing it to scale to trillions of parameters efficiently. While 3D parallelism can be highly efficient, it requires i) significant model code refactoring to split the model into model and pipeline parallel components, ii) models with complex dependency graphs are difficult to be expressed into load-balanced pipeline stages and iii) the model size is limited by the total available GPU memory. We refer the reader to Ben-Nun and Hoefler <ref type="bibr" target="#b18">[19]</ref> for a thorough survey on parallelism in DL.</p><p>ZeRO: Zero Redundancy Optimizer ZeRO <ref type="bibr" target="#b10">[11]</ref> removes the memory redundancies across data-parallel processes by partitioning the three model states (i.e., optimizer states, gradients, and parameters) across data-parallel processes instead of replicating them. By doing so, it boosts memory efficiency compared to classic data parallelism while retaining its computational granularity and communication efficiency. There are three stages in ZeRO corresponding to three model states: the first stage (ZeRO-1) partitions only the optimizer states, the second stage (ZeRO-2) partitions both the optimizer states and the gradients, and the final stage (ZeRO-3) partitions all three model states. In ZeRO-3, the parameters in each layer of the model are owned by a unique data parallel process. During the training, ZeRO-3 ensures that the parameters required for the forward or backward pass of an operator are available right before its execution by issuing broadcast communication collectives from the owner process. After the execution of the operator, ZeRO-3 also removes the parameters as they are no longer needed until the next forward or backward pass of the operator. Additionally, during the parameter update phase of training, ZeRO-3 ensures that each data-parallel process only updates the optimizer states corresponding to the parameters that it owns. Thus, ZeRO-3 can keep all the model states partitioned throughout the training except for the parameters that are required by the immediate computation.</p><p>Heterogeneous Training Approaches Out of several heterogeneous CPU memory based training approaches <ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref>, ZeRO-Offload <ref type="bibr" target="#b11">[12]</ref> is the state-of-the-art (SOTA) for large model training on multi-GPUs. ZeRO-Offload is built on top of ZeRO-2 and stores the gradients and the optimizer states in CPU memory. ZeRO-Offload leverages CPU memory in the absence of enough GPU devices to store the optimizer states and gradients. However, it still requires the parameters to be stored in GPU memory and replicated across all devices. Thus, the model scale with ZeRO-Offload is limited to the total number of parameters that the memory on a single GPU device can host. ZeRO-Offload also requires a large batch size to remain efficient due to suboptimal data partitioning and limited PCIe bandwidth. We address these limitations of ZeRO-Offload with ZeRO-Infinity. In terms of NVMe based approaches, Zhao et al. <ref type="bibr" target="#b26">[27]</ref> use a hierarchical parameter server-based design to offload sparse parameters to SSD for creating a massive scale DL Ads System. In contrast, ZeRO-Infinity is designed to be a generic DL system for training massive dense models.</p><p>Reducing Activation Memory Activations are the intermediate results produced during the forward propagation that need to be retained to compute the gradients during backward propagation. Multiple efforts have focused on reducing the memory required by activations through compression <ref type="bibr" target="#b27">[28]</ref>, activation checkpointing <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref>, or live analysis <ref type="bibr" target="#b30">[31]</ref>. ZeRO-Infinity works together with activation checkpointing to reduce activation memory.</p><p>Adam Optimizer and Mixed Precision Training Adaptive optimization methods <ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref> are crucial to achieving SOTA performance and accuracy for effective model training of large models. Compared to SGD, by maintaining fine-grained first-order and second-order statistics for each model parameter and gradient at the cost of significant memory footprint. Adam <ref type="bibr" target="#b32">[33]</ref> is the optimizer used most prominently in large model training.</p><p>Large model training is generally trained in mixed precision, where the forward and backward propagation are done in FP16 and the parameter updates in FP32 <ref type="bibr" target="#b35">[36]</ref>. This leverages the performance acceleration of the tensor core units available on modern GPUs <ref type="bibr" target="#b36">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MEMORY REQUIREMENTS</head><p>This section characterizes the memory requirements for DL training. While our methodology is generic, we focus the concrete analysis on Transformer <ref type="bibr" target="#b37">[38]</ref> based architectures since all of the SOTA models with over a billion parameters follow that. Our analysis assumes mixed precision training with the Adam optimizer since this recipe is the de facto standard for training Transformer based models.</p><p>The memory required for training can be categorized into two components: i) Model states including optimizer states, gradients, and model parameters, ii) Residual states primarily referring to activation memory. To study training on heterogeneous resources, we also characterize the GPU working memory, describing the minimum amount of memory that must be available on the GPU to support training, assuming the model and residual states can be successfully offloaded from GPU memory.</p><p>Memory for Model States: The model states are comprised of optimizer states, gradients, and parameters. For mixed precision training with Adam optimizer, the parameters and gradients are stored in FP16 while the optimizer states consist of FP32 momentum, variance, parameters, and gradients. In total, each parameter requires 20 bytes of memory. The total number of parameters in a Transformer based model primarily depends on the hidden dimension (ℎ𝑑) and the number of Transformer layers (𝑛𝑙). Nearly all the parameters in a Transformer block come from four linear layers within each block with sizes: (ℎ𝑑, 3ℎ𝑑), (ℎ𝑑, ℎ𝑑), (ℎ𝑑, 4ℎ𝑑) and (4ℎ𝑑, ℎ𝑑), respectively. Thus, the total parameters in a Transformer based model and can be approximated as 12 × 𝑛𝑙 × ℎ𝑑 2  (1) requiring a total memory</p><formula xml:id="formula_0">240 × 𝑛𝑙 × ℎ𝑑 2 (2)</formula><p>in bytes to store the model states. Figure <ref type="figure" target="#fig_1">2a</ref> column 5 shows the memory required to store the model states of a GPT-3 like Transformer based model with 100 billion to a 100 trillion parameters created by varying hidden dimension and number of layers. To put the memory requirements in context, Figure <ref type="figure" target="#fig_1">2b</ref> column 3 shows the aggregate GPU memory available on a single NVIDIA V100 DGX-2 box as well as a DGX-2 SuperPOD cluster. Note that it requires 64 GPUs to just fit the model states for a 100B parameter model. Fitting a trillion parameter model requires over 512 GPUs, while a 10 trillion parameter model is beyond the scope of even a massive 1536 GPU cluster.</p><p>Memory for Residual States: The residual states primarily consist of the activation memory, which depends on the model architecture, batch size (𝑏𝑠𝑧) and sequence length (𝑠𝑒𝑞), and it can be quite large. On the positive side, the memory required for activation can be significantly reduced via activation checkpointing <ref type="bibr" target="#b28">[29]</ref>, which trades off activation memory at the expense of 0.33x additional recomputation. Large models such as Turing-NLG 17.2B and GPT-3 175B were all trained using activation checkpointing. The memory required to store activation checkpoints is estimated as Model State Working Memory (MSWM) is the minimum amount of GPU memory required to perform forward or backward propagation on the largest single operator in the model after all the model states have been offload to CPU or NVMe. This is approximately given by the size of the parameters and gradients of that operator in the model, since there must be at least enough memory to hold the parameter and its gradient for backward propagation.</p><formula xml:id="formula_1">2 × 𝑏𝑠𝑧 × 𝑠𝑒𝑞 × ℎ𝑑 × 𝑛𝑙/𝑐𝑖<label>(3)</label></formula><p>For a Transformer based model, the largest operator is a linear layer that transforms hidden states from ℎ𝑑 to 4ℎ𝑑. The size of the parameter and gradients of this linear layer in bytes is</p><formula xml:id="formula_2">4 × ℎ𝑑 × 4ℎ𝑑<label>(4)</label></formula><p>Note that MSWM (Figure <ref type="figure" target="#fig_1">2a</ref> Column 8) grows significantly beyond a 100 billion parameters, requiring multiple gigabytes in contiguous memory, which can result in running out of memory during training due to lack of enough contiguous memory to satisfy these requirements. State-of-art approaches like 3D Parallelism, addresses this issue via model parallelism, by splitting individual operator across multiple GPUs. In Sec. 5.1.3, we discuss a novel approach for addressing these massive model state working memory without requiring model parallelism.</p><p>Activation Working Memory (AWM) is the memory required in the backward propagation for recomputing the activations before performing the actual backward propagation. This is the size of the activations between two consecutive activation checkpoints. For example, if we create one activation checkpoint per Transformer block, the memory is given by the size of the total activation per Transformer block. This is given in bytes by approximately</p><formula xml:id="formula_3">𝑏𝑠𝑧 × 𝑠𝑒𝑞 × 𝑐𝑖 × (16 × ℎ𝑑 + 2 × 𝑎𝑡𝑡𝑛_ℎ𝑒𝑎𝑑𝑠 × 𝑠𝑒𝑞).<label>(5)</label></formula><p>Figure <ref type="figure" target="#fig_1">2a</ref> column 8 shows that AWM gets large beyond 10 trillion parameters, even with 𝑐𝑖 = 1. Unlike MSWM that is only composed of a single parameter and gradient, AWM is composed of dozens of activations, and does not cause memory issues due to lack of contiguous memory as long as the total AWM can fit in GPU memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">BANDWIDTH REQUIREMENTS</head><p>A critical question of offloading to CPU and NVMe memory is whether their limited bandwidth will hurt training efficiency. This section characterizes the impact of bandwidth on training efficiency. We start from defining an efficiency metric. Assuming a workload execution without any compute and communication overlap, we can use the peak computational throughput (𝑝𝑒𝑎𝑘 𝑡𝑝 ), data movement bandwidth (𝑏𝑤) and its arithmetic intensity (𝑎𝑖𝑡) to estimate the training efficiency.</p><p>The arithmetic intensity (AIT) of a workload is the ratio between the total computation and the data required by the computation. It describes the amount of computation per data movement. Higher AIT means a lower requirement on the data movement bandwidth, since for each data loaded the accelerator can do more computations. The efficiency metric can derived as follows:</p><formula xml:id="formula_4">𝑐𝑜𝑚𝑝𝑢𝑡𝑒_𝑡𝑖𝑚𝑒 = 𝑡𝑜𝑡𝑎𝑙_𝑐𝑜𝑚𝑝𝑢𝑡𝑎𝑡𝑖𝑜𝑛 𝑝𝑒𝑎𝑘 𝑡𝑝 𝑎𝑖𝑡 = 𝑡𝑜𝑡𝑎𝑙_𝑐𝑜𝑚𝑝𝑢𝑡𝑎𝑡𝑖𝑜𝑛 𝑡𝑜𝑡𝑎𝑙_𝑑𝑎𝑡𝑎_𝑚𝑜𝑣𝑒𝑚𝑒𝑛𝑡 𝑐𝑜𝑚𝑚𝑢𝑛𝑖𝑐𝑎𝑡𝑖𝑜𝑛_𝑡𝑖𝑚𝑒 = 𝑡𝑜𝑡𝑎𝑙_𝑑𝑎𝑡𝑎_𝑚𝑜𝑣𝑒𝑚𝑒𝑛𝑡 𝑏𝑤 = 𝑡𝑜𝑡𝑎𝑙_𝑐𝑜𝑚𝑝𝑢𝑡𝑎𝑡𝑖𝑜𝑛 𝑎𝑖𝑡 ×𝑏𝑤 𝑒 𝑓 𝑓 𝑖𝑐𝑖𝑒𝑛𝑐𝑦 = 𝑐𝑜𝑚𝑝𝑢𝑡𝑒_𝑡𝑖𝑚𝑒 𝑐𝑜𝑚𝑝𝑢𝑡𝑒_𝑡𝑖𝑚𝑒+𝑐𝑜𝑚𝑚𝑢𝑛𝑖𝑐𝑎𝑡𝑖𝑜𝑛_𝑡𝑖𝑚𝑒</formula><p>The efficiency can be written as a function of 𝑝𝑒𝑎𝑘 𝑡𝑝 , 𝑏𝑤 and 𝑎𝑖𝑡:</p><formula xml:id="formula_5">𝑒 𝑓 𝑓 𝑖𝑐𝑖𝑒𝑛𝑐𝑦 = 𝑎𝑖𝑡 × 𝑏𝑤 𝑎𝑖𝑡 × 𝑏𝑤 + 𝑝𝑒𝑎𝑘 𝑡𝑝 (6)</formula><p>We will use this simple efficiency equation to characterize the data movement bandwidth required for training massive models. But before that, we will first quantify 𝑎𝑖𝑡 for DL training workloads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Quantifying AIT in DL training</head><p>Model states and activation checkpoints can have varying 𝑎𝑖𝑡. We can quantify them by first identifying the total computation in each iteration of DL training, and then identifying the data movement volume for each of the model states and activations. Total Computation per Iteration The total computation per iteration is dominated by the computation in the linear layers of the Transformer. For the forward propagation this can be approximated as a function of the number of parameters, sequence length, and batch size, given by 2 × 𝑏𝑠𝑧 × 𝑠𝑒𝑞 × 𝑝𝑎𝑟𝑎𝑚𝑠. The cost of backward propagation is approximately twice that of forward propagation. Additionally, activation checkpointing requires an additional forward computation as part of recomputation during backward propagation. Therefore, the total computation per iteration is:</p><formula xml:id="formula_6">𝑐𝑜𝑚𝑝𝑢𝑡𝑎𝑡𝑖𝑜𝑛_𝑝𝑒𝑟 _𝑖𝑡𝑒𝑟 = 2 × 4 × 𝑏𝑠𝑧 × 𝑠𝑒𝑞 × 𝑝𝑎𝑟𝑎𝑚𝑒𝑡𝑒𝑟𝑠 (7) = 2 × 4 × 12 × 𝑏𝑠𝑧 × 𝑠𝑒𝑞 × 𝑛𝑙 × ℎ𝑑 2 (8)</formula><p>AIT w.r.t. Parameters and Gradients During forward and back propagation, model parameters must be loaded from the source location to GPU registers at least twice, i) during forward, ii) during the actual backward, resulting in a data movement of 2×𝑝𝑎𝑟𝑎𝑚𝑒𝑡𝑒𝑟𝑠.</p><p>In presence of activation checkpointing, the parameters may be loaded one additional time for re-computation during the backward pass, adding another 1 × 𝑝𝑎𝑟𝑎𝑚𝑒𝑡𝑒𝑟𝑠. Furthermore, the gradients must be stored from the GPU registers to its final location at least once, adding a final 1 × 𝑝𝑎𝑟𝑎𝑚𝑒𝑡𝑒𝑟𝑠 in data movement. Therefore, assuming that parameters and gradients are stored at the same final location, the total data movement during the forward and backward pass would be 4 ×𝑝𝑎𝑟𝑎𝑚𝑒𝑡𝑒𝑟𝑠, i.e. 2 × 4 ×𝑝𝑎𝑟𝑎𝑚𝑒𝑡𝑒𝑟𝑠 in bytes. The total computation per iteration is given by Sec. <ref type="bibr" target="#b3">4</ref>.1. Therefore the 𝑎𝑖𝑡 w.r.t parameter and gradients is 𝑠𝑒𝑞 × 𝑏𝑠𝑧. (9) AIT w.r.t. Optimizer States During the optimizer step, the optimizer states must be read at least once, and the optimizer states must be written at least once. So the total data movement is 2 × 𝑜𝑝𝑡𝑖𝑚𝑖𝑧𝑒𝑟 _𝑠𝑡𝑎𝑡𝑒𝑠, which is approximately 2×16×𝑝𝑎𝑟𝑎𝑚𝑒𝑡𝑒𝑟𝑠 bytes. The total computation per iteration is given by Sec. 4.1. Therefore 𝑎𝑖𝑡 w.r.t optimizer states during a full training iteration is 𝑠𝑒𝑞 × 𝑏𝑠𝑧/4. (10) AIT w.r.t. Activation Checkpoints During the forward propagation activation checkpoints must be saved to their final location, and must be retrieved during the backward propagation. Therefore, the total data movement w.r.t activation checkpoints in bytes is given by 2×𝑡𝑜𝑡𝑎𝑙_𝑎𝑐𝑡𝑖𝑣𝑎𝑡𝑖𝑜𝑛_𝑐ℎ𝑒𝑐𝑘𝑝𝑜𝑖𝑛𝑡𝑠_𝑖𝑛_𝑏𝑦𝑡𝑒𝑠 which is given by 4 × 𝑛𝑙/𝑐𝑖 × ℎ𝑑 × 𝑠𝑒𝑞 ×𝑏𝑠𝑧 from Eq. (3). The total computation per iteration is given by Sec. 4.1. So the 𝑎𝑖𝑡 w.r.t activation checkpoints is given by 24 × ℎ𝑑 × 𝑐𝑖. (11)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Bandwidth Requirements</head><p>Due to the variation in the AIT, model states and activation checkpoints have very different bandwidth requirements to achieve good efficiency. The former only depends on the batch size and sequence length, while the latter only depends on the frequency of activation checkpoints and hidden dimension size of the model. Besides AIT, the bandwidth requirement for efficiency also depends on 𝑝𝑒𝑎𝑘 𝑡𝑝 , as shown in Eq. ( <ref type="formula">6</ref>). Using 𝑝𝑒𝑎𝑘 𝑡𝑝 , and 𝑎𝑖𝑡 we first show how efficiency varies with bandwidth w.r.t to different model and residual states, and then discuss the bandwidth requirements on these states for DL training to be efficient. Our methodology is generic and can be applied to understanding the bandwidth requirements on any current or future generation clusters. Here, we use NVIDIA V100 DGX-2 SuperPOD cluster as our example platform.</p><p>Using the 𝑎𝑖𝑡 expression from Sec. 4.1 and efficiency metric based on Eq. ( <ref type="formula">6</ref>), Figure <ref type="figure" target="#fig_2">3</ref> shows the relationship between efficiency and available bandwidth w.r.t. parameter and gradients, optimizer states, and activation checkpoints. To produce these plots, we computed the 𝑎𝑖𝑡 based on expressions derived in Sec. 4.1, for varying batch sizes, sequence length and model configurations. More specifically, we use a sequence length of 1024, the same sequence length used for GPT-2 <ref type="bibr" target="#b1">[2]</ref>, Megatron-LM <ref type="bibr" target="#b6">[7]</ref>, and Turing-NLG <ref type="bibr" target="#b38">[39]</ref>. We vary batch size range from 1 to 16 to capture large GPU and small GPU experiments, respectively. A small batch size per GPU is used when running on large number of GPUs, while a large batch size per GPU is used when training on relatively fewer GPUs to maintain a reasonable effective batch size for training. Our hidden size ranges from 8K-64K representing models with hundreds of billions of parameters, to tens of trillions of parameters as shown in Figure <ref type="figure" target="#fig_1">2a</ref>.</p><p>To identify 𝑝𝑒𝑎𝑘 𝑡𝑝 for this analysis, we use an empirical approach <ref type="foot" target="#foot_3">4</ref> . We ran models with aforementioned configurations on a single NVIDIA V100 DGX-2 box with all non-GPU communication turned off to simulate a virtually unlimited bandwidth scenario. The performance achieved ranged from 62-78 TFlops/GPU based on the hidden size of 8K-64K, respectively. We used the average of 70 TFlops/GPU to represent 𝑝𝑒𝑎𝑘 𝑡𝑝 for the purpose of this analysis<ref type="foot" target="#foot_4">foot_4</ref> .</p><p>Bandwidth w.r.t. Parameter and Gradients Figure <ref type="figure" target="#fig_2">3a</ref> shows that with a bandwidth of over 70 GB/s for parameter and gradients, we can achieve over 50% efficiency for even the smallest batch size. At this bandwidth, the data movement in theory can be completely overlapped with the computation to achieve a 100% efficiency.</p><p>Bandwidth w.r.t. Optimizer States Figure <ref type="figure" target="#fig_2">3b</ref> shows that optimizer states require nearly 4x higher bandwidth to achieve 50% efficiency compared to parameters and gradients. Furthermore, the optimizer states are updated at the end of the forward and backward propagation and cannot be overlapped with the computation. As a result they require significantly larger bandwidth to keep the overall DL workload efficient. For example achieving 90% efficiency with batch size of 2 per GPU requires nearly 1.5 TB/s of effective bandwidth, which is greater than even the GPU memory bandwidth.</p><p>Bandwidth w.r.t. activation memory Figure <ref type="figure" target="#fig_2">3c</ref> also shows that with activation checkpointing enabled, a meager bandwidth of 2 GB/s is able to sustain over 50% efficiency even for a hidden size of 2𝐾. The bandwidth requirement drops down to less than 1 GB/s once the hidden size grows over 8𝐾.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">ZERO-INFINITY DESIGN OVERVIEW</head><p>In this section we present an overview of the design choices in ZeRO-Infinity that enable it to achieve unprecedented model scale while offering excellent training efficiency and ease of use. A bird's eye view of ZeRO-Infinity is illustrated in Figure <ref type="figure" target="#fig_4">4</ref> and discussed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Design for Unprecedented Scale</head><p>Modern GPU clusters are highly heterogeneous in terms of memory storage. In addition to the GPU memory, they have CPU memory as well as massive NVMe storage that is over 50x larger than the GPU memory and nearly 20x larger than CPU memory (See Fig. <ref type="figure" target="#fig_1">2b</ref>).</p><p>We developed ZeRO-Infinity, a parallel system for DL training that can transcend the GPU memory wall by exploiting these heterogeneous memory systems in modern GPU clusters. Figure <ref type="figure" target="#fig_0">1</ref> compares the maximum achieved model size of 3D parallelism and ZeRO-Infinity. ZeRO-Infinity supports one trillion parameters per NVIDIA V100 DGX-2 node, a 50x increase over 3D parallelism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.1.1</head><p>Infinity offload engine for model states. ZeRO-Infinity is built on top of ZeRO-3 <ref type="bibr" target="#b10">[11]</ref> which partitions all model states to remove memory redundancy as discussed in Sec. 2. Unlike any of the existing ZeRO family of technology, ZeRO-Infinity is designed with a powerful offload mechanism called the infinity offload engine which can offload all of the partitioned model states to CPU or NVMe memory, or keep them on the GPU based on the memory requirements. Note from Fig. <ref type="figure" target="#fig_1">2a</ref> and Fig. <ref type="figure" target="#fig_1">2b</ref>, even the model states required by a 100 trillion parameter model can fit in the aggregate NVMe memory of a DGX-2 cluster with 96 nodes (1536 GPUs). Therefore, the infinity offload engine allows ZeRO-Infinity to fit model states of models with hundreds of trillions of parameters. See Sec. 6 for more details.  0 is the portion of layer 0's parameters owned by 𝐺𝑃𝑈 (2) .</p><p>of CPU memory available on a DGX-2 system, while the 3 TBs of activation checkpoints required by a 100 trillion parameter is within reach of the CPU memory of the next generation hardware. Therefore, by offloading activation checkpoints to CPU memory, ZeRO-Infinity can fit the activation checkpoints of models with hundreds of trillions of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Memory-centric tiling for working memory.</head><p>To reduce the working memory requirements of DL training for large models, ZeRO-Infinity introduces a novel technique called memory-centric tiling that exploits the data fetch and release pattern of ZeRO-3 to reduce the working memory requirements by breaking down a large operator into smaller tiles that can be executed sequentially.</p><p>For example, to reduce the working memory for a large linear operator, ZeRO-Infinity represents the operator as a mathematically equivalent sequence of smaller linear operators consisting of tiles of parameters from the original operator, and executes them sequentially. When combined with ZeRO-3, the parameter and gradients of each tile can be fetched and released one at a time, reducing the working memory proportional to the number of tiles. Therefore, ZeRO-Infinity can support operators of arbitrary sizes, without relying on model parallelism to fit them in limited GPU memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Design for Excellent Training Efficiency</head><p>Offloading all model states and activations to CPU or NVMe is only practical if ZeRO-Infinity can achieve high efficiency despite the offload. In reality this is extremely challenging since CPU memory is an order of magnitude slower than GPU memory bandwidth, while the NVMe bandwidth is yet another order of magnitude slower than the CPU memory bandwidth. Furthermore, reading and writing to these memory from GPU is even slower (see Fig. <ref type="figure" target="#fig_1">2b</ref>).</p><p>On a system like the DGX-2, the bandwidth must be greater than 70GB/s, 1.5TB/s, and 1-4 GB/s w.r.t. parameter and gradients, optimizer states, and activation checkpoints, respectively for DL training to be efficient, based on our analysis in Sec. 4. Here we discuss how ZeRO-Infinity achieves the necessary bandwidths to achieve excellent efficiency. 5.2.1 Efficiency w.r.t Parameter and Gradients. The data movement bandwidth for parameters and gradients must be greater than 70GB/s, close to the GPU-GPU bandwidth available on DGX-2 clusters <ref type="bibr" target="#b39">[40]</ref>. Therefore, a DL parallel training solution like ZeRO-3 <ref type="bibr" target="#b10">[11]</ref> where parameters are broadcasted from the owner GPU to the rest before using them in forward or backward propagation can run efficiently as long as the communication is overlapped.</p><p>On the contrary, a meager 12 GB/s PCIe bandwidth from a single GPU to CPU memory or NVMe (see Fig. <ref type="figure" target="#fig_1">2b</ref>) or vice-versa is simply not sufficient to support heterogeneous training at scale<ref type="foot" target="#foot_5">foot_5</ref> . Therefore, existing heterogeneous solutions like ZeRO-Offload where the parameters must be first moved from CPU to owner GPU before broadcasting requires significantly large batch sizes per GPU to achieve enough 𝑎𝑖𝑡 necessary to be efficient under the limited bandwidth. This poses two problems: i) for massive models the activation memory will get too large to fit even in CPU memory, and ii) the effective batch size becomes too large when scaling to hundreds or thousands of GPUs for effective convergence.</p><p>ZeRO-Infinity addresses these challenges in two ways: i) bandwidthcentric partitioning: a novel data mapping and parallel data retrieval strategy for offloaded parameters and gradients that allows ZeRO-Infinity to achieve virtually unlimited heterogeneous memory bandwidth (details in Sec. 6.1), and ii) an overlap centric design that allows ZeRO-Infinity to overlap not only GPU-GPU communication with computation but also NVMe-CPU and CPU-GPU communications over the PCIe (details in Sec. 5.1.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Efficiency w.r.t Optimizer States.</head><p>Unlike parameters and gradients that are consumed and produced sequentially during the forward and backward propagation, optimizer states can be updated in parallel, all at once. This property is leveraged by both ZeRO-3 and ZeRO-Offload, that store and update the optimizer states in GPU and CPU memory, respectively, in parallel across all available GPUs and CPUs. As a result the aggregate GPU or CPU memory bandwidth can get much higher than the required 1.5TB/s with increase in GPU or CPU count.</p><p>Since ZeRO-Infinity is built upon ZeRO-3, it can also leverage the aggregate GPU and CPU memory bandwidth as well as the aggregate CPU compute for optimizer step, when offloading optimizer states to CPU memory. However, with NVMe offload, it is necessary to bring the data from NVMe to CPU memory and back in chunks that can fit in the CPU memory to perform the optimizer step, one chunk at a time. The optimizer step is therefore limited by the NVMe-CPU memory bandwidth: while ZeRO-Infinity can achieve aggregate NVMe bandwidth across multiple nodes, it is crucial to achieve near peak NVMe bandwidth per node, to allow supporting the necessary bandwidth of over 1.5 TB/s with as few nodes, and as small batch size as possible. Furthermore, the process of bringing data in and out of NVMe to CPU memory, or from CPU memory to GPU memory can cause CPU memory fragmentation in both GPU and CPU that can result in out of memory even with plenty of memory still available.</p><p>The infinity offload engine can not only achieve near peak NVMe bandwidth, it can also allows ZeRO-Infinity to overlap NVMe to CPU reads with CPU to NVMe writes, as well as the CPU computation for the optimizer step at the same time to allow ZeRO-Infinity to remain efficient with a modest batch size on small number of GPUs and with small batch sizes on large numbers of GPUs. At the same time, it minimizes memory fragmentation by carefully reusing temporary buffers for data movement. We discuss the optimizations in infinity offload engine and in detail in Sec. 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Efficiency w.r.t Activations.</head><p>On a DGX-2 node, each GPU can read and write data at about 3 GB/s to CPU memory in parallel over the PCIe allowing activation checkpoints to be offloaded to CPU memory while retaining over 80% efficiency for hidden size larger 8𝐾 or larger. To also allow for high efficiency at smaller hidden sizes, ZeRO-Infinity can decrease the frequency of activation checkpoints as well as effectively overlap the communication of activation checkpoints both to and from CPU memory with the forward and backward computation on the GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Design for Ease of Use</head><p>With ZeRO-Infinity, data scientists no longer have to adapt their model to multiple forms of parallelism like in 3D parallelism. This is possible due to memory-centric tiling in ZeRO-Infinity discussed in Sec. 5.1.3 aimed at reducing GPU memory requirements of large individual layers that would otherwise require model parallelism (tensor-slicing) to fit the layers in GPU memory.</p><p>In addition, ZeRO-Infinity is implemented in PyTorch in a way that eliminates the need for manual model code refactoring even when scaling to trillions of parameters. This is made possible through an ease-inspired implementation with two automated features: i) automated data movement to gather and partition parameters right before and after they are required during the training. ZeRO-Infinity does this by injecting i) pre forward/backward hooks into PyTorch submodules that trigger allgather collectives to collect the parameters required before its forward/backward pass and ii) post forward/backward hooks that trigger parameter/gradient partiitoning and optionally offloading them to CPU or NVMe (see Sec. 7.1 for details).</p><p>ii) automated model partitioning during initialization such that models that can not fit within single GPU or CPU memory can still be initialized without requiring manual partitioning of the model across data parallel processes. ZeRO-Infinity achieves this by wrapping the constructor of all module classes so that parameters of each submodule are partitioned and offloaded immediately after they are created during initialization. The entire model is never fully instantiated on a single data parallel process (see Sec. 7.2 for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Bandwidth-Centric Partitioning</head><p>ZeRO-Infinity implements a novel data mapping and retrieval strategy to address the NVMe and CPU memory bandwidth limitations. Unlike ZeRO <ref type="bibr" target="#b10">[11]</ref> and ZeRO-Offload <ref type="bibr" target="#b11">[12]</ref>, where parameters of each layer are owned by a single data parallel process, which broadcasts them to the rest when needed, ZeRO-Infinity partitions individual parameters across all the data parallel process, and uses an allgather instead of a broadcast when a parameter needs to be accessed. Note that both broadcast and allgather communication collectives have the same communication cost when it comes to data movement volume if the data is located on the GPU. Therefore, this makes no difference for a GPU-only training. However, this is a game changer when the data is located in NVMe or CPU.</p><p>In the broadcast-based approach, since each parameter is fully owned by one of the data parallel processes, the parameter must be first communicated from its source location to the GPU memory via the PCIe before the broadcast can happen. Note that only a single PCIe can be active for this process, while all the PCIe links connected to all the other GPUs are idle. On the contrary, with the partitioned parameter and allgather based approach in ZeRO-Infinity, all PCIe links are active in parallel, each bringing in 1/𝑑𝑝 𝑡ℎ portion of the parameter where 𝑑𝑝 is the data parallel degree. As a result, the effective communication bandwidth between NVMe or CPU to the GPU, increases with the 𝑑𝑝 degree.</p><p>For example, with broadcast-based approach, the CPU/NVMe to GPU bandwidth stays constant at about 12 GB/s with PCIe Gen 3, even with 16-way data parallelism on the DGX-2 box. However, with the all-gather-based approach, the effective achievable bandwidth increases to about 48/25 GB/s (3.0/1.6 GB/s per GPU), respectively (see Fig. <ref type="figure" target="#fig_1">2b</ref>), limited only by the max aggregate PCIe bandwidth and max NVMe bandwidth per DGX-2 node. From here, the bandwidth grows linearly with more nodes. When training a massive model at massive scale, ZeRO-Infinity can therefore offer significantly more heterogeneous memory bandwidth than necessary (virtually unlimited) for the training to remain efficient. For example, on 64 DGX-2 nodes, ZeRO-Infinity has access to over 3TB/s of CPU memory bandwidth and over 1.5TB/s of NVMe bandwidth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Overlap Centric Design</head><p>While ZeRO-Infinity can leverage sufficient heterogeneous memory bandwidth on a multi-node setup, the bandwidth can still be a bottleneck on a single GPU or single node setup. Even the GPU-GPU allgather communication has a big impact on efficiency when running with a small batch size (Fig. <ref type="figure" target="#fig_2">3</ref>). Furthermore, accessing NVMe memory requires a three step process: i) read data from NVMe to CPU memory (nc-transfer), ii) copy the data from CPU memory to GPU memory (cg-transfer), iii) execute allgather to construct the full parameter on all GPUs (gg-transfer). The sequential nature of these data movements means that if done naively, the total communication time would be the sum of each of these three data movement cost, resulting in poor efficiency even if the bandwidth for data movement at each of these stages is individually sufficient.</p><p>To address these issues, ZeRO-Infinity has an overlap engine that not only overlaps GPU-GPU communication with GPU computation, but also overlaps the NVMe to CPU, and CPU to GPU communication, all at the same time. The overlap engine has two components: i) A dynamic prefetcher for overlapping the data movement required to reconstruct parameters before they are consumed in the forward or backward pass, and ii) a communication and offload overlapping mechanism for executing the data movement required by gradients in parallel with the backward computation.</p><p>The dynamic prefetcher in ZeRO-Infinity traces the forward and backward computation on that fly, constructing an internal map of the operator sequence for each iteration. During each iteration, the prefetcher keeps track of where it is in the operator sequence and prefetches the parameter requires by the future operators. The prefetcher is aware of the three step communication process, and therefore can overlap the nc-transfer for one parameter, with cgtransfer and gg-transfer of other parameters. For instance, before executing the 𝑖 𝑡ℎ operator, the prefetcher can invoke nc, cg, and gg-transfer for parameters required by 𝑖 + 3, 𝑖 + 2, and 𝑖 + 1 operators, respectively. Note that all of these data movement can happen in parallel with the execution of the 𝑖 𝑡ℎ operator. Furthermore, ZeRO-Infinity can update the operator sequence map in case of dynamic workflow, allowing for appropriate prefetching even when the forward and backward propagation changes across iterations.</p><p>Similarly, in the backward pass, ZeRO-Infinity can overlap the reduce-scatter for gradients of the parameters in (𝑖 + 1) 𝑡ℎ operator with the computation of the 𝑖 𝑡ℎ operator, while simultaneous transferring the partitioned gradients from the reduce-scatter of the gradients of the (𝑖 + 2) 𝑡ℎ operator to the CPU or NVMe.</p><p>With this powerful overlap centric design, ZeRO-Infinity hides significant portions of data movement even when training with a small number of GPUs and small batch size per GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Infinity Offload Engine</head><p>The infinity offload engine is composed of two main components:</p><p>DeepNVMe, a powerful C++ NVMe read/write library in the infinity offload engine that supports bulk read/write requests for asynchronous completion, and explicit synchronization requests to flush ongoing read/writes. The support for asynchrony allows ZeRO-Infinity to overlap these requests with GPU/GPU or GPU/CPU communication or computation.</p><p>Most importantly, DeepNVMe is capable of achieving near peak sequential read and write bandwidths on the NVMe storage device. It achieves this high performance through a number of optimizations, including aggressive parallelization of I/O requests (whether from a single user thread or across multiple user threads), smart work scheduling, avoiding data copying, and memory pinning.</p><p>Pinned memory management layer To ensure high performance tensor reads (or writes) from (to) NVMe/CPU storage, the source (or destination) tensors must reside in pinned memory buffers. However, pinned memory buffers are scarce system resources, and their oversubscription by a single process can degrade overall system performance or cause system instability. This layer manages the limited supply of pinned memory by reusing a small amount (tens of GBs) for offloading the entire model states (up to tens of TBs) to CPU or NVMe. The reuse of memory buffer prevents memory fragmentation in CPU and GPU memory. This layer also provides PyTorch tensors with pinned memory data, allowing inplace computation of the tensors so that they can then be written to NVMe without any further copies to improve bandwidth.</p><p>ZeRO-Infinity is implemented on top of PyTorch, and is designed to be used without any model code refactoring similar to standard data-parallel training in PyTorch. This section details some of the challenges faced in implementing such a system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Automating Data Movement</head><p>ZeRO-Infinity must coordinate the movement of tensors comprising the model parameters, gradients, and optimizer states. When a tensor is not in active use, it remains partitioned among workers and potentially offloaded to CPU or NVMe memory. The system must ensure that the tensors are resident in GPU memory in time for use and then later re-partitioned.</p><p>PyTorch models are expressed as a hierarchy of modules that represent the layers of a neural network. For example, Transformer architectures <ref type="bibr" target="#b37">[38]</ref> contain submodules such as self-attention and feedforward networks. The self-attention submodules are further comprised of linear transformations and other submodules.</p><p>ZeRO-Infinity recursively injects hooks into the submodules of a model to automate the required data movement. At the start of a submodule's forward pass, these hooks ensure that the submodule's parameters are available for computations, otherwise it will execute the appropriate allgather collectives and block until the parmaeters become available. The overlap-centric design detailed in Sec. 6.2 is critical to minimizing stalls due to parameter communication. At the end of the submodule's forward pass, we partition the parameters again and optionally offload them. The backward pass is handled in a similar fashion.</p><p>7.1.1 Auto Registration of External Parameters. In the ideal case, a submodule's parameters and gradients are only accessed within its own forward and backward passes, making it straight forward to identify and automate the data movement as discussed in section above. However, some model architectures are exceptions, where parameters defined and allocated in a submodule is used in forward and backward propagation of a different submodule. For example, language models such as GPT <ref type="bibr" target="#b40">[41]</ref> share the weights of the embedding layer at both the beginning and the end of the network to map words to vectors and vice versa. We refer to the parameters that are used across module boundaries as external parameters. In presence of external parameters, it is difficult to know which parameters to gather at the beginning of a submodule's forward and backward pass.</p><p>One way to address this is to register external parameters with ZeRO-Infinity so that they are collected for the forward and backward passes of the submodule that access them. After registration, an external parameter is treated like all others and will be included in the prefetching system as described in Sec. 6.2. We provide APIs for manual registration of external parameters.</p><p>In order to improve user experience, we also provide mechanisms to detect these scenarios and automatically register external parameters so the the user does not have to make any code change:</p><p>Intercepting partitioned parameter accesses PyTorch modules store their tensor parameters in a hash table. At the initialization time, we replace the hash table with a subclassed type that overrides the tensor accesses. When a partitioned parameter is accessed, we do a blocking allgather on the parameter, register it as an external parameter, and then return the gathered parameter.</p><p>Activation introspection A submodule may return a parameter from its forward pass to be consumed by another submodule's forward and backward passes. For example, Megatron-LM returns bias vectors from the forward pass of linear layers and they are consumed by the parent Transformer layer modules. We inspect the activation outputs returned from each submodule's forward pass for partitioned parameters. If one is discovered, we collect and register it as an external parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Automatic Model Partitioning during Initialization</head><p>If the model is large, then it may not be possible to fully initialize the model with traditional data parallel approach, replicating it on each data parallel process before it can be partitioned for ZeRO-Infinity. For example, a 500 billion parameter model will occupy 1 TB of memory in half precision, and thus a system with 8 GPUs per node requires 8 TB of aggregate CPU or GPU memory just for the initial data parallel allocation step. This is beyond the GPU or CPU memory available on a node.</p><p>To address this limitation, the parameters corresponding to each layer of the model must be partitioned at the time of initialization, and not after the entire model is initialized. To do this, we provide a Python ZeRO-Infinity context which decorates the __init__ method of torch.nn.Module, so that parameters allocated under each module/sub-module are partitioned immediately after its initialization among the group of data parallel processes.</p><p>As a result, only individual sub-modules are fully initialized before they are partitioned, and the full model is never replicated on all the data parallel process. In the example above, the 500 billion parameter model can therefore be fully partitioned during its initialization requiring only 1 TB of aggregate CPU memory regardless of the total number of data parallel process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">EVALUATION</head><p>This section evaluates ZeRO-Infinity, demonstrating that it achieves excellent training efficiency and scalability for models with tens of trillion parameters. We also show the impact of various technologies within ZeRO-Infinity on model scale and performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Methodology</head><p>Hardware. We conducted our experiments on a cluster of up to 512 V100 SXM3 32 GB GPUs (32 DGX-2 nodes) with 800 Gbps internode communication bandwidth.</p><p>Baseline. For experiments without model parallelism (mp), we use torch's distributed data parallel (DDP <ref type="bibr" target="#b41">[42]</ref>) as a baseline. For experiments with model parallelism, we use Megatron-LM <ref type="bibr" target="#b6">[7]</ref>. As a baseline for each experiment we use the relevant state-of-the-art method among 3D Parallelism <ref type="bibr" target="#b12">[13]</ref>, ZeRO <ref type="bibr" target="#b10">[11]</ref>, or ZeRO-Offload <ref type="bibr" target="#b11">[12]</ref>.</p><p>Model Configurations. We use GPT-like Transformer based models. We fix the sequence length to 1024 and vary the hidden dimension and number of layers to obtain models with different number of parameters.</p><p>Table 1 provides the specific model configurations # nodes # params hidden dim # layers batch/GPU mp fp16 param Opt State 1 10 B 4K 50 8 1 GPU GPU 1 50, 100 B 8K 62, 125 26, 24 1 CPU NVMe 1 0.5, 1 T 18K, 25K 124, 128 8, 7 1 NVMe NVMe 32 0.5, 1 T 18K, 25K 124, 128 7, 5 4 GPU GPU 32 5, 10, 20 T 48K, 64K, 88K 174, 200, 205 3, 2, 1.25 4, 4, 8 NVMe NVMe Table 1: Experiment configurations. Sizes are expressed in B for billions, T for trillions, and K for 1024.  used throughout our evaluation, (see Appendix A) for additional configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Model Size and Speed</head><p>Model Size ZeRO-Infinity trains models over 32 trillion parameters compared to about 650B parameters with 3D parallelism, state of the art, offering a leap of 50x in model scale (Figure <ref type="figure" target="#fig_0">1</ref>). Model Speed Figure <ref type="figure" target="#fig_6">5a</ref> shows the performance of ZeRO-Infinity on up to 20 trillion parameter models on 512 GPUs. For the 500B model (close to the largest that 3D parallelism can run on these resources), ZeRO-Infinity and 3D parallelism achieve nearly identical throughput, indicating ZeRO-Infinity is on par with the training efficiency of the state-of-the-art. When increasing the model size further, 3D parallelism simply runs out of memory, while ZeRO-Infinity trains up to 20 trillion parameter models (40x larger) with excellent throughput of up to 49 TFlops/GPU. At the extreme-scale, Figure <ref type="figure" target="#fig_6">5a</ref> shows a performance drop from 10T (43 TFlops/GPU), and 20T (34 TFlops/GPU). This drop is not due to NVMe bandwidth as both model sizes use NVMe offload, but instead due to an extremely small batch size per GPU (Table <ref type="table">1</ref>) at 20T scale as a result of limited CPU memory to store activation checkpoints. This can be improved by increasing the CPU memory or offloading activation checkpoints to NVMe in a future implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Superlinear Scalability</head><p>Figure <ref type="figure" target="#fig_6">5b</ref> shows that ZeRO-Infinity achieves super-linear scalability from 4 nodes (64 GPUs) to 32 nodes (512 GPUs) when training a 1T model. This is a weak scaling result where we keep batch size per node as constant and increase total batch size with increased number of nodes. ZeRO-Infinity exceeds perfect linear scaling by effectively leveraging the linear increase in aggregate PCIe and NVMe bandwidth to accelerate the offloading of parameters and optimizer states, and leveraging CPU compute from additional nodes for parameter update. In addition, ZeRO-Infinity already achieves over 2.8 petaflops (44 Tflops/GPU) with just 4 nodes, demonstrating that the aggregated NVMe bandwidth is sufficient to achieve good efficiency even at a modest scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4">Democratizing Large Model Training</head><p>Figure <ref type="figure" target="#fig_6">5c</ref> shows performance of training 10B to 1T models on a single node (16 GPUs) with ZeRO-Infinity without any model parallelism. With models up to 100 billion parameters, ZeRO-Infinity achieves excellent performance of over 40 TFlops/GPU, making it possible to fine-tuning models such as GPT-3 with just a single DGX-2 box. In contrast, 3D parallelism is unable to scale to models with over 20 billion parameters. These results demonstrate two aspects of ZeRO-Infinity: i) Accessibility to fine-tuning large models with up to a trillion parameters on a single NVIDIA DGX-2 node, empowering users without access to large GPU clusters. ii) Ease-of-Use: Models at this scale can be trained using ZeRO-Infinity without combining model or pipeline parallelism, or requiring model code refactoring, making it easy for data scientists to scale up their models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.5">Impact of System Features on Model Scale</head><p>We show the impact of different device placement strategies on model scale and impact of memory-centric tiling (Sec. 5.1.3) on maximum hidden size using a single DGX-2 system (16 GPUs).</p><p>Maximum model size Figure <ref type="figure" target="#fig_8">6a</ref> shows the effect of different device placement and partitioning strategies (see Table <ref type="table">2</ref>) on maximum model size. By using data parallelism alone we're limited to only 1.4B parameters, due to limited GPU memory and significant  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Name</head><p>Optimizer + Grad (devices/partitioned)</p><formula xml:id="formula_7">Parameters (devices/partitioned) Data parallel [GPU] / ✗ [GPU] / ✗ ZeRO 2 [GPU] / ✓ [GPU] / ✗ ZeRO-Offload [CPU,GPU] / ✓ [GPU] / ✗ 3D Parallelism [GPU] / ✓ [GPU] / ✓ ZeRO 3 [GPU] / ✓ [GPU] / ✓ ZeRO-Inf-CPU [CPU, GPU] / ✓ [CPU,GPU] / ✓ ZeRO-Inf-NVMe [NVMe,CPU,GPU] / ✓ [NVMe,CPU,GPU] / ✓</formula><p>Table 2: Device placement options and partitioning strategies for optimizer, gradient, and parameter states.</p><p>model state redundancies. As we introduce optimizer/gradient partitioning and offloading to CPU with ZeRO-2 and ZeRO-Offload, we are able to scale up 9x to 13B parameters on a single node. Partitioning and offloading parameter states to CPU in ZeRO-Infinity allows us to almost reach 100B parameters. However, the final major jump in scale comes from offloading model states to NVMe which finally gets us to 1T parameters, resulting in a 700x increase in model size relative to data parallelism alone.</p><p>Maximum Hidden Size We evaluate the impact of memorycentric tiling in enabling large hidden sizes in the presence of memory fragmentation. We train a single layer transformer model with different hidden sizes and tiling factors to identify the largest hidden size that can be trained with and without tiling. To keep memory fragmentation consistent across all the experiments, we pre fragment the total GPU memory into 2 GB contiguous chunks so that all memory allocation requests larger than 2GB will fail.</p><p>Figure <ref type="figure" target="#fig_8">6b</ref> shows the largest hidden size that can be trained without memory-centric tiling is 8K, while we can even train a massive hidden size of 64K using memory-centric tiling factor of 16. With memory-centric tiling, ZeRO-Infinity greatly simplifies DL system stack by avoiding the need for model parallelism, making it easy for data scientists to train with large hidden sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.6">Impact of System Features on Performance</head><p>We evaluate the effects of the infinity offload engine (Sec. 5), bandwidthcentric partitioning (Sec. 6.1), overlap-centric design (Sec. 6.2), and activation checkpoint offloading (Sec. 4.1) on training speed.</p><p>ZeRO-Infinity vs ZeRO-Offload Figure <ref type="figure" target="#fig_8">6c</ref> shows the impact of offloading gradients to CPU memory with ZeRO-Infinity vs ZeRO-Offload on the back propagation time of an 8B parameter model. ZeRO-Infinity leverages the aggregate PCIe bandwidth across GPUs to offload the gradients, resulting in a speedup of nearly 2x at 64 GPUs compared to ZeRO-Offload which is limited by single PCIe bandwidth.</p><p>Prefetching and Overlapping Figure <ref type="figure" target="#fig_8">6d</ref> shows the relative throughput difference with communication overlapping and prefetching turned on and off for an 8B parameter model with 64 GPUs. The figure shows that prefetching and overlapping are crucial to achieving good performance at small batch sizes per GPU, while its impact diminishes at large batch sizes.</p><p>Activation checkpoint offload Figure <ref type="figure" target="#fig_8">6e</ref> shows that CPU offloading of activation checkpoints in ZeRO-Infinity reduces the training throughput by up to 1.2x for small hidden sizes, but for hidden sizes 32K and 64K, the impact is minimal, demonstrating that it is possible to offload activation checkpoints to CPU memory without impacting efficiency for large hidden sizes.  <ref type="table">3</ref>: Bandwidth (bw) requirements for ZeRO-Infinity to remain efficient on a cluster of 512 accelerator devices with 10x and 100x more achievable compute than NVIDIA V100 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">CONCLUSION &amp; FUTURE IMPLICATIONS</head><p>In this paper, we presented ZeRO-Infinity, a novel heterogeneous system technology that leverages GPU, CPU, and NVMe memory to allow for unprecedented model scale that is accessible and easy to use while achieving excellent efficiency. It offers a paradigm shift in how we think about memory for large model training. It is no longer necessary to fit DL training on ultra-fast but expensive and limited memory like HBM2. ZeRO-Infinity demonstrates that it is possible to transcend the GPU memory wall by leveraging cheap and slow, but massive, CPU or NVMe memory in parallel across multiple devices to achieve the aggregate bandwidth necessary for efficient training on current generation of GPU clusters.</p><p>As we look into the future, the GPUs and other accelerators will become more powerful, and this aggregate bandwidth required for efficient training will also increase. Table <ref type="table">3</ref> shows that even when the compute of the accelerators increases by 10x compared to the NVIDIA V100 GPUs, on a cluster with 512 of them, ZeRO-Infinity only requires a bandwidth of 30 GB/s between each accelerator and the slow memory to remain efficient. In fact, this is already possible with today's technology by connecting accelerators to slow memory via NVLink <ref type="bibr" target="#b42">[43]</ref>. For example, the Summit Supercomputer launched in 2018 <ref type="bibr" target="#b43">[44]</ref> connects NVIDIA V100 GPUs with the CPU memory at 40GB/s per GPU.</p><p>It is clear that with ZeRO-Infinity, accelerator device memory is no longer a limitation on model scale or training efficiency. However, training models with tens or hundreds of trillions of parameters in a reasonable time still requires massive leaps in compute, and running efficiently on these future devices requires a proportional leap in device-to-device bandwidth (Table <ref type="table">3</ref>).</p><p>We hope that, with device memory no longer a limitation, ZeRO-Infinity will inspire a more compute and device-device bandwidth focused innovations of ultra-powerful accelerator devices and supercomputing clusters in the future to support the next 1000x growth in model scale and the advancements that they can offer.    Table 8: Model configurations for Figure 6(e)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX</head><note type="other">Figure</note></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: ZeRO-Infinity can train a model with 32 trillion parameters on 32 NVIDIA V100 DGX-2 nodes (512 GPUs), 50x larger than 3D parallelism, the existing state-of-the-art.</figDesc><graphic coords="1,320.36,193.47,235.44,111.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (a) Memory requirements for massive models. (b) Available memory and achievable bandwidth on NVIDIA V100 DGX-2 Cluster (The reported bandwidths represent per GPU bandwidth when all GPUs are reading data in parallel from the designated memory).bytes where 𝑐𝑖 is the number of Transformer blocks between two activation checkpoints, and 𝑏𝑠𝑧 × 𝑠𝑒𝑞 × ℎ𝑑 is the size of the input to each Transformer block. Figure2acolumn 7 shows the memory required to store activation checkpoints for batch size of 32 and sequence length of 1024 assuming we store one activation per Transformer block. Many modern GPU clusters have 8-16 GPUs per node, and so we chose a batch size of 2-4 per GPU, resulting in a batch size of 32 as a conservative estimate of activation within each node. While the resulting activation checkpoints are orders of magnitude smaller than the full set of activations (column 6) , beyond a trillion parameters they still get too large to fit in GPU memory for the batch size and sequence length under consideration.Model State Working Memory (MSWM) is the minimum amount of GPU memory required to perform forward or backward propagation on the largest single operator in the model after all the model states have been offload to CPU or NVMe. This is approximately given by the size of the parameters and gradients of that operator in the model, since there must be at least enough memory to hold the parameter and its gradient for backward propagation.For a Transformer based model, the largest operator is a linear layer that transforms hidden states from ℎ𝑑 to 4ℎ𝑑. The size of the parameter and gradients of this linear layer in bytes is</figDesc><graphic coords="4,53.80,83.69,226.98,73.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Impact of bandwidth on efficiency assuming an accelerator with 70 TFlops of single GPU peak achievable throughput.</figDesc><graphic coords="5,53.80,88.00,161.41,99.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>5. 1</head><label>1</label><figDesc>.2 CPU Offload for activations. In addition to model states, ZeRO-Infinity can offload activation memory to CPU memory, when necessary. Note that the activation checkpoints (0.76 TB) required by a 10 trillion parameter model can easily fit in the 1.5TB</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: A snapshot of ZeRO-Infinity training a model with two layers on four data parallel (DP) ranks. Communication for the backward pass of the first layer is depicted. Partitioned parameters are moved from slow memory to GPU and then collected to form the full layer. After gradients are computed, they are aggregated, repartitoned, and then offloaded to slow memory. Layers are denoted with subscripts and DP ranks are denoted with superscripts. For example, 𝑃(2)</figDesc><graphic coords="6,317.96,83.69,240.25,150.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><figDesc>(a) ZeRO-Infinity efficiently trains 40x larger models than 3D parallelism on 512 GPUs. (b) ZeRO-Infinity exceeds linear scaling from 64 to 512 GPUs for a 1T parameter model. (c) ZeRO-Infinity can train up to 1T model on a DGX-2 node without model parallelism.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Efficiency and scalability of ZeRO-Infinity for training multi-trillion parameter models.</figDesc><graphic coords="10,394.55,208.15,161.41,93.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><figDesc>(a) Max model size w.r.t. ZeRO strategies. (b) Max hidden dim. with different tiling factors. (c) ZeRO-Infinity vs ZeRO Offload (d) Speedup from communication overlap. (e) Overhead of offloading activation chkpt to CPU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Impact of system features on model scale and performance.</figDesc><graphic coords="11,155.94,99.01,95.84,66.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><figDesc>-GPU bw (GB/s) 70.0 700.0 7000.0 Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>6(a) Model sizeNumber of GPUs MP Layers Hidden Size Attention head Batch size Total batch size Model configurations for Figure6(a)</figDesc><table><row><cell>1.4B</cell><cell>16</cell><cell>1</cell><cell>40</cell><cell>1536</cell><cell>16</cell><cell>1</cell><cell>16</cell></row><row><cell>10B</cell><cell>16</cell><cell>1</cell><cell>50</cell><cell>4096</cell><cell>16</cell><cell>1</cell><cell>16</cell></row><row><cell>13B</cell><cell>16</cell><cell>1</cell><cell>64</cell><cell>4096</cell><cell>16</cell><cell>1</cell><cell>16</cell></row><row><cell cols="2">20B (ZeRO-3) 16</cell><cell>1</cell><cell>98</cell><cell>4096</cell><cell>32</cell><cell>1</cell><cell>16</cell></row><row><cell cols="2">20B (3D Par.) 16</cell><cell>4</cell><cell>98</cell><cell>4096</cell><cell>32</cell><cell>1</cell><cell>16</cell></row><row><cell>70B</cell><cell>16</cell><cell>1</cell><cell>125</cell><cell>8192</cell><cell>32</cell><cell>1</cell><cell>16</cell></row><row><cell>1000B</cell><cell>16</cell><cell>4</cell><cell>128</cell><cell>25600</cell><cell>256</cell><cell>5</cell><cell>20</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Figure 6(b)</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">Hidden size Number of GPUs MP Layers Model size Attention head Batch size Total batch size</cell></row><row><cell>8192</cell><cell>16</cell><cell>1</cell><cell>1</cell><cell>900M</cell><cell>16</cell><cell>1</cell><cell>16</cell></row><row><cell>16384</cell><cell>16</cell><cell>1</cell><cell>1</cell><cell>3B</cell><cell>16</cell><cell>1</cell><cell>16</cell></row><row><cell>32768</cell><cell>16</cell><cell>1</cell><cell>1</cell><cell>13B</cell><cell>16</cell><cell>1</cell><cell>16</cell></row><row><cell>65536</cell><cell>16</cell><cell>1</cell><cell>1</cell><cell>50B</cell><cell>32</cell><cell>1</cell><cell>16</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Model configurations forFigure 6(b) Figure 6(c) Number of GPUs Hidden size MP Layers Model size Attention head Batch size Total batch size<ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b31">32,</ref>64]   </figDesc><table><row><cell>8192</cell><cell>1</cell><cell>10</cell><cell>8B</cell><cell>16</cell><cell>2</cell><cell>[8,32,64,128]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Model configurations for Figure6(c)</figDesc><table><row><cell>Figure 6(d)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Model configurations forFigure 6(d) Figure 6(e) Hidden size Number of GPUs Opt Device MP Layers Model size Attention head Batch size Total batch size</figDesc><table><row><cell>2048</cell><cell>32</cell><cell>CPU</cell><cell>1</cell><cell>5</cell><cell>275M</cell><cell>16</cell><cell>4</cell><cell>128</cell></row><row><cell>8192</cell><cell>32</cell><cell>CPU</cell><cell>1</cell><cell>5</cell><cell>4B</cell><cell>16</cell><cell>4</cell><cell>128</cell></row><row><cell>16384</cell><cell>32</cell><cell>CPU</cell><cell>1</cell><cell>5</cell><cell>16B</cell><cell>16</cell><cell>4</cell><cell>128</cell></row><row><cell>32768</cell><cell>32</cell><cell>CPU</cell><cell>1</cell><cell>5</cell><cell>64B</cell><cell>16</cell><cell>4</cell><cell>128</cell></row><row><cell>65536</cell><cell>64</cell><cell>NVMe</cell><cell>1</cell><cell>5</cell><cell>260B</cell><cell>16</cell><cell>4</cell><cell>128</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>DeepSpeed (https://www.deepspeed.ai/) is a deep learning optimization library designed to make distributed training easy, efficient, and effective. DeepSpeed has been extensively adopted by the DL community.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://www.deepspeed.ai/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>In this paper, we make a distinction between model parallelism and pipeline parallelism, where the former is limited specifically to mean tensor-slicing based approaches, and does not include pipeline parallelism.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>Note that 𝑝𝑒𝑎𝑘 𝑡𝑝 is not the theoretical hardware peak, but instead the achievable peak in the absence of any communication bottleneck.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>Results will vary based on the value of 𝑝𝑒𝑎𝑘 𝑡𝑝 used, and this analysis is a single data point, meant as a guide for understanding relationship between efficiency and bandwidth for DL workloads specifically on the NVIDIA V100 DGX-2 clusters. Furthermore, the result only considers the relationship between efficiency and bandwidth of model states and activations, one at a time, assuming infinite bandwidth for others to isolate the bandwidth requirement for each state separately.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>CPU and NVMe bandwidth are in the order of 100 GB/s and 25 GB/s, respectively, but reading data from CPU or NVMe to a single GPU is limited by the achievable PCIe bandwidth which is around 10-12 GB/s</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_6"><p>EFFICIENCY OPTIMIZATIONSIn this section, we deep dive into the optimizations introduced in Sec. 5 that allow ZeRO-Infinity to achieve excellent efficiency.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENT</head><p>We thank <rs type="person">Elton Zheng</rs>, <rs type="person">Reza Yazdani Aminabadi</rs>, <rs type="person">Arash Ashari</rs> for their help on improving various components of the code, and <rs type="person">Cheng Li</rs> for her help in proof reading the paper. We thank <rs type="person">Andrey Proskurin</rs>, <rs type="person">Gopi Kumar</rs>, <rs type="person">Junhua Wang</rs>, <rs type="person">Mikhail Parakhin</rs>, and <rs type="person">Rangan Majumder</rs> for their continuous support.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">BERT: pretraining of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>CoRR, abs/1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Scaling laws for neural language models</title>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Mark</forename><surname>Matthew E Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Megatron-lm: Training multi-billion parameter language models using model parallelism</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Pipedream: Fast and efficient pipeline parallel DNN training</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Harlap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amar</forename><surname>Phanishayee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Seshadri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><forename type="middle">R</forename><surname>Devanur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">R</forename><surname>Ganger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><forename type="middle">B</forename><surname>Gibbons</surname></persName>
		</author>
		<idno>CoRR, abs/1806.03377</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Gpipe: Efficient training of giant neural networks using pipeline parallelism</title>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<idno>ArXiv, abs/1811.06965</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amar</forename><surname>Phanishayee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiyu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09503</idno>
		<title level="m">Memory-efficient pipeline-parallel dnn training</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">ZeRO: Memory Optimizations toward Training Trillion Parameter Models</title>
		<author>
			<persName><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Rasley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olatunji</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC &apos;20</title>
		<meeting>the International Conference for High Performance Computing, Networking, Storage and Analysis, SC &apos;20</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reza</forename><surname>Yazdani Aminabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olatunji</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuangyan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjia</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><surname>Zero-Offload</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Democratizing Billion-Scale Model Training</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">DeepSpeed: Extreme-scale model training for everyone</title>
		<ptr target="https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/" />
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Microsoft</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<ptr target="https://github.com/NVIDIA/Megatron-LM" />
		<title level="m">Megatron-LM: Software repository</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">DeepSpeed: Extreme-scale model training for everyone</title>
		<author>
			<persName><forename type="first">Deepspeed</forename><surname>Team</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<ptr target="https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Ai and memory wall</title>
		<author>
			<persName><forename type="first">Amir</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sehoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>RiseLab Medium Post</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Mesh-tensorflow: Deep learning for supercomputers</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youlong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Penporn</forename><surname>Koanantakool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingsheng</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blake</forename><forename type="middle">A</forename><surname>Hechtman</surname></persName>
		</author>
		<idno>CoRR, abs/1811.02084</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Supporting very large models using automatic dataflow graph partitioning</title>
		<author>
			<persName><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chien-Chin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth EuroSys Conference 2019, EuroSys &apos;19</title>
		<meeting>the Fourteenth EuroSys Conference 2019, EuroSys &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Demystifying parallel and distributed deep learning: An in-depth concurrency analysis</title>
		<author>
			<persName><forename type="first">Tal</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torsten</forename><surname>Hoefler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="43" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Autotm: Automatic tensor movement in heterogeneous memory systems using integer linear programming</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hildebrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jawad</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Trika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Lowe-Power</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Venkatesh</forename><surname>Akella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="875" to="890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Swapadvisor: Pushing deep learning beyond the gpu memory limit via smart swapping</title>
		<author>
			<persName><forename type="first">Chien-Chin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gu</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1341" to="1355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Layer-centric memory reuse and data migration for extremescale deep learning on many-core architectures</title>
		<author>
			<persName><forename type="first">Hai</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanhua</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingsheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaofeng</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Architecture and Code Optimization (TACO)</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Capuchin: Tensor-based gpu memory management for deep learning</title>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanhua</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hulin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiliang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuehai</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="891" to="905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sentinel: Efficient tensor migration and allocation on heterogeneous memory systems for deep learning</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaolin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjia</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyeran</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on High Performance Computer Architecture</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">vdnn: Virtualized deep neural networks for scalable, memory-efficient neural network design</title>
		<author>
			<persName><forename type="first">Minsoo</forename><surname>Rhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Clemons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arslan</forename><surname>Zulfiqar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Superneurons: Dynamic gpu memory management for training deep neural networks</title>
		<author>
			<persName><forename type="first">Linnan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinmian</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiyang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuaiwen</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Leon</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zenglin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Kraska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGPLAN symposium on principles and practice of parallel programming</title>
		<meeting>the 23rd ACM SIGPLAN symposium on principles and practice of parallel programming</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="41" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Distributed hierarchical gpu parameter server for massive scale deep learning ads systems</title>
		<author>
			<persName><forename type="first">Weijie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deping</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronglai</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulei</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiquan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Lingjia Tang, and Gennady Pekhimenko. Gist: Efficient data encoding for deep neural network training</title>
		<author>
			<persName><forename type="first">Animesh</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amar</forename><surname>Phanishayee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Mars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Computer Architecture (ISCA 2018)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Training deep nets with sublinear memory cost</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<idno>CoRR, abs/1604.06174</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Checkmate: Breaking the memory wall with optimal tensor rematerialization</title>
		<author>
			<persName><forename type="first">Paras</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aniruddha</forename><surname>Nrusimha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<idno>ArXiv, abs/1910.02653</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Superneurons: Dynamic GPU memory management for training deep neural networks</title>
		<author>
			<persName><forename type="first">Linnan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinmian</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiyang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuaiwen</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Leon</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zenglin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Kraska</surname></persName>
		</author>
		<idno>CoRR, abs/1801.04380</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07">July 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR 2015</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">May 7-9, 2015. 2015</date>
		</imprint>
	</monogr>
	<note>Yoshua Bengio and Yann LeCun Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Scaling SGD batch size to 32k for imagenet training</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Gitman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno>CoRR, abs/1708.03888</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Reducing BERT pre-training time from 3 days to 76 minutes</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Hseu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<idno>CoRR, abs/1904.00962</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Paulius</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonah</forename><surname>Alben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksii</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganesh</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Mixed precision training</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cores</forename><surname>Nvidia Tensor</surname></persName>
		</author>
		<ptr target="https://www.nvidia.com/en-us/data-center/tensor-cores/" />
		<imprint>
			<date type="published" when="2018-04">2018. -April-2021</date>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno>CoRR, abs/1706.03762</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m">17-billion-parameter language model by microsoft</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">NVIDIA DGX SuperPOD delivers world record supercomputing to any enterprise</title>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<ptr target="https://developer.nvidia.com/blog/dgx-superpod-world-record-supercomputing-enterprise/" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Shen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omkar</forename><surname>Salpekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Vaughan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pritam</forename><surname>Damania</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.15704</idno>
		<title level="m">Pytorch distributed: Experiences on accelerating data parallel training</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Ultra-performance pascal gpu and nvlink interconnect</title>
		<author>
			<persName><forename type="first">Denis</forename><surname>Foley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Danskin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>IEEE Micro</publisher>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="7" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<ptr target="https://www.ornl.gov/news/ornl-launches-summit-supercomputer" />
		<title level="m">ORNL Launches Summit Supercomputer</title>
		<imprint>
			<date type="published" when="2018-04">2018. April-2021</date>
			<biblScope unit="volume">08</biblScope>
		</imprint>
		<respStmt>
			<orgName>Oak Ridge National Laboratory</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
