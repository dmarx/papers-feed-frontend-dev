{
  "arxivId": "2406.07522",
  "title": "Samba: Simple Hybrid State Space Models for Efficient Unlimited Context\n  Language Modeling",
  "authors": "Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, Weizhu Chen",
  "abstract": "Efficiently modeling sequences with infinite context length has long been a\nchallenging problem. Previous approaches have either suffered from quadratic\ncomputational complexity or limited extrapolation ability in length\ngeneralization. In this work, we present Samba, a simple hybrid architecture\nthat layer-wise combines Mamba, a selective State Space Model (SSM), with\nSliding Window Attention (SWA). Samba selectively compresses a given sequence\ninto recurrent hidden states while still maintaining the ability to precisely\nrecall recent memories with the attention mechanism. We scale Samba up to 3.8B\nparameters with 3.2T training tokens and demonstrate that it significantly\noutperforms state-of-the-art models across a variety of benchmarks. Pretrained\non sequences of 4K length, Samba shows improved perplexity in context lengths\nof up to 1M in zero-shot. When finetuned on 4K-length sequences, Samba\nefficiently extrapolates to a 256K context length with perfect memory recall on\nthe Passkey Retrieval task, and exhibits superior retrieval extrapolation on\nthe challenging Phonebook task compared to full-attention models. As a\nlinear-time sequence model, Samba achieves a 3.73x higher throughput compared\nto Transformers with grouped-query attention for user prompts of 128K length,\nand a 3.64x speedup when generating 64K tokens with unlimited streaming. Our\ncode for training on open source data is publicly available at\nhttps://github.com/microsoft/Samba.",
  "url": "https://arxiv.org/abs/2406.07522",
  "issue_number": 985,
  "issue_url": "https://github.com/dmarx/papers-feed/issues/985",
  "created_at": "2025-01-17T05:34:38.132852",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 12,
  "last_read": "2025-01-17T05:37:04.997640",
  "last_visited": "2025-01-17T05:35:26.692000+00:00",
  "main_tex_file": null,
  "published_date": "2024-06-11T17:50:51Z",
  "arxiv_tags": [
    "cs.CL",
    "cs.LG"
  ]
}