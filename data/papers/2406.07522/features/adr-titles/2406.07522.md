- Decision to use a hybrid architecture combining Mamba and Sliding Window Attention (SWA)
- Choice of layer-wise interleaving for Mamba and SWA
- Selection of Mamba as the backbone for efficient decoding
- Implementation of selective state spaces in Mamba
- Use of SwiGLU activation function in MLP layers
- Decision to scale SAMBA up to 3.8B parameters
- Choice of training on 3.2T tokens
- Selection of 4K sequence length for pre-training
- Decision to enable extrapolation to 1M context length
- Use of FlashAttention 2 for efficient self-attention implementation
- Choice of 2048 sliding window size for SWA
- Decision to apply RoPE within the sliding window
- Implementation of a gating mechanism similar to Gated Linear Unit
- Choice of hyperparameters for different model sizes
- Decision to conduct extensive ablation studies across model sizes
- Selection of benchmarks for performance evaluation
- Decision to focus on commonsense reasoning, language understanding, truthfulness, and math/coding tasks
- Choice of specific datasets for downstream evaluations
- Decision to make the code publicly available on GitHub
- Implementation of hardware-aware parallel scan algorithm for Mamba layers
- Decision to explore other linear recurrent layers as potential substitutions for Mamba