<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SAMBA: SIMPLE HYBRID STATE SPACE MODELS FOR EFFICIENT UNLIMITED CONTEXT LANGUAGE MODELING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-12-03">3 Dec 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Liliang</forename><surname>Ren</surname></persName>
							<email>liliangren@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yadong</forename><surname>Lu</surname></persName>
							<email>yadonglu@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yelong</forename><surname>Shen</surname></persName>
							<email>yelong.shen@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chen</forename><surname>Liang</surname></persName>
							<email>chenliang1@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
							<email>wzchen@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName><surname>Microsoft</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SAMBA: SIMPLE HYBRID STATE SPACE MODELS FOR EFFICIENT UNLIMITED CONTEXT LANGUAGE MODELING</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-12-03">3 Dec 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">AD17645135F0882468E673209C97CD28</idno>
					<idno type="arXiv">arXiv:2406.07522v2[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Efficiently modeling sequences with infinite context length has long been a challenging problem. Previous approaches have either suffered from quadratic computational complexity or limited extrapolation ability in length generalization. In this work, we present SAMBA, a simple hybrid architecture that layer-wise combines Mamba, a selective State Space Model (SSM), with Sliding Window Attention (SWA). SAMBA selectively compresses a given sequence into recurrent hidden states while still maintaining the ability to precisely recall recent memories with the attention mechanism. We scale SAMBA up to 3.8B parameters with 3.2T training tokens and demonstrate that it significantly outperforms state-of-the-art models across a variety of benchmarks. Pretrained on sequences of 4K length, SAMBA shows improved perplexity in context lengths of up to 1M in zero-shot. When finetuned on 4K-length sequences, SAMBA efficiently extrapolates to a 256K context length with perfect memory recall on the Passkey Retrieval task, and exhibits superior retrieval extrapolation on the challenging Phonebook task compared to full-attention models. As a linear-time sequence model, SAMBA achieves a 3.73× higher throughput compared to Transformers with grouped-query attention for user prompts of 128K length, and a 3.64× speedup when generating 64K tokens with unlimited streaming. Our code for training on open source data is publicly available at <ref type="url" target="https://github.com/microsoft/Samba">https://github.com/microsoft/Samba</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Attention-based models <ref type="bibr" target="#b83">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b6">Bahdanau et al., 2014)</ref> have dominated the neural architectures of Large Language Models (LLMs) <ref type="bibr" target="#b62">(Radford et al., 2019;</ref><ref type="bibr" target="#b10">Brown et al., 2020;</ref><ref type="bibr">OpenAI, 2023;</ref><ref type="bibr" target="#b11">Bubeck et al., 2023)</ref> due to their ability to capture complex long-term dependencies and the efficient parallelization for large-scale training <ref type="bibr">(Dao et al., 2022a)</ref>. Recently, State Space Models (SSMs) <ref type="bibr" target="#b28">(Gu et al., 2021;</ref><ref type="bibr">Smith et al., 2023;</ref><ref type="bibr">Gu et al., 2022;</ref><ref type="bibr" target="#b27">Gu &amp; Dao, 2023)</ref> have emerged as a promising alternative, offering linear computation complexity and the potential for better extrapolation to longer sequences than seen during training. Specifically, Mamba <ref type="bibr" target="#b27">(Gu &amp; Dao, 2023)</ref>, a variant of SSMs equipped with selective state spaces, has demonstrated notable promise through strong empirical performance and efficient hardware-aware implementation. Recent work also shows that transformers have poorer modeling capacities than input-dependent SSMs in state tracking problems <ref type="bibr" target="#b49">(Merrill et al., 2024)</ref>. However, SSMs struggle with memory recall due to their recurrent nature <ref type="bibr" target="#b3">(Arora et al., 2023)</ref>, and experimental results on information retrieval-related tasks <ref type="bibr">(Fu et al., 2023;</ref><ref type="bibr" target="#b87">Wen et al., 2024;</ref><ref type="bibr" target="#b4">Arora et al., 2024)</ref>, have further shown that SSMs are not as competitive as their attention-based counterparts.</p><p>Previous works <ref type="bibr" target="#b100">(Zuo et al., 2022;</ref><ref type="bibr">Fu et al., 2023;</ref><ref type="bibr" target="#b45">Ma et al., 2023;</ref><ref type="bibr" target="#b65">Ren et al., 2023)</ref> have explored various approaches to hybridize SSMs with the attention mechanism, but none have demonstrated significantly better language modeling performance compared to state-of-the-art Transformer architectures. Existing length extrapolation techniques <ref type="bibr" target="#b30">(Han et al., 2023;</ref><ref type="bibr" target="#b89">Xiao et al., 2023;</ref><ref type="bibr" target="#b23">Jin et al., Preprint 2024</ref>) designed for attention mechanisms are constrained by quadratic computational complexity or insufficient context extrapolation performance, particularly when evaluated under perplexity metrics. In this paper, we introduce SAMBA, a simple neural architecture that harmonizes the strengths of both the SSM and the attention-based models, while achieving a potentially infinite length extrapolation with linear time complexity. SAMBA combines SSMs with attention through layer-wise interleaving Mamba <ref type="bibr" target="#b27">(Gu &amp; Dao, 2023)</ref>, SwiGLU <ref type="bibr" target="#b73">(Shazeer, 2020)</ref>, and Sliding Window Attention (SWA) <ref type="bibr" target="#b7">(Beltagy et al., 2020)</ref>. Mamba layers capture the time-dependent semantics and provide a backbone for efficient decoding, while SWA fills in the gap modeling complex, non-recurrent dependencies. A detailed discussion of related work is included in Appendix A.</p><p>We scale SAMBA with 421M, 1.3B, 1.7B and up to 3.8B parameters with 3.2T tokens. In particular, the largest 3.8B post-trained model achieves a 47.9 score for MMLU-Pro <ref type="bibr" target="#b33">(Hendrycks et al., 2021)</ref>, 70.1 for HumanEval <ref type="bibr" target="#b12">(Chen et al., 2021)</ref>, and 86.4 for GSM8K <ref type="bibr" target="#b18">(Cobbe et al., 2021)</ref>, substantially outperforming strong open source language models up to 8B parameters, as detailed in Table <ref type="table" target="#tab_7">8</ref>. Despite being pre-trained in the 4K sequence length, SAMBA can be extrapolated to 1M length in zero shot with improved perplexity on Proof-Pile <ref type="bibr" target="#b98">(Zhangir Azerbayev &amp; Piotrowski, 2022)</ref>, achieving a 256× extrapolation ratio, while still maintaining the linear decoding time complexity with unlimited token streaming, as shown in Figure <ref type="figure">2</ref>. We show that when instruction-tuned in a 4K context length with only 500 steps, SAMBA can be extrapolated to a 256K context length with perfect memory recall in Passkey Retrieval <ref type="bibr" target="#b52">(Mohtashami &amp; Jaggi, 2023)</ref>. In contrast, the fine-tuned SWA-based model simply cannot recall memories beyond 4K length. We further demonstrate that the instruction-tuned SAMBA 3.8B model can achieve significantly better performance than the SWA-based models on downstream long-context summarization tasks, while still keeping its impressive performance on the short-context benchmarks. In a more challenging multiple key-value retrieval task, Phonebook <ref type="bibr" target="#b36">(Jelassi et al., 2024)</ref>, we demonstrate that instruction fine-tuning enables SAMBA to bridge the retrieval performance gap with full-attention models, while exhibiting significantly better extrapolation ability when retrieving phone numbers beyond the training context length. Finally, we perform extensive analyzes and ablation studies across model sizes up to 1.7B parameters to validate the architectural design of SAMBA. We also offer potential explanations for the effectiveness of our simple hybrid approach through the lens of attention/selection entropy. To the best of our knowledge, Samba is the first hybrid model showing that linear complexity models can be substantially better than state-of-the-art Transformer models on short-context tasks at large scale, while still being able to extrapolate to extremely long sequences under the perplexity metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHODOLOGY</head><p>We explore different hybridization strategies consisting of the layers of Mamba, Sliding Window Attention (SWA), and Multi-Layer Perceptron <ref type="bibr" target="#b73">(Shazeer, 2020;</ref><ref type="bibr">Dauphin et al., 2016)</ref>. We conceptualize the functionality of Mamba as the capture of recurrent sequence structures, SWA as the precise retrieval of memory, and MLP as the recall of factual knowledge. We also explore other linear recurrent layers including Multi-Scale Retention <ref type="bibr" target="#b77">(Sun et al., 2023)</ref> and GLA <ref type="bibr" target="#b94">(Yang et al., 2023)</ref> as potential substitutions for Mamba in Section 3.2. Our goal of hybridization is to harmonize between these distinct functioning blocks and find an efficient architecture for language modeling with unlimited length extrapolation ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">ARCHITECTURE</head><p>As illustrated in Figure <ref type="figure">1</ref>, we explore three kinds of layerwise hybridization strategies on the 1.7B scale: Samba, Mamba-SWA-MLP, and Mamba-MLP. We also explore other hybridization approaches with full self-attention on smaller scales in Section 4. The number of layers N is set to 48 for Samba, Mamba-MLP, and Mamba, while Mamba-SWA-MLP has 54 layers, so each model has approximately 1.7B parameters. We only modify the layer-level arrangement for each of the models and keep every other configuration the same to have apple-to-apple comparisons. More details on the configuration of each layer are explained in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">MAMBA LAYER</head><p>Mamba <ref type="bibr" target="#b27">(Gu &amp; Dao, 2023</ref>) is a recently proposed SSM-based model with selective state spaces. It enables input-dependent gating to both the recurrent states and the input representation for a soft Figure <ref type="figure">1</ref>: From left to right: Samba, Mamba-SWA-MLP, Mamba-MLP, and Mamba. The illustrations depict the layer-wise integration of Mamba with various configurations of Multi-Layer Perceptrons (MLPs) and Sliding Window Attention (SWA). We assume the total number of intermediate layers to be N , and omit the embedding layers and output projections for simplicity. Pre-Norm <ref type="bibr" target="#b91">(Xiong et al., 2020;</ref><ref type="bibr" target="#b97">Zhang &amp; Sennrich, 2019)</ref> and skip connections <ref type="bibr" target="#b31">(He et al., 2016)</ref> are applied for each of the intermediate layers.</p><p>selection of the input sequence elements. Given an input sequence representation X ∈ R n×dm , where n is the length of the sequence and d m is the hidden size, Mamba first expands the inputs to a higher dimension d e , i.e., H = XW in ∈ R n×de where W in ∈ R dm×de is a learnable projection matrix. Then a Short Convolution (SC) <ref type="bibr" target="#b3">(Poli et al., 2023)</ref> operator is applied to smooth the input signal,</p><formula xml:id="formula_0">U = SC(H) = SiLU(DepthwiseConv(H, W conv )) ∈ R n×de<label>(1)</label></formula><p>where W conv ∈ R k×de and the kernel size k is set to 4 for hardware-aware efficiency. The Depthwise Convolution <ref type="bibr" target="#b32">(He et al., 2019)</ref> is applied over the sequence dimension followed by a SiLU <ref type="bibr" target="#b24">(Elfwing et al., 2017)</ref> activation function. The selective gate is then calculated through a low-rank projection followed by <ref type="bibr">Softplus (Zheng et al., 2015)</ref>,</p><formula xml:id="formula_1">∆ = Softplus(UW r W q + b) ∈ R n×de (2)</formula><p>where W r ∈ R de×dr , W q ∈ R dr×de and d r is the low-rank dimension. b ∈ R de is carefully initialized so that ∆ ∈ [∆ min , ∆ max ] after the initialization stage. We set [∆ min , ∆ max ] = [0.001, 0.1], and find that these values are not sensitive to language modeling performance under the perplexity metric. The input dependence is also introduced for the parameters B and C of SSM,</p><formula xml:id="formula_2">B = UW b ∈ R n×ds C = UW c ∈ R n×ds where d s is the state dimension. For each time step 1 ≤ t ≤ n, the recurrent inference of the Selective SSM (S6) is performed in an expanded state space Z t ∈ R de×ds , i.e., Z t = exp(-∆ t ⊙ exp(A)) ⊙ Z t-1 + ∆ t ⊙ (B t ⊗ U t ) ∈ R de×ds Y t = Z t C t + D ⊙ U t ∈ R de</formula><p>where Z 0 = 0, ⊙ means the point-wise product, ⊗ means the outer product and exp means the point-wise natural exponential function. D ∈ R de is a learnable vector initialized as D i = 1 and A ∈ R de×ds is a learnable matrix initialized as A ij = log(j), 1 ≤ j ≤ d s , following the S4D-Real <ref type="bibr">(Gu et al., 2022)</ref> initialization. In practice, Mamba implements a hardware-aware parallel scan algorithm for efficient parallelizable training. The final output is obtained through a gating mechanism similar to Gated Linear Unit <ref type="bibr" target="#b73">(Shazeer, 2020;</ref><ref type="bibr">Dauphin et al., 2016)</ref>,</p><formula xml:id="formula_3">O = Y ⊙ SiLU(XW g )W out ∈ R n×dm</formula><p>where W g ∈ R dm×de and W out ∈ R de×dm are learnable parameters. In this work, we set d e = 2d m , d r = d m /16, and d s = 16. The Mamba layer in SAMBA is expected to capture the time-dependent semantics of the input sequence through its recurrent structure. The input selection mechanism in the Mamba layer enables the model to focus on relevant inputs, thereby allowing the model to memorize important information in the long term. We include Sliding Window Attention <ref type="bibr" target="#b7">(Beltagy et al., 2020)</ref> layers to address the limitations of Mamba layers in capturing non-recurrent dependencies in sequences. Our SWA layer operates on a window size w = 2048 that slides over the input sequence, ensuring that the computational complexity remains linear with respect to the sequence length. RoPE <ref type="bibr" target="#b76">(Su et al., 2021)</ref> is applied within the sliding window, with a base frequency of 10,000. By directly accessing the contents in the context window through attention, the SWA layer can retrieve high-definition signals from the middle to short-term history that cannot be clearly captured by the recurrent states of Mamba. We use FlashAttention 2 <ref type="bibr" target="#b20">(Dao, 2023)</ref> for the efficient implementation of self-attention throughout this work. We also choose the 2048 sliding window size for efficiency consideration; FlashAttention 2 has the same training speed as Mamba's selective parallel scan at the sequence length of 2048 based on the measurements in <ref type="bibr" target="#b27">(Gu &amp; Dao, 2023)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">MULTI-LAYER PERCEPTRON (MLP) LAYER</head><p>The MLP layers in SAMBA serve as the architecture's primary mechanism for nonlinear transformation and recall of factual knowledge <ref type="bibr" target="#b19">(Dai et al., 2022)</ref>. We use SwiGLU <ref type="bibr" target="#b73">(Shazeer, 2020)</ref> for all the models trained in this paper and denote its intermediate hidden size as d p . As shown in Figure <ref type="figure">1</ref>, Samba applies separate MLPs for different types of information captured by Mamba and the SWA layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS AND RESULTS</head><p>We pre-train four SAMBA models with different parameter sizes, 421M, 1.3B, 1.7B and 3.8B, to investigate its performance across different scales. The details of the hyperparameters for the training and architecture designs are shown in Table <ref type="table" target="#tab_1">12</ref> of Appendix G. We also train other hybrid architectures as mentioned in Section 2.1, including the baseline Mamba <ref type="bibr" target="#b27">(Gu &amp; Dao, 2023)</ref>, <ref type="bibr">Llama-3 (MetaAI, 2024;</ref><ref type="bibr">Dubey et al., 2024)</ref>, and Mistral <ref type="bibr" target="#b37">(Jiang et al., 2023)</ref> architecture on a scale of around 1.7B, with detailed hyperparameters in Table <ref type="table">11</ref> of Appendix G. We do comprehensive downstream evaluations on a wide range of benchmarks, focusing on four main capabilities of the models: commonsense reasoning (ARC <ref type="bibr" target="#b17">(Clark et al., 2018)</ref>, PIQA <ref type="bibr" target="#b8">(Bisk et al., 2020)</ref>, WinoGrande <ref type="bibr" target="#b67">(Sakaguchi et al., 2021)</ref>, SIQA <ref type="bibr" target="#b68">(Sap et al., 2019)</ref>), language understanding (HellaSwag <ref type="bibr" target="#b96">(Zellers et al., 2019)</ref>, BoolQ <ref type="bibr" target="#b16">(Clark et al., 2019)</ref>, OpenbookQA <ref type="bibr" target="#b51">(Mihaylov et al., 2018)</ref>, SQuAD <ref type="bibr" target="#b63">(Rajpurkar et al., 2016)</ref>, MMLU <ref type="bibr" target="#b33">(Hendrycks et al., 2021)</ref>, MMLU-Pro <ref type="bibr" target="#b85">(Wang et al., 2024)</ref>, GPQA <ref type="bibr" target="#b64">(Rein et al., 2023)</ref>), truthfulness (TruthfulQA <ref type="bibr" target="#b43">(Lin et al., 2022)</ref>) and math and coding (GSM8K <ref type="bibr" target="#b18">(Cobbe et al., 2021)</ref>, MBPP <ref type="bibr" target="#b5">(Austin et al., 2021)</ref>, HumanEval <ref type="bibr" target="#b12">(Chen et al., 2021)</ref>).</p><p>Table <ref type="table">1</ref>: Downstream performance comparison between Samba-3.8B-IT (preview) and Phi-3-mini-4K on both long-context and short-context tasks. We report 5-shot accuracy (averaged by category) for MMLU, 8-shot CoT <ref type="bibr" target="#b86">(Wei et al., 2022)</ref> for GSM8K, 0-shot pass@1 for HumanEval, ROUGE-L for both GovReport and SQuALITY. † Results from the Phi-3 technical report <ref type="bibr">(Abdin et al., 2024)</ref>.</p><formula xml:id="formula_4">Model MMLU GSM8K HumanEval GovReport SQuALITY Phi-3-mini-4K-</formula><p>instruct † 68.8 82.5 58.5 14.4 21.6 Samba-3.8B-IT (preview) 71.9 87.6 62.8 18.9 21.2 3.1 LANGUAGE MODELING ON TEXTBOOK QUALITY DATA</p><p>We first present results from our largest 3.8B SAMBA model, trained on the same data set used by Phi3 <ref type="bibr">(Abdin et al., 2024)</ref> with 3.2T tokens. We follow the same multiphase pretraining strategy as Phi3-mini, and apply both the original Phi-3-mini post-training recipe and the Phi3-mini-June-2024 recipe to produce our instruction-tuned SAMBA 3.8B models, i.e., Samba-3.8B-IT (preview) and Samba-3.8B (June) respectively. We report comprehensive benchmark results of the Samba 3.8B base model and Samba-3.8B (June) in Appendix B. As shown in Table <ref type="table">1</ref>, we evaluate the downstream performance of Samba-3.8B-IT (preview) on both long-context summarization tasks <ref type="bibr">(GovReport (Huang et al., 2021)</ref>, SQuALITY <ref type="bibr">(Wang et al., 2022)</ref>) and major short-context benchmarks (MMLU, GSM8K, HumanEval). We can see that Samba has substantially better performance than Phi-3-mini-4k-instruct on both the short-context (MMLU, GSM8K, HumanEval) and long-context (GovReport) Preprint tasks, while still having the 2048 window size of its SWA layer and maintaining the linear complexity for efficient processing of long documents. Details of data statistics and evaluation setup for long context tasks are included in Appendix F. To examine the different hybridization strategies mentioned in Section 2.1, we train 6 models with around 1.7B parameters on the Phi2 <ref type="bibr" target="#b41">(Li et al., 2023)</ref> dataset with 230B tokens and evaluate them in the full suite of 15 downstream benchmarks to have a holistic assessment of hybrid and purebred architectures. As shown in Table <ref type="table" target="#tab_1">2</ref>, SAMBA demonstrates superior performance on a diverse set of tasks, including commonsense reasoning (ARC-Challenge), language understanding (MMLU, SQuAD), TruthfulQA and code generation (HumanEval, MBPP). It outperforms both the pure attention-based and SSM-based models in most tasks and achieves the best average performance. By comparing the performance of Mamba-MLP and Mamba in Table <ref type="table" target="#tab_1">2</ref>, we can observe that replacing Mamba blocks with MLPs does not harm common sense reasoning ability, but its performance in language understanding and complex reasoning ability, such as coding and mathematical reasoning, degenerates significantly. We can also see that pure Mamba models fall short on retrieval intensive tasks such as SQuAD due to their lack of precise memory retrieval ability. The best results are achieved through the combination of the attention and Mamba modules, as shown with our Samba architecture. We can also notice that Mamba-SWA-MLP has significantly better performance on GSM8K, potentially resulting from a closer collaboration between the Mamba and the SWA layers.</p><p>The distinct downstream performances of different hybridization strategies pose interesting future work for developing task-adaptive dynamic architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">EXPLORATION ON HYBRIDIZING ATTENTION AND LINEAR RECURRENCE</head><p>Since SSMs belong to a broader realm of linear recurrent models <ref type="bibr">(Orvieto et al., 2023;</ref><ref type="bibr" target="#b60">Qin et al., 2023;</ref><ref type="bibr" target="#b94">Yang et al., 2023;</ref><ref type="bibr" target="#b39">Katsch, 2023;</ref><ref type="bibr" target="#b61">Qin et al., 2024)</ref>, there exist multiple alternatives other than Mamba when combing attention-based layers with recurrent neural networks. We also add architecture ablation studies to justify the design choices of Samba. Specifically, in addition to Llama-2, Mamba, Samba and Mamba-SWA-MLP, we investigate the comparative analysis of the following architectures:</p><p>• Llama-2-SWA is a pure attention-based architecture that replaces all full attention layers in Llama-2 with sliding window attention. • Sliding RetNet replaces Mamba layers in the Samba architecture with Multi-Scale Retention <ref type="bibr" target="#b77">(Sun et al., 2023)</ref> layers. RetNet is a linear attention model with fixed and input-independent decay applying to the recurrent hidden states. • Sliding GLA replaces Mamba layers in the Samba architecture with Gated Linear Attention (GLA) <ref type="bibr" target="#b94">(Yang et al., 2023)</ref>. GLA is a more expressive variant of linear attention with input-dependent gating. • Mega-S6 replaces all MD-EMA modules in the Mega <ref type="bibr" target="#b45">(Ma et al., 2023)</ref> architecture with the ShortConv+S6 combinations from Mamba to adapt Mega to the modern Mamba architecture. Rotary position embedding, RMSNorm and Softmax attention are also adopted. We set the intermediate dimension of the Mega-S6 layer to be d m so that it has a roughly 5d 2 m number of parameters. This represents a classical baseline that conducts sequential intra-layer SSM-Attention hybridization.</p><p>• MLP2-SWA-MLP replaces all Mamba layers in the Samba architecture to SwiGLU layers with 6d 2 m number of parameters. • Samba-NoPE removes the rotary relative position embedding in Samba and does not have any position embedding in the architecture.</p><p>We pre-train all models on the same SlimPajama <ref type="bibr" target="#b75">(Soboleva et al., 2023)</ref>  In Table <ref type="table" target="#tab_3">4</ref>, we evaluate all our 1.3B scale models on five typical commonsense reasoning tasks (ARC-Easy, HellaSwag, WinoGrande, PIQA and the OpenAI variant<ref type="foot" target="#foot_0">foot_0</ref> of LAMBADA <ref type="bibr" target="#b56">(Paperno et al., 2016)</ref> ) to understand the effect of architecture designs on downstream performances. We can see that Samba has the best average accuracy, outperforming the LLaMA 2 architectures by a large margin. Similar to our perplexity evaluation, Samba and Samba-NoPE have similar average accuracies, whereas Mamba-SWA-MLP falls slightly behind. We observe that different architectures excel at different tasks. Mamba-SWA-MLP performs best on ARC-Easy, while Samba and Samba-NoPE achieve superior results on LAMBADA. Hybrid models based on Mamba generally outperform hybrid linear attention models and pure softmax-attention models on HellaSwag. Figure <ref type="figure">2</ref>: SAMBA shows improved prediction up to 1M tokens in the Proof-Pile test set while achieving a 3.64× faster decoding throughput than the Llama-3 architecture on 64K generation length. We also include an SE-Llama-3 1.6B baseline which applies the SelfExtend <ref type="bibr" target="#b38">(Jin et al., 2024)</ref> approach for zero-shot length extrapolation. All models are trained with 4K sequence length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">EFFICIENT LENGTH EXTRAPOLATION</head><p>We use the test split of the Proof-Pile (Zhangir Azerbayev &amp; Piotrowski, 2022) dataset to evaluate the length extrapolation ability of our models at a scale of around 1.7B parameters. We follow Position</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preprint</head><p>Interpolation <ref type="bibr">(Chen et al., 2023a)</ref> for data pre-processing. The sliding window approach <ref type="bibr" target="#b59">(Press et al., 2021)</ref> is used for the perplexity evaluation with a window size of 4096. Besides having the decoding throughput in Figure <ref type="figure">2</ref> for the generation efficiency metric, we also measure the prompt processing speed in Figure <ref type="figure">6</ref> of Appendix B for the models SAMBA 1.7B, Mistral 1.6B, Mamba 1.8B, Llama-3 1.6B and its Self-Extended <ref type="bibr" target="#b38">(Jin et al., 2024)</ref> version SE-Llama-3 1.6B with the prompt length sweeping from 1K to 128K. We set the group size to 4 and the neighborhood window to 1024 for Self-Extension. We fix the total processing tokens per measurement to be 128K and varying the batch size accordingly. The throughput is measured on a single A100 GPU with the precision of bfloat16. We repeat the measurements 10 times and report the averaged results. We can see that Samba achieves 3.73× higher throughput in prompt processing compared to Llama-3 1.6B at the 128K prompt length, and the processing time remains linear with respect to the sequence length.</p><p>We can also observe that the existing zero-shot length extrapolation technique introduces significant inference latency overhead on the full-attention counterpart, while it still cannot extrapolate infinitely with perplexity performance comparable to that of Samba. In Figure <ref type="figure">2</ref>, we can also see that Mamba has a slowly and stably increasing perplexity up to 1M sequence length, which indicates that linear recurrent models can still not extrapolate infinitely if the context length is extremely large.  Beyond its efficiency in processing long context, Samba can also extrapolate its memory recall ability to 256K context length through supervised fine-tuning, and still keeps its linear computation complexity. We fine-tune Samba 1.7B on Passkey Retrieval with a 4K training sequence length for only 500 steps. As presented in Figure <ref type="figure" target="#fig_3">3</ref>, SAMBA 1.7B demonstrates a remarkable ability to recall information from significantly longer contexts compared to Mistral 1.6B, a model based solely on Sliding Window Attention (SWA). This capability is particularly evident in the heatmap, where SAMBA maintains the perfect retrieval performance across a wider range of pass-key positions in a long document of up to 256K length. We also draw the training loss curve and the overall passkey retrieval accuracy across the fine-tuning procedure in Figure <ref type="figure">7</ref> and Figure <ref type="figure">8</ref> of Appendix C. We find that despite the fact that both architectures can reach near-zero training loss in less than 250 steps, Samba can achieve near-perfect retrieval early at 150 training steps, while the Mistral architecture struggles at around 30% accuracy throughout the training process. This shows that Samba can have better long-range retrieval ability than SWA due to the input selection mechanism introduced by the Mamba layers. In Figure <ref type="figure">8</ref>, we can also notice that the pre-trained base Samba model has a retrieval accuracy (at step 0) similar to that of Mistral, highlighting the need for future work to improve Samba's zero-shot retrieval capabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">LONG-CONTEXT UNDERSTANDING</head><p>The encouraging results on Passkey Retrieval drives us to further explore the limits of our finetuning approach. We perform instruction tuning to the Samba-3.8B base model on Phonebook <ref type="bibr" target="#b36">(Jelassi et al., 2024)</ref> with only 100 steps on 4K sequence length and evaluate the resulting Samba-3.8B-FT model for a sequence length up to 8K. The evaluation setting requires the models to retrieve a random phone number from a phone book containing 20 (length 400) to 480 (length 8400) name-number pairs, resulting in a pressure test of memorization to Samba which has a constant memory state size. Surprisingly, as shown in Figure <ref type="figure">4</ref>, we can see that the Samba-3.8B-FT model can close most of its gap with a full-attention model (Llama2 7B) that has twice the parameter size within the 4K training length, and achieves much better extrapolation accuracy compared to all other models including Preprint the Phi3 base model which also uses 2K sliding window attention. Since both Passkey Retrieval and Phonebook require models to remember numbers in a long context document, it is interesting to investigate if a model instruction-tuned on one task can transfer its ability to the other task in zero-shot. We directly evaluate the Passkey Retrieval finetuned Samba 1.7B and Mistral 1.6B models (named Samba 1.7B PK-FT and Mistral 1.6B PK-FT respectively) on the Phonebook task. As shown in Figure <ref type="figure">4</ref>, Samba 1.7B has slightly better retrieval accuracy than Mistral 1.6B, but both models cannot generalize their number recall ability beyond its sliding window size. We leave it for future work to further explore the transferability of long-context capabilities in linear complexity models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ANALYSIS</head><p>In this section, we analyze the experimental results of SAMBA by answering the following research questions. The perplexity results on SlimPajama have a fluctuation around ±0.3%. Training speed is measured on 8×H100 GPUs by default. All the models in this section are trained on SlimPajama with 20B tokens and 4K sequence length, unless otherwise specified. We also have additional analyses on the effectiveness of short convolution in Appendix D.</p><p>Why not hybridize with full attention? Some previous works <ref type="bibr">(Fu et al., 2023;</ref><ref type="bibr" target="#b42">Lieber et al., 2024)</ref> suggest a hybrid architecture of Mamba with full attention. However, as shown in Table <ref type="table" target="#tab_4">5</ref>, the extrapolation perplexity is exploding at a context length of 16K even if a single full attention layer is placed at the beginning of the model. Although hybridization with full attention in the second and middle sixth blocks (the fourth row in the table), following <ref type="bibr">Dao et al. (2022b)</ref>, can bridge the perplexity gap between full-attention hybrids and Samba, they still cannot extrapolate beyond the training sequence lengths. Samba also has much better training throughput compared to Mamba-MLP alternatives because self-attention with the FlashAttention 2 implementation is more training efficient than Mamba when the sequence length is 4096. How many parameters should be allocated to Attention? Given that Mamba can already capture low-rank information in the sequences through recurrent compression, the attention layers in Samba theoretically will only need to focus on information retrieval where a small number of attention heads should suffice. In Table <ref type="table" target="#tab_5">6</ref>, we explore the techniques of query head grouping <ref type="bibr">(Ainslie et al., 2023;</ref><ref type="bibr" target="#b72">Shazeer, 2019)</ref>, for both the Llama and Samba models. Surprisingly, both the Llama-2-SWA architecture and the Samba architecture show improved validation perplexity when there is only one key-value head. We conjecture that this is because small language models can be more easily optimized with fewer KV heads to pay attention to the contexts. We can also see that Samba has a 2× smaller optimal number of query heads than the SWA model, which confirms our hypothesis that Samba can support a smaller number of attention heads.</p><p>Potential explanations on why hybrid is better? We examine the entropy of the attention distributions for both the Samba 1.7B and the Mistral 1.6B models. As shown in Figure <ref type="figure">5a</ref>, the Samba model has a larger variance of the attention entropy distributed over the layer indices, with an interesting pattern that the upper and lower layers have entropy higher than the middle layers. This may indicate that the attention layers are more specialized in the Samba architecture, with the middle layers focusing on precise retrieval with low-entropy attention, and the top and bottom layers focusing on integrating the global information through high-entropy attention. We can also see in Figure <ref type="figure">5b</ref> that, compared to the Mamba-MLP model, Samba has a higher entropy of input selection probabilities in the middle layers. This indicates that, given the memory recalling ability of the attention layers, the Mamba layers can focus more on modeling the recurrent structure rather than performing retrieval with precise input selections. This kind of specialization can be beneficial for the downstream model performance, which may explain the impressive results from the Samba architecture. Details on how entropy is calculated are included in Appendix E. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we introduce SAMBA, a simple yet powerful hybrid neural architecture designed for efficient language modeling with unlimited context length. We show that SAMBA substantially outperforms state-of-the-art pure attention-based and SSM-based models across a wide range of benchmarks including common-sense reasoning, language understanding, mathematics and coding. Furthermore, SAMBA exhibits remarkable efficiency in processing long contexts, achieving substantial speedups in prompt processing and decoding throughput compared to the state-of-the-art Transformer architecture. The architecture's ability to extrapolate memory recall to very long contexts (up to 256K) through minimal fine-tuning underscores its practical applicability for real-world tasks requiring extensive context understanding. This efficient long-term memorization ability is further demonstrated to be useful by our evaluations in downstream long-context summarization tasks. Our analyses also provide insight into the optimal training configurations for hybrid models and underscore the benefits of combining attention mechanisms with SSMs. We find that allocating fewer parameters to the attention mechanism while leveraging Mamba's strengths for capturing recurrent structures leads to more efficient and effective language modeling. Our results suggest that SAMBA is a strong neural architecture for language modeling with unlimited context length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preprint</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A RELATED WORKS</head><p>Hybrid Recurrent Models Many recent works <ref type="bibr" target="#b57">(Park et al., 2024;</ref><ref type="bibr" target="#b36">Jelassi et al., 2024;</ref><ref type="bibr" target="#b2">Akyürek et al., 2024)</ref> point out the lack of retrieval ability of linear SSMs, and propose hybridization of SSMs with the Attention mechanism. However, the history of SSM/RNN-Attention hybridization can be directly dated back to the birth of the Attention mechanism <ref type="bibr" target="#b6">(Bahdanau et al., 2014)</ref> which is proposed as a soft feature alignment technique for recurrent models to cope better with long sequences. The revitalization of the fact that linear recurrent models are sequentially parallelizable <ref type="bibr" target="#b47">(Martin &amp; Cundy, 2018;</ref><ref type="bibr" target="#b28">Gu et al., 2021)</ref> has catalyzed a contemporary renaissance in hybrid recurrent architectures. SPADE <ref type="bibr" target="#b100">(Zuo et al., 2022)</ref>, GSS <ref type="bibr" target="#b48">(Mehta et al., 2023)</ref>, MEGA <ref type="bibr" target="#b45">(Ma et al., 2023)</ref>, Block State transformers <ref type="bibr" target="#b25">(Fathi et al., 2023)</ref> and Megalodon <ref type="bibr" target="#b46">(Ma et al., 2024)</ref> combine SSMs with chunked attention, while H3 <ref type="bibr">(Dao et al., 2022b)</ref>, Mambaformer <ref type="bibr" target="#b57">(Park et al., 2024)</ref> and Jamba <ref type="bibr" target="#b42">(Lieber et al., 2024;</ref><ref type="bibr">Team et al., 2024)</ref> propose to hybridize with quadratic self-attention. Our works focus particularly on the wall-time efficiency and the length extrapolatability of the hybrid SSM-Attention models, and propose to interleave SSMs with Sliding Window Attention (SWA), which has both linear computation complexity and the translation-invariant property over the sequence length. Infini-Attention <ref type="bibr" target="#b53">(Munkhdalai et al., 2024</ref>) is a recently proposed method that implements an intra-layer hybridization <ref type="bibr" target="#b88">(Wu et al., 2022)</ref> between SWA and Linear Attention with the delta rule <ref type="bibr" target="#b69">(Schlag et al., 2021)</ref>. While the preliminary results look promising, its performance in the setting of large-scale pre-training from scratch remains questionable. The most similar work to ours is Griffin <ref type="bibr" target="#b9">(De et al., 2024)</ref>, which interleaves the Real-Gated Linear Recurrent Unit (RG-LRU) with Sliding Window Attention (SWA). However, Samba hybridizes SWA with Mamba instead of RG-LRU and shows that this simple hybrid architecture can provide substantially better performance over state-of-the-art Transformer architectures across scales, while Griffin and its follow-up work RecurrentGemma <ref type="bibr" target="#b9">(Botev et al., 2024)</ref> only show comparable or worse results than Transformers. The original Mamba paper <ref type="bibr" target="#b27">(Gu &amp; Dao, 2023</ref>) also explores hybridizing pure Mamba models with full attention or MLP layers, but it does not consider the wall-time efficiency of these hybridization and only achieves marginally better performance than the pure Mamba model. In contrast, we are the first to show that interleaving Mamba with both SWA and MLP can substantially outperform modern Transformers (and Mamba) at a scale up to 3.8B parameters, while achieving comparable training speed and better length extrapolation ability under the perplexity metrics.</p><p>Efficient Sparse Attention Previous works have proposed sparsifying self-attention <ref type="bibr" target="#b83">(Vaswani et al., 2017)</ref> with a static attention pattern <ref type="bibr" target="#b15">(Child et al., 2019;</ref><ref type="bibr" target="#b95">Zaheer et al., 2020;</ref><ref type="bibr" target="#b7">Beltagy et al., 2020)</ref> or a dynamic learnable pattern <ref type="bibr" target="#b66">(Roy et al., 2020;</ref><ref type="bibr" target="#b40">Kitaev et al., 2020;</ref><ref type="bibr" target="#b65">Ren et al., 2023)</ref> to model long sequences with subquadratic complexity over the sequence length. However, due to the lack of hardware-aware efficient implementation, its actual wall-time training efficiency is often worse than the dense attention optimized with FlashAttention <ref type="bibr">(Dao et al., 2022a;</ref><ref type="bibr" target="#b20">Dao, 2023;</ref><ref type="bibr" target="#b70">Shah et al., 2024)</ref>.</p><p>In this work, we choose Sliding Window Attention, a simple static sparse attention pattern, because it can easily leverage the highly optimized FlashAttention kernels to enjoy an actual training speed-up over its dense self-attention counterpart.</p><p>Length Extrapolation Many previous works have focused on extending the context length of pretrained Transformers to improve their performance on long-context tasks. Methods such as LM-Infinite <ref type="bibr" target="#b30">(Han et al., 2023)</ref>, StreamingLLM <ref type="bibr" target="#b90">(Xiao et al., 2024)</ref>, and LongLoRA <ref type="bibr">(Chen et al., 2023b)</ref> achieve linear complexity for length extrapolation, but they can only stabilize perplexity beyond the training sequence length rather than significantly improve it. In contrast, we demonstrate that pretraining Transformers with Sliding Window Attention from scratch enables natural improvements in perplexity beyond the training sequence length. Other approaches, including LLaMA-2-Long <ref type="bibr" target="#b92">(Xiong et al., 2023)</ref>, LongLLaMA <ref type="bibr" target="#b81">(Tworkowski et al., 2023)</ref>, PI <ref type="bibr">(Chen et al., 2023a</ref><ref type="bibr">), LongRoPE (Ding et al., 2024)</ref> and Self-Extend <ref type="bibr" target="#b38">(Jin et al., 2024)</ref>, attempt to extend the full attention through modifying position embedding or continual training strategies, but they typically retain quadratic complexity in the attention mechanism with additional computation or memory I/O overhead, therefore they do not scale well to very long sequences. Although these methods achieve an improved perplexity on a sequence length that is multiple times longer than the training sequence length, their perplexity still explodes if the sequence is extremely long. Our method achieves both linear complexity and superior extrapolation performance compared to zero-shot length extrapolation methods, such as Self-Extend, under the perplexity metric. However, we acknowledge that, in terms of zero-shot retrieval performance, our method still lags behind these approaches. This underscores a trade-off between perplexity and retrieval performance in length extrapolation, which we plan to explore and address in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B ADDITIONAL EVALUATION RESULTS</head><p>In Table <ref type="table" target="#tab_6">7</ref>, we conduct comprehensive evaluations on a diverse subset of benchmarks to assess SAMBA 3.8B base model's performance across all the domains mentioned in Section 3 to ensure a thorough examination of the model's capabilities. We also report the performance of the Trans-former++ (TFM++) model, which uses the same architecture, pre-training recipe as Phi3-mini, for a fair comparison. The details of the generation configurations are included in Appendix G. We compare with several strong baselines, including Llama 2 <ref type="bibr" target="#b80">(Touvron et al., 2023)</ref>, Mistral <ref type="bibr" target="#b37">(Jiang et al., 2023)</ref>, Mamba <ref type="bibr" target="#b27">(Gu &amp; Dao, 2023)</ref>, Gemma <ref type="bibr">(Team, 2024)</ref>, Recurrent-Gemma (R-Gemma) <ref type="bibr" target="#b9">(Botev et al., 2024)</ref>, Llama 3 (MetaAI, 2024) and TFM++. As shown in Table <ref type="table" target="#tab_6">7</ref>, SAMBA achieves the highest average score on all benchmarks, demonstrating its superior performance in handling various language comprehension tasks. Notably, SAMBA excels in the GSM8K benchmark, achieving an absolute 18.1% higher accuracy than TFM++ trained on the same dataset. This shows the surprising complementary effect of combining SSM with the attention mechanism. We conjecture that when combined with attention, Mamba, as an input-dependent SSM, can focus more on performing the arithmetic operation through its recurrent states than on doing the retrieval operation which can be easily learned by the sliding window attention. As shown in Table <ref type="table" target="#tab_7">8</ref>, we can see that post-trained hybrid models can achieve superior performance compared to industry-standard Transformer-based LLMs such as Llama-3.1-Instruct 8B and Llama-3.2-Instruct 3B, and SSM-based LLMs such as FalconMamba 2 . Recent progress on hybrid LLMs, including Jamba 1.5 <ref type="bibr">(Team et al., 2024)</ref> and our own work on SAMBA, shows significant improvement over earlier approaches like R-Gemma <ref type="bibr" target="#b9">(Botev et al., 2024)</ref>, which hybridizes attention with linear recurrent models but is trained on smaller data scales. SAMBA delivers comparable performance to Jamba-1.5-Mini while using around 3× fewer active parameters and 13× fewer total parameters, due to an advanced text-book data synthesis technique <ref type="bibr">(Abdin et al., 2024)</ref>. Additionally, SAMBA outperforms the Phi3 architecture, which is trained on the same data and optimization setting, further highlighting the superiority of our hybrid architecture over modern Transformer models.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C ADDITIONAL EXPERIMENT DETAILS</head><p>We perform instruction tuning for both Mistral 1.6B and Samba 1.7B on Passkey Retrieval using document length 4096, where we generated the data on the fly through randomly sampling a 5-digit integer passkey value and a location/depth between zero and the document length to insert the passkey.</p><p>The model is then asked to generate the passkey given the full document. We train both models using batch size 2048, 250 warm-up steps with a peak learning rate of 1e -4 , and 0.1 weight decay with AdamW <ref type="bibr" target="#b44">(Loshchilov &amp; Hutter, 2018)</ref> optimizer. In both cases, the loss converges quickly in 100-200 steps. During the evaluation, we measure the overall average accuracies of the passkey retrieval at the document length of <ref type="bibr">[4k, 8k, 16k, 32k, 64k, 128k, 256k]</ref>, for each length we evaluate at 11 different depths of the document (from 0, 0.1, 0.2, ... to 1.0). In addition, for each location of the passkey (depth) in the document, we evaluate the model with five different passkeys to measure accuracy. As seen in Figure <ref type="figure">8</ref>, the average passkey retrieval accuracy for Samba 1.7B almost reaches 100% in around 150 steps, while the accuracy for Mistral 1.6B remains low, demonstrating the extrapolation ability of the Samba architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D ADDITIONAL ANALYSES</head><p>How to train models with Sliding Window Attention (SWA)? Since SWA has linear complexity with respect to the sequence length, it seems alluring to trade off the batch size to have a longer training sequence length without substantially decreasing the training throughput. However, as shown in Table <ref type="table" target="#tab_8">9</ref>, when the sequence length is increased, the validation perplexity also increases in all context lengths due to smaller batch sizes <ref type="bibr" target="#b82">(Varis &amp; Bojar, 2021)</ref>, and the optimal ratio of sequence length/window size observed is 2, resulting in a training length of 4096.  Fair comparison between Mamba and other linear recurrent models? We can notice that the Short Convolution (SC) operator in Equation ( <ref type="formula" target="#formula_0">1</ref>) is independent to the design of other parts of Mamba and can be applied to other linear recurrent models. As shown in Table <ref type="table" target="#tab_9">10</ref>, we explore the effect of SC on model performance through enhancing Llama-2-SWA, Sliding GLA, and Sliding RetNet with SC. Surprisingly, besides boosting the performance of RetNet, adding SC can also significantly improve the SWA's performance, while the effect on GLA is less prominent. We think this is because GLA already has the fine-grained decays at the channel level, so the depthwise convolution doesn't add much of the useful inductive bias for better modeling power. Notably, even with the SC enhancer, Sliding GLA and Sliding RetNet still fall short than the original Samba 421M's performance shown in Table <ref type="table" target="#tab_2">3</ref>. This further justifies our choice of using Mamba for hybridization. We also find that adding SC to both the SWA and the linear attention layers in hybrid models produces negative results, and we leave it as a future work to understand the surprising effectiveness of SC in language modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E DETAILS OF ENTROPY MEASUREMENT</head><p>Given a causal attention probability matrix A ∈ R h×n×n , A ijk = 0 ∀j &lt; k, with h number of heads and a sequence length of n, and the generation length 0 &lt; l &lt; n, we calculate the average attention entropy per decoding step as follows,</p><formula xml:id="formula_5">H a = - 1 l • h h i=1 n j=n-l+1 n k=1</formula><p>A ijk log(A ijk ).</p><p>For the selective gate ∆ ∈ R n×de used by S6 in Equation (2) of the Mamba layers, we first normalize it to be in the simplex [0, 1] n×de , i.e.,</p><formula xml:id="formula_6">∆ ′ = ∆ n i=1 ∆ i ∈ [0, 1] n×de .</formula><p>The average selection entropy of S6 throughout the entire sequence is then calculated as</p><formula xml:id="formula_7">H s = - 1 d e de j=1 n i=1 ∆ ′ ij log(∆ ′ ij ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F DETAILS OF DOWNSTREAM LONG-CONTEXT EVALUATION</head><p>We use the GovReport <ref type="bibr" target="#b35">(Huang et al., 2021)</ref> and the SQuALITY <ref type="bibr">(Wang et al., 2022)</ref> datasets from the ZeroSCROLLS <ref type="bibr">(Shaham et al., 2023)</ref> benchmark to evaluate models' long-context summarization capability in the real world. After tokenizing with the Phi3-mini-4k tokenizer, the average document length for the GovReport dataset is 11,533 tokens, with a median of 10,332, a minimum of 1,493, and a maximum of 40,592 tokens. For the SQuALITY dataset, the average sequence length is 7,974 tokens, with a median of 8,145, a minimum of 5,457, and a maximum of 10,757 tokens. For evaluation, we use greedy decoding for both tasks. A maximum generation length of 450 tokens is applied for GovReport and 600 for SQuALITY.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preprint H LIMITATIONS &amp; BROADER IMPACT</head><p>Although Samba demonstrates promising memory retrieval performance through instruction tuning, its pre-trained base model has retrieval performance similar to that of the SWA-based model, as shown in Figure <ref type="figure">8</ref>. This opens up future direction on further improving the Samba's retrieval ability without compromising its efficiency and extrapolation ability. In addition, the hybridization strategy of Samba is not consistently better than other alternatives in all tasks. As shown in Table <ref type="table" target="#tab_1">2</ref>, Mamba-SWA-MLP shows improved performance on tasks such as WinoGrande, SIQA, and GSM8K. This gives us the potential to invest in a more sophisticated approach to perform input-dependent dynamic combinations of SWA-based and SSM-based models <ref type="bibr" target="#b65">(Ren et al., 2023)</ref>. With the improved short-context performance and the long-term memorization ability of linear complexity LLMs such as Samba, cost-effective applications can be developed for personalized learning and automated tutoring. Samba can also be used for emotional accompaniment. The efficiency of the Samba architecture can save inference energy costs for models deployed on the edges, resulting in greener and more sustainable AI applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><figDesc>Perplexity on the test set of Proof-Pile (b) Decoding throughput with batch size 16</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Passkey Retrieval performance up to 256K context length for SAMBA 1.7B (Left) vs. Mistral 1.6B (right) instruction tuned on 4K sequence length with 500 steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><figDesc>Figure5: The average entropy of the attention mechanism and the Mamba's S6 input selection mechanism at each block of layers on 100 random samples from the GSM8K dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :Figure 7 :Figure 8 :</head><label>678</label><figDesc>Figure 6: Prompt processing throughput of different models with around 1.7B parameters.</figDesc><graphic coords="21,206.79,296.88,198.40,159.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Downstream evaluation of the architectures trained on 230B tokens of the Phi2 dataset. We report the unnormalized accuracy for multiple choice tasks. GSM8K is evaluated with 5-shot examples while other tasks are in zero-shot. Best results are in bold, second best underlined.</figDesc><table><row><cell>Benchmark</cell><cell cols="6">Llama-3 Mistral Mamba Mamba-SWA-MLP Mamba-MLP SAMBA 1.6B 1.6B 1.8B 1.6B 1.9B 1.7B</cell></row><row><cell>ARC-Easy</cell><cell>76.85</cell><cell>77.02</cell><cell>77.99</cell><cell>76.68</cell><cell>78.91</cell><cell>79.25</cell></row><row><cell>ARC-Challenge</cell><cell>43.26</cell><cell>44.20</cell><cell>45.22</cell><cell>46.16</cell><cell>47.35</cell><cell>48.21</cell></row><row><cell>PIQA</cell><cell>76.66</cell><cell>75.79</cell><cell>77.31</cell><cell>76.50</cell><cell>78.84</cell><cell>77.10</cell></row><row><cell>WinoGrande</cell><cell>70.01</cell><cell>70.72</cell><cell>73.40</cell><cell>73.72</cell><cell>72.38</cell><cell>72.93</cell></row><row><cell>SIQA</cell><cell>51.23</cell><cell>52.00</cell><cell>53.12</cell><cell>55.12</cell><cell>54.30</cell><cell>53.68</cell></row><row><cell>HellaSwag</cell><cell>46.98</cell><cell>47.19</cell><cell>49.80</cell><cell>49.71</cell><cell>50.14</cell><cell>49.74</cell></row><row><cell>BoolQ</cell><cell>68.20</cell><cell>70.70</cell><cell>74.83</cell><cell>74.74</cell><cell>73.70</cell><cell>75.57</cell></row><row><cell>OpenbookQA</cell><cell>34.00</cell><cell>32.80</cell><cell>36.60</cell><cell>33.80</cell><cell>35.40</cell><cell>37.20</cell></row><row><cell>SQuAD</cell><cell>74.88</cell><cell>72.82</cell><cell>67.66</cell><cell>76.73</cell><cell>63.86</cell><cell>77.64</cell></row><row><cell>MMLU</cell><cell>43.84</cell><cell>43.54</cell><cell>45.28</cell><cell>47.39</cell><cell>43.68</cell><cell>48.01</cell></row><row><cell>TruthfulQA (MC1)</cell><cell>25.70</cell><cell>25.09</cell><cell>26.81</cell><cell>26.20</cell><cell>26.44</cell><cell>27.78</cell></row><row><cell>TruthfulQA (MC2)</cell><cell>40.35</cell><cell>38.80</cell><cell>40.66</cell><cell>40.80</cell><cell>40.04</cell><cell>41.62</cell></row><row><cell>GSM8K</cell><cell>32.68</cell><cell>32.45</cell><cell>32.07</cell><cell>44.05</cell><cell>27.52</cell><cell>38.97</cell></row><row><cell>MBPP</cell><cell>46.30</cell><cell>47.08</cell><cell>47.86</cell><cell>47.08</cell><cell>47.08</cell><cell>48.25</cell></row><row><cell>HumanEval</cell><cell>36.59</cell><cell>36.59</cell><cell>35.98</cell><cell>37.80</cell><cell>31.10</cell><cell>39.02</cell></row><row><cell>Average</cell><cell>51.17</cell><cell>51.12</cell><cell>52.31</cell><cell>53.77</cell><cell>51.38</cell><cell>54.33</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Perplexity on the validation set of SlimPajama for different attention and linear recurrent model architectures trained at 4,096 context length. We use window size 2,048 for Sliding Window Attention (SWA). The perplexity results have a fluctuation around ±0.3%.</figDesc><table><row><cell>Architecture</cell><cell>Size</cell><cell>Layers</cell><cell cols="4">Training Speed Validation Context Length (×10 5 tokens/s) 4096 8192 16384</cell></row><row><cell cols="3">20B training tokens on 8×A100 GPUs</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Llama-2</cell><cell>438M</cell><cell>24</cell><cell>4.85</cell><cell cols="2">11.14 47.23</cell><cell>249.03</cell></row><row><cell>Llama-2-SWA</cell><cell>438M</cell><cell>24</cell><cell>4.96</cell><cell cols="2">11.12 10.66</cell><cell>10.57</cell></row><row><cell>Mamba</cell><cell>432M</cell><cell>60</cell><cell>2.46</cell><cell cols="2">10.70 10.30</cell><cell>10.24</cell></row><row><cell>Sliding GLA</cell><cell>438M</cell><cell>24</cell><cell>4.94</cell><cell cols="2">10.43 10.00</cell><cell>9.92</cell></row><row><cell>Sliding RetNet</cell><cell>446M</cell><cell>24</cell><cell>4.32</cell><cell>10.38</cell><cell>9.96</cell><cell>9.87</cell></row><row><cell>Mega-S6</cell><cell>422M</cell><cell>24</cell><cell>3.26</cell><cell cols="2">12.63 12.25</cell><cell>12.25</cell></row><row><cell cols="2">Mamba-SWA-MLP 400M</cell><cell>24</cell><cell>4.21</cell><cell>10.07</cell><cell>9.67</cell><cell>9.59</cell></row><row><cell>MLP2-SWA-MLP</cell><cell>417M</cell><cell>24</cell><cell>5.08</cell><cell cols="2">10.95 10.50</cell><cell>10.41</cell></row><row><cell>SAMBA-NoPE</cell><cell>421M</cell><cell>24</cell><cell>4.48</cell><cell cols="2">10.11 28.97</cell><cell>314.78</cell></row><row><cell>SAMBA</cell><cell>421M</cell><cell>24</cell><cell>4.46</cell><cell>10.06</cell><cell>9.65</cell><cell>9.57</cell></row><row><cell cols="3">100B training tokens on 64×H100 GPUs</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Llama-2</cell><cell>1.3B</cell><cell>40</cell><cell>25.9</cell><cell>7.60</cell><cell>44.32</cell><cell>249.64</cell></row><row><cell>Llama-2-SWA</cell><cell>1.3B</cell><cell>40</cell><cell>26.2</cell><cell>7.60</cell><cell>7.37</cell><cell>7.21</cell></row><row><cell>Mamba</cell><cell>1.3B</cell><cell>48</cell><cell>17.8</cell><cell>7.47</cell><cell>7.26</cell><cell>7.15</cell></row><row><cell>Sliding GLA</cell><cell>1.2B</cell><cell>36</cell><cell>25.9</cell><cell>7.58</cell><cell>7.35</cell><cell>7.19</cell></row><row><cell>Sliding RetNet</cell><cell>1.4B</cell><cell>36</cell><cell>23.0</cell><cell>7.56</cell><cell>7.35</cell><cell>7.56</cell></row><row><cell>Mega-S6</cell><cell>1.3B</cell><cell>36</cell><cell>17.9</cell><cell>9.01</cell><cell>8.81</cell><cell>8.68</cell></row><row><cell>Mamba-SWA-MLP</cell><cell>1.3B</cell><cell>36</cell><cell>23.5</cell><cell>7.37</cell><cell>7.16</cell><cell>7.00</cell></row><row><cell>MLP2-SWA-MLP</cell><cell>1.3B</cell><cell>36</cell><cell>26.6</cell><cell>7.81</cell><cell>7.58</cell><cell>7.42</cell></row><row><cell>SAMBA-NoPE</cell><cell>1.3B</cell><cell>36</cell><cell>25.2</cell><cell>7.33</cell><cell>20.40</cell><cell>326.17</cell></row><row><cell>SAMBA</cell><cell>1.3B</cell><cell>36</cell><cell>25.2</cell><cell>7.32</cell><cell>7.11</cell><cell>6.96</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>dataset under both around 438M and 1.3B settings, and evaluate these models by calculating perplexity on the validation set with context length at 4096, 8192, and 16384 tokens to investigate their zero-shot length extrapolation ability. Peak training throughput is also measured as an efficiency metric. The details of the hyperparameter settings are included in Appendix G. As shown in Table3, SAMBA consistently outperforms all other models in different context lengths and model sizes. The training speed of This also indicates that Mamba-SWA-MLP will have slower decoding speed than Samba due to larger total cache size resulting from more SSMs and Attention layers. We can further observe that replacing Mamba with MLP speeds up the training but harms perplexity significantly, indicating the importance of Mamba layers in the Samba architecture. Interestingly, even though we use SWA in Samba architecture, Samba-NoPE still has exploded perplexities beyond its training length without RoPE. We can also find that while RetNet can extrapolate well under the 438M scale, it has an increasing perplexity on 16K length at the 1.4B scale, which may indicate that its input-independent decay may need specific tuning at different scales to work well. Downstream evaluation of models pre-trained with 100B tokens from SlimPajama. We measure the character-normalized accuracy for HellaSwag following<ref type="bibr" target="#b27">Gu &amp; Dao (2023)</ref>. All tasks are evaluated in zero-shot.</figDesc><table><row><cell>Architecture</cell><cell>Size</cell><cell cols="5">ARC-Easy HellaSwag Wino. PIQA LAMBADA acc ↑ acc_norm ↑ acc ↑ acc ↑ acc ↑</cell><cell>Avg.</cell></row><row><cell>LLaMA-2</cell><cell>1.3B</cell><cell>55.09</cell><cell>52.32</cell><cell>53.35</cell><cell>71.11</cell><cell>48.52</cell><cell>56.08</cell></row><row><cell>LLaMA-2-SWA</cell><cell>1.3B</cell><cell>56.65</cell><cell>52.59</cell><cell>54.93</cell><cell>71.60</cell><cell>47.56</cell><cell>56.67</cell></row><row><cell>Sliding GLA</cell><cell>1.2B</cell><cell>56.94</cell><cell>52.52</cell><cell>56.75</cell><cell>71.38</cell><cell>48.17</cell><cell>57.15</cell></row><row><cell>Sliding RetNet</cell><cell>1.4B</cell><cell>57.66</cell><cell>52.64</cell><cell>56.75</cell><cell>71.33</cell><cell>48.34</cell><cell>57.34</cell></row><row><cell>Mega-S6</cell><cell>1.3B</cell><cell>50.63</cell><cell>41.91</cell><cell>52.96</cell><cell>68.17</cell><cell>37.88</cell><cell>50.31</cell></row><row><cell>Mamba</cell><cell>1.3B</cell><cell>58.08</cell><cell>54.93</cell><cell>53.99</cell><cell>71.98</cell><cell>45.97</cell><cell>56.99</cell></row><row><cell cols="2">Mamba-SWA-MLP 1.3B</cell><cell>59.64</cell><cell>54.50</cell><cell>55.25</cell><cell>72.42</cell><cell>49.12</cell><cell>58.19</cell></row><row><cell>MLP2-SWA-MLP</cell><cell>1.3B</cell><cell>55.18</cell><cell>50.32</cell><cell>52.80</cell><cell>70.67</cell><cell>48.11</cell><cell>55.42</cell></row><row><cell>SAMBA-NoPE</cell><cell>1.3B</cell><cell>58.38</cell><cell>54.62</cell><cell>56.51</cell><cell>72.03</cell><cell>51.08</cell><cell>58.52</cell></row><row><cell>SAMBA</cell><cell>1.3B</cell><cell>58.21</cell><cell>54.73</cell><cell>55.72</cell><cell>72.36</cell><cell>51.68</cell><cell>58.54</cell></row></table><note><p>SAMBA is competitive compared to pure Transformer-based models on the 1.3B scale. Mamba has significantly worse training throughput because Mamba layers have slower training speed than MLP layers, and the purebred Mamba models need to have more layers than other models at the same number of parameters. Comparing Mamba-SWA-MLP with Samba, we can see that Samba has slightly better perplexity scores and higher training throughput. Mamba-SWA-MLP trades off the MLP layers with more I/O intensive Mamba and Attention layers, leading to slower training speed.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Perplexity on SlimPajama of Mamba-MLP architectures with full attention layers replacing Mamba layers at different block indices. We define a block as two consecutive layers with a Mamba/Attention layer followed by an MLP. All the models have 12 blocks in total.</figDesc><table><row><cell>Architecture</cell><cell>Size</cell><cell>Block Index of Full Attention</cell><cell cols="4">Training Speed Validation Context Length (×10 5 tokens/s) 4096 8192 16384</cell></row><row><cell></cell><cell>449M</cell><cell>11</cell><cell>7.78</cell><cell cols="2">10.29 10.53</cell><cell>13.66</cell></row><row><cell>Mamba-MLP</cell><cell>449M 449M</cell><cell>5 0</cell><cell>7.78 7.78</cell><cell cols="2">10.10 10.05 10.89 10.55</cell><cell>12.83 10.63</cell></row><row><cell></cell><cell>443M</cell><cell>1, 5</cell><cell>7.93</cell><cell cols="2">10.06 10.34</cell><cell>13.57</cell></row><row><cell>SAMBA</cell><cell cols="2">421M SWA at odd indices</cell><cell>8.59</cell><cell>10.06</cell><cell>9.65</cell><cell>9.57</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Perplexity on SlimPajama of Llama-2-SWA and Samba models at the 430M scales trained with different number of Query and Key-Value heads. "KV Size" means the size of Key-Value vectors per token and attention layer. Since grouped query attention will reduce the parameters for attention from 4d 2 m to roughly 2d 2 m , we increase the intermediate size of MLP from 8/3d m to 3d m = 4608 to have roughly the same number of total parameters as the original models.</figDesc><table><row><cell cols="2">Query Key-Value</cell><cell>Head</cell><cell>KV</cell><cell cols="5">Model Training Speed Validation Context Length</cell></row><row><cell>Head</cell><cell>Head</cell><cell>Dim.</cell><cell>Size</cell><cell>Size</cell><cell cols="2">(×10 5 tokens/s) 4096</cell><cell>8192</cell><cell>16384</cell></row><row><cell cols="3">Llama-2-SWA Architecture</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>12</cell><cell>2</cell><cell>128</cell><cell>512</cell><cell>419M</cell><cell>10.01</cell><cell cols="2">11.11 10.64</cell><cell>10.56</cell></row><row><cell>6</cell><cell>1</cell><cell>256</cell><cell>512</cell><cell>419M</cell><cell>9.98</cell><cell cols="2">11.09 10.62</cell><cell>10.54</cell></row><row><cell>12</cell><cell>1</cell><cell>128</cell><cell>256</cell><cell>414M</cell><cell>10.25</cell><cell cols="2">10.89 10.44</cell><cell>10.35</cell></row><row><cell>12</cell><cell>4</cell><cell>128</cell><cell>1024</cell><cell>428M</cell><cell>9.85</cell><cell cols="2">11.11 10.64</cell><cell>10.56</cell></row><row><cell cols="2">Samba Architecture</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>12</cell><cell>2</cell><cell>128</cell><cell>512</cell><cell>426M</cell><cell>8.55</cell><cell>10.09</cell><cell>9.68</cell><cell>9.60</cell></row><row><cell>6</cell><cell>1</cell><cell>256</cell><cell>512</cell><cell>426M</cell><cell>8.46</cell><cell>9.99</cell><cell>9.59</cell><cell>9.51</cell></row><row><cell>12</cell><cell>1</cell><cell>128</cell><cell>256</cell><cell>424M</cell><cell>8.62</cell><cell>10.07</cell><cell>9.66</cell><cell>9.58</cell></row><row><cell>12</cell><cell>4</cell><cell>128</cell><cell>1024</cell><cell>431M</cell><cell>8.57</cell><cell>10.02</cell><cell>9.62</cell><cell>9.55</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Downstream performance comparison of the SAMBA 3.8B base model with other pretrained base language models without instruction tuning. ARC-C and HellaSwag are measured with characternormalized accuracy. MMLU and GSM8K are measured in 5-shot, while others are in zero-shot. We report the MC2 score for TruthfulQA, maj@1 for GSM8K, and pass@1 for HumanEval. * Measured by ours. The fair comparison should only be considered between TFM++ and Samba.</figDesc><table><row><cell>Model</cell><cell cols="3">Size Tokens MMLU</cell><cell cols="7">Hella-ARC-Wino-Truth. GSM Hum. Avg. Swag C Gran. QA 8K Eval</cell></row><row><cell>Llama 2</cell><cell>6.7B</cell><cell>2T</cell><cell>45.3</cell><cell>77.2</cell><cell>45.9</cell><cell>69.2</cell><cell>38.8</cell><cell>14.6</cell><cell>12.8</cell><cell>43.4</cell></row><row><cell></cell><cell>13B</cell><cell>2T</cell><cell>54.8</cell><cell>80.7</cell><cell>49.4</cell><cell>72.8</cell><cell>37.4</cell><cell>28.7</cell><cell>18.3</cell><cell>48.9</cell></row><row><cell>Mistral</cell><cell>7.2B</cell><cell>-</cell><cell>60.1</cell><cell>81.3</cell><cell>55.5</cell><cell>75.3</cell><cell>42.2</cell><cell>35.4</cell><cell>30.5</cell><cell>53.6</cell></row><row><cell>Mamba</cell><cell>2.8B</cell><cell>600B</cell><cell>26.2</cell><cell>71.0</cell><cell>41.7</cell><cell>65.9</cell><cell>34.4  *</cell><cell>3.6  *</cell><cell>7.3  *</cell><cell>35.7</cell></row><row><cell>Gemma</cell><cell>2.5B</cell><cell>3T</cell><cell>42.3</cell><cell>71.4</cell><cell>42.1</cell><cell>65.4</cell><cell>33.1</cell><cell>17.7</cell><cell>22.0</cell><cell>42.0</cell></row><row><cell></cell><cell>8.5B</cell><cell>6T</cell><cell>64.3</cell><cell>81.2</cell><cell>53.2</cell><cell>72.3</cell><cell>44.8</cell><cell>46.4</cell><cell>32.3</cell><cell>56.4</cell></row><row><cell cols="2">R-Gemma 2.7B</cell><cell>2T</cell><cell>38.4</cell><cell>71.0</cell><cell>42.3</cell><cell>67.8</cell><cell>35.1</cell><cell>13.4</cell><cell>21.3</cell><cell>41.3</cell></row><row><cell>Llama 3</cell><cell>8.0B</cell><cell>15T+</cell><cell>66.6</cell><cell>79.2  *</cell><cell>53.2  *</cell><cell>72.6  *</cell><cell>43.9</cell><cell>45.8</cell><cell>28.7  *</cell><cell>55.8</cell></row><row><cell>TFM++</cell><cell>3.8B</cell><cell>3.2T</cell><cell>67.2</cell><cell>76.6</cell><cell>53.8</cell><cell>72.6</cell><cell>47.3</cell><cell>51.5</cell><cell>51.8</cell><cell>60.1</cell></row><row><cell>SAMBA</cell><cell>3.8B</cell><cell>3.2T</cell><cell>71.2</cell><cell>77.4</cell><cell>55.7</cell><cell>77.1</cell><cell>43.4</cell><cell>69.6</cell><cell>54.9</cell><cell>64.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Post-trained models quality on representative benchmarks under the chat mode. The fair comparison should only be considered between SAMBA and Phi3 as we control the training recipes and datasets to be the same. Best results are in bold, second best underlined.</figDesc><table><row><cell cols="2">Category Benchmark</cell><cell>SAMBA (June) 3.8B</cell><cell>Phi3 (June) 3.8B</cell><cell>R-Gemma 9B</cell><cell>FalconMamba 7B</cell><cell>Jamba-1.5-Mini 12B/52B</cell><cell>Llama-3.2-In 3B</cell><cell>Llama-3.1-In 8B</cell></row><row><cell>MMLU</cell><cell>MMLU (5-shot) MMLU-Pro (0-shot, CoT)</cell><cell>69.0 47.9</cell><cell>67.2 46.5</cell><cell>60.5 17.8</cell><cell>62.1 14.5</cell><cell>69.7 42.5</cell><cell>61.8 39.2</cell><cell>68.1 44</cell></row><row><cell>Reasoning</cell><cell>ARC-C (10-shot) GPQA (0-shot, CoT)</cell><cell>87.8 29.5</cell><cell>86.8 29.0</cell><cell>52.0 4.7</cell><cell>62.0 8.1</cell><cell>85.7 32.3</cell><cell>76.1 26.6</cell><cell>83.1 26.3</cell></row><row><cell>Math</cell><cell>GSM8K (8-shot, CoT)</cell><cell>86.4</cell><cell>84.8</cell><cell>42.6</cell><cell>52.5</cell><cell>75.8</cell><cell>75.6</cell><cell>77.4</cell></row><row><cell>Code</cell><cell>HumanEval (0-shot) MBPP (3-shot)</cell><cell>70.1 71.7</cell><cell>66.5 70.0</cell><cell>31.1 42.0</cell><cell>--</cell><cell>62.8 75.8</cell><cell>62.8 67.2</cell><cell>66.5 69.4</cell></row><row><cell cols="2">Average</cell><cell>66.1</cell><cell>64.4</cell><cell>35.8</cell><cell>-</cell><cell>63.5</cell><cell>58.5</cell><cell>62.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Perplexity on SlimPajama of Llama-2-SWA 438M models trained on different context sizes and batch sizes. We fix the sliding window size as 2048 and the training tokens per step as 2M.</figDesc><table><row><cell>Batch Size</cell><cell>Sequence Length</cell><cell cols="4">Training Speed (×10 5 tokens/s) 2048 Validation Context Length 4096 8192 16384</cell></row><row><cell>1024</cell><cell>2048 (Full Attention)</cell><cell>10.4</cell><cell cols="3">11.59 38.12 156.18 357.32</cell></row><row><cell>512</cell><cell>4096</cell><cell>9.88</cell><cell>11.87 11.16</cell><cell>10.69</cell><cell>10.61</cell></row><row><cell>256</cell><cell>8192</cell><cell>9.66</cell><cell>11.98 11.26</cell><cell>10.79</cell><cell>10.69</cell></row><row><cell>128</cell><cell>16384</cell><cell>9.48</cell><cell>12.37 11.63</cell><cell>11.12</cell><cell>11.02</cell></row><row><cell>64</cell><cell>32768</cell><cell>9.29</cell><cell>12.94 12.46</cell><cell>11.96</cell><cell>11.86</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 :</head><label>10</label><figDesc>Perplexity on the SlimPajama validation set of different linear recurrent and sliding window attention models with Short Convolution (SC) modules added separately to query, key and value representations. For hybrid models, SC is applied only to linear attention layers. The training speed is measured on 8×A100 GPUs.</figDesc><table><row><cell>Architecture</cell><cell>Size</cell><cell cols="4">Training Speed Validation Context Length (×10 5 tokens/s) 4096 8192 16384</cell></row><row><cell>Llama-2-SWA</cell><cell>438M</cell><cell>4.96</cell><cell cols="2">11.12 10.66</cell><cell>10.57</cell></row><row><cell cols="2">+ SC 438M</cell><cell>4.69</cell><cell cols="2">10.83 10.39</cell><cell>10.31</cell></row><row><cell>Sliding GLA</cell><cell>438M</cell><cell>4.94</cell><cell cols="2">10.43 10.00</cell><cell>9.92</cell></row><row><cell cols="2">+ SC 438M</cell><cell>4.44</cell><cell>10.39</cell><cell>9.96</cell><cell>9.87</cell></row><row><cell cols="2">Sliding RetNet 446M</cell><cell>4.32</cell><cell>10.38</cell><cell>9.96</cell><cell>9.87</cell></row><row><cell cols="2">+ SC 446M</cell><cell>3.80</cell><cell>10.25</cell><cell>9.82</cell><cell>9.74</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://huggingface.co/datasets/EleutherAI/lambada_openai</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://huggingface.co/tiiuae/falcon-mamba-7b-instruct</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://github.com/sustcsonglin/flash-linear-attention</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://github.com/datamllab/LongLM/blob/master/self_extend_patch/Llama.py</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>https://github.com/jzhang38/TinyLlama</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENT</head><p>We want to thank <rs type="person">Shuohang Wang</rs> and <rs type="person">Liyuan Liu</rs> for helping with the training infrastructure, <rs type="person">Mojan Javaheripi</rs> and the team for the pre-training data, <rs type="person">Ziyi Yang</rs>, <rs type="person">Jianwen Zhang</rs>, <rs type="person">Junheng Hao</rs> and the team for helping with post-training. The first author also wants to thank <rs type="person">Songlin Yang</rs> for her Triton implementation of Mamba.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="bibr">Fei Sun, Felix Kreuk, Feng Tian, Firat Ozgenel, Francesco Caggioni, Francisco Guzmán, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Govind Thattai, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry</ref> <p>Aspegren, Hunter Goldman, Igor Molybog, Igor Tufanov, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Preprint G IMPLEMENTATION DETAILS  In the generation configurations for the downstream tasks, we use greedy decoding for GSM8K, and Nucleus Sampling <ref type="bibr" target="#b34">(Holtzman et al., 2019)</ref> with a temperature of τ = 0.2 and top-p = 0.95 for HumanEval. For MBPP and SQuAD, we set τ = 0.01 and top-p = 0.95.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Marah</forename><surname>Abdin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><forename type="middle">Ade</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ammar</forename><surname>Ahmad Awan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jyoti</forename><surname>Aneja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Awadallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hany</forename><surname>Awadalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nguyen</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Bahree</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arash</forename><surname>Bakhtiari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harkirat</forename><surname>Behl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alon</forename><surname>Benhaim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Misha</forename><surname>Bilenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johan</forename><surname>Bjorck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Bubeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caio</forename><surname>César</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teodoro</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parul</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allie</forename><surname>Del Giorno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustavo</forename><surname>De Rosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Dixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronen</forename><surname>Eldan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Iter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suriya</forename><surname>Gunasekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emman</forename><surname>Haider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junheng</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><forename type="middle">J</forename><surname>Hewett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mojan</forename><surname>Javaheripi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piero</forename><surname>Kauffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Karampatziakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahoud</forename><surname>Khademi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lev</forename><surname>Kurilenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">R</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Tat Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weishung</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeqi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Madan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arindam</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hardik</forename><surname>Modi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Norick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barun</forename><surname>Patra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Perez-Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Portet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reid</forename><surname>Pryzant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heyang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marko</forename><surname>Radmilac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corby</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sambudha</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olli</forename><surname>Saarikivi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sonali</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengruidong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cyril</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><forename type="middle">Lyna</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiren</forename><surname>Zhou</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2404.14219v1" />
		<editor>Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Xia Song, Olatunji Ruwase, Xin Wang, Rachel Ward, Guanhua Wang, Philipp Witte, Michael Wyatt, Can Xu,</editor>
		<imprint/>
	</monogr>
	<note>Phi-3 technical report: A highly capable language model locally on your phone. arXiv preprint arXiv: 2404.14219, 2024</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Training generalized multi-query transformer models from multi-head checkpoints</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee-Thorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michiel</forename><surname>De Jong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yury</forename><surname>Zemlyanskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Lebr'on</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sumit</surname></persName>
		</author>
		<author>
			<persName><surname>Sanghai</surname></persName>
		</author>
		<author>
			<persName><surname>Gqa</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2305.13245</idno>
		<ptr target="https://arxiv.org/abs/2305" />
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date>13245v3</date>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Ekin</forename><surname>Akyürek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bailin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.12973</idno>
		<ptr target="https://arxiv.org/abs/2401.12973v2" />
		<title level="m">-context language learning: Architectures and algorithms</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Zoology: Measuring and improving recall in efficient language models</title>
		<author>
			<persName><forename type="first">Simran</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sabri</forename><surname>Eyuboglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aman</forename><surname>Timalsina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isys</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Poli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.04927</idno>
		<ptr target="https://arxiv.org/abs/2312.04927v1" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Simple linear attention language models balance the recall-throughput tradeoff</title>
		<author>
			<persName><forename type="first">Simran</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sabri</forename><surname>Eyuboglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aman</forename><surname>Timalsina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silas</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dylan</forename><surname>Zinsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.18668</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Program synthesis with large language models</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Nye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henryk</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carrie</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Terry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07732</idno>
		<ptr target="https://arxiv.org/abs/2108.07732v1" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1409.0473v7" />
	</analytic>
	<monogr>
		<title level="m">International Conference On Learning Representations</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><surname>Longformer</surname></persName>
		</author>
		<idno>arXiv: Arxiv-2004.05150</idno>
		<ptr target="https://arxiv.org/abs/2004.05150v2" />
		<title level="m">The long-document transformer</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">PIQA: reasoning about physical commonsense in natural language</title>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v34i05.6239</idno>
		<ptr target="https://doi.org/10.1609/aaai.v34i05.6239" />
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020">February 7-12, 2020. 2020</date>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="7432" to="7439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Botev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soham</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anushan</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George-Cristian</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruba</forename><surname>Muraru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonard</forename><surname>Haroun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Berrada</surname></persName>
		</author>
		<author>
			<persName><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Pier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Sessa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léonard</forename><surname>Dadashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johan</forename><surname>Hussenot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sertan</forename><surname>Ferret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Girgin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alek</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathleen</forename><surname>Andreev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Kenealy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cassidy</forename><surname>Mesnard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Hardin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shreya</forename><surname>Bhupatiraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgane</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihir</forename><surname>Rivière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juliette</forename><surname>Sanjay Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pouya</forename><surname>Love</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Tafti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Fiedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutian</forename><surname>Senter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srivatsan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnaud</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharad</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Vikram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charlie</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonia</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenny</forename><surname>Paterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meg</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raj</forename><surname>Risdal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nesh</forename><surname>Gundluru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Devanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preprint</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Nilay Chauhan</surname></persName>
		</author>
		<author>
			<persName><surname>Culliton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustavo</forename><surname>Luiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elisa</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Bandy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Glenn</forename><surname>Huntsperger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Cameron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tris</forename><surname>Zucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludovic</forename><surname>Warkentin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh</forename><surname>Peran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Giang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clément</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demis</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raia</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName><surname>Hadsell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.07839</idno>
		<ptr target="https://arxiv.org/abs/2404.07839v1" />
		<title level="m">Recurrentgemma: Moving past transformers for efficient open language models</title>
		<imprint>
			<publisher>Yee Whye Teh, and Nando de Frietas</publisher>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2005" />
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020. 14165v4</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Bubeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronen</forename><surname>Eldan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Gehrke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Horvitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ece</forename><surname>Kamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Tat Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harsha</forename><surname>Nori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamid</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Tulio Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.12712</idno>
		<ptr target="https://arxiv.org/abs/2303.12712v5" />
		<title level="m">Sparks of artificial general intelligence: Early experiments with gpt-4</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiming</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrique</forename><surname>Ponde De Oliveira Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harri</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heidy</forename><surname>Khlaaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brooke</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alethea</forename><surname>Power</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Tillet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felipe</forename><forename type="middle">Petroski</forename><surname>Such</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dave</forename><surname>Cummings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fotios</forename><surname>Chantzis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Hebgen Guss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Paino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolas</forename><surname>Tezak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suchir</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shantanu</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">N</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedant</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Morikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miles</forename><surname>Brundage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mira</forename><surname>Murati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bob</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.03374</idno>
		<ptr target="https://arxiv.org/abs/2107.03374v2" />
		<title level="m">Evaluating large language models trained on code</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Extending context window of large language models via positional interpolation</title>
		<author>
			<persName><forename type="first">Shouyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherman</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangjian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.15595</idno>
		<ptr target="https://arxiv.org/abs/2306.15595v2" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Longlora: Efficient fine-tuning of long-context large language models</title>
		<author>
			<persName><forename type="first">Yukang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengju</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2309.12307</idno>
		<ptr target="https://arxiv.org/abs/2309.12307v1" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1904.10509v1" />
	</analytic>
	<monogr>
		<title level="j">PREPRINT</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exploring the surprising difficulty of natural yes/no questions</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Boolq</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1300</idno>
		<ptr target="https://doi.org/10.18653/v1/n19-1300" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<title level="s">Long and Short Papers</title>
		<editor>
			<persName><forename type="first">Jill</forename><surname>Burstein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Christy</forename><surname>Doran</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thamar</forename><surname>Solorio</surname></persName>
		</editor>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">June 2-7, 2019. 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2924" to="2936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Think you have solved question answering? try arc, the ai2 reasoning challenge</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isaac</forename><surname>Cowhey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carissa</forename><surname>Schoenick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05457</idno>
		<ptr target="https://arxiv.org/abs/1803.05457v1" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Karl</forename><surname>Cobbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reiichiro</forename><surname>Nakano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.14168</idno>
		<ptr target="https://arxiv.org/abs/2110" />
		<title level="m">Training verifiers to solve math word problems</title>
		<imprint>
			<date type="published" when="2021">2021. 14168v2</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Knowledge neurons in pretrained transformers</title>
		<author>
			<persName><forename type="first">Damai</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaru</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2104.08696v2" />
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Flashattention-2: Faster attention with better parallelism and work partitioning</title>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.08691</idno>
		<ptr target="https://arxiv.org/abs/2307.08691v1" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">FlashAttention: Fast and memoryefficient exact attention with IO-awareness</title>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hungry hungry hippos: Towards language modeling with state space models</title>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khaled</forename><surname>Kamal Saab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2212.14052</idno>
		<ptr target="https://arxiv.org/abs/2212.14052v3" />
	</analytic>
	<monogr>
		<title level="m">International Conference On Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Preprint</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Cummings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Carvill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Mcphie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kam</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katayoun</forename><surname>Zand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathy</forename><surname>Matosich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaushik</forename><surname>Veeraraghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelly</forename><surname>Michelena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keqian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunal</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kushal</forename><surname>Lakhotia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lailin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lakshya</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lavender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leandro</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangpeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liron</forename><surname>Moshkovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Wehrstedt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manav</forename><surname>Avalani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manish</forename><surname>Bhatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Tsimpoukelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martynas</forename><surname>Mankus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matan</forename><surname>Hasson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Lennie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Reso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Groshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Naumov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maya</forename><surname>Lathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meghan</forename><surname>Keneally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">L</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Valko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michelle</forename><surname>Restrepo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihir</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mik</forename><surname>Vyatskov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikayel</forename><surname>Samvelyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Macey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miquel</forename><forename type="middle">Jubert</forename><surname>Hermoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mo</forename><surname>Metanat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Munish</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nandhini</forename><surname>Santhanam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natascha</forename><surname>Parks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natasha</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navyata</forename><surname>Bawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nayan</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Egebo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Pavlovich Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norman</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleg</forename><surname>Chernoguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivia</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omkar</forename><surname>Salpekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ozlem</forename><surname>Kalinli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parkin</forename><surname>Kent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parth</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Saab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavan</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Rittner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Bontrager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Polina</forename><surname>Zvyagina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prashant</forename><surname>Ratanchandani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pritish</forename><surname>Yuvraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachad</forename><surname>Alao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafi</forename><surname>Ayub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghotham</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghu</forename><surname>Nayani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rebekkah</forename><surname>Hogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Battey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rocky</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Maheswari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><surname>Howes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruty</forename><surname>Rinott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayesh</forename><surname>Sai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samyak</forename><surname>Bondu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Chugh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sargun</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasha</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satadru</forename><surname>Sidorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seiji</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharadh</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaun</forename><surname>Ramaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaun</forename><surname>Lindsay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Lindsay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenghao</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cindy</forename><surname>Shengxin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiva</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuqiang</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sinong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sneha</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soji</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Sajuyigbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Max</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenchen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wes</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Bouaziz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaocheng</forename><surname>Constable</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xide</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xilun</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinbo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanjun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yenda</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yossi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngjin</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yundi</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuzi</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zach</forename><surname>Rait</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zef</forename><surname>Rosnbrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoduo</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.21783</idno>
		<ptr target="https://arxiv.org/abs/2407.21783v1" />
		<title level="m">The llama 3 herd of models</title>
		<editor>
			<persName><forename type="first">Steve</forename><surname>Kehoe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Steve</forename><surname>Satterfield</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sudarshan</forename><surname>Govindaprasad</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sumit</forename><surname>Gupta</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sungmin</forename><surname>Cho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sunny</forename><surname>Virk</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Suraj</forename><surname>Subramanian</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sy</forename><surname>Choudhury</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sydney</forename><surname>Goldman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tal</forename><surname>Remez</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tamar</forename><surname>Glaser</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tamara</forename><surname>Best</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thilo</forename><surname>Kohler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thomas</forename><surname>Robinson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tianhe</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tianjun</forename><surname>Zhang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tim</forename><surname>Matthews</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Timothy</forename><surname>Chou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tzook</forename><surname>Shaked</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Varun</forename><surname>Vontimitta</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Victoria</forename><surname>Ajayi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Victoria</forename><surname>Montanez</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Vijai</forename><surname>Mohan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Satish</forename><surname>Vinay</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Vishal</forename><surname>Kumar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Vlad</forename><surname>Mangla</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Vlad</forename><surname>Ionescu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Vlad</forename><surname>Poenaru</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Vladimir</forename><surname>Tiberiu Mihailescu</surname></persName>
		</editor>
		<editor>
			<persName><surname>Ivanov</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sigmoid-weighted linear units for neural network function approximation in reinforcement learning</title>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Elfwing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Uchibe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Doya</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neunet.2017.12.012</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Block-state transformers</title>
		<author>
			<persName><forename type="first">Mahan</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Pilault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Luc</forename><surname>Bacon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Goroshin</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2306.09539v4" />
	</analytic>
	<monogr>
		<title level="j">NEURIPS</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hungry hungry hippos: Towards language modeling with state space models</title>
		<author>
			<persName><forename type="first">Tri</forename><surname>Daniel Y Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khaled</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armin</forename><forename type="middle">W</forename><surname>Kamal Saab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><surname>Re</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=COZDy0WYGg" />
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><surname>Mamba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.00752</idno>
		<title level="m">Linear-time sequence modeling with selective state spaces</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficiently modeling long sequences with structured state spaces</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><surname>Christopher R'e</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference On Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">On the parameterization and initialization of diagonal state space models</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2206.11893</idno>
	</analytic>
	<monogr>
		<title level="j">ARXIV.ORG</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Lm-infinite: Simple on-the-fly length generalization for large language models</title>
		<author>
			<persName><forename type="first">Chi</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.16137</idno>
		<ptr target="https://arxiv.org/abs/2308.16137v3" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1512.03385v1" />
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Depth-wise decomposition for accelerating separable convolutions in efficient convolutional neural networks</title>
		<author>
			<persName><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianing</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianren</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cindy</forename><forename type="middle">X</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Congrui</forename><surname>Hetang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianwei</forename><surname>Yue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.09455</idno>
		<ptr target="https://arxiv.org/abs/1910.09455v3" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Measuring massive multitask language understanding</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Collin</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=d7KBjmI3GmQ" />
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">May 3-7, 2021</date>
		</imprint>
	</monogr>
	<note>OpenReview.net, 2021</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The curious case of neural text degeneration</title>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1904.09751v2" />
	</analytic>
	<monogr>
		<title level="m">ternational Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Efficient attentions for long document summarization</title>
		<author>
			<persName><forename type="first">Luyang</forename><surname>Preprint</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolaus</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><surname>Parulian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter</title>
		<meeting>the 2021 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1419" to="1436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Repeat after me: Transformers are better than state space models at copying</title>
		<author>
			<persName><forename type="first">Samy</forename><surname>Jelassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Brandfonbrener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eran</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><surname>Malach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.01032</idno>
		<ptr target="https://arxiv.org/abs/2402.01032v1" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Albert</forename><forename type="middle">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Bamford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devendra</forename><surname>Singh Chaplot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>De Las Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Bressand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gianna</forename><surname>Lengyel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucile</forename><surname>Saulnier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renard</forename><surname>Lélio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lavaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothée</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">El</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><surname>Sayed</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.06825</idno>
		<ptr target="https://arxiv.org/abs/2310.06825v1" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">Mistral 7b. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Llm maybe longlm: Self-extend llm context window without tuning</title>
		<author>
			<persName><forename type="first">Hongye</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaotian</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfeng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhimeng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zirui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chia-Yuan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huiyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.01325</idno>
		<ptr target="https://arxiv.org/abs/2401.01325v1" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Gateloop: Fully data-controlled linear recurrence for sequence modeling</title>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Katsch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.01927</idno>
		<ptr target="https://arxiv.org/abs/2311.01927v1" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.04451</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee</title>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Bubeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronen</forename><surname>Eldan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.05463</idno>
		<ptr target="https://arxiv.org/abs/2309.05463v1" />
	</analytic>
	<monogr>
		<title level="m">Textbooks are all you need ii: phi-1.5 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Opher</forename><surname>Lieber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barak</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hofit</forename><surname>Bata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gal</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jhonathan</forename><surname>Osin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itay</forename><surname>Dalmedigos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erez</forename><surname>Safahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaked</forename><surname>Meirom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shai</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omri</forename><surname>Abend</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raz</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomer</forename><surname>Asida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Glozman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Gokhman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avashalom</forename><surname>Manevich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nir</forename><surname>Ratner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Rozen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erez</forename><surname>Shwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mor</forename><surname>Zusman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Shoham</surname></persName>
		</author>
		<author>
			<persName><surname>Jamba</surname></persName>
		</author>
		<idno>arXiv: 2403</idno>
		<ptr target="https://arxiv.org/abs/2403" />
		<title level="m">A hybrid transformer-mamba language model</title>
		<imprint>
			<date type="published" when="2024">19887. 2024. 19887v1</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">TruthfulQA: Measuring how models mimic human falsehoods</title>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Owain</forename><surname>Evans</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.229</idno>
		<ptr target="https://aclanthology.org/2022.acl-long.229" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">Preslav</forename><surname>Smaranda Muresan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aline</forename><surname>Nakov</surname></persName>
		</editor>
		<editor>
			<persName><surname>Villavicencio</surname></persName>
		</editor>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-05">may 2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3214" to="3252" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Mega: Moving average equipped gated attention</title>
		<author>
			<persName><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangke</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=qNLe3iq2El" />
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaomeng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beidi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lili</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><surname>Megalodon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.08801</idno>
		<ptr target="https://arxiv.org/abs/2404.08801v1" />
		<title level="m">Efficient llm pretraining and inference with unlimited context length</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Parallelizing linear recurrent neural nets over sequence length</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Cundy</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HyUNwulC-" />
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations, ICLR 2018</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-05-03">April 30 -May 3, 2018. 2018</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Long range language modeling via gated state spaces</title>
		<author>
			<persName><forename type="first">Harsh</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashok</forename><surname>Cutkosky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Behnam</forename><surname>Neyshabur</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=5MkYIYCbva" />
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations, ICLR 2023</title>
		<meeting><address><addrLine>Kigali, Rwanda</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">May 1-5, 2023. 2023</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">The illusion of state in state-space models</title>
		<author>
			<persName><forename type="first">William</forename><surname>Merrill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackson</forename><surname>Petty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.08819</idno>
		<ptr target="https://arxiv.org/abs/2404.08819v1" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Introducing meta llama 3: The most capable openly available llm to date</title>
		<author>
			<persName><surname>Metaai</surname></persName>
		</author>
		<ptr target="https://ai.meta.com/blog/meta-llama-3/" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Can a suit of armor conduct electricity? a new dataset for open book question answering</title>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1260</idno>
		<ptr target="https://arxiv.org/abs/1809.02789v1" />
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Landmark attention: Random-access infinite context length for transformers</title>
		<author>
			<persName><forename type="first">Amirkeivan</forename><surname>Preprint</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Mohtashami</surname></persName>
		</author>
		<author>
			<persName><surname>Jaggi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.16300</idno>
		<ptr target="https://arxiv.org/abs/2305.16300v2" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Leave no context behind: Efficient infinite context transformers with infini-attention</title>
		<author>
			<persName><forename type="first">Tsendsuren</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Gopal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.07143</idno>
		<ptr target="https://arxiv.org/abs/2404.07143v1" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title/>
		<ptr target="https://arxiv.org/abs/2303.08774v4" />
	</analytic>
	<monogr>
		<title level="j">OpenAI. Gpt-4 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>PREPRINT</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Resurrecting recurrent neural networks for long sequences</title>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Orvieto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anushan</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soham</forename><surname>De</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2303.06349</idno>
		<ptr target="https://arxiv.org/abs/2303.06349v1" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">Denis</forename><surname>Paperno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">N</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandro</forename><surname>Pezzelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gemma</forename><surname>Boleda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fernández</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1144</idno>
		<title level="m">The lambada dataset: Word prediction requiring a broad discourse context. Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Can mamba learn how to learn? a comparative study on in-context learning tasks</title>
		<author>
			<persName><forename type="first">Jongho</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaeseung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheyang</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nayoung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoong</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samet</forename><surname>Oymak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kangwook</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Papailiopoulos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.04248</idno>
		<ptr target="https://arxiv.org/abs/2402.04248v1" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">Michael</forename><surname>Poli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Massaroli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baccus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2302.10866</idno>
		<ptr target="https://arxiv.org/abs/2302" />
		<title level="m">Hyena hierarchy: Towards larger convolutional language models. International Conference On Machine Learning</title>
		<imprint>
			<date>10866v3</date>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Train short, test long: Attention with linear biases enables input length extrapolation</title>
		<author>
			<persName><forename type="first">Ofir</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2108" />
	</analytic>
	<monogr>
		<title level="m">International Conference On Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021. 12409v2</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Hierarchically gated recurrent neural network for sequence modeling</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songlin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiran</forename><surname>Zhong</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2311.04823</idno>
		<ptr target="https://arxiv.org/abs/2311.04823v1" />
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Hgrn2: Gated linear rnns with state expansion</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songlin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weixuan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuyang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weigao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiran</forename><surname>Zhong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.07904</idno>
		<ptr target="https://arxiv.org/abs/2404.07904v1" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://api.semanticscholar.org/CorpusID" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">160025533</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1606.05250v3" />
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<author>
			<persName><forename type="first">David</forename><surname>Rein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Betty</forename><forename type="middle">Li</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asa</forename><forename type="middle">Cooper</forename><surname>Stickland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackson</forename><surname>Petty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">Yuanzhe</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Dirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.12022</idno>
		<ptr target="https://arxiv.org/abs/2311.12022v1" />
		<title level="m">Gpqa: A graduate-level google-proof qa benchmark</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Sparse modular activation for efficient sequence modeling</title>
		<author>
			<persName><forename type="first">Liliang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2306.11197v1" />
	</analytic>
	<monogr>
		<title level="j">NEURIPS</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Efficient content-based sparse attention with routing transformers</title>
		<author>
			<persName><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saffar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00353</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference On Topology, Algebra And Categories In Logic</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Winogrande: An adversarial winograd schema challenge at scale</title>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1907.10641v2" />
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="99" to="106" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Lebras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><surname>Socialiqa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09728</idno>
		<ptr target="https://arxiv.org/abs/1904.09728v3" />
		<title level="m">Commonsense reasoning about social interactions</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Linear transformers are secretly fast weight programmers</title>
		<author>
			<persName><forename type="first">Imanol</forename><surname>Schlag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuki</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v139/schlag21a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
		<editor>
			<persName><forename type="first">Marina</forename><surname>Meila</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<date type="published" when="2021-07-24">18-24 July 2021. 2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="9355" to="9366" />
		</imprint>
	</monogr>
	<note>PMLR</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Flashattention-3: Fast and accurate attention with asynchrony and low-precision</title>
		<author>
			<persName><forename type="first">Jay</forename><surname>Preprint</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganesh</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Bikshandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Thakkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tri</forename><surname>Ramani</surname></persName>
		</author>
		<author>
			<persName><surname>Dao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.08608</idno>
		<ptr target="https://arxiv.org/abs/2407.08608v2" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Zeroscrolls: A zero-shot benchmark for long text understanding</title>
		<author>
			<persName><forename type="first">Uri</forename><surname>Shaham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maor</forename><surname>Ivgi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avia</forename><surname>Efrat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2305.14196</idno>
		<ptr target="https://arxiv.org/abs/2305" />
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date>14196v2</date>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Fast transformer decoding: One write-head is all you need</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.02150</idno>
		<ptr target="https://arxiv.org/abs/1911.02150v1" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05202</idno>
		<ptr target="https://arxiv.org/abs/2002.05202v1" />
		<title level="m">Glu variants improve transformer</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Simplified state space layers for sequence modeling</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Jimmy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Warrington</surname></persName>
		</author>
		<author>
			<persName><surname>Linderman</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Ai8Hw3AXqks" />
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Slimpajama: A 627b token cleaned and deduplicated version of redpajama</title>
		<author>
			<persName><forename type="first">Daria</forename><surname>Soboleva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Al-Khateeb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><forename type="middle">R</forename><surname>Steeves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Hestness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nolan</forename><surname>Dey</surname></persName>
		</author>
		<ptr target="https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Roformer: Enhanced transformer with rotary position embedding</title>
		<author>
			<persName><forename type="first">Jianlin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengfeng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Murtadha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfeng</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.09864</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Retentive network: A successor to transformer for large language models</title>
		<author>
			<persName><forename type="first">Yutao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqing</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jilong</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.08621</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<author>
			<persName><forename type="first">Gemma</forename><surname>Team</surname></persName>
		</author>
		<author>
			<persName><surname>Gemma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.08295</idno>
		<ptr target="https://arxiv.org/abs/2403.08295v1" />
		<title level="m">Open models based on gemini research and technology</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<author>
			<persName><forename type="first">Jamba</forename><surname>Team</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barak</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Arazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avshalom</forename><surname>Manevich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barak</forename><surname>Peleg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Aviram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Almagor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Fridman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Padnos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gissin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Jannai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dor</forename><surname>Muhlgay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dor</forename><surname>Zimberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Edden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Gerber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eran</forename><surname>Dolev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erez</forename><surname>Krakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erez</forename><surname>Safahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gal</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gal</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haim</forename><surname>Shachaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hofit</forename><surname>Rozenblum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Bata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inbal</forename><surname>Blass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itay</forename><surname>Magar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jhonathan</forename><surname>Dalmedigos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julie</forename><surname>Osin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Fadlon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matan</forename><surname>Rozman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Danos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mor</forename><surname>Gokhman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naama</forename><surname>Zusman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nir</forename><surname>Gidron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Ratner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Gat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oded</forename><surname>Rozen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ohad</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Leshno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omri</forename><surname>Antverg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Opher</forename><surname>Abend</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Or</forename><surname>Lieber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orit</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raz</forename><surname>Cohavi</surname></persName>
		</author>
		<author>
			<persName><surname>Alon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.12570</idno>
		<ptr target="https://arxiv.org/abs/2408.12570v1" />
		<title level="m">Jamba-1.5: Hybrid transformer-mamba models at scale</title>
		<editor>
			<persName><forename type="first">Roi</forename><surname>Belson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rom</forename><surname>Cohen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Roman</forename><surname>Gilad</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Shahar</forename><surname>Glozman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Shaked</forename><surname>Lev</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tal</forename><surname>Meirom</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tal</forename><surname>Delbari</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tomer</forename><surname>Ness</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tom</forename><surname>Asida</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tom</forename><surname>Ben Gal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Uriya</forename><surname>Braude</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yehoshua</forename><surname>Pumerantz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yonatan</forename><surname>Cohen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yuval</forename><surname>Belinkov</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yuval Peleg</forename><surname>Globerson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yoav</forename><surname>Levy</surname></persName>
		</editor>
		<editor>
			<persName><surname>Shoham</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Llama 2: Open foundation and fine-tuned chat models</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasmine</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prajjwal</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Bikel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Blecher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristian Canton</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Esiobu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jude</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyin</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Fuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedanuj</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Hartshorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saghar</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hakan</forename><surname>Inan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Kardas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktor</forename><surname>Kerkez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabel</forename><surname>Kloumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Artem</forename><surname>Korenev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Punit</forename><surname>Singh Koura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenya</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Liskovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinghai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuning</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushkar</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Molybog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Poulton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Reizenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rashi</forename><surname>Rungta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalyan</forename><surname>Saladi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Schelten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruan</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">Michael</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ranjan</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellen</forename><surname>Xiaoqing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binh</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adina</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Puxin</forename><surname>Xiang Kuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iliyan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Zarov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Kambadur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelien</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Stojnic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><surname>Scialom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.09288</idno>
		<ptr target="https://arxiv.org/abs/2307.09288v2" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Focused transformer: Contrastive training for context scaling</title>
		<author>
			<persName><forename type="first">Szymon</forename><surname>Tworkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konrad</forename><surname>Staniszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikołaj</forename><surname>Pacek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henryk</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Miłoś</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2307.03170v2" />
	</analytic>
	<monogr>
		<title level="j">NEURIPS</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Sequence length is a domain: Length-based overfitting in transformer models</title>
		<author>
			<persName><forename type="first">Dusan</forename><surname>Varis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.650</idno>
		<ptr target="https://aclanthology.org/2021.emnlp-main.650" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>
			<persName><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Scott</forename><surname>Wen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">-Tau</forename><surname>Yih</surname></persName>
		</editor>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-11">nov 2021</date>
			<biblScope unit="page" from="8246" to="8257" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><forename type="middle">M</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Squality: Building a long-document summarization dataset the hard way</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Preprint</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">Yuanzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelica</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2205.11465</idno>
		<ptr target="https://arxiv.org/abs/2205.11465v1" />
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Mmlu-pro: A more robust and challenging multi-task language understanding benchmark</title>
		<author>
			<persName><forename type="first">Yubo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueguang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ge</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuansheng</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhranil</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiguang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiming</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaran</forename><surname>Arulraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianle</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.01574</idno>
		<ptr target="https://arxiv.org/abs/2406.01574v4" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Chain-of-thought prompting elicits reasoning in large language models</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2201.11903v6" />
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<author>
			<persName><forename type="first">Kaiyue</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyu</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Lyu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.18510</idno>
		<ptr target="https://arxiv.org/abs/2402.18510v1" />
		<title level="m">Rnns are not transformers (yet): The key bottleneck on in-context retrieval</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<author>
			<persName><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><forename type="middle">N</forename><surname>Rabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Delesley</forename><forename type="middle">S</forename><surname>Hutchins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2203.08913</idno>
		<ptr target="https://arxiv.org/abs/2203.08913v1" />
		<title level="m">Memorizing transformers. International Conference On Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Efficient streaming language models with attention sinks</title>
		<author>
			<persName><forename type="first">Guangxuan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beidi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.17453</idno>
		<ptr target="https://arxiv.org/abs/2309" />
		<imprint>
			<date type="published" when="2023">2023. 17453v1</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Efficient streaming language models with attention sinks</title>
		<author>
			<persName><forename type="first">Guangxuan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beidi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=NG7sS51zVF" />
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations, ICLR 2024</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2024">May 7-11, 2024. 2024</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">On layer normalization in the transformer architecture</title>
		<author>
			<persName><forename type="first">Ruibin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunchang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v119/xiong20b.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, ICML 2020</title>
		<meeting>the 37th International Conference on Machine Learning, ICML 2020</meeting>
		<imprint>
			<date type="published" when="2020-07-18">13-18 July 2020. 2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="10524" to="10533" />
		</imprint>
	</monogr>
	<note>PMLR</note>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">Effective long-context scaling of foundation models</title>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Molybog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hejia</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prajjwal</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rashi</forename><surname>Rungta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Abinav Sankararaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kshitiz</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.16039</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Fla: A triton-based library for hardware-efficient implementations of linear attention mechanism</title>
		<author>
			<persName><forename type="first">Songlin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://github.com/sustcsonglin/flash-linear-attention" />
		<imprint>
			<date type="published" when="2024-01">January 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">Gated linear attention transformers with hardware-efficient training</title>
		<author>
			<persName><forename type="first">Songlin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bailin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rameswar</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.06635</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Big bird: Transformers for longer sequences</title>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guru</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avinava</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifan</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2007" />
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="17283" to="17297" />
			<date type="published" when="2020">2020. 14062v2</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Hellaswag: Can a machine really finish your sentence?</title>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1472</idno>
		<ptr target="https://arxiv.org/abs/1905.07830v1" />
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">Root mean square layer normalization. Neural Information Processing Systems</title>
		<author>
			<persName><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<idno type="DOI">10.5167/UZH-177483</idno>
		<ptr target="https://arxiv.org/abs/1910.07467v1" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">Proof-pile</title>
		<author>
			<persName><forename type="first">Edward</forename><surname>Ayers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangir</forename><surname>Azerbayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bartosz</forename><surname>Piotrowski</surname></persName>
		</author>
		<ptr target="https://github.com/zhangir-azerbayev/proof-pile" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Improving deep neural networks using softplus units</title>
		<author>
			<persName><forename type="first">Zhanlei</forename><surname>Hao Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenju</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jizhong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanpeng</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/IJCNN.2015.7280459</idno>
		<ptr target="https://ieeexplore.ieee.org/document/7280459" />
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title level="m" type="main">Efficient long sequence modeling via state space augmented transformer</title>
		<author>
			<persName><forename type="first">Simiao</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eren</forename><surname>Manavoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.08136</idno>
		<ptr target="https://arxiv.org/abs/2212.08136v1" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
