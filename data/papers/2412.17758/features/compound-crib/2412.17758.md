## Detailed Technical Explanations and Justifications for Researchers' Decisions

### 1. ARC Challenge vs. ARC Easy

The distinction between ARC Challenge and ARC Easy is primarily rooted in the evaluation setup rather than the inherent complexity of the tasks. Researchers have observed that the perceived difficulty of these benchmarks can be significantly influenced by how the evaluation is conducted. 

- **Evaluation Setup Influence**: The ARC Challenge is often perceived as more difficult because it employs a separation evaluation method, where each answer choice is evaluated independently. This method does not allow the model to leverage contextual information from other options, which can lead to a misrepresentation of the model's reasoning capabilities. In contrast, ARC Easy may appear simpler under the same evaluation conditions, but this is not necessarily indicative of the tasks' inherent complexity.

- **Switching Evaluation Methods**: By transitioning to an evaluation method that allows for simultaneous consideration of all answer choices (the options method), researchers can provide a more accurate reflection of a model's reasoning abilities. This shift can lead to substantial improvements in accuracy, as models can utilize comparative reasoning to arrive at the correct answer.

### 2. Evaluation Setup

The two primary conventions for evaluating multiple-choice problems are:

- **Separation**: In this method, the model evaluates each answer independently, without the context of other options. This can lead to challenges in answering questions that require comparative reasoning, as the model lacks the necessary context to make informed decisions.

- **Options**: This method allows the model to evaluate all candidate answers together, simulating a more natural reasoning process. By presenting all options simultaneously, the model can make direct comparisons, which is essential for questions that are inherently comparative.

### 3. Impact of Evaluation Method

The impact of switching from the separation method to the options method is profound:

- **Accuracy Improvements**: Research indicates that this switch can improve model accuracy by as much as 35%. For instance, the Llama 3.1 70B model's accuracy on the ARC Challenge increased from 64% (using separation) to 93% (using options). This dramatic improvement underscores the importance of evaluation methodology in accurately assessing model performance.

### 4. Hardly Answerable in Separation

A significant portion of questions in both ARC Easy (21%) and ARC Challenge (31%) datasets are inherently comparative. These questions are difficult to answer effectively when evaluated in separation because:

- **Comparative Nature**: Questions that require comparing multiple options cannot be adequately addressed without the context of all available choices. For example, determining which animal has the greatest mass necessitates comparing the masses of all options, which is not possible in a separation setup.

### 5. Model Performance Discrepancies

The discrepancies in model performance across different evaluation methods can lead to misleading interpretations of a model's reasoning capabilities:

- **False Implications of Deficits**: Previous evaluations using the separation method may suggest that models have significant reasoning deficits. For example, the Llama 3.1 70B model scored only 48% on OpenBookQA in the separation method but achieved 89% accuracy when evaluated using the options method. This stark contrast highlights how evaluation methods can distort perceptions of model capabilities.

### 6. Recommendations for Multi-Choice Evaluation

Based on the findings, researchers recommend the following for evaluating multiple-choice questions:

- **Use Options for Evaluation**: This method aligns more closely with human reasoning and allows models to leverage comparative information, leading to a more accurate assessment of their capabilities.

- **Avoid Separation for Complex Questions**: Separation should only be used for straightforward questions (e.g., yes/no) where comparative reasoning is not necessary.

### 7. Likelihood Scoring

While likelihood-based scoring is suitable for language modeling tasks, it may not be ideal for all multiple-choice problems:

- **Suitability**: Likelihood scoring works well for tasks that involve generating text or selecting from a constrained set of options. However, for comparative reasoning tasks, the options method may provide a more accurate reflection of model performance.

### 8. Limitations of Current Evaluations

The lack of transparency in reporting evaluation methods in LLM studies complicates the understanding of performance metrics:

- **Need for Clarity**: Researchers often do not disclose whether they used separation or options in their evaluations, making it difficult to interpret results accurately. This lack of clarity can lead to misconceptions about model capabilities.

### 9. Conclusion

The shift from evaluating answers in isolation to evaluating them alongside all options is crucial for accurately assessing model capabilities in multiple-choice problems. By adopting the options method, researchers can provide a more realistic evaluation of models' reasoning abilities, ultimately leading to a better understanding of their performance across various benchmarks. This change is essential for ensuring that evaluations reflect the true complexity of the tasks at hand, rather than being influenced by the evaluation methodology itself.