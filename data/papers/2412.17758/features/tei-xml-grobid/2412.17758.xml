<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">In Case You Missed It: ARC &apos;Challenge&apos; Is Not That Challenging</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-12-23">23 Dec 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Łukasz</forename><surname>Borchmann</surname></persName>
							<email>lukasz.borchmann@snowflake.com</email>
							<affiliation key="aff0">
								<orgName type="department">Snowflake AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">In Case You Missed It: ARC &apos;Challenge&apos; Is Not That Challenging</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-12-23">23 Dec 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">F6A47836BF3F1D1A69161D43DB6BA630</idno>
					<idno type="arXiv">arXiv:2412.17758v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ARC Challenge appears more difficult than ARC Easy for modern LLMs primarily due to an evaluation setup that prevents direct comparison of answer choices rather than inherent complexity. Although some researchers have quietly shifted to a more appropriate scheme over the last year, the implications of this change have yet to be widely acknowledged. We highlight this overlooked shift, show how similar evaluation practices falsely imply reasoning deficits in other benchmarks, and demonstrate that fairer methods dramatically reduce performance gaps (e.g. on SIQA) and even yield superhuman results (OpenBookQA). In doing so, we reveal how evaluation shapes perceived difficulty and offer guidelines to ensure that multiple-choice evaluations accurately reflect actual model capabilities.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A substantial set of benchmarks regularly employed in LLM testing consists of multiple-choice problems, commonly considered in a setup where each provided option is scored under the model, and the one with the highest likelihood is compared against the gold standard to determine accuracy. This refers, among others, to popular evaluators of MMLU <ref type="bibr" target="#b14">(Hendrycks et al., 2021)</ref>, ARC Easy and Challenge <ref type="bibr" target="#b5">(Clark et al., 2018)</ref>, BoolQ <ref type="bibr" target="#b4">(Clark et al., 2019)</ref>, <ref type="bibr">RACE (Lai et al., 2017)</ref>, OpenBookQA <ref type="bibr" target="#b17">(Mihaylov et al., 2018)</ref>, PIQA <ref type="bibr" target="#b2">(Bisk et al., 2019)</ref>, SIQA <ref type="bibr" target="#b23">(Sap et al., 2019)</ref>, COPA <ref type="bibr" target="#b12">(Gordon et al., 2011)</ref>, and HellaSwag <ref type="bibr" target="#b26">(Zellers et al., 2019)</ref>.</p><p>Details of this setup differ but generally follow one of the two conventions. Under one convention, the model considers each candidate answer in separation , without alternative options displayed (Figure <ref type="figure">2</ref>), while under the other, the model sees all candidate options together in the prompt (Figure <ref type="figure" target="#fig_0">3</ref>). We argue that the first setup is commonly overused and rarely preferred since it does not simulate the natural reasoning context in which multiple</p><p>Model Separation Options Challenge (separation) Challenge (options) ARC Easy (separation) ARC Easy (options) Mistral Large (2407) -18% -3% 67% 95% 86% 98% Qwen 2.5 72B -20% -4% 63% 95% 83% 99% Qwen 2.5 32B -18% -3% 59% 94% 77% 98% Llama 3.1 70B -20% -5% 64% 93% 84% 98% Yi 1.5 34B -22% -5% 60% 93% 82% 97% Mixtral 8x22B -18% -6% 67% 91% 86% 97% Mixtral 8x7B -19% -9% 66% 85% 85% 95% Llama 3.1 8B -25% -11% 55% 82% 80% 93% -30% -22% -15% -8% 0% Mistral Large Qwen 2.5 72B Qwen 2.5 32B Llama 3.1 70B -5% -3% -4% -3% -20% -18% -20% -18% Separation Options -30% -23% -15% -8% 0% Yi 1.5 34B Mixtral 8x22B Mixtral 8x7B Llama 3.1 8B -11% -9% -6% -5% -25% -19% -18% -22% 1 Figure 1: Difference between ARC Challenge and ARC Easy accuracies when considering each answer separately compared to seeing all options. The gap is vastly reduced, up to six times in this comparison.</p><p>choices are compared directly. Importantly, it introduces a false notion of how challenging a particular problem is, as switching from the first to the second might result in a 35% improvement in model accuracy, as shown in Section 2 experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Hardly answerable in separation</head><p>Consider the question, 'Which of these items contains only a solution?' Given the option 'a jar of pickles,' confronting a single item with a question and assessing whether pickles fulfill the definition of the solution suffices. They do not, so this option is incorrect. The question can be addressed under both evaluation setups because it does not require the availability of other options, such as 'a can of mixed fruit.'</p><p>Q: How is a pond different from a lake? A: SCORE CHOICES Lakes are used for recreation Ponds have moving water Ponds are smaller and shallower Ponds are not surrounded by land Ponds support different ecosystems 0.1 0.3 0.2 0.2 0.2 NORM BY LENGTH 0.2 0.2 0.3 0.2 0.1</p><p>Figure 2: Model considers particular choices in separation without knowing the alternative (prompt includes only the question). Because options may vary in length, it is a good practice to normalize them <ref type="bibr" target="#b8">(Gao, 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q:</head><p>How is a pond different from a lake? In contrast, some questions inherently demand comparative evaluation: let us think about 'Which of these most likely has the greatest mass?' and the option 'puppy.' This question's answer cannot be determined without comparing the mass of the 'puppy' to the masses of all other provided options. It is the greatest compared to 'chicken' or 'lizard' but not in the context of 'horse' or 'elephant.' Though it can work to some extent, relying on the likelihood assigned in separation to each of the animals is an unreasonable way of determining the heaviest one. It feels natural to provide the model with the options to choose from instead because it allows the model to directly compare and contextualize choices, reflecting a more authentic reasoning process. This aspect, however, is commonly overlooked.</p><p>Specifically, such 'hardly answerable in separation' questions are prevalent in ARC datasets, constituting 21% of ARC Easy and 31% of ARC Challenge (see Appendix B). Despite this fact, it is widespread to evaluate them without seeing all of the options simultaneously <ref type="bibr">(Touvron et al., 2023a,b;</ref><ref type="bibr">Jiang et al., 2023;</ref><ref type="bibr" target="#b20">Peng et al., 2023;</ref><ref type="bibr">01. AI et al., 2024;</ref><ref type="bibr">Gemma Team et al., 2024b, inter alia)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Impact on evaluation results</head><p>Figure <ref type="figure">4</ref> shows the difference in model accuracy when options are presented in isolation versus all at once. Not surprisingly, different setups hugely change the evaluation results, partly because of the vast presence of 'hardly answerable in separation' questions and partially because such a setup, equivalent to what human test takers see, doesn't introduce unnecessary obstacles.</p><p>For example, switching from separation to options improves the Llama 3.1 70B ARC Challenge accuracy from 64% to 93%, rendering this ARC subset significantly less challenging. Moreover, since the procedure change has a much higher impact on ARC Challenge than on ARC Easy, switching reduces the accuracy gap between these subsets as much as six-fold (Figure <ref type="figure">1</ref>). These findings suggest that the previously perceived difficulty was primarily an artifact of the evaluation method rather than the tasks' complexity.</p><p>The difference seems somewhat known in the LLM community, but not broadly, and needs to be stated explicitly. E.g. concerning the Llama family, authors seem to silently switch from separation to options between Llama 2 and Llama 3, similar to Mistral between Mixtral 8x7B and Mixtral 8x22B, or DeepSeek before their V2 (detailed assessment available in Appendix A).</p><p>3 Are other benchmarks affected?</p><p>Yes. Analogous changes in evaluation procedures would vastly improve OpenBookQA scores. Concerning Llama 3.1 70B, one can achieve improvement from 48% to 89% (see Figure <ref type="figure">5</ref>). For some reason, most authors who switched from separation to options in ARC evaluation did not follow on some other multi-choice problems.</p><p>If they switched, they could notice that Open-BookQA is essentially solved, as current models achieve scores above human performance (92% compared to 96% of Qwen 2.5 72B).</p><p>In the case of SIQA, reformulation leads to a 24% increase in Llama 70B accuracy. However, the best models perform 5% below the human baseline,</p><p>0% 25% 50% 75% 100% Mistral Large Qwen 2.5 72B Qwen 2.5 32B Llama 3.1 70B Yi 1.5 34B Mixtral 8x22B Mixtral 8x7B Llama 3.1 8B 82% 85% 91% 93% 93% 94% 95% 95% 55% 66% 67% 60% 64% 59% 63% 67% Separation Options 1 Figure 4: ARC Challenge evaluation results depending on whether the model sees other options or considers each answer separately. Differences reach up to 35%, and assumed setup impacts model rankings. 0% 50% 100% Llama 3.1 8B Llama 3.1 70B Mistral Large Qwen 2.5 72B 96% 91% 89% 74% 47% 53% 48% 45% Separation Options -3% 67% 95% 86% 98% -4% 63% 95% 83% 99% -3% 59% 94% 77% 98% -5% 64% 93% 84% 98% -5% 60% 93% 82% 97% -6% 67% 91% 86% 97% -9% 66% 85% 85% 95% 11% 55% 82% 80% 93% 74% 89% 91% 96% -30% -23% -15% -8% 0% Yi 1.5 34B Mixtral 8x22B Mixtral 8x7B Llama 3.1 8B -11% -9% -6% -5% -25% -19% -18% -22% Human 1 Figure 5: OpenBookQA evaluation results depending on whether the model sees other options or considers each answer separately. In a setup with options, current models outperform human test takers.</p><p>suggesting room for improvement (Figure <ref type="figure">6</ref>).</p><p>These dramatic increases, however, call into question previous interpretations of model capability on both SIQA and OpenBookQA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Why does it matter?</head><p>We argue that the benchmark's challenge should result from the inherent complexity of the knowledge or reasoning required, not its formulation or evaluation procedure.</p><p>The separation setup is unnecessarily complicated and not consistent with how humans would approach the multi-choice problem, leading to existing assessments of human performance being incompatible. For example, the fact that strong LLMs perform 30% worse than humans on SIQA doesn't mean they are deficient in commonsense reasoning about social situations if under options the difference largely disappears. This mismatch can falsely suggest deficits in reasoning capabilities that are not truly present.</p><p>0% 50% 100% Llama 3.1 8B Llama 3.1 70B Mistral Large Qwen 2.5 72B 81% 76% 76% 66% 63% 52% 52% 50% Separation Options OpenBookQA Llama 3.1 8B 45% 74% Llama 3.1 70B 48% 89% Mistral Large 53% 91% Qwen 2.5 72B 47% 96% SocialIQA Llama 3.1 8B 50% 66% Llama 3.1 70B 52% 76% Mistral Large 52% 76% Qwen 2.5 72B 63% 81% Llama 3.1 405B 56% 75% -30% -23% -15% -8% 0% Yi 1.5 34B Mixtral 8x22B Mixtral 8x7B Llama 3.1 8B -11% -9% -6% -5% -25% -19% -18% -22% Human 1 Figure 6: SIQA evaluation results depending on whether the model sees other options or considers each answer separately. Reformulation leads to up to 24% improvement.</p><p>Notably, the gap between LLMs and humans on SIQA has been previously used to argue that LLMs might lack social intelligence and social commonsense <ref type="bibr" target="#b22">(Sap et al., 2022)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Suggestions for multi-choice eval</head><p>There are many arguments for using the options for the evaluation of multi-choice QA problems. We have already described a few, including the presence of 'hardly answerable in separation' questions and the fact it is consistent with the usual approach to assessing human performance, as humans naturally consider all choices in a single context.</p><p>Other benefits include enabling compatible evaluation in a likelihood and generative manner, allowing one to obtain comparable scores with LLMs behind closed and limited APIs. Moreover, it eliminates the need to decide which normalization method to use when aggregating scores from several option tokens, which is, to some extent, arbitrary and impacts model ranking.</p><p>Nevertheless, it is not preferred in all cases commonly considered under the likelihood-scoring evaluation scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Why likelihood scoring in the first place?</head><p>Likelihood-based scoring is a natural choice for problems from pure language modeling, Winograd schemas, or fill-in-the-gap setups such as encountered in LAMBADA <ref type="bibr">(Paperno et al., 2016)</ref>, Hel-laSwag <ref type="bibr" target="#b2">(Bisk et al., 2019)</ref>, or WindoGrande <ref type="bibr" target="#b21">(Sakaguchi et al., 2019)</ref> datasets.</p><p>For other problems, it is effectively a variant of constrained decoding, that is, the model is restricted to selecting from given candidate options rather than generating open-ended text. It guarantees that models will not emit CoTs before answering the question and removes the need for output postprocessing, such as extracting the letter associated with the selected option and normalizing its casing. Moreover, it allows us to obtain meaningful results with base models, e.g. intermediate checkpoints from LLMs' self-supervised pretraining since we are constraining the output to one of the most probable options under the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">To show, or not to show options</head><p>Suppose the options are of equal length, and it is not helpful to consider them simultaneously. This is the case when we deal with a straightforward yes/no response, and no comparative reasoning is necessary, as in the BoolQ dataset <ref type="bibr" target="#b4">(Clark et al., 2019)</ref>. In similar scenarios, there are no arguments for dropping separation in favor of options .</p><p>We are in the position that the options variant is preferred if there is a risk of a 'hardly answerable in separation' question presence (Section 1.1) or it simply makes it easier to consider all of the options at once because it ensures the model can leverage direct comparisons. This seems to be the case for virtually all other multi-choice QA problems, such as MMLU <ref type="bibr" target="#b14">(Hendrycks et al., 2021)</ref>, ARC <ref type="bibr" target="#b5">(Clark et al., 2018)</ref>, OpenBookQA <ref type="bibr" target="#b17">(Mihaylov et al., 2018)</ref>, PIQA <ref type="bibr" target="#b2">(Bisk et al., 2019)</ref>, or SIQA <ref type="bibr" target="#b23">(Sap et al., 2019)</ref>. In fact, most similar problems are already being evaluated in the options scheme.</p><p>Arguably, options has some possible or actual disadvantages: the order of the presented choices might impact the evaluation results (e.g. models might bias toward the first listed choice), it might be easier to exploit pattern recognition, and the setup requires slightly more compute. Nevertheless, we consider these to be outweighed by benefits and recommend broad use of the options</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>Despite our recommendations and the benefits they entail, several limitations and uncertainties exist in identifying precisely how evaluation methods were employed in previously reported results.</p><p>Firstly, because authors of LLMs' technical reports rarely or never report such details, our assessment of which of the options and separation they employed is based on attempting to replicate reported accuracy scores under both setups and observing which condition aligns best (Appendix A). Fortunately, given the magnitude of the observed differences, the performance gap is so large that one can differentiate between the mentioned approaches with a high degree of certainty.</p><p>Secondly, in a search for the separation overuse candidates, we mainly relied on intuition. We considered the most popular benchmarks due to their widespread use and availability of performance data, later confirming the intuition experimentally. Though it was a tale of ARC, Open-BookQA, and SIQA, many widely used benchmarks may benefit from revisiting their evaluation setup.</p><p>Finally, it could be a blog post, but it feels better to write a PDF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Summary</head><p>We draw the community's attention to shifting from evaluating answers in isolation to evaluating them alongside all other options. Over the last year, such a change happened in the reported ARC Challenge and ARC Easy scores, vastly impacting their evaluation results. After discussing the implications, we considered whether other popular benchmarks might undergo similar reformulation, identifying OpenBookQA and SIQA as candidates. In the former, recent models outperform humans, even though there is a room of 40% between humans and LLMs in the widespread setup. The fact that the gap drastically narrows under the all-options evaluation method highlights how the testing format can distort perceived difficulty.</p><p>We concluded with a guideline for evaluating multi-choice problems, arguing that the setup where the model sees all options is preferred over considering each answer separately, except for casual or masked language modeling problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Claiming setup used by other authors</head><p>As authors of LLM technical reports rarely or never provide such details, we redo their evaluations in options and separation setups. If the reported score is in the same ballpark as one of these, and visibly distant from the other one, we claim they used the first. This assessment is backed by the notion that no other change in the prompt could cause a 20%+ improvement in ARC Challenge scores. The exception could be using a generative setup with CoT for some heavy reasoners, but we do not suspect authors to use CoT if they are not reporting this because it would be a serious flaw.</p><p>The results of this analysis for the ARC Challenge are presented in Table <ref type="table" target="#tab_5">1</ref>. Prompt 'reverse engineering' becomes more troublesome in the context of SIQA and OpenBookQA datasets (Table <ref type="table">2</ref>-3) as some authors do not directly report scores but average them with other commonsense reasoning problems. We're not claiming any setup for these.</p><p>Finally, some authors tackling OpenBookQA followed <ref type="bibr" target="#b3">Brown et al. (2020)</ref> in normalizing the likelihood by the likelihood of the completion given 'Answer:' as context. To address this possibility, we introduce two additional variants referred to as separation b and options b .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Estimating number of questions hardly answerable in separation</head><p>To determine whether questions are answerable given a single option or require the context of other options, we process them in batches of 20 using gpt-4o-2024-11-20 model and the following prompt with few-shot examples:</p><p>Consider the question, "Which of these items → contains only a solution?" Given the → option "a jar of pickles," confronting a → single item with a question and assessing → whether pickles fulfill the definition → of the solution suffices. They do not, so → this option is incorrect. Now let us think about "Which of these most → likely has the greatest mass?" and the → option "puppy." It can be considered only → with other options because it is the → greatest compared to "chicken" or "lizard → " but not in the context of "horse" or " → elephant".</p><p>These questions represent two classes of → questions: "answerable without other → options" and "unanswerable without other → options". Other examples of "answerable without other → options" are: -Kerry made a simple flashlight. She recorded → the following statements in her lab book. → Which statement is an inference? ( → Answerable, because it suffices to → compare options against the definition of → inference) -A scientist on a field trip discovered a new → organism. She examined its cells under a → microscope and observed several different → structures, including a nucleus, a cell → wall, and some chloroplasts. This → organism would correctly be classified in → which of the following kingdoms? ( → Answerable, because it can be answered by → deciding if the kingdom provided in the → option can be associated with having a → nucleus, a cell wall, and chloroplasts) -Many types of motion occur in our solar system. → Which type of motion describes one Earth → year? (Answerable, because it suffices → to validate if the motion describes one → year or not) -When trees develop leaves in the spring, 10 → changes occur on the forest floor. Why → does the development of leaves cause → changes on the forest floor? (Answerable, → because it is enough to verify if a → particular option described the possible → cause of change) -Using a softball bat to hit a softball is an → example of using which simple machine? ( → Answerable, because all one needs to do → is to check if the described simple → machine is the explanation of how a → softball bat works) -Which is a statement about climate? ( → Answerable, because it is possible to → verify a single option against the → climate definition) -How do word processors on computers benefit → most students? (Answerable, because it → can be answered in separation whether → most students benefit from this feature → of the word processor) -Photosynthesis occurs in which of these → organisms? (Answerable because it → suffices to check if the organism → mentioned in the option performs → photosynthesis) -Which two theories of Moon formation propose → that much or all of the material → comprising the Moon came from Earth? ( → Because it suffices to validate if both → theories mentioned in a single option → describe the Moon as formed from Earth → material) -Plants and animals are composed of organic → compounds. Which of the following are the → common elements found in organic → compounds? (Answerable, because it → suffices to check if the option consists → of compounds appearing in both plants and → animals) Other examples of "unanswerable without other → options" are: -A ball is dropped from different heights. When → the ball is dropped from the highest Model Reported Measured s / o Assessment Llama 65B (Touvron et al., 2023a) 56.0 55.6 / 70.2 separation Llama 2 70B <ref type="bibr">(Touvron et al., 2023b)</ref> 57.4 57.4 / 79.6 separation Llama 3 70B <ref type="bibr" target="#b13">(Grattafiori et al., 2024)</ref> 92.9 64.2 / 91.3 options Mistral 7B <ref type="bibr">(Jiang et al., 2023)</ref> 55.5 54.1 / 74.6 separation Mixtral 8x7B <ref type="bibr" target="#b16">(Jiang et al., 2024)</ref> 59.7 59.9 / 83.3 separation Mixtral 8x22B <ref type="bibr" target="#b18">(Mistral AI, 2024)</ref> 91.3 † 70.7 / 91.8 options DeepSeek 67B <ref type="bibr">(DeepSeek AI et al., 2024a)</ref> 59.0 60.1 / 84.6 options DeepSeek V2 <ref type="bibr">(DeepSeek AI et al., 2024b)</ref> 92.4 † 70.3 / 92.2 options Qwen 14B <ref type="bibr" target="#b1">(Bai et al., 2023)</ref> 84.4 47.3 / 86.6 options Yi 6B <ref type="bibr">(01. AI et al., 2024)</ref> 50.3 † 55.7 / 80.5 separation Gemma 7B (Gemma <ref type="bibr">Team et al., 2024b)</ref> 53.2 53.2 / 79.0 separation Gemma 2 27B (Gemma <ref type="bibr">Team et al., 2024a)</ref> 71.4 65.8 / 90.0 separation  <ref type="bibr" target="#b1">(Bai et al., 2023)</ref> 77.9 56.2 / 78.6 options Yi 6B <ref type="bibr">(01. AI et al., 2024)</ref> -52.5 / 71.0 -Gemma 7B (Gemma <ref type="bibr">Team et al., 2024b)</ref> 51.8 51.8 / 60.0 separation Gemma 2 27B (Gemma <ref type="bibr">Team et al., 2024a)</ref> 53.7 58.3 / 70.0 separation Table <ref type="table">2</ref>: Measured and reported SIQA scores with our assessment of the setup used by authors. Some authors do not directly report scores but average them with other commonsense reasoning problems (denoted by ⋄ ), making our assessment unlikely to succeed.</p><p>→ height, it makes the greatest noise or → vibration when it lands on the ground. → What is the best explanation for the ball → making the greatest noise? (Unanswerable → , because in order to choose the best → explanation, one needs to consider → several explanations) -If an experiment results in data that do not → support the hypothesis, what is the most → likely step to take next? (Unanswerable, → because in order to choose the most → likely step, one needs to consider the → less likely alternative) -When an igneous intrusion comes into contact → with surrounding rock, the surrounding → rock will (Unanswerable, because one can → easily verify if an option describes the → possible outcome of contact with → surrounding rock) -A research scientist writes a paper on the → initial regrowth of a forest after a fire → has damaged the entire ecosystem. Which → title would be best for the paper? ( → Unanswerable, because it is impossible to → decide the best title without comparing → it to other titles) -Jessica wants to see cells in an oak tree leaf.</p><p>→ Which tool is best for Jessica to use to → see the cells? (Unanswerable, because → choosing the best tool depends on the set → of tools considered and is ambiguous → without a complete list of options → considered) -Which factor is most likely to cause the → number of rabbits living in an area to → increase? (Unanswerable, because choosing → the most likely case requires checking → all of the causes under consideration)</p><p>Now classify the following statements either as → "unanswerable" or "answerable" in → separation.</p><p>Answer in a form of JSONL file containing " → question", "category", and "explanation" → keys.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Model sees the context of all possible options in the prompt. Because all of the options are single letters (likely single tokens), scores require no normalization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1 :</head><label>1</label><figDesc>Measured and reported ARC Challenge scores with our assessment of the setup used by authors. The 25-shot prompting used in contrast to the 0-shot is denoted by † (in the case authors use such a setup in their report).</figDesc><table><row><cell>Model</cell><cell cols="2">Reported Measured s / o</cell><cell>Assessment</cell></row><row><cell>Llama 65B (Touvron et al., 2023a)</cell><cell>52.3</cell><cell>50.3 / 60.1</cell><cell>separation</cell></row><row><cell>Llama 2 70B (Touvron et al., 2023b)</cell><cell>50.7</cell><cell>50.8 / 66.9</cell><cell>separation</cell></row><row><cell>Llama 3 70B (Grattafiori et al., 2024)</cell><cell>52.2</cell><cell>51.2 / 72.9</cell><cell>separation</cell></row><row><cell>Mistral 7B (Jiang et al., 2023)</cell><cell>-⋄</cell><cell>50.9 / 62.4</cell><cell>-</cell></row><row><cell>Mixtral 8x7B (Jiang et al., 2024)</cell><cell>-⋄</cell><cell>49.4 / 65.1</cell><cell>-</cell></row><row><cell>Mixtral 8x22B (Mistral AI, 2024)</cell><cell>-</cell><cell>51.1 / 67.3</cell><cell>-</cell></row><row><cell>DeepSeek 67B (DeepSeek AI et al., 2024a)</cell><cell>-</cell><cell>51.6 / 61.6</cell><cell>-</cell></row><row><cell>DeepSeek V2 (DeepSeek AI et al., 2024b)</cell><cell>-</cell><cell>52.2 / 70.0</cell><cell>-</cell></row><row><cell>Qwen 14B</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Reported Measured s / o / s b / o b Assessment Llama 65B <ref type="bibr">(Touvron et al., 2023a)</ref> 60.2 47.0 / 59.0 / 60.2 / 56.2 separation b Llama 2 70B <ref type="bibr">(Touvron et al., 2023b)</ref> 60.2 48.8 / 73.0 / 60.0 / 65.8 separation b Llama 3 70B <ref type="bibr" target="#b13">(Grattafiori et al., 2024)</ref> 47 The model returned batches of JSONL, such as:</p><p>{"question": "Which best describes the structure → of an atom?", "category": "unanswerable → ", "explanation": "Determining the best → description requires comparing all → options to identify the most accurate one → ."} {"question": "Which is a statement about climate → ?", "category": "answerable", " → explanation": "It is possible to verify → each option against the definition of → climate to determine the correct answer → ."} {"question": "During which activity should a → student wear goggles?", "category": " → answerable", "explanation": "It suffices → to check if the activity described in the → option requires goggles for safety."} {"question": "Which natural event occurs with → the most frequency?", "category": " → unanswerable", "explanation": " → Determining the most frequent event → requires comparing the frequency of all → listed events."}</p><p>During this procedure, we estimated the percentage as 21% for ARC Easy and 31% for ARC Challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Evaluation details</head><p>All evaluations were conducted using lm_eval 1170ef9 <ref type="bibr" target="#b9">(Gao et al., 2024)</ref>. We used HF implementations and base variants of models (exact versions in Table <ref type="table">4</ref>) with either default prompts and acc_norm metric or prompts outlined below. Inferences were performed with bf16 precision, flash attention (whenever available), and dynamic batch size, using transformers 4.47.0 and torch 2.5.1 on eight NVIDIA H100 GPUs.</p><p>Model huggyllama/llama-65b meta-llama/Llama-2-70b-hf meta-llama/Meta-Llama-3-70B mistralai/Mistral-7B-v0.1 mistralai/Mixtral-8x7B-v0.1 mistralai/Mixtral-8x22B-v0.1 deepseek-ai/deepseek-llm-67b-base deepseek-ai/DeepSeek-V2 Qwen/Qwen-14B 01-ai/Yi-6B google/gemma-7b google/gemma-2-27b with doc_to_text() defined as: def doc_to_text(doc): prompt = "Question: " + doc["question"] + "\ → nOptions:\n" for l, t in zip(doc["choices"]["label"], doc → ["choices"]["text"]): prompt += l + '. ' + t + '\n' prompt += "Answer: " return prompt Analogous changes were introduced to Open-BookQA and SIQA templates.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Yi: Open Foundation Models by 01</title>
		<author>
			<persName><surname>Ai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<publisher>AI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jinze</forename><surname>Bai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">Qwen Technical Report</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">PIQA: Reasoning about Physical Commonsense in Natural Language</title>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
		<idno>CoRR, abs/1911.11641</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Language Models are Few-Shot Learners</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><surname>Brown</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2020. 2005.14165</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<title level="m">BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Think you have Solved Question Answering?</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isaac</forename><surname>Cowhey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carissa</forename><surname>Schoenick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Try ARC, the AI2 Reasoning Challenge</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">2024a. DeepSeek LLM: Scaling Open-Source Language Models with Longtermism</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Deepseek</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Deepseek</surname></persName>
		</author>
		<title level="m">DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Multiple Choice Normalization in LM Evaluation</title>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A framework for few-shot language model evaluation</title>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.12608602</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">2024a. Gemma 2: Improving Open Language Models at a Practical Size</title>
		<author>
			<persName><forename type="first">Gemma</forename><surname>Team</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Gemma</forename><surname>Team</surname></persName>
		</author>
		<title level="m">Gemma: Open Models Based on Gemini Research and Technology</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Choice of plausible alternatives: An evaluation of commonsense causal reasoning</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">S</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melissa</forename><surname>Roemmele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Grattafiori</surname></persName>
		</author>
		<title level="m">The Llama 3 Herd of Models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Collin</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<title level="m">Measuring Massive Multitask Language Understanding</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Albert</forename><forename type="middle">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Bamford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devendra</forename><surname>Singh Chaplot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>De Las Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Bressand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gianna</forename><surname>Lengyel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucile</forename><surname>Saulnier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renard</forename><surname>Lélio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lavaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothée</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Lacroix</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>and William El Sayed. 2023. Mistral 7B</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Albert</forename><forename type="middle">Q</forename><surname>Jiang</surname></persName>
		</author>
		<title level="m">Large-scale ReAding Comprehension Dataset From Examinations</title>
		<imprint>
			<date type="published" when="2017">2024. 2017</date>
		</imprint>
	</monogr>
	<note>Mixtral of Experts</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<title level="m">Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Mistral</surname></persName>
		</author>
		<title level="m">Cheaper, Better, Faster, Stronger</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Denis</forename><surname>Paperno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ngoc</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raffaella</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandro</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Pezzelle</surname></persName>
		</author>
		<author>
			<persName><surname>Baroni</surname></persName>
		</author>
		<title level="m">Gemma Boleda, and Raquel Fernández. 2016. The lambada dataset: Word prediction requiring a broad discourse context</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">RWKV: Reinventing RNNs for the Transformer Era</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Peng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Winogrande: An adversarial winograd schema challenge at scale</title>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neural theory-of-mind? on the limits of social intelligence in large LMs</title>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.emnlp-main.248</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Abu Dhabi</addrLine></address></meeting>
		<imprint>
			<publisher>United Arab Emirates. Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3762" to="3780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Lebras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<title level="m">Socialiqa: Commonsense reasoning about social interactions</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothée</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Rozière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Azhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelien</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Edouard Grave, and Guillaume Lample. 2023a. LLaMA: Open and Efficient Foundation Language Models</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">2023b. Llama 2: Open Foundation and Fine-Tuned Chat Models</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">HellaSwag: Can a Machine Really Finish Your Sentence?</title>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
