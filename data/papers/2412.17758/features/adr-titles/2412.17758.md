- Evaluation Setup for Multiple-Choice Problems
- Comparison of Separation vs. Options in Model Evaluation
- Impact of Evaluation Method on Perceived Difficulty
- Handling 'Hardly Answerable in Separation' Questions
- Implications of Evaluation Practices on Benchmark Performance
- Guidelines for Fair Multi-Choice Evaluations
- Transition from Separation to Options in ARC Evaluation
- Effects of Evaluation Method on Model Accuracy
- Addressing Misinterpretations of Model Capabilities
- Consistency of Evaluation with Human Test-Taking Approaches
- Normalization Methods for Aggregating Scores
- Likelihood-Based Scoring vs. Open-Ended Generation
- Use Cases for Showing vs. Not Showing Options
- Influence of Question Complexity on Evaluation Method Choice
- Recommendations for Future Multi-Choice QA Evaluations