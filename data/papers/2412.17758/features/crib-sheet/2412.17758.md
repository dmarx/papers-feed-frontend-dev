- **ARC Challenge vs. ARC Easy**: ARC Challenge appears more difficult due to evaluation setup rather than inherent complexity; switching evaluation methods can significantly alter perceived difficulty.
  
- **Evaluation Setup**: Two conventions exist for evaluating multiple-choice problems:
  - **Separation**: Model evaluates each answer independently.
  - **Options**: Model evaluates all candidate answers together, simulating natural reasoning.

- **Impact of Evaluation Method**: 
  - Switching from separation to options can improve model accuracy by up to 35%.
  - Example: Llama 3.1 70B accuracy on ARC Challenge improves from 64% (separation) to 93% (options).

- **Hardly Answerable in Separation**: 
  - 21% of ARC Easy and 31% of ARC Challenge questions are inherently comparative, making them difficult to answer in separation.
  
- **Model Performance Discrepancies**: 
  - Previous evaluations may falsely imply reasoning deficits due to the separation method.
  - Example: Llama 3.1 70B scores 48% on OpenBookQA in separation but 89% in options.

- **Recommendations for Multi-Choice Evaluation**:
  - Use options for evaluation to reflect true model capabilities and align with human reasoning.
  - Avoid separation unless questions are straightforward (e.g., yes/no).

- **Likelihood Scoring**: 
  - Likelihood-based scoring is suitable for language modeling tasks but may not be ideal for all multi-choice problems.
  
- **Limitations of Current Evaluations**: 
  - Lack of transparency in reporting evaluation methods in LLM studies complicates understanding of performance metrics.
  
- **Conclusion**: 
  - Shift to evaluating answers alongside all options is crucial for accurate assessment of model capabilities in multi-choice problems.