{
  "arxivId": "2412.17758",
  "title": "In Case You Missed It: ARC 'Challenge' Is Not That Challenging",
  "authors": "\u0141ukasz Borchmann",
  "abstract": "ARC Challenge appears more difficult than ARC Easy for modern LLMs primarily\ndue to an evaluation setup that prevents direct comparison of answer choices\nrather than inherent complexity. Although some researchers have quietly shifted\nto a more appropriate scheme over the last year, the implications of this\nchange have yet to be widely acknowledged. We highlight this overlooked shift,\nshow how similar evaluation practices falsely imply reasoning deficits in other\nbenchmarks, and demonstrate that fairer methods dramatically reduce performance\ngaps (e.g. on SIQA) and even yield superhuman results (OpenBookQA). In doing\nso, we reveal how evaluation shapes perceived difficulty and offer guidelines\nto ensure that multiple-choice evaluations accurately reflect actual model\ncapabilities.",
  "url": "https://arxiv.org/abs/2412.17758",
  "issue_number": 458,
  "issue_url": "https://github.com/dmarx/papers-feed/issues/458",
  "created_at": "2025-01-04T14:49:27.229963",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 6,
  "last_read": "2025-01-04T14:49:27.230757",
  "last_visited": "2024-12-29T08:28:16.461Z",
  "main_tex_file": null,
  "published_date": "2024-12-23T18:14:36Z",
  "arxiv_tags": [
    "cs.CL",
    "cs.AI"
  ]
}