<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ActionPiece: Contextually Tokenizing Action Sequences for Generative Recommendation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-02-19">19 Feb 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Yupeng</forename><surname>Hou</surname></persName>
							<email>&lt;yphou@ucsd.edu&gt;.</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>San Diego</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianmo</forename><surname>Ni</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Google DeepMind</orgName>
								<address>
									<addrLine>245 284 747 923 1067 76 362 679 941 1100 92 276 560 943 1131 245 291 635 799 747 923 245 1067 284 76 679 362 941 1100 92 276 560 943 1131 291 635 245 799 1123 747 245 1067 284 679 941 76 362 92 1131 291 635 245 799 1123 923 1100 560 943 276 Token Sequence 1 Segment 2 14844 21063 284 76 679</addrLine>
									<postCode>1123 20155 1100 7995 1734 6784</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhankui</forename><surname>He</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Google DeepMind</orgName>
								<address>
									<addrLine>245 284 747 923 1067 76 362 679 941 1100 92 276 560 943 1131 245 291 635 799 747 923 245 1067 284 76 679 362 941 1100 92 276 560 943 1131 291 635 245 799 1123 747 245 1067 284 679 941 76 362 92 1131 291 635 245 799 1123 923 1100 560 943 276 Token Sequence 1 Segment 2 14844 21063 284 76 679</addrLine>
									<postCode>1123 20155 1100 7995 1734 6784</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Noveen</forename><surname>Sachdeva</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Google DeepMind</orgName>
								<address>
									<addrLine>245 284 747 923 1067 76 362 679 941 1100 92 276 560 943 1131 245 291 635 799 747 923 245 1067 284 76 679 362 941 1100 92 276 560 943 1131 291 635 245 799 1123 747 245 1067 284 679 941 76 362 92 1131 291 635 245 799 1123 923 1100 560 943 276 Token Sequence 1 Segment 2 14844 21063 284 76 679</addrLine>
									<postCode>1123 20155 1100 7995 1734 6784</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wang-Cheng</forename><surname>Kang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Google DeepMind</orgName>
								<address>
									<addrLine>245 284 747 923 1067 76 362 679 941 1100 92 276 560 943 1131 245 291 635 799 747 923 245 1067 284 76 679 362 941 1100 92 276 560 943 1131 291 635 245 799 1123 747 245 1067 284 679 941 76 362 92 1131 291 635 245 799 1123 923 1100 560 943 276 Token Sequence 1 Segment 2 14844 21063 284 76 679</addrLine>
									<postCode>1123 20155 1100 7995 1734 6784</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Google DeepMind</orgName>
								<address>
									<addrLine>245 284 747 923 1067 76 362 679 941 1100 92 276 560 943 1131 245 291 635 799 747 923 245 1067 284 76 679 362 941 1100 92 276 560 943 1131 291 635 245 799 1123 747 245 1067 284 679 941 76 362 92 1131 291 635 245 799 1123 923 1100 560 943 276 Token Sequence 1 Segment 2 14844 21063 284 76 679</addrLine>
									<postCode>1123 20155 1100 7995 1734 6784</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>San Diego</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Derek</forename><forename type="middle">Zhiyuan</forename><surname>Cheng</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Google DeepMind</orgName>
								<address>
									<addrLine>245 284 747 923 1067 76 362 679 941 1100 92 276 560 943 1131 245 291 635 799 747 923 245 1067 284 76 679 362 941 1100 92 276 560 943 1131 291 635 245 799 1123 747 245 1067 284 679 941 76 362 92 1131 291 635 245 799 1123 923 1100 560 943 276 Token Sequence 1 Segment 2 14844 21063 284 76 679</addrLine>
									<postCode>1123 20155 1100 7995 1734 6784</postCode>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ActionPiece: Contextually Tokenizing Action Sequences for Generative Recommendation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-02-19">19 Feb 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">19114AC7E83733D43F962F6ADBE7CDCF</idno>
					<idno type="arXiv">arXiv:2502.13581v1[cs.IR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generative recommendation (GR) is an emerging paradigm where user actions are tokenized into discrete token patterns and autoregressively generated as predictions. However, existing GR models tokenize each action independently, assigning the same fixed tokens to identical actions across all sequences without considering contextual relationships. This lack of context-awareness can lead to suboptimal performance, as the same action may hold different meanings depending on its surrounding context. To address this issue, we propose ActionPiece to explicitly incorporate context when tokenizing action sequences. In Ac-tionPiece, each action is represented as a set of item features, which serve as the initial tokens. Given the action sequence corpora, we construct the vocabulary by merging feature patterns as new tokens, based on their co-occurrence frequency both within individual sets and across adjacent sets. Considering the unordered nature of feature sets, we further introduce set permutation regularization, which produces multiple segmentations of action sequences with the same semantics. Experiments on public datasets demonstrate that Ac-tionPiece consistently outperforms existing action tokenization methods, improving NDCG@10 by 6.00% to 12.82%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Generative recommendation (GR) <ref type="bibr" target="#b8">(Geng et al., 2022;</ref><ref type="bibr" target="#b35">Rajput et al., 2023;</ref><ref type="bibr">Zheng et al., 2024a;</ref><ref type="bibr" target="#b54">Zhai et al., 2024)</ref> is an emerging paradigm for the sequential recommendation task <ref type="bibr" target="#b10">(Hidasi et al., 2016;</ref><ref type="bibr" target="#b17">Kang &amp; McAuley, 2018)</ref>. By tokenizing the user actions (typically represented by the interacted items) into discrete tokens, GR models learn to autoregressively generate tokens, which are then parsed into recommended items. These tokens share a compact vocabulary that does not scale with the item pool size, improving model scalability, memory efficiency, and recommendation performance <ref type="bibr" target="#b35">(Rajput et al., 2023;</ref><ref type="bibr" target="#b54">Zhai et al., 2024)</ref>. The input action sequence is vital in understanding user intentions <ref type="bibr" target="#b21">(Li et al., 2017;</ref><ref type="bibr" target="#b17">Kang &amp; McAuley, 2018)</ref>, which organizes a user's historical interactions in chronological order. The same action (e.g., purchasing the same item) may have different meanings in different action sequences. Evidence of taking a certain action can be found in the context, such as whether other items in the sequence share the same brand, color tone, or price range <ref type="bibr" target="#b56">(Zhang et al., 2019;</ref><ref type="bibr" target="#b63">Zhou et al., 2020;</ref><ref type="bibr" target="#b11">Hou et al., 2022;</ref><ref type="bibr">2023;</ref><ref type="bibr" target="#b52">Yuan et al., 2023)</ref>.</p><p>Despite the importance of contextual relations among actions, existing methods tokenize each action independently of its context (summarized in Table <ref type="table" target="#tab_0">1</ref>). The typical pipeline for tokenizing action sequences involves two steps: (1) Tokenizing each action/item individually into a pattern of tokens; (2) Replacing each action in the input sequence with its corresponding token pattern. In this way, the tokens do not explicitly contain the context. Instead, they solely rely on the autoregressive model's parameters being well-trained to generalize effectively in understanding the context, which challenges the capabilities of GR models. As a comparison, tokenization in language modeling also originates from context-independent methods, such as word-level tokeniza-tion <ref type="bibr" target="#b42">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b1">Bahdanau et al., 2015)</ref>. A decade of progress has led to most tokenization methods for modern large language models (LLMs) <ref type="bibr">(OpenAI, 2022;</ref><ref type="bibr" target="#b0">Anil et al., 2023;</ref><ref type="bibr" target="#b44">Touvron et al., 2023;</ref><ref type="bibr" target="#b60">Zhao et al., 2023)</ref> adopting context-aware approaches, including BPE <ref type="bibr" target="#b37">(Sennrich et al., 2016)</ref> and Unigram tokenization <ref type="bibr" target="#b18">(Kudo, 2018)</ref>, which tokenize the same characters along with their adjacent context into different tokens.</p><p>In this work, we aim to make the first step towards contextaware tokenization for modeling action sequences. In analogy to how characters or bytes serve as the basic units in language modeling, we consider the associated features of an item as initial tokens. The idea is to iteratively find the most commonly co-occurring pairs of tokens among the training action sequences, then merge them into new tokens to represent segments of context. However, it's non-trivial to achieve this. Unlike text, where characters naturally form a sequence, the features associated with an action form an unordered set <ref type="bibr" target="#b56">(Zhang et al., 2019;</ref><ref type="bibr" target="#b63">Zhou et al., 2020)</ref>. Thus, the proposed tokenization algorithm should be applied on sequences of token sets. We need to carefully consider which pairs of tokens should be counted, whether within a single set or between two adjacent sets, and how much weight should be given to these different types of relationships.</p><p>To this end, we propose ActionPiece, which enables the same actions to be tokenized into different tokens based on their surrounding context. (1) Vocabulary construction:</p><p>We first initialize the vocabulary to include every unique feature as initial tokens. The vocabulary is then constructed by iteratively learning merge rules. Each merge rule specifies that a pair of tokens can be merged into a new token. In each iteration, we enumerate the training corpus to count the co-occurrence of existing tokens. Considering the structural differences between token pairs, e.g., whether they occur within a single set or between two adjacent sets, we assign different weights to different pairs during the counting process. (2) Segmentation: The next step is to segment action sequences into token sequences for GR model training and inference. To fully exploit the unordered nature of the feature set for each action, we introduce set permutation regularization. By randomly permuting the features within each set, we can produce multiple token sequences of a single action sequence that preserve the same semantics. These variations act as natural augmentations for training data and enable inherent ensembling during model inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Generative recommendation. Conventional sequential recommendation models often relies on large embedding tables to store representations for all items, leading to significant engineering and optimization challenges <ref type="bibr" target="#b10">(Hidasi et al., 2016;</ref><ref type="bibr" target="#b17">Kang &amp; McAuley, 2018)</ref>. Generative recommenda- Product Quantization VQ-Rec <ref type="bibr" target="#b12">(Hou et al., 2023</ref>) Hierarchical Clustering P5-CID <ref type="bibr" target="#b14">(Hua et al., 2023)</ref> Residual Quantization TIGER <ref type="bibr" target="#b35">(Rajput et al., 2023)</ref> Text Tokenization LMIndexer <ref type="bibr" target="#b15">(Jin et al., 2024</ref>) Raw Features HSTU <ref type="bibr" target="#b54">(Zhai et al., 2024</ref>) SentencePiece SPM-SID <ref type="bibr" target="#b40">(Singh et al., 2024)</ref> ActionPiece Ours tion <ref type="bibr" target="#b4">(Deldjoo et al., 2024;</ref><ref type="bibr" target="#b35">Rajput et al., 2023;</ref><ref type="bibr">Zheng et al., 2024a)</ref> addresses these issues by tokenizing each item as tokens from a shared table. By autoregressively generating the next tokens as recommendations, this generative paradigm offers benefits such as memory efficiency <ref type="bibr" target="#b35">(Rajput et al., 2023;</ref><ref type="bibr" target="#b50">Yang et al., 2024;</ref><ref type="bibr" target="#b5">Ding et al., 2024)</ref>, scalability <ref type="bibr" target="#b54">(Zhai et al., 2024;</ref><ref type="bibr">Liu et al., 2024c)</ref>, and easier alignment with LLMs <ref type="bibr">(Zheng et al., 2024a;</ref><ref type="bibr" target="#b15">Jin et al., 2024;</ref><ref type="bibr" target="#b43">Tan et al., 2024;</ref><ref type="bibr" target="#b20">Li et al., 2025)</ref>. Existing research has developed different action tokenization techniques, such as hierarchical clustering <ref type="bibr" target="#b14">(Hua et al., 2023;</ref><ref type="bibr" target="#b39">Si et al., 2024)</ref>, quantization <ref type="bibr" target="#b35">(Rajput et al., 2023;</ref><ref type="bibr">Wang et al., 2024a;</ref><ref type="bibr" target="#b64">Zhu et al., 2024)</ref>, or jointly training with recommendation models <ref type="bibr">(Liu et al., 2024a)</ref>.</p><p>Other works incorporate additional modalities like collaborative filtering <ref type="bibr" target="#b32">(Petrov &amp; Macdonald, 2023;</ref><ref type="bibr">Wang et al., 2024c;</ref><ref type="bibr">b;</ref><ref type="bibr">Liu et al., 2024c</ref>), text <ref type="bibr">(Zheng et al., 2024a;</ref><ref type="bibr" target="#b15">Jin et al., 2024;</ref><ref type="bibr" target="#b13">Hou et al., 2024;</ref><ref type="bibr" target="#b55">Zhang et al., 2025)</ref>, and vision signals <ref type="bibr">(Liu et al., 2024b)</ref>. However, current methods tokenize each action independently, ignoring the surrounding context. In this work, we propose the first context-aware action tokenization method, where the same actions are tokenized differently in different action sequences.</p><p>Tokenization for language modeling. Tokenization is the process of transforming raw text into discrete token sequences <ref type="bibr" target="#b19">(Kudo &amp; Richardson, 2018)</ref>. Early word-level methods are context-independent and struggle to tokenize out-of-vocabulary words <ref type="bibr" target="#b42">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b1">Bahdanau et al., 2015)</ref>. Consequently, subword-level tokenization has gradually become the more mainstream choice. The vocabularies of these subword-level tokenizers are constructed iteratively, either bottom-up (starting with a small vocabulary and merging commonly occurring token pairs as new tokens) <ref type="bibr" target="#b49">(Wu et al., 2016;</ref><ref type="bibr" target="#b37">Sennrich et al., 2016)</ref>, or top-down (starting with a large vocabulary and pruning tokens to minimize likelihood decrease) <ref type="bibr" target="#b18">(Kudo, 2018;</ref><ref type="bibr" target="#b51">Yehezkel &amp; Pinter, 2023)</ref>. Once the vocabulary is built, the text can be segmented either using the same method employed during vocabulary construction or based on additional objectives <ref type="bibr" target="#b9">(He et al., 2020;</ref><ref type="bibr" target="#b33">Provilkov et al., 2020;</ref><ref type="bibr" target="#b11">Hofmann et al., 2022;</ref><ref type="bibr" target="#b36">Schmidt et al., 2024)</ref>. As an analogy, existing action tokeniz-ers are context-independent and function like "word-level" language tokenizers. In this work, we take the first step toward context-aware subaction-level action tokenizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we present ActionPiece, a context-aware method for tokenizing action sequences in generative recommendation. First, we formulate the task in Section 3.1. Then, we introduce the proposed tokenizer, covering vocabulary construction and sequence segmentation, in Section 3.2. Finally, we describe the model training and inference process using ActionPiece-tokenized sequences in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Formulation</head><p>Given a user's historical actions S = {i 1 , i 2 , . . . , i t }, organized sequentially by their timestamps, the task is to predict the next item i t+1 the user will interact with.</p><p>Action as an unordered feature set. In the development of modern recommender systems, each item i j is usually associated with a set of features A j <ref type="bibr" target="#b56">(Zhang et al., 2019;</ref><ref type="bibr" target="#b63">Zhou et al., 2020;</ref><ref type="bibr" target="#b2">Cheng et al., 2016)</ref>. Assuming there are m features per item, the k-th feature of item i j is denoted as f j,k ∈ F k , where F k is the collection of all possible choices for the k-th feature. Compared to representing actions using ordered semantic IDs (e.g., those produced by RQ-VAE <ref type="bibr" target="#b35">(Rajput et al., 2023;</ref><ref type="bibr" target="#b40">Singh et al., 2024)</ref>), the unordered set setting offers two key advantages: (1) It does not require a specific order among features, which aligns better with how items or actions are represented in most recommender systems;</p><p>(2) It enables the inclusion of more general discrete and numeric features, such as category, brand, and price <ref type="bibr" target="#b31">(Pazzani &amp; Billsus, 2007;</ref><ref type="bibr" target="#b16">Juan et al., 2016)</ref>.</p><p>Action sequence as a sequence of sets. Representing each item as an unordered set, the input action sequence can be written as S ′ = {A 1 , A 2 , . . . , A t }, which is a chronologically ordered sequence of sets. There is no order within each set, but there are orders between the features from different sets. The tokenizer design should account for the ordered and unordered relationships among features.</p><p>Generative recommendation task. In this work, we aim to design a tokenizer that maps an input action sequence S ′ to a token sequence C = {c 1 , c 2 , . . . , c l }, where l denotes the number of tokens in the sequence. Note that l is typically greater than the number of actions t. Next, we train a GR model to autoregressively generate tokens {c l+1 , . . . , c q }, which can be parsed as next-item predictions ît+1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Contextual Action Sequence Tokenizer</head><p>The proposed tokenizer is designed to transform action sequences (represented as sequences of feature sets) into token Algorithm 1 ActionPiece Vocabulary Construction input Sequence corpus S ′ , initial tokens V0, target size Q output Merge rules R, constructed vocabulary V 1: Initialize vocabulary V ← V0 # each initial token corresponds to one unique item feature 2: R ← ∅ 3: while |V| &lt; Q do 4: # Count: accumulate weighted token co-occurrences 5: count(•, •) ← Count(S ′ , V) # Algorithm 2 6: # Update: Merge a frequent token pair into a new token 7: Select (cu, cv) ← arg max (c i ,c j ) count(ci, cj) 8: S ′ ← Update(S ′ , {(cu, cv) → cnew}) # Algorithm 3 9: R ← R ∪ {(cu, cv) → cnew} # new merge rule 10:</p><p>V ← V ∪ {cnew} # add new token to the vocabulary 11: end while return R, V sequences. In the ActionPiece-tokenized sequences, each token corresponds to a set containing varying numbers of features. For example, a token can represent: (1) a subset of features from one item; (2) a set with a single feature;</p><p>(3) all features of one item; or (4) features from multiple items. We also label these four types of tokens in Figure <ref type="figure" target="#fig_0">1</ref>. Below, we first describe how to construct the ActionPiece tokenizer's vocabulary given a corpus of action sequences (Section 3.2.1). Then, we introduce how to segment action sequences into a new sequence of sets, where each set corresponds to a token from the constructed vocabulary (Section 3.2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">VOCABULARY CONSTRUCTION ON ACTION SEQUENCE CORPUS</head><p>Given a corpus of action sequences S ′ , the goal of vocabulary construction is to create a vocabulary V of Q tokens. Each token represents a combination of features that frequently occur in the corpus. Similar to BPE <ref type="bibr" target="#b37">(Sennrich et al., 2016)</ref>, we construct the vocabulary using a bottom-up approach. The process starts with an initial vocabulary of tokens V 0 . The construction proceeds iteratively, adding one new token to the vocabulary at each iteration until the predefined target size is reached. Each iteration consists of two consecutive steps: count, where the most frequently occurring token pair is identified, and update, where the corpus is modified by merging the selected pair into a new token. An algorithmic workflow is illustrated in Algorithm 1.</p><p>Vocabulary initialization. In BPE, each token represents a sequence of bytes. Thus, the most fundamental unitsthe initial tokens-are single bytes, which form the initial vocabulary of BPE. Similarly, each token in ActionPiece represents a set of features. Therefore, we initialize Action-Piece with a vocabulary in which each token represents a set containing one unique item feature. Formally, we denote the initial vocabulary as</p><formula xml:id="formula_0">V 0 = {c = {f }|f ∈ F 1 ∪ . . . ∪ F m }.</formula><p>After initializing the vocabulary, each action sequence (of feature sets) can be represented as a sequence of token sets.</p><formula xml:id="formula_1">••• ••• flattening ••• •••</formula><p>Count: context-aware token co-occurrence counting. In each iteration of vocabulary construction, the first step is to count the co-occurrence of token pairs in the corpus. These pairs capture important feature combinations, which are encoded by creating new tokens. There are two types of token co-occurrence within a sequence of sets: (1) two tokens exist within the same set, or (2) two tokens exist in adjacent sets in the sequence. Notably, the second type allows ActionPiece to explicitly include context information.</p><p>Weighted co-occurrence counting. In one-dimensional token sequences (e.g., text), all token pairs are typically treated equally. However, in sequences of token sets, token pairs vary based on their types and the sizes of their respective sets. To account for these differences, we propose assigning different weights to token pairs. To determine the weight for each token pair, we relate sequences of token sets to token sequences by randomly permuting the tokens within each set and flattening them into a single token sequence. Let P (c, c ′ ) represent the expected probability that tokens c and c ′ are adjacent in the flattened sequence. For two tokens from the same set, we have:</p><formula xml:id="formula_2">P (c 1 , c 2 ) = P (c 2 , c 1 ) = |A i | -1 |Ai| 2 = 2 |A i | , c 1 , c 2 ∈ A i ,</formula><p>(1) and for two tokens from adjacent sets, we have:</p><formula xml:id="formula_3">P (c 1 , c 3 ) = 1 |A i | × |A i+1 | , c 1 ∈ A i , c 3 ∈ A i+1 . (2)</formula><p>By considering the probabilities of all adjacent token pairs in the flattened sequence as 1, the weights for token pairs in the original sequence of token sets correspond to the probabilities given in Equations ( <ref type="formula">1</ref>) and ( <ref type="formula">2</ref>). An illustration is shown in Figure <ref type="figure" target="#fig_1">2</ref>.</p><p>Accumulating co-occurrence weights. The weights described above are calculated based solely on the cooccurrence type and the set size. They do not take into account the specific tokens being analyzed. Tokens c i and c j might appear in the same set in one sequence but in two adjacent sets in another sequence. By iterating through the corpus, we sum up the weights for each token pair whenever they appear together multiple times.</p><p>Update: corpus updating with action-intermediate nodes. The next step in each iteration is to merge the token pair with the highest accumulated co-occurrence weight.</p><p>Since token merging may change the set size, we use a double-ended linked list <ref type="bibr" target="#b65">(Zouhar et al., 2023)</ref> to maintain the action sequences, where each node represents a set of tokens. Merging tokens within the same set is straightforward, i.e., replacing the two tokens with a new one. However, merging tokens from two adjacent sets is more complex, e.g., determining which set should include the new token.</p><p>Intermediate Node. We introduce the concept of "intermediate node" to handle tokens that combine features from multiple sets. Initially, all nodes in the maintained linked list contain features specific to their corresponding actions. These nodes are referred to as "action nodes."</p><p>(1) When tokens from two adjacent action nodes are being merged, we insert a new intermediate node between the two action nodes. The new token is stored in the intermediate node, and the merged tokens are removed from their respective action nodes;</p><p>(2) When merging tokens from an action node and an intermediate node, the new token replaces the original token in the intermediate node. The reason is that this new token also combines features from multiple actions. After the merge, the token from the action node is removed.</p><p>Following the above update rules ensures that there is at most one intermediate node between any two action nodes, and each intermediate node contains no more than one token. When calculating co-occurrence weights involving an intermediate node, it can simply be treated as a set of size 1.</p><p>Efficient implementation. Naively counting and updating the corpus requires a total time complexity of O(QN Lm 2 ), where Q is the target vocabulary size, N is the number of action sequences in the training corpus, and L is the average length of these sequences. However, it is unnecessary to count co-occurrences from scratch in each iteration. This is because only a small portion of the maintained linked lists is modified compared to the previous iteration. Data structures. To address this, we propose maintaining a hash table to store the accumulated co-occurrences for each token pair. Additionally, we use inverted indices to map token pairs to all the linked lists that contain them. A global heap is maintained to return the token pair with the highest accumulated co-occurrence. The key challenge lies in updating these data structures. We carefully compute the changes in accumulated co-occurrences and update both the hash table and the inverted indices. For the heap, we employ a lazy-update strategy. We insert the latest weights with a tag. When fetching a value from the heap, we check the tag to verify if the value is up-to-date. If it is not, we discard the value and fetch the next one. Time complexity. Let H = O(N Lm) represent the maximal heap size. Using the proposed algorithm, we successfully reduce the original time complexity to O(log Q log H • N Lm 2 ), achieving efficient vocabulary construction. In practice, the later iterations take significantly less time than the initial ones. This is expected and because tokens with higher accumulated co-occurrence weights typically appear frequently in the early stages. However, the overall construction time benefits from the reduced amortized complexity. Further details about the vocabulary construction algorithm are provided in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">SEGMENTATION BY SET PERMUTATION REGULARIZATION</head><p>Segmentation is to convert original action sequences into a sequence of feature sets. Each set in the segmented sequence corresponds to a token in the vocabulary.</p><p>Naive segmentation. One segmentation strategy in Action-Piece involves applying the same technique used to construct the vocabulary. Specifically, this technique iteratively identifies token pairs with high priorities (represented by the IDs of tokens, where tokens added earlier may have higher priority). However, we observed that this strategy can lead to a bias, where only a subset of tokens in the vocabulary is frequently used (as shown empirically in Section 4.4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Set permutation regularization (SPR).</head><p>To address this issue and account for the unordered nature of sets, we propose set permutation regularization, which generates multiple segmentations for each action sequence. The key idea is to avoid enumerating all possible pairs between tokens in a set or adjacent sets. Instead, we generate a random permutation of each set and treat it as a one-dimensional sequence. By concatenating all the permutations, we create a long token sequence. This sequence can then be segmented using traditional BPE segmentation methods <ref type="bibr" target="#b37">(Sennrich et al., 2016)</ref>. In this approach, different permutations can produce distinct segmented token sequences with the same semantics. These sequences serve as natural augmentations for model training (Section 3.3.1) and enable inherent ensembling during model inference (Section 3.3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Generative Recommendation Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">TRAINING ON AUGMENTED TOKEN SEQUENCES</head><p>For an action sequence and its ground-truth next action in the training corpus, we tokenize them into token sequences C in and C out , respectively. Taking C in as input, we train a Transformer encoder-decoder module <ref type="bibr" target="#b34">(Raffel et al., 2020)</ref> to autoregressively generate C out (e.g., next-token prediction objective <ref type="bibr" target="#b35">(Rajput et al., 2023)</ref>). During training, we tokenize the action sequence using the set permutation regularization described in Section 3.2.2 in each epoch. This approach naturally augments the training sequences, which empirically improves model performance, as shown in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">INFERENCE-TIME ENSEMBLING</head><p>During model inference, we tokenize each action sequence q times using set permutation regularization. By passing these q tokenized sequences through the model, we obtain q Table <ref type="table">2</ref>. Performance comparison of different methods on the Amazon Reviews dataset <ref type="bibr">(McAuley et al., 2015)</ref>. The best and second-best performance is denoted in bold and underlined fonts. "R@K" and "N@K" are short for "Recall@K" and "NDCG@K", respectively. "Improv." denotes the percentage improvement of our method compared to the strongest baseline method.</p><p>Datasets Metric ID-based Feature + ID Generative Improv. BERT4Rec SASRec FDSA S 3 -Rec VQ-Rec P5-CID TIGER LMIndexer HSTU SPM-SID ActionPiece Sports R@5 0.0115 0.0233 0.0182 0.0251 0.0181 0.0287 0.0264 0.0222 0.0258 0.0280 0.0316 ± 0.0005 +12.86% N@5 0.0075 0.0154 0.0122 0.0161 0.0132 0.0179 0.0181 0.0142 0.0165 0.0180 0.0205 ± 0.0002 +11.71% R@10 0.0191 0.0350 0.0288 0.0385 0.0251 0.0426 0.0400 -0.0414 0.0446 0.0500 ± 0.0007 +12.11% N@10 0.0099 0.0192 0.0156 0.0204 0.0154 0.0224 0.0225 -0.0215 0.0234 0.0264 ± 0.0003 +12.82% Beauty R@5 0.0203 0.0387 0.0267 0.0387 0.0434 0.0468 0.0454 0.0415 0.0469 0.0475 0.0511 ± 0.0014 +7.58% N@5 0.0124 0.0249 0.0163 0.0244 0.0311 0.0315 0.0321 0.0262 0.0314 0.0321 0.0340 ± 0.0011 +5.92% R@10 0.0347 0.0605 0.0407 0.0647 0.0741 0.0701 0.0648 -0.0704 0.0714 0.0775 ± 0.0017 +4.59% N@10 0.0170 0.0318 0.0208 0.0327 0.0372 0.0400 0.0384 -0.0389 0.0399 0.0424 ± 0.0011 +6.00% CDs R@5 0.0326 0.0351 0.0226 0.0213 0.0314 0.0505 0.0492 -0.0417 0.0509 0.0544 ± 0.0005 +6.88% N@5 0.0201 0.0177 0.0137 0.0130 0.0209 0.0326 0.0329 -0.0275 0.0337 0.0359 ± 0.0004 +6.53% R@10 0.0547 0.0619 0.0378 0.0375 0.0485 0.0785 0.0748 -0.0638 0.0778 0.0830 ± 0.0008 +5.73% N@10 0.0271 0.0263 0.0186 0.0182 0.0264 0.0416 0.0411 -0.0346 0.0424 0.0451 ± 0.0005 +6.37%</p><p>output ranking lists (e.g., using beam search for inference when TIGER <ref type="bibr" target="#b35">(Rajput et al., 2023)</ref> is the GR backbone). We then combine these ranking lists by averaging the scores of each predicted item. This approach applies data-level ensembling, which has been shown to enhance recommendation performance, as discussed in Section 4.4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Datasets. We use three categories from the Amazon Reviews dataset <ref type="bibr">(McAuley et al., 2015)</ref> for our experiments: "Sports and Outdoors" (Sports), "Beauty" (Beauty), and "CDs and Vinyl" (CDs). Each user's historical reviews are considered "actions" and are sorted chronologically as action sequences, with earlier reviews appearing first. To evaluate the models, we adopt the widely used leave-lastout protocol <ref type="bibr" target="#b17">(Kang &amp; McAuley, 2018;</ref><ref type="bibr" target="#b59">Zhao et al., 2022;</ref><ref type="bibr" target="#b35">Rajput et al., 2023)</ref>, where the last item and second-to-last item in each action sequence are used for testing and validation, respectively. More details about the datasets can be found in Appendix E.</p><p>Compared methods. We compare the performance of Ac-tionPiece with the following methods: (1) ID-based sequential recommendation methods, including BERT4Rec <ref type="bibr" target="#b41">(Sun et al., 2019)</ref>, and SASRec <ref type="bibr" target="#b17">(Kang &amp; McAuley, 2018)</ref>;</p><p>(2) feature-enhanced sequential recommendation methods, such as FDSA <ref type="bibr" target="#b56">(Zhang et al., 2019)</ref>, S 3 -Rec <ref type="bibr" target="#b63">(Zhou et al., 2020)</ref>, and VQ-Rec <ref type="bibr" target="#b12">(Hou et al., 2023)</ref>; and (3) generative recommendation methods, including P5-CID <ref type="bibr" target="#b14">(Hua et al., 2023)</ref>, TIGER <ref type="bibr" target="#b35">(Rajput et al., 2023)</ref>, LMIndexer <ref type="bibr" target="#b15">(Jin et al., 2024)</ref>, HSTU <ref type="bibr" target="#b54">(Zhai et al., 2024)</ref>, and SPM-SID <ref type="bibr" target="#b40">(Singh et al., 2024)</ref>, each representing a different action tokenization method (Table <ref type="table" target="#tab_0">1</ref>). A detailed description of these baselines is provided in Appendix F.</p><p>Evaluation settings. Following <ref type="bibr" target="#b35">Rajput et al. (2023)</ref>, we use Recall@K and NDCG@K as metrics to evaluate the methods, where K ∈ {5, 10}. Model checkpoints with the best performance on the validation set are used for evaluation on the test set. We run the experiments with five random seeds and report the average metrics.</p><p>Implementation details. Please refer to Appendix G for detailed implementation and hyperparameter settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Overall Performance</head><p>We compare ActionPiece with sequential recommendation and generative recommendation baselines, which use various action tokenization methods, across three public datasets. The results are shown in Table <ref type="table">2</ref>.</p><p>For the compared methods, we observe that those using item features generally outperform item ID-only methods. This indicates that incorporating features enhances recommendation performance. Among the methods leveraging item features ("Feature + ID" and "Generative"), generative recommendation models achieve better performance. These results further confirm that injecting semantics into item indexing and optimizing at a sub-item level enables generative models to better use semantic information and improve recommendation performance. Among all the baselines, SPM-SID achieves the best results. By incorporating the SentencePiece model <ref type="bibr" target="#b19">(Kudo &amp; Richardson, 2018)</ref>, SPM-SID replaces popular semantic ID patterns within each item with new tokens, benefiting from a larger vocabulary.</p><p>Our proposed ActionPiece consistently outperforms all baselines across three datasets, achieving a significant improvement in NDCG@10. It surpasses the best-performing baseline method by 6.00% to 12.82%. tokens depending on its surrounding context. This allows ActionPiece to capture important sequence-level feature patterns that enhance recommendation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>We conduct ablation analyses in Table <ref type="table" target="#tab_2">3</ref> to study how each proposed technique contributes to ActionPiece.</p><p>(1) We increase the vocabulary size of TIGER, to determine whether the performance gain of ActionPiece is solely due to scaling up the number of tokens in the vocabulary. By increasing the number of semantic ID digits per item (4 → 6) and the number of candidate semantic IDs per digit (2 8 → 2 13 or 2 14 ), we create two variants with vocabularies larger than ActionPiece. However, these TIGER variants perform worse than ActionPiece, and even the original TIGER with only 1024 tokens. The experimental results suggest that scaling up the vocabulary size for generative recommendation models is challenging, consistent with the observations from <ref type="bibr" target="#b57">Zhang et al. (2024)</ref>.</p><p>Token Utilization Rate (%) 95.33% 87.01% 56.89% SPR (#epochs=5) SPR (#epochs=1) Naive Figure 5. Analysis of token utilization rate (%) during model training w.r.t. segmentation strategy.</p><p>(2) To evaluate the effectiveness of the proposed vocabulary construction techniques, we introduce the following variants: (2.1) w/o tokenization, which skips vocabulary construction, using item features directly as tokens; (2.2) w/o context-aware, which only considers co-occurrences and merges tokens within each action during vocabulary construction and segmentation; and (2.3) w/o weighted counting, which treats all token pairs equally rather than using the weights defined in Equations ( <ref type="formula">1</ref>) and (2). The results indicate that removing any of these techniques reduces performance, demonstrating the importance of these methods for building a context-aware tokenizer.</p><p>(3) To evaluate the effectiveness of SPR, we revert to naive segmentation, as described in Section 3.2.2, during model training and inference, respectively. The results show that replacing SPR with naive segmentation in either training or inference degrades performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Further Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1.">PERFORMANCE AND EFFICIENCY W.R.T. VOCABULARY SIZE</head><p>Vocabulary size is a key hyperparameter for language tokenizers <ref type="bibr" target="#b27">(Meta AI, 2024;</ref><ref type="bibr" target="#b3">Dagan et al., 2024)</ref>. In this study, we investigate how adjusting vocabulary size affects the generative recommendation models. We use the normalized sequence length (NSL) <ref type="bibr" target="#b3">(Dagan et al., 2024)</ref> to measure the length of tokenized sequences, where a smaller NSL indicates fewer tokens per tokenized sequence. We experiment with vocabulary sizes in {N/A, 5k, 10k, 20k, 30k, 40k}, where "N/A" represents the direct use of item features as tokens. As shown in Figure <ref type="figure">4</ref>, increasing the vocabulary size improves recommendation performance and reduces</p><p>N/A 1 3 5 7 0.025 0.026 NDCG@10 Sports N/A 1 3 5 7 Model Inference Ensemble Times (q) 0.040 0.042 Beauty N/A 1 3 5 7 0.043 0.044 0.045 CDs Figure 6. Analysis of performance (NDCG@10, ↑) w.r.t. the number of ensembled segments q during model inference. the tokenized sequence length. Conversely, reducing the vocabulary size lowers the number of model parameters, improving memory efficiency. This analysis demonstrates that adjusting vocabulary size enables a trade-off between model performance, sequence length, and memory efficiency. 4.4.2. TOKEN UTILIZATION RATE W.R.T. SEGMENTATION STRATEGY As described in Section 3.3.1, applying SPR augments the training corpus by producing multiple token sequences that share the same semantics. In Table 3, we observe that incorporating SPR significantly improves recommendation performance. One possible reason is that SPR increases token utilization rates. To validate this assumption, we segment the action sequences in each training epoch using two strategies: naive segmentation and SPR. As shown in Figure 5, naive segmentation uses only 56.89% of tokens for model training, limiting the model's ability to generalize to unseen action sequences. In contrast, SPR achieves a token utilization rate of 87.01% after the first training epoch, with further increases as training progresses. These results demonstrate that the proposed SPR segmentation strategy improves the utilization of ActionPiece tokens, enabling better generalization and enhanced performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3.">PERFORMANCE W.R.T. INFERENCE-TIME ENSEMBLES</head><p>As described in Section 3.3.2, ActionPiece supports inference-time ensembling by using SPR segmentation. We vary the number of ensembled segments, q, in {N/A, 1, 3, 5, 7}, where "N/A" indicates using naive segmentation during model inference. As shown in Figure <ref type="figure">6</ref>, ensembling more tokenized sequences improves ActionPiece's recommendation performance. However, the performance gains slow down as q increases to 5 and 7. Since a higher q also increases the computational cost of inference, this creates a trade-off between performance and computational budget in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Case Study</head><p>To understand how GR models benefit from the unordered feature setting and context-aware action sequence tokenization, we present an illustrative example in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>Each item in the action sequence is represented as a feature set, with each item consisting of five features. The features within an item do not require a specific order. The first step of tokenization leverages the unordered nature of the feature set and applies set permutation regularization (Section 3.2.2). This process arranges each feature set into a specific permutation and iteratively groups features based on the constructed vocabulary (Section 3.2.1). This results in different segments that convey the same semantics. Each segment is represented as a sequence of sets, where each set corresponds to a token in the vocabulary.</p><p>By examining the segments and their corresponding token sequences, we identify four types of tokens, as annotated in Figure <ref type="figure" target="#fig_0">1:</ref> (1) a subset of features from a single item (token 14844 corresponds to features 747 and 923 of the T-shirt);</p><p>(2) a set containing a single feature (feature 76 of the socks);</p><p>(3) all features of a single item (token 7995 corresponds to all features of the shorts); and (4) features from multiple items (e.g., token 8316 includes feature 923 from the Tshirt and feature 679 from the socks, while token 19895 includes feature 1100 from the socks as well as features 560 and 943 from the shorts). Notably, the fourth type of token demonstrates that the features of one action can be segmented and grouped with features from adjacent actions. This results in different tokens for the same action depending on the surrounding context, showcasing the context-aware tokenization process of ActionPiece.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we introduce ActionPiece, the first contextaware action sequence tokenizer for generative recommendation. By considering the surrounding context, the same action can be tokenized into different tokens in different sequences. We formulate generative recommendation as a task on sequences of feature sets and merge important feature patterns into tokens. During vocabulary construction, we propose assigning weights to token pairs based on their structures, such as those within a single set or across adjacent sets. To enable efficient vocabulary construction, we use double-ended linked lists to maintain the corpus and introduce intermediate nodes to store tokens that combine features across adjacent sets. Additionally, we propose set permutation regularization, which segments a single action sequence into multiple token sequences with the same semantics. These segments serve as natural augmentations for training and as ensemble instances for inference.</p><p>In the future, we plan to align user actions with other modalities by constructing instructions that combine ActionPiece tokens and other types of tokens. We also aim to extend the proposed tokenizer to other tasks that can be framed as set sequence modeling problems, including audio modeling, sequential decision-making, and time series forecasting.</p><p>Table 4. Notations and explanations.</p><p>Notation Explaination i, i 1 , i j item, item identifier, item ID t the number of actions in the input action sequence; the timestamp when the model makes a prediction i t+1 the ground-truth next item ît+1 the predicted next item S = {i 1 , i 2 , . . . , i t } the action sequence where each action is represented with the interacted item ID A, A 1 , A j a set of item features or tokens m = |A j | the number of features associated with each item f j,k the k-th feature of item i j F k the collection of all possible choices for the k-th feature The number of segmentations produced using set permutation regularization during inference</p><formula xml:id="formula_4">S ′ = {A 1 , A 2 , . . . , A t }</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Notations</head><p>We summarize the notations used in this paper in Table <ref type="table">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Algorithmic Details</head><p>In this section, we provide detailed algorithms for vocabulary construction and segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Vocabulary Construction Algorithm</head><p>The overall procedure for vocabulary construction is illustrated in Algorithm 1. As described in Section 3.2.1, this process involves iterative Count (Algorithm 2) and Update (Algorithm 3) operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Segmentation with Set Permutation Regularization Algorithm</head><p>The detailed algorithm for segmenting action sequences into token sequences using set permutation regularization (SPR) is shown in Algorithm 4. In practice, we often run Algorithm 4 multiple times to augment the training corpus or ensemble recommendation outputs, as described in Sections 3.3.1 and 3.3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Efficient Vocabulary Construction Implementation</head><p>To efficiently construct the ActionPiece vocabulary, we propose using data structures such as heaps with a lazy update trick, linked lists, and inverted indices to speed up each iteration of the construction process. The key idea is to avoid recalculating token co-occurrences in every iteration and instead update the data structures. The pseudocode is shown in Figure <ref type="figure">7</ref>.</p><p>Algorithm 2 ActionPiece Vocabulary Construction -Count (Figure <ref type="figure" target="#fig_1">2</ref>) input Action sequence corpus S ′ , current vocabulary V output Accumulated weighted token co-occurrences count(•, •)</p><formula xml:id="formula_5">1: for i ← 0 to |V|, j ← 0 to |V| do 2:</formula><p>count(c i , c j ) ← 0 3: end for 4: for all sequence S ′ ∈ S ′ do 5:</p><p>t ← length(S ′ ) # number of action nodes in sequence 6:</p><formula xml:id="formula_6">for k ← 0 to t -1 do 7: A k ← S ′ [k] # current action node 8:</formula><p># Process all unordered token pairs within A k 9:</p><formula xml:id="formula_7">for all c i , c j ∈ A k , i ̸ = j do 10: count(c i , c j ) ← count(c i , c j ) + 2/|A k | # weight of tokens within a single set (Equation (1)) 11: count(c j , c i ) ← count(c j , c i ) + 2/|A k | # symmetric update 12:</formula><p>end for # Process all ordered token pairs between A k and A k+1 13: end for 20: end for return count(•, •)</p><formula xml:id="formula_8">if k &lt; t -1 then 14: A k+1 ← S ′ [k + 1] 15: for all c i ∈ A k , c j ∈ A k+1 do 16: count(c i , c j ) ← count(c i , c j ) + 1/(|A k | × |A k+1 |) #</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Data Structures</head><p>The data structures used in the proposed algorithm are carefully designed to optimize the efficiency of vocabulary construction.</p><p>Here is a detailed discussion of their roles and implementations:</p><p>• Linked list: Each action sequence in the training corpus is stored as a linked list. This allows efficient local updates during token merging. When a token pair (c u , c v ) is replaced by a new token c new , only the affected nodes and their neighbors in the linked list need to be modified (as shown in Algorithm 3 and Figure <ref type="figure" target="#fig_2">3</ref>).</p><p>• Heap with lazy update trick: A max-heap prioritizes token pairs by their co-occurrences. Instead of recalculating the heap entirely in each iteration, a "lazy update" strategy is employed: outdated entries (with mismatched co-occurrence counts) are retained but skipped during extraction. In the pseudocode, the loop checks if the top element is outdated via is outdated. Invalid entries are discarded, and only valid ones are processed. Updated co-occurrences are pushed as new entries (with negative counts for max-heap emulation).</p><p>• Inverted indices: The pair2head dictionary maps token pairs to the sequences containing them. When a pair (c u , c v ) is merged, the algorithm directly retrieves affected sequence IDs via pair2head[(c u, c v)], avoiding a full corpus scan. After merging, the inverted indices are incrementally updated: new token pairs (e.g., (c prev , c new ) and (c new , c next )) are added to pair2head, while obsolete pairs are removed. This enables targeted updates and ensures subsequent iterations efficiently access relevant sequences.</p><p>These structures collectively reduce time complexity by focusing computation on dynamically changing parts of the corpus and avoiding redundant global operations. The linked list enables localized edits, the heap minimizes priority recalculation, and the inverted indices eliminate brute-force searches, making the algorithm scalable to large corpora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Time Complexity</head><p>The time complexity of the efficient vocabulary construction algorithm can be analyzed through two main components: initialization and iterative merging.</p><p>Algorithm 3 ActionPiece Vocabulary Construction -Update (Figure <ref type="figure" target="#fig_2">3</ref>) input Action sequence corpus S ′ before updating, current merge rule {(c u , c v ) → c new } output Updated action sequence corpus S ′ 1: for all sequence S ′ ∈ S ′ do 2:</p><p>t ← length(S ′ )</p><p>3:</p><p>for k ← 0 to t -1 do 4: # Merge tokens from two adjacent nodes 10:</p><formula xml:id="formula_9">A k ← S ′ [k]</formula><p>if k &lt; t -1 then 11:</p><formula xml:id="formula_10">A k+1 ← S ′ [k + 1] 12: if c u ∈ A k and c v ∈ A k+1 then 13: if A k , A</formula><p>k+1 are both action nodes then 14: Create intermediate node M between A k and A k+1 15:</p><formula xml:id="formula_11">M ← {c new } # linked list: A k → M → A k+1 16: A k ← A k \ c u 17: A k+1 ← A k+1 \ c v 18: else if A k is intermediate node then 19: A k ← {c new } 20: A k+1 ← A k+1 \ c v 21: else if A k+1 is intermediate node then 22: A k ← A k \ c u 23: A k+1 ← {c new } 24:</formula><p>end if</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>25:</head><p>end if</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>26:</head><p>end if</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>27:</head><p>end for 28: end for return S ′</p><p>• Initialization phase involves building the initial max-heap to track co-occurrence frequencies. Given N input sequences (each with an average length of L), we count co-occurrences for all O(m 2 ) token pairs within each set of size m. This requires O(N Lm 2 ) time.</p><p>• Iterative merging phase dynamically processes the involved sequences. The total number of such sequences across all iterations is approximately</p><formula xml:id="formula_12">O N |V 0 | + O N |V 0 | + 1 + • • • + O N Q ≃ O(log QN ).</formula><p>For each sequence, updating the linked list requires O(Lm) time, counting co-occurrences takes O(Lm 2 ) time, and inserting co-occurrences into the max-heap requires at most O(Lm 2 log H) time. Here, H represents the heap size, which is at most O(N Lm). Thus, the overall time complexity for iterative merging is</p><formula xml:id="formula_13">O(log QN (Lm + Lm 2 + Lm 2 log H)) = O(log Q log H • N Lm 2 ).</formula><p>Therefore, the overall time complexity of our proposed vocabulary construction algorithm is</p><formula xml:id="formula_14">O(log Q log H • N Lm 2 ),</formula><p>where the iterative merging phase dominates. This complexity is significantly better than the naive vocabulary construction complexity of O(QN Lm 2 ).</p><p>Algorithm 4 Segmentation via Set Permutation Regularization (SPR) (Section 3.2.2) input Action sequence S, merge rules R output Segmented token sequences C 1: C ← [ ] # initialize permuted initial token sequence 2: for all token set A i ∈ S do 3: Generate random permutation of A i as [c 1 , c 2 , . . . , c |Ai| ] 4: Extend C with [c 1 , c 2 , . . . , c |Ai| ] # concatenate permutations 5: end for 6: 7: # Apply BPE (Sennrich et al., 2016) segmentation on permuted sequence 8: repeat 9: R ′ ← ∅ # candidate merge rules 10: While ActionPiece follows a similar algorithmic framework as BPE, its design is fundamentally different because it is tailored for tokenizing action sequences. To clarify, we summarize the key differences in Table <ref type="table" target="#tab_8">5</ref>.</p><formula xml:id="formula_15">for i ← 0 to |C| -1 do 11: if {(c i , c i+1 ) → c ′ } ∈ R then 12: R ′ ← R ′ ∪ {(c i , c i+1 ) → c ′ } 13: end if 14: end for 15: Select {(c k , c k+1 ) → c ′ } ∈ R ′ with the smallest index among all merge rules R 16: C ← [c 1 , . . . , c k-1 , c ′ , c k+2 , . . . ] # replace (c k , c k+1 ) with a new token c ′ 17: until R ′ is ∅ return C</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Datasets</head><p>Categories. Among all the datasets, "Sports" and "Beauty" are two widely used benchmarks for evaluating generative recommendation models <ref type="bibr" target="#b35">(Rajput et al., 2023;</ref><ref type="bibr" target="#b15">Jin et al., 2024;</ref><ref type="bibr" target="#b14">Hua et al., 2023)</ref>. We conduct experiments on these benchmarks to ensure fair comparisons with existing results. Additionally, we introduce "CDs", which contains about 4× more interactions than "Sports", making it a larger dataset for evaluating the scalability of GR models. For "CDs", we apply the same data processing strategy as the public benchmarks. The statistics of the processed datasets are shown in Table <ref type="table" target="#tab_9">6</ref>.</p><p>Sequence truncation length. Following Rajput et al. (2023), we filter out users with fewer than 5 reviews and truncate 1 def vocab_construction_iteration(max_heap, vocab, rules, pair2head): 2 """Performs one iteration of efficient vocabulary construction. 3 4 Args: 5 max_heap (PriorityQueue): Max-heap storing (co_occurrence, (c_u, c_v)) pairs. 6 vocab (List[Tuple]): Current vocabulary with merge rules. 7 rules (Dict): Merge rule {(c_u, c_v): c_new} mapping. 8 pair2head (Dict): Inverted indices that store mappings from the token pair to all sequences that contain this token pair. 9 """ 10 # Get most frequent valid pair (c_u, c_v) from max-heap 11 # Efficient version of "Count" in Algorithm 1 12 # Avoid recalculating co-occurrences for each iteration, by maintaining them in a max-heap with the lazy update trick 13 while not max_heap.empty(): 14 co_occurrence, (c_u, c_v) = max_heap.get() 15 if not is_outdated((c_u, c_v), co_occurrence): # outdated values are lazily removed. 16 break 17 18 # Create new token and update vocabulary 19 c_new = len(vocab) 20 vocab.append((c_u, c_v)) 21 rules[(c_u, c_v)] = c_new 22 23 # Update sequences containing (c_u, c_v) 24 seq_ids = pair2head[(c_u, c_v)].copy() # IDs of affected sequences 25 delta_counts = defaultdict(int) 26 27 for sid in seq_ids: 28 # Merge all (c_u, c_v) pairs in sequence 29 new_seq = merge_sequence(seqs[sid], c_u, c_v, c_new) # replace pairs 30 seqs[sid] = new_seq 31 32 # Calculate pair co-occurrence changes 33 new_freqs = count_pairs(new_seq) # get token co-occurrences of the updated sequence 34 delta = diff_counts(new_freqs, old_freqs[sid]) # compute co-occurrence differences 35 update_index(pair2head, delta, sid) # update inverted index 36 old_counts[sid] = new_freqs 37 38 # Accumulate global co-occurrence changes 39 for p, cnt in delta.items(): 40 delta_counts[p] += cnt 41 42 # Lazy update max-heap with new co-occurrences 43 for (c_u, c_v), delta in delta_counts.items(): 44 if abs(delta) &lt; eps: continue # eps: minimum update threshold 45 all_pair_freqs[(c_u, c_v)] += delta # Global co-occurrences 46 max_heap.put( (-all_pair_freqs[(c_u, c_v)], (c_u, c_v)) ) Figure 7. Pseudocode for a single iteration of the efficient vocabulary construction algorithm, illustrating how a max-heap with lazy updates is used to track and merge frequent token pairs.</p><p>action sequences to a maximum length of 20 for "Generative" methods, including ActionPiece. For "ID-based" and "Feature + ID" baselines, we set the maximum length to 50, as suggested in their original papers.</p><p>Item text features. Following <ref type="bibr" target="#b35">Rajput et al. (2023)</ref>; <ref type="bibr">Zheng et al. (2024a)</ref>; <ref type="bibr" target="#b38">Sheng et al. (2025)</ref>, the first step for feature engineering is to combine multiple raw text features into a single sentence for each item. Then, we use a pretrained sentence embedding model to encode this sentence into a vector representation. In all our implementations, we concatenate title, price, brand, feature, categories, and description, and use sentence-t5-base <ref type="bibr" target="#b28">(Ni et al., 2022)</ref> as the sentence embedding model.</p><p>• The encoded sentence embeddings of 768 dimension are directly used as textual item representations for UniSRec.</p><p>• We quantize the sentence embeddings using residual quantization (RQ) <ref type="bibr" target="#b35">(Rajput et al., 2023;</ref><ref type="bibr" target="#b53">Zeghidour et al., 2021;</ref><ref type="bibr">Zheng et al., 2024b)</ref> into three codes, each with 256 candidates. To prevent conflicts, we add an extra identification code. These four codes together serve as the RQ-based semantic IDs for TIGER and SPM-SID.</p><p>• For other baselines that require item features, such as FDSA, S 3 -Rec, VQ-Rec, HSTU, and our method, we follow <ref type="bibr" target="#b12">Hou et al. (2023)</ref> and quantize the sentence embeddings using optimized product quantization (OPQ) <ref type="bibr" target="#b7">(Ge et al., 2013)</ref>. Except for VQ-Rec, where the sentence embeddings are quantized into 32 codes as suggested in the original paper, we quantize the sentence embeddings into 4 codes for all other methods to ensure a fair comparison. The codebook size is 256 for each digit of code. For generative methods HSTU and ActionPiece, we also include an additional identification code to prevent conflicts. Note that, unlike RQ-based semantic IDs, features produced by product/vector quantization do not require a specific order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Baselines</head><p>We compare ActionPiece with the following representative baselines:</p><p>F.1. ID-Based Sequential Recommendation Methods</p><p>• SASRec <ref type="bibr" target="#b17">(Kang &amp; McAuley, 2018)</ref> represents each item using its unique item ID. It encodes item ID sequences with a self-attentive Transformer decoder. The model is trained by optimizing a binary cross-entropy objective.</p><p>• BERT4Rec <ref type="bibr" target="#b41">(Sun et al., 2019</ref>) also represents each item using its unique item ID. Unlike SASRec, it encodes sequences of item IDs with a bidirectional Transformer encoder. The model is trained using a masked prediction objective.</p><p>F.2. Feature-Enhanced Sequential Recommendation Methods</p><p>• FDSA <ref type="bibr" target="#b56">(Zhang et al., 2019)</ref> integrates item feature embeddings with vanilla attention layers to obtain feature representations. It then processes item ID sequences and feature sequences separately through self-attention blocks.</p><p>• S 3 -Rec <ref type="bibr" target="#b63">(Zhou et al., 2020)</ref> first employs self-supervised pre-training to capture the correlations between item features and item IDs. Then the checkpoints are loaded and fine-tuned for next-item prediction, using only item IDs.</p><p>• VQ-Rec <ref type="bibr" target="#b12">(Hou et al., 2023)</ref> encodes text features into dense vectors using pre-trained language models. It then applies product quantization to convert these dense vectors into semantic IDs. The semantic ID embeddings are pooled together to represent each item. Since the experiments are not performed in a transfer learning setting, we omit the two-stage training strategy outlined in the original paper. Instead, we reuse the model architecture and train it from scratch using an in-batch contrastive loss with a batch size of 256.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3. Generative Recommendation Methods</head><p>Each generative recommendation baseline corresponds to an action tokenization method described in Table <ref type="table" target="#tab_0">1</ref>.</p><p>• P5-CID <ref type="bibr" target="#b14">(Hua et al., 2023)</ref> is an extension of P5 <ref type="bibr" target="#b8">(Geng et al., 2022)</ref>, which formulates recommendation tasks in a text-to-text format. Building on P5, the authors explored several tokenization methods to index items for better recommendations. In this study, we use P5-CID as a representative hierarchical clustering-based action tokenization method. It organizes the eigenvectors of the Laplacian matrix of user-item interactions into a hierarchy and assigns cluster IDs at each level as item indices. When implementing this baseline method, we adopt the same model backbone as ActionPiece (encoder-decoder Transformers trained from scratch) and use the indices produced by P5-CID.</p><p>• TIGER <ref type="bibr" target="#b35">(Rajput et al., 2023)</ref> encodes text features similarly to VQ-Rec but quantizes them into semantic IDs using RQ-VAE. The model is then trained to autoregressively predict the next semantic ID and employs beam search for inference. We use a beam size of 50 in beam search to generate the top-K recommendations.</p><p>• LMIndexer <ref type="bibr" target="#b15">(Jin et al., 2024)</ref> takes text as input and predicts semantic IDs. The text description of each item is first tokenized using a text tokenizer. The resulting text tokens are then concatenated to form input action sequences. The selected from {1 × 10 -3 , 3 × 10 -3 , 5 × 10 -3 } with a warmup step of 10,000. We use a dropout rate of 0.1 and tune the weight decay from {0.07, 0.1, 0.15, 0.2}. For all methods implemented by us, we conduct five repeated experiments using random seeds {2024, 2025, 2026, 2027, 2028}. The model checkpoints with the best average NDCG@10 on the validation set are selected for evaluation on the test set, and we report these results. Each model is trained on a single 40G NVIDIA A100 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Reproduction</head><p>To improve reproducibility, we provide the algorithms of vocabulary construction and segmentation processes in Algorithms 1 and 2 to 4. We also provide the pseudocode for the efficient vocabulary construction implementation in Figure <ref type="figure">7</ref>. In addition, we provide the best hyperparameters of ActionPiece for all experimental datasets in Table <ref type="table">7</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Illustration of the tokenization process of ActionPiece. Each action is represented as an unordered feature set. This figure presents two possible tokenized sequences. The same action can be tokenized into different tokens depending on the surrounding context. A detailed case study can be found in Section 4.5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure2. Illustration of how weights of co-occurring token pairs are counted during vocabulary construction. In this example, two adjacent sets in the sequence are considered: one with 4 tokens (represented as ⃝) and another with 3 tokens (represented as □). Token pairs are counted within a single set (&lt; ⃝, ⃝ &gt; and &lt; □, □ &gt;) and across the two adjacent sets (&lt; ⃝, □ &gt;).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Illustration of how the linked list, which maintains the action sequence, is updated when merging two tokens into a new token. Three cases are considered: (1) both tokens are in the same action node; (2) the tokens are in two adjacent action nodes; (3) one token is in an action node, while the other is in an intermediate node.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><figDesc>the action sequence where each action is represented with a set of item features c, c 1 , c j input &amp; generated tokens l the number of tokens in the token sequence C = {c 1 , c 2 , . . . , c l } the token sequence tokenized from the input action sequence S ′ {c l+1 , . . . , c q } the tokens generated by the GR model V vocabulary of ActionPiece tokenizer R merge rules of ActionPiece tokenizer {(c u , c v ) → c new } one merge rule indicating two adjacent tokens c u and c v can be replaced by a token c new Q = |V| size of ActionPiece vocabulary P (c, c ′ ) probability that tokens c and c ′ are adjacent when flattening a sequence of sets into a token sequence N the number of action sequences in the training corpus L the average length of action sequences in the training corpus H Maximal heap size, O(N Lm) q</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><figDesc>weight of tokens from two adjacent sets (Equation (2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><figDesc>if c u ∈ A k and c v ∈ A k then 7:Replace c u and c v in A k with c</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of different action tokenization methods for generative recommendation. "Contextual" denotes whether the same actions can be tokenized into different tokens based on the surrounding context. "Unordered" denotes whether the item features or semantic IDs are used in an order-agnostic manner.</figDesc><table><row><cell>Action Tokenization</cell><cell>Example</cell><cell>Contextual Unordered</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Analysis of recommendation performance (NDCG@10, ↑) and average tokenized sequence length (NSL, ↓) w.r.t. vocabulary size across three datasets. "N/A" indicates that ActionPiece is not applied, i.e., action sequences are represented solely by initial tokens.Ablation analysis of ActionPiece. The recommendation performance is measured using NDCG@10. The best performance is denoted in bold fonts.</figDesc><table><row><cell>NDCG@10</cell><cell>0.022 0.023 0.024 0.025 0.026</cell><cell>Sports</cell><cell>0.7 0.8 0.9 1.0 NSL</cell><cell>NDCG@10</cell><cell>0.036 0.038 0.040 0.042</cell><cell>Beauty</cell><cell>0.7 0.8 0.9 1.0 NSL</cell><cell>NDCG@10</cell><cell>0.036 0.038 0.040 0.042 0.044</cell><cell>CDs</cell><cell>1.00 0.80 0.95 0.85 0.90 NSL</cell></row><row><cell></cell><cell></cell><cell cols="2">N/A5k 10k 20k 30k 40k Vocabulary Size</cell><cell></cell><cell></cell><cell cols="2">N/A5k 10k 20k 30k 40k Vocabulary Size</cell><cell></cell><cell></cell><cell>N/A5k 10k 20k 30k 40k Vocabulary Size</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>NDCG@10</cell><cell>NSL</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Figure 4. Variants</cell><cell cols="3">Sports Beauty</cell><cell>CDs</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">TIGER with larger vocabularies</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">(1.1) TIGER -1k (4 × 2 8 )</cell><cell cols="4">0.0225 0.0384 0.0411</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">(1.2) TIGER-49k (6 × 2 13 )</cell><cell cols="4">0.0162 0.0317 0.0338</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">(1.3) TIGER-66k (4 × 2 14 )</cell><cell>0.0194</cell><cell cols="2">N/A  †</cell><cell>0.0319</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Vocabulary construction</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">(2.1) w/o tokenization</cell><cell cols="4">0.0215 0.0389 0.0346</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">(2.2) w/o context-aware</cell><cell cols="4">0.0258 0.0416 0.0429</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">(2.3) w/o weighted counting 0.0257 0.0412 0.0435</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">Set permutation regularization</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">(3.1) only for inference</cell><cell cols="4">0.0192 0.0316 0.0329</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">(3.2) only for training</cell><cell cols="4">0.0244 0.0387 0.0422</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">ActionPiece (40k)</cell><cell cols="4">0.0264 0.0424 0.0451</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>Unlike existing methods, ActionPiece is the first context-aware action sequence tokenizer, i.e., the same action can be tokenized into different † not applicable as 2 14 is larger than #items in Beauty.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 .</head><label>5</label><figDesc>Comparison between ActionPiece and BPE.</figDesc><table><row><cell>Aspect</cell><cell>BPE</cell><cell>ActionPiece</cell></row><row><cell>Data Type</cell><cell>text sequences</cell><cell>action (unordered feature set) sequences</cell></row><row><cell>Token</cell><cell>a byte sequence</cell><cell>a feature set</cell></row><row><cell>Initial Vocabulary</cell><cell>single bytes</cell><cell>single-feature sets</cell></row><row><cell>Merging Unit</cell><cell>adjacent byte pairs</cell><cell>feature pairs within one set or between adjacent sets</cell></row><row><cell cols="2">Co-occurrence Weighting raw frequency counting</cell><cell>probabilistic weighting (Figure 2)</cell></row><row><cell>Segmentation Strategy</cell><cell cols="2">greedy fixed-order merging set permutation regularization (Algorithm 4)</cell></row><row><cell>Intermediate Structures</cell><cell>N/A</cell><cell>intermediate nodes for cross-action merges</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 .</head><label>6</label><figDesc>Statistics of the processed datasets. "Avg. t" denotes the average number of actions in an action sequence.</figDesc><table><row><cell>Datasets</cell><cell>#Users</cell><cell>#Items</cell><cell>#Actions</cell><cell>Avg. t</cell></row><row><cell>Sports</cell><cell>18,357</cell><cell>35,598</cell><cell>260,739</cell><cell>8.32</cell></row><row><cell>Beauty</cell><cell>22,363</cell><cell>12,101</cell><cell>176,139</cell><cell>8.87</cell></row><row><cell>CDs</cell><cell>75,258</cell><cell>64,443</cell><cell>1,022,334</cell><cell>14.58</cell></row><row><cell cols="4">D. Discussion: Comparison Between ActionPiece and BPE</cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>model is trained with self-supervised objectives to learn the semantic IDs of target items. The reported results in Table <ref type="table">2</ref> are taken from the original paper. We do not report the results of LMIndexer on the large dataset "CDs" because it does not converge under similar computing budget as the other methods.</p><p>• HSTU <ref type="bibr" target="#b54">(Zhai et al., 2024)</ref> discretizes raw item features into tokens, treating them as input tokens for generative recommendation. The authors also propose a lightweight Transformer layer that improves both performance and efficiency. For action tokenization, we use the same item features as our method and arrange them in a specific order to form the tokenized tokens of each item.</p><p>• SPM-SID <ref type="bibr" target="#b40">(Singh et al., 2024)</ref> first tokenizes each item into semantic IDs. It then uses the SentencePiece model (SPM) <ref type="bibr" target="#b19">(Kudo &amp; Richardson, 2018)</ref> to merge important semantic ID patterns within each item into new tokens in the vocabulary. While the original paper introduces this method for ranking models, we adapt it for the generative recommendation task. Specifically, we concatenate the SPM tokens as inputs, feed them into the T5 model, and autoregressively generate SPM tokens as recommendations.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schalkwyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hauth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Glaese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pitler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Molloy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hennigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Doherty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Moreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ayoub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Piqueras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Savinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Andreassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Von Glehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yagati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Khalman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sygnowski</surname></persName>
		</author>
		<idno>arxiv:2312.11805</idno>
		<title level="m">A family of highly capable multimodal models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Wide &amp; deep learning for recommender systems</title>
		<author>
			<persName><forename type="first">H.-T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Koc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Harmsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Aradhye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ispir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Deep Learning for Recommender Systems</title>
		<meeting>the 1st Workshop on Deep Learning for Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="7" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Getting the most out of your tokenizer for pre-training and domain adaptation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Roziere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A review of modern recommender systems using generative models (gen-recsys)</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Deldjoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Korikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sanner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramisa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sathiamoorthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kasirzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Milano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="6448" to="6458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Inductive generative recommendation via retrieval-based speculation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.02939</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guzhva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Szilvasy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-E</forename><surname>Mazaré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lomeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.08281</idno>
		<title level="m">The faiss library</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Optimized product quantization for approximate nearest neighbor search</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2946" to="2953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Recommendation as language processing (rlp): A unified pretrain, personalized prompt &amp; predict paradigm (p5)</title>
		<author>
			<persName><forename type="first">S</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RecSys</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="299" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dynamic programming encoding for subword segmentation in neural machine translation</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3042" to="3051" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Session-based recommendations with recurrent neural networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hidasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karatzoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Baltrunas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tikk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An embarrassingly simple method to mitigate undesirable properties of pretrained language model tokenizers</title>
		<author>
			<persName><forename type="first">V</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schuetze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pierrehumbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-R</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="page" from="585" to="593" />
		</imprint>
	</monogr>
	<note>Towards universal sequence representation learning for recommender systems KDD</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning vector-quantized item representation for transferable sequential recommenders</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>WWW</publisher>
			<biblScope unit="page" from="1162" to="1171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Large language models are zero-shot rankers for recommender systems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECIR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">How to index item ids for recommendation foundation models</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR-AP</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="195" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Language models as semantic indexers</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Field-aware factorization machines for ctr prediction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-S</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RecSys</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="43" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Self-attentive sequential recommendation</title>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="197" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Subword regularization: Improving neural network translation models with multiple subword candidates</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kudo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="66" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semantic convergence: Harmonizing recommender systems via two-stage alignment and behavioral semantic tokenization</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neural attentive session-based recommendation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1419" to="1428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">End-to-end learnable item tokenization for generative recommendation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.05546</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><surname>Mmgrec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.16555</idno>
		<title level="m">Multimodal generative recommendation with transformer model</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-behavior generative recommendation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Image-based recommendations on styles and substitutes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Targett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hengel</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Introducing Meta Llama 3: The most capable openly available LLM to date</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Meta</surname></persName>
		</author>
		<ptr target="https://ai.meta.com/blog/meta-llama-3/" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Abrego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of ACL</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1864" to="1874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title/>
		<ptr target="https://openai.com/index/chatgpt/" />
	</analytic>
	<monogr>
		<title level="j">OpenAI. Introducing ChatGPT</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Content-based recommendation systems</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Pazzani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Billsus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The adaptive web: methods and strategies of web personalization</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="325" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Generative sequential recommendation with gptrec</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.11114</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Bpe-dropout: Simple and effective subword regularization</title>
		<author>
			<persName><forename type="first">I</forename><surname>Provilkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Emelianenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Voita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1882" to="1892" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Recommender systems with generative retrieval</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rajput</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Keshavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Heldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">Q</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Samost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sathiamoorthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Tokenization is more than compression</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alameddine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Uzan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pinter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tanner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Language representations can be what recommenders need: Findings and potentials</title>
		<author>
			<persName><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Generative retrieval with semantic tree-structured item identifiers via contrastive learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR-AP</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Better generalization with semantic ids: A case study in ranking for recommendations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Keshavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sathiamoorthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Heldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RecSys</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Bert4rec: Sequential recommendation with bidirectional encoder representations from transformer</title>
		<author>
			<persName><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1441" to="1450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Idgenrec: Llm-recsys alignment with textual id learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rozière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Azhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><surname>Llama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13971</idno>
		<title level="m">Open and efficient foundation language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learnable tokenizer for llm-based generative recommendation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-K</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Content-based collaborative generation for recommender systems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Eager: Twostream generative recommender with behavior-semantic collaboration</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="3245" to="3254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kazawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Riesa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rudnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<title level="m">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Unifying generative and dense retrieval for sequential recommendation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Paischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Noorshams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Eghbalzadeh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2411.18814</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Incorporating context into subword vocabularies</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yehezkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pinter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="623" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Where to go next for recommender systems? id-vs. modality-based recommender models revisited</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2639" to="2649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Soundstream: An end-to-end neural audio codec</title>
		<author>
			<persName><forename type="first">N</forename><surname>Zeghidour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Luebs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Skoglund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tagliasacchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="495" to="507" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Actions speak louder than words: Trillion-parameter sequential transducers for generative recommendations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Recommendation as instruction following: A large language model empowered recommendation approach</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-R</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst</title>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Feature-level deeper self-attention network for sequential recommendation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4320" to="4326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Towards scalable semantic representation for recommendation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.09560</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Towards a unified, comprehensive and efficient framework for recommendation algorithms</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-R</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><surname>Recbole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A revisiting study of appropriate offline evaluation for top-n recommendation algorithms</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-R</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="41" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.18223</idno>
		<title level="m">A survey of large language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Adapting large language models by integrating collaborative semantics for recommendation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-R</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="1435" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Enhancing graph contrastive learning with reliable and informative augmentation for recommendation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-R</forename><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.05633</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">S3-rec: Self-supervised learning for sequential recommendation with mutual information maximization</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-R</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1893" to="1902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Cost: Contrastive quantization based semantic tokenization for generative recommendation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RecSys</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">) and set the token embedding dimension to 128 and the intermediate feed-forward layer dimension to 1024. This results in a total of 4.46M non-embedding parameters. For the larger &quot;CDs&quot; dataset, we use a token embedding dimension of 256 and an intermediate feed-forward layer dimension of 2048, leading to 13.11M non-embedding parameters. For model inference, we use beam search with a beam size of 50. Note that the baselines P5-CID, TIGER, and SPM-SID use the same model architecture, differing only in their action tokenization methods. For ActionPiece-specific hyperparameters</title>
		<author>
			<persName><forename type="first">V</forename><surname>Zouhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gastaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vieira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sachan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The results of BERT4Rec, SASRec, FDSA, S 3 -Rec, TIGER, and LMIndexer on the &quot;Sports&quot; and &quot;Beauty&quot; benchmarks are taken directly from existing papers</title>
		<imprint>
			<date type="published" when="2019">2023. 2020. 2023. 2024. 2021. 2019. 2020</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="598" to="614" />
		</imprint>
	</monogr>
	<note>Findings of ACL Rajput Implementation Details Baselines We implement BERT4Rec, SASRec, FDSA, and S 3 -Rec using the open-source recommendation library RecBole For other methods, we implement them ourselves with HuggingFace Transformers (Wolf et al., 2020) and PyTorch We use FAISS (Douze et al., 2024) to quantize sentence representations For other results, we carefully implement the baselines and tune hyperparameters according to the suggestions in their original papers we follow Rajput et al. (2023 We use four layers for both the encoder and decoder. The multi-head attention module has six heads, each with a dimension of 64. For the public benchmarks &quot;Sports&quot; and &quot;Beauty we set the number of segmentations produced using set permutation regularization during inference to q = 5. We tune the vocabulary size in {5k, 10k, 20k, 30k, 40k}</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">We train the GR models from scratch for up to 200 epochs, using early stopping if the model does not achieve a better NDCG@10 on the validation set for 20 consecutive epochs. The training batch size is set to 256</title>
	</analytic>
	<monogr>
		<title level="j">Training</title>
		<imprint/>
	</monogr>
	<note>The learning rate is</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
