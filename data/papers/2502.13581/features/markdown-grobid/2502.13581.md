# ActionPiece: Contextually Tokenizing Action Sequences for Generative Recommendation

## Abstract

## 

Generative recommendation (GR) is an emerging paradigm where user actions are tokenized into discrete token patterns and autoregressively generated as predictions. However, existing GR models tokenize each action independently, assigning the same fixed tokens to identical actions across all sequences without considering contextual relationships. This lack of context-awareness can lead to suboptimal performance, as the same action may hold different meanings depending on its surrounding context. To address this issue, we propose ActionPiece to explicitly incorporate context when tokenizing action sequences. In Ac-tionPiece, each action is represented as a set of item features, which serve as the initial tokens. Given the action sequence corpora, we construct the vocabulary by merging feature patterns as new tokens, based on their co-occurrence frequency both within individual sets and across adjacent sets. Considering the unordered nature of feature sets, we further introduce set permutation regularization, which produces multiple segmentations of action sequences with the same semantics. Experiments on public datasets demonstrate that Ac-tionPiece consistently outperforms existing action tokenization methods, improving NDCG@10 by 6.00% to 12.82%.

## Introduction

Generative recommendation (GR) [(Geng et al., 2022;](#b8)[Rajput et al., 2023;](#b35)[Zheng et al., 2024a;](#)[Zhai et al., 2024)](#b54) is an emerging paradigm for the sequential recommendation task [(Hidasi et al., 2016;](#b10)[Kang & McAuley, 2018)](#b17). By tokenizing the user actions (typically represented by the interacted items) into discrete tokens, GR models learn to autoregressively generate tokens, which are then parsed into recommended items. These tokens share a compact vocabulary that does not scale with the item pool size, improving model scalability, memory efficiency, and recommendation performance [(Rajput et al., 2023;](#b35)[Zhai et al., 2024)](#b54). The input action sequence is vital in understanding user intentions [(Li et al., 2017;](#b21)[Kang & McAuley, 2018)](#b17), which organizes a user's historical interactions in chronological order. The same action (e.g., purchasing the same item) may have different meanings in different action sequences. Evidence of taking a certain action can be found in the context, such as whether other items in the sequence share the same brand, color tone, or price range [(Zhang et al., 2019;](#b56)[Zhou et al., 2020;](#b63)[Hou et al., 2022;](#b11)[2023;](#)[Yuan et al., 2023)](#b52).

Despite the importance of contextual relations among actions, existing methods tokenize each action independently of its context (summarized in Table [1](#tab_0)). The typical pipeline for tokenizing action sequences involves two steps: (1) Tokenizing each action/item individually into a pattern of tokens; (2) Replacing each action in the input sequence with its corresponding token pattern. In this way, the tokens do not explicitly contain the context. Instead, they solely rely on the autoregressive model's parameters being well-trained to generalize effectively in understanding the context, which challenges the capabilities of GR models. As a comparison, tokenization in language modeling also originates from context-independent methods, such as word-level tokeniza-tion [(Sutskever et al., 2014;](#b42)[Bahdanau et al., 2015)](#b1). A decade of progress has led to most tokenization methods for modern large language models (LLMs) [(OpenAI, 2022;](#)[Anil et al., 2023;](#b0)[Touvron et al., 2023;](#b44)[Zhao et al., 2023)](#b60) adopting context-aware approaches, including BPE [(Sennrich et al., 2016)](#b37) and Unigram tokenization [(Kudo, 2018)](#b18), which tokenize the same characters along with their adjacent context into different tokens.

In this work, we aim to make the first step towards contextaware tokenization for modeling action sequences. In analogy to how characters or bytes serve as the basic units in language modeling, we consider the associated features of an item as initial tokens. The idea is to iteratively find the most commonly co-occurring pairs of tokens among the training action sequences, then merge them into new tokens to represent segments of context. However, it's non-trivial to achieve this. Unlike text, where characters naturally form a sequence, the features associated with an action form an unordered set [(Zhang et al., 2019;](#b56)[Zhou et al., 2020)](#b63). Thus, the proposed tokenization algorithm should be applied on sequences of token sets. We need to carefully consider which pairs of tokens should be counted, whether within a single set or between two adjacent sets, and how much weight should be given to these different types of relationships.

To this end, we propose ActionPiece, which enables the same actions to be tokenized into different tokens based on their surrounding context. (1) Vocabulary construction:

We first initialize the vocabulary to include every unique feature as initial tokens. The vocabulary is then constructed by iteratively learning merge rules. Each merge rule specifies that a pair of tokens can be merged into a new token. In each iteration, we enumerate the training corpus to count the co-occurrence of existing tokens. Considering the structural differences between token pairs, e.g., whether they occur within a single set or between two adjacent sets, we assign different weights to different pairs during the counting process. (2) Segmentation: The next step is to segment action sequences into token sequences for GR model training and inference. To fully exploit the unordered nature of the feature set for each action, we introduce set permutation regularization. By randomly permuting the features within each set, we can produce multiple token sequences of a single action sequence that preserve the same semantics. These variations act as natural augmentations for training data and enable inherent ensembling during model inference.

## Related Work

Generative recommendation. Conventional sequential recommendation models often relies on large embedding tables to store representations for all items, leading to significant engineering and optimization challenges [(Hidasi et al., 2016;](#b10)[Kang & McAuley, 2018)](#b17). Generative recommenda- Product Quantization VQ-Rec [(Hou et al., 2023](#b12)) Hierarchical Clustering P5-CID [(Hua et al., 2023)](#b14) Residual Quantization TIGER [(Rajput et al., 2023)](#b35) Text Tokenization LMIndexer [(Jin et al., 2024](#b15)) Raw Features HSTU [(Zhai et al., 2024](#b54)) SentencePiece SPM-SID [(Singh et al., 2024)](#b40) ActionPiece Ours tion [(Deldjoo et al., 2024;](#b4)[Rajput et al., 2023;](#b35)[Zheng et al., 2024a)](#) addresses these issues by tokenizing each item as tokens from a shared table. By autoregressively generating the next tokens as recommendations, this generative paradigm offers benefits such as memory efficiency [(Rajput et al., 2023;](#b35)[Yang et al., 2024;](#b50)[Ding et al., 2024)](#b5), scalability [(Zhai et al., 2024;](#b54)[Liu et al., 2024c)](#), and easier alignment with LLMs [(Zheng et al., 2024a;](#)[Jin et al., 2024;](#b15)[Tan et al., 2024;](#b43)[Li et al., 2025)](#b20). Existing research has developed different action tokenization techniques, such as hierarchical clustering [(Hua et al., 2023;](#b14)[Si et al., 2024)](#b39), quantization [(Rajput et al., 2023;](#b35)[Wang et al., 2024a;](#)[Zhu et al., 2024)](#b64), or jointly training with recommendation models [(Liu et al., 2024a)](#).

Other works incorporate additional modalities like collaborative filtering [(Petrov & Macdonald, 2023;](#b32)[Wang et al., 2024c;](#)[b;](#)[Liu et al., 2024c](#)), text [(Zheng et al., 2024a;](#)[Jin et al., 2024;](#b15)[Hou et al., 2024;](#b13)[Zhang et al., 2025)](#b55), and vision signals [(Liu et al., 2024b)](#). However, current methods tokenize each action independently, ignoring the surrounding context. In this work, we propose the first context-aware action tokenization method, where the same actions are tokenized differently in different action sequences.

Tokenization for language modeling. Tokenization is the process of transforming raw text into discrete token sequences [(Kudo & Richardson, 2018)](#b19). Early word-level methods are context-independent and struggle to tokenize out-of-vocabulary words [(Sutskever et al., 2014;](#b42)[Bahdanau et al., 2015)](#b1). Consequently, subword-level tokenization has gradually become the more mainstream choice. The vocabularies of these subword-level tokenizers are constructed iteratively, either bottom-up (starting with a small vocabulary and merging commonly occurring token pairs as new tokens) [(Wu et al., 2016;](#b49)[Sennrich et al., 2016)](#b37), or top-down (starting with a large vocabulary and pruning tokens to minimize likelihood decrease) [(Kudo, 2018;](#b18)[Yehezkel & Pinter, 2023)](#b51). Once the vocabulary is built, the text can be segmented either using the same method employed during vocabulary construction or based on additional objectives [(He et al., 2020;](#b9)[Provilkov et al., 2020;](#b33)[Hofmann et al., 2022;](#b11)[Schmidt et al., 2024)](#b36). As an analogy, existing action tokeniz-ers are context-independent and function like "word-level" language tokenizers. In this work, we take the first step toward context-aware subaction-level action tokenizer.

## Method

In this section, we present ActionPiece, a context-aware method for tokenizing action sequences in generative recommendation. First, we formulate the task in Section 3.1. Then, we introduce the proposed tokenizer, covering vocabulary construction and sequence segmentation, in Section 3.2. Finally, we describe the model training and inference process using ActionPiece-tokenized sequences in Section 3.3.

## Problem Formulation

Given a user's historical actions S = {i 1 , i 2 , . . . , i t }, organized sequentially by their timestamps, the task is to predict the next item i t+1 the user will interact with.

Action as an unordered feature set. In the development of modern recommender systems, each item i j is usually associated with a set of features A j [(Zhang et al., 2019;](#b56)[Zhou et al., 2020;](#b63)[Cheng et al., 2016)](#b2). Assuming there are m features per item, the k-th feature of item i j is denoted as f j,k ∈ F k , where F k is the collection of all possible choices for the k-th feature. Compared to representing actions using ordered semantic IDs (e.g., those produced by RQ-VAE [(Rajput et al., 2023;](#b35)[Singh et al., 2024)](#b40)), the unordered set setting offers two key advantages: (1) It does not require a specific order among features, which aligns better with how items or actions are represented in most recommender systems;

(2) It enables the inclusion of more general discrete and numeric features, such as category, brand, and price [(Pazzani & Billsus, 2007;](#b31)[Juan et al., 2016)](#b16).

Action sequence as a sequence of sets. Representing each item as an unordered set, the input action sequence can be written as S ′ = {A 1 , A 2 , . . . , A t }, which is a chronologically ordered sequence of sets. There is no order within each set, but there are orders between the features from different sets. The tokenizer design should account for the ordered and unordered relationships among features.

Generative recommendation task. In this work, we aim to design a tokenizer that maps an input action sequence S ′ to a token sequence C = {c 1 , c 2 , . . . , c l }, where l denotes the number of tokens in the sequence. Note that l is typically greater than the number of actions t. Next, we train a GR model to autoregressively generate tokens {c l+1 , . . . , c q }, which can be parsed as next-item predictions ît+1 .

## Contextual Action Sequence Tokenizer

The proposed tokenizer is designed to transform action sequences (represented as sequences of feature sets) into token Algorithm 1 ActionPiece Vocabulary Construction input Sequence corpus S ′ , initial tokens V0, target size Q output Merge rules R, constructed vocabulary V 1: Initialize vocabulary V ← V0 # each initial token corresponds to one unique item feature 2: R ← ∅ 3: while |V| < Q do 4: # Count: accumulate weighted token co-occurrences 5: count(•, •) ← Count(S ′ , V) # Algorithm 2 6: # Update: Merge a frequent token pair into a new token 7: Select (cu, cv) ← arg max (c i ,c j ) count(ci, cj) 8: S ′ ← Update(S ′ , {(cu, cv) → cnew}) # Algorithm 3 9: R ← R ∪ {(cu, cv) → cnew} # new merge rule 10:

V ← V ∪ {cnew} # add new token to the vocabulary 11: end while return R, V sequences. In the ActionPiece-tokenized sequences, each token corresponds to a set containing varying numbers of features. For example, a token can represent: (1) a subset of features from one item; (2) a set with a single feature;

(3) all features of one item; or (4) features from multiple items. We also label these four types of tokens in Figure [1](#fig_0). Below, we first describe how to construct the ActionPiece tokenizer's vocabulary given a corpus of action sequences (Section 3.2.1). Then, we introduce how to segment action sequences into a new sequence of sets, where each set corresponds to a token from the constructed vocabulary (Section 3.2.2).

## VOCABULARY CONSTRUCTION ON ACTION SEQUENCE CORPUS

Given a corpus of action sequences S ′ , the goal of vocabulary construction is to create a vocabulary V of Q tokens. Each token represents a combination of features that frequently occur in the corpus. Similar to BPE [(Sennrich et al., 2016)](#b37), we construct the vocabulary using a bottom-up approach. The process starts with an initial vocabulary of tokens V 0 . The construction proceeds iteratively, adding one new token to the vocabulary at each iteration until the predefined target size is reached. Each iteration consists of two consecutive steps: count, where the most frequently occurring token pair is identified, and update, where the corpus is modified by merging the selected pair into a new token. An algorithmic workflow is illustrated in Algorithm 1.

Vocabulary initialization. In BPE, each token represents a sequence of bytes. Thus, the most fundamental unitsthe initial tokens-are single bytes, which form the initial vocabulary of BPE. Similarly, each token in ActionPiece represents a set of features. Therefore, we initialize Action-Piece with a vocabulary in which each token represents a set containing one unique item feature. Formally, we denote the initial vocabulary as

$V 0 = {c = {f }|f ∈ F 1 ∪ . . . ∪ F m }.$After initializing the vocabulary, each action sequence (of feature sets) can be represented as a sequence of token sets.

$••• ••• flattening ••• •••$Count: context-aware token co-occurrence counting. In each iteration of vocabulary construction, the first step is to count the co-occurrence of token pairs in the corpus. These pairs capture important feature combinations, which are encoded by creating new tokens. There are two types of token co-occurrence within a sequence of sets: (1) two tokens exist within the same set, or (2) two tokens exist in adjacent sets in the sequence. Notably, the second type allows ActionPiece to explicitly include context information.

Weighted co-occurrence counting. In one-dimensional token sequences (e.g., text), all token pairs are typically treated equally. However, in sequences of token sets, token pairs vary based on their types and the sizes of their respective sets. To account for these differences, we propose assigning different weights to token pairs. To determine the weight for each token pair, we relate sequences of token sets to token sequences by randomly permuting the tokens within each set and flattening them into a single token sequence. Let P (c, c ′ ) represent the expected probability that tokens c and c ′ are adjacent in the flattened sequence. For two tokens from the same set, we have:

$P (c 1 , c 2 ) = P (c 2 , c 1 ) = |A i | -1 |Ai| 2 = 2 |A i | , c 1 , c 2 ∈ A i ,$(1) and for two tokens from adjacent sets, we have:

$P (c 1 , c 3 ) = 1 |A i | × |A i+1 | , c 1 ∈ A i , c 3 ∈ A i+1 . (2)$By considering the probabilities of all adjacent token pairs in the flattened sequence as 1, the weights for token pairs in the original sequence of token sets correspond to the probabilities given in Equations ( [1](#)) and ( [2](#)). An illustration is shown in Figure [2](#fig_1).

Accumulating co-occurrence weights. The weights described above are calculated based solely on the cooccurrence type and the set size. They do not take into account the specific tokens being analyzed. Tokens c i and c j might appear in the same set in one sequence but in two adjacent sets in another sequence. By iterating through the corpus, we sum up the weights for each token pair whenever they appear together multiple times.

Update: corpus updating with action-intermediate nodes. The next step in each iteration is to merge the token pair with the highest accumulated co-occurrence weight.

Since token merging may change the set size, we use a double-ended linked list [(Zouhar et al., 2023)](#b65) to maintain the action sequences, where each node represents a set of tokens. Merging tokens within the same set is straightforward, i.e., replacing the two tokens with a new one. However, merging tokens from two adjacent sets is more complex, e.g., determining which set should include the new token.

Intermediate Node. We introduce the concept of "intermediate node" to handle tokens that combine features from multiple sets. Initially, all nodes in the maintained linked list contain features specific to their corresponding actions. These nodes are referred to as "action nodes."

(1) When tokens from two adjacent action nodes are being merged, we insert a new intermediate node between the two action nodes. The new token is stored in the intermediate node, and the merged tokens are removed from their respective action nodes;

(2) When merging tokens from an action node and an intermediate node, the new token replaces the original token in the intermediate node. The reason is that this new token also combines features from multiple actions. After the merge, the token from the action node is removed.

Following the above update rules ensures that there is at most one intermediate node between any two action nodes, and each intermediate node contains no more than one token. When calculating co-occurrence weights involving an intermediate node, it can simply be treated as a set of size 1.

Efficient implementation. Naively counting and updating the corpus requires a total time complexity of O(QN Lm 2 ), where Q is the target vocabulary size, N is the number of action sequences in the training corpus, and L is the average length of these sequences. However, it is unnecessary to count co-occurrences from scratch in each iteration. This is because only a small portion of the maintained linked lists is modified compared to the previous iteration. Data structures. To address this, we propose maintaining a hash table to store the accumulated co-occurrences for each token pair. Additionally, we use inverted indices to map token pairs to all the linked lists that contain them. A global heap is maintained to return the token pair with the highest accumulated co-occurrence. The key challenge lies in updating these data structures. We carefully compute the changes in accumulated co-occurrences and update both the hash table and the inverted indices. For the heap, we employ a lazy-update strategy. We insert the latest weights with a tag. When fetching a value from the heap, we check the tag to verify if the value is up-to-date. If it is not, we discard the value and fetch the next one. Time complexity. Let H = O(N Lm) represent the maximal heap size. Using the proposed algorithm, we successfully reduce the original time complexity to O(log Q log H • N Lm 2 ), achieving efficient vocabulary construction. In practice, the later iterations take significantly less time than the initial ones. This is expected and because tokens with higher accumulated co-occurrence weights typically appear frequently in the early stages. However, the overall construction time benefits from the reduced amortized complexity. Further details about the vocabulary construction algorithm are provided in Appendix C.

## SEGMENTATION BY SET PERMUTATION REGULARIZATION

Segmentation is to convert original action sequences into a sequence of feature sets. Each set in the segmented sequence corresponds to a token in the vocabulary.

Naive segmentation. One segmentation strategy in Action-Piece involves applying the same technique used to construct the vocabulary. Specifically, this technique iteratively identifies token pairs with high priorities (represented by the IDs of tokens, where tokens added earlier may have higher priority). However, we observed that this strategy can lead to a bias, where only a subset of tokens in the vocabulary is frequently used (as shown empirically in Section 4.4.2).

## Set permutation regularization (SPR).

To address this issue and account for the unordered nature of sets, we propose set permutation regularization, which generates multiple segmentations for each action sequence. The key idea is to avoid enumerating all possible pairs between tokens in a set or adjacent sets. Instead, we generate a random permutation of each set and treat it as a one-dimensional sequence. By concatenating all the permutations, we create a long token sequence. This sequence can then be segmented using traditional BPE segmentation methods [(Sennrich et al., 2016)](#b37). In this approach, different permutations can produce distinct segmented token sequences with the same semantics. These sequences serve as natural augmentations for model training (Section 3.3.1) and enable inherent ensembling during model inference (Section 3.3.2).

## Generative Recommendation Models

## TRAINING ON AUGMENTED TOKEN SEQUENCES

For an action sequence and its ground-truth next action in the training corpus, we tokenize them into token sequences C in and C out , respectively. Taking C in as input, we train a Transformer encoder-decoder module [(Raffel et al., 2020)](#b34) to autoregressively generate C out (e.g., next-token prediction objective [(Rajput et al., 2023)](#b35)). During training, we tokenize the action sequence using the set permutation regularization described in Section 3.2.2 in each epoch. This approach naturally augments the training sequences, which empirically improves model performance, as shown in Section 4.3.

## INFERENCE-TIME ENSEMBLING

During model inference, we tokenize each action sequence q times using set permutation regularization. By passing these q tokenized sequences through the model, we obtain q Table [2](#). Performance comparison of different methods on the Amazon Reviews dataset [(McAuley et al., 2015)](#). The best and second-best performance is denoted in bold and underlined fonts. "R@K" and "N@K" are short for "Recall@K" and "NDCG@K", respectively. "Improv." denotes the percentage improvement of our method compared to the strongest baseline method.

Datasets Metric ID-based Feature + ID Generative Improv. BERT4Rec SASRec FDSA S 3 -Rec VQ-Rec P5-CID TIGER LMIndexer HSTU SPM-SID ActionPiece Sports R@5 0.0115 0.0233 0.0182 0.0251 0.0181 0.0287 0.0264 0.0222 0.0258 0.0280 0.0316 ± 0.0005 +12.86% N@5 0.0075 0.0154 0.0122 0.0161 0.0132 0.0179 0.0181 0.0142 0.0165 0.0180 0.0205 ± 0.0002 +11.71% R@10 0.0191 0.0350 0.0288 0.0385 0.0251 0.0426 0.0400 -0.0414 0.0446 0.0500 ± 0.0007 +12.11% N@10 0.0099 0.0192 0.0156 0.0204 0.0154 0.0224 0.0225 -0.0215 0.0234 0.0264 ± 0.0003 +12.82% Beauty R@5 0.0203 0.0387 0.0267 0.0387 0.0434 0.0468 0.0454 0.0415 0.0469 0.0475 0.0511 ± 0.0014 +7.58% N@5 0.0124 0.0249 0.0163 0.0244 0.0311 0.0315 0.0321 0.0262 0.0314 0.0321 0.0340 ± 0.0011 +5.92% R@10 0.0347 0.0605 0.0407 0.0647 0.0741 0.0701 0.0648 -0.0704 0.0714 0.0775 ± 0.0017 +4.59% N@10 0.0170 0.0318 0.0208 0.0327 0.0372 0.0400 0.0384 -0.0389 0.0399 0.0424 ± 0.0011 +6.00% CDs R@5 0.0326 0.0351 0.0226 0.0213 0.0314 0.0505 0.0492 -0.0417 0.0509 0.0544 ± 0.0005 +6.88% N@5 0.0201 0.0177 0.0137 0.0130 0.0209 0.0326 0.0329 -0.0275 0.0337 0.0359 ± 0.0004 +6.53% R@10 0.0547 0.0619 0.0378 0.0375 0.0485 0.0785 0.0748 -0.0638 0.0778 0.0830 ± 0.0008 +5.73% N@10 0.0271 0.0263 0.0186 0.0182 0.0264 0.0416 0.0411 -0.0346 0.0424 0.0451 ± 0.0005 +6.37%

output ranking lists (e.g., using beam search for inference when TIGER [(Rajput et al., 2023)](#b35) is the GR backbone). We then combine these ranking lists by averaging the scores of each predicted item. This approach applies data-level ensembling, which has been shown to enhance recommendation performance, as discussed in Section 4.4.3.

## Experiments

## Experimental Setup

Datasets. We use three categories from the Amazon Reviews dataset [(McAuley et al., 2015)](#) for our experiments: "Sports and Outdoors" (Sports), "Beauty" (Beauty), and "CDs and Vinyl" (CDs). Each user's historical reviews are considered "actions" and are sorted chronologically as action sequences, with earlier reviews appearing first. To evaluate the models, we adopt the widely used leave-lastout protocol [(Kang & McAuley, 2018;](#b17)[Zhao et al., 2022;](#b59)[Rajput et al., 2023)](#b35), where the last item and second-to-last item in each action sequence are used for testing and validation, respectively. More details about the datasets can be found in Appendix E.

Compared methods. We compare the performance of Ac-tionPiece with the following methods: (1) ID-based sequential recommendation methods, including BERT4Rec [(Sun et al., 2019)](#b41), and SASRec [(Kang & McAuley, 2018)](#b17);

(2) feature-enhanced sequential recommendation methods, such as FDSA [(Zhang et al., 2019)](#b56), S 3 -Rec [(Zhou et al., 2020)](#b63), and VQ-Rec [(Hou et al., 2023)](#b12); and (3) generative recommendation methods, including P5-CID [(Hua et al., 2023)](#b14), TIGER [(Rajput et al., 2023)](#b35), LMIndexer [(Jin et al., 2024)](#b15), HSTU [(Zhai et al., 2024)](#b54), and SPM-SID [(Singh et al., 2024)](#b40), each representing a different action tokenization method (Table [1](#tab_0)). A detailed description of these baselines is provided in Appendix F.

Evaluation settings. Following [Rajput et al. (2023)](#b35), we use Recall@K and NDCG@K as metrics to evaluate the methods, where K ∈ {5, 10}. Model checkpoints with the best performance on the validation set are used for evaluation on the test set. We run the experiments with five random seeds and report the average metrics.

Implementation details. Please refer to Appendix G for detailed implementation and hyperparameter settings.

## Overall Performance

We compare ActionPiece with sequential recommendation and generative recommendation baselines, which use various action tokenization methods, across three public datasets. The results are shown in Table [2](#).

For the compared methods, we observe that those using item features generally outperform item ID-only methods. This indicates that incorporating features enhances recommendation performance. Among the methods leveraging item features ("Feature + ID" and "Generative"), generative recommendation models achieve better performance. These results further confirm that injecting semantics into item indexing and optimizing at a sub-item level enables generative models to better use semantic information and improve recommendation performance. Among all the baselines, SPM-SID achieves the best results. By incorporating the SentencePiece model [(Kudo & Richardson, 2018)](#b19), SPM-SID replaces popular semantic ID patterns within each item with new tokens, benefiting from a larger vocabulary.

Our proposed ActionPiece consistently outperforms all baselines across three datasets, achieving a significant improvement in NDCG@10. It surpasses the best-performing baseline method by 6.00% to 12.82%. tokens depending on its surrounding context. This allows ActionPiece to capture important sequence-level feature patterns that enhance recommendation performance.

## Ablation Study

We conduct ablation analyses in Table [3](#tab_2) to study how each proposed technique contributes to ActionPiece.

(1) We increase the vocabulary size of TIGER, to determine whether the performance gain of ActionPiece is solely due to scaling up the number of tokens in the vocabulary. By increasing the number of semantic ID digits per item (4 → 6) and the number of candidate semantic IDs per digit (2 8 → 2 13 or 2 14 ), we create two variants with vocabularies larger than ActionPiece. However, these TIGER variants perform worse than ActionPiece, and even the original TIGER with only 1024 tokens. The experimental results suggest that scaling up the vocabulary size for generative recommendation models is challenging, consistent with the observations from [Zhang et al. (2024)](#b57).

Token Utilization Rate (%) 95.33% 87.01% 56.89% SPR (#epochs=5) SPR (#epochs=1) Naive Figure 5. Analysis of token utilization rate (%) during model training w.r.t. segmentation strategy.

(2) To evaluate the effectiveness of the proposed vocabulary construction techniques, we introduce the following variants: (2.1) w/o tokenization, which skips vocabulary construction, using item features directly as tokens; (2.2) w/o context-aware, which only considers co-occurrences and merges tokens within each action during vocabulary construction and segmentation; and (2.3) w/o weighted counting, which treats all token pairs equally rather than using the weights defined in Equations ( [1](#)) and (2). The results indicate that removing any of these techniques reduces performance, demonstrating the importance of these methods for building a context-aware tokenizer.

(3) To evaluate the effectiveness of SPR, we revert to naive segmentation, as described in Section 3.2.2, during model training and inference, respectively. The results show that replacing SPR with naive segmentation in either training or inference degrades performance.

## Further Analysis

## PERFORMANCE AND EFFICIENCY W.R.T. VOCABULARY SIZE

Vocabulary size is a key hyperparameter for language tokenizers [(Meta AI, 2024;](#b27)[Dagan et al., 2024)](#b3). In this study, we investigate how adjusting vocabulary size affects the generative recommendation models. We use the normalized sequence length (NSL) [(Dagan et al., 2024)](#b3) to measure the length of tokenized sequences, where a smaller NSL indicates fewer tokens per tokenized sequence. We experiment with vocabulary sizes in {N/A, 5k, 10k, 20k, 30k, 40k}, where "N/A" represents the direct use of item features as tokens. As shown in Figure [4](#), increasing the vocabulary size improves recommendation performance and reduces

N/A 1 3 5 7 0.025 0.026 NDCG@10 Sports N/A 1 3 5 7 Model Inference Ensemble Times (q) 0.040 0.042 Beauty N/A 1 3 5 7 0.043 0.044 0.045 CDs Figure 6. Analysis of performance (NDCG@10, ↑) w.r.t. the number of ensembled segments q during model inference. the tokenized sequence length. Conversely, reducing the vocabulary size lowers the number of model parameters, improving memory efficiency. This analysis demonstrates that adjusting vocabulary size enables a trade-off between model performance, sequence length, and memory efficiency. 4.4.2. TOKEN UTILIZATION RATE W.R.T. SEGMENTATION STRATEGY As described in Section 3.3.1, applying SPR augments the training corpus by producing multiple token sequences that share the same semantics. In Table 3, we observe that incorporating SPR significantly improves recommendation performance. One possible reason is that SPR increases token utilization rates. To validate this assumption, we segment the action sequences in each training epoch using two strategies: naive segmentation and SPR. As shown in Figure 5, naive segmentation uses only 56.89% of tokens for model training, limiting the model's ability to generalize to unseen action sequences. In contrast, SPR achieves a token utilization rate of 87.01% after the first training epoch, with further increases as training progresses. These results demonstrate that the proposed SPR segmentation strategy improves the utilization of ActionPiece tokens, enabling better generalization and enhanced performance.

## PERFORMANCE W.R.T. INFERENCE-TIME ENSEMBLES

As described in Section 3.3.2, ActionPiece supports inference-time ensembling by using SPR segmentation. We vary the number of ensembled segments, q, in {N/A, 1, 3, 5, 7}, where "N/A" indicates using naive segmentation during model inference. As shown in Figure [6](#), ensembling more tokenized sequences improves ActionPiece's recommendation performance. However, the performance gains slow down as q increases to 5 and 7. Since a higher q also increases the computational cost of inference, this creates a trade-off between performance and computational budget in practice.

## Case Study

To understand how GR models benefit from the unordered feature setting and context-aware action sequence tokenization, we present an illustrative example in Figure [1](#fig_0).

Each item in the action sequence is represented as a feature set, with each item consisting of five features. The features within an item do not require a specific order. The first step of tokenization leverages the unordered nature of the feature set and applies set permutation regularization (Section 3.2.2). This process arranges each feature set into a specific permutation and iteratively groups features based on the constructed vocabulary (Section 3.2.1). This results in different segments that convey the same semantics. Each segment is represented as a sequence of sets, where each set corresponds to a token in the vocabulary.

By examining the segments and their corresponding token sequences, we identify four types of tokens, as annotated in Figure [1:](#fig_0) (1) a subset of features from a single item (token 14844 corresponds to features 747 and 923 of the T-shirt);

(2) a set containing a single feature (feature 76 of the socks);

(3) all features of a single item (token 7995 corresponds to all features of the shorts); and (4) features from multiple items (e.g., token 8316 includes feature 923 from the Tshirt and feature 679 from the socks, while token 19895 includes feature 1100 from the socks as well as features 560 and 943 from the shorts). Notably, the fourth type of token demonstrates that the features of one action can be segmented and grouped with features from adjacent actions. This results in different tokens for the same action depending on the surrounding context, showcasing the context-aware tokenization process of ActionPiece.

## Conclusion

In this paper, we introduce ActionPiece, the first contextaware action sequence tokenizer for generative recommendation. By considering the surrounding context, the same action can be tokenized into different tokens in different sequences. We formulate generative recommendation as a task on sequences of feature sets and merge important feature patterns into tokens. During vocabulary construction, we propose assigning weights to token pairs based on their structures, such as those within a single set or across adjacent sets. To enable efficient vocabulary construction, we use double-ended linked lists to maintain the corpus and introduce intermediate nodes to store tokens that combine features across adjacent sets. Additionally, we propose set permutation regularization, which segments a single action sequence into multiple token sequences with the same semantics. These segments serve as natural augmentations for training and as ensemble instances for inference.

In the future, we plan to align user actions with other modalities by constructing instructions that combine ActionPiece tokens and other types of tokens. We also aim to extend the proposed tokenizer to other tasks that can be framed as set sequence modeling problems, including audio modeling, sequential decision-making, and time series forecasting.

Table 4. Notations and explanations.

Notation Explaination i, i 1 , i j item, item identifier, item ID t the number of actions in the input action sequence; the timestamp when the model makes a prediction i t+1 the ground-truth next item ît+1 the predicted next item S = {i 1 , i 2 , . . . , i t } the action sequence where each action is represented with the interacted item ID A, A 1 , A j a set of item features or tokens m = |A j | the number of features associated with each item f j,k the k-th feature of item i j F k the collection of all possible choices for the k-th feature The number of segmentations produced using set permutation regularization during inference

$S ′ = {A 1 , A 2 , . . . , A t }$
## Appendices

## A. Notations

We summarize the notations used in this paper in Table [4](#).

## B. Algorithmic Details

In this section, we provide detailed algorithms for vocabulary construction and segmentation.

## B.1. Vocabulary Construction Algorithm

The overall procedure for vocabulary construction is illustrated in Algorithm 1. As described in Section 3.2.1, this process involves iterative Count (Algorithm 2) and Update (Algorithm 3) operations.

## B.2. Segmentation with Set Permutation Regularization Algorithm

The detailed algorithm for segmenting action sequences into token sequences using set permutation regularization (SPR) is shown in Algorithm 4. In practice, we often run Algorithm 4 multiple times to augment the training corpus or ensemble recommendation outputs, as described in Sections 3.3.1 and 3.3.2.

## C. Efficient Vocabulary Construction Implementation

To efficiently construct the ActionPiece vocabulary, we propose using data structures such as heaps with a lazy update trick, linked lists, and inverted indices to speed up each iteration of the construction process. The key idea is to avoid recalculating token co-occurrences in every iteration and instead update the data structures. The pseudocode is shown in Figure [7](#).

Algorithm 2 ActionPiece Vocabulary Construction -Count (Figure [2](#fig_1)) input Action sequence corpus S ′ , current vocabulary V output Accumulated weighted token co-occurrences count(•, •)

$1: for i ← 0 to |V|, j ← 0 to |V| do 2:$count(c i , c j ) ← 0 3: end for 4: for all sequence S ′ ∈ S ′ do 5:

t ← length(S ′ ) # number of action nodes in sequence 6:

$for k ← 0 to t -1 do 7: A k ← S ′ [k] # current action node 8:$# Process all unordered token pairs within A k 9:

$for all c i , c j ∈ A k , i ̸ = j do 10: count(c i , c j ) ← count(c i , c j ) + 2/|A k | # weight of tokens within a single set (Equation (1)) 11: count(c j , c i ) ← count(c j , c i ) + 2/|A k | # symmetric update 12:$end for # Process all ordered token pairs between A k and A k+1 13: end for 20: end for return count(•, •)

$if k < t -1 then 14: A k+1 ← S ′ [k + 1] 15: for all c i ∈ A k , c j ∈ A k+1 do 16: count(c i , c j ) ← count(c i , c j ) + 1/(|A k | × |A k+1 |) #$
## C.1. Data Structures

The data structures used in the proposed algorithm are carefully designed to optimize the efficiency of vocabulary construction.

Here is a detailed discussion of their roles and implementations:

• Linked list: Each action sequence in the training corpus is stored as a linked list. This allows efficient local updates during token merging. When a token pair (c u , c v ) is replaced by a new token c new , only the affected nodes and their neighbors in the linked list need to be modified (as shown in Algorithm 3 and Figure [3](#fig_2)).

• Heap with lazy update trick: A max-heap prioritizes token pairs by their co-occurrences. Instead of recalculating the heap entirely in each iteration, a "lazy update" strategy is employed: outdated entries (with mismatched co-occurrence counts) are retained but skipped during extraction. In the pseudocode, the loop checks if the top element is outdated via is outdated. Invalid entries are discarded, and only valid ones are processed. Updated co-occurrences are pushed as new entries (with negative counts for max-heap emulation).

• Inverted indices: The pair2head dictionary maps token pairs to the sequences containing them. When a pair (c u , c v ) is merged, the algorithm directly retrieves affected sequence IDs via pair2head[(c u, c v)], avoiding a full corpus scan. After merging, the inverted indices are incrementally updated: new token pairs (e.g., (c prev , c new ) and (c new , c next )) are added to pair2head, while obsolete pairs are removed. This enables targeted updates and ensures subsequent iterations efficiently access relevant sequences.

These structures collectively reduce time complexity by focusing computation on dynamically changing parts of the corpus and avoiding redundant global operations. The linked list enables localized edits, the heap minimizes priority recalculation, and the inverted indices eliminate brute-force searches, making the algorithm scalable to large corpora.

## C.2. Time Complexity

The time complexity of the efficient vocabulary construction algorithm can be analyzed through two main components: initialization and iterative merging.

Algorithm 3 ActionPiece Vocabulary Construction -Update (Figure [3](#fig_2)) input Action sequence corpus S ′ before updating, current merge rule {(c u , c v ) → c new } output Updated action sequence corpus S ′ 1: for all sequence S ′ ∈ S ′ do 2:

t ← length(S ′ )

3:

for k ← 0 to t -1 do 4: # Merge tokens from two adjacent nodes 10:

$A k ← S ′ [k]$if k < t -1 then 11:

$A k+1 ← S ′ [k + 1] 12: if c u ∈ A k and c v ∈ A k+1 then 13: if A k , A$k+1 are both action nodes then 14: Create intermediate node M between A k and A k+1 15:

$M ← {c new } # linked list: A k → M → A k+1 16: A k ← A k \ c u 17: A k+1 ← A k+1 \ c v 18: else if A k is intermediate node then 19: A k ← {c new } 20: A k+1 ← A k+1 \ c v 21: else if A k+1 is intermediate node then 22: A k ← A k \ c u 23: A k+1 ← {c new } 24:$end if

## 25:

end if

## 26:

end if

## 27:

end for 28: end for return S ′

• Initialization phase involves building the initial max-heap to track co-occurrence frequencies. Given N input sequences (each with an average length of L), we count co-occurrences for all O(m 2 ) token pairs within each set of size m. This requires O(N Lm 2 ) time.

• Iterative merging phase dynamically processes the involved sequences. The total number of such sequences across all iterations is approximately

$O N |V 0 | + O N |V 0 | + 1 + • • • + O N Q ≃ O(log QN ).$For each sequence, updating the linked list requires O(Lm) time, counting co-occurrences takes O(Lm 2 ) time, and inserting co-occurrences into the max-heap requires at most O(Lm 2 log H) time. Here, H represents the heap size, which is at most O(N Lm). Thus, the overall time complexity for iterative merging is

$O(log QN (Lm + Lm 2 + Lm 2 log H)) = O(log Q log H • N Lm 2 ).$Therefore, the overall time complexity of our proposed vocabulary construction algorithm is

$O(log Q log H • N Lm 2 ),$where the iterative merging phase dominates. This complexity is significantly better than the naive vocabulary construction complexity of O(QN Lm 2 ).

Algorithm 4 Segmentation via Set Permutation Regularization (SPR) (Section 3.2.2) input Action sequence S, merge rules R output Segmented token sequences C 1: C ← [ ] # initialize permuted initial token sequence 2: for all token set A i ∈ S do 3: Generate random permutation of A i as [c 1 , c 2 , . . . , c |Ai| ] 4: Extend C with [c 1 , c 2 , . . . , c |Ai| ] # concatenate permutations 5: end for 6: 7: # Apply BPE (Sennrich et al., 2016) segmentation on permuted sequence 8: repeat 9: R ′ ← ∅ # candidate merge rules 10: While ActionPiece follows a similar algorithmic framework as BPE, its design is fundamentally different because it is tailored for tokenizing action sequences. To clarify, we summarize the key differences in Table [5](#tab_8).

$for i ← 0 to |C| -1 do 11: if {(c i , c i+1 ) → c ′ } ∈ R then 12: R ′ ← R ′ ∪ {(c i , c i+1 ) → c ′ } 13: end if 14: end for 15: Select {(c k , c k+1 ) → c ′ } ∈ R ′ with the smallest index among all merge rules R 16: C ← [c 1 , . . . , c k-1 , c ′ , c k+2 , . . . ] # replace (c k , c k+1 ) with a new token c ′ 17: until R ′ is ∅ return C$
## E. Datasets

Categories. Among all the datasets, "Sports" and "Beauty" are two widely used benchmarks for evaluating generative recommendation models [(Rajput et al., 2023;](#b35)[Jin et al., 2024;](#b15)[Hua et al., 2023)](#b14). We conduct experiments on these benchmarks to ensure fair comparisons with existing results. Additionally, we introduce "CDs", which contains about 4× more interactions than "Sports", making it a larger dataset for evaluating the scalability of GR models. For "CDs", we apply the same data processing strategy as the public benchmarks. The statistics of the processed datasets are shown in Table [6](#tab_9).

Sequence truncation length. Following Rajput et al. (2023), we filter out users with fewer than 5 reviews and truncate 1 def vocab_construction_iteration(max_heap, vocab, rules, pair2head): 2 """Performs one iteration of efficient vocabulary construction. 3 4 Args: 5 max_heap (PriorityQueue): Max-heap storing (co_occurrence, (c_u, c_v)) pairs. 6 vocab (List[Tuple]): Current vocabulary with merge rules. 7 rules (Dict): Merge rule {(c_u, c_v): c_new} mapping. 8 pair2head (Dict): Inverted indices that store mappings from the token pair to all sequences that contain this token pair. 9 """ 10 # Get most frequent valid pair (c_u, c_v) from max-heap 11 # Efficient version of "Count" in Algorithm 1 12 # Avoid recalculating co-occurrences for each iteration, by maintaining them in a max-heap with the lazy update trick 13 while not max_heap.empty(): 14 co_occurrence, (c_u, c_v) = max_heap.get() 15 if not is_outdated((c_u, c_v), co_occurrence): # outdated values are lazily removed. 16 break 17 18 # Create new token and update vocabulary 19 c_new = len(vocab) 20 vocab.append((c_u, c_v)) 21 rules[(c_u, c_v)] = c_new 22 23 # Update sequences containing (c_u, c_v) 24 seq_ids = pair2head[(c_u, c_v)].copy() # IDs of affected sequences 25 delta_counts = defaultdict(int) 26 27 for sid in seq_ids: 28 # Merge all (c_u, c_v) pairs in sequence 29 new_seq = merge_sequence(seqs[sid], c_u, c_v, c_new) # replace pairs 30 seqs[sid] = new_seq 31 32 # Calculate pair co-occurrence changes 33 new_freqs = count_pairs(new_seq) # get token co-occurrences of the updated sequence 34 delta = diff_counts(new_freqs, old_freqs[sid]) # compute co-occurrence differences 35 update_index(pair2head, delta, sid) # update inverted index 36 old_counts[sid] = new_freqs 37 38 # Accumulate global co-occurrence changes 39 for p, cnt in delta.items(): 40 delta_counts[p] += cnt 41 42 # Lazy update max-heap with new co-occurrences 43 for (c_u, c_v), delta in delta_counts.items(): 44 if abs(delta) < eps: continue # eps: minimum update threshold 45 all_pair_freqs[(c_u, c_v)] += delta # Global co-occurrences 46 max_heap.put( (-all_pair_freqs[(c_u, c_v)], (c_u, c_v)) ) Figure 7. Pseudocode for a single iteration of the efficient vocabulary construction algorithm, illustrating how a max-heap with lazy updates is used to track and merge frequent token pairs.

action sequences to a maximum length of 20 for "Generative" methods, including ActionPiece. For "ID-based" and "Feature + ID" baselines, we set the maximum length to 50, as suggested in their original papers.

Item text features. Following [Rajput et al. (2023)](#b35); [Zheng et al. (2024a)](#); [Sheng et al. (2025)](#b38), the first step for feature engineering is to combine multiple raw text features into a single sentence for each item. Then, we use a pretrained sentence embedding model to encode this sentence into a vector representation. In all our implementations, we concatenate title, price, brand, feature, categories, and description, and use sentence-t5-base [(Ni et al., 2022)](#b28) as the sentence embedding model.

• The encoded sentence embeddings of 768 dimension are directly used as textual item representations for UniSRec.

• We quantize the sentence embeddings using residual quantization (RQ) [(Rajput et al., 2023;](#b35)[Zeghidour et al., 2021;](#b53)[Zheng et al., 2024b)](#) into three codes, each with 256 candidates. To prevent conflicts, we add an extra identification code. These four codes together serve as the RQ-based semantic IDs for TIGER and SPM-SID.

• For other baselines that require item features, such as FDSA, S 3 -Rec, VQ-Rec, HSTU, and our method, we follow [Hou et al. (2023)](#b12) and quantize the sentence embeddings using optimized product quantization (OPQ) [(Ge et al., 2013)](#b7). Except for VQ-Rec, where the sentence embeddings are quantized into 32 codes as suggested in the original paper, we quantize the sentence embeddings into 4 codes for all other methods to ensure a fair comparison. The codebook size is 256 for each digit of code. For generative methods HSTU and ActionPiece, we also include an additional identification code to prevent conflicts. Note that, unlike RQ-based semantic IDs, features produced by product/vector quantization do not require a specific order.

## F. Baselines

We compare ActionPiece with the following representative baselines:

F.1. ID-Based Sequential Recommendation Methods

• SASRec [(Kang & McAuley, 2018)](#b17) represents each item using its unique item ID. It encodes item ID sequences with a self-attentive Transformer decoder. The model is trained by optimizing a binary cross-entropy objective.

• BERT4Rec [(Sun et al., 2019](#b41)) also represents each item using its unique item ID. Unlike SASRec, it encodes sequences of item IDs with a bidirectional Transformer encoder. The model is trained using a masked prediction objective.

F.2. Feature-Enhanced Sequential Recommendation Methods

• FDSA [(Zhang et al., 2019)](#b56) integrates item feature embeddings with vanilla attention layers to obtain feature representations. It then processes item ID sequences and feature sequences separately through self-attention blocks.

• S 3 -Rec [(Zhou et al., 2020)](#b63) first employs self-supervised pre-training to capture the correlations between item features and item IDs. Then the checkpoints are loaded and fine-tuned for next-item prediction, using only item IDs.

• VQ-Rec [(Hou et al., 2023)](#b12) encodes text features into dense vectors using pre-trained language models. It then applies product quantization to convert these dense vectors into semantic IDs. The semantic ID embeddings are pooled together to represent each item. Since the experiments are not performed in a transfer learning setting, we omit the two-stage training strategy outlined in the original paper. Instead, we reuse the model architecture and train it from scratch using an in-batch contrastive loss with a batch size of 256.

## F.3. Generative Recommendation Methods

Each generative recommendation baseline corresponds to an action tokenization method described in Table [1](#tab_0).

• P5-CID [(Hua et al., 2023)](#b14) is an extension of P5 [(Geng et al., 2022)](#b8), which formulates recommendation tasks in a text-to-text format. Building on P5, the authors explored several tokenization methods to index items for better recommendations. In this study, we use P5-CID as a representative hierarchical clustering-based action tokenization method. It organizes the eigenvectors of the Laplacian matrix of user-item interactions into a hierarchy and assigns cluster IDs at each level as item indices. When implementing this baseline method, we adopt the same model backbone as ActionPiece (encoder-decoder Transformers trained from scratch) and use the indices produced by P5-CID.

• TIGER [(Rajput et al., 2023)](#b35) encodes text features similarly to VQ-Rec but quantizes them into semantic IDs using RQ-VAE. The model is then trained to autoregressively predict the next semantic ID and employs beam search for inference. We use a beam size of 50 in beam search to generate the top-K recommendations.

• LMIndexer [(Jin et al., 2024)](#b15) takes text as input and predicts semantic IDs. The text description of each item is first tokenized using a text tokenizer. The resulting text tokens are then concatenated to form input action sequences. The selected from {1 × 10 -3 , 3 × 10 -3 , 5 × 10 -3 } with a warmup step of 10,000. We use a dropout rate of 0.1 and tune the weight decay from {0.07, 0.1, 0.15, 0.2}. For all methods implemented by us, we conduct five repeated experiments using random seeds {2024, 2025, 2026, 2027, 2028}. The model checkpoints with the best average NDCG@10 on the validation set are selected for evaluation on the test set, and we report these results. Each model is trained on a single 40G NVIDIA A100 GPU.

## H. Reproduction

To improve reproducibility, we provide the algorithms of vocabulary construction and segmentation processes in Algorithms 1 and 2 to 4. We also provide the pseudocode for the efficient vocabulary construction implementation in Figure [7](#). In addition, we provide the best hyperparameters of ActionPiece for all experimental datasets in Table [7](#).

![Figure 1. Illustration of the tokenization process of ActionPiece. Each action is represented as an unordered feature set. This figure presents two possible tokenized sequences. The same action can be tokenized into different tokens depending on the surrounding context. A detailed case study can be found in Section 4.5.]()

![Figure2. Illustration of how weights of co-occurring token pairs are counted during vocabulary construction. In this example, two adjacent sets in the sequence are considered: one with 4 tokens (represented as ⃝) and another with 3 tokens (represented as □). Token pairs are counted within a single set (< ⃝, ⃝ > and < □, □ >) and across the two adjacent sets (< ⃝, □ >).]()

![Figure 3. Illustration of how the linked list, which maintains the action sequence, is updated when merging two tokens into a new token. Three cases are considered: (1) both tokens are in the same action node; (2) the tokens are in two adjacent action nodes; (3) one token is in an action node, while the other is in an intermediate node.]()

![the action sequence where each action is represented with a set of item features c, c 1 , c j input & generated tokens l the number of tokens in the token sequence C = {c 1 , c 2 , . . . , c l } the token sequence tokenized from the input action sequence S ′ {c l+1 , . . . , c q } the tokens generated by the GR model V vocabulary of ActionPiece tokenizer R merge rules of ActionPiece tokenizer {(c u , c v ) → c new } one merge rule indicating two adjacent tokens c u and c v can be replaced by a token c new Q = |V| size of ActionPiece vocabulary P (c, c ′ ) probability that tokens c and c ′ are adjacent when flattening a sequence of sets into a token sequence N the number of action sequences in the training corpus L the average length of action sequences in the training corpus H Maximal heap size, O(N Lm) q]()

![weight of tokens from two adjacent sets (Equation (2]()

![if c u ∈ A k and c v ∈ A k then 7:Replace c u and c v in A k with c]()

![Comparison of different action tokenization methods for generative recommendation. "Contextual" denotes whether the same actions can be tokenized into different tokens based on the surrounding context. "Unordered" denotes whether the item features or semantic IDs are used in an order-agnostic manner.]()

![Analysis of recommendation performance (NDCG@10, ↑) and average tokenized sequence length (NSL, ↓) w.r.t. vocabulary size across three datasets. "N/A" indicates that ActionPiece is not applied, i.e., action sequences are represented solely by initial tokens.Ablation analysis of ActionPiece. The recommendation performance is measured using NDCG@10. The best performance is denoted in bold fonts.]()

![Comparison between ActionPiece and BPE.]()

![Statistics of the processed datasets. "Avg. t" denotes the average number of actions in an action sequence.]()

