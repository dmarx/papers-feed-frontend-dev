<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">xLSTM: Extended Long Short-Term Memory</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-12-06">6 Dec 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Maximilian</forename><surname>Beck</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">LIT AI Lab</orgName>
								<orgName type="department" key="dep2">Institute for Machine Learning</orgName>
								<orgName type="institution" key="instit1">ELLIS Unit</orgName>
								<orgName type="institution" key="instit2">JKU Linz</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Korbinian</forename><surname>PÃ¶ppel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">LIT AI Lab</orgName>
								<orgName type="department" key="dep2">Institute for Machine Learning</orgName>
								<orgName type="institution" key="instit1">ELLIS Unit</orgName>
								<orgName type="institution" key="instit2">JKU Linz</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">NXAI Lab</orgName>
								<address>
									<settlement>Linz</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">NXAI GmbH</orgName>
								<address>
									<settlement>Linz</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Markus</forename><surname>Spanring</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">LIT AI Lab</orgName>
								<orgName type="department" key="dep2">Institute for Machine Learning</orgName>
								<orgName type="institution" key="instit1">ELLIS Unit</orgName>
								<orgName type="institution" key="instit2">JKU Linz</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andreas</forename><surname>Auer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">LIT AI Lab</orgName>
								<orgName type="department" key="dep2">Institute for Machine Learning</orgName>
								<orgName type="institution" key="instit1">ELLIS Unit</orgName>
								<orgName type="institution" key="instit2">JKU Linz</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">NXAI Lab</orgName>
								<address>
									<settlement>Linz</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Oleksandra</forename><surname>Prudnikova</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">LIT AI Lab</orgName>
								<orgName type="department" key="dep2">Institute for Machine Learning</orgName>
								<orgName type="institution" key="instit1">ELLIS Unit</orgName>
								<orgName type="institution" key="instit2">JKU Linz</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">NXAI Lab</orgName>
								<address>
									<settlement>Linz</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">NXAI GmbH</orgName>
								<address>
									<settlement>Linz</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Kopp GÃ¼nter Klambauer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">LIT AI Lab</orgName>
								<orgName type="department" key="dep2">Institute for Machine Learning</orgName>
								<orgName type="institution" key="instit1">ELLIS Unit</orgName>
								<orgName type="institution" key="instit2">JKU Linz</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">NXAI Lab</orgName>
								<address>
									<settlement>Linz</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">NXAI GmbH</orgName>
								<address>
									<settlement>Linz</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Johannes</forename><surname>Brandstetter</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">LIT AI Lab</orgName>
								<orgName type="department" key="dep2">Institute for Machine Learning</orgName>
								<orgName type="institution" key="instit1">ELLIS Unit</orgName>
								<orgName type="institution" key="instit2">JKU Linz</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">NXAI Lab</orgName>
								<address>
									<settlement>Linz</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">NXAI GmbH</orgName>
								<address>
									<settlement>Linz</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">LIT AI Lab</orgName>
								<orgName type="department" key="dep2">Institute for Machine Learning</orgName>
								<orgName type="institution" key="instit1">ELLIS Unit</orgName>
								<orgName type="institution" key="instit2">JKU Linz</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">NXAI Lab</orgName>
								<address>
									<settlement>Linz</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">NXAI GmbH</orgName>
								<address>
									<settlement>Linz</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">xLSTM: Extended Long Short-Term Memory</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-12-06">6 Dec 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">1FEBEA53C26E25E50B02318365F75C84</idno>
					<idno type="arXiv">arXiv:2405.04517v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a matrix memory and a covariance update rule. Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Memory Cells</head><p>ðŸ † Constant Error Carousel ðŸ † Sigmoid Gating ðŸ † Recurrent Inference ðŸ † Recurrent Training sLSTM + New Memory Mixing Memory Cells xLSTM Blocks xLSTM mLSTM + Exponential Gating + Parallel Training + Covariance Update Rule + Matrix Memory LSTM + Exponential Gating Figure 1: The extended LSTM (xLSTM) family. From left to right: 1. The original LSTM memory cell with constant error carousel and gating. 2. New sLSTM and mLSTM memory cells that introduce exponential gating. sLSTM offers a new memory mixing technique. mLSTM is fully parallelizable with a novel matrix memory cell state and new covariance update rule. 3. mLSTM and sLSTM in residual blocks yield xLSTM blocks. 4. Stacked xLSTM blocks give an xLSTM architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Long Short-Term Memory (LSTM) ideas <ref type="bibr" target="#b33">(Hochreiter, 1991;</ref><ref type="bibr">Hochreiter &amp; Schmidhuber, 1997b,a)</ref>, i.e., the constant error carousel and gating, were introduced to overcome the vanishing gradient problem of recurrent neural networks <ref type="bibr" target="#b33">(Hochreiter, 1991;</ref><ref type="bibr" target="#b36">Hochreiter et al., 2000)</ref>:</p><formula xml:id="formula_0">c t = f t c t-1 + i t z t , h t = o t Ïˆ( c t ) .<label>(1)</label></formula><p>The constant error carousel is the additive update of the cell state c t-1 (green) by cell inputs z t and moderated by sigmoid gates (blue). The input gate i t and the forget gate f t control this update, while the output gate o t controls the output of the memory cell, i.e. the hidden state h t . The cell state is normalized or squashed by Ïˆ and then output gating gives the hidden state.</p><p>LSTMs have been successfully applied to various domains <ref type="bibr" target="#b37">(Hochreiter et al., 2001</ref><ref type="bibr" target="#b38">(Hochreiter et al., , 2007;;</ref><ref type="bibr" target="#b94">Schmidhuber, 2015)</ref>, and prevailed over text generation until the dawn of Transformers in 2017 <ref type="bibr" target="#b113">(Vaswani et al., 2017)</ref>. The effectiveness of LSTMs has been demonstrated at numerous sequence-related tasks such as generating text <ref type="bibr" target="#b26">(Graves, 2013;</ref><ref type="bibr" target="#b42">Karpathy, 2015)</ref>, generating handwritings <ref type="bibr" target="#b26">(Graves, 2013)</ref>, sequence-to-sequence translation <ref type="bibr" target="#b106">(Sutskever et al., 2014)</ref>, evaluating computer programs <ref type="bibr" target="#b123">(Zaremba &amp; Sutskever, 2014)</ref>, generating image captions <ref type="bibr" target="#b44">(Karpathy &amp; Fei-Fei, 2015;</ref><ref type="bibr" target="#b40">Hossain et al., 2019)</ref>, generating source code <ref type="bibr" target="#b42">(Karpathy, 2015)</ref>, rainfall-runoff modeling <ref type="bibr" target="#b49">(Kratzert et al., 2018</ref><ref type="bibr" target="#b50">(Kratzert et al., , 2019))</ref>, or hydrological models for flooding warnings <ref type="bibr" target="#b69">(Nearing et al., 2024)</ref>. In reinforcement learning, LSTMs are the best performing sequence models, e.g., the AlphaStar model for StarCraft II <ref type="bibr" target="#b114">(Vinyals et al., 2017)</ref>, the OpenAI Five model for Dota 2 <ref type="bibr" target="#b43">(Karpathy, 2019)</ref>, and models of the magnetic controller for nuclear fusion <ref type="bibr" target="#b19">(Degrave et al., 2022)</ref>. LSTMs excel at learning abstractions, i.e., adeptly extracting semantic information and storing it in their memory cells <ref type="bibr" target="#b42">(Karpathy, 2015)</ref>, which for example became evident by number and syntax neurons <ref type="bibr" target="#b54">(Lakretz et al., 2019)</ref>, linguistic neurons <ref type="bibr" target="#b6">(Bau et al., 2019)</ref>, and sentiment neurons <ref type="bibr" target="#b82">(Radford et al., 2017)</ref>. LSTMs are still used in highly relevant applications <ref type="bibr" target="#b19">(Degrave et al., 2022;</ref><ref type="bibr" target="#b69">Nearing et al., 2024)</ref> and have stood the test of time.</p><p>Figure <ref type="figure" target="#fig_12">2</ref>: LSTM limitations. Left: Nearest Neighbor Search problem in terms of mean squared error (MSE). Given a reference vector, a sequence is scanned sequentially for the most similar vector with the objective to return its attached value at sequence end. LSTM struggles to revise a stored value when a more similar vector is found. Our new xLSTM overcomes this limitation by exponential gating. Right: Rare Token Prediction. The perplexity (PPL) of token prediction on Wikitext-103, in partitions of token frequency. LSTM performs worse on predicting rare tokens because of its limited storage capacities, whereas our new xLSTM solves this problem via a matrix memory.</p><p>Despite their tremendous successes, LSTMs have three main limitations: (i) Inability to revise storage decisions. We exemplify this limitation via the Nearest Neighbor Search problem (see also Appendix B): With a reference vector given, a sequence must be scanned sequentially for the most similar vector in order to provide its attached value at sequence end. The left panel of Figure <ref type="figure" target="#fig_12">2</ref> shows the mean squared error at this task. LSTM struggles to revise a stored value when a more similar vector is found, while our new xLSTM remediates this limitation by exponential gating. (ii) Limited storage capacities, i.e., information must be compressed into scalar cell states. We exemplify this limitation via Rare Token Prediction. In the right panel of Figure <ref type="figure" target="#fig_12">2</ref>, the perplexity of token prediction on Wikitext-103 <ref type="bibr" target="#b64">(Merity et al., 2017)</ref> is given for partitions of different token frequency. LSTM performs worse on rare tokens because of its limited storage capacities. Our new xLSTM solves this problem by a matrix memory. (iii) Lack of parallelizability due to memory mixing, i.e., the hidden-hidden connections between hidden states from one time step to the next, which enforce sequential processing.</p><p>These limitations of LSTM have paved the way for the emergence of Transformers <ref type="bibr" target="#b113">(Vaswani et al., 2017)</ref> in language modeling. What performances can we achieve in language modeling when overcoming these limitations and scaling LSTMs to the size of current Large Language Models?</p><p>2 Extended Long Short-Term Memory</p><p>To overcome the LSTM limitations, Extended Long Short-Term Memory (xLSTM) introduces two main modifications to the LSTM idea of Equation ( <ref type="formula" target="#formula_0">1</ref>). Those modifications -exponential gating and novel memory structures -enrich the LSTM family by two members: (i) the new sLSTM (see Section 2.2) with a scalar memory, a scalar update, and memory mixing, and (ii) the new mLSTM (see Section 2.3) with a matrix memory and a covariance (outer product) update rule, which is fully parallelizable. Both sLSTM and mLSTM enhance the LSTM through exponential gating. To enable parallelization, the mLSTM abandons memory mixing, i.e., the hidden-hidden recurrent connections. Both mLSTM and sLSTM can be extended to multiple memory cells, where sLSTM features memory mixing across cells. Further, the sLSTM can have multiple heads without memory mixing across the heads, but only memory mixing across cells within each head. This introduction of heads for sLSTM together with exponential gating establishes a new way of memory mixing. For mLSTM multiple heads and multiple cells are equivalent.</p><p>Integrating these new LSTM variants into residual block modules results in xLSTM blocks (see Section 2.4). Residually stacking those xLSTM blocks in architectures provides xLSTM architectures (see Section 2.4). See Figure <ref type="figure" target="#fig_5">1</ref> for the xLSTM architecture with its components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Review of the Long Short-Term Memory</head><p>The original LSTM idea <ref type="bibr" target="#b33">(Hochreiter, 1991;</ref><ref type="bibr">Hochreiter &amp; Schmidhuber, 1997b,a)</ref> introduced the scalar memory cell as a central processing and storage unit that avoids vanishing gradients <ref type="bibr" target="#b33">(Hochreiter, 1991;</ref><ref type="bibr" target="#b36">Hochreiter et al., 2000)</ref> through the constant error carousel (the cell state update). The memory cell contains three gates: input, output, and forget gate. The forget gate has been introduced by <ref type="bibr" target="#b24">Gers et al. (2000)</ref>. The update rules of the LSTM memory cell at time step t are:</p><formula xml:id="formula_1">c t = f t c t-1 + i t z t cell state (2)</formula><p>h t = o t ht , ht = Ïˆ c t hidden state (3)</p><formula xml:id="formula_2">z t = Ï† (z t ) , zt = w âŠ¤ z x t + r z h t-1 + b z cell input (4) i t = Ïƒ Ä©t , Ä©t = w âŠ¤ i x t + r i h t-1 + b i input gate (5) f t = Ïƒ ft , ft = w âŠ¤ f x t + r f h t-1 + b f forget gate (6) o t = Ïƒ (Ãµ t ) , Ãµt = w âŠ¤ o x t + r o h t-1 + b o output gate (7)</formula><p>The weight vectors w z , w i , w f , and w o correspond to the input weight vectors between inputs x t and cell input, input gate, forget gate, and output gate, respectively. The weights r z , r i , r f , and r o correspond to the recurrent weights between hidden state h t-1 and cell input, input gate, forget gate, and output gate, respectively. b z , b i , b f , and b o are the corresponding bias terms. Ï† and Ïˆ are the cell input and hidden state activation functions (typically tanh). Ïˆ is used to normalize or squash the cell state, which would be unbounded otherwise. All gate activation functions are sigmoid, i.e., Ïƒ (x) = 1/(1 + exp(-x)). In later formulations, multiple scalar memory cells c t âˆˆ R d were combined in a vector, which allows the usage of recurrent weight matrices R âˆˆ R dÃ—d to mix the cell outputs of memory cells <ref type="bibr" target="#b28">(Greff et al., 2015)</ref>, for more details see Appendix A.1. Ablation studies showed that all components of the memory cell are crucial <ref type="bibr" target="#b28">(Greff et al., 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">sLSTM</head><p>To empower LSTMs with the ability to revise storage decisions, we introduce exponential gates (red) together with normalization and stabilization. In particular, input and forget gates can have exponential activation functions. For normalization, we introduce a normalizer state that sums up the product of the input gate times all future forget gates.</p><p>The scalar sLSTM forward pass is:</p><formula xml:id="formula_3">c t = f t c t-1 + i t z t cell state<label>(8)</label></formula><p>n t = f t n t-1 + i t normalizer state <ref type="bibr" target="#b131">(9)</ref> h t = o t ht , ht = c t / n t hidden state (10)</p><formula xml:id="formula_4">z t = Ï† (z t ) , zt = w âŠ¤ z x t + r z h t-1 + b z cell input (11) i t = exp Ä©t , Ä©t = w âŠ¤ i x t + r i h t-1 + b i input gate (12) f t = Ïƒ ft OR exp ft , ft = w âŠ¤ f x t + r f h t-1 + b f forget gate (13) o t = Ïƒ (Ãµ t ) , Ãµt = w âŠ¤ o x t + r o h t-1 + b o output gate (14)</formula><p>We transfer the original LSTM gating techniques, i.e., input-and/or hidden-dependent gating plus bias term, to the new architectures. Exponential activation functions can lead to large values that cause overflows. Therefore, we stabilize gates with an additional state m t <ref type="bibr" target="#b67">(Milakov &amp; Gimelshein, 2018)</ref>:</p><formula xml:id="formula_5">m t =</formula><p>max log( f t ) + m t-1 , log( i t ) stabilizer state (15) i â€² t = exp log i t -m t = exp Ä©t -m t stabil. input gate (16)</p><formula xml:id="formula_6">f â€² t = exp log f t + m t-1 -m t stabil. forget gate<label>(17)</label></formula><p>We show in Appendix A.2, that replacing f t by f â€² t and i t by i â€² t in the forward pass does neither change the output of the whole network nor the derivatives of the loss with respect to the parameters.</p><p>New Memory Mixing. sLSTM can have multiple memory cells like the original LSTM (see Appendix A.2). Multiple memory cells enable memory mixing via recurrent connections R z , R i , R f , R o from hidden state vector h to memory cell input z and the gates i, f , o, respectively. A new aspect in memory mixing is the effect of exponential gating. The new sLSTM can have multiple heads with memory mixing within each head but not across heads. The introduction of heads for sLSTM together with exponential gating establishes a new way of memory mixing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">mLSTM</head><p>To enhance storage capacities of LSTMs, we increase the LSTM memory cell from a scalar c âˆˆ R to a matrix C âˆˆ R dÃ—d . Hence, retrieval is performed via a matrix multiplication. At time t, we want to store a pair of vectors, the key k t âˆˆ R d and the value v t âˆˆ R d (we use the Transformer terminology). Later at time t + Ï„ , the value v t should be retrieved by a query vector q t+Ï„ âˆˆ R d . This is the setting of Bidirectional Associative Memories (BAMs) <ref type="bibr">(Kohonen, 1972;</ref><ref type="bibr" target="#b2">Anderson, 1972;</ref><ref type="bibr" target="#b68">Nakano, 1972;</ref><ref type="bibr" target="#b1">Anderson et al., 1977)</ref>. The covariance update rule <ref type="bibr" target="#b96">(Sejnowski, 1977;</ref><ref type="bibr" target="#b17">Dayan &amp; Willshaw, 1991)</ref> for storing a key-value pair is</p><formula xml:id="formula_7">C t = C t-1 + v t k âŠ¤ t .<label>(18)</label></formula><p>We assume a layer-norm before projecting inputs to keys and values, therefore they have zero mean.</p><p>The covariance update rule is optimal <ref type="bibr" target="#b17">(Dayan &amp; Willshaw, 1991)</ref> for a maximal separability of retrieved binary vectors, which is equivalent to a maximal signal/noise ratio. Higher separability is possible when limiting retrieval to pairwise interactions and conceding quadratic complexity like attention <ref type="bibr" target="#b52">(Krotov &amp; Hopfield, 2016</ref><ref type="bibr" target="#b128">, 2017;</ref><ref type="bibr" target="#b86">Ramsauer et al., 2021)</ref>. The covariance update rule is equivalent to Fast Weight Programmers <ref type="bibr" target="#b93">(Schmidhuber, 1992;</ref><ref type="bibr" target="#b92">Schlag et al., 2021)</ref>, which have later been equipped with a constant decay rate multiplied to C t-1 and a constant learning rate multiplied to v t k âŠ¤ t <ref type="bibr">(Ba et al., 2016a)</ref>. In this spirit, we integrate the covariance update rule into the LSTM framework, where the forget gate corresponds to decay rate and the input gate to the learning rate, while the output gate scales the retrieved vector.</p><p>For this matrix memory, the normalizer state is the weighted sum of key vectors, where each key vector is weighted by the input gate and all future forget gates. Again, the normalizer state keeps record of the strength of the gates. Since the dot product between query and normalizer state can be close to zero, we use the absolute value of this dot product and lower bound it by a threshold (typically 1.0) as done previously <ref type="bibr" target="#b103">(Sun et al., 2023)</ref>. The mLSTM forward pass is:</p><formula xml:id="formula_8">C t = f t C t-1 + i t v t k âŠ¤ t cell state (19) n t = f t n t-1 + i t k t normalizer state (20) h t = o t âŠ™ ht , ht = C t q t / max n âŠ¤ t q t , 1 hidden state (21) q t = W q x t + b q query input (22) k t = 1 âˆš d W k x t + b k key input (23) v t = W v x t + b v value input (24) i t = exp Ä©t , Ä©t = w âŠ¤ i x t + b i input gate (25) f t = Ïƒ ft OR exp ft , ft = w âŠ¤ f x t + b f forget gate (26) o t = Ïƒ (Ãµ t ) , Ãµt = W o x t + b o output gate (27)</formula><p>mLSTM can have multiple memory cells like the original LSTM. For mLSTM, multiple heads and multiple cells are equivalent as there is no memory mixing. In order to stabilize the exponential gates of mLSTM, we use the same stabilization techniques as for sLSTM (see Equation <ref type="formula">15</ref>). Since the mLSTM has no memory mixing, this recurrence can be reformulated in a parallel version. For more details we refer to Appendix A.3. xLSTM Blocks. An xLSTM block should non-linearly summarize the past in a high-dimensional space to better separate different histories or contexts. Separating histories is the prerequisite to correctly predict the next sequence element such as the next token. We resort to Cover's Theorem <ref type="bibr" target="#b14">(Cover, 1965)</ref>, which states that in a higher dimensional space non-linearly embedded patterns can more likely be linearly separated than in the original space. We consider two residual block architectures: (i) A residual block with post upprojection (like Transformers), which non-linearly summarizes the past in the original space, then linearly maps into a high-dimensional space, applies a non-linear activation function, and linearly maps back to the original space; see left panel of Figure <ref type="figure" target="#fig_0">3</ref> and third column in Figure <ref type="figure" target="#fig_5">1</ref>. A more detailed version is depicted in Figure <ref type="figure" target="#fig_5">10</ref> in the appendix. (ii) A residual block with pre up-projection (like State Space Models), which linearly maps to a high-dimensional space, non-linearly summarizes the past in the high-dimensional space and then linearly maps back to the original space. For an xLSTM block containing an sLSTM, we use the post up-projection block. For an xLSTM block containing an mLSTM, we use the pre up-projection block since the memory capacity becomes larger in the high-dimensional space. We refer to the left panel of Figure <ref type="figure" target="#fig_0">3</ref> and third column in Figure <ref type="figure" target="#fig_5">1</ref>, or Figure <ref type="figure" target="#fig_5">10</ref> in the appendix for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">xLSTM Architecture</head><p>xLSTM Architecture.</p><p>An xLSTM architecture is constructed by residually stacking building blocks <ref type="bibr" target="#b102">(Srivastava et al., 2015;</ref><ref type="bibr" target="#b32">He et al., 2016)</ref>. We rely on the most commonly used pre-LayerNorm <ref type="bibr">(Ba et al., 2016b)</ref> residual backbones as used in contemporary Large Language Models. See last column in Figure <ref type="figure" target="#fig_5">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Memory and Speed Considerations</head><p>Contrary to Transformers, xLSTM networks have a linear computation and a constant memory complexity with respect to the sequence length. Since the xLSTM memory is compressive, it is well suited for industrial applications and implementations on the edge.</p><p>The memory of mLSTM does not require parameters but is computationally expensive through its dÃ—d matrix memory and d Ã— d update. We trade off memory capacity against computational complexity. Nevertheless, the computations can be done in parallel on GPUs, therefore these computations have only a minor effect on the wall clock time.</p><p>While mLSTM is parallelizable analog to FlashAttention <ref type="bibr" target="#b16">(Dao et al., 2022;</ref><ref type="bibr" target="#b15">Dao, 2024)</ref> or GLA <ref type="bibr" target="#b121">(Yang et al., 2023)</ref>, sLSTM is not parallelizable due to the memory mixing (hidden-hidden connections). However, we developed a fast CUDA implementation with GPU memory optimizations to the register level which is typically less than two times slower than mLSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head><p>Linear Attention. Several methods have been suggested to overcome the quadratic complexity in terms of context length of the Transformer and make attention linear in the context length. The Synthesizer learns synthetic attention weights without token-token interactions <ref type="bibr" target="#b107">(Tay et al., 2020)</ref>. Linformer realizes self-attention by a low-rank matrix and even linearly approximates it <ref type="bibr" target="#b116">(Wang et al., 2020)</ref>. Linear Transformer linearizes the attention mechanism <ref type="bibr" target="#b45">(Katharopoulos et al., 2020)</ref>. Performer linearly approximates the attention softmax by positive orthogonal random features approach <ref type="bibr">(Choromanski et al., 2021)</ref>. Attention has been replaced by fast long convolutions in the Structured Global Convolution (SGConv) <ref type="bibr" target="#b55">(Li et al., 2022)</ref> and the Hyena Hierarchy <ref type="bibr" target="#b3">(Poli et al., 2023)</ref>.</p><p>State Space Models. Recently, State Space Models (SSMs) became very popular since they are linear in the context length and show promising performance compared to Transformers. One of the first proposed models was Structured State Space sequence model (S4) <ref type="bibr" target="#b30">(Gu et al., 2021)</ref>, followed by Diagonal State Space (DSS) model <ref type="bibr" target="#b31">(Gupta et al., 2022)</ref>, Gated State Space (GSS) models <ref type="bibr" target="#b63">(Mehta et al., 2022)</ref>, S5 model <ref type="bibr" target="#b98">(Smith et al., 2022)</ref>, Bidirectional Gated SSM (BiGS) <ref type="bibr" target="#b115">(Wang et al., 2022)</ref>, H3 model <ref type="bibr" target="#b22">(Fu et al., 2023)</ref>, and Mamba <ref type="bibr">(Gu &amp; Dao, 2023)</ref>.</p><p>Recurrent Neural Networks. Recurrent Neural Networks (RNNs) have been suggested to replace Transformer and attention due to their linearity in the context length. RNNs with Deep Linear Recurrent Units (LRUs) showed promising results for language modeling <ref type="bibr">(Orvieto et al., 2023;</ref><ref type="bibr" target="#b18">De et al., 2024)</ref>, as did Hierarchically Gated Linear RNN (HGRN) <ref type="bibr" target="#b79">(Qin et al., 2023)</ref> and HGRN2 <ref type="bibr" target="#b80">(Qin et al., 2024)</ref>. A well-known RNN approach to large language modeling is RWKV <ref type="bibr" target="#b75">(Peng et al., 2023</ref><ref type="bibr" target="#b76">(Peng et al., , 2024))</ref>, showcasing competitive performance to Transformers.</p><p>Gating. One of the key ideas of LSTM is gating, which was rediscovered and reinterpreted in many recent approaches. Gating was used in HGRN <ref type="bibr" target="#b79">(Qin et al., 2023)</ref>, HGRN2 <ref type="bibr" target="#b80">(Qin et al., 2024)</ref>, Gated Linear Attention (GLA) <ref type="bibr" target="#b121">(Yang et al., 2023)</ref>, Gated State Space (GSS) models <ref type="bibr" target="#b63">(Mehta et al., 2022)</ref>, Bidirectional Gated SSM (BiGS) <ref type="bibr" target="#b115">(Wang et al., 2022)</ref>, Moving Average Equipped Gated Attention (MEGA) <ref type="bibr" target="#b60">(Ma et al., 2022)</ref>, RWKV <ref type="bibr" target="#b75">(Peng et al., 2023)</ref>, and Mamba <ref type="bibr">(Gu &amp; Dao, 2023)</ref>.</p><p>Covariance Update Rule. To enhance storage capacities, we equipped the mLSTM cell with a matrix memory with a covariance update rule. Other methods which build on such an update mechanism are Fast Weight Programmers <ref type="bibr" target="#b93">(Schmidhuber, 1992;</ref><ref type="bibr" target="#b92">Schlag et al., 2021)</ref>, RWKV-5 and RWKV-6 <ref type="bibr" target="#b76">(Peng et al., 2024)</ref>, Retention <ref type="bibr" target="#b103">(Sun et al., 2023)</ref>, Linear Transformer <ref type="bibr" target="#b45">(Katharopoulos et al., 2020)</ref>, and HGRN2 <ref type="bibr" target="#b80">(Qin et al., 2024)</ref>.</p><p>Most Related. Conceptually the closest models to xLSTM are Retention <ref type="bibr" target="#b103">(Sun et al., 2023)</ref>, RWKV <ref type="bibr" target="#b75">(Peng et al., 2023</ref><ref type="bibr" target="#b76">(Peng et al., , 2024))</ref>, and HGRN2 <ref type="bibr" target="#b80">(Qin et al., 2024)</ref>. These models share the concepts matrix memory and/or gating. However, in contrast to the new sLSTM, these approaches do not allow memory mixing. Memory mixing enables to solve state tracking problems, and therefore LSTMs are more expressive than State Space Models (SSMs) and Transformers <ref type="bibr" target="#b66">(Merrill et al., 2024;</ref><ref type="bibr" target="#b20">DelÃ©tang et al., 2023)</ref>. State tracking is required to evaluate code or to track entities in a long narrative.</p><p>Residually Stacking Architectures. Like almost all contemporary large deep learning models, xLSTM architectures are constructed by residually stacking building blocks <ref type="bibr" target="#b102">(Srivastava et al., 2015;</ref><ref type="bibr" target="#b32">He et al., 2016)</ref>. This construction enabled deep convolutional networks <ref type="bibr" target="#b32">(He et al., 2016)</ref> and Transformers <ref type="bibr" target="#b113">(Vaswani et al., 2017)</ref>. Transformers are the ultimate force behind Large Language Models (LLMs) like GPT-3 <ref type="bibr" target="#b9">(Brown et al., 2020)</ref>, ChatGPT <ref type="bibr">(Schulman et al., 2022)</ref>, <ref type="bibr">GPT-4 (Achiam et al., 2023)</ref>, Megatron-LM <ref type="bibr" target="#b97">(Shoeybi et al., 2019)</ref>, Gopher <ref type="bibr" target="#b84">(Rae et al., 2021)</ref>, ERNIE 3.0 Titan <ref type="bibr" target="#b117">(Wang et al., 2021)</ref>, GLaM <ref type="bibr" target="#b21">(Du et al., 2021)</ref>, Chinese M6 <ref type="bibr" target="#b57">(Lin et al., 2021)</ref>, mutilingual AlexaTM 20B <ref type="bibr" target="#b101">(Soltan et al., 2022)</ref>, OPT <ref type="bibr" target="#b126">(Zhang et al., 2022)</ref>, Chinchilla <ref type="bibr" target="#b39">(Hoffmann et al., 2022)</ref>, BLOOM <ref type="bibr" target="#b91">(Scao et al., 2022)</ref>, GLM-130B <ref type="bibr" target="#b125">(Zeng et al., 2022)</ref>, LaMDA <ref type="bibr" target="#b109">(Thoppilan et al., 2022)</ref>, PaLM <ref type="bibr" target="#b11">(Chowdhery et al., 2022)</ref>, Llama <ref type="bibr" target="#b111">(Touvron et al., 2023</ref><ref type="bibr" target="#b25">), Gemini (Google, 2023;</ref><ref type="bibr" target="#b88">Reid et al., 2024)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we experimentally evaluate xLSTM and compare it to existing methods with a focus on language modeling. We investigate xLSTM's specific capabilities on synthetic tasks in Section 4.1. In Section 4.2, we compare the validation set perplexity of various current language modeling methods that were trained on 15B tokens from SlimPajama <ref type="bibr" target="#b99">(Soboleva et al., 2023)</ref>. On the same dataset, we perform ablation studies for xLSTM. Then, we assess the scaling behavior of the different methods analogous to <ref type="bibr" target="#b41">Kaplan et al. (2020)</ref> and <ref type="bibr" target="#b9">Brown et al. (2020)</ref>. In Section 4.3, we conduct a more thorough language modeling experiment. We compare xLSTM and the best performing methods from Section 4.2 after being trained on 300B tokens from SlimPajama <ref type="bibr" target="#b99">(Soboleva et al., 2023)</ref>. First, we assess how well the methods perform in extrapolating to longer contexts, secondly we test the methods via validation perplexity and performance on downstream tasks <ref type="bibr" target="#b105">(Sutawika et al., 2024)</ref>, thirdly we evaluate the methods on 571 text domains of the PALOMA language benchmark dataset <ref type="bibr" target="#b62">(Magnusson et al., 2023)</ref>, fourthly we again assess the scaling behavior of the different methods, but now with 20 times more training data.</p><p>For all experiments, we use the notation xLSTM[a:b] for the ratio a/b of mLSTM-based versus sLSTM-based xLSTM blocks. For example, xLSTM[7:1] means that out of eight blocks, seven are mLSTM-based blocks and one is an sLSTM-based block. For a common total block number of 48, this translates to 6 sLSTM-based blocks and 42 mLSTM-based blocks. Further, for all experiments, we use pre and post up-projection blocks for mLSTM and sLSTM, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Synthetic Tasks and Long Range Arena</head><p>First, we test the effectiveness of xLSTM's new exponential gating with memory mixing on formal languages <ref type="bibr" target="#b20">(DelÃ©tang et al., 2023)</ref>. Then, we assess the effectiveness of xLSTM's new matrix memory on the Multi-Query Associative Recall task <ref type="bibr" target="#b3">(Arora et al., 2023)</ref>. Finally, xLSTM's performance at processing long sequences in the Long Range Arena is evaluated <ref type="bibr" target="#b108">(Tay et al., 2021)</ref>.</p><p>Test of xLSTM's Exponential Gating with Memory Mixing. We test xLSTM's new exponential gating with memory mixing, which should enable it to solve state tracking problems <ref type="bibr" target="#b66">(Merrill et al., 2024;</ref><ref type="bibr" target="#b65">Merrill &amp; Sabharwal, 2023)</ref>. We implement and extend the formal language tasks from <ref type="bibr" target="#b20">DelÃ©tang et al. (2023)</ref> to enable multi-length training for length extrapolation. For a detailed description of all tasks and extended results see Appendix B.1.1. We compare xLSTM to other methods including Transformers, State Space Models, and Recurrent Neural Networks. The accuracy of the tested methods is evaluated on those tokens relevant to the task. The accuracy is scaled between 0 (random) and 1 (perfect). We compare 2-block architectures of the following methods on these tasks: xLSTM[0:1] (i.e., only sLSTM), xLSTM[1:0] (i.e., only mLSTM), xLSTM[1:1], Llama, Mamba, RWKV, Retention, Hyena, LSTM, and LSTM in Transformer blocks (LSTM (Block)). The results of this experiment are shown in Figure <ref type="figure" target="#fig_1">4</ref>. Models such as Transformers or State Space Models without memory mixing (no state tracking) cannot solve, e.g. regular grammars like the parity task. This result is in agreement with findings that Transformers and State Space models are fundamentally less powerful than RNNs <ref type="bibr" target="#b66">(Merrill et al., 2024;</ref><ref type="bibr" target="#b65">Merrill &amp; Sabharwal, 2023;</ref><ref type="bibr" target="#b20">DelÃ©tang et al., 2023)</ref>.</p><p>Test of xLSTM's Memory Capacities on Associative Recall Tasks. In this experiment, we test xLSTM's new matrix memory in terms of the memory capacity on the Multi-Query Associative Recall task <ref type="bibr" target="#b3">(Arora et al., 2023)</ref>: For each sequence, key-value pairs are randomly chosen from a large vocabulary, which must be memorized for later retrieval. To enhance the difficulty of the original task, we increase the number of key-value pairs up to 256 and extend the context length up to 2048. Thus, we have broader tests for the memory capacities of different models. We compare 2-block architectures of Llama, Mamba, RWKV-5, RWKV-6, xLSTM[1:1] and xLSTM[1:0]. The models are evaluated by the accuracy at recalling the pairs. Since Transformers (e.g. Llama) have a memory that is exponential in the coding dimension <ref type="bibr" target="#b86">(Ramsauer et al., 2021)</ref>, they constitute the gold standard at this task. Results are shown in Figure <ref type="figure">5</ref>. xLSTM[1:1] performs best among all non-Transformer models, also for small models. Interestingly, the sLSTM block does not diminish the memory capacity but rather leverages it, which becomes evident at the most difficult task with 256 key-value pairs. Additional results are presented in Appendix B.</p><p>1.2, where extrapolation analyses indicate that xLSTM's enhanced memory capacities also allow for extrapolating to contexts that are longer than those seen during training. 32 64 128 256 512 Model Dim 0.00 0.25 0.50 0.75 1.00 Accuracy KV Pairs = 48 32 64 128 256 512 Model Dim KV Pairs = 96 32 64 128 256 512 Model Dim KV Pairs = 256 Llama Mamba RWKV-5 RWKV-6 xLSTM[1:0] xLSTM[1:1] Figure 5: Test of memory capacities of different models at the Multi-Query Associative Recall task with context length 2048. Each panel is dedicated to a different number of key-value pairs. The x-axis displays the model size and the y-axis the validation accuracy. Test of xLSTM's Long Context Capabilities on Long Range Arena. To assess xLSTM's performance on long sequences and large contexts, we compare different methods on the Long Range Arena (Tay et al., 2021). xLSTM demonstrates consistent strong performance on all of the tasks, suggesting that the xLSTM architecture is remarkably efficient in handling different aspects of long context problems. For more details, see Appendix B.1.3. 4.2 Method Comparison and Ablation Study To address the main question of our paper, i.e. what can our new LSTM variants achieve when scaled up in language modelling, we train xLSTMs, Transformers, State Space Models, and other methods on 15B tokens from SlimPajama in the same auto-regressive setting. We compare the trained models on the validation set and perform ablation studies for the xLSTMs. Model #Params M SlimPajama (15B) ppl â†“ GPT-3 356 14.26 Llama 407 14.25 H3 420 18.23 Mamba 423 13.70 Hyena 435 17.59 RWKV-4 430 15.62 RWKV-5 456 14.25 RWKV-6 442 15.03 RetNet 431 16.23 HGRN 411 17.59 GLA 412 16.15 HGRN2 411 14.32 xLSTM[1:0] 409 13.43 xLSTM[7:1] 408 13.48 Table 1: Method comparison on next token prediction when trained on 15B tokens from SlimPajama. Best validation perplexities within model classes, i.e., Transformers, LSTMs, SSMs, RNNs, and linear Transformers are underlined and overall best is in bold. For each model class, the best performing methods are later used in Section 4.3 for LLM training. xLSTMs with new memory (xLSTM[1:0] and xLSTM[7:1]) perform best.</p><p>Comparing xLSTM to Other Methods. For comparison, we train models on 15B tokens from SlimPajama <ref type="bibr" target="#b99">(Soboleva et al., 2023)</ref>. The trained models are evaluated by their perplexity on the validation set. We compare the following methods: xLSTM (our new method), GPT-3 (Transformer) <ref type="bibr" target="#b9">(Brown et al., 2020)</ref>, Llama (Transformer) <ref type="bibr" target="#b111">(Touvron et al., 2023)</ref>, H3 (SSM) <ref type="bibr" target="#b22">(Fu et al., 2023)</ref>, Mamba (SSM) <ref type="bibr">(Gu &amp; Dao, 2023)</ref>, RWKV-4 (RNN) <ref type="bibr" target="#b75">(Peng et al., 2023)</ref>, RWKV-5 (RNN) <ref type="bibr" target="#b76">(Peng et al., 2024)</ref>, RWKV-6 (RNN) <ref type="bibr" target="#b76">(Peng et al., 2024)</ref>, GLA (linear Transformer) <ref type="bibr" target="#b121">(Yang et al., 2023)</ref>, HGRN (RNN) <ref type="bibr" target="#b79">(Qin et al., 2023)</ref>, HGRN2 (RNN) <ref type="bibr" target="#b80">(Qin et al., 2024)</ref>. RetNet (linear Transformer) <ref type="bibr" target="#b103">(Sun et al., 2023)</ref>, Hyena (linear Transformer) <ref type="bibr" target="#b3">(Poli et al., 2023)</ref>, xLSTM[1:0], and xLSTM[7:1] (see Section 4). The models were trained with mixed precision, for RWKV-5, RWKV-6, GLA, HGRN2, the mixed-precision training did not utilize the PyTorch automated mixed precision (see also Appendix Section B.2). We categorize the methods into (a) Transformers, (b) State Space Models (SSMs), and (c) Recurrent Neural Networks (RNNs) together with linear Transformers. Linear Transformers are linear methods that substitute the Transformer attention mechanism. The models match a GPT-3 model with 350M parameters in size, i.e. embedding dim 1024 and 24 residual blocks. Only GPT-3 uses shared weights for token and output embeddings, therefore has fewer parameters. The results in Table <ref type="table">1</ref> show that xLSTM outperforms all existing methods in validation perplexity. For details see Appendix B.2. Figure <ref type="figure">6</ref> shows the scaling behaviour for this experiment, indicating that xLSTM will also perform favorably for larger models.</p><p>Ablation Studies.</p><p>Table 1 and Figure 6 demonstrate that xLSTM achieves excellent results at language modeling when being trained on 15B tokens from SlimPajama. To ablate the changes from LSTM to xLSTM, we morph a vanilla LSTM architecture step-by-step into an xLSTM architecture. Firstly, we integrate LSTM layers into pre-LayerNorm residual backbones. Secondly, we extend this to a post up-projection block. Finally, we add exponential gating and matrix memory. The results are shown in Table 2 (top). The ablation studies attribute the strong performance improvement to both the exponential gating and the matrix memory. Additionally, due to the importance of gating in RNNs and State Space Models, we ablate different gating mechanisms. In Table 2 (bottom), we conclude that having each gate learnable and influenced by the input has an incremental positive effect. Additional studies on the individual backbone components are discussed in Appendix B.2. 15B Tokens Figure 6: Method comparison on next token prediction when trained on 15B tokens from SlimPajama. Performance measure in validation perplexity for the best methods of each model class (see Table 1) are reported. The performance degradation of xLSTM[7:1] at 2.7B is due to initially slower training convergence that leads to an especially undertrained model. xLSTM is the best method at all sizes. Ablation studies on the new xLSTM components. Model Modification Exponential Gating Matrix Memory #Params M SlimPajama (15B) ppl â†“ LSTM Vanilla Multi-Layer LSTM âœ— âœ— 607.8 2417.86 Adding Resnet Backbone âœ— âœ— 506.1 35.46 Adding Up-Projection Backbone âœ— âœ— 505.9 26.01 xLSTM[0:1] Adding Exponential Gating âœ“ âœ— 427.3 17.70 xLSTM[7:1] Adding Matrix Memory âœ“ âœ“ 408.4 13.48 Ablation studies on different gating techniques. Learnable Gates Forget Gate Input Gate SlimPajama (15B) ppl â†“ Input Dependent Learnable Bias Bias Init Input Dependent Learnable Bias Bias Init</p><p>No Gates</p><formula xml:id="formula_9">âœ— âœ— +âˆž âœ— âœ— 0 NaN No Gates âœ— âœ— [3, 6] âœ— âœ— 0 13.95 Forget Gate âœ“ âœ“ [3, 6] âœ— âœ— 0 13.58 Input Gate âœ— âœ— [3, 6] âœ“ âœ“ N (0, 0.1) 13.69 Forget Gate Bias âœ— âœ“ [3, 6] âœ— âœ— 0 13.76 Forget + Input Gate Bias âœ— âœ“ [3, 6] âœ— âœ“ N (0, 0.1) 13.73 Forget Gate + Input Gate Bias âœ“ âœ“ [3, 6] âœ— âœ“ N (0, 0.1) 13.55 Forget Gate + Input Gate âœ“ âœ“ [3, 6] âœ“ âœ“ N (0, 0.1) 13.43</formula><p>Table 2: Ablation studies. Top: Ablation studies on the new xLSTM components, contributing the strong performance improvement of xLSTM over vanilla LSTM to both the exponential gating and the matrix memory. Bottom: Ablation studies on different gating techniques. We consider an xLSTM[1:0] with sigmoid forget gate and exponential input gate. Bias initialization âˆž means that the forget gate is set to one, <ref type="bibr">[3,</ref><ref type="bibr">6]</ref> indicates that values are taken equidistant in the respective interval, and N (0, 0.1) that values are randomly chosen from a Gaussian with mean 0 and std 0.1. PPL denotes validation perplexity. The first two lines correspond to models similar to linearized attention, line four to Retention, line five to RWKV-5, and line six to RWKV-6. Dependencies of the gates on the input lead to better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">xLSTM as Large Language Model</head><p>We culminate this study in large-scale language modeling experiments, testing the potential of xLSTM as an LLM. We therefore increase the amount of training data and train on 300B tokens from SlimPajama. The same number of tokens is used in, e.g., Mamba <ref type="bibr">(Gu &amp; Dao, 2023)</ref> and Griffin <ref type="bibr" target="#b18">(De et al., 2024)</ref>. We compare xLSTM to RWKV-4, Llama, and Mamba -one method from each respective method class in Section 4.2. We select RWKV-4 as RNN representative since for RWKV-5, RWKV-6 and HGRN2 a reasonable training precision setting has been found only after the training start of the 300B token experiments (see Appendix B.2). We train different model sizes (125M, 350M, 760M, 1.3B), test all models for length extrapolation capabilities and evaluate their performance on the validation set. We assess their performance on downstream tasks, test their performance in language modeling on 571 text domains of the PALOMA benchmark, and, finally, investigate their scaling law behavior.</p><p>Sequence Length Extrapolation. Firstly, we test the sequence length extrapolation for 1.3B-sized, large models of xLSTM, RWKV-4, Llama, and Mamba. All models are trained on context length 2048, and then tested for context lengths up to 16384. See Figure <ref type="figure">7</ref> for the results. In contrast to other methods, xLSTM models maintain low perplexities for longer contexts.</p><p>Model SlimPajama (300B) ppl â†“ at 16k Llama 337.83 Mamba 14.00 RWKV-4 13.75 xLSTM[7:1] 8.92 xLSTM[1:0] 9.01 Figure 7: Sequence extrapolation in language modeling. This is a comparison of 1.3B-sized, large models of xLSTM, RWKV-4, Llama, and Mamba at next token prediction on the SlimPajama validation set after training on 300B tokens from SlimPajama. Models are trained with context length 2048 and then tested for context lengths up to 16384. Left: Token perplexities evaluated at different context lengths. In contrast to other methods, xLSTM models remain at low perplexities for longer contexts. Right: Prediction quality when extrapolating to long context sizes in terms of validation perplexity (PPL). xLSTM yields the best PPL values (best in bold, second best underlined).</p><p>Validation Perplexity and Downstream Tasks. Secondly, for all model sizes, we evaluate the performance of xLSTM, RWKV-4, Llama, and Mamba models on the SlimPajama validation set for next token prediction and on downstream tasks that measure common sense reasoning. The third column of Table <ref type="table" target="#tab_7">3</ref> lists the validation set perplexities of different methods. Both xLSTM[1:0] and xLSTM[7:1] are the best models for all model sizes with respect to the validation set perplexity. The other columns of Table <ref type="table" target="#tab_7">3</ref> provide the performance on downstream tasks. In the vast majority of tasks and across all model sizes xLSTM is the best method -only on the ARC task Mamba is in some cases the best method. For details see Appendix B.3.</p><p>Performance on PALOMA Language Tasks. Thirdly, for all model sizes, we test the next token prediction performance of xLSTM, RWKV-4, Llama, and Mamba models on PALOMA language tasks <ref type="bibr" target="#b62">(Magnusson et al., 2023)</ref>. We measure the performance by the perplexity for next token prediction on 571 text domains, which range from nytimes.com to r/depression on Reddit. Table <ref type="table">4</ref> shows token prediction perplexity grouped into language modeling (first seven columns) and finegrained domain benchmarks (last 5 columns</p><p>). xLSTM[1:0] performs better than xLSTM[7:1] on these language tasks. xLSTM[1:0] has in 568 out of 571 (99.5%) text domains a lower perplexity Model #Params M SlimPajama (300B) ppl â†“ LAMBADA ppl â†“ LAMBADA acc â†‘ HellaSwag acc â†‘ PIQA acc â†‘ ARC-E acc â†‘ ARC-C acc â†‘ WinoGrande acc â†‘ Average acc â†‘ 125M RWKV-4 169.4 16.66 54.72 23.77 34.03 66.00 47.94 24.06 50.91 41.12 Llama 162.2 15.89 39.21 31.54 34.09 65.45 45.33 23.63 50.67 41.78 Mamba 167.8 15.08 27.76 34.14 36.47 66.76 48.86 24.40 51.14 43.63 xLSTM[1:0] 163.8 14.63 25.98 36.52 36.74 65.61 47.81 24.83 51.85 43.89 xLSTM[7:1] 163.7 14.60 26.59 36.08 36.75 66.87 48.32 25.26 51.70 44.16 350M RWKV-4 430.5 12.62 21.57 36.62 42.47 69.42 54.46 25.43 51.22 46.60 Llama 406.6 12.19 15.73 44.19 44.45 69.15 52.23 26.28 53.59 48.32 Mamba 423.1 11.64 12.83 46.24 47.55 69.70 55.47 27.56 54.30 50.14 xLSTM[1:0] 409.3 11.31 11.49 49.33 48.06 69.59 55.72 26.62 54.38 50.62 xLSTM[7:1] 408.4 11.37 12.11 47.74 47.89 71.16 56.61 27.82 53.28 50.75 760M RWKV-4 891.0 10.55 10.98 47.43 52.29 72.69 58.84 28.84 55.41 52.58 Llama 834.1 10.60 9.90 51.41 52.16 70.95 56.48 28.75 56.67 52.74 Mamba 870.5 10.24 9.24 50.84 53.97 71.16 60.44 29.78 56.99 53.86 xLSTM[1:0] 840.4 9.86 8.09 54.78 55.72 72.69 62.75 32.59 58.17 56.12 xLSTM[7:1] 839.7 9.91 8.07 55.27 56.12 72.74 61.36 29.61 56.43 55.26 1.3B RWKV-4 1515.2 9.83 9.84 49.78 56.20 74.70 61.83 30.63 55.56 54.78 Llama 1420.4 9.44 7.23 57.44 57.81 73.12 62.79 31.74 59.04 56.99 Mamba 1475.3 9.14 7.41 55.64 60.45 74.43 66.12 33.70 60.14 58.41 xLSTM[1:0] 1422.6 8.89 6.86 57.83 60.91 74.59 64.31 32.59 60.62 58.48 xLSTM[7:1] 1420.1 9.00 7.04 56.69 60.26 74.92 65.11 32.34 59.27 58.10 Scaling Laws. Fourthly, we assess the power-law scaling behavior, which allows to extrapolate the performance to larger model sizes <ref type="bibr" target="#b41">(Kaplan et al., 2020;</ref><ref type="bibr" target="#b9">Brown et al., 2020)</ref>. Figure <ref type="figure">8</ref> presents the scaling behavior. All models share a similar scaling behavior but with different offsets. RWKV-4 performs worst, followed by Llama and Mamba. xLSTM is better than Mamba with a similar margin to Mamba as Mamba has to Llama. The scaling behavior indicates that for larger models xLSTM will continue to perform favourable compared to Transformers and State-Space models. 0.2 0.4 1.0 1.4 Number of Parameters Ã—10 9 9 10 11 12 13 14 15 16 17 Validation Perplexity Llama Mamba RWKV-4 xLSTM[7:1] xLSTM[1:0] 300B Tokens Figure 8: Scaling laws. Next token prediction perplexity of xLSTM, RWKV-4, Llama, and Mamba on the SlimPajama validation set when trained on 300B tokens from SlimPajama. Model sizes are 125M, 350M, 760M, and 1.3B. Best models for each model class, see Table 1, were selected. The scaling laws indicate that for larger models xLSTM will perform well too.</p><p>Generation Times and Maximal Throughput. Finally, we measure the text generation time in Figure <ref type="figure" target="#fig_2">9</ref> and the maximal throughput in Figure <ref type="figure" target="#fig_2">9</ref> (left) for our xLSTM variants at 1.3B scale. We compare against similar sized Mamba, Llama and RWKV implementions from HuggingFace, including a static key-value cache for the Llama model. At the time of the experiments, both full cache compilation of the Transformer Model and compilation of the Mamba model with torch.compile did not work. For the text generation experiments all of the models are tested at batch size 1 and pre-fill 16. This pre-fill should be maximally favorable for the Transformer. Figure <ref type="figure" target="#fig_2">9</ref> shows the linear scaling of the xLSTM and the other recurrent models Mamba and RWKV-4 compared to the quadratic scaling of Llama. For the decoding throughput we measure different batch sizes and prefill for the Llama model. Figure <ref type="figure" target="#fig_2">9</ref> (right) shows that xLSTM can use much higher batch sizes than Llama due to its constant memory and thus achieves the highest throughput. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Limitations</head><p>(i) In contrast to mLSTM, memory mixing of the sLSTM prohibits parallelizable operations, and therefore does not allow a fast parallel implementation. Nevertheless, we developed a fast CUDA kernel for sLSTM, which is currently less than two times slower than our parallel mLSTM implementation. (ii) The CUDA kernels for mLSTM are not optimized, and therefore the current implementation is about 4 times slower than FlashAttention or the scan used in Mamba. Faster CUDA kernels could be obtained in the vein of FlashAttention. (iii) The matrix memory of mLSTM has high computation complexity since d Ã— d matrices must be processed. Still, the memory update and retrieval does not use parameters and can be parallelized using standard matrix operations, therefore the wall clock time overhead due to the complex memory is minor. (iv) The initialization of the forget gates must be chosen carefully. (v) Since the matrix memory is independent of the sequence length, increasing the sequence length might overload the memory for longer context sizes. Still, this does not appear to be a limitation for contexts up to 16k, see Section 4.3. (vi) Due to the expensive computational load for large language experiments, we did neither fully optimize the architecture nor the hyperparameters, especially for larger xLSTM architectures. We anticipate that an extensive optimization process is needed for xLSTM to reach its full potential.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have partly answered our simple question: How far do we get in language modeling when scaling LSTM to billions of parameters</p><p>? So far, we can answer: "At least as far as current technologies like Transformers or State Space Models". We have enhanced LSTM to xLSTM by exponential gating with memory mixing and a new memory structure. xLSTM models perform favorably on language modeling when compared to state-of-the-art methods like Transformers and State Space Models. The scaling laws indicate that larger xLSTM models will be serious competitors to current Large Language Models that are built with the Transformer technology. xLSTM has the potential to considerably impact other fields like Reinforcement Learning, Time Series Prediction, or the modeling of physical systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Extended Long Short-Term Memory</head><p>A.1 Vanilla Long Short-Term Memory Formulation: Vector Notation</p><p>The vanilla LSTM memory cell update rules <ref type="bibr" target="#b28">(Greff et al., 2015)</ref> at time step t extend the scalar cell state formulation to a vector of cell states: <ref type="formula">29</ref>)</p><formula xml:id="formula_10">c t = f t âŠ™ c t-1 + i t âŠ™ z t cell state (28) h t = o t âŠ™ ht , ht = Ïˆ c t hidden state (</formula><formula xml:id="formula_11">z t = Ï† ( zt ) , zt = W z x t + R z h t-1 + b z cell input (30) i t = Ïƒ Ä©t , Ä©t = W i x t + R i h t-1 + b i input gate (31) f t = Ïƒ ft , ft = W f x t + R f h t-1 + b f forget gate (32) o t = Ïƒ (Ãµ t ) , Ãµt = W o x t + R o h t-1 + b o output gate (33)</formula><p>The matrices W z , W i , W f , and W o correspond to the input weights between inputs x t and cell input, input gate, forget gate, and output gate, respectively. The matrices R z , R i , R f , and R o correspond to the recurrent weights between hidden state h t-1 and cell input, input gate, forget gate, and output gate, respectively. b z , b i , b f , and b o are the corresponding bias vectors. Ï† and Ïˆ are the cell input and hidden state activation functions (typically tanh). Ïˆ is used to normalize or squash the cell state, which would be unbounded otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 sLSTM</head><p>Similar to the LSTM in Section A.1, also the sLSTM can be vectorized to multiple cells:</p><formula xml:id="formula_12">c t = f t âŠ™ c t-1 + i t âŠ™ z t cell state (34) n t = f t âŠ™ n t-1 + i t normalizer state (35) h t = o t âŠ™ ht , ht = c t âŠ™ n -1 t hidden state (36) z t = Ï† ( zt ) , zt = W z x t + R z h t-1 + b z cell input (37) i t = exp Ä©t , Ä©t = W i x t + R i h t-1 + b i input gate (38) f t = exp ft OR Ïƒ ft , ft = W f x t + R f h t-1 + b f forget gate (39) o t = Ïƒ (Ãµ t ) , Ãµt = W o x t + R o h t-1 + b o output gate (40)</formula><p>Here, the cell input activation function Ï† is tanh, the hidden state activation function is the identity. Ï† helps stabilizing the recurrence.</p><p>Considering external gradient contribution Î´ ext ht from subsequent layers and recurrent gradient contribution Î´ R ht from gradients from future states flowing over the cell interaction matrix R, we obtain the recursive backward pass of sLSTM, where Î´ a indicates gradients with respect to parameter / internal variable a:</p><formula xml:id="formula_13">Î´ ht = Î´ ext ht + Î´ R ht (41) Î´ ct-1 = f t âŠ™ Î´ ct + o t-1 âŠ™ n t-1 -1 âŠ™ Î´ ht-1 (42) Î´ nt-1 = f t âŠ™ Î´ nt -o t-1 âŠ™ c t-1 âŠ™ n -2 t-1 âŠ™ Î´ ht-1 (43) Î´ ft = f â€² t âŠ™ c t-1 âŠ™ Î´ ct + f â€² t âŠ™ n t-1 âŠ™ Î´ nt (44) Î´ Ä©t = i â€² t âŠ™ z t âŠ™ Î´ ct + i â€² t âŠ™ Î´ nt (<label>45</label></formula><formula xml:id="formula_14">)</formula><formula xml:id="formula_15">Î´ zt = i t âŠ™ Ï† â€² (z t ) âŠ™ Î´ ct (<label>46</label></formula><formula xml:id="formula_16">)</formula><formula xml:id="formula_17">Î´ Ãµt = o â€² t âŠ™ c t âŠ™ n -1 t âŠ™ Î´ ht<label>(47)</label></formula><formula xml:id="formula_18">Î´ xt = gâˆˆ{f ,i,z,o} W âŠ¤ g Î´ gt (48) Î´ R ht-1 = gâˆˆ{f ,i,z,o} R âŠ¤ g Î´ gt (49) Î´ âŠ¤ Rg = t h t-1 Î´ âŠ¤ gt , g âˆˆ {i, f , z, o}<label>(50)</label></formula><formula xml:id="formula_19">Î´ âŠ¤ Wg = t x t Î´ âŠ¤ gt , g âˆˆ {i, f , z, o}<label>(51)</label></formula><p>with the derivatives of the respective gate activation function</p><formula xml:id="formula_20">i â€² t = exp â€² ( Ä©t ) = exp( Ä©t ) = i t , o â€² t = Ïƒ â€² (Ãµ t ), and f â€² t = Ïƒ â€² ( ft ) or f â€² t = f t depending on the forget gate activation. Ï† â€² (z)</formula><p>is the derivative of the cell input activation function Ï†(z).</p><p>The matrices R z , R i , R f , R o are block-diagonal which is analogous to multiple heads in the mLSTM. This way, the parameters reduce to d 2 /(N h ), where N h is the number of heads, limiting the cell interactions to individual heads. This parameter efficient formulation of cell interactions together with the exponential gating is called the new memory mixing. Finally, to stabilize the backward pass, we clip the magnitude of Î´ R ht to 10, as a means to prohibit exploding gradients for long context lengths.</p><p>Proof of Equivalence for sLSTM Stabilized Version. The stabilization state m, see Equation (15) in the main paper, has no gradient, and hence does not influence the other gradients. We go back to the scalar version (Equation <ref type="formula" target="#formula_3">8</ref>) here for simplicity. We re-define c (s) t and n (s) t as stabilized cell and normalizer states:</p><formula xml:id="formula_21">c t = c (s) t exp m t (52) n t = n (s) t exp m t<label>(53)</label></formula><p>Inserting Equation <ref type="formula">15</ref>into Equation 8 yields:</p><formula xml:id="formula_22">h(s) t = c (s) t /n (s) t = (54) = exp log (f t ) + m t-1 -m t c (s) t-1 + exp log (i t ) -m t z t exp log (f t ) + m t-1 -m t n (s) t-1 + exp log (i t ) -m t (55) = exp log (f t ) + m t-1 c (s) t-1 + exp (log (i t )) z t exp log (f t ) + m t-1 n (s) t-1 + exp (log (i t )) (56) = exp (log (f t )) c t-1 + exp (log (i t )) z t exp (log (f t )) n t-1 + exp (log (i t )) (57) = f t c t-1 + i t z t f t n t-1 + i t = c t /n t = ht<label>(58)</label></formula><p>Therefore, since the loss solely depends on h t , there's no dependency on m t , and consequently, no gradient exists for this stabilization state. Note that m t can be chosen arbitrarily. We choose m t = max (log (f t ) + m t-1 , log (i t )), which stabilizes the exponential function. One can even find m t , such that the normalizer state n t can be eliminated, but this version was experimentally found to be numerically unstable in the backward pass.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 mLSTM</head><p>Throughout this section, 1 âˆˆ R T denotes a column vector of ones and 1 âŠ¤ âˆˆ R 1Ã—T a row vector of ones, where T is the dimension of this vector.</p><p>Recurrent mLSTM Backward Pass. The recurrent formulation of the mLSTM cell in Equation <ref type="formula">19</ref>yields the following backward pass recurrence, where Î´ a indicates gradients with respect to parameter or internal variable a and Î´ ext ht denotes gradients from subsequent layers:</p><formula xml:id="formula_23">Î´ ht = o t âŠ™ Î´ ext ht (59) Î´ âŠ¤ Ct-1 = f t Î´ âŠ¤ Ct + q t-1 Î´ âŠ¤ ht-1 max n âŠ¤ t-1 q t-1 , 1<label>(60)</label></formula><formula xml:id="formula_24">Î´ nt-1 = f t Î´ nt - q âŠ¤ t-1 C âŠ¤ t-1 Î´ ht-1 max n âŠ¤ t-1 q t-1 , 1 2 â„¦ n âŠ¤ t-1 q t-1 q t-1<label>(61)</label></formula><formula xml:id="formula_25">Î´ âŠ¤ vt = i t k âŠ¤ t Î´ âŠ¤ Ct (62) Î´ âŠ¤ kt = i t v âŠ¤ t Î´ Ct + Î´ âŠ¤ nt (<label>63</label></formula><formula xml:id="formula_26">)</formula><formula xml:id="formula_27">Î´ qt = C âŠ¤ t Î´ ht max n âŠ¤ t q t , 1 - q âŠ¤ t C âŠ¤ t Î´ ht max n âŠ¤ t q t , 1 2 â„¦ n âŠ¤ t q t n t (<label>64</label></formula><formula xml:id="formula_28">)</formula><formula xml:id="formula_29">Î´ xt = gâˆˆ{q,k,v} W âŠ¤ g Î´ gt (65) Î´ âŠ¤ Wg = t x t Î´ âŠ¤ gt , g âˆˆ {q, k, v}<label>(66)</label></formula><formula xml:id="formula_30">Î´ bg = t Î´ gt , g âˆˆ {q, k, v}<label>(67)</label></formula><formula xml:id="formula_31">Î´ ft = 1 âŠ¤ (C t-1 âŠ™ Î´ Ct ) 1 + 1 âŠ¤ (n t-1 âŠ™ Î´ nt ) Î³ ft<label>(68)</label></formula><formula xml:id="formula_32">Î´ Ä©t = 1 âŠ¤ v t k âŠ¤ t âŠ™ Î´ Ct 1 + 1 âŠ¤ (k t âŠ™ Î´ nt ) exp Ä©t<label>(69)</label></formula><formula xml:id="formula_33">Î´ Ãµt = ht âŠ™ Ïƒ â€² (Ãµ t ) âŠ™ Î´ ht (<label>70</label></formula><formula xml:id="formula_34">)</formula><p>and â„¦ (z) = Î˜ (z -1) -Î˜ (-z -1), Î˜ (z) being the Heaviside step function. Î³ (z) is either Ïƒ â€² (z) or exp (z), depending on the forget gate activation.</p><p>Parallel mLSTM Forward Pass. The mLSTM recurrence in Equations (19-27) can be reformulated in a parallel form, which is used to speed up training. After training we can still use the recurrent formulation for fast text generation.</p><p>Instead of processing each input x t âˆˆ R d at time step t sequentially, the parallel version processes all timesteps of a full sequence X âˆˆ R T Ã—d at once, where T is the sequence length and d is the head dimension. We present the forward pass of the mLSTM for a single head and drop the head dimension for simplicity.</p><p>Let f âˆˆ R T be the forget gate pre-activations and Ä© âˆˆ R T be the input gate pre-activations for a full sequence. We construct the forget gate activation matrix F âˆˆ R T Ã—T by</p><formula xml:id="formula_35">F ij = ï£± ï£´ ï£² ï£´ ï£³ 0 for i &lt; j 1 for i = j i k=j+1 Ïƒ fk for i &gt; j ,<label>(71)</label></formula><p>and the input gate pre-activation matrix Ä¨ âˆˆ R T Ã—T by</p><formula xml:id="formula_36">Ä¨ij = 0 for i &lt; j Ä©j for i â©¾ j . (<label>72</label></formula><formula xml:id="formula_37">)</formula><p>By applying the elementwise exponential input gate activation function naively, we obtain the unstabilized gate activation matrix D âˆˆ R T Ã—T as</p><formula xml:id="formula_38">D = F âŠ™ exp( Ä¨) .<label>(73)</label></formula><p>In order to avoid overflow due to the exponential function we apply the same stabilization as in the recurrent sLSTM, see Equation <ref type="formula">15</ref>. In the parallel formulation of the mLSTM we get a numerically stable gate activation matrix D â€² âˆˆ R T Ã—T by taking the logarithm of D element-wise and subtracting the row-wise maximum value of D from each element:</p><formula xml:id="formula_39">D = log D = log F âŠ™ exp( Ä¨) = log F + Ä¨<label>(74)</label></formula><formula xml:id="formula_40">D â€² = exp( D -max D)<label>(75)</label></formula><p>Given the queries, keys and values Q, K, V âˆˆ R T Ã—d , for a full sequence we can compute all hidden pre-activation states H âˆˆ R T Ã—d in parallel for the un-stabilized version by</p><formula xml:id="formula_41">H = C V , with C = C max | T j=1 C ij |, 1</formula><p>, and</p><formula xml:id="formula_42">C = QK âŠ¤ âˆš d âŠ™ D . (<label>76</label></formula><formula xml:id="formula_43">)</formula><p>Note that we extract the 1 âˆš d factor for K explicitly here and further on. For the stabilized version this yields</p><formula xml:id="formula_44">H = C V , with C = C â€² max | T j=1 C â€² ij |, exp(-max D)</formula><p>, and</p><formula xml:id="formula_45">C â€² = QK âŠ¤ âˆš d âŠ™ D â€² , (<label>77</label></formula><formula xml:id="formula_46">)</formula><p>where for both versions the hidden pre-activation states H are identical.</p><p>With the output gate pre-activations O âˆˆ R T Ã—d we can compute the hidden states H âˆˆ R T Ã—d for all timesteps by applying the output gate in parallel for each timestep element-wise:</p><formula xml:id="formula_47">H = Ïƒ( O) âŠ™ H . (<label>78</label></formula><formula xml:id="formula_48">)</formula><p>This gives the parallel forward pass of the mLSTM for a full input sequence X âˆˆ R T Ã—d .</p><p>Parallel mLSTM Backward Pass. We present the backward pass of the mLSTM for the stabilized version only. For completeness we summarize the forward pass in the stabilized version before we present the backward pass.</p><p>Given the forget gate matrix F âˆˆ R T Ã—T , the logarithm of the forget gate matrix F = log F âˆˆ R T Ã—T , and the input gate matrix I âˆˆ R T Ã—T as introduced above, together with the queries, keys and values Q, K, V âˆˆ R T Ã—d , we can write the forward pass of the mLSTM in the stabilized version as:</p><formula xml:id="formula_49">D = F + Ä¨ (79) m = max j D ij , row-wise maximum (80) D â€² = exp( D -m 1 âŠ¤ ) (81) C â€² = QK âŠ¤ âˆš d âŠ™ D â€² (82) b = T j=1 C â€² ij = C â€² 1 , row-wise sum (83) n = max (|b|, exp(-m))<label>(84)</label></formula><formula xml:id="formula_50">C = C â€² âŠ™ n -1 1 âŠ¤ (85) H = C V<label>(86)</label></formula><p>With this forward pass we can compute the gradients Î´ a for all intermediate and input variables to the mLSTM forward pass in the backward pass. We denote the gradient with respect to variable a as Î´ a .</p><p>Given the output gradient Î´ H âˆˆ R T Ã—d we can compute the backward pass for the intermediate gradients as:</p><formula xml:id="formula_51">Î´ âŠ¤ C = V Î´ âŠ¤ H (<label>87</label></formula><formula xml:id="formula_52">)</formula><formula xml:id="formula_53">Î´ n = -C â€² âŠ™ n -2 1 âŠ¤ âŠ™ Î´ C 1 (88) = -C â€² âŠ™ Î´ C 1 âŠ™ n -2<label>(89)</label></formula><formula xml:id="formula_54">Î´ b = sign (n) âŠ™ Î´ n âŠ™ 1 if |b| &gt; exp(-m) 0 otherwise (90) Î´ C â€² ,C = n -1 1 âŠ¤ âŠ™ Î´ C , column-wise broadcast (91) Î´ âŠ¤ C â€² ,b = 1 Î´ âŠ¤ b , column-wise broadcast (92) Î´ C â€² = Î´ C â€² ,C + Î´ C â€² ,B<label>(93)</label></formula><formula xml:id="formula_55">Î´ D â€² = QK âŠ¤ âˆš d âŠ™ Î´ C â€² (94) Î´ D = exp( D -m) âŠ™ Î´ D â€² = D â€² âŠ™ Î´ D â€²<label>(95)</label></formula><p>We do not compute the gradients for m as they cancel out (see the proof in the recurrent sLSTM).</p><p>With these intermediate gradients the gradients for the logarithmic forget gate matrix Î´ F âˆˆ R T Ã—T , the input gate matrix Î´ I âˆˆ R T Ã—T , and the queries, keys and values Î´ Q , Î´ K , Î´ V âˆˆ R T Ã—d are given by</p><formula xml:id="formula_56">Î´ F = Î´ D (96) Î´ I = Î´ D (97) Î´ Q = D â€² âŠ™ Î´ C â€² K âˆš d (98) Î´ K = D â€² âŠ™ Î´ C â€² âŠ¤ Q âˆš d (99) Î´ V = C âŠ¤ Î´ H<label>(100)</label></formula><p>Having computed the gradients for the logarithmic forget gate matrix Î´ F , we can compute the gradients for the forget gate pre-activations</p><formula xml:id="formula_57">Î´ f = Î´ f1 , Î´ f2 , ..., Î´ fT âŠ¤ âˆˆ R T .</formula><p>Recall the logarithmic forget gate matrix F = log F is computed by</p><formula xml:id="formula_58">F ij = log F ij = ï£± ï£´ ï£´ ï£´ ï£´ ï£² ï£´ ï£´ ï£´ ï£´ ï£³ -âˆž for i &lt; j 0 for i = j i k=j+1 log Ïƒ fk =:f k = i k=j+1 f k for i &gt; j .<label>(101)</label></formula><p>With the substitution f = log Ïƒ( f ) we compute the gradients for the logarithmic forget gate activations</p><formula xml:id="formula_59">Î´ f = Î´ f1 , Î´ f2 , ..., Î´ f T âŠ¤ âˆˆ R T as Î´ f k = k-1 j=1 T i=k Î´ F ij ,<label>(102)</label></formula><formula xml:id="formula_60">Î´ fk = Ïƒ(-fk ) â€¢ Î´ f k ,<label>(103)</label></formula><p>where the last equation makes use of the following:</p><formula xml:id="formula_61">d dx (log Ïƒ(x)) = -(1 + exp(-x)) -1 â€¢ exp(-x) â€¢ (-1) = exp(-x) 1 + exp(-x) = 1 1 + exp(x) = Ïƒ(-x) (104)</formula><p>Finally, we compute the input gate pre-activations' gradients Î´ Ä© = Î´ Ä©1 , Î´ Ä©2 , ..., Î´ Ä©S âŠ¤ âˆˆ R T as the column-wise sum over the rows of the input gate matrix Î´ I : </p><formula xml:id="formula_62">Î´ Ä©k = T i=k (Î´ I ) ik<label>(</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GeLU</head><p>Figure <ref type="figure" target="#fig_5">10</ref>: Schematic representation of an sLSTM Block -post up-projection: Embedded in a pre-LayerNorm residual structure, the input is optionally passed through a causal convolution of window size 4 that includes a Swish activation for input and forget gates. Then, for all input, forget and output gates i, f, o, and the cell update z the input is fed through a block-diagonal linear layer with four diagonal blocks or "heads". These diagonal blocks coincide with the recurrent gate pre-activations from the last hidden state, which corresponds to an sLSTM with four heads depicted with the circular arrows. The resulting hidden state goes through a GroupNorm layer <ref type="bibr" target="#b118">(Wu &amp; He, 2018</ref>) -a head-wise LayerNorm for each of the four heads. Finally, the output is up-and down-projected using a gated MLP, with GeLU activation function and projection factor 4/3 to match parameters. pre-LayerNorm residual structure, the input is up-projected first with projection factor 2, once for an externalized output gate and once as input for the mLSTM cells. The mLSTM cell input is dimension-wise causally convoluted (kernel size 4), before entering a learnable skip connection. We obtain input q and k via block-diagonal projection matrices of block size 4. The values v are fed directly, skipping the convolution part. After the mLSTM sequence mixing, outputs are normalized via GroupNorm <ref type="bibr" target="#b118">(Wu &amp; He, 2018</ref>) -a head-wise layer norm for each of the four heads. Finally, the learnable skip input is added and the result is gated component-wise with the external output gate. The output is down-projected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Experiments</head><p>Training Setup. For all experiments, we use Python<ref type="foot" target="#foot_0">foot_0</ref> 3.11 with PyTorch 2.2.0<ref type="foot" target="#foot_1">foot_1</ref> , and CUDA 12.1<ref type="foot" target="#foot_2">foot_2</ref> on NVIDIA A100 GPUs. We developed and trained all our models and baselines over the course of three months on a cluster with 128 nodes of eight NVIDIA A100 GPUs each. More than 95% of this compute were used for the Language Modeling experiments in Sections 4.2 and 4.3.</p><p>Nearest Neighbor Search Task. For this auxiliary task, we use randomly sampled feature vectors of dimension 2 and unit norm. The attached value is a uniformly distributed random number from [0, 1], leading to inputs vectors of dimension 3. The first feature vector serves as search key, with the first value being ignored. Then the model has to predict the value of the nearest neighbor so far in the sequence. We train on 8192 sequences of context length up to 64 (uniformly sampled) and validate on 8192 different samples. All models have two blocks and embedding dimension 128. We use a dropout of 0.1, 10% linear warm-up steps and cosine decay to 1e-7 for 100k total training steps. We sweep over learning rates 1e-4, 1e-3, 1e-2, 1e-1 and 5 seeds each. The reported values in Figure <ref type="figure" target="#fig_12">2</ref> are mean values for the best learning rate and 99% confidence intervals. Note that LSTM requires very high learning rates, whereas Transformers (Llama) perform best at the smallest learning rate. The xLSTM[0:1] reaches similar performance across all learning rates.</p><p>Wikitext-103 Rare Token Prediction. For this exemplary experiment on rare token prediction, we trained 125M-sized models on Wikitext-103 <ref type="bibr" target="#b64">(Merity et al., 2017)</ref>. All models have an embedding dimension of 768 in a post up-projection structure of 12 residual blocks. The Transformer model (Llama) uses Multi-Head Attention, for what is called LSTM the Multi-Head Attention is replaced by an LSTM and the xLSTM[1:0] contains mLSTM layers with matrix memory. Models were trained with maximum learning rate 1e-3, 4k steps linear warm-up and cosine decay for in total 50k steps, using a batch size of 256 and context length of 512. We use the validation perplexity as a stopping criterion and evaluate on the test set. We evaluate xLSTM on a suite of formal language tasks to test its exponential gating and memory mixing mechanism.</p><p>Formal languages provide a framework to probe the generalization capabilities of models. They allow to specifically test different expressivity levels, e.g. along the Chomsky hierarchy. Typical language model architectures do not necessarily fit perfectly in these hierarchies <ref type="bibr" target="#b20">(DelÃ©tang et al., 2023)</ref> nevertheless these languages allow to illustrate differences in generalization expressivity between different architectures. Our evaluation tasks are heavily based on the work of <ref type="bibr" target="#b20">DelÃ©tang et al. (2023)</ref>.</p><p>Experiment Setup. The different formal language tasks in the experiment (see individual tasks description below) encompass different levels of the Chomsky hierarchy as well as additional counting and memory-focused tasks. We use different lengths per sample, which allows us to validate in a length extrapolation setting. We train on a varying task length up to 40. The evaluation is done for task lengths between 40 and 256 as we are only interested in the "task generalization capabilities" of the models.</p><p>In all experiments, we use two blocks (or layers for the pure LSTM) for all models. We compare Llama, Mamba, Retention, Hyena, RWKV-4, RWKV-5, RWKV-6, LSTM, xLSTM[0:1], xLSTM[1:0] and xLSTM[1:1]. The sLSTM block is used without a convolution and with normal weight initialization. LSTM (Block) refers to an architecture where a vanilla LSTM is used instead of self-attention inside a Transformer block.</p><p>All models are trained with 3 different learning rates (1e-2, 1e-3, 1e-4), each with two seeds. Batch size is 256 -cosine annealing (min lr: 1e-5) with 10% warm-up steps is applied. We use AdamW <ref type="bibr" target="#b59">(Loshchilov &amp; Hutter, 2019)</ref> (Î² 1 = 0.9, Î² 2 = 0.99) and a weight decay of 0.1 for training.      In each experiment we train for 100k steps -the samples are generated randomly, however, all experiments are trained and evaluated on the same samples.</p><p>Additional Formal Language Results. Figure <ref type="figure" target="#fig_11">12</ref> showcases supplementary results on formal language task, detailing tasks where no model attained a minimum scaled accuracy of 0.3. Although no model achieves proper extrapolation of the task to a larger context length, xLSTM performs best among the evaluated models.</p><p>Individual Task Description. The majority of tasks are based on <ref type="bibr" target="#b20">DelÃ©tang et al. (2023)</ref>. We provide the vocabulary size |V | and the random accuracy s rand (for accuracy scaling), used in the evaluation. As we evaluate different task lengths each task has a padding token which is used to pad the sequence to the given context length. In Listing 1 there is an example for each task.</p><p>â€¢ Bucket Sort Given a string of tokens of a sorted alphabet, compute the sorted string.</p><p>|V | = 11 s rand = 1</p><p>|V |-1</p><p>â€¢ Cycle Nav Given a string of "movement tokens" (+1, -1, STAY) compute the end position of the agent with start position 0. The position must be computed modulo the maximum position.</p><formula xml:id="formula_63">|V | = 9 s rand = 1 |V |-4</formula><p>â€¢ Even Pairs Given a binary string of a and b tokens, compute whether the number of ab and ba is even. This task can be solved by checking if the first and last token of the string are equal.</p><p>|V | = 3 s rand = 0.5</p><p>â€¢ Majority Given a string of tokens, compute the token that occurred most often in the sequence.</p><formula xml:id="formula_64">|V | = 64 s rand = 1 |V |-1</formula><p>â€¢ Majority Count Given a string of tokens of an ordered alphabet. Compute the count of the token that occurred most often in the sequence. If the count exceeds the vocab size, the highest vocab token should be outputted.</p><formula xml:id="formula_65">|V | = 64 s rand = 1 |V |-1</formula><p>â€¢ Missing Duplicate Given a string of tokens. The string is repeated but one of the tokens is masked in the repetition. Output the token that is masked.</p><formula xml:id="formula_66">|V | = 11 s rand = 1 |V |-2</formula><p>â€¢ Mod Arithmetic (w/o Brackets) Calculate the result -modulo the max number -of the arithmetic operations in the context. The maximum number is the vocabulary size minus the number of special tokens (+,-,*,=, [PAD]).</p><p>|V | = 10 s rand = 1</p><p>|V |-5</p><p>â€¢ Mod Arithmetic (w Brackets) Calculate the result -modulo the maximum number -of the arithmetic operations in the context. The maximum number is vocabulary size minus the number of special tokens (+,-,*,=,(,), [PAD]).</p><formula xml:id="formula_67">|V | = 12 s rand = 1 |V |-7</formula><p>â€¢ Odds First An string of tokens t 1 , t 2 , t 3 , ...t n is given. Output all tokens with and odd index (t 1 , t 3 , ...) then the token with an even index (t 2 , t 4 ,..) . Apart from that keep the ordering of the initial string.</p><formula xml:id="formula_68">|V | = 12 s rand = 1 |V |-2</formula><p>â€¢ Parity Given a binary string of a and b tokens, compute if the number of b's is even. If the number is even output a otherwise b. This is equivalent to sequentially calculating the half-adder sum.</p><p>|V | = 3 s rand = 0.5</p><p>â€¢ Repetition Given a string of tokens -repeat it.</p><formula xml:id="formula_69">|V | = 12 s rand = 1 |V |-2</formula><p>â€¢ Reverse String Given a string of tokens -repeat it in reverse order.</p><formula xml:id="formula_70">|V | = 12 s rand = 1 |V |-2</formula><p>â€¢ Stack Manipulation An initial stack content is given, followed by a sequence of push and pop operations. Compute the stack content after the operations Listing 1: Examples of the formal language tasks. Red tokens are evaluated for loss and accuracy metrics, but are padded for the input. The tokens are illustrated in a way that allows easy semantic interpretation for the given task -hence, some tokens are represented by multiple characters.</p><formula xml:id="formula_71">|V | = 11 s rand = 1 âŒŠ |V |-3 2 âŒ‹ â€¢ Set</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.2 Test of xLSTM's Memory Capacities on Associative Recall Tasks.</head><p>We test the memory capacity of xLSTM with the Multi-Query Associative Recall task proposed by <ref type="bibr" target="#b3">Arora et al. (2023)</ref>. Figure <ref type="figure" target="#fig_5">13</ref> illustrates the basic task setup.</p><p>Why Multi-Query Associative Recall for Memory Tests of LLM Architectures. Associative Recall (AR), the ability to retrieve a specific value (information) associated with a given key (information), constitutes a key capability for LLM to perform well <ref type="bibr" target="#b78">(Poli et al., 2024;</ref><ref type="bibr" target="#b3">Arora et al., 2023;</ref><ref type="bibr" target="#b70">Olsson et al., 2022)</ref>. Especially its quality of in-context learning seems to be strongly connected to this capability <ref type="bibr" target="#b70">(Olsson et al., 2022)</ref>. <ref type="bibr" target="#b3">Arora et al. (2023)</ref> attribute performance gaps between early non-Transformer and Transformer language models specifically to performance gaps in associative recall. They argue that prior AR evaluations fall short of capturing these differences and propose MQAR, which can show the AR performance differences that translate to performance differences in language modeling performance. Hence, MQAR is especially suitable to analyze the memory capacity of LLM. Transformer (e.g. Llama) models can be seen as the gold standard for this task as their memory is exponential in the coding dimension <ref type="bibr" target="#b86">(Ramsauer et al., 2021)</ref>.</p><p>Experiment Setup. There are two relevant variables that determine different experimental setups.</p><p>( In all experiments, we use two blocks (or layers for the pure LSTM) for all models. LSTM (Block) model refers to an architecture where a vanilla LSTM is used instead of self-attention inside a Transformer block.</p><p>For each task setup, we train each model with 4 different learning rates (batch size &gt; 24: {1e-2, 2.15e-3, 4.6e-4, 1e-4}, batch size 24: {1e-3, 2.2e-4, 5e-5, 1e-5}). The batch size (BS) changes depending on the context length (CL) (CL=64/128: BS=512; CL=256: BS=256; CL=756: BS=128; CL=1024: BS=96; CL=2048: BS=24). We vary the embedding dimension (Model Dim) between different experiments -different numbers of heads are used accordingly. For each experiment, we generate 100,000 training samples (validation: 3,000 samples) and train for 64 epochs. We apply cosine annealing (min lr: 1e-4 and 1e-5) with 10% warm-up steps. We use AdamW <ref type="bibr" target="#b59">(Loshchilov &amp; Hutter, 2019)</ref> and a weight decay of 0.1 for training.</p><p>We conduct three different experiments:</p><p>â€¢ MQAR-Experiment 1 evaluates, in the same fashion as</p><p>Arora et al. (2023), a variety of models (Llama, Mamba, Mamba (noWT) -i.e. without weight tying, Retention, Hyena, H3, RWKV-4, RWKV-5, RWKV-6, LSTM, LSTM (Block), xLSTM[0:1], xLSTM[1:0] and xLSTM[1:1]) on increasing task difficulty by increasing the context length and number of key-value pairs simultaneously. We benchmark three parameter settings: CL,KV={(64,4),(128,8),(256,16)}. â€¢ MQAR-Experiment 2 increases the task difficulty notably and goes beyond previous evaluations on this task. We individually scale the context length (CL={756, 1024, 2048}) and the key-value pairs (KV={48, 96, 256}) and evaluate all combinations. This experiment especially probes the memory capacity because the number of key-value pairs is high.</p><p>To reduce the computational burden we only evaluate models that perform flawlessly in Experiment 1 -additionally we evaluate Transformer only in the hardest setting (CL=2048) as sanity check, because no performance decrease is expected. â€¢ MQAR-Experiment 3 analyzes whether the AR capability learned on a certain context length extrapolates to bigger context lengths. For each KV setting of Experiment 2, we use the models (we select the 3 biggest model dimensions) trained on CL=2048 and evaluate bigger context lengths (CL={4096, 6144, 8192}).</p><p>Extended Results. The result of Experiment 1 can be found in The results of Experiment 2 are presented in Figure <ref type="figure" target="#fig_5">15</ref>. Scaling the context length has a low impact on the performance of the models. However, while xLSTM[1:1] and xLSTM[1:0] show no clear decay, both RWKV variants slightly, but consistently lose performance with increasing context lengths. The varying number of key-value pairs, which mainly probes the memory capacity of the non-Transformer models, has a more notable impact across all models. RWKV-5 seems to outperform RWKV-6. The latter fails to learn the task at all in some KV=256 settings. Overall xLSTM[1:1] is the best-performing non-Transformer model -suggesting that it provides enhanced memory capacity, also in long contexts. We assess the performance of xLSTM across tasks in the Long Range Arena benchmark <ref type="bibr" target="#b108">(Tay et al., 2021)</ref>, examining its ability to effectively handle longer context lengths and diverse data types.</p><p>Our experiments on Long Range Arena benchmark are composed of five tasks:</p><p>â€¢ Retrieval: The task is to predict if two documents have a citation link. The dataset of text documents is derived from the ACL Anthology Network <ref type="bibr" target="#b81">(Radev et al., 2009)</ref>. â€¢ ListOps: This is a set of modular arithmetic tasks including brackets and lists of numbers, using the operations MIN, MAX, MEDIAN and SUMMOD (modular sum). A particular example is:</p><formula xml:id="formula_73">[MAX 4 3 [MIN 2 3 ] 1 0 [MEDIAN 1 5 8 9, 2]] - â†’<label>5</label></formula><p>â€¢ Image: This task is based on a version of the CIFAR dataset <ref type="bibr" target="#b51">(Krizhevsky, 2009)</ref>, where images are transformed to a sequence of pixels and this sequence has to be classified into the usual CIFAR classes. We test both a gray-scale (G-Image) and RGB (RGB-Image) version of this dataset, as <ref type="bibr">Orvieto et al. (2023)</ref> uses colored images contrary to the standard setup. â€¢ Pathfinder: The input for this task is a 32x32 gray-scale image, given as pixel sequence, with two dots and several curved lines on it. The task is to predict if the two dots are connected by any of the lines <ref type="bibr" target="#b58">(Linsley et al., 2018)</ref>.</p><p>We omit the Text classification task <ref type="bibr" target="#b61">(Maas et al., 2011)</ref>, as the language modeling experiments already test this kind of data, and the Pathfinder-X version of Pathfinder.</p><p>Experiment Setup. The architectures that are tested in this experiment comprise Llama, Mamba, LSTM, RWKV-4, and xLSTM. LSTM (Block) refers to an architecture where a vanilla LSTM is used inside a post up-projection block (like Transformer with attention replaced by LSTM). For xLSTM we choose the best performing of xLSTM[0:1] or xLSTM[1:0] on the validation set, specifically the former for the Image tasks and the latter for all other ones.</p><p>We use the hyperparameter settings of the S5 model <ref type="bibr" target="#b98">(Smith et al., 2022)</ref> and Linear Recurrent Unit model <ref type="bibr">(Orvieto et al., 2023)</ref>, with additional hyperparamter search on learning rates and schedulers for all models. We use two different schedulers: Linear Warm-up Cosine Annealing and Linear Warm-up Cosine Annealing with Restarts. Both learning rate schedulers were evaluated with learning rates of 1e-3, 6e-4 and 1e-4. For the second scheduler, the number of restarts (R) is set to 3. The model hyperparameters for each dataset are displayed in Table <ref type="table">5</ref>.</p><p>Results. Table <ref type="table" target="#tab_11">6</ref> shows the result of experiments on the Long Range Arena benchmark. xLSTM demonstrates consistent strong performance on all of the tasks, suggesting that the proposed architecture is remarkably efficient in handling different aspects of long context problems.  General Training Procedure. We tokenize our datasets using the HuggingFace GPT-2 tokenizer <ref type="bibr" target="#b83">(Radford et al., 2019;</ref><ref type="bibr" target="#b9">Brown et al., 2020)</ref> <ref type="foot" target="#foot_4">foot_4</ref> and use this tokenizer for all models in this paper. In general, we try to follow <ref type="bibr" target="#b9">Brown et al. (2020)</ref> for the general training setup, i.e. we choose context length 2048 and batch sizes 256 or 512 for our models. We use the AdamW <ref type="bibr" target="#b59">(Loshchilov &amp; Hutter, 2019)</ref> optimizer with beta parameters (Î² 1 , Î² 2 )=(0.9, 0.95) and an epsilon parameter of 1e-5. As learning rate scheduler we use a linear warm-up with 750 steps and cosine decay to 10% of the peak learning rate. We apply a weight decay of 0.1 to all our models and always exclude the token embedding matrix from weight decay. If not specified otherwise, we do not tie the weights of the token embedding and the language model head. For parallelization, we use PyTorch FSDP in SHARD_GRAD_OP mode with mixed precision in bfloat16, where applicable. For small models we use NO_SHARD. We keep the weights in float32 and reduce the gradients across GPUs in float32.</p><p>We use torch.compile to speed up models, except for Transformer models as their training curves did not match the non-compiled versions. For xLSTM[7:1], we use positions <ref type="bibr">[3,</ref><ref type="bibr">5,</ref><ref type="bibr">7,</ref><ref type="bibr">40,</ref><ref type="bibr">42,</ref><ref type="bibr">44]</ref> for sLSTM-based blocks, except for the 125M size, where we use <ref type="bibr">[3,</ref><ref type="bibr">20]</ref> (this is actually a [11:1] ratio).</p><p>We do not use any positional encoding for our xLSTM models.</p><p>Details on Comparison to Other Methods. For the model comparison on 15B training tokens of SlimPajama we train all models with context length 2048 and batch size 256. We use a peak learning rate of 1e-3 for all models for comparability. The learning rate decays over 30k training steps. The models are compared after one epoch at training step 28170. As model implementations we use the original repositories' code for Mamba <ref type="bibr">(Gu &amp; Dao, 2023)</ref> <ref type="foot" target="#foot_5">foot_5</ref> , RWKV-5, RWKV-6 (Peng  et al., 2024) 7 . For RWKV-4 we use a cleaned and validated re-implementation based on the original repo and kernels <ref type="bibr" target="#b75">(Peng et al., 2023)</ref>. In our RWKV-4 implementation we enable weight decay on all parameters except biases, the token embedding weight and all LayerNorm weights. For HGRN <ref type="bibr" target="#b79">(Qin et al., 2023)</ref>, GLA <ref type="bibr" target="#b121">(Yang et al., 2023)</ref>, HGRN2 <ref type="bibr" target="#b80">(Qin et al., 2024)</ref> we use the a re-implementation flash-linear-attention <ref type="bibr" target="#b120">(Yang &amp; Zhang, 2024)</ref> by the authors of GLA <ref type="bibr" target="#b121">(Yang et al., 2023;</ref><ref type="bibr" target="#b120">Yang &amp; Zhang, 2024)</ref> <ref type="foot" target="#foot_7">foot_7</ref> . For GPT-3 and Llama-like Transformers, we use our own implementations based on PyTorch. Note that for all xLSTMs, Transformers, Mamba and RWKV-4, we use Mixed Precision training with bfloat16 and weights in float32 precision. Following the general training procedure we use torch.compile for all models, except for Transformers and models using the flash-linear-attention library because of compilation problems.</p><p>As RWKV-6 performs worse than RWKV-5, we also train a model with peak learning rate 4e-4, as reported in the original repository for 350M parameter models <ref type="foot" target="#foot_8">9</ref> . This model reaches a perplexity of 16.38, worse than the 15.03 for the standard peak learning rate 1e-3 as reported in Table <ref type="table">1</ref>. Similarly, we tested the repository learning rates for other model sizes and all performed worse than the ones we also use for xLSTM (see Table <ref type="table">7</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Details on Training Precision for Baselines.</head><p>For models from flash-linear-attention and RWKV-5/6 models we found that PyTorch automatic mixed precision training did not work, but casting the model weights to float32 initially with FSDP parameter precision bfloat16 led to a working configuration. In this setting, models perform better than in full bfloat16 training, where the weights are casted to bfloat16 initially as well. Full float32 training did not work because of the custom kernels that require bfloat16.</p><p>General Details on Ablation Studies. We follow our general training procedure and train all models with context length 2048, batch size 256 and peak learning rate 1e-3. We report perplexity values on the validation set.</p><p>Additional Ablation Study on Matrix Memory. As default block configuration we use the mLSTM in the pre up-projection block (see Figure <ref type="figure" target="#fig_4">11</ref>) and the sLSTM in the post up-projection block (see Figure <ref type="figure" target="#fig_5">10</ref>). In this experiment we study combination of mLSTM with different block variants using the xLSTM[1:0] architecture. We compare the mLSTM in a post up-projection block (see Figure <ref type="figure" target="#fig_0">3</ref> and 10) with ReLU 2 activation function and non-gated feed-forward network to mLSTM in a pre up-projection block with and without a dimension-wise causal convolution. Table <ref type="table" target="#tab_12">8</ref> shows that the matrix memory benefits from the pre up-projection block structure, and that the convolution within this block is important.  <ref type="bibr">Vaswani et al.)</ref>, which corresponds to the post up-projection backbone (see Figure <ref type="figure" target="#fig_0">3</ref>) further boosts performance. Adding Exponential Gating to this architecture yields the sLSTM as depicted in Figure <ref type="figure" target="#fig_5">10</ref>, with another large performance improvement. Finally, adding the best Matrix Memory variant found in Table <ref type="table" target="#tab_12">8</ref> by replacing some sLSTM blocks with the mLSTM (see Figure <ref type="figure" target="#fig_4">11</ref>) gives xLSTM[7:1] with the best performance.</p><p>Details on Gating Technique Ablation Study. In Table <ref type="table">2</ref> (bottom), we investigate the effect of trainable and input-dependent gates for mLSTM. The results show that, in contrast to other methods <ref type="bibr" target="#b45">(Katharopoulos et al., 2020;</ref><ref type="bibr" target="#b103">Sun et al., 2023;</ref><ref type="bibr" target="#b79">Qin et al., 2023;</ref><ref type="bibr" target="#b46">Katsch, 2023;</ref><ref type="bibr" target="#b121">Yang et al., 2023;</ref><ref type="bibr" target="#b80">Qin et al., 2024;</ref><ref type="bibr" target="#b76">Peng et al., 2024)</ref>, having the gates both learnable and input dependent gives the best results.</p><p>Details on Scaling Experiments. We follow our general training procedure (see paragraph above) and train all models, including the 1.3B and 2.7B model sizes, with context length 2048 and batch size 256. We use the peak learning rates from Table <ref type="table">7</ref>. For Llama and Mamba we use the learning rates reported by <ref type="bibr">Gu &amp; Dao (2023)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 xLSTM Large Language Models -SlimPajama300B</head><p>General Training Procedure. We use the same general training procedure as in Section B.2 with peak learning rates from Table <ref type="table">7</ref>. For Llama and Mamba we use the learning rates reported by <ref type="bibr">Gu &amp; Dao (2023)</ref>. All models are trained with context length 2048. The 125M, 350M and 760M models are trained with batch size 256 for 600k training steps, whereas the 1.3B models are trained with batch size 512 for 300k training steps. We keep the same learning rate scheduler across all models.</p><p>Details on Downstream Evaluation. We use the LM Evaluation Harness from EleutherAI <ref type="bibr" target="#b104">(Sutawika et al., 2023)</ref> for evaluating the following tasks that measure common sense reasoning: LAMBADA (OpenAI version in LM Evaluation Harness) <ref type="bibr" target="#b73">(Paperno et al., 2016)</ref>, HellaSwag <ref type="bibr" target="#b124">(Zellers et al., 2019)</ref>, PIQA <ref type="bibr" target="#b7">(Bisk et al., 2020)</ref>, ARC-challenge, ARC-easy <ref type="bibr" target="#b13">(Clark et al., 2018)</ref>, WinoGrande <ref type="bibr" target="#b90">(Sakaguchi et al., 2021)</ref>. This selection of downstream tasks is also used in previous work by <ref type="bibr">Gu &amp; Dao (2023)</ref>.</p><p>Table <ref type="table">9</ref>: Perplexity values per domain.</p><p>In order to evaluate the perplexity values on each data source, we split the text documents into sequences of length 2048, which corresponds to the pre-training context length of all models. For documents longer than 2048 tokens we split each document into non-overlapping input sequences. In this case for the last input sequence, we follow the LM Evaluation Harness and fill up the full 2048 token context window with previous tokens, but compute the perplexity only on the remaining tokens.</p><p>We compute the token perplexities per data source in Table <ref type="table">4</ref> as the exponential of the negative loglikelihoods per domain weighted by the number of tokens per domain in that data source as it is defined in <ref type="bibr">Magnusson et al. (2023, Equation</ref>  <ref type="formula" target="#formula_0">1</ref>)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Detailed Results on PALOMA Language Model Evaluation</head><p>We report the perplexity values on each of the 571 subdomains of PALOMA in Table <ref type="table" target="#tab_13">10</ref>. Note that the aggregated perplexity values in Table <ref type="table">4</ref> are not macro averages of the values shown in Table <ref type="table" target="#tab_13">10</ref>. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: xLSTM blocks. Left: A residual sLSTM block with post up-projection (like Transformers):The input is fed into an sLSTM -with an optional convolution -followed by a gated MLP. Right: A residual mLSTM block with pre up-projection (like State Space models): mLSTM is wrapped inside two MLPs, via a convolution, a learnable skip connection, and an output gate that acts componentwise. See Figure10and Figure11in the appendix for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: Test of xLSTM's exponential gating with memory mixing. Results are given by the scaled accuracy of different models at solving formal language tasks, of which some require state tracking. The different tasks are grouped by the Chomsky hierarchy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Inference Generative Speed. Left: Generation times of different 1.3B models for a pre-fill context of 16 tokens (to mitigate cache initialization). The recurrent models (xLSTM[1:0], xLSTM[7:1], Mamba and RWKV-4) show linear behavior, whereas the Transformer (Llama) inference/decoding time is quadratic in sequence length. Right: Token throughput for different batch sizes on a A100-80GB GPU for 1.3B sized models. Note that the Transformer / Llama model goes out of memory (OOM) already for small batch sizes, whereas xLSTM and Mamba can sustain very large batch sizes. xLSTM[1:0] consistently outperforms Mamba in throughput.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><figDesc>105)This completes the backward pass of the parallel mLSTM for a full input sequence X âˆˆ R T Ã—d .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 11 :</head><label>11</label><figDesc>Figure11: Schematic representation of an mLSTM block -pre up-projection: Embedded in a pre-LayerNorm residual structure, the input is up-projected first with projection factor 2, once for an externalized output gate and once as input for the mLSTM cells. The mLSTM cell input is dimension-wise causally convoluted (kernel size 4), before entering a learnable skip connection. We obtain input q and k via block-diagonal projection matrices of block size 4. The values v are fed directly, skipping the convolution part. After the mLSTM sequence mixing, outputs are normalized via GroupNorm<ref type="bibr" target="#b118">(Wu &amp; He, 2018</ref>) -a head-wise layer norm for each of the four heads. Finally, the learnable skip input is added and the result is gated component-wise with the external output gate. The output is down-projected.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>B. 1</head><label>1</label><figDesc>Synthetic Tasks and Long Range Arena B.1.1 Test of xLSTM's Exponential Gating with Memory Mixing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 :</head><label>12</label><figDesc>Figure12: Supplementary results given by scaled accuracy of different models at solving formal language tasks. Tasks are grouped by the Chomsky hierarchy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>|V |- 2 â€¢|V</head><label>2</label><figDesc>Given a string of tokens, compute the ordered set of the tokens. Keep the ordering so that tokens that occurred first are also outputted first.|V | = 128 s rand = 1Solve Equation Given is an equation with the operators {+,-,*,=,(,)}, number, and an unknown variable x. Compute the value of the variable modulo the max number. The maximum number is vocabulary size minus the number of special tokens (+,-,*,=,(,), [PAD],[ACT]). |V | = 14 s rand = 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>)</head><figDesc>Context Length (CL): Length of the sequence of one sample -this influences the distances between the key-value definition and the recall. (2) Number Key-Value Pairs (KV): Influences how many key-value pairs the model needs to keep track of. The vocabulary size is always 8192.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 14 .</head><label>14</label><figDesc>In accordance to the results of<ref type="bibr" target="#b3">Arora et al. (2023)</ref> H3, Hyena, RWKV-4 fail to solve the task with a smaller model dimension. In contrast, xLSTM[1:1], xLSTM[1:0], Mamba, RWKV-5 and RWKV-6 are able to solve these settings for all model dimensions. The comparison of xLSTM[0:1] with both original LSTM variants indicates that the exponential gating mechanism improves the AR capabilities of the model. However, both fall short because of the reduced memory capacity compared to xLSTM[1:1] and xLSTM[1:0].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 16 Figure 13 :</head><label>1613</label><figDesc>Figure16shows the extrapolation results from Experiment 3. For xLSTM[1:1], xLSTM[1:0], and Mamba the model performance does not change in the extrapolation setting. The RWKV models (especially RWKV5) degrade slightly with increasing context length. xLSTM[1:1] performs best, as it maintains its superior performance of Experiment 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: Result of MQAR-Experiment 3 (Extrapolation). All evaluated models were trained on context length 2048 and the number of key-value pairs given by the columns of the plot. The rows show the different context lengths used in the evaluation. The x-axis gives the model size and the y-axis the validation accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Mamba, in 486 out of 571 (85.1%) a lower perplexity than Llama, in 570 out of 571 (99.8%) a lower perplexity than RWKV-4. For details see Appendix B.3.</figDesc><table><row><cell></cell><cell>Model</cell><cell>#Params M</cell><cell>C4</cell><cell>MC4 EN</cell><cell>Wikitext 103</cell><cell>Penn Treebank</cell><cell>Red Pajama</cell><cell>Refined Web</cell><cell>Dolma</cell><cell>M2D2 S2ORC</cell><cell>M2D2 Wikipedia</cell><cell>C4 Domains</cell><cell>Dolma Subreddits</cell><cell>Dolma Coding</cell><cell>Average</cell></row><row><cell></cell><cell>RWKV-4</cell><cell>169.4</cell><cell cols="2">26.25 22.33</cell><cell>29.18</cell><cell>38.45</cell><cell>8.99</cell><cell>32.47</cell><cell>17.04</cell><cell>23.86</cell><cell>21.42</cell><cell>22.68</cell><cell>37.08</cell><cell>5.12</cell><cell>23.74</cell></row><row><cell>125M</cell><cell>Llama Mamba xLSTM[1:0]</cell><cell>162.2 167.8 163.8</cell><cell cols="2">24.64 17.23 23.12 17.04 22.54 16.32</cell><cell>23.16 22.49 21.98</cell><cell>31.56 30.63 30.47</cell><cell>8.26 7.96 7.80</cell><cell>29.15 27.73 27.21</cell><cell>15.10 14.60 14.35</cell><cell>19.71 19.38 19.02</cell><cell>20.41 19.36 19.04</cell><cell>21.45 20.14 19.65</cell><cell>36.73 34.32 34.15</cell><cell>3.61 3.77 3.64</cell><cell>20.92 20.05 19.68</cell></row><row><cell></cell><cell>xLSTM[7:1]</cell><cell>163.7</cell><cell cols="2">22.39 16.13</cell><cell>21.47</cell><cell>30.01</cell><cell>7.75</cell><cell>26.91</cell><cell>14.13</cell><cell>18.6</cell><cell>18.84</cell><cell>19.52</cell><cell>33.9</cell><cell>3.59</cell><cell>19.44</cell></row><row><cell></cell><cell>RWKV-4</cell><cell>430.5</cell><cell cols="2">19.55 15.82</cell><cell>19.64</cell><cell>27.58</cell><cell>6.97</cell><cell>24.28</cell><cell>12.94</cell><cell>17.59</cell><cell>15.96</cell><cell>16.98</cell><cell>29.40</cell><cell>3.90</cell><cell>17.55</cell></row><row><cell>350M</cell><cell>Llama Mamba xLSTM[1:0]</cell><cell>406.6 423.1 409.3</cell><cell cols="2">18.38 13.28 17.33 13.05 17.01 12.55</cell><cell>16.41 16.11 15.17</cell><cell>21.82 22.24 22.51</cell><cell>6.56 6.34 6.20</cell><cell>22.09 21.04 20.66</cell><cell>11.76 11.42 11.16</cell><cell>15.05 14.83 14.44</cell><cell>15.25 14.53 14.27</cell><cell>15.99 15.16 14.85</cell><cell>28.30 27.02 26.70</cell><cell>3.12 3.20 3.08</cell><cell>15.67 15.19 14.88</cell></row><row><cell></cell><cell>xLSTM[7:1]</cell><cell>408.4</cell><cell cols="2">16.98 12.68</cell><cell>15.43</cell><cell>21.86</cell><cell>6.23</cell><cell>20.70</cell><cell>11.22</cell><cell>14.62</cell><cell>14.30</cell><cell>14.85</cell><cell>26.61</cell><cell>3.11</cell><cell>14.88</cell></row><row><cell></cell><cell>RWKV-4</cell><cell>891.0</cell><cell cols="2">15.51 12.76</cell><cell>14.84</cell><cell>21.39</cell><cell>5.91</cell><cell>19.28</cell><cell>10.70</cell><cell>14.27</cell><cell>13.04</cell><cell>13.68</cell><cell>24.22</cell><cell>3.32</cell><cell>14.08</cell></row><row><cell>760M</cell><cell>Llama Mamba xLSTM[1:0]</cell><cell>834.1 870.5 840.4</cell><cell cols="2">15.75 11.59 15.08 11.54 14.60 11.03</cell><cell>13.47 13.47 12.61</cell><cell>18.33 19.34 17.74</cell><cell>5.82 5.69 5.52</cell><cell>19.04 18.43 17.87</cell><cell>10.33 10.15 9.85</cell><cell>13.00 13.05 12.50</cell><cell>13.05 12.62 12.20</cell><cell>13.76 13.25 12.81</cell><cell>24.80 23.94 23.46</cell><cell>2.90 2.99 2.87</cell><cell>13.49 13.30 12.76</cell></row><row><cell></cell><cell>xLSTM[7:1]</cell><cell>839.7</cell><cell cols="2">14.72 11.11</cell><cell>12.68</cell><cell>17.61</cell><cell>5.55</cell><cell>18.01</cell><cell>9.87</cell><cell>12.59</cell><cell>12.25</cell><cell>12.89</cell><cell>23.43</cell><cell>2.88</cell><cell>12.80</cell></row><row><cell></cell><cell>RWKV-4</cell><cell cols="3">1515.2 14.51 12.04</cell><cell>13.73</cell><cell>19.37</cell><cell>5.62</cell><cell>18.25</cell><cell>10.11</cell><cell>13.46</cell><cell>12.10</cell><cell>12.87</cell><cell>22.85</cell><cell>3.25</cell><cell>13.18</cell></row><row><cell>1.3B</cell><cell cols="4">Llama Mamba xLSTM[1:0] 1422.6 13.13 10.09 1420.4 13.93 10.44 1475.3 13.35 10.40</cell><cell>11.74 11.76 11.41</cell><cell>15.92 16.65 15.92</cell><cell>5.29 5.21 5.10</cell><cell>17.03 16.50 16.25</cell><cell>9.35 9.17 9.01</cell><cell>11.61 11.73 11.43</cell><cell>11.53 11.18 10.95</cell><cell>12.24 11.83 11.60</cell><cell>22.63 21.43 21.29</cell><cell>2.74 2.83 2.73</cell><cell>12.04 11.84 11.58</cell></row><row><cell></cell><cell cols="4">xLSTM[7:1] 1420.1 13.31 10.21</cell><cell>11.32</cell><cell>16.00</cell><cell>5.16</cell><cell>16.48</cell><cell>9.11</cell><cell>11.61</cell><cell>11.10</cell><cell>11.76</cell><cell>21.50</cell><cell>2.75</cell><cell>11.69</cell></row><row><cell cols="16">Table 4: Performance on PALOMA Language Modeling Tasks. Comparison of xLSTM, RWKV-4,</cell></row><row><cell cols="16">Llama, and Mamba by the perplexity of next token prediction on the PALOMA language benchmark</cell></row><row><cell cols="16">after training on 300B tokens from SlimPajama. Model sizes are 125M, 250M, 760M, and 1.3B.</cell></row><row><cell cols="16">The second column shows the actual number of parameters. The 571 text domains are grouped into</cell></row><row><cell cols="16">language modeling (next seven columns) and fine-grained domain benchmarks (further 5 columns).</cell></row></table><note><p>Validation set perplexity and downstream tasks. Comparison of xLSTM, RWKV-4, Llama, and Mamba on the validation set at next token prediction and on downstream tasks after training on 300B tokens from SlimPajama. Model sizes are 125M, 250M, 760M, and 1.3B. The first column shows the methods and the second the actual number of parameters. The third column lists the validation set perplexities, while the remaining columns show the performance on downstream tasks. Best model per model size is depicted bold and the second best is underlined. In the vast majority of tasks and across all model sizes xLSTM is the best method -only on the ARC task Mamba is in some cases the best method. xLSTM[1:0] and xLSTM[7:1] are the two best models with respect to validation set perplexity.</p><p>The last column shows the average perplexity across all of these tasks. Best model per model size is given in bold and the second best is underlined. xLSTM yields the best performance. than</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Long Range Arena test accuracy. Bold highlights the best performing model, underlined the second best. X denotes models that fail to outperform random baselines. xLSTM is the best of xLSTM[1:0], xLSTM[0:1] based on validation dataset accuracy.B.2 Method Comparison and Ablation Study on SlimPajama (15B)</figDesc><table><row><cell></cell><cell>.500</cell><cell>0.100</cell><cell>0.500</cell><cell>0.100</cell><cell>0.100</cell><cell></cell></row><row><cell>Llama</cell><cell>0.845</cell><cell>0.379</cell><cell>0.887</cell><cell>0.541</cell><cell>0.629</cell><cell>5.2</cell></row><row><cell>Mamba</cell><cell>0.902</cell><cell>0.325</cell><cell>0.992</cell><cell>0.689</cell><cell>0.765</cell><cell>2.2</cell></row><row><cell>RWKV-4</cell><cell>0.898</cell><cell>0.389</cell><cell>0.914</cell><cell>0.691</cell><cell>0.757</cell><cell>3.0</cell></row><row><cell>LSTM</cell><cell>X</cell><cell>0.275</cell><cell>X</cell><cell>0.675</cell><cell>0.718</cell><cell>5.4</cell></row><row><cell>LSTM (Block)</cell><cell>0.880</cell><cell>0.495</cell><cell>X</cell><cell>0.690</cell><cell>0.756</cell><cell>3.4</cell></row><row><cell>xLSTM</cell><cell>0.906</cell><cell>0.411</cell><cell>0.919</cell><cell>0.695</cell><cell>0.761</cell><cell>1.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Matrix Memory variants. We study different configurations for the matrix memory. Matrix memory in the pre up-projection block performs best and gives xLSTM[1:0]. Notably, it seems that the dimension-wise causal convolution within the pre up-projection block is important.Details on new xLSTM Components Ablation Study. In Table2(top), we show our modifications to the vanilla LSTM that transform the vanilla LSTM into the xLSTM. We start with a large default PyTorch LSTM with 24 layers and 1536 hidden size. Due to a lack of skip-connections and LayerNorms, vanilla LSTMs of this size are not trainable. We then add skip-connections and pre-LayerNorms before each LSTM layer corresponding to a residual architecture. This enables training for LSTMs at this scale. Replacing every second LSTM layer by a non-gated feed-forward network with GeLU activation function (similar to</figDesc><table><row><cell>Model</cell><cell>Details</cell><cell>#Blocks</cell><cell>Embedding Dim</cell><cell>#Params M</cell><cell>SlimPajama (15B) ppl â†“</cell></row><row><cell></cell><cell>Post Up-Projection Block (ReLU2)</cell><cell>24</cell><cell>1024</cell><cell>430.4</cell><cell>13.90</cell></row><row><cell>xLSTM[1:0]</cell><cell>Pre Up-Projection Block, No Convolution</cell><cell>48</cell><cell>1024</cell><cell>408.8</cell><cell>15.41</cell></row><row><cell></cell><cell>Pre Up-Projection Block, With Convolution</cell><cell>48</cell><cell>1024</cell><cell>409.3</cell><cell>13.43</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 :</head><label>10</label><figDesc>PPL Evaluations: For the 1.3B sized models trained on 300B SlimPajama tokens, these are the detailed evaluation results on the respective validation datasets.</figDesc><table><row><cell>Dataset</cell><cell>Llama</cell><cell>Mamba</cell><cell cols="3">RWKV-4 xLSTM[7:1] xLSTM[1:0]</cell></row><row><cell>#Params (M)</cell><cell>1420</cell><cell>1475</cell><cell>1515</cell><cell>1420</cell><cell>1423</cell></row><row><cell>4chan_meta_sep_val-00000000</cell><cell>9.58</cell><cell>9.72</cell><cell>11.37</cell><cell>9.53</cell><cell>9.55</cell></row><row><cell>4chan_meta_sep_val-00000001</cell><cell>9.95</cell><cell>10.06</cell><cell>11.57</cell><cell>9.91</cell><cell>9.88</cell></row><row><cell>4chan_meta_sep_val-00000002</cell><cell>9.42</cell><cell>9.53</cell><cell>11.00</cell><cell>9.40</cell><cell>9.38</cell></row><row><cell>4chan_meta_sep_val-00000003</cell><cell>9.78</cell><cell>9.93</cell><cell>11.48</cell><cell>9.77</cell><cell>9.77</cell></row><row><cell>c4_100dom_val_100_www.ign.com</cell><cell>16.22</cell><cell>15.75</cell><cell>17.10</cell><cell>15.67</cell><cell>15.43</cell></row><row><cell>c4_100dom_val_10_www.eventbrite.com</cell><cell>12.72</cell><cell>12.33</cell><cell>13.33</cell><cell>12.30</cell><cell>12.12</cell></row><row><cell>c4_100dom_val_11_link.springer.com</cell><cell>8.66</cell><cell>8.54</cell><cell>9.31</cell><cell>8.42</cell><cell>8.33</cell></row><row><cell>c4_100dom_val_12_www.chicagotribune.com</cell><cell>12.09</cell><cell>11.60</cell><cell>12.49</cell><cell>11.55</cell><cell>11.37</cell></row><row><cell>c4_100dom_val_13_www.foxnews.com</cell><cell>9.59</cell><cell>9.21</cell><cell>9.83</cell><cell>9.16</cell><cell>9.08</cell></row><row><cell>c4_100dom_val_14_www.aljazeera.com</cell><cell>10.97</cell><cell>10.61</cell><cell>11.31</cell><cell>10.50</cell><cell>10.40</cell></row><row><cell>c4_100dom_val_15_www.dailymail.co.uk</cell><cell>12.42</cell><cell>11.97</cell><cell>12.87</cell><cell>11.85</cell><cell>11.69</cell></row><row><cell>c4_100dom_val_16_www.ncbi.nlm.nih.gov</cell><cell>7.39</cell><cell>7.31</cell><cell>7.98</cell><cell>7.11</cell><cell>7.07</cell></row><row><cell>c4_100dom_val_17_www.express.co.uk</cell><cell>11.57</cell><cell>11.04</cell><cell>11.84</cell><cell>10.99</cell><cell>10.79</cell></row><row><cell>c4_100dom_val_18_en.m.wikipedia.org</cell><cell>9.28</cell><cell>8.95</cell><cell>9.52</cell><cell>8.89</cell><cell>8.80</cell></row><row><cell>c4_100dom_val_19_www.cnet.com</cell><cell>12.61</cell><cell>12.23</cell><cell>13.12</cell><cell>12.09</cell><cell>11.97</cell></row><row><cell>c4_100dom_val_1_www.nytimes.com</cell><cell>13.13</cell><cell>12.66</cell><cell>14.04</cell><cell>12.68</cell><cell>12.44</cell></row><row><cell>c4_100dom_val_20_www.telegraph.co.uk</cell><cell>13.71</cell><cell>13.10</cell><cell>14.28</cell><cell>13.06</cell><cell>12.88</cell></row><row><cell>c4_100dom_val_21_www.theatlantic.com</cell><cell>14.70</cell><cell>14.17</cell><cell>15.54</cell><cell>14.17</cell><cell>13.97</cell></row><row><cell>c4_100dom_val_22_forums.macrumors.com</cell><cell>17.77</cell><cell>17.34</cell><cell>19.15</cell><cell>17.22</cell><cell>16.95</cell></row><row><cell>c4_100dom_val_23_www.oreilly.com</cell><cell>13.36</cell><cell>12.99</cell><cell>14.31</cell><cell>13.02</cell><cell>12.88</cell></row><row><cell>c4_100dom_val_24_www.washingtonpost.com</cell><cell>12.06</cell><cell>11.58</cell><cell>12.98</cell><cell>11.64</cell><cell>11.41</cell></row><row><cell>c4_100dom_val_25_www.zdnet.com</cell><cell>13.22</cell><cell>12.86</cell><cell>13.80</cell><cell>12.78</cell><cell>12.61</cell></row><row><cell>c4_100dom_val_26_www.foxbusiness.com</cell><cell>9.32</cell><cell>9.03</cell><cell>9.58</cell><cell>8.92</cell><cell>8.81</cell></row><row><cell>c4_100dom_val_27_www.reuters.com</cell><cell>10.67</cell><cell>10.13</cell><cell>11.16</cell><cell>10.13</cell><cell>9.97</cell></row><row><cell>c4_100dom_val_28_www.ibtimes.co.uk</cell><cell>11.36</cell><cell>11.01</cell><cell>11.71</cell><cell>10.89</cell><cell>10.76</cell></row><row><cell>c4_100dom_val_29_www.rt.com</cell><cell>13.59</cell><cell>12.96</cell><cell>14.24</cell><cell>12.98</cell><cell>12.74</cell></row><row><cell>c4_100dom_val_2_en.wikipedia.org</cell><cell>10.75</cell><cell>10.45</cell><cell>11.32</cell><cell>10.32</cell><cell>10.19</cell></row><row><cell>c4_100dom_val_30_www.prweb.com</cell><cell>11.18</cell><cell>10.88</cell><cell>11.92</cell><cell>10.83</cell><cell>10.65</cell></row><row><cell>c4_100dom_val_31_www.deviantart.com</cell><cell>21.78</cell><cell>21.05</cell><cell>22.78</cell><cell>21.00</cell><cell>20.69</cell></row><row><cell>c4_100dom_val_32_www.si.com</cell><cell>11.49</cell><cell>11.00</cell><cell>11.92</cell><cell>10.90</cell><cell>10.76</cell></row><row><cell>c4_100dom_val_33_www.bbc.com</cell><cell>9.35</cell><cell>8.91</cell><cell>9.41</cell><cell>8.80</cell><cell>8.70</cell></row><row><cell>c4_100dom_val_34_github.com</cell><cell>11.57</cell><cell>11.49</cell><cell>12.94</cell><cell>11.40</cell><cell>11.28</cell></row><row><cell>c4_100dom_val_35_nypost.com</cell><cell>14.31</cell><cell>13.41</cell><cell>15.29</cell><cell>13.62</cell><cell>13.31</cell></row><row><cell>c4_100dom_val_36_itunes.apple.com</cell><cell>16.49</cell><cell>15.88</cell><cell>17.15</cell><cell>15.98</cell><cell>15.69</cell></row><row><cell>c4_100dom_val_37_www.instructables.com</cell><cell>16.75</cell><cell>16.33</cell><cell>17.73</cell><cell>16.28</cell><cell>15.97</cell></row><row><cell>c4_100dom_val_38_www.youtube.com</cell><cell>8.42</cell><cell>8.24</cell><cell>8.83</cell><cell>8.22</cell><cell>8.07</cell></row><row><cell>c4_100dom_val_39_www.booking.com</cell><cell>8.84</cell><cell>8.49</cell><cell>8.83</cell><cell>8.41</cell><cell>8.32</cell></row><row><cell>c4_100dom_val_40_www.etsy.com</cell><cell>11.93</cell><cell>11.66</cell><cell>12.66</cell><cell>11.52</cell><cell>11.43</cell></row><row><cell>c4_100dom_val_41_www.marketwired.com</cell><cell>7.66</cell><cell>7.47</cell><cell>7.88</cell><cell>7.33</cell><cell>7.27</cell></row><row><cell>c4_100dom_val_42_sites.google.com</cell><cell>14.23</cell><cell>13.81</cell><cell>14.91</cell><cell>13.68</cell><cell>13.51</cell></row><row><cell>c4_100dom_val_43_www.baltimoresun.com</cell><cell>11.57</cell><cell>11.16</cell><cell>11.96</cell><cell>11.09</cell><cell>10.95</cell></row><row><cell>c4_100dom_val_44_www.agreatertown.com</cell><cell>13.56</cell><cell>12.94</cell><cell>13.57</cell><cell>12.77</cell><cell>12.64</cell></row><row><cell>c4_100dom_val_45_www.npr.org</cell><cell>10.59</cell><cell>10.30</cell><cell>11.14</cell><cell>10.19</cell><cell>10.12</cell></row><row><cell>c4_100dom_val_46_www.fool.com</cell><cell>11.03</cell><cell>10.63</cell><cell>11.35</cell><cell>10.56</cell><cell>10.42</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://python.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://pytorch.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://docs.nvidia.com/cuda/archive/12.1.0/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>The keys are distributed on the "evaluation part" of the sequence given a power-law distribution. This is motivated by similar structures in natural language text.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>https://huggingface.co/docs/transformers/en/model_doc/gpt2</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>https://github.com/state-spaces/mamba</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>https://github.com/BlinkDL/RWKV-LM/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7"><p>https://github.com/sustcsonglin/flash-linear-attention</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8"><p>https://github.com/BlinkDL/RWKV-LM/blob/64b7fe4c66fcf7da37019630268075b0558f6dc5/ RWKV-v5/train.py#L44</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9"><p>https://huggingface.co/datasets/allenai/paloma</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We thank <rs type="person">Sebastian Lehner</rs>, <rs type="person">Daniel Klotz</rs>, <rs type="person">Thomas Adler</rs>, <rs type="person">Matthias Dellago</rs>, <rs type="person">Gerald Gutenbrunner</rs>, <rs type="person">Fabian Paischer</rs>, <rs type="person">Vihang Patil</rs>, <rs type="person">Niklas Schmidinger</rs>, <rs type="person">Benedikt Alkin</rs>, <rs type="person">Kajetan Schweighofer</rs>, <rs type="person">Anna Zimmel</rs>, <rs type="person">Lukas Aichberger</rs>, <rs type="person">Lukas Hauzenberger</rs>, <rs type="person">Bernhard SchÃ¤fl</rs> and <rs type="person">Johannes Lehner</rs> for helpful discussions and feedback.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">GPT-4 technical report</title>
		<author>
			<persName><forename type="first">J</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<idno>ArXiv, 2303.08774</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Distinctive features, categorical perception, and probability learning: Some applications of a neural model</title>
		<author>
			<persName><forename type="first">J</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Silverstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jones</surname></persName>
		</author>
		<idno type="DOI">10.1037/0033-295X.84.5.413</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="413" to="451" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A simple neural network generating an interactive memory</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Anderson</surname></persName>
		</author>
		<idno type="DOI">10.1016/0025-5564(72)90075-2</idno>
	</analytic>
	<monogr>
		<title level="j">Mathematical Biosciences</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Zoology: Measuring and improving recall in efficient language models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Eyuboglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Timalsina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Poli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>RÃ©</surname></persName>
		</author>
		<idno>ArXiv, 2312.04927</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Using fast weights to attend to the recent past</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4331" to="4339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno>ArXiv, 1607.06450</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Identifying and controlling important neurons in neural machine translation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sajjad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Durrani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=H1z-PsR5KX" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Piqa: Reasoning about physical commonsense in natural language</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lebras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="7432" to="7439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Demographic dialectal variation in social media: A case study of African-American English</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Blodgett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>O'connor</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1120</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1119" to="1130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rethinking attention with performers</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>SarlÃ³s</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Q</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Colwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Weller</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Ua6zuk0WRH" />
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations (ICLR). OpenReview.net, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">PaLM: scaling language modeling with pathways</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2022">2204.02311, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient hierarchical domain adaptation for pretrained language models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chronopoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dodge</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.naacl-main.96</idno>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter</title>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1336" to="1351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Cowhey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schoenick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Tafjord</surname></persName>
		</author>
		<idno>ArXiv, 1803.05457</idno>
		<title level="m">Think you have solved question answering? Try ARC, the AI2 reasoning challenge</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition. Electronic Computers</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="326" to="334" />
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Flashattention-2: Faster attention with better parallelism and work partitioning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dao</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=mZn2Xyh9Ec" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Flashattention: Fast and memory-efficient exact attention with IO-awareness</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>RÃ©</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=H4DqfPSibmx" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Oh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Belgrave</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Optimising synaptic learning rules in linear associative memories</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Willshaw</surname></persName>
		</author>
		<idno type="DOI">10.1007/bf00206223</idno>
	</analytic>
	<monogr>
		<title level="j">Biological Cybernetics</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mixing gated linear recurrences with local attention for efficient language models</title>
		<author>
			<persName><forename type="first">S</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Botev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cristian-Muraru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Haroun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Berrada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Defreitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><surname>Griffin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2024">2402.19427, 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Magnetic control of tokamak plasmas through deep reinforcement learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Degrave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Felici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Buchli</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41586-021-04301-9</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">602</biblScope>
			<biblScope unit="page" from="414" to="419" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural networks and the Chomsky hierarchy</title>
		<author>
			<persName><forename type="first">G</forename><surname>DelÃ©tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ruoss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Grau-Moya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Genewein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Wenliang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Catt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cundy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Ortega</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=WbxHAzkeQcn" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">GLaM: efficient scaling of language models with mixture-ofexperts</title>
		<author>
			<persName><forename type="first">N</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2021">2112.06905, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hungry hungry hippos: Towards language modeling with state space models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Saab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Re</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=COZDy0WYGg" />
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nabeshima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Presser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leahy</surname></persName>
		</author>
		<idno>ArXiv, 2101.00027</idno>
		<title level="m">The Pile: An 800gb dataset of diverse text for language modeling</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to forget: Continual prediction with LSTM</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cummins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Compututation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2451" to="2471" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gemini: A family of highly capable multimodal models</title>
		<author>
			<persName><forename type="first">Gemini</forename><surname>Team</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Google</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2023">2312.11805, 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<idno>ArXiv, 1308.0850</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The international corpus of English (ICE) project</title>
		<author>
			<persName><forename type="first">S</forename><surname>Greenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Nelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">World Englishes</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="15" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">LSTM: A search space odyssey</title>
		<author>
			<persName><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>KoutnÃ­k</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Steunebrink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno>ArXiv, 1503.04069</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Mamba: Linear-time sequence modeling with selective state spaces</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dao</surname></persName>
		</author>
		<idno>ArXiv, 2312.00752</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Efficiently modeling long sequences with structured state spaces</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>RÃ©</surname></persName>
		</author>
		<idno>ArXiv, 2111.00396</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Diagonal state spaces are as effective as structured state spaces</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2022">2203.14343, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Untersuchungen zu dynamischen neuronalen Netzen. Master&apos;s thesis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
		<respStmt>
			<orgName>Technische UniversitÃ¤t MÃ¼nchen</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">LSTM can solve hard long time lag problems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Mozer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Petsche</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="473" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Gradient flow in recurrent nets: the difficulty of learning long-term dependencies</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<editor>J. Kolen and S. Kremer</editor>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
	<note>A Field Guide to Dynamical Recurrent Networks</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning to learn using gradient descent</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Steven</forename><surname>Younger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">R</forename><surname>Conwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Artificial Neural Networks (ICANN 2001)</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Dorffner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Hornik</surname></persName>
		</editor>
		<meeting>Int. Conf. on Artificial Neural Networks (ICANN 2001)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="87" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fast model-based protein homology detection without alignment</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Obermayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="1728" to="1736" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Training compute-optimal large language models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mensch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2022">2203.15556, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A comprehensive survey of deep learning for image captioning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Shiratuddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Laga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">118</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Scaling laws for neural language models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="page">8361</biblScope>
			<date type="published" when="2001">2001. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">The unreasonable effectiveness of recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<ptr target="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">OpenAI Five defeats Dota 2 world champions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<ptr target="https://openai.com/research/openai-five-defeats-dota-2-world-champions" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Transformers are RNNs: Fast autoregressive transformers with linear attention</title>
		<author>
			<persName><forename type="first">A</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>DaumÃ©</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Iii</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</editor>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="5156" to="5165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">GateLoop: Fully data-controlled linear recurrence for sequence modeling</title>
		<author>
			<persName><forename type="first">T</forename><surname>Katsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="1927">2311.01927, 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">The Stack: 3 TB of permissively licensed source code</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kocetkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Benallal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mu Nozferrandis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Devries</surname></persName>
		</author>
		<idno>ArXiv, 2211.15533</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Correlation matrix memories</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
		<idno type="DOI">10.1109/tc.1972.5008975</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers, C</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">1972</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Rainfall-runoff modelling using long short-term memory (LSTM) networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Kratzert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Klotz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Brenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Herrnegger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Hydrology and Earth System Sciences</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="6005" to="6022" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Benchmarking a catchment-aware long short-term memory network (LSTM) for large-scale hydrological modeling</title>
		<author>
			<persName><forename type="first">F</forename><surname>Kratzert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Klotz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shalev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Nearing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="1907">1907.08456, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images. Master&apos;s thesis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>Deptartment of Computer Science, University of Toronto</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Dense associative memory for pattern recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Krotov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Hopfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1172" to="1180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Dense associative memory is robust to adversarial inputs</title>
		<author>
			<persName><forename type="first">D</forename><surname>Krotov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Hopfield</surname></persName>
		</author>
		<idno>ArXiv, 1701.00939</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">The emergence of number and syntax units in LSTM language models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lakretz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Desbordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hupkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dehaene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baroni</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1002</idno>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Burstein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Doran</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Solorio</surname></persName>
		</editor>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">What makes convolutional models great on long sequence modeling?</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dey</surname></persName>
		</author>
		<idno>ArXiv, 2210.09298</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Holistic evaluation of language models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of the New York Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">1525</biblScope>
			<biblScope unit="page" from="140" to="146" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">M6: A Chinese multimodal pretrainer</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<idno>ArXiv, 2103.00823</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Learning long-range spatial dependencies with horizontal gated recurrent units</title>
		<author>
			<persName><forename type="first">D</forename><surname>Linsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Veerabadran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Windolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Bkg6RiCqY7" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Mega: Moving average equipped gated attention</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno>ArXiv, 2209.10655</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<author>
			<persName><forename type="first">I</forename><surname>Magnusson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bhagia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A benchmark for evaluating language model fit</title>
		<imprint>
			<date type="published" when="2023">2312.10523, 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Long range language modeling via gated state spaces</title>
		<author>
			<persName><forename type="first">H</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cutkosky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Neyshabur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2022">2206.13947, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Pointer sentinel mixture models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Byj72udxe" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICRL)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">The parallelism tradeoff: Limitations of log-precision transformers</title>
		<author>
			<persName><forename type="first">W</forename><surname>Merrill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sabharwal</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00562</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="531" to="545" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">The illusion of state in state-space models</title>
		<author>
			<persName><forename type="first">W</forename><surname>Merrill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Petty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sabharwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2024">2404.08819, 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Online normalizer calculation for softmax</title>
		<author>
			<persName><forename type="first">M</forename><surname>Milakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="1805">1805.02867, 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Associatron -a model of associative memory</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nakano</surname></persName>
		</author>
		<idno type="DOI">10.1109/TSMC.1972.4309133</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="380" to="388" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Global prediction of extreme floods in ungauged watersheds</title>
		<author>
			<persName><forename type="first">G</forename><surname>Nearing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dube</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gauch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Gilon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Harrigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hassidim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Klotz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kratzert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Metzger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nevo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pappenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Prudhomme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shalev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shenzis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Tekalign</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weitzner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M B</forename><surname>Kosko</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41586-024-07145-1</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">627</biblScope>
			<biblScope unit="page" from="559" to="563" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">In-context learning and induction heads</title>
		<author>
			<persName><forename type="first">C</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Elhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nanda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2022">2209.11895, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Resurrecting recurrent neural networks for long sequences</title>
		<author>
			<persName><forename type="first">A</forename><surname>Orvieto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>De</surname></persName>
		</author>
		<idno type="DOI">10.5555/3618408.3619518</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International Conference on Machine Learning (ICML)</title>
		<meeting>the 40th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Raiders of the lost KeK: 3.5 years of augmented 4chan posts from the politically incorrect board</title>
		<author>
			<persName><forename type="first">A</forename><surname>Papasavva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zannettou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Decristofaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Stringhini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Blackburn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International AAAI Conference on Web and Social Media (ICWSM)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="885" to="894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">The LAMBADA dataset: Word prediction requiring a broad discourse context</title>
		<author>
			<persName><forename type="first">D</forename><surname>Paperno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N.-Q</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pezzelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gemma</forename><forename type="middle">G</forename><surname>Boleda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>FernÃ¡ndez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1525" to="1534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">The RefinedWeb dataset for Falcon LLM: Outperforming curated corpora with web data, and web data only</title>
		<author>
			<persName><forename type="first">G</forename><surname>Penedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Malartic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hesslow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cojocaru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cappelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Alobeidli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pannier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Almazrouei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Launay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="1116">2306.01116, 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">RWKV: Reinventing RNNs for the transformer era</title>
		<author>
			<persName><forename type="first">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Alcaide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Anthony</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2023">2305.13048, 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Eagle and Finch: RWKV with matrix-valued states and dynamic recurrence</title>
		<author>
			<persName><forename type="first">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Anthony</surname></persName>
		</author>
		<idno>ArXiv, 2404.05892</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Hyena hierarchy: Towards larger convolutional language models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Poli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Massaroli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baccus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>RÃ©</surname></persName>
		</author>
		<idno type="DOI">10.5555/3618408.3619572</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International Conference on Machine Learning (ICML)</title>
		<meeting>the 40th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Mechanistic design and scaling of hybrid architectures</title>
		<author>
			<persName><forename type="first">M</forename><surname>Poli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ponnusamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Deiseroth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>RÃ©</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Massaroli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2024">2403.17844, 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Hierarchically gated recurrent neural network for sequence modeling</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=P1TCHxJwLB" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">37</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">HGRN</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>Gated linear RNNs with state expansion. ArXiv, 2404.07904</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">The ACL anthology network corpus</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Muthukrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Qazvinian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Text and Citation Analysis for Scholarly Digital Libraries (NLPIR4DL)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="54" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Learning to generate reviews and discovering sentiment</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="1444">1704.01444, 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://openai.com/index/better-language-models" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Scaling language models: Methods, analysis &amp; insights from training Gopher</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2021">2112.11446, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="1910">1910.10683, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Hopfield networks is all you need</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>SchÃ¤fl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Seidl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Widrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gruber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Holzleitner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>PavloviÄ‡</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">K</forename><surname>Sandve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Greiff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kreil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kopp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Brandstetter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<publisher>OpenReview</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">M2D2: A massively multi-domain language modeling dataset</title>
		<author>
			<persName><forename type="first">M</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="964" to="975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context</title>
		<author>
			<persName><forename type="first">M</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Savinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Teplyashin</surname></persName>
		</author>
		<idno>2403.05530</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">The evolution of the manosphere across the web</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Blackburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bradlyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Decristofaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Stringhini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zannettou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international AAAI conference on web and social media</title>
		<meeting>the international AAAI conference on web and social media</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="196" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Winogrande: An adversarial winograd schema challenge at scale</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="99" to="106" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">BLOOM: A 176B-parameter open-access multilingual language model</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Akiki</surname></persName>
		</author>
		<idno>ArXiv, 2211.05100</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Linear transformers are secretly fast weight programmers</title>
		<author>
			<persName><forename type="first">I</forename><surname>Schlag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning (ICML)</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Meila</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the 38th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="9355" to="9366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Learning to control fast-weight memories: An alternative to recurrent nets</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="131" to="139" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Deep learning in neural networks: An overview</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neunet.2014.09.003</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="85" to="117" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hilton</surname></persName>
		</author>
		<ptr target="https://openai.com/blog/chatgpt/,2022.OpenAIResearch" />
		<title level="m">Optimizing language models for dialogue</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Storing covariance with nonlinearly interacting neurons</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF00275079</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Biology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">Megatron-LM: Training multi-billion parameter language models using model parallelism</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno>ArXiv, 1909.08053</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">Simplified state space layers for sequence modeling</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T H</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Warrington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Linderman</surname></persName>
		</author>
		<idno>ArXiv, 2208.04933</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">SlimPajama: A 627B token cleaned and deduplicated version of RedPajama</title>
		<author>
			<persName><forename type="first">D</forename><surname>Soboleva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Al-Khateeb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Steeves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hestness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dey</surname></persName>
		</author>
		<ptr target="https://huggingface.co/datasets/cerebras/SlimPajama-627B" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Dolma: an open corpus of three trillion tokens for language model pretraining research</title>
		<author>
			<persName><forename type="first">L</forename><surname>Soldaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bhagia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="1116">2306.01116, 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">AlexaTM 20B: Few-shot learning using a large-scale multilingual Seq2Seq model</title>
		<author>
			<persName><forename type="first">S</forename><surname>Soltan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ananthakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hamza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Peris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rawls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rumshisky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Triefenbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Natarajan</surname></persName>
		</author>
		<idno>ArXiv, 2208.01448</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Training very deep networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<title level="m" type="main">Retentive network: A successor to transformer for large language models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<idno>ArXiv, 2307.08621</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<title level="m" type="main">EleutherAI/lm-evaluation-harness: Major refactor</title>
		<author>
			<persName><forename type="first">L</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schoelkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">L</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schoelkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Abbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fattori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lovering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fazz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><surname>Aflah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Etxaniz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Kasner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Khalid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Kanekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ammanamanchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andyzwei</forename><surname>Boykis</surname></persName>
		</author>
		<ptr target="EleutherAI/lm-evaluation-harness" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27 (NIPS&apos;13)</title>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Synthesizer: Rethinking selfattention in transformer models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-C</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2005">2005.00743, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Long range arena: A benchmark for efficient transformers</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=qVyeW-grC2k" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICRL)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<title level="m" type="main">LaMDA: Language models for dialog applications</title>
		<author>
			<persName><forename type="first">R</forename><surname>Thoppilan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Defreitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hall</surname></persName>
		</author>
		<idno>ArXiv, 2201.08239</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<monogr>
		<title level="m" type="main">Redpajama: an open dataset for training large language models</title>
		<author>
			<persName><surname>Togethercomputer</surname></persName>
		</author>
		<ptr target="https://github.com/togethercomputer/RedPajama-Data" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<title level="m" type="main">Llama: Open and efficient foundation language models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-A</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>RoziÃ¨re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Azhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<idno>ArXiv, 2302.1397</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Parsing noun phrases in the Penn Treebank</title>
		<author>
			<persName><forename type="first">D</forename><surname>Vadas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Curran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="753" to="809" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<monogr>
		<title level="m" type="main">Starcraft II: A new challenge for reinforcement learning</title>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ewalds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bartunov</surname></persName>
		</author>
		<idno>ArXiv, 1708.04782</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Pretraining without attention</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2022">2212.10544, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Linformer: Self-attention with linear complexity</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2006">2006.04768, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Titan: Exploring larger-scale knowledge enhanced pre-training for language understanding and generation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<idno>ERNIE 3.0</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2021">2112.12731, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">mT5: A massively multilingual pre-trained text-to-text transformer</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Barua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.41</idno>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter</title>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="483" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<monogr>
		<title level="m" type="main">FLA: A Triton-based library for hardware-efficient implementations of linear attention mechanism</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://github.com/sustcsonglin/flash-linear-attention" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Gated linear attention transformers with hardwareefficient training</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<idno>2312.06635</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">What is Gab: A bastion of free speech or an alt-right echo chamber</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zannettou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bradlyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Decristofaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sirivianos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Stringini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Blackburn</surname></persName>
		</author>
		<idno type="DOI">10.1145/3184558.3191531</idno>
	</analytic>
	<monogr>
		<title level="m">The Web Conference</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1007" to="1014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Learning to execute</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="volume">4615</biblScope>
			<date type="published" when="1410">1410. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">HellaSwag: Can a machine really finish your sentence</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4791" to="4800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">GLM-130B: An open bilingual pre-trained model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2022">2210.02414, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<monogr>
		<title level="m" type="main">OPT: Open pre-trained transformer language models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dewan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">V</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Simig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Koura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno>ArXiv, 2205.01068</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<monogr>
		<author>
			<persName><forename type="first">Following</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">&amp;</forename><surname>Dao</surname></persName>
		</author>
		<title level="m">we report accuracy for LAMADA, WinoGrande, PIQA, and ARC-easy, and accuracy normalized by sequence length for HellaSwag and ARC-challenge</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">For each model we select the best value respectively. Details on PALOMA. We use 16 out of the 18 data sources of the PALOMA dataset</title>
		<author>
			<persName><surname>Magnusson</surname></persName>
		</author>
		<idno>MC4-EN</idno>
	</analytic>
	<monogr>
		<title level="m">PennTreebank (Vadas &amp; Curran, 2011), RedPajama (TogetherComputer, 2023), Falcon Refinedweb</title>
		<imprint>
			<date type="published" when="2017">2023. 2019. 2021. 2017</date>
			<biblScope unit="page">103</biblScope>
		</imprint>
	</monogr>
	<note>We evaluate all models in full float32, full bfloat16 and bfloat16 Mixed Precision with weights in float32 We use C4 Refined Web Penedo et al., 2023), Dolma v1.5 (Soldaini et al., 2023</note>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">We leave out ThePile (Gao et al., 2021) and ICE (Greenbaum &amp; Nelson, 1996) as they are not part of Paloma&apos;s Huggingface dataset repository 10 . A detailed description of these datasets can be</title>
		<author>
			<persName><surname>S2orc ; Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Results on the data sources TwitterAAE, Manosphere, GAB and 4CHAN are reported in Table 9 and for each individual dataset the results are given in Section C. Model #Params M Twitter AAE Manosphere 4CHAN GAB</title>
		<editor>
			<persName><surname>Magnusson</surname></persName>
		</editor>
		<imprint>
			<publisher>Manosphere Corpus</publisher>
			<date type="published" when="2016">2022. 2022. 2023. 2023. 2022. 2016. 2023. Ribeiro et al., 2021. 2020. 2023</date>
		</imprint>
		<respStmt>
			<orgName>Dolma Subreddits</orgName>
		</respStmt>
	</monogr>
	<note>C4-100-Domains (C4 Domains Wikipedia Dolma-100-Subreddits Dolma-100-Programming Languages All models are evaluated in bfloat16 Mixed Precision GAB Corpus (Zannettou et al., 2018), 4CHAN Corpus</note>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Dataset Llama Mamba RWKV-4 xLSTM</title>
		<imprint/>
	</monogr>
	<note>7:1] xLSTM[1:0] m2d2_s2orc_unsplit_val_physics.med-ph</note>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Dataset Llama Mamba RWKV-4 xLSTM</title>
		<imprint/>
	</monogr>
	<note>7:1] xLSTM[1:0]</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
