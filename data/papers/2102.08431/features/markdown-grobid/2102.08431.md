# Complex Momentum for Optimization in Games

## Abstract

## 

We generalize gradient descent with momentum for optimization in differentiable games to have complex-valued momentum. We give theoretical motivation for our method by proving convergence on bilinear zero-sum games for simultaneous and alternating updates. Our method gives real-valued parameter updates, making it a drop-in replacement for standard optimizers. We empirically demonstrate that complex-valued momentum can improve convergence in realistic adversarial games-like generative adversarial networks-by showing we can find better solutions with an almost identical computational cost. We also show a practical generalization to a complex-valued Adam variant, which we use to train BigGAN to better inception scores on CIFAR-10.

## Introduction

Gradient-based optimization has been critical for the success of machine learning, updating a single set of parameters to minimize a single loss. A growing number of applications require learning in games, which generalize single-objective optimization. Common examples are GANs [[1]](#b0), actor-critic models [[2]](#b1), curriculum learning [[3]](#b2)[[4]](#b3)[[5]](#b4), hyperparameter optimization [[6]](#b5)[[7]](#b6)[[8]](#b7)[[9]](#b8), adversarial examples [[10,](#b9)[11]](#b10), learning models [[12]](#b11)[[13]](#b12)[[14]](#b13), domain adversarial adaptation [[15]](#b14), neural architecture search [[16,](#b15)[17]](#b16), and meta-learning [[18,](#b17)[19]](#b18).

Games consist of multiple players, each with parameters and objectives. We often want solutions where no player gains from changing their strategy unilaterally, e.g., Nash equilibria [[20]](#b19) or Stackelberg equilibria [[21]](#b20). Classical gradient-based learning often fails to find these equilibria due to rotational dynamics [[22]](#b21). Numerous saddle point finding algorithms for zero-sum games have been proposed [[23,](#b22)[24]](#b23). [[25]](#b24) generalizes GD with momentum to games, showing we can use a negative momentum to converge if the eigenvalues of the Jacobian of the gradient vector field have a large imaginary part. We use the terminology in [[25]](#b24) and say (purely) cooperative or adversarial games for games with (purely) real or imaginary eigenvalues. Setups like GANs are not purely adversarial, but rather have both purely cooperative and adversarial eigenspaces -i.e., eigenspaces with purely real or imaginary eigenvalues. In cooperative eigenspaces, the players do not interfere with each other.

We want solutions that converge with simultaneous and alternating updates in purely adversarial games -a setup where existing momentum methods fail. Also, we want solutions that are robust to different mixtures of adversarial and cooperative eigenspaces, because this depends on the games eigendecomposition which can be intractable. To solve this we unify and generalize existing momentum methods [[26,](#b25)[25]](#b24) to recurrently linked momentum -a setup with multiple recurrently linked momentum buffers with potentially negative coefficients shown in Figure [2c](#fig_5).

We show that selecting two of these recurrently linked buffers with appropriate momentum coefficients can be interpreted as the real and imaginary parts of a single complex buffer and complex momentum coefficient -see Figure [2d](#fig_5). This setup (a) allows us to converge in adversarial games with simultaneous updates, (b) only introduces one new optimizer parameter -the phase or arg of our momentum, (c) allows us to gain intuitions via complex analysis, (d) is trivial to implement in libraries supporting complex arithmetic, and (e) robustly converges for different eigenspace mixtures.

Intuitively, our complex buffer stores historical gradient information, oscillating between adding or subtracting at a frequency dictated by the momentum coefficient. Classical momentum only adds gradients, and negative momentum changes between adding or subtracting each iteration, while we oscillate at an arbitrary (fixed) frequency -see Figure [4a](#fig_2). This reduces rotational dynamics during training by canceling out opposing updates.

## Contributions

• We provide generalizations and variants of classical [[27]](#b26)[[28]](#b27)[[29]](#b28), negative [[25,](#b24)[30]](#b29), and aggregated [[26]](#b25) momentum for learning in differentiable games. • We show our methods converges on adversarial games -including bilinear zero-sum games and the Dirac-GAN -with simultaneous and alternating updates. • We illustrate a robustness during optimization, converging faster and over a larger range of mixtures of cooperative and adversarial games than existing first-order methods. • We give a practical extension of our method to a complex-valued Adam [[31]](#b30) variant, which we use to train a BigGAN [[32]](#b31) on CIFAR-10, improving [[32]](#b31)'s inception scores.

## Background

Actual JAX implementation: changes in green mass = .8 + .3j def momentum ( step_size, mass ): ... def update ( i, g, state ):

x, velocity = state velocity = mass * velocity + g x =x -jnp.real ( step_size ( i )* velocity ) return x, velocity ...

## Figure 1:

How to modify JAX's SGD with momentum here to use complex momentum. The only changes are in green. jnp.real gets the real part of step_size times the momentum buffer (called velocity here). We use a complex mass for our method in this case β " |β| exppi argpβqq " 0.9 exppi π {8q « .8 `.3i.

Appendix Table [2](#) summarizes our notation. Consider the optimization problem: θ ˚:" arg min θ Lpθq (1) We can find local minima of loss L using (stochastic) gradient descent with step size α. We denote the loss gradient at parameters θ j by g j :"gpθ j q:"∇ θ Lpθq| θ j .

θ j`1 " θ j ´αg j (SGD) Momentum can generalize SGD. For example, Polyak's Heavy Ball [[27]](#b26):

θ j`1 " θ j ´αg j `βpθ j ´θj´1 q (2) Which can be equivalently written with momentum buffer µ j " pθ j ´θj´1 q {α. µ j`1 " βµ j ´gj , θ j`1 " θ j `αµ j`1 (SGDm) We can also generalize SGDm to aggregated momentum [[26]](#b25), shown in Appendix Algorithm 3.

## Game Formulations

Another class of problems is learning in games, which includes problems like generative adversarial networks (GANs) [[1]](#b0). We focus on 2-player games -with players denoted by A and B-where each player minimizes their loss L A , L B with their parameters θ A P R d A , θ B P R d B . Solutions to 2-player games -which are assumed unique for simplicity -can be defined as:

$θ Å :" arg min θ A L A pθ A ,θ B q, θ B :" arg min θ B L B pθ Å ,θ B q(3$) In deep learning, losses are non-convex with many parameters, so we often focus on finding local solutions. If we have a player ordering, then we have a Stackelberg game. For example, in GANs, the generator is the leader, and the discriminator is the follower. In hyperparameter optimization, the hyperparameters are the leader, and the network parameters are the follower. If θ B pθ A q denotes player B's best-response function, then Stackelberg game solutions can be defined as:

$θ A ˚:" arg min θ A L A pθ A ,θ B pθ A qq, θ B pθ A q :" arg min θ B L B pθ A ,θ B q(4$) If L A and L B are differentiable in θ A and θ B we say the game is differentiable. We may be able to approximately find θ Å efficiently if we can do SGD on:

$L Å pθ A q :" L A pθ A , θ B pθ A qq(5)$Unfortunately, SGD would require computing dL Å{dθ A , which often requires dθ B{dθ A , but θ B pθ A q and its Jacobian are typically intractable. A common optimization algorithm to analyze for finding solutions is simultaneous SGD (SimSGD) -sometimes called gradient descent ascent for zero-sum gameswhere g j A :" g A pθ j A , θ j B q and g j B :" g B pθ j A , θ j B q are estimators for

$∇ θ A L A | θ j A ,θ j B and ∇ θ B L B | θ j A ,θ j B : θ j`1 A " θ j A ´αg j A , θ j`1 B " θ j B ´αg j B (SimSGD)$We simplify notation with the concatenated or joint-parameters ω :" rθ A , θ B s P R d and the jointgradient vector field ĝ : R d Ñ R d , which at the j th iteration is the joint-gradient denoted: ĝj :" ĝpω j q :" rg A pω j q, g B pω j qs " rg j A , g j B s (6) We extend to n-player games by treating ω and ĝ as concatenations of the players' parameters and loss gradients, allowing for a concise expression of the SimSGD update with momentum (SimSGDm): µ j`1 " βµ j ´ĝ j , ω j`1 " ω j `αµ j`1 (SimSGDm) [[25]](#b24) show classical momentum choices of β P r0, 1q do not improve solution speed over SimSGD in some games, while negative momentum helps if the Jacobian of the joint-gradient vector field ∇ ω ĝ has complex eigenvalues. Thus, for purely adversarial games with imaginary eigenvalues, any non-negative momentum and step size will not converge. For cooperative games -i.e., minimization -∇ ω ĝ has strictly real eigenvalues because it is a losses Hessian, so classical momentum works well.

## Limitations of Existing Methods

Higher-order: Methods using higher-order gradients are often harder to parallelize across GPUs, [[33]](#b32), get attracted to bad saddle points [[34]](#b33), require estimators for inverse Hessians [[35,](#b34)[36]](#b35), are complicated to implement, have numerous optimizer parameters, and can be more expensive in iteration and memory cost [[37,](#b36)[36,](#b35)[35,](#b34)[[38]](#b37)[[39]](#b38)[[40]](#b39). Instead, we focus on first-order methods.

First-order: Some first-order methods such as extragradient [[41]](#b40) require a second, costly, gradient evaluation per step. Similarly, methods alternating player updates are bottlenecked by waiting until after the first player's gradient is used to evaluate the second player's gradient. But, many deep learning setups can parallelize computation of both players' gradients, making alternating updates effectively cost another gradient evaluation. We want a method which updates with the effective cost of one gradient evaluation. Also, simultaneous updates are a standard choice in some settings [[15]](#b14).

Robust convergence: We want our method to converge in purely adversarial game's with simultaneous updates -a setup where existing momentum methods fail [[25]](#b24). Furthermore, computing a games eigendecomposition is often infeasibly expensive, so we want methods that robustly converge over different mixtures of adversarial and cooperative eigenspaces. We are particularly interested in eigenspace mixtures that that are relevant during GAN training -see Figure [7](#) and Appendix Figure [9](#fig_9).

## Coming up with our Method

Combining existing methods: Given the preceding limitations, we would like a robust first-order method using a single, simultaneous gradient evaluation. We looked at combining aggregated [[26]](#b25) with negative [[25]](#b24) momentum by allowing negative coefficients, because these methods are firstorder and use a single gradient evaluation -see Figure [2b](#fig_5). Also, aggregated momentum provides robustness during optimization by converging quickly on problems with wide range of conditioning, while negative momentum works in adversarial setups. We hoped to combine their benefits, gaining robustness to different mixtures of adversarial and cooperative eigenspaces. However, with this setup we could not find solutions that converge with simultaneous updates in purely adversarial games.

Generalize to allow solutions: We generalized the setup to allow recurrent connections between momentum buffers, with potentially negative coefficients -see Figure [2c](#fig_5) and Appendix Algorithm 4.

There are optimizer parameters so this converges with simultaneous updates in purely adversarial games, while being first-order with a single gradient evaluation -see Corollary 1. However, in general, this setup could introduce many optimizer parameters, have unintuitive behavior, and not be amenable to analysis. So, we choose a special case of this method to help solve these problems. Figure [2](#fig_5): We show computational diagrams for momentum variants simultaneously updating all players parameters, which update the momentum buffers µ at iteration j `1 with coefficient β via µ j`1 " pβµ j ´gradientq. Our parameter update is a linear combination of the momentum buffers weighted by step sizes α. (a) Classical momentum [[27,](#b26)[29]](#b28), with a single buffer and coefficient β P r0, 1q. (b) Aggregated momentum [[26]](#b25) which adds multiple buffers with different coefficients.

(c) Recurrently linked momentum, which adds cross-buffer coefficients and updates the buffers with µ j`1 pkq " p ř l β pl,kq µ j plq ´gradientq. We allow β pl,kq to be negative like negative momentum [[25]](#b24) for solutions with simultaneous updates in adversarial games. (d) Complex momentum is a special case of recurrently linked momentum with two buffers and β p1,1q " β p2,2q " pβq, β p1,2q " ´βp2,1q " pβq.

## Analyzing other recurrently linked momentum setups is an open problem.

A simple solution: With two momentum buffers and correctly chosen recurrent weights, we can interpret our buffers as the real and imaginary part of one complex buffer -see Figure [2d](#fig_5). This method is (a) capable of converging in purely adversarial games with simultaneous updates -Corollary 1, (b) only introduces one new optimizer parameter -the phase of the momentum coefficient, (c) is tractable to analyze and have intuitions for with Euler's formula -ex., Eq. ( [8](#formula_6)), (d) is trivial to implement in libraries supporting complex arithmetic -see Figure [1](#fig_8), and (e) can be robust to games with different mixtures of cooperative and adversarial eigenspaces -see Figure [5](#).

## Complex Momentum

We describe our proposed method, where the momentum coefficient β P C, step size α P R, momentum buffer µ P C d , and player parameters ω P R d . The simultaneous (or Jacobi) update is: µ j`1 " βµ j ´ĝ j , ω j`1 " ω j ` pαµ j`1 q (SimCM)

There are many ways to get a real-valued update from µ P C, but we only consider updates equivalent to classical momentum when β P R. Specifically, we simply update the parameters using the real component of the momentum: pµq.

Algorithm 1 (SimCM) Momentum

1: β, α P C, µ P C d , ω 0 P R d 2: for i " 1 . . . N do 3:

µ j`1 " βµ j ´ĝ j 4:

ω j`1 " ω j ` pαµ j`1 q return ω N

We show the SimCM update in Algorithm 1 and visualize it in Figure [2d](#fig_5). We also show the alternating (or Gauss-Seidel) update, which is common for GAN training:

$µ j`1$A " βµ j A ´ĝ A pω j q, θ j`1 A " θ j A ` pαµ j`1 A q (AltCM)

$µ j`1 B " βµ j B ´ĝ B pθ j`1 A , θ j B q, θ j`1 B " θ j B ` pαµ j`1 B q$Generalizing negative momentum: Consider the negative momentum from [[25]](#b24): ω j`1 " ω j άĝ j `βpω j ´ωj´1 q. Expanding (SimCM) with µ j " pω j ´ωj´1 q {α for real momentum shows the negative momentum method of [[25]](#b24) is a special case of our method: ω j`1 " ω j ` pαpβ pω j ´ωj´1 q {α ´ĝ j qq " ω j ´αĝ j `βpω j ´ωj´1 q (7)

## Dynamics of Complex Momentum

For simplicity, we assume Numpy-style [[43]](#b42) component-wise broadcasting for operations like taking the real-part pzq of vector z " rz 1 , . . . , z n s P C n , with proofs in the Appendix.

Expanding the buffer updates with the polar components of β gives intuition for complex momentum: µ j`1 " βµ j ´ĝ j ðñ µ j`1 " βpβp¨¨¨q ´ĝ j´1 q ´ĝ j ðñ µ j`1 " ´k"j ÿ k"0 β k ĝj´k ðñ pµ j`1 q " ´k"j ÿ k"0 |β| k cospk argpβqqĝ j´k , pµ j`1 q " ´k"j ÿ k"0

$|β| k sinpk argpβqqĝ j´k(8)$The final line is simply by Euler's formula [(26)](#b25). From (8) we can see β controls the momentum buffer µ by having |β| dictate prior gradient decay rates, while argpβq controls oscillation frequency between adding and subtracting prior gradients, which we visualize in Figure [4a](#fig_2).

Discriminator Generator Norm of joint-gradient }ĝ}  We show the real part of our momentum buffer -which dictates the parameter update -at the 50 th iteration pµ 50 q dependence on past gradients ĝk for k " 1 . . . 50. The momentum magnitude is fixed to |β| " 0.9 as in Figure [3](#fig_1). Euler's formula is used in [(8)](#b7) to for finding dependence or coefficient of ĝk via pµ 50 q " ´řk"50 k"0 |β| k cospk argpβqqĝ j´k . Complex momentum allows smooth changes in the buffers dependence on past gradients.

## Momentum phase argpβq

## Momentum magnitude |β|

## Number of steps to converge

Figure [4](#fig_2)(b): How many steps simultaneous complex momentum on a Dirac-GAN takes for a set solution distance. We fix step size α " 0.1 as in Figure [3](#fig_1), while varying the phase and magnitude of our momentum β " |β| exppi argpβqq. There is a red star at the optima, dashed red lines at real β, and a dashed magenta line for simultaneous gradient descent. There are no real-valued β that converge for this -or any α with simultaneous updates [[25]](#b24). Appendix Figure [8](#) compares this with alternating updates (AltCM).

Expanding the parameter updates with the Cartesian components of α and β is key for Theorem 1, which characterizes the convergence rate: µ j`1 " βµ j ´ĝ j ðñ pµ j`1 q " pβq pµ j q´ pβq pµ j q´ pĝ j q, pµ j`1 q " pβq pµ j q` pβq pµ j q (9)

ω j`1 " ω j ` pαµ j`1 q ðñ ω j`1 " ω j ´αĝ j ` pαβq pµ j q´ pαβq pµ j q (10)

So, we can write the next iterate with a fixed-point operator: r pµ j`1 q, pµ j`1 q,ω j`1 s" F α,β pr pµ j q, pµ j q,ω j sq (11) ( [9](#)) and ( [10](#)) allow us to write the Jacobian of F α,β which can be used to bound convergence rates near fixed points, which we name the Jacobian of the augmented dynamics of buffer µ and joint-parameters ω and denote with:

$R :" ∇ rµ,ωs F α,β " « pβqI ´ pβqI ´∇ω ĝ pβqI pβqI 0 pαβqI ´ pαβqI I ´α∇ ω ĝff(12)$So, for quadratic losses our parameters evolve via: r pµ j`1 q, pµ j`1 q,ω j`1 s J " R r pµ j q, pµ j q,ω j s J

We can bound convergence rates by looking at the spectrum of R with Theorem 1.

Theorem 1 (Consequence of Prop. 4.4.1 [[44]](#b43)). Convergence rate of complex momentum: If the spectral radius ρpRq " ρp∇ rµ,ωs F α,β q ă 1, then, for rµ, ωs in a neighborhood of rµ ˚, ω ˚s, the distance of rµ j , ω j s to the stationary point rµ ˚, ω ˚s converges at a linear rate OppρpRq` q j q, @ ą 0.

Here, linear convergence means lim jÑ8 }ω j`1 ´ω˚} {}ω j ´ω˚} P p0, 1q, where ω ˚is a fixed point. We should select optimization parameters α, β so that the augmented dynamics spectral radius SppRpα, βqq ă 1-with the dependence on α and β now explicit. We may want to express SppRpα, βqq in terms of the spectrum Spp∇ ω ĝq, as in Theorem 3 in [[25]](#b24):

$f pSpp∇ ω ĝq, α, βq " SppRpα, βqq(14)$We provide a Mathematica command in Appendix A.2 for a cubic polynomial p characterizing f with coefficients that are functions of α, β & λ P Spp∇ ω ĝq, whose roots are eigenvalues of R, which we use in subsequent results. [[45,](#b44)[26]](#b25) mention that in practice we do not know the condition number, eigenvalues -or the mixture of cooperative and adversarial eigenspaces -of a set of functions that we are optimizing, so we try to design algorithms which work over a large range. Sharing this motivation, we consider convergence behavior on games ranging from purely adversarial to cooperative.

In Section 4.2 at every non-real β we could select α and |β| so Algorithm 1 converges. We define almost-positive to mean argpβq " for small , and show there are almost-positive β which converge.

## Corollary 1 (Convergence of Complex Momentum).

There exist α P R, β P C so Algorithm 1 converges for bilinear zero-sum games. More-so, for small (we show for " π 16 ), if argpβq " (i.e., almost-positive) or argpβq " π ´ (i.e., almost-negative), then we can select α, |β| to converge.

Why show this? Our result complements [[25]](#b24) who show that for all real α, β Algorithm 1 does not converge. We include the proof for bilinear zero-sum games, but the result generalizes to some games that are purely adversarial near fixed points, like Dirac GANs [[34]](#b33). The result's second part shows evidence there is a sense in which the only β that do not converge are real (with simultaneous updates on purely adversarial games). It also suggests a form of robustness, because almost-positive β can approach acceleration in cooperative eigenspaces, while converging in adversarial eigenspaces, so almost-positive β may be desirable when we have games with an uncertain or variable mixtures of real and imaginary eigenvalues like GANs. Sections 4.2, 4.3, and 4.4 investigate this further.

## What about Acceleration?

With classical momentum, finding the step size α and momentum β to optimize the convergence rate tractable if 0 ă l ď L and Spp∇ ω ĝq P rl, Ls d [46] -i.e., we have an l-strongly convex and L-Lipschitz loss. The conditioning κ " L {l can characterize problem difficulty. Gradient descent with an appropriate α can achieve a convergence rate of κ´1 κ`1 , but using momentum with appropriate pα ˚, β ˚q can achieve an accelerated rate of ρ ˚" ? κ´1 ? κ`1 . However, there is no consensus for constraining Spp∇ ω ĝq in games for tractable and useful results. Candidate constraints include monotonic vector fields generalizing notions of convexity, or vector fields with bounded eigenvalue norms capturing a kind of sensitivity [[47]](#b46). Figure [7](#) shows Spp∇ ω ĝq for a GAN -we can attribute some eigenvectors to a single player's parameters. The discriminator can be responsible for the largest and smallest norm eigenvalues, suggesting we may benefit from varying α and β for each player as done in Section 4.4.

## Implementing Complex Momentum

Complex momentum is trivial to implement with libraries supporting complex arithmetic like JAX [[48]](#b47) or Pytorch [[49]](#b48). Given an SGD implementation, we often only need to change a few lines of code -see Figure [1](#fig_8). Also, ( [9](#)) and ( [10](#)) can be easily used to implement Algorithm 1 in a library without complex arithmetic. More sophisticated optimizers like Adam can trivially support complex optimizer parameters with real-valued updates, which we explore in Section 4.4.

## Scope and Limitations

For some games, we need higher than first-order information to converge -ex., pure-response games [[7]](#b6) -because the first-order information for a player is identically zero. So, momentum methods only using first-order info will not converge in general. However, we can combine methods with second-order information and momentum algorithms [[7,](#b6)[9]](#b8). Complex momentum's computational cost is almost identical to classical and negative momentum, except we now have a buffer with twice as many real parameters. We require one more optimization hyperparameter than classical momentum, which we provide an initial guess for in Section 4.5.

## Experiments

We investigate complex momentum's performance in training GANs and games with different mixtures of cooperative and adversarial eigenspaces, showing improvements over standard baselines. Code for experiments will be available on publication, with reproducibility details in Appendix C.

Overview: We start with a purely adversarial Dirac-GAN and zero-sum games, which have known solutions ω ˚" pθ Å , θ B q and spectrums Spp∇ ω ĝq, so we can assess convergence rates. Next, we evaluate GANs generating 2D distributions, because they are simple enough to train with a plain, alternating SGD. Finally, we look at scaling to larger-scale GANs on images which have brittle optimization, and require optimizers like Adam. Complex momentum provides benefits in each setup.

We only compare to first-order optimization methods, despite there being various second-order methods due limitations discussed in Section 2.2.

## Optimization in Purely Adversarial Games

Here, we consider the optimizing the Dirac-GAN objective, which is surprisingly hard and where many classical optimization methods fail, because Spp∇ ω ĝq is imaginary near solutions:

$min x max y ´logp1 `expp´xyqq ´logp2q(15)$Figure [3](#fig_1) empirically verifies convergence rates given by Theorem 1 with [(14)](#b13), by showing the optimization trajectories with simultaneous updates.

Figure [4b](#fig_2) investigates how the components of the momentum β affect convergence rates with simultaneous updates and a fixed step size. The best β was almost-positive (i.e., argpβq " for small ). We repeat this experiment with alternating updates in Appendix Figure [8](#), which are standard in GAN training. There, almost-positive momentum is best (but negative momentum also converges), and the benefit of alternating updates can depend on if we can parallelize player gradient evaluations. 

## How Adversarialness Affects Convergence Rates

Here, we compare optimization with first-order methods for purely adversarial, cooperative, and mixed games. We use the following game, allowing us to easily interpolate between these regimes: min Max adversarialness γ max Figure [5](#): We compare first-order methods convergence rates on the game in [(16)](#b15), with A " B 1 " B 2 diagonal and entries linearly spaced in r 1 {4, 4s. We interpolate from purely cooperative to a mixture of purely cooperative and adversarial eigenspaces in Spp∇ ω ĝq by making γ diagonal with γ j " U r0, γ max s, inducing j th eigenvalue pair to have argpλ j q « ˘γj π 2 . So, γ max controls the largest possible eigenvalue arg or max adversarialness. Every method generalizes gradient descentascent (GDA) by adding an optimizer parameter, tuned via grid search. Positive momentum and negative momentum do not converge if there are purely adversarial eigenspaces (i.e., γ max " 1). Almostpositive momentum argpβq " ą 0 like π {8 allows us to approach the acceleration of positive momentum if sufficiently cooperative (i.e., γ max ă 0.5), while still converging if there are purely adversarial eigenspaces (i.e., γ max " 1). Tuning argpβq with complex momentum performs competitively with extragradient (EG), optimistic gradient (OG) for any adversarialness -ex., argpβq " π {2 does well if there are purely adversarial eigenspaces (i.e., γ max " 1).

If γ " I the game is purely adversarial, while if the γ " 0 the game is purely cooperative.

Figure [6](#) explores SppRq in purely adversarial games for a range of α, β, generalizing Figure [4](#fig_2) in [[25]](#b24). At every non-real β-i.e., argpβq ‰ π or 0-we could select α, |β| that converge.

Figure [5](#) compares first-order algorithms as we interpolate from the purely cooperative games (i.e., minimization) to mixtures of purely adversarial and cooperative eigenspaces, because this setup range can occur during GAN training -see Figure [7](#). Our baselines are simultaneous SGD (or gradient descent-ascent (GDA)), extragradient (EG) [[41]](#b40), optimistic gradient (OG) [[50]](#b49)[[51]](#b50)[[52]](#b51), and momentum variants. We added extrapolation parameters for EG and OG so they are competitive with momentum -see Appendix Section C.3. We show how many gradient evaluations for a set solution distance, and EG costs two evaluations per update. We optimize convergence rates for each game and method by grid search, as is common for optimization parameters in deep learning.

Takeaway: In the cooperative regime -i.e., γ max ă .5 or max λPSpp∇ω ĝq | argpλq| ă π {4the best method is classical, positive momentum, otherwise we benefit from a method for learning in games. If we have purely adversarial eigenspaces then GDA, positive and negative momentum fail to converge, while EG, OG, and complex momentum can converge. In games like GANs, our eigendecomposition is infeasible to compute and changes during training -see Appendix Figure [9](#fig_9) -so we want an optimizer that converges robustly. Choosing any non-real momentum β allows robust convergence for every eigenspace mixture. More so, almost-positive momentum β allows us to approach acceleration when cooperative, while still converging if there are purely adversarial eigenspaces.

## Training GANs on 2D Distributions

Here, we investigate improving GAN training using alternating gradient descent updates with complex momentum. We look at alternating updates, because they are standard in GAN training [[1,](#b0)[32,](#b31)[53]](#b52). It is not clear how EG and OG generalize to alternating updates, so we use positive and negative momentum as our baselines. We train to generate a 2D mixture of Gaussians, because more complicated distribution require more complicated optimizers than SGD. Figure [1](#fig_8) shows all changes necessary to use the JAX momentum optimizer for our updates, with full details in Appendix C.4. We evaluate the log-likelihood of GAN samples under the mixture as an imperfect proxy for matching.

Appendix Figure [10](#fig_8) shows heatmaps for tuning argpβq and |β| with select step sizes. Takeaway: The best momentum was found at the almost-positive β « 0.7 exppi π {8q with step size α " 0.03, and for each α we tested a broad range of non-real β outperformed any real β. This suggests we may be able to often improve GAN training with alternating updates and complex momentum.

## Training BigGAN with a Complex Adam

Here, we investigate improving larger-scale GAN training with complex momentum. However, largerscale GANs train with more complicated optimizers than gradient descent -like Adam [[31]](#b30) -and have notoriously brittle optimization. We look at training BigGAN [[32]](#b31) on CIFAR-10 [54], but were unable to succeed with optimizers other than [[32]](#b31)-supplied setups, due to brittle optimization. So, we attempted to change procedure minimally by taking [[32]](#b31)-supplied code here which was trained with Adam, and making the β 1 parameter -analogous to momentum -complex. The modified complex Adam is shown in Algorithm 2, where the momentum bias correction is removed to better match our theory. It is an open question on how to best carry over the design of Adam (or other optimizers) to the complex setting. Training each BigGAN took 10 hours on an NVIDIA T4 GPU, so Figure [12a](#fig_12) and Table [1](#) took about 1000 and 600 GPU hours respectively.

Figure [12a](#fig_12) shows a grid search over argpβ 1 q and |β 1 | for a BigGAN trained with Algorithm 2. We only changed β 1 for the discriminator's optimizer. Takeaway: The best momentum was at the almostpositive β 1 « 0.8 exppi π {8q, whose samples are in Appendix Figure [11b](#fig_10). We tested the best momentum value over 10 seeds against the author-provided baseline in Appendix Figure [12b](#fig_12), with the results summarized in Table [1](#). [[32]](#b31) reported a single inception score (IS) on CIFAR-10 of 9.22, but the best we could reproduce over the seeds with the provided PyTorch code and settings was 9.10. Complex momentum improves the best IS found with 9.25p`.15 over author code, `.03 author reportedq. We trained a real momentum |β 1 | " 0.8 to see if the improvement was solely from tuning the momentum magnitude. This occasionally failed to train and decreased the best IS over re-runs, showing we benefit from a non-zero argpβ 1 q.

## A Practical Initial Guess for Optimizer Parameter argpβq

Here, we propose a practical initial guess for our new hyperparameter argpβq. Corollary 1 shows we can use almost-real momentum coefficients (i.e., argpβq is close to 0). Figure [5](#) shows almost-positive β approach acceleration in cooperative eigenspaces, while converging in all eigenspaces. Figure [7](#) shows GANs can have both cooperative and adversarial eigenspaces. Figures [10](#fig_8) and [12a](#fig_12) do a grid search over argpβq for GANs, finding that almost-positive argpβq « π {8 works in both cases. Also, by minimally changing argpβq from 0 to a small , we can minimally change other hyperparameters in our model, which is useful to adapt existing, brittle setups like in GANs. Based on this, we propose an initial guess of argpβq " for a small ą 0, where " π {8 worked in our GAN experiments.

## Algorithm 2 Complex Adam variant without momentum bias-correction

1: β 1 P C, β 2 P r0,1q 2: α P R `, P R 3:

for j " 1 . . . N do 4: µ j`1 " β 1 µ j ´gj 5: v j`1 " β 2 v j `p1´β 2 qpg j q 2 6: vj`1 " v j`1 1´pβ2q j 7: ω j`1 " ω j `α pµ j q ? vj`

1 ` return ω N CIFAR-10 BigGAN Best IS for 10 seeds Discriminator β 1 Min Max 0 -[32]'s default 8.9 9.1 0.8 exppi π {8q -ours 8.96p`.06q 9.25p`.15q 0.8 3.12p´5.78q 9.05p´0.05q Table [1](#): We display the best inception scores (IS) found over 10 runs for training BigGAN on CIFAR-10 with various optimizer settings. We use a complex Adam variant outlined in Algorithm 2, where we only tuned β 1 for the discriminator. The best parameters found in Figure [12a](#fig_12) were β 1 " 0.8 exppi π {8q, which improved the min and max IS from our runs of the BigGAN authors baseline, which was the SoTA optimizer in this setting to best of our knowledge. We tested β 1 " 0.8 to see if the gain was solely from tuning |β 1 |, which occasionally failed and decreased the best IS. Does eigenvector point at a player?

Figure [7](#): A log-polar coordinate visualization reveals structure in the spectrum for a GAN at the end of the training on a 2D mixture of Gaussians with a 1-layer (disc)riminator and (gen)erator, so the joint-parameters ω P R 723 . It is difficult to see structure by graphing the Cartesian (i.e., and ) parts of eigenvalues, because they span orders of magnitude, while being positive and negative. Appendix Figure [9](#fig_9) shows the spectrum through training.

There is a mixture of many cooperative (i.e., real or argpλq « 0, ˘π) and some adversarial (i.e., imaginary or argpλq « ˘π 2 ) eigenvalues, so -contrary to what the name may suggest -generative adversarial networks are not purely adversarial. We may benefit from optimizers leveraging this structure like complex momentum. Eigenvalues are colored if the associated eigenvector is mostly in one player's part of the joint-parameter space -see Appendix Figure [9](#fig_9) for details on this. Many eigenvectors lie mostly in the the space of (or point at) a one player. The structure of the set of eigenvalues for the disc. Accelerated first-order methods: A broad body of work exists using momentum-type methods [[27,](#b26)[28,](#b27)[55,](#b54)[56]](#b55), with a recent focus on deep learning [[29,](#b28)[[57]](#b56)[[58]](#b57)[[59]](#b58)[[60]](#b59). But, these works focus on momentum for minimization as opposed to in games.

Learning in games: Various works approximate response-gradients -some by differentiating through optimization [[61,](#b60)[34,](#b33)[62]](#b61). Multiple works try to leverage game eigenstructure during optimization [[63-65, 39, 66, 67]](#).

First-order methods in games: In some games, we can get away with using only first-order methods - [[68-72, 47, 73, 74]](#) discuss when and how these methods work. [[25]](#b24) is the closest work to ours, showing a negative momentum can help in some games. [[30]](#b29) note the suboptimality of negative momentum in a class of games. [[75,](#b74)[76]](#b75) investigate acceleration in some games.

Bilinear zero-sum games: [[77]](#b76) study the convergence of gradient methods in bilinear zero-sum games. Their analysis extends [[25]](#b24), showing that we can achieve faster convergence by having separate step sizes and momentum for each player or tuning the extragradient step size. [[78]](#b77) provide convergence guarantees for games satisfying a sufficiently bilinear condition.

Learning in GANs: Various works try to make GAN training easier with methods leveraging the game structure [[79-81, 53, 82]](#). [[83]](#b82) approximate the discriminator's response function by differentiating through optimization. [[34]](#b33) find solutions by minimizing the norm of the players' updates. Both of these methods and various others [[84]](#b83)[[85]](#b84)[[86]](#b85) require higher-order information. [[52,](#b51)[87,](#b86)[88]](#b87) look at first-order methods. [[42]](#b41) explore problems for GAN training convergence and [[22]](#b21) show that GANs have significant rotations affecting learning.

## Conclusion

In this paper we provided a generalization of existing momentum methods for learning in differentiable games by allowing a complex-valued momentum with real-valued updates. We showed that our method robustly converges in games with a different range of mixtures of cooperative and adversarial eigenspaces than current first-order methods. We also presented a practical generalization of our method to the Adam optimizer, which we used to improve BigGAN training. More generally, we highlight and lay groundwork for investigating optimizers which work well with various mixtures of cooperative and competitive dynamics in games.

## Societal Impact

Our main contribution in this work is methodological -specifically, a scalable algorithm for optimizing in games. Since our focus is on improving optimization methods, we do not expect there to be direct negative societal impacts from this contribution.

Table 2: Notation SGD Stochastic Gradient Descent CM Complex Momentum SGDm, SimSGDm, . . . . . . with momentum SimSGD, SimCM Simultaneous . . . AltSGD, AltCM Alternating . . . GAN Generative Adversarial Network [1] EG Extragradient [41] OG Optimistic Gradient [52] IS Inception Score [89] :" Defined to be equal to x, y, z, ¨¨¨P C Scalars x, y, z, ¨¨¨P C n Vectors X, Y , Z, ¨¨¨P C nˆn Matrices X J The transpose of matrix X I The identity matrix pzq, pzq The real or imaginary component of z P C i The imaginary unit. z P C ùñ z " pzq `i pzq s z The complex conjugate of z P C |z| :" ? zs z The magnitude or modulus of z P C argpzq The argument or phase of z P C ùñ z " |z| exppi argpzqq z P C is almost-positive argpzq " for small respectively A, B A symbol for the outer/inner players d A , d B P N

The number of weights for the outer/inner players θ A symbol for the parameters or weights of a player

$θ A P R d A , θ B P R d B$The outer/inner parameters or weights L : R n Ñ R A symbol for a loss

$L A pθ A , θ B q, L B pθ A , θ B q$The outer/inner losses -

$R d A `dB Þ Ñ R g A pθ A , θ B q, g B pθ A , θ B q$Gradient of outer/inner losses w.r.t. their weights in R d A {d B θ Bpθ A q :" arg min

$θ B L B pθ A ,θ B q$The best-response of the inner player to the outer player

$L Å pθ A q :" L A pθ A ,θ B pθ A qq$The outer loss with a best-responding inner player θ Å :" arg min

$θ A L Å pθ A q$Outer optimal weights with a best-responding inner player

$d :" d A `dB$The combined number of weights for both players ω :" rθ A , θ B s P R d A concatenation of the outer/inner weights ĝpωq :" rg A pωq, g B pωqs P R d A concatenation of the outer/inner gradients ω 0 " rθ 0 A , θ 0 B s P R d

The initial parameter values j An iteration number ĝj :" ĝpω j q P R d

The joint-gradient vector field at weights ω j ∇ ω ĝj :" ∇ ω ĝ| ω j P R dˆd The Jacobian of the joint-gradient ĝ at weights ω j α P C

The step size or learning rate β P C The momentum coefficient

$β 1 P C$The first momentum parameter for Adam µ P C d

The momentum buffer λ P C Notation for an arbitrary eigenvalue SppM q P C n

The spectrum -or set of eigenvalues -of M P R nˆn Purely adversarial/cooperative game Spp∇ ω ĝq is purely real/imaginary ρpM q :" max zPSppM q |z|

The spectral radius in R `of M P R nˆn F α,β prµ, ωsq Fixed point op. for CM, or augmented learning dynamics R :" ∇ rµ,ωs F α,β P R 3dˆ3d

Jacobian of the augmented learning dynamics in Corollary 1 α ˚, β ˚:" arg min α,β

## ρpRpα,βqq

The optimal step size and momentum coefficient

$ρ ˚:" ρpRpα ˚, β ˚qq$The optimal spectral radius or convergence rate κ :" max Spp∇ωgq min Spp∇ωgq Condition number, for convex single-objective optimization σ 2 min pM q :" max SppM J M q The minimum singular value of a matrix M

## A Supporting Results

First, some basic results about complex numbers that are used: z " pzq `i pzq " |z| exppi argpzqq

s z " pzq ´i pzq " |z| expp´i argpzqq (18) exppizq `expp´izq " 2 cospzq (

$Ě z 1 z 2 " s z 1 s z 2 (20) 1 {2pz `s zq " pzq19)$) pz 1 z 2 q " pz 1 q pz 2 q ´ pz 1 q pz 2 q (22) z 1 `z2 " p pz 1 q ` pz 2 qq `ip pz 1 q ` pz 2 qq (23) z 1 z 2 " p pz 1 q pz 2 q ´ pz 1 q pz 2 qq `ip pz 1 q pz 2 q ` pz 1 q pz 2 qq (24)

$z 1 z 2 " |z 1 ||z 2 | exppipargpz 1 q `argpz 2 qqq (25) z k " |z| k exppi argpzqkq " |z| k pcospk argpzqq `i sinpk argpzqq(26)$This Lemma shows how we expand the complex-valued momentum buffer µ into its Cartesian components as in [(9)](#b8). Lemma 1. µ j`1 " βµ j ´ĝ j ðñ pµ j`1 q " pβq pµ j q ´ pβq pµ j q ´ pĝ j q, pµ j`1 q " pβq pµ j q ` pβq pµ j q ´ pĝ j q Proof. µ j`1 " βµ j ´ĝ j ðñ µ j`1 " p pβq `i pβqq ` pµ j q `i pµ j q ˘´´ pĝ j q `i pĝ j q ðñ µ j`1 " ` pβq pµ j q ´ pβq pµ j q ˘ì ` pβq pµ j q ` pβq pµ j q ˘´´ pĝ j q `i pĝ j q ðñ µ j`1 "

´ pβq pµ j q ´ pβq pµ j q ´ pĝ j q ¯ì

´ pβq pµ j q ` pβq pµ j q ´ pĝ j q ðñ pµ j`1 q " pβq pµ j q´ pβq pµ j q´ pĝ j q, pµ j`1 q " pβq pµ j q` pβq pµ j q´ pĝ j q

We further assume pĝ j q is 0 -i.e., our gradients are real-valued. This Lemma shows how we can decompose the joint-parameters ω at the next iterate as a linear combination of the joint-parameters, joint-gradient, and Cartesian components of the momentum-buffer at the current iterate as in [(10)](#b9). Lemma 2. ω j`1 " ω j ` pαµ j`1 q ðñ ω j`1 " ω j ´ pαqĝ j ` pαβq pµ j q ´ pαβq pµ j q Proof. pαµ j`1 q " ` pαq pµ j`1 q ´ pαq pµ j`1 q "

´ pαq

´ pβq pµ j q ´ pβq pµ j q ´ĝ j ¯´ pαq ` pβq pµ j q ` pβq pµ j q ˘"

´ pαqĝ j `` pαq ` pβq pµ j q ´ pβq pµ j q ˘´ pαq ` pβq pµ j q ` pβq pµ j q ˘"

´ pαqĝ j `p pαq pβq ´ pαq pβqq pµ j q ´p pαq pβq ` pαq pβqq pµ j q " ´ pαqĝ j ` pαβq pµ j q ´ pαβq pµ j q Thus, ω j`1 " ω j ` pαµ j`1 q ðñ ω j`1 " ω j ´ pαqĝ j ` pαβq pµ j q ´ pαβq pµ j q A.1 Theorem 1 Proof Sketch Theorem 1 (Consequence of Prop. 4.4.1 [[44]](#b43)). Convergence rate of complex momentum: If the spectral radius ρpRq " ρp∇ rµ,ωs F α,β q ă 1, then, for rµ, ωs in a neighborhood of rµ ˚, ω ˚s, the distance of rµ j , ω j s to the stationary point rµ ˚, ω ˚s converges at a linear rate OppρpRq` q j q, @ ą 0.

Proof. We reproduce the proof for a simpler case of quadratic games, which is simple case of [[27]](#b26)'s well-known method for analyzing the convergence of iterative methods. [[44]](#b43) generalizes this result from quadratic games to when we are sufficiently close to any stationary point.

For quadratic games, we have that ĝj " p∇ ω ĝq J ω j . Well, by Lemma 1 and Lemma 2 we have:

$¨ pµ j`1 q pµ j`1 q ω j`1 ‚" R ¨ pµ j q pµ j q ω j ‚(27)$By telescoping the recurrence for the j th augmented parameters:

$¨ pµ j q pµ j q ω j ‚" R j ¨ pµ 0 q pµ 0 q ω 0 ‚(28)$We can compare µ j with the value it converges to µ ˚which exists if R is contractive. We do the same with ω. Because µ ˚" Rµ ˚" R j µ ˚:

$¨ pµ j q ´ pµ ˚q pµ j q ´ pµ ˚q ω j ´ω˚‚ " R j ¨ pµ 0 q ´ pµ ˚q pµ 0 q ´ pµ ˚q ω 0 ´ω˚‚(29)$By taking norms:

$› › › › › › ¨ pµ j q ´ pµ ˚q pµ j q ´ pµ ˚q ω j ´ω˚‚ › › › › › › 2 " › › › › › › R j ¨ pµ 0 q ´ pµ ˚q pµ 0 q ´ pµ ˚q ω 0 ´ω˚‚ › › › › › › 2(30)$ùñ

$› › › › › › ¨ pµ j q ´ pµ ˚q pµ j q ´ pµ ˚q ω j ´ω˚‚ › › › › › › 2 ď › › R j › › 2 › › › › › › ¨ pµ 0 q ´ pµ ˚q pµ 0 q ´ pµ ˚q ω 0 ´ω˚‚ › › › › › › 2(31)$With Lemma 11 from [[90]](#b89), we have there exists a matrix norm @ ą 0 such that:

$}R j } ď pρ pRq ` q j(32)$We also have an equivalence of norms in finite-dimensional spaces. So for all norms }¨}, DC ě B ą 0 such that:

$B}R j } ď }R j } 2 ď C}R j }(33)$Combining ( [32](#formula_29)) and [(33)](#b32) we have:

$› › › › › › ¨ pµ j q ´ pµ ˚q pµ j q ´ pµ ˚q ω j ´ω˚‚ › › › › › › 2 ď C pρ pRq ` q j › › › › › › ¨ pµ 0 q ´ pµ ˚q pµ 0 q ´ pµ ˚q ω 0 ´ω˚‚ › › › › › › 2(34)$So, we have:

$› › › › › › ¨ pµ j q ´ pµ ˚q pµ j q ´ pµ ˚q ω j ´ω˚‚ › › › › › › 2 " OppρpRq ` q j q(35)$Thus, we converge linearly with a rate of OpρpRq ` q.

## A.2 Characterizing the Augmented Dynamics Eigenvalues

Here, we present polynomials whose roots are the eigenvalues of our the Jacobian of our augmented dynamics SppRq, given the eigenvalues of the Jacobian of the joint-gradient vector field Spp∇ ω ĝq.

We use a similar decomposition as [[25]](#b24).

We can expand ∇ ω ĝ " P T P ´1 where T is an upper-triangular matrix and λ i is an eigenvalue of ∇ ω ĝ. The command gives us the polynomial associated with eigenvalue λ k " r `iu:

$p k pxq " ´a2 x`a 2 `acrx`iacux`2ax 2 ´2ax´b 2 x`b 2 `bdrx`ibdux´crx 2 ´icux 2 ´x3 `x2(38)$Consider the case where λ k is imaginary -i.e, r " 0 -which is true in all purely adversarial and bilinear zero-sum games. Then [(38)](#b37) simplifies to:

$p k pxq " ´a2 x `a2 `iacux `2ax 2 ´2ax ´b2 x `b2 `ibdux ´icux 2 ´x3 `x2(39)$Our complex λ k come in conjugate pairs where λ k " u k i and λk " ´uk i. [(39)](#b38) has the same roots for λ k and λk , which can be verified by writing the roots with the cubic formula. This corresponds to spiraling around the solution in either a clockwise or counterclockwise direction. Thus, we restrict to analyzing λ k where u k is positive without loss of generality.

If we make the step size α real -i.e., d " 0 -then (39) simplifies to:

$p k pxq " xp´a 2 `iacu ´2a ´b2 q `a2 `x2 p2a ´icu `1q `b2 ´x3(40)$Using a heuristic from single-objective optimization, we look at making step size proportional to the inverse of the magnitude of eigenvalue k -i.e., α k " α 1 |λ k | " α 1 u k . With this, (40) simplifies to:

$p k pxq " xp´a 2 `iaα 1 ´2a ´b2 q `a2 `x2 p2a ´iα 1 `1q `b2 ´x3(41)$Notably, in [(41)](#b40) there is no dependence on the components of imaginary eigenvalue λ k " r `iu " 0 `iu, by selecting a α that is proportional to the eigenvalues inverse magnitude. We can simplify further with a 2 `b2 " |β| 2 :

$p k pxq " xp pβqpiα 1 ´2q ´|β| 2 q `x2 p2 pβq ´iα 1 `1q `|β| 2 ´x3(42)$We could expand this in polar form for β by noting pβq " |β| cospargpβqq:

$p k pxq " xp|β| cospargpβqqpiα 1 ´2q ´|β| 2 q `x2 p2|β| cospargpβqq ´iα 1 `1q `|β| 2 ´x3 (43)$We can simplify further by considering an imaginary β -i.e., pβq " 0 or cospargpβqq " 0:

$p k pxq " |β| 2 ´x|β| 2 ´x2 piα 1 ´1q ´x3(44)$The roots of these polynomials can be trivially evaluated numerically or symbolically with the by plugging in β, α, and λ k then using the cubic formula. This section can be easily modified for the eigenvalues of the augmented dynamics for variants of complex momentum by defining the appropriate R and modifying the Mathematica command to get the characteristic polynomial for each component, which can be evaluated if it is a sufficiently low degree using known formulas.

## A.3 Convergence Bounds

Corollary 1 (Convergence of Complex Momentum). There exist α P R, β P C so Algorithm 1 converges for bilinear zero-sum games. More-so, for small (we show for " π 16 ), if argpβq " (i.e., almost-positive) or argpβq " π ´ (i.e., almost-negative), then we can select α, |β| to converge.

Proof. Note that Theorem 1 bounds the convergence rate of Algorithm 1 by SppRq. Also, [(40)](#b39) gives a formula for 3 eigenvalues in SppRq given α, β, and an eigenvalue λ P Spp∇ ω ĝq. The formula works by giving outputting a cubic polynomial whose roots are eigenvalues of SppRq, which can be trivially evaluated with the cubic formula.

We denote the k th eigenspace of Spp∇ ω ĝq with eigenvalue λ k " ic k and |c 1 | ď ¨¨¨ď |c n |, because bilinear zero-sum games have purely imaginary eigenvalues due to ∇ ω ĝ being antisymmetric. Eigenvalues come in a conjugate pairs, where λk " ip´c k q If we select momentum coefficient β " |β| exppi argpβqq and step size α k "

$α 1 k |c k | ,$and use that λ P Spp∇ ω ĝq are imaginary, then -as shown in Appendix Section A.2 -(40) simplifies to: p k pxq " xp|β| cospargpβqqpiα 1 k ´2q ´|β| 2 q `x2 p2|β| cospargpβqq ´iα 1 k `1q `|β| 2 ´x3 (45) So, with these parameter selections, the convergence rate of Algorithm 1 in the k th eigenspace is bounded by the largest root of (45). First, consider argpβq " π ´ , where " π 16 . We select α 1 k " 0.75 (equivalently, α k " 0.75 |c k | ) and |β| " 0.986 via grid search. Using the cubic formula on the associated ppxq from (45) the maximum magnitude root has size « 0.9998 ă 1, so this selection converges in the k th eigenspace. So, selecting: α ď min k α k (46) " min k 0.75 c k (47) " 0.75 max k c k (48) " 0.75 }∇ ω ĝ} 2

with β " 0.986 exppipπ ´ qq will converge in each eigenspace. Now, consider argpβq " " π 16 with α 1 k " 0.025 and |β| " 0.9. Using the cubic formula on the associated ppxq from (45) the maximum magnitude root has size « 0.973 ă 1, so this selection converges in the k th eigenspace. So, selecting:

$α ď min k α k(50)$"

$min k 0.025 c k (51) " 0.025 max k c k (52) " 0.025 }∇ ω ĝ} 2(53)$with β " 0.9 exppi q will converge in each eigenspace.

Thus, for any of the choices of argpβq we can select α, |β| that converges in every eigenspace, and thus converges.

In the preceding proof, our prescribed selection of α depends on knowing the largest norm eigenvalue of Spp∇ ω ĝq, because our selections of α 9

1 }∇ω ĝ}2 . We may not have access to largest norm eigenvalue of Spp∇ ω ĝq in-practice. Nonetheless, this shows that a parameter selection exists to converge, even if it may be difficult to find. Often, in convex optimization we describe choices of α, β in terms of the largest and smallest norm eigenvalues of Spp∇ ω ĝq (i.e. the Hessian of the loss) [[91]](#b90).

## B Algorithms

Here, we include additional algorithms, which may be of use to some readers. Algorithm 3 show aggregated momentum [[26]](#b25). Algorithm 4 shows the recurrently linked momentum that generalizes and unifies aggregated momentum with negative momentum [[25]](#b24). Algorithm 5 shows our algorithm with alternating updates, which we use for training GANs. Algorithm 6 shows our method with all real-valued objects, if one wants to implement complex momentum in a library that does not support complex arithmetic. Algorithm 3 Aggregated Momentum 1: Select number of buffers K P N 2: Select β pkq P r0, 1q for k " 1 . . . K 3: Select α pkq P R `for k " 1 . . . K 4: Initialize µ 0 pkq for k " 1 . . . K 5: for j " 1 . . . N do 6:

for k " 1 . . . K do 7:

$µ j`1$pkq " β pkq µ j pkq ´ĝ j 8:

$ω j`1 " ω j `řK k"1 α pkq µ j`$1 pkq return ω N Algorithm 4 Recurrently Linked Momentum 1: Select number of buffers K P N 2: Select β pl,kq P R for l " 1 . . . K and k " 1 . . . K 3: Select α pkq P R `for k " 1 . . . K 4: Initialize µ 0 pkq for k " 1 . . . K 5: for j " 1 . . . N do 6:

for k " 1 . . . K do 7:

$µ j`1$pkq " ř l β pl,kq µ j plq ´ĝ j 8:

$ω j`1 " ω j `řK k"1 α pkq µ j`1 pkq return ω N Algorithm 5 (AltCM) Momentum 1: Select β P C, α P R 2: Initialize µ 0 A , µ 0 B 3: for j " 1 . . . N do 4: µ j`1 A " βµ j A ´gj A 5: θ j`1 A " θ j A ` pαµ j`1 A q 6: µ j`1 B " βµ j B ´gB pθ j`1 A , θ j B q 7: θ j`1 B " θ j B ` pαµ j`1 B q return ω N Algorithm 6 (SimCM) Complex Momentum -R valued 1:$Select pβq, pβq, pαq, pαq P R 2: Select pβq, pβq, pαq, pαq P R 3: Initialize pµq 0 , pµq 0 4: for j " 1 . . . N do 5: pµ j`1 q " pβq pµ j q ´ pβq pµ j q ´ĝ j 6: pµ j`1 q " pβq pµ j q ` pβq pµ j q 7: ω j`1 " ω j ´ pαqĝ j ` pαβq pµ j q´ pαβq pµ j q return ω N

## B.1 Complex Momentum in PyTorch

Our method can be easily implemented in PyTorch 1.6+ by using complex tensors. The only necessary change to the SGD with momentum optimizer is extracting the real-component from momentum buffer as with JAX -see here.

In older versions of Pytorch, we can use a tensor to represent the momentum buffer µ, step size α, and momentum coefficient β. Specifically, we represent the real and imaginary components of the complex number independently. Then, we redefine the operations __add__ and __mult__ to satisfy the rules of complex arithmetic -i.e., equations ( [23](#)) and [(24)](#b23). 

## C Experiments

## C.2 Optimization in Purely Adversarial Games

We include the alternating update version of Figure [4b](#fig_2) in Figure [8](#), which allows us to contrast simultaneous and alternating updates. With alternating updates on a Dirac-GAN for α " 0.1 the best value for the momentum coefficient β was complex, but we could converge with real, negative momentum. Simultaneous updates may be a competitive choice with alternating updates, only if alternating updates cost two gradient evaluations per step, which is common in deep learning setups. " }v1:337}1 }v}1 . If this ratio is near 1 (or 0) and say the eigenvector mostly points at D (or G). The blue eigenvector mostly points at G, while the orange eigenvector is unclear. Finding useful ways to attribute eigenvalues to players is an open problem. Bottom: The spectrum of the Jacobian of the joint-gradient Spp∇ ω ĝj q is shown in log-polar coordinates, because it is difficult to see structure when graphing in Cartesian (i.e., and ) coordinates, due to eigenvalues spanning orders of magnitude, while being positive and negative. The end of training is when we stop making progress on the log-likelihood. We have imaginary eigenvalues at argpλq " ˘π{2, positive eigenvalues at argpλq " 0, and negative eigenvalues at argpλq " ˘π. Takeaway: There is a banded structure for the coloring of the eigenvalues that persists through training. We may want different optimizer parameters for the discriminator and generator, due to asymmetry in their associated eigenvalues. Also, the magnitude of the eigenvalues grows during training, and the args spread out indicating the game can change eigenstructure near solutions.

## Momentum phase argpβq

Step size α " 0.001

## Momentum Magnitude |β|

Step Size α " 0.003 NLL, lower = better Figure [10](#fig_8): Heatmaps of the negative log-likelihood (NLL) for tuning argpβq, |β| with various fixed α on a 2D mixture of Gaussians GAN. We highlight the best performing cell in red, which had argpβq « π {8. Runs equivalent to alternating SGD are shown in a magenta box. We compare to negative momentum with alternating updates as in [[25]](#b24) in the top row with argpβq " π. Left: Tuning the momentum with α " 0.001. Right: Tuning the momentum with α " 0.003.    [1](#).

![Figure 3: Complex momentum helps correct rotational dynamics when training a Dirac-GAN [42].Left: Parameter trajectories with step size α " 0.1 and momentum β " 0.9 exppi π {8q. We include the classical, real and positive momentum which diverges for any step size. Right: The distance from optimum, which has a linear convergence rate matching our prediction with Theorem 1 and (14).]()

![Figure4(a): We show the real part of our momentum buffer -which dictates the parameter update -at the 50 th iteration pµ 50 q dependence on past gradients ĝk for k " 1 . . . 50. The momentum magnitude is fixed to |β| " 0.9 as in Figure3. Euler's formula is used in(8) to for finding dependence or coefficient of ĝk via pµ 50 q " ´řk"50 k"0 |β| k cospk argpβqqĝ j´k . Complex momentum allows smooth changes in the buffers dependence on past gradients.]()

![Figure6: The spectrum of the augmented learning dynamics R is shown, whose spectral norm is the convergence rate in Theorem 1. Each image is a different momentum phase argpβq for a range of α,|β| P r0,1s. The opacity of an eigenvalue (eig) is the step size α and the color corresponds to momentum magnitude |β|. A red unit circle shows where all eigs must lie to converge for a fixed α, β. If the max eig norm ă 1, we draw a green circle whose radius is our convergence rate and a green star at the associated eig. Notably, at every non-real β we can select α,|β| for convergence. The eigs are symmetric over the x-axis, and eigs near pλq " 1 dictate convergence rate. Eigs near the center are due to state augmentation, have small magnitudes, and do not impact convergence rate. Simultaneous gradient descent corresponds to the magenta values where |β| " 0.]()

![J pγAqy `xJ ppI ´γqB 1 qx ´yJ ppI ´γqB 2 qy (16) = 0 (classical) arg( ) = 8 (ours) arg( ) = 2 (ours) arg( ) = (negative) EG OG GDA # grad. eval. to converge]()

![Phase of eigenvalue argpλqSpectrum of Jacobian of joint-grad Spp∇ ω ĝj q for GAN Log-magnitude of eigenvalue logp|λ|q disc.unsure gen.]()

![Figure7: A log-polar coordinate visualization reveals structure in the spectrum for a GAN at the end of the training on a 2D mixture of Gaussians with a 1-layer (disc)riminator and (gen)erator, so the joint-parameters ω P R 723 . It is difficult to see structure by graphing the Cartesian (i.e., and ) parts of eigenvalues, because they span orders of magnitude, while being positive and negative. Appendix Figure9shows the spectrum through training. There is a mixture of many cooperative (i.e., real or argpλq « 0, ˘π) and some adversarial (i.e., imaginary or argpλq « ˘π 2 ) eigenvalues, so -contrary to what the name may suggest -generative adversarial networks are not purely adversarial. We may benefit from optimizers leveraging this structure like complex momentum. Eigenvalues are colored if the associated eigenvector is mostly in one player's part of the joint-parameter space -see Appendix Figure9for details on this. Many eigenvectors lie mostly in the the space of (or point at) a one player. The structure of the set of eigenvalues for the disc. (green) is different than the gen. (red), but further investigation of this is an open problem. Notably, this may motivate separate optimizer choices for each player as in Section 4.4.]()

![up into components for each eigenvalue, giving us submatrices R k P C 3ˆ3 : the characteristic polynomial of R k with the following Mathematica command, where we use substitute the symbols r `iu " λ k , a " pβq, b " pβq, c " pαq, and d " pαq. CharacteristicPolynomial[{{a, -b, -(r + u I)}, {b, a, 0}, {a c -b d, -(b c + a d), 1 -c (r + u I)}}, x]]()

![Computing Infrastructure and RuntimeFor the purely adversarial experiments in Sections 4.1 and 4.2, we do our computing in CPU. Training each 2D GAN in Section 4.3 takes 2 hours and we can train 10 simultaneously on an NVIDIA T4 GPU. Training each CIFAR GAN in Section 4.4 takes 10 hours and we can only train 1 model per NVIDIA T4 GPU.]()

![Figure 9: These plots investigate the spectrum of the Jacobian of the joint-gradient for the GAN in Figure 7 through training. The spectrum is key for bounding convergence rates in learning algorithms. Top left: The Jacobian ∇ ω ĝ for a GAN on a 2D mixture of Gaussians with a two-layer, fully-connected 16 hidden unit discriminator (D) and generator (G) at the end of training. In the concatenated parameters ω P R 723 , the first 337 are for D, while the last 386 are for G. We display the log of the absolute value of each component plus " 10 ´10 . The upper left and lower right quadrants are the Hessian of D and G's losses respectively. Top Right: We visualize two randomly sampled eigenvectors from ∇ ω ĝ. The first part of the parameters is for the discriminator, while the second part is for the generator. Given an eigenvalue with eigenvector v, we roughly approximate attributing eigenvectors to players by calculating how much of it lies in D's parameter space with }v 1:|D| }1 }v}1]()

![Figure 11(a): Mixture of Gaussian samples from GAN with the best hyperparameters from the heatmaps in Appendix Figure 10]()

![Figure 11(b): Class-conditional CIFAR-10 samples from GAN with the best hyperparameters from the heatmap in 12a]()

![Figure 12(a): The inception score (IS) for a grid search on argpβ1q and |β1| for training Big-GAN on CIFAR-10 with the Adam variant in Algorithm 2. The β1 is complex for the discriminator, while the generator's optimizer is fixed to authorsupplied defaults. Red points are runs that failed to train to the minimum IS in the color bar. The vertical magenta line denotes runs equivalent to alternating SGD. Negative momentum failed to train for any momentum magnitude |β1| ą .5, so we do not display it for more resolution near values of interest.]()

