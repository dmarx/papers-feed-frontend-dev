<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GFlowVLM: Enhancing Multi-step Reasoning in Vision-Language Models with Generative Flow Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-03-09">9 Mar 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Haoqiang</forename><surname>Kang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Honda Research Institute</orgName>
								<address>
									<country>USA;</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Diego</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Enna</forename><surname>Sachdeva</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Honda Research Institute</orgName>
								<address>
									<country>USA;</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Piyush</forename><surname>Gupta</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Honda Research Institute</orgName>
								<address>
									<country>USA;</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sangjae</forename><surname>Bae</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Honda Research Institute</orgName>
								<address>
									<country>USA;</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kwonjoon</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Honda Research Institute</orgName>
								<address>
									<country>USA;</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Honda Research Institute</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GFlowVLM: Enhancing Multi-step Reasoning in Vision-Language Models with Generative Flow Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-03-09">9 Mar 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">8EF3DB75671C4302E508D32D228CC123</idno>
					<idno type="arXiv">arXiv:2503.06514v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-03-16T06:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Vision-Language Models (VLMs) have recently shown promising advancements in sequential decision-making tasks through task-specific fine-tuning. However, common fine-tuning methods, such as Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) techniques like Proximal Policy Optimization (PPO), present notable limitations: SFT assumes Independent and Identically Distributed (IID) data, while PPO focuses on maximizing cumulative rewards. These limitations often restrict solution diversity and hinder generalization in multi-step reasoning tasks. To address these challenges, we introduce a novel framework, GFlowVLM, a framework that fine-tune VLMs using Generative Flow Networks (GFlowNets) to promote generation of diverse solutions for complex reasoning tasks. GFlowVLM models the environment as a non-Markovian decision process, allowing it to capture long-term dependencies essential for real-world applications. It takes observations and task descriptions as inputs to prompt chain-of-thought (CoT) reasoning which subsequently guides action selection. We use task based rewards to fine-tune VLM with GFlowNets. This approach enables VLMs to outperform prior fine-tuning methods, including SFT and RL. Empirical results demonstrate the effectiveness of GFlowVLM on complex tasks such as card games (NumberLine, BlackJack) and embodied planning tasks (ALFWorld), showing enhanced training efficiency, solution diversity, and stronger generalization capabilities across both in-distribution and out-ofdistribution scenarios.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Vision-Language Models (VLMs) have achieved remarkable results in generalized tasks such as image captioning and visual question answering <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b44">45]</ref>. However, they struggle with structured reasoning in sequential decision making tasks that require causal understanding data <ref type="bibr" target="#b2">[3]</ref>, especially in long horizon planning for tasks such as embod-ied AI, where agent must capture long term dependencies.</p><p>Recent advancements in large language models demonstrate emergent reasoning capabilities by leveraging Chainof-Thought (CoT) <ref type="bibr" target="#b34">[35]</ref> reasoning, that enhances decisionmaking in multi-step interactive environments <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b34">35]</ref>. Typically, these models are fine-tuned using specialized visual instruction-following datasets through Supervised Fine Tuning (SFT) methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20]</ref>, without active interaction with the environment, or optimized through Reinforcement Learning (RL) approaches, such as Proximal Policy Optimization (PPO) <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b40">41]</ref>. However, SFT approaches often limits generalization to unseen scenarios, as training relies on maximizing the likelihood over a limited, specialized dataset, thereby restricting diversity in the solution space <ref type="bibr" target="#b16">[17]</ref>. Furthermore, for cases where RL methods tend to focus on immediate rewards, can hinder the model's ability to consider long-term outcomes. Consequently, limited exploration in these models can result in an oversight of more optimal long-term strategies, leading to suboptimal performance in complex tasks <ref type="bibr" target="#b43">[44]</ref>.</p><p>Unlike traditional reinforcement learning (RL) methods, which focus on maximizing cumulative rewards <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>, Generative Flow Networks (GFlowNets) <ref type="bibr" target="#b0">[1]</ref> train stochastic policies to sample diverse, high-reward sequences (e.g., token sequences) with probabilities proportional to a specified reward function R(x) <ref type="bibr" target="#b1">[2]</ref>. This approach samples sequences based on the reward function's distribution, enabling it to find a broader range of high-reward solutions beyond those typically identified by reward-maximizing techniques. Recent studies have applied GFlowNets to multi-step reasoning within the Large Language Models (LLMs) framework, demonstrating their effectiveness over maximum likelihood training and traditional reward-maximization methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b39">40]</ref>. However, these methods lack the multimodal capabilities essential for embodied AI tasks that require integrated visual and textual inputs. Additionally, related works, such as FoR <ref type="bibr" target="#b39">[40]</ref>, assume Markovian structures within this framework, which may not capture the dependencies necessary for complex, long-horizon, real-world reasoning tasks.</p><p>To address these gaps in VLMs, we propose a novel approach GFlowVLM-that leverages GFlowNets for endto-end fine-tuning in sequential tasks, explicitly modeling non-Markovian flows for richer multimodal reasoning. This framework is, to our knowledge, the first to integrate GFlowNets with VLMs in an end-to-end setting, tackling the unique challenges of multimodal, sequential reasoning essential for complex planning tasks. Our approach initializes policy with a pretrained VLM and fine-tunes it using GFlowNets, guiding VLMs toward structured reasoning processes that capture logical dependencies between successive states. By implicitly representing reasoning as a tree structure-where nodes correspond to states with prior actions and observations, and edges represent actions leading to the next state-GFlowVLM enhances efficient learning of diverse and complex reasoning sequences. Empirical results demonstrate that our method outperforms standard fine-tuning techniques, such as SFT and RL methods like PPO, in achieving structured, multimodal reasoning.</p><p>Our main contributions are as follows: • We introduce a novel framework that integrates GFlowNets with common-sense capabilities of VLMs for multi-step decision making tasks, enhancing their reasoning abilities. To the best of our knowledge, this is the first work to explore this integration. • By fine-tuning VLMs with GFlowNets, we improve their capacity to handle complex reasoning tasks, enabling better exploration of reasoning paths, generating diverse solutions, achieving stronger generalization to out of distribution tasks. • Through extensive experimentation, we demonstrate that our framework achieves better training efficiency, higher success rate and diversity in solution generation tasks compared to existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works Multi-Step Reasoning with Vision Language Models</head><p>Recent research has advanced the reasoning capabilities of large foundation models through specialized prompting techniques <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b38">39]</ref> and fine-tuning methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b31">32]</ref> that often add MLP or transformer layers to frozen models to interface with action spaces. Reinforcement learning from human feedback (RLHF) also aids in developing reward models <ref type="bibr" target="#b25">[26]</ref>. The RL4VLM approach <ref type="bibr" target="#b40">[41]</ref> uses PPO to train VLMs but lacks the structured reasoning enabled by our GFlowNets method, which is designed for deeper understanding of complex tasks. VLM reasoning in interactive environments, particularly embodied AI, has gained attention <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b40">41]</ref>, but our GFlowNets approach uniquely enables structured reasoning, enhancing task comprehension.</p><p>GFlowNets GFlowNets <ref type="bibr" target="#b0">[1]</ref> were originally created to learn sampling policies from unnormalized distributions, primarily aiding scientific discovery by generating diverse, high-reward samples <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b29">30]</ref>. They have since been applied in recommendation systems <ref type="bibr" target="#b20">[21]</ref>, domain adaptation <ref type="bibr" target="#b45">[46]</ref>, combinatorial optimization <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b41">42]</ref>, and enhancing neural network interpretability <ref type="bibr" target="#b15">[16]</ref>. GFlowNets also support sampling from complex posteriors <ref type="bibr" target="#b8">[9]</ref>, sparse reward RL <ref type="bibr" target="#b26">[27]</ref>, and multi-objective optimization <ref type="bibr" target="#b11">[12]</ref>. Recent adaptations fine-tune LLMs for multi-step reasoning tasks <ref type="bibr" target="#b39">[40]</ref>, yet lack the multimodal capability for embodied AI planning, which we address in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminaries</head><p>GFlowNets GFlowNets are models that amortize the cost of sampling from a target distribution over terminal states X by learning an approximation of this distribution based on its reward function. Given a directed acyclic graph (DAG) G = (S, A) with states S and directed actions A, there is an initial state s 0 and terminal states X ⊂ S. A trajectory τ = (s 0 → . . . → s n ) represents a complete sequence ending in a terminal state x ∈ X . The trajectory flow F : T → R + defines flows over trajectories, with state flow F (s) = s∈τ F (τ ). A forward policy P F (•|s), often parametrized by a neural network, induces a distribution over trajectories and a marginal distribution over terminal states, with probabilities given by: P</p><formula xml:id="formula_0">F (τ ) = P F (s 0 → . . . → s n ) = n-1 t=0 P F (s t+1 |s t ) ∀τ ∈ T . Similarly, a backward policy P B (τ ) = P B (s n → . . . → s 0 ) = n-1</formula><p>t=0 P B (s t |s t+1 ) ∀τ ∈ T . Given a non-negative reward function R : X → R + , GFlowNets aim to estimate a policy where the likelihood of sampling x ∈ X is proportional to R(x). Thus, there exists a constant Z such that: R(x) = Z τ =(s0→...→sn=x) P F (τ ) ∀x ∈ X , where Z = F (s 0 ) = τ ∈T F (τ ) is total flow at the initial state.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Off-Policy Training</head><p>An advantage of GFlowNets is their ability to leverage off-policy training data by reusing transitions from past trajectories to update the forward policy P F <ref type="bibr" target="#b0">[1]</ref>. Unlike on-policy reinforcement learning methods, GFlowNets handle diverse, multimodal distributions effectively, using off-policy samples to approximate R(x). This approach improves sample efficiency and accelerates convergence, especially in settings where generating trajectories is costly or where leveraging prior data is beneficial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Motivating Experiment</head><p>To demonstrate the limitations of traditional approaches and highlight the dependencies captured by GFlowNets, we design a toy experiment combining two types of numerical sequences: (i) the Fibonacci sequence, defined by</p><formula xml:id="formula_1">F (n) = F (n -1) + F (n -2)</formula><p>, where each term is the sum of the previous two, and (ii) an arithmetic sequence with a constant increment, S(n) = S(n -1) + k, where k is a</p><p>1, 2, 3 Prompt: Given the sequence of numbers in the image, predict the next number. First, read and list the numbers from the image, and format them as follows: State 0: [first number in the sequence]; State 1: [second number in the sequence]; State 2: [third number in the sequence]; A er reading the numbers, output your predic on for the next number in the sequence in this format: State 3: [predicted number]; PPO GFlownets State 0: 1; State 1: 2; State 2: 3; State 3: 4; State 0: 1; State 1: 2; State 2: 3; State 3: 4; State 3: 5; x 1 x 2 Fabonacci Arithmetic Figure 1. Overview of the prediction of diverse sequence using Gflownets as compared to PPO. The model takes the image of sequence and prompt as input, and generates the next number of sequence by implicitly modeling the causality. Methods Temp. α SR (%) # Solutions w/o fine tuning 1 15.7 1.10 w/o fine tuning 1.2 16.1 1.12 SFT 1 21.7 1.03 SFT 1.2 22.0 1.09 PPO 1 50.2 1.13 PPO 1.2 49.8 1.15 GFlowVLM 1 76.4 1.60 GFlowVLM 1.2 77.9 1.61 Table 1. Results of motivating experiments. α denotes the temperature parameter of decoding. fixed step size (e.g., k = 2 for sequences like [2, 4, 6, . . . ]).</p><p>The task presents the model with an image of a partial sequence and a prompt (as shown in Fig. <ref type="figure">1</ref>), to predict the next number in the sequence. We evaluate the performance of fine-tuning VLM (LLAVA-v1.6-Mistral-7B <ref type="bibr" target="#b18">[19]</ref>) using SFT, PPO, and GFlowNets, with temperature parameters α = 1 and α &gt; 1 to assess stochastic performance, as shown in Tab.</p><p>1. Success rate (SR), measured as the percentage of correct next-number predictions across 1,000 samples, shows GFlowVLM outperform PPO by 26% and generate 40% more diverse solutions. Compared to SFT, GFlowVLM achieves a 54% higher success rate and yield 59% more diversity in responses, underscoring their strength in learning and generalizing causal structures. This advantage stems from GFlowNets' ability to infer underlying causal reasoning structure of sequence by sequentially sampling reasoning paths, in contrast to the limited diversity observed with SFT and PPO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methodology</head><p>This work utilizes GFlowNet's structure learning to enhance the VLM's ability to obtain high-quality, diverse solutions whose distribution is proportional to the reward function. By fine tuning VLMs using GFlowNets, it allows the solutions to be sampled from the distribution of the reward function, which prevents learning policies settled around a small number of modes. Fig. <ref type="figure">2</ref> shows the overall pipeline of our proposed framework.</p><p>The model takes current observation image o t and designed task specific prompt p t as the input. p t contains the description of the goal, history actions a 1:t-1 , history states s 1:t-1 and admissible action space corresponding to the current observation o t . To incorporate Non-Markovian assumption, input z 0:t include history actions a 0:t and states s 0:t , respectively along with the input image o t . * The desired output format includes the CoT reasoning c t and action a t , where a t directly interacts with the environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">VLM as a policy: Fine tuning VLMs using GFlowNets to estimate actions</head><p>We use a non-Markovian approach, essential for reasoning tasks that depend on multiple past states to capture long-term dependencies, and tackle longer sequences-challenges that the Markovian assumption cannot adequately address. We fine tune VLM of LLaVA <ref type="bibr" target="#b18">[19]</ref> as a policy for structured reasoning, where VLM serves as the forward policy P F , selecting the next action a t that advance the reasoning chain at every step t. For each task W, the model takes the visual observation o t and prompt p t as inputs, and outputs the CoT and action.</p><p>Prompt Design We retain the same prompt format as <ref type="bibr" target="#b40">[41]</ref> for a fair comparison. However, to incorporate historical context in decision-making, we modify the prompt template to include the history of states and actions predicted by the VLM, as shown in Table <ref type="table">Tab</ref>. 2. The textual prompt p t contains the goal description g, the history of states s 0:t and actions a 0:t , and the action space A t+1 available after interacting with the environment. For certain tasks q that may contain observation-dependent information, such as the textual description d(o t+1 ) of the observation o t+1 , the function f generates the prompt p t+1 as:</p><formula xml:id="formula_2">p t+1 = f (d(o t+1 ) • I {Task=q} ,</formula><p>s 0:t , a 0:t , A t+1 ), where I is an indicator function which is 1 only for a certain task q if the observation-dependent information is available. Action Selection Before selecting an action at each step t, we incorporate a CoT reasoning mechanism, where the model generates intermediate reasoning steps to guide the action selection process. At time t, the VLM first generates Language model MLP Projector Vision Encoder Text Tokenizer Environment Terminal Rewards Gflownet Update VLM Buffer Action CoT Goal Description History states History actions Action space Desired output: 1. CoT 2. Action Figure 2. Overall framework of proposed method: We propose a framework for fine-tuning large VLMs using Gflownets.The input z0:t at time step t consists of a visual observation ot and an input prompt pt containing goal description, history states s0:t, history actions a0:t, and admissible actions At, and outputs CoT reasoning ct, and action at. The at is executed in the environment to obtain reward rt(st, at), next observation ot+1, and action space At+1. f generates the next prompt pt+1 using description of next observation ot+1 (if applicable), history of states s0:t and actions a0:t and next admissible actions At+1. The sequence of transitions &lt; st, at, rt, ct &gt; is added to buffer to update the forward policy PF using Gflownets. xn represents the terminal state of a sequence. a reasoning CoT c t , which includes a description of the image and intermediate thoughts. Since VLMs are pre-trained on large-scale image-caption data, CoT steps provide additional context and help the model explicitly consider dependencies between different states before selecting the next action. The CoT then guides the action selection. The probabilities for the CoT and action sequences of tokens are defined as follows: P CoT (c t |z 0:t , g; θ) = (1) nc j=1 log P VLM (w j |w &lt;j , z 0:t , g; θ) (2) P Action (a t |c t , z 0:t , g; θ) = (3) na i=1 log P VLM (w i |w &lt;i , c t , z 0:t , g; θ)</p><p>where n c and n a represent the number of tokens in the CoT sequence c t and action sequence a t , respectively, and w i represents the i-th text token in a sequence. Here, P VLM (w i |w &lt;i , z 0:t , c t , g; θ) and P VLM (w j |w &lt;j , z 0:t , g; θ) denote the VLM's token-level log probabilities for the action and CoT sequences, conditioned on previous tokens, the history of states z 0:t , the history of actions a 0:t-1 and goal description g. Then, the final forward policy P F (z t+1 |z 0:t , g; θ) is then computed as a weighted sum of the action probabilities P CoT (c t |z 0:t , g; θ), based on the CoT reasoning, and the original action probabilities P Action (a t |z 0:t , g; θ):</p><formula xml:id="formula_4">P F (z t+1 |z 0:t , g; θ) = P Action (a t |z 0:t , c t , g; θ)+ λP CoT (c t |z 0:t , g; θ),<label>(5)</label></formula><p>where λ ∈ [0, 1] is a weighting factor that controls the influence of the CoT reasoning on the final action selection. The CoT probabilities P CoT (c t |z 0:t , g; θ) provide a structured, intermediate reasoning context that refines the decision-making process, ensuring that the final action is selected with consideration of both direct state information and the model's internal thought process. We perform an ablation study on the effect of λ, and we selected λ = 0.4 in our work, as discussed in Supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training Objectives</head><p>We adopt three different objective functions of GFlowNets, Variance Trajectory-Balanced (TB) <ref type="bibr" target="#b22">[23]</ref>, Subtrajectory-</p><p>CoT prompt pt for task M You are trying to solve a task M. {Description of the task}. The action space of M is {all legal actions a ∈ A}. Use [DONE] when you think you have completed the task. Task: {Task description} State 0: {Initial observation} Action 0: {First action} State 1: {Observation for step 1} Admissible Next Actions: {"action1", "action2", " [DONE]" (if applicable)} Your response should be a valid JSON file in the following format: { "thoughts": "first describe what you see in the image using the text description, then carefully think about which action to take to complete the task.", "action": "an admissible action." } Formatted text output { "thoughts": "Given the current state and previous steps, I should choose [at] as the next action.", "action": "at" } Table 2. A template showing the input prompt and corresponding output. The green text highlights the chain-of-thought reasoning which may contain task-specific descriptions, while the red text indicates the action based on the description.</p><p>Balanced (SubTB) <ref type="bibr" target="#b21">[22]</ref>, and Detailed-Balanced (DB) <ref type="bibr" target="#b1">[2]</ref>, to finetune a VLM. We define z t as the state in the trajectory sequence that includes both a t , s t , and o t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Variance Trajectory Balanced (Var-TB) Loss</head><p>The Trajectory-Balanced (TB) objective ensures that the probability of generating a complete trajectory τ = (z 0 →</p><formula xml:id="formula_5">z 1 → • • • → z n → x) is proportional to the reward R(x).</formula><p>This objective is given by:</p><formula xml:id="formula_6">L VarTB (τ ; θ) = 1 N N i=1 ζ(τ i ; θ) -E τ ζ(τ ; θ) 2 ,<label>(6)</label></formula><p>where ζ(•) is the estimated initial flow (see Eq. 13 for details) and N represents the number of sampled trajectories. The details are available in the Supplementary material. This loss ensures that the high-reward trajectories are sampled more frequently by the policy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Subtrajectory Balanced (SubTB) Loss</head><p>The Subtrajectory-Balanced (SubTB) loss operates on subtrajectories of the form</p><formula xml:id="formula_7">z 0:m = (z 0 → z 1 → • • • → z m ).</formula><p>It ensures that each segment of the reasoning path or structure remains consistent, where the flows are balanced locally between forward and backward transitions. We use a modified version of SubTB loss <ref type="bibr" target="#b7">[8]</ref> as follows:</p><formula xml:id="formula_8">L SubTB (z 0:m , g; θ) = 0≤i&lt;j≤m log R(z 0:i ⊤) j k=i+1 P F (z k |z 0:k-1 , g; θ)P F (⊤|z 0:j , g; θ) R(z 0:i ⊤)P F (⊤|z 0:i , g; θ) 2 (<label>7</label></formula><formula xml:id="formula_9">)</formula><p>where ⊤ is the [DONE] symbol, and process continues until [DONE] symbol ⊤ is generated similar to <ref type="bibr" target="#b7">[8]</ref>. This loss penalizes discrepancies in local transitions and ensures that all subsegments of a trajectory follow the correct balance conditions, reducing variance in smaller parts of the trajectory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.">Detailed Balanced (DB) Loss</head><p>The Detailed-Balanced (DB) loss is used to ensure that each transition z t → z t+1 between two states is balanced by matching the forward and backward flows at every step of the trajectory. Since it takes transition as input, we need dense rewards. The DB loss is formulated as:</p><formula xml:id="formula_10">L DB (z 0:t → z t+1 , g; θ) = log R(z 0:t ⊤)P F (z t+1 |z 0:t , g; θ)P F (⊤|z 0:t+1 , g; θ) R(z 0:t+1 ⊤)P F (⊤|z 0:t , g; θ) 2 .<label>(8)</label></formula><p>This loss ensures that every state-to-state transition follows the correct flow, preventing inconsistencies in the trajectory construction.</p><p>Remark One challenge when implementing both the SubTB and DB losses is accurately estimating the termination probability, P F (⊤|z 0:t , g; θ), which represents the likelihood of reaching a terminal state at any point in the trajectory. Incorrect estimation of this probability can lead to suboptimal training and unbalanced flows.</p><p>To address this, we introduce a new token, [DONE], into the tokenizer to explicitly model the terminal state, and use distinct prompt designs as shown in the Tab. 2. Moreover, we perform an additional SFT step on correctly labeled examples before applying GFlowNets training. This initialization helps the model better estimate termination probabilities, resulting in improved overall performance (See ablation study in Sec. 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We evaluate the performance of GFlowVLM on three distinct tasks that require multi-step visual-language reasoning. The Numberline and Blackjack tasks assess GFlowVLM's arithmetic reasoning capabilities with maximum steps set as 10, while Alfworld focuses on decisionmaking tasks that demand visual-semantic understanding with max steps set as 35 in a sequence. We mainly compare</p><p>Algorithm 1 Training VLM with GFlowNets Input: An environment env, an initial VLM with parameters θ0, a CoT reasoning scaling factor λ, maximum episode length T , number of tasks W , number of collected trajectories per task K. for w = 1, . . . , W do Bw = ∅ for k = 1, . . . , K do t = 0 g, ot, At = env.reset() pt = f (ot, At) while t ≤ T do z0:t = ⟨ot, pt⟩ ct, at = arg max PF (zt+1|z0:t, g; θw-1) rt, ot+1, At+1 = env.step(at) Bw = Bw ∪ {(st, ct, at, rt} pt+1 = f d(ot+1) • I {q} , s0:t, a0:t, At+1 t = t + 1 if t = T or task w is completed then break end if end while end for Update θw-1 on the collected trajectories Bw for task w to obtain θw end for Output: Updated parameters θW after W tasks.</p><p>the performance with RL4VLM <ref type="bibr" target="#b40">[41]</ref> and SFT methods. For fair comparison, we use the same base VLM of LLAVA-v1.6-Mistral-7B <ref type="bibr" target="#b18">[19]</ref>. We conduct 4 independent runs with different random seeds, reporting mean and standard deviation. Episode success rate measures reasoning performance across tasks, while the diversity metric (Div@N) <ref type="bibr" target="#b39">[40]</ref> quantifies unique correct solutions across N samples. The minimum for Div@N is 1. GFlowNets' reward function is modified for Numberline and Blackjack as outlined in Sec. 5.2. RL4VLM We compare with RL4VLM <ref type="bibr" target="#b40">[41]</ref>, which uses PPO to fine-tune the VLM. RL4VLM follows the same environment reward scheme as used in the GymCards tasks, where rewards are set to [0, -1, 1]. Additionally, it em-ploys a Markovian approach by excluding history information from the prompt. To ensure a fair comparison, we modify the original setup with two additional configurations: one that replaces the default environment reward function with our custom reward function, and another that includes history information in the prompt in a non-Markovian manner. These adjustments allow us to evaluate the model's performance under different reward functions and prompt history settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Baselines</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Environments</head><p>Numberline. This task involves moving a current number "Current: y t " to a "Target: t". The agent's goal is to align y t with t by outputting an action a t from the discrete set {"+", "-", [DONE](if applicable)}. In-distribution examples include numbers from 0 to 5, and OOD examples range from 10 to 50. We revise the reward function to replace the original discrete rewards of -1, 0, and 1 with non-negative values as follows: R(target, current) = c |target-current|+1 , where c is a scaling constant set to 100. This reward incentivizes the model to bring the current number closer to the target, progressively increasing the reward as the gap decreases. For fair comparison, we run RL4VLM <ref type="bibr" target="#b40">[41]</ref> with revised reward structure.</p><p>Blackjack. The Blackjack task requires VLM to reason with visual information and adapt to stochastic outcomes. The agent aims to win by selecting an action a t from {"stand", "hit", [DONE](if applicable)}. We revise the reward function to replace the original discrete rewards of -1, 0, and 1 with non-negative values as follows: R(s t , a t ) = max(1 × 10 -10 , (r(s t , a t ) + 1) × 10), where r(s t , a t ) represents the environment's original reward for state s t and action a t . This scales the rewards and ensures they are strictly non-negative. For fair comparison, we run RL4VLM <ref type="bibr" target="#b40">[41]</ref> with revised reward structure.</p><p>ALFWorld ALFWorld <ref type="bibr" target="#b30">[31]</ref> is an embodied AI environment combining a text-based interactive setup with a vision-language planning dataset. ALFWorld has a state-dependent action space A t ; Our prompt instructs the VLM to choose from the admissible actions A t , and we evaluate out-of-distribution (OOD) performance using a test set of previously unseen scenes. We use the same non-negative reward function as used in <ref type="bibr" target="#b40">[41]</ref>, making it suitable for Var-TB and SubTB losses. However, since it lacks dense rewards for every transition, DB does not perform effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results Analysis</head><p>Improved VLM Reasoning abilities on In-distribution samples. Our experiments show that GFlowVLM significantly enhances VLM reasoning in tasks like NumberLine, Diverse Solutions Our method generates more diverse solutions than other baselines. In ALFWorld, GFlowNets achieve 25% and 33% higher diversity than RL4VLM and SFT ( Tab. 4), as measured by diversity metric, capturing a wider range of plausible solutions, offering a distinct advantage in scenarios that benefit from broader strategy exploration.</p><p>Improved Generalization on OOD samples Our method enhances VLM reasoning on OOD examples, with GFlowNets achieving higher OOD success rates than RL4VLM in NumberLine and ALFWorld tasks by 322% and 156%, respectively. This demonstrates GFlowNets' capacity for robust generalization through diverse, accurate trajectory sampling, enabling effective handling of complex, unseen scenarios.</p><p>Benefits from Off-policy data Since GFlowNets allow for off-policy <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b39">40]</ref> along with on-policy learning unlike PPO <ref type="bibr" target="#b28">[29]</ref>, we adopt an off-policy data generation approach to evaluate the impact of using more accurate trajectories during training.</p><p>• Numberline: During training, if the on-policy trajectory produced by the model fails to correctly move the current number towards the target, we add to buffer an additional trajectory with an off-policy ground-truth trajectory. This ensures that the model learns from successful trajectories, even when its own predictions deviate from the correct trajectory. By integrating these ground-truth trajectories, we aim to improve the model's ability to generalize to new instances by leveraging more accurate training data. • Blackjack: Due to the stochastic nature of this task, deterministic ground-truth trajectories are unavailable. Instead, we generate high-quality off-policy trajectories using a rule-based approach: the agent "stands" if the hand value is 17 or higher; otherwise, it "hits". This aligns with basic Blackjack strategy, where drawing on a hand of 17 or more risks exceeding 21, balancing improvement chances with busting risk.</p><p>Tab. 3 shows results for NumberLine (NL), out-ofdistribution NumberLine (NL-OOD), and BlackJack (BJ) tasks. GFlowVLM with Var-TB, SubTB, and DB, demonstrate improvements with offline data , averaging a 36.2% performance increase over online-only approaches. These results indicate that each loss function benefits from offpolicy high-quality data, leveraging both correct and incorrect solutions to enhance performance, even in challenging OOD scenarios.</p><p>Method Assump. Pick Look Clean Heat Cool Pick2 Avg. OOD Div@16 SFT-w/o-[DONE] -39.2 0 14.4 11.1 0 28.6 17.1 3.3 1.06 SFT-w/-[DONE] -32.7 0 10.3 10.8 0 21.8 15.9 3.0 1.02 RL4VLM [41] M 47.4 14.7 10.4 14.4 18.8 18.0 21.7 4.8 1.12 RL4VLM [41] NM 49.1 13.5 9.8 15.2 20.1 20.6 22.1 6.1 1.11 GFlowVLM w/ SubTB NM 50.0 23.1 10.0 18.7 24.3 23.7 26.1 12.3 1.40 GFlowVLM w/ Var-TB NM 50.0 22.2 10.2 16.1 22.7 21.9 25.7 10.9 1.41 Table <ref type="table">4</ref>. Results of ALFWorld. Since Alfworld does not provide dense rewards, we can not not using DB loss here. Furthermore, while RL4VLM and GFVLM with SubTB are trained with SFT initialization, GFVLM with TB-Var is without STF initialization since we do not need to model the flow.</p><p>Figure <ref type="figure">3</ref>. The training curves of in-distribution episode success rates (%) of different methods across three tasks. For the Numberline and BlackJack, RL4VLM has been trained with original reward function, while the GFlowVLM with various loss functions have been trained with revised reward function. This is because RL4VLM serves a strong baseline when trained with original rewards. Since, we did not revise the rewards for ALFWorld, we used the same reward function across all the methods. These models are trained with On-policy sampling method.</p><p>Training Efficiency As shown in Fig. <ref type="figure">3</ref>, GFlowNets converge faster than RL4VLM on NumberLine, Blackjack, and ALFWorld, reaching optimal performance with significantly fewer environment steps-about 10,000 fewer than RL4VLM. This efficiency reduces training time and computational demands, supporting scalability for complex reasoning tasks.</p><p>Comparisons with different loss functions As shown in the training curves, all three loss functions-DB, Var-TB, and SubTB-converge at a similar rate. DB, which requires dense rewards for every transition since it utilize transitions as input, demonstrates the best generalization, as evidenced in Tab. 3, in the NumberLine and Blackjack tasks. Both SubTB and TB achieve comparable performance in terms of in-distribution and OOD generalization, making them equally effective for a wide range of reasoning tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SFT Initialization for SubTB and DB Losses</head><p>The termination probability P F (⊤|•) in DB and SubTB losses estimates the modified flow in GFlowNet <ref type="bibr" target="#b26">[27]</ref>, which Var-TB lacks. To enable VLMs to accurately model this for DB and SubTB, we first apply SFT on correctly completed trajecto-ries before fine tuning with GFlowNets. As shown in Tab. 3, SFT initialization significantly boosts SubTB and DB performance on the NumberLine and Blackjack tasks. Without SFT, both losses perform poorly, especially on NL and BJ tasks. With SFT, SubTB and DB improve by 50% and 36% for NumberLine and by 107% and 103% for Blackjack, largely due to better estimation of terminal probability P F (⊤|•).</p><p>Markovian and non-Markovian assumptions GFlowVLM outperforms RL4VLM in Non-Markovian settings, excelling in complex, long-horizon tasks. In ALFWorld (Tab. 4), GFlowVLM achieves higher average performance by 18%, OOD robustness by 100%, and diversity by 27%. It also achieves better success rates in gym tasks (Tab. 3), where history aids decision-making, underscoring GFlowNets' advantage over PPO-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion, Limitation, Future Works</head><p>We introduced a novel framework using GFlowNets to enhance structured reasoning in Vision Language Models (VLMs), to capture relationships among reasoning steps for improved generalization. Unlike traditional methods like Supervised Fine-Tuning and Proximal Policy Optimization, which are limited by certain assumptions, our approach supports complex, long-term reasoning tasks. Experiments in card games and embodied planning showed enhanced training efficiency, diversity, and generalization. We focus on a single-agent task setting, leaving multi-agent task and alternative prompting methods as future directions. Limited computational resources led us to use small sized VLMs, but larger models may further benefit from GFlowNets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Environments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. ALFWorld</head><p>ALFWorld <ref type="bibr" target="#b30">[31]</ref> is an embodied AI environment combining a text-based interactive setup with a vision-language planning dataset. It includes six goal-conditioned tasks: "Pick &amp; Place", "Examine in Light", "Clean &amp; Place", "Heat &amp; Place", "Cool &amp; Place", and "Pick Two &amp; Place". The agent must plan and act based on visual cues and textual instructions (e.g., "go to shelf 1") that specify the task. Unlike gym cards, where all states share the same action space, ALFWorld has a state-dependent action space A t ; actions are context-dependent (e.g., "put some pillows on the armchair", the agent can only place a pillow after picking it up). Our prompt instructs the VLM to choose from the admissible actions A t , and we evaluate Out-Of-Distribution (OOD) performance using a test set of previously unseen scenes (see detailed prompt templates in Tab. 9 and Tab. 10). We use the same non-negative components of the reward function used in <ref type="bibr" target="#b40">[41]</ref>, which includes sub-goal and goal rewards: r(s t , a t , s t+1 |g task ) = 50 * 1{s t+1 = g task } + 1{s t+1 = g task }. We do not include the negative component of the reward function represented as -1{a t / ∈ A t (s t )} in <ref type="bibr" target="#b40">[41]</ref>, since the actions are always selected from the admissible actions provided in the input prompt p t . The rewards are nonnegative, making it suitable for Var-TB and SubTB losses. However, since it lacks dense rewards for every transition, we didn't GFlowVLM with DB loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. NumberLine</head><p>This task involves moving a number along a synthetic number line to reach a target. NumberLine requires identifying two numbers in an image: "Target: x" and "Current: y t ", where x and y t are both integers such that x, y t ∈ [n min , n max ]. The agent's goal is to align y t with x by outputting an action a t from the discrete set {"+", "-", [DONE](if applicable)}. Actions "+" and "-" adjust y t ± 1, while [DONE] signals task completion (see detailed prompt template in Tab. <ref type="figure">7</ref>). An episode ends when the y t = x, or when the maximum step T = 2n max is reached, which is the default setup of the environment. We set n min and n max as 0 and 5, respectively for the in-distribution examples, and set n min and n max as 10 and 50 for generating OOD examples. In the reward function used in <ref type="bibr" target="#b40">[41]</ref>, an agent receives a reward of r(s t , a t ) = 1 when y t+1 = x, a penalty of r(s t , a t ) = -1 upon taking an action that does not move the current number y t to the target x, and a reward of r(s t , a t ) = 0, otherwise. For GFlowVLM, we revise the reward function with non-negative values as follows:</p><formula xml:id="formula_11">R(x, y t ) = c |x -y t | + 1 (<label>9</label></formula><formula xml:id="formula_12">)</formula><p>where c is a scaling constant set to 100. This reward incentivizes the model to bring the current number closer to the target, progressively increasing the reward as the gap decreases. For fair comparison, we run RL4VLM <ref type="bibr" target="#b40">[41]</ref> with revised reward structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Blackjack</head><p>The Blackjack task requires the VLM to reason with visual information and adapt to stochastic outcomes. The observation o t includes two dealer cards (one face-down) and the player's cards. The agent aims to win by selecting an action a t from {"stand", "hit", [DONE](if applicable)} (see detailed prompt template in Tab. 8). In the reward function used in <ref type="bibr" target="#b40">[41]</ref>, an agent receives a reward of r(s t , a t ) = 1, 0, -1 upon win, draw and loss, respectively. We revise the reward function to replace non-negative values as follows: R(s t , a t ) = max(1 × 10 -10 , (r(s t , a t ) + 1) × 10), <ref type="bibr" target="#b9">(10)</ref> where r(s t , a t ) represents the environment's original reward for state s t and action a t . This scales the rewards and ensures they are strictly non-negative. For fair comparison, we run RL4VLM <ref type="bibr" target="#b40">[41]</ref> with revised reward structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training Objectives</head><p>We adopt three different objective functions of GFlowNets, Trajectory-Balance (TB), Subtrajectory-Balance (SubTB), and Detailed-Balance (DB), to fine tune the VLM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Variance Trajectory Balanced (Var-TB) Loss</head><p>The Trajectory-Balanced (TB) objective <ref type="bibr" target="#b22">[23]</ref> ensures that the probability of generating a complete trajectory τ = (s 0 → s 1 → • • • → s n → x) is proportional to the reward R(τ ). Under the Markovian assumption, the forward policy P F (s t |s t-1 ) transitions from state s t-1 to s t , while the backward policy P B (s t-1 |s t ) ensures consistency between forward and backward flows. This objective is given by: <ref type="bibr" target="#b10">(11)</ref> where Z is the partition function that normalizes the distribution.</p><formula xml:id="formula_13">Z n t=1 P F (s t |s t-1 ; θ) = R(x) n t=1 P B (s t-1 |s t ; θ),</formula><p>We now change s to z to match our definition of state in the main paper, where z 0:t consists of a visual observation o t and an input prompt p t containing goal description, history states s 0:t-1 , history actions a 0:t-1 , and admissible actions A t . We use ⊤, which is the [DONE] symbol, to represent the terminal state x of a trajectory. We adopt this notation because, in practice, the VLM predicts the action ⊤ to signify termination. This practical adaptation ensures consistency between the theoretical representation of terminal states and the actual predictions made by the VLM during inference.</p><p>Under the non-Markovian assumption of generating a complete trajectory τ = (z 0 → z 1 → • • • → z n → x), and after adding goal into condition, we have:</p><formula xml:id="formula_14">Z n t=1 P F (z t |z 0:t-1 , g; θ) = R(x) n t=1 P B (z t-1 |z t:n , g; θ),<label>(12)</label></formula><p>From <ref type="bibr" target="#b42">[43]</ref>, an estimation Z for each trajectory τ can be expressed as:</p><formula xml:id="formula_15">ζ(τ ; θ, g) = log n t=1 P F (z t |z 0:t-1 ), g; θ) R(x) n t=1 P B (z t-1 |z t:n , g; θ) = log n t=1 P F (z t |z 0:t-1 , g; θ) R(x)<label>(13)</label></formula><p>where P B = 1 in our case since we formulate the trajectories as a tree structure, where a child state has only one parent state. In the optimal case, ζ(τ ; θ, g) is equal to true logZ. The Variance-Trajectory-Balanced loss function aim to minimize the variance of ζ(τ ; θ, g) across trajectories to make the balance of the trajectories. The final Variance-Trajectory-Balanced loss is then defined as:</p><formula xml:id="formula_16">L VarTB (τ ; θ) = 1 N N i=1 ζ(τ i ; θ, g) -E τ ζ(τ ; θ, g) 2 ,<label>(14)</label></formula><p>where N represents the number of sampled trajectories. This loss ensures that high-reward trajectories are sampled more frequently by the policy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Subtrajectory Balanced (SubTB) Loss</head><p>The Subtrajectory-Balanced (SubTB) loss <ref type="bibr" target="#b21">[22]</ref> operates on subtrajectories of the form τ</p><formula xml:id="formula_17">= (z 0 → z 1 → • • • → z m ).</formula><p>The subtrajectory balance ensures that each segment of the reasoning path or structure remains consistent, where the flows are balanced locally between forward and backward transitions. Under the non-Markovian assumption and after adding goal into conditions, the subtrajectory balance condition is expressed as:</p><formula xml:id="formula_18">F (z 0 ) m t=1 P F (z t |z 0:t-1 ), g; θ) = F (z m ) m t=1 P B (z t-1 |z t:m ), g; θ),<label>(15)</label></formula><p>where F (z 0 ) and F (z m ) represent the flow into the initial (z 0 ) and final state (z m ) of the subtrajectory, respectively. Following <ref type="bibr" target="#b26">[27]</ref>, when all states z t are terminable with ⊤, we have F (z t )P F (⊤|z 0:t ) = R(⊤). Then the SubTB loss can be formulated as:</p><formula xml:id="formula_19">L SubTB (z 0:m , g; θ) = 0≤i&lt;j≤m log R(z 0:i ⊤) j k=i+1 P F (z k |z 0:k-1 , g; θ)P F (⊤|z 0:j , g; θ) R(z 0:i ⊤)P F (⊤|z 0:i , g; θ) 2 (<label>16</label></formula><formula xml:id="formula_20">)</formula><p>where ⊤ is the [DONE] symbol, denoting the terminal state, and process continues until [DONE] symbol ⊤ is generated similar to <ref type="bibr" target="#b7">[8]</ref>. This loss penalizes discrepancies in local transitions and ensures that all subsegments of a trajectory follow the correct balance conditions, reducing variance in smaller parts of the trajectory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.1. Detailed Balanced (DB) Loss</head><p>The Detailed-Balanced (DB) loss <ref type="bibr" target="#b1">[2]</ref> is used to ensure that each transition s t → s t+1 between two states is balanced by matching the forward and backward flows at every step of the trajectory. The detailed balance condition is expressed as:</p><formula xml:id="formula_21">F (s t )P F (s t+1 |s t ) = F (s t+1 )P B (s t |s t+1 ),<label>(17)</label></formula><p>where F (s t ) and F (s t+1 ) represent the flow at states s t and s t+1 , respectively. Under the non-Markovian assumption of generating a complete trajectory τ = (z 0 → z 1 → • • • → z n → ⊤), where ⊤ is the terminal state of the sequence, DB loss is formulated as:</p><formula xml:id="formula_22">L DB (z 0:t → z 0:t+1 , g; θ) = log R(z 0:t ⊤)P F (z t+1 |z 0:t , g; θ)P F (⊤|z 0:t+1 , g; θ) R(z 0:t+1 ⊤)P F (⊤|z 0:t , g; θ) 2 .<label>(18)</label></formula><p>This loss ensures that every state-to-state transition follows the correct flow, preventing inconsistencies in the trajectory construction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Details of Experimental Setup</head><p>In this section, we outline the experimental setup used to evaluate our approach across various tasks. We describe the key components of our implementation, including the data collection, diversity metric, and hyperparameters. By providing these details, we aim to ensure reproducibility and clarify how the proposed method integrates into different experimental frameworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Off-Policy Data Collection</head><p>In this section, we describe our approach to off-policy data collection used in GFlowVLM for two distinct tasks, Numberline and Blackjack, emphasizing the integration of highquality trajectories to enhance model training. These strategies ensure that the model learns from both successful and diverse trajectories, even when its on-policy performance falls short.</p><p>Numberline During training, if the on-policy trajectory generated by the model fails to move the current number correctly towards the target, we augment the dataset by adding an off-policy, ground-truth trajectory to the buffer. These ground-truth trajectories represent successful paths that the model can follow to achieve the goal. By incorporating these accurate trajectories, we provide the model with additional supervision, which helps it learn to generalize better to unseen instances. This ensures the model benefits from examples of correct behavior, even when its predictions deviate from the optimal path. Fig. <ref type="figure" target="#fig_2">5</ref> illustrates the generation of both correct and incorrect trajectories, highlighting how diversity in training trajectories is encouraged to improve robustness.</p><p>Blackjack For the stochastic Blackjack task, deterministic ground-truth trajectories are not directly available due to the probabilistic outcomes of card draws. Instead, we generate high-quality off-policy trajectories using a rulebased heuristic: The agent "stands" when the hand value is 17 or higher and "hits" otherwise. This strategy aligns with fundamental Blackjack principles, balancing the risk of exceeding a hand value of 21 against the potential for improvement by drawing additional cards. By leveraging this rule-based approach, we ensure that the training buffer includes trajectories that reflect a realistic yet principled decision-making process. Figure <ref type="figure" target="#fig_3">6</ref> demonstrates how both correct and incorrect trajectories are generated in a tree structure, promoting diversity in the training data and enabling the model to better handle a range of scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. SFT Dataset Collection</head><p>To create the SFT dataset, we iteratively interact with the environment to generate successful trajectories. For each successful trajectory, we manually append the "[DONE]" token as the final action in the last state, explicitly marking the completion of the task. This approach aims to teach the model to predict the "[DONE]" token as the appropriate action when the goal state is achieved.</p><p>Numberline For the Numberline task, we execute ground-truth actions in the environment until the current state matches the target state. At this point, we append the "[DONE]" token to indicate task completion. This process generated 8,000 data points with "[DONE]" actions and 20,000 additional data points for other actions, using the base SFT dataset in <ref type="bibr" target="#b40">[41]</ref>.</p><p>Blackjack For Blackjack, we adhere to the standard 17point rule to determine actions. When the optimal decision is to take no further action, we append the "[DONE]" token to the trajectory. This yielded 15,000 data points with "[DONE]" actions and 50,000 for other actions, utilizing the SFT dataset from <ref type="bibr" target="#b40">[41]</ref>.</p><p>ALFWorld For ALFWorld, we rely on expert actions derived from a heuristic <ref type="bibr" target="#b30">[31]</ref>. At the end of each successful trajectory, we append the "[DONE]" token to signify task completion. This resulted in 15,000 data points with "[DONE]" actions and 45,000 for other actions using the SFT dataset from <ref type="bibr" target="#b40">[41]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Diversity Metric</head><p>The diversity metric introduced in <ref type="bibr" target="#b39">[40]</ref> calculates the diversity of successful trajectories found by a policy under the same number of samplings at inference time. Specifically, it is defined as follows:</p><formula xml:id="formula_23">Div = n i=1 S i • I(S i ≥ 1) n i=1 I(S i ≥ 1) ≥ 1 (<label>19</label></formula><formula xml:id="formula_24">)</formula><p>where n is the total number of tasks, S i is the number of successful trajectories found for the i-th task, and I(S i ≥ 1) is an indicator function that equals 1 if at least one successful trajectory is found for the i-th task, and 0 otherwise. The denominator represents the number of tasks where the model finds at least one successful trajectory, while the numerator sums the total number of successful trajectories across all tasks. The smallest possible Div is 1, indicating that a method finds at least one successful trajectory on average. For example, a Div = 1.2 suggests that, on average, a method finds 1.2 different successful trajectories. The (Div@N) metric used in the main paper represents the diversity of successful trajectories after sampling N trajectories' samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4. General Setup for Baselines and GFlowVLM</head><p>All experiments are conducted on an H100 DGX machine with 80GB of memory. During VLM training, we directly optimize all trainable components, including the vision encoder, LLM, and MLP projector. For baseline methods, we utilize the open-source implementations provided in the original papers for SFT and RL4VLM <ref type="bibr" target="#b40">[41]</ref>. A CosineAn-nealingLR scheduler is adopted, starting with an initial learning rate of 1 × 10 -5 , decaying to a final learning rate of 1 × 10 -9 , and reaching its maximum learning rate at step 25. For GFlowVLM, a buffer size of 4 is used across all tasks. To ensure a fair comparison, we report the number of environment steps for each method.</p><p>C.5. CoT Weighting Factor λ P F (z t+1 |z 0:t , g; θ) = P Action (a t |z 0:t , c t , g; θ)+ λP CoT (c t |z 0:t , g; θ),</p><p>The CoT weighting factor, λ ∈ [0, 1], controls the influence of CoT reasoning within our framework, as discussed briefly in the main paper (rewritten here in Eq. ( <ref type="formula" target="#formula_25">20</ref>)). To assess the impact of λ, we compute the average performance of our proposed framework, GFlowVLM, using three loss functions, each evaluated with four random seeds. As shown in Figure <ref type="figure" target="#fig_1">4</ref>, a moderate λ (e.g., 0.4) yields the best performance on NumberLine tasks across three different loss functions. When λ is too high (0.8) or too low (0.2), P CoT (c t |z 0:t , g; θ) or P Action (a t |z 0:t , c t , g; θ) overly influences the estimation of P F , respectively, leading to imbalanced learning dynamics. Thus, setting λ = 0.4 effectively balances CoT and action learning, enhancing reasoning performance. We use the same value of λ = 0.4 across all experiments in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Qualitative Results</head><p>We present an example in ALFWorld in Tab. 11, with the goal of "put some keychains on the ottoman" to illustrate key insights into our method.</p><p>Our method encourages exploration by sampling proportional to the reward, allowing it to avoid getting stuck in suboptimal states-a common limitation observed in PPO. This exploration not only prevents suboptimal convergence  but also enables the model to generate more diverse solutions, as demonstrated by the multiple trajectories shown in Tab. 11. Through repeated sampling, our method effectively considers a wider range of potential paths to achieve the goal.</p><p>PPO, in contrast, tends to rely on superficial semantic Table 6. Ablations of GFlowVLM with Markovian assumption for ALFWorld. Since Alfworld does not provide dense rewards, we can not not using DB loss here. Furthermore, while RL4VLM and GFlowVLM with SubTB are trained with SFT initialization, GFVLM with TB-Var is without STF initialization since we do not need to model the flow. M and NM stands for Markovian and Non-Markovian assumption respectively.</p><p>patterns to make decisions. For instance, it may prioritize reaching the "ottoman" directly without first retrieving the keychains, as the term "ottoman" semantically aligns with the goal. This behavior highlights the risk of overfitting to pattern recognition rather than aligning actions with the ultimate reward. { "current number": "x", "target number": "x", "thoughts": "first read out the current and target number, then think carefully about which action to choose", "action": "-" or "+" or "[DONE]" } NumberLine prompt template with history information (Non-Markovian) You are playing a game called number line. You will see a target number and a current number in the image. And your goal is to move the current number closer to the target by choosing either adding or subtracting one to the current number. Below are the history actions and states you've done. { "current number": "x", "target number": "x", "thoughts": "first read out the current and target number, then think carefully about which action to choose", "action": "-" or "+"or "[DONE]" } Table <ref type="table">7</ref>. Prompt Template with Markovian and Non-Markovian assump. for NumberLine. The sentence in brown is only applicable for SubTB and DB losses. Based on the history information, you need to first give an explanation and then you can choose between [''stand", ''hit"]. Use "[DONE]" when you think you have completed the task. Your response should be a valid JSON file in the following format: { "thoughts": "first describe your total points and the dealer's total points then think about which action to choose", "action": "stand" or "hit" or "[DONE]" }</p><p>Table 8. Prompt Templates with Markovian and Non-Markovian assump. for BlackJack. The sentence in brown is only applicable for SubTB and DB losses.</p><p>Goal: put some keychains on ottoman. PPO Ours-Traj. 1 Ours-Traj. 2 Action: go to coffeetable 1 Action: open drawer 6 Action: open drawer 7 Action: go to ottoman 1 Action: close drawer 6 Action: close drawer 7 Action: take pillow 1 from ottoman 1 Action: go to drawer 5 Action: go to drawer 5 Action: inventory Action: open drawer 5 Action: open drawer 5</p><p>Action: go to drawer 7</p><p>Action: take keychain 1 from drawer 5</p><p>Action: take keychain 1 from drawer 5</p><p>Action: look Action: go to ottoman 1</p><p>Action: go to ottoman 1</p><p>Action: go to coffeetable 1</p><p>Action: put keychain 1 in/on ottoman 1</p><p>Action: put keychain 1 in/on ottoman 1</p><p>Table 11. Qualitative results for ALFWorld task. GFlowVLM generates diverse trajectories in contrast to PPO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Ablation Study of Markovian and Non-Markovian</head><p>To evaluate the impact of Markovian and Non-Markovian assumptions on performance, we conduct an ablation study with our method, GFlowVLM with both On-Policy and Off-Policy training, and RL4VLM <ref type="bibr" target="#b40">[41]</ref> across 3 tasks: Number-Line and Blackjack and ALFWorld. The primary difference between these two assumptions lies in the prompt template used during training. Under the Markovian assumption, the model operates with prompts that do not include historical information about prior actions and states, relying solely on the current state. Conversely, the Non-Markovian assumption incorporates the history of actions and states into the prompt, providing richer contextual information (see prompt templates in Tab. 7, Tab. 8, Tab. 10, Tab. 9 for details).</p><p>As shown in Tab. 5, the Non-Markovian assumption leads to consistently better performance across all tasks. In NumberLine and Blackjack, GFlowVLM achieves substantial improvements in both in-distribution and out-ofdistribution scenarios under the Non-Markovian assumption.For instance, in the Numberline task, GFlowVLM with the DB loss demonstrates improved out-of-distribution performance when transitioning from Markovian to Non-Markovian assumptions. Specifically, with on-policy training, the performance increases from 5.3 to 9.1, while with off-policy training, it rises from 16.3 to 18.6. Similarly, in Blackjack, Non-Markovian prompts result in a higher average success rate.</p><p>In ALFWorld tasks, as demonstrated in Tab. 6, the Non-Markovian assumption yields marked gains in both average performance and out-of-distribution generalization. For instance, GFlowVLM with SubTB achieves an average success rate of 26.1 under the Non-Markovian assumption compared to 22.1 under the Markovian setup. These results highlight the importance of historical context in improving task performance, particularly for challenging scenarios requiring long-term dependencies.</p><p>Interestingly, the Non-Markovian assumption also benefits the baselines, including RL4VLM, resulting in a performance increase from 3.1 to 4.4 for Numberline for OOD tasks. This suggests that GFlowVLM is better equipped to leverage the additional context provided by Non-Markovian prompts, enabling it to capture richer dependencies and improve both accuracy and diversity. Overall, the findings confirm that the Non-Markovian assumption provides a more effective framework for reasoning-based tasks, particularly when combined with GFlowVLM's structured learning approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>SFT</head><figDesc>We employ two versions of SFT in our baseline: SFT-w/o-[DONE] and SFT-w/-[DONE]. SFT-w/o-DONE uses the same GPT-4o dataset as in [41]. For SFT-w/-DONE, we include the [DONE] action in the training inputs and add correct examples where outputs explicitly contain [DONE]. We fine-tune the LLaVA-1.6-7B model on this dataset for 1 epoch using the official script. To ensure consistency, downstream GFlowNets training for both SubTB and DB losses starts from the same SFT checkpoint that includes [DONE].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Average success rates (%) of our method under different CoT weighting factor λ on NumberLine across three loss functions.</figDesc><graphic coords="13,82.13,72.00,189.00,170.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. An example of off-policy data collection for Number-Line in a tree structure.</figDesc><graphic coords="13,317.25,72.00,236.25,155.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. An example of off-policy data collection for BlackJack in a tree structure.</figDesc><graphic coords="13,317.25,272.48,236.25,315.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><figDesc>without history information (Markovian)You are playing a game called number line. You will see a target number and a current number in the image. And your goal is to move the current number closer to the target by choosing either adding or subtracting one to the current number. You need to first give the thoughts and then you can choose between ["+", "-"]. Use "[DONE]" when you think you have completed the task. Your response should be a valid JSON file in the following format:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>State 0: 1 Action 1 :</head><label>11</label><figDesc>"+"State 1: 2 Based on the history information, you need to first give the thoughts and then you can choose between ["+", "-"]. Use "[DONE]" when you think you have completed the task.Your response should be a valid JSON file in the following format:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><figDesc>Image input:BlackJack prompt template without history information (Markovian) You are a blackjack player. You are observing the current game state. You need to first give an explanation and then you can choose between ["stand", "hit"]. Use "[DONE]" when you think you have completed the task. Your response should be a valid JSON file in the following format: { "thoughts": "first describe your total points and the dealer's total points then think about which action to choose", "action": "stand" or "hit" or "[DONE]" } BlackJack prompt template with history information (Non-Markovian) You are a blackjack player. You are observing the current game state. Below are the history actions and states. State 0: 14 points Action 1: "hit" State 1: 15 points</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="8,83.25,215.01,445.52,127.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 .</head><label>3</label><figDesc>Performance comparisons across baseline models for NumberLine (NL) and BlackJack (BJ) tasks for in-distribution and out-ofdistributions (OOD) tasks. * We use the same reward function as ours. † We use the same prompt as ours to include history information for Non-Markovian setting. NL-OOD stands for Number line with out-of-distribution tasks. On and Off represent On-Policy and Off-Policy, respectively. M and NM stands for Markovian and Non-Markovian assumption respectively.</figDesc><table><row><cell>Method</cell><cell cols="3">Train Data Assump. SFT Init.</cell><cell>NL</cell><cell>NL-OOD</cell><cell>BJ</cell></row><row><cell>SFT-w/o-[DONE]</cell><cell>Off</cell><cell>-</cell><cell>-</cell><cell>24.8</cell><cell>0.0</cell><cell>23.1</cell></row><row><cell>SFT-w/-[DONE]</cell><cell>Off</cell><cell>-</cell><cell>-</cell><cell>24.0</cell><cell>0.0</cell><cell>20.2</cell></row><row><cell>RL4VLM [41]</cell><cell>On</cell><cell>M</cell><cell>✓</cell><cell>89.4</cell><cell>3.1</cell><cell>40.2</cell></row><row><cell>RL4VLM [41]  †</cell><cell>On</cell><cell>NM</cell><cell>✓</cell><cell>90.3</cell><cell>4.4</cell><cell>41.0</cell></row><row><cell>RL4VLM [41]  *</cell><cell>On</cell><cell>M</cell><cell>✓</cell><cell>34.8</cell><cell>1.9</cell><cell>23.5</cell></row><row><cell>GFlowVLM w/ Var-TB</cell><cell>On</cell><cell>NM</cell><cell>✓</cell><cell>100.0</cell><cell>6.2</cell><cell>41.4</cell></row><row><cell>GFlowVLM w/ SubTB</cell><cell>On</cell><cell>NM</cell><cell>✓</cell><cell>100.0</cell><cell>7.0</cell><cell>41.7</cell></row><row><cell>GFlowVLM w/ DB</cell><cell>On</cell><cell>NM</cell><cell>✓</cell><cell>100.0</cell><cell>9.1</cell><cell>42.2</cell></row><row><cell cols="4">Ablations -w/ Off-Policy Training data</cell><cell></cell><cell></cell><cell></cell></row><row><cell>GFlowVLM w/ Var-TB</cell><cell>Off</cell><cell>NM</cell><cell>✓</cell><cell>100.0</cell><cell>17.3</cell><cell>43.0</cell></row><row><cell>GFlowVLM w/ SubTB</cell><cell>Off</cell><cell>NM</cell><cell>✓</cell><cell>100.0</cell><cell>16.7</cell><cell>42.4</cell></row><row><cell>GFlowVLM w/ DB</cell><cell>Off</cell><cell>NM</cell><cell>✓</cell><cell>100.0</cell><cell>18.6</cell><cell>43.8</cell></row><row><cell></cell><cell cols="3">Ablations -w/o SFT Initialization</cell><cell></cell><cell></cell><cell></cell></row><row><cell>GFlowVLM w/ SubTB</cell><cell>On</cell><cell>NM</cell><cell>×</cell><cell>23.0</cell><cell>0.0</cell><cell>8.4</cell></row><row><cell>GFlowVLM w/ DB</cell><cell>On</cell><cell>NM</cell><cell>×</cell><cell>24.3</cell><cell>0.0</cell><cell>6.8</cell></row><row><cell>GFlowVLM w/ SubTB</cell><cell>Off</cell><cell>NM</cell><cell>×</cell><cell>34.4</cell><cell>0.0</cell><cell>17.4</cell></row><row><cell>GFlowVLM w/ DB</cell><cell>Off</cell><cell>NM</cell><cell>×</cell><cell>33.1</cell><cell>0.0</cell><cell>13.8</cell></row></table><note><p>Blackjack, and ALFWorld. As shown in Tab. 3, it improves success rates on in-distribution examples by 12% over RL4VLM, with an 8% gain in Blackjack due to highquality, off-policy trajectories. For ALFWorld, GFlowVLM achieves a 29% success rate improvement (Tab. 4), highlighting GFlowNets' role in generating accurate trajectories crucial for VLM reasoning.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 .</head><label>5</label><figDesc>Ablations of GFlowVLM with Markovian assumption for NumberLine (NL) and BlackJack (BJ) tasks for in-distribution and out-of-distributions (OOD) tasks.</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="5">Train Data Assump. SFT Init.</cell><cell>NL</cell><cell>NL-OOD</cell><cell>BJ</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">Ablations of RL4VLM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">RL4VLM [41]  *</cell><cell>On</cell><cell></cell><cell>M</cell><cell>✓</cell><cell></cell><cell>34.8</cell><cell>1.9</cell><cell>23.5</cell><cell></cell></row><row><cell>RL4VLM [41]</cell><cell></cell><cell>On</cell><cell></cell><cell>M</cell><cell>✓</cell><cell></cell><cell>89.4</cell><cell>3.1</cell><cell>40.2</cell><cell></cell></row><row><cell>RL4VLM [41]</cell><cell></cell><cell>On</cell><cell></cell><cell>NM</cell><cell>✓</cell><cell></cell><cell>90.3</cell><cell>4.4</cell><cell>41.0</cell><cell></cell></row><row><cell cols="8">Ablations of GFlowVLM w/ Var-TB w/ On and Off-Policy</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">GFlowVLM w/ Var-TB</cell><cell>On</cell><cell></cell><cell>M</cell><cell>✓</cell><cell></cell><cell>93.4</cell><cell>4.7</cell><cell>41.0</cell><cell></cell></row><row><cell cols="2">GFlowVLM w/ Var-TB</cell><cell>On</cell><cell></cell><cell>NM</cell><cell>✓</cell><cell></cell><cell>100.0</cell><cell>6.2</cell><cell>41.4</cell><cell></cell></row><row><cell cols="2">GFlowVLM w/ Var-TB</cell><cell>Off</cell><cell></cell><cell>M</cell><cell>✓</cell><cell></cell><cell>94.5</cell><cell>17.2</cell><cell>42.0</cell><cell></cell></row><row><cell cols="2">GFlowVLM w/ Var-TB</cell><cell>Off</cell><cell></cell><cell>NM</cell><cell>✓</cell><cell></cell><cell>100.0</cell><cell>17.3</cell><cell>43.0</cell><cell></cell></row><row><cell cols="8">Ablations of GFlowVLM w/ SubTB w/ On and Off-Policy</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">GFlowVLM w/ SubTB</cell><cell>On</cell><cell></cell><cell>M</cell><cell>✓</cell><cell></cell><cell>91.7</cell><cell>4.0</cell><cell>40.2</cell><cell></cell></row><row><cell cols="2">GFlowVLM w/ SubTB</cell><cell>On</cell><cell></cell><cell>NM</cell><cell>✓</cell><cell></cell><cell>100.0</cell><cell>7.0</cell><cell>41.7</cell><cell></cell></row><row><cell cols="2">GFlowVLM w/ SubTB</cell><cell>Off</cell><cell></cell><cell>M</cell><cell>✓</cell><cell></cell><cell>94.8</cell><cell>17.3</cell><cell>40.5</cell><cell></cell></row><row><cell cols="2">GFlowVLM w/ SubTB</cell><cell>Off</cell><cell></cell><cell>NM</cell><cell>✓</cell><cell></cell><cell>100.0</cell><cell>16.7</cell><cell>42.4</cell><cell></cell></row><row><cell cols="8">Ablations of GFlowVLM w/ DB w/ On and Off-Policy</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">GFlowVLM w/ DB</cell><cell>On</cell><cell></cell><cell>M</cell><cell>✓</cell><cell></cell><cell>90.1</cell><cell>5.3</cell><cell>40.0</cell><cell></cell></row><row><cell cols="2">GFlowVLM w/ DB</cell><cell>On</cell><cell></cell><cell>NM</cell><cell>✓</cell><cell></cell><cell>100.0</cell><cell>9.1</cell><cell>42.2</cell><cell></cell></row><row><cell cols="2">GFlowVLM w/ DB</cell><cell>Off</cell><cell></cell><cell>M</cell><cell>✓</cell><cell></cell><cell>93.6</cell><cell>16.3</cell><cell>41.5</cell><cell></cell></row><row><cell cols="2">GFlowVLM w/ DB</cell><cell>Off</cell><cell></cell><cell>NM</cell><cell>✓</cell><cell></cell><cell>100.0</cell><cell>18.6</cell><cell>43.8</cell><cell></cell></row><row><cell>Method</cell><cell cols="8">Assump. Pick Look Clean Heat Cool Pick2 Avg.</cell><cell cols="2">OOD Div@16</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Ablations of RL4VLM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RL4VLM [41]</cell><cell>M</cell><cell>47.4</cell><cell>14.7</cell><cell>10.4</cell><cell>14.4</cell><cell>18.8</cell><cell>18.0</cell><cell>21.7</cell><cell>4.8</cell><cell>1.12</cell></row><row><cell>RL4VLM [41]</cell><cell>NM</cell><cell>49.1</cell><cell>13.5</cell><cell>9.8</cell><cell>15.2</cell><cell>20.1</cell><cell>20.6</cell><cell>22.1</cell><cell>6.1</cell><cell>1.11</cell></row><row><cell></cell><cell></cell><cell cols="5">Ablations of GFlowVLM w/ SubTB</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GFlowVLM w/ SubTB</cell><cell>M</cell><cell>46.0</cell><cell>10.1</cell><cell>9.7</cell><cell>14.7</cell><cell>24.6</cell><cell>23.7</cell><cell>22.1</cell><cell>8.0</cell><cell>1.34</cell></row><row><cell>GFlowVLM w/ SubTB</cell><cell>NM</cell><cell>50.0</cell><cell>23.1</cell><cell>10.0</cell><cell>18.7</cell><cell>24.3</cell><cell>23.7</cell><cell>26.1</cell><cell>12.3</cell><cell>1.40</cell></row><row><cell></cell><cell></cell><cell cols="5">Ablations of GFlowVLM w/ Var-TB</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GFlowVLM w/ Var-TB</cell><cell>M</cell><cell>45.1</cell><cell>12.2</cell><cell>11.3</cell><cell>15.7</cell><cell>20.6</cell><cell>24.7</cell><cell>22.9</cell><cell>7.6</cell><cell>1.37</cell></row><row><cell>GFlowVLM w/ Var-TB</cell><cell>NM</cell><cell>50.0</cell><cell>22.2</cell><cell>10.2</cell><cell>16.1</cell><cell>22.7</cell><cell>21.9</cell><cell>25.7</cell><cell>10.9</cell><cell>1.41</cell></row></table><note><p>* We use the same reward function as ours. NL-OOD stands for Number line with out-of-distribution tasks. On and Off represent On-Policy and Off-Policy, respectively. M and NM stands for Markovian and Non-Markovian assumption respectively.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>* Only the current input image ot is used, as including every intermediate image would be computationally costly, and current VLMs do not perform optimally with multiple images as input.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image input:</head><p>ALFWorld prompt template without history information (Markovian)</p><p>You are an ALFWorld Embodied Environment expert. Your goal is to select the best next action from the Admissible Next Actions based on the current state and image to complete the task. Use "[DONE]" when you think you have completed the task.</p><p>Task: Your task is to put a cool mug in cabinet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Current State:</head><p>"['You arrive at loc 1. The cabinet 1 is open. On the cabinet 1, you see a pan 1, a kettle 1, a winebottle 1, a apple 1, a stoveknob 1, a stoveknob 2, a stoveknob 3, a stoveknob 4, a knife 1, a saltshaker 1, and a bread 1.']." Admissible Next Actions: ['go to countertop 1', 'go to cabinet 2', 'go to countertop 2', 'go to stoveburner 1', 'go to drawer 1', 'go to drawer 2', 'go to drawer 3', 'go to stoveburner 2', 'go to stoveburner 3', 'go to stoveburner 4', 'go to drawer 4', 'go to cabinet 3', 'go to cabinet 4', 'go to microwave 1', 'go to cabinet 5', 'go to cabinet 6', 'go to cabinet 7', 'go to sink 1', 'go to sinkbasin 1', 'go to fridge 1', 'go to toaster 1', 'go to coffeemachine 1', 'go to cabinet 8', 'go to drawer 5', 'go to drawer 6', 'go to drawer 7', 'go to drawer 8', 'go to shelf 1', 'go to shelf 2', 'go to countertop 3', 'go to shelf 3', 'go to drawer 9', 'go to garbagecan 1', 'open cabinet 1', 'close cabinet 1', 'take pan 1 from cabinet 1', 'take kettle 1 from cabinet 1', 'take winebottle 1 from cabinet 1', 'take apple 1 from cabinet 1', 'take stoveknob 1 from cabinet 1', 'take stoveknob 2 from cabinet 1', 'take stoveknob 3 from cabinet 1', 'take stoveknob 4 from cabinet 1', 'take knife 1 from cabinet 1', 'take saltshaker 1 from cabinet 1', 'take bread 1 from cabinet 1', 'inventory', 'look', 'examine cabinet 1'].</p><p>Your response should be a valid JSON file in the following format: { "thoughts": "first describe what do you see in the image using the text description, then carefully think about which action to complete the task.", "action": "an admissible action" or "[DONE]" }  ['go to countertop 1', 'go to cabinet 2', 'go to countertop 2', 'go to stoveburner 1', 'go to drawer 1', 'go to drawer 2', 'go to drawer 3', 'go to stoveburner 2', 'go to stoveburner 3', 'go to stoveburner 4', 'go to drawer 4', 'go to cabinet 3', 'go to cabinet 4', 'go to microwave 1', 'go to cabinet 5', 'go to cabinet 6', 'go to cabinet 7', 'go to sink 1', 'go to sinkbasin 1', 'go to fridge 1', 'go to toaster 1', 'go to coffeemachine 1', 'go to cabinet 8', 'go to drawer 5', 'go to drawer 6', 'go to drawer 7', 'go to drawer 8', 'go to shelf 1', 'go to shelf 2', 'go to countertop 3', 'go to shelf 3', 'go to drawer 9', 'go to garbagecan 1', 'open cabinet 1', 'close cabinet 1', 'take pan 1 from cabinet 1', 'take kettle 1 from cabinet 1', 'take winebottle 1 from cabinet 1', 'take apple 1 from cabinet 1', 'take stoveknob 1 from cabinet 1', 'take stoveknob 2 from cabinet 1', 'take stoveknob 3 from cabinet 1', 'take stoveknob 4 from cabinet 1', 'take knife 1 from cabinet 1', 'take saltshaker 1 from cabinet 1', 'take bread 1 from cabinet 1', 'inventory', 'look', 'examine cabinet 1'].</p><p>Your response should be a valid JSON file in the following format: { "thoughts": "first describe what do you see in the image using the text description, then carefully think about which action to complete the task.", "action": "an admissible action" or "[DONE]" } Table <ref type="table">10</ref>. Prompt template with Non-Markovian assump. for ALFWorld. The sentence in brown is only applicable for SubTB and DB losses.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Flow network based generative models for non-iterative diverse candidate generation</title>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moksh</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maksym</forename><surname>Korablyov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Gflownet foundations</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salem</forename><surname>Lahlou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mo</forename><surname>Tiwari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Vision-language models provide promptable representations for reinforcement learning</title>
		<author>
			<persName><forename type="first">William</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oier</forename><surname>Mees</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aviral</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.02651</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A survey on in-context learning</title>
		<author>
			<persName><forename type="first">Qingxiu</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damai</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingyuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heming</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.00234</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deterministic sequencing of exploration and exploitation for reinforcement learning</title>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaibhav</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE 61st Conference on Decision and Control (CDC)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2313" to="2318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Towards physically adversarial intelligent networks (pains) for safer self-driving</title>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demetris</forename><surname>Coleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">E</forename><surname>Siegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Control Systems Letters</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1063" to="1068" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Generalized mission planning for heterogeneous multi-robot teams via llm-constructed hierarchical trees</title>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Isele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enna</forename><surname>Sachdeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pin-Hao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Behzad</forename><surname>Dariush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwonjoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sangjae</forename><surname>Bae</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2501.16539</idno>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moksh</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Younesse</forename><surname>Elmoznino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Kaddar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Malkin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.04363</idno>
		<title level="m">Amortizing intractable inference in large language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Gflownet-em for learning compositional latent variable models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moksh</forename><surname>Malkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><forename type="middle">E</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Everett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Graikos</surname></persName>
		</author>
		<author>
			<persName><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="13528" to="13549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Biological sequence design with gflownets</title>
		<author>
			<persName><forename type="first">Moksh</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Hernandez-Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jarrid</forename><surname>Rector-Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">P</forename><surname>Bonaventure</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chanakya</forename><surname>Dossou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Ajit Ekbote</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinghuai</forename><surname>Kilgour</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<idno>PMLR, 2022. 2</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="9786" to="9801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Gflownets for ai-driven scientific discovery</title>
		<author>
			<persName><forename type="first">Moksh</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Hartford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng-Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Hernandez-Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Discovery</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="557" to="577" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-objective gflownets</title>
		<author>
			<persName><forename type="first">Moksh</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharath</forename><surname>Chandra Raparthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Hernández-Garcıa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jarrid</forename><surname>Rector-Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Miret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="14631" to="14653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Improved off-policy reinforcement learning in biological sequence design</title>
		<author>
			<persName><forename type="first">Hyeonah</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minsu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taeyoung</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanghyeok</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Hernández-García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinkyoo</forename><surname>Park</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.04461</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Ant colony sampling with gflownets for combinatorial optimization</title>
		<author>
			<persName><forename type="first">Minsu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanghyeok</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwoo</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyeonah</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinkyoo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.07041</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Blip: Bootstrapping language-image pretraining for unified vision-language understanding and generation</title>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Dag matters! gflownets enhanced explainer for graph neural networks</title>
		<author>
			<persName><forename type="first">Wenqian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinchuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhigang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianye</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Pang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.02448</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distilling large visionlanguage model with out-of-distribution generalizability</title>
		<author>
			<persName><forename type="first">Xuanlin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minghua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhan</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2492" to="2503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improved baselines with visual instruction tuning</title>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="26296" to="26306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong Jae</forename><surname>Lee</surname></persName>
		</author>
		<title level="m">Llavanext: Improved reasoning, ocr, and world knowledge</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Visual instruction tuning</title>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generative flow network for listwise recommendation</title>
		<author>
			<persName><forename type="first">Shuchang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingpeng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhankui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Gai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1524" to="1534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning gflownets from partial episodes for improved convergence and stability</title>
		<author>
			<persName><forename type="first">Kanika</forename><surname>Madan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jarrid</forename><surname>Rector-Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maksym</forename><surname>Korablyov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moksh</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><forename type="middle">Cristian</forename><surname>Nica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Bosc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Malkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Trajectory balance: Improved credit assignment in gflownets</title>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Malkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moksh</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Malkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salem</forename><surname>Lahlou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Everett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinghuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.00580</idno>
		<title level="m">Gflownets and variational inference</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Embodiedgpt: Visionlanguage pre-training via embodied chain of thought</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinglong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengkang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Training language models to follow instructions with human feedback. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carroll</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarina</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="27730" to="27744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Better training of gflownets with local credit and incomplete trajectories</title>
		<author>
			<persName><forename type="first">Ling</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Malkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinghuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Generative agents: Interactive simulacra of human behavior</title>
		<author>
			<persName><forename type="first">Sung</forename><surname>Joon</surname></persName>
		</author>
		<author>
			<persName><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carrie</forename><forename type="middle">Jun</forename><surname>Brien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meredith</forename><forename type="middle">Ringel</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th annual acm symposium on user interface software and technology</title>
		<meeting>the 36th annual acm symposium on user interface software and technology</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleg</forename><surname>Klimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347</idno>
		<title level="m">Proximal policy optimization algorithms</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Tony</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Ester</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.03223</idno>
		<title level="m">Tacogfn: Target conditioned gflownet for structure-based drug design</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Alfworld: Aligning text and embodied environments for interactive learning</title>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Shridhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc-Alexandre</forename><surname>Côté</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Hausknecht</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.03768</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Large language models as generalizable policies for embodied tasks</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Szot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Schwarzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harsh</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bogdan</forename><surname>Mazoure</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rin</forename><surname>Metcalf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walter</forename><surname>Talbott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalie</forename><surname>Mackraz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">T</forename><surname>Toshev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Gflownet fine-tuning for diverse correct solutions in mathematical reasoning tasks</title>
		<author>
			<persName><forename type="first">Ryoichi</forename><surname>Takase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masaya</forename><surname>Tsunokake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuta</forename><surname>Tsuchiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shota</forename><surname>Inuzuka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.20147</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents</title>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaofei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanzhou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anji</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yitao</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.01560</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenxiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiwen</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boyang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Senjie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enyu</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.07864</idno>
		<title level="m">The rise and potential of large language model based agents: A survey</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Shunyu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Izhak</forename><surname>Shafran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.03629</idno>
		<title level="m">React: Synergizing reasoning and acting in language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">React: synergizing reasoning and acting in language models</title>
		<author>
			<persName><forename type="first">Shunyu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Izhak</forename><surname>Shafran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.03629</idno>
		<imprint>
			<date type="published" when="2022">2022. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Tree of thoughts: Deliberate problem solving with large language models</title>
		<author>
			<persName><forename type="first">Shunyu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Izhak</forename><surname>Shafran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Fangxu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lai</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqiang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shibo</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianhui</forename><surname>Qin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.05673</idno>
		<title level="m">Flow of reasoning: Efficient training of llm policy with divergent thinking</title>
		<imprint>
			<date type="published" when="2024">2024. 1, 2, 6, 7, 12</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Fine-tuning large vision-language models as decision-making agents via reinforcement learning</title>
		<author>
			<persName><forename type="first">Yuexiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zipeng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayi</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengbang</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alane</forename><surname>Suhr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.10292</idno>
		<imprint>
			<date type="published" when="1920">2024. 1, 2, 3, 6, 7, 8, 10, 12, 14, 20</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Let the flows tell: Solving graph combinatorial optimization problems with gflownets</title>
		<author>
			<persName><forename type="first">Dinghuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Malkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Pan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.17010</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Corrado</forename><surname>David W Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Rainone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Peschl</surname></persName>
		</author>
		<author>
			<persName><surname>Bondesan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.05446</idno>
		<title level="m">Robust scheduling with gflownets</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Proximal policy optimization via enhanced exploration efficiency</title>
		<author>
			<persName><forename type="first">Junwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenghao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Lü</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">609</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="750" to="765" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Minigpt-4: Enhancing visionlanguage understanding with advanced large language models</title>
		<author>
			<persName><forename type="first">Deyao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqian</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<idno>ArXiv, abs/2304.10592</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Generalized universal domain adaptation with generative flow networks</title>
		<author>
			<persName><forename type="first">Didi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinchuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfeng</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianye</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st ACM International Conference on Multimedia</title>
		<meeting>the 31st ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="8304" to="8315" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
