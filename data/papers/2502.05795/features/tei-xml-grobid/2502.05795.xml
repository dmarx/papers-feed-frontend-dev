<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Curse of Depth in Large Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-02-09">9 Feb 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wenfang</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Medical Artificial Intelligence Labora- tory</orgName>
								<orgName type="institution" key="instit2">Westlake University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xinyuan</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Emory University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pengxiang</forename><surname>Li</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lu</forename><surname>Yin</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">University of Surrey</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yefeng</forename><surname>Zheng</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Medical Artificial Intelligence Labora- tory</orgName>
								<orgName type="institution" key="instit2">Westlake University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shiwei</forename><surname>Liu</surname></persName>
						</author>
						<title level="a" type="main">The Curse of Depth in Large Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-02-09">9 Feb 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">AD05EE2CE8DF8C97CD5AC3B241545617</idno>
					<idno type="arXiv">arXiv:2502.05795v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we introduce the Curse of Depth, a concept that highlights, explains, and addresses the recent observation in modern Large Language Models (LLMs) where nearly half of the layers are less effective than expected. We first confirm the wide existence of this phenomenon across the most popular families of LLMs such as Llama, Mistral, DeepSeek, and Qwen. Our analysis, theoretically and empirically, identifies that the underlying reason for the ineffectiveness of deep layers in LLMs is the widespread usage of Pre-Layer Normalization (Pre-LN). While Pre-LN stabilizes the training of Transformer LLMs, its output variance exponentially grows with the model depth, which undesirably causes the derivative of the deep Transformer blocks to be an identity matrix, and therefore barely contributes to the training. To resolve this training pitfall, we propose Layer-Norm Scaling, which scales the variance of output of the layer normalization inversely by the square root of its depth. This simple modification mitigates the output variance explosion of deeper Transformer layers, improving their contribution. Our experimental results, spanning model sizes from 130M to 1B, demonstrate that LayerNorm Scaling significantly enhances LLM pre-training performance compared to Pre-LN. Moreover, this improvement seamlessly carries over to supervised fine-tuning. All these gains can be attributed to the fact that LayerNorm Scaling enables deeper layers to contribute more effectively during training. Our code is available at LayerNorm-Scaling.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent studies reveal that the deeper layers (Transformer blocks) in modern LLMs tend to be less effective than the On the other hand, having many layers ineffective is undesirable as modern LLMs are extremely resource-intensive to train, often requiring thousands of GPUs trained for multiple months, let alone the labor used for data curation and administration <ref type="bibr" target="#b0">(Achiam et al., 2023;</ref><ref type="bibr" target="#b35">Touvron et al., 2023)</ref>. Ideally, we want all layers in a model to be well-trained, with sufficient diversity in features from layer to layer, to maximize the utility of resources <ref type="bibr">(Li et al., 2024b)</ref>. The existence of ill-trained layers suggests that there must be something off with current LLM paradigms. Addressing such limitations is a pressing need for the community to avoid the waste of valuable resources, as new versions of LLMs are usually trained with their previous computing paradigm which results in ineffective layers.</p><p>To seek the immediate attention of the community, we introduce the concept of the Curse of Depth (CoD) to systematically present the phenomenon of ineffective deep layers in various LLM families, to identify the underlying reason behind it, and to rectify it by proposing LayerNorm Scaling. We first state the Curse of Depth below.</p><p>The Curse of Depth. The Curse of Depth refers to the observed phenomenon where deeper layers in modern large language models (LLMs) contribute significantly less to learning and representation compared to earlier layers. These deeper layers often exhibit remarkable robustness to pruning and perturbations, implying they fail to perform meaningful transformations. This behavior prevents these layers from effectively contributing to training and representation learning, resulting in resource inefficiency.</p><p>Empirical Evidence of CoD. The ineffectiveness of deep layers in LLMs has been previously reported. <ref type="bibr" target="#b43">Yin et al. (2024)</ref> found that deeper layers of LLMs can tolerate significantly higher levels of pruning compared to shallower layers, achieving high sparsity. Similarly, Gromov et al. (2024) and <ref type="bibr" target="#b25">Men et al. (2024)</ref> demonstrated that removing early layers causes a dramatic decline in model performance, whereas removing deep layers does not. <ref type="bibr" target="#b18">Lad et al. (2024)</ref> showed that the middle and deep layers of GPT-2 and Pythia exhibit remarkable robustness to perturbations such as layer swapping and layer dropping. Recently, <ref type="bibr">Li et al. (2024a)</ref> highlighted that early layers contain more outliers and are therefore more critical for fine-tuning. While these studies effectively highlight the limitations of deep layers in LLMs, they stop short of identifying the root cause of this issue or proposing viable solutions to address it.</p><p>To demonstrate that the Curse of Depths is prevalent across popular families of LLMs, we conduct layer pruning experiments on various models, including LLaMA2-7/13B, Mistral-7B, DeepSeek-7B, and Qwen-7B. We measure performance degradation on the Massive Multitask Language Understanding (MMLU) benchmark <ref type="bibr" target="#b14">(Hendrycks et al., 2021)</ref> by pruning entire layers of each model, one at a time, and directly evaluating the resulting pruned models on MMLU without any fine-tuning in Figure <ref type="figure" target="#fig_1">2</ref>.</p><p>Results: 1). Most LLMs utilizing Pre-LN exhibit remarkable robustness to the removal of deeper layers, whereas BERT with Post-LN shows the opposite trend. 2). The number of layers that can be pruned without significant performance degradation increases with model size.</p><p>Identifying the Root Cause of CoD. We theoretically and empirically identify the root cause of CoD as the use of Pre-Layer Normalization (Pre-LN) <ref type="bibr" target="#b2">(Baevski and Auli, 2019;</ref><ref type="bibr" target="#b8">Dai et al., 2019)</ref>, which normalizes layer inputs before applying the main computations, such as attention or feedforward operations, rather than after. Specifically, while stabilizing training, we observe that the output variance of Pre-LN accumulates significantly with layer depth (see Appendix C), causing the derivatives of deep Pre-LN layers to approach an identity matrix. This behavior prevents these layers from introducing meaningful transformations, leading to diminished representation learning.</p><p>Mitigating CoD through LayerNorm Scaling. We pro-pose LayerNorm Scaling, which scales the output of Layer Normalization by the square root of the depth 1 √ l . Layer-Norm Scaling effectively scales down the output variance across layers of Pre-LN, leading to considerably lower training loss and achieving the same loss as Pre-LN using only half tokens. Figure <ref type="figure" target="#fig_0">1</ref> compares the layerwise output variance across different setups: (1) Pre-LN, (2) Pre-LN with Scaled Initialization <ref type="bibr">(Takase et al., 2023b)</ref>, and (3) Lay-erNorm Scaling. As shown, Pre-LN exhibits significant variance explosion in deeper layers. In contrast, LayerNorm Scaling effectively reduces output variance across layers, enhancing the contribution of deeper layers during training. This adjustment leads to significantly lower training loss compared to Pre-LN. Unlike previous LayerNorm variants <ref type="bibr">(Li et al., 2024b;</ref><ref type="bibr" target="#b22">Liu et al., 2020)</ref>, LayerNorm Scaling is simple to implement, requires no hyperparameter tuning, and introduces no additional parameters during training. Furthermore, we further show that the model pre-trained with LayerNorm Scaling achieves better performance on downstream tasks in self-supervised fine-tuning, all thanks to the more effective deep layers learned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions.</head><p>• We introduce the Curse of Depth to highlight, understand, and rectify the phenomenon in LLMs that is commonly overlooked-deep layers fail to contribute as effectively as they should.</p><p>• We identify the root cause as Pre-LN, which causes output variance to grow exponentially with model depth. This leads to deep Transformer blocks having derivatives close to the identity matrix, rendering them ineffective during training. While scaled initialization <ref type="bibr" target="#b30">(Shoeybi et al., 2020)</ref> helps mitigate variance at initialization, it does not prevent explosion during training.</p><p>• To mitigate this issue, we propose LayerNorm Scaling, which inversely scales the output of Pre-LN by the square root of the depth. This adjustment ensures that all layers contribute effectively to learning, thereby improving LLM performance. • We hope this work brings greater attention to this issue, contributes to the improvement of LLMs, and maximizes the utilization of computational resources dedicated to training large models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Empirical Evidence of the Curse of Depth</head><p>To empirically analyze the impact of layer normalization on the Curse of Depth in LLMs, we conduct a series of evaluations inspired by <ref type="bibr">Li et al. (2024b)</ref>, extending their methodology to compare Pre-LN and Post-LN models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Experimental Setup</head><p>Methods: We evaluate Pre-LN and Post-LN models by assessing the impact of layer pruning at different depths.</p><p>Our hypothesis is that Pre-LN models exhibit diminishing effectiveness in deeper layers, whereas Post-LN has less effective early layers. To verify this, we empirically quantify the contribution of individual layers to overall model performance across a diverse set of LLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LLMs:</head><p>We conduct experiments on multiple widely adopted LLMs: BERT-Large <ref type="bibr" target="#b9">(Devlin, 2019)</ref>, <ref type="bibr">Mistral-7B (Jiang et al., 2023)</ref>, LLaMA2-7B/13B <ref type="bibr" target="#b35">(Touvron et al., 2023)</ref>, DeepSeek-7B <ref type="bibr" target="#b5">(Bi et al., 2024)</ref>, and Qwen-7B <ref type="bibr" target="#b3">(Bai et al., 2023)</ref>. These models were chosen to ensure architectural and application diversity. BERT-Large represents a Post-LN model, whereas the rest are Pre-LN-based. This selection enables a comprehensive evaluation of the effects of layer normalization across varying architectures and model scales.</p><p>Evaluation Metric: To empirically assess the impact of deeper layers in LLMs, we adopt the Performance Drop metric ∆P (ℓ) , inspired by <ref type="bibr">Li et al. (2024b)</ref>. This metric quantifies the contribution of each layer by measuring the degradation in model performance following its removal. Specifically, it is defined as:</p><formula xml:id="formula_0">∆P (ℓ) = P (ℓ) pruned -P original ,<label>(1)</label></formula><p>where P original represents the performance of the unpruned model, and</p><formula xml:id="formula_1">P (ℓ)</formula><p>pruned denotes the performance after removing layer ℓ. A lower ∆P (ℓ) suggests that the pruned layer plays a minor role in the model's overall effectiveness. For BERT-Large, we evaluate performance on the SQuAD v1.1 dataset <ref type="bibr" target="#b28">(Rajpurkar, 2016)</ref>, which measures reading comprehension. For Mistral-7B, LLaMA2-13B, and Qwen-7B, we assess model performance on the MMLU benchmark <ref type="bibr" target="#b14">(Hendrycks et al., 2021)</ref>, a widely-used dataset for multi-task language understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Layer Pruning Analysis</head><p>Figure <ref type="figure" target="#fig_1">2</ref> presents the performance drop (∆P (ℓ) ) across different layers for six LLMs, including one Post-LN model (BERT-Large) and five Pre-LN models (Mistral-7B, LLaMA2-13B, Qwen-7B, DeepSeek-7B and LLaMA2-7B).</p><p>As shown in Figure <ref type="figure" target="#fig_1">2</ref> (a), pruning deeper layers in BERT-Large leads to a significant decline in accuracy on SQuAD v1.1, while pruning earlier layers has minimal impact. The performance drop ∆P (ℓ) becomes particularly severe beyond the 10th layer, highlighting the crucial role of deeper layers in maintaining overall performance in Post-LN models. In contrast, removing layers in the first half of the network results in negligible changes, indicating their limited contribution to the final output.</p><p>However, as shown in Figure <ref type="figure" target="#fig_1">2</ref> (b)-(f), Pre-LN models exhibit a contrast pattern, where deeper layers contribute significantly less to the overall model performance. For instance, as shown in Figure <ref type="figure" target="#fig_1">2</ref> (b) and (c), pruning layers in the last third of Mistral-7B and Qwen-7B results in a minimal performance drop on MMLU, indicating their limited contribution to overall accuracy. In contrast, pruning the first few layers leads to a substantial accuracy degradation, highlighting their crucial role in feature extraction. Similarly, Figure <ref type="figure" target="#fig_1">2 (d</ref>) and (e) show that DeepSeek-7B and LLaMA2-7B follow a similar pattern, where deeper layers have little impact on performance, while earlier layers play a more significant role. Finally, as shown in Figure <ref type="figure" target="#fig_1">2</ref> (f), more than half of the layers in LLaMA2-13B can be safely removed. This suggests that as model size increases, the contrast between shallow and deep layers becomes more pronounced, with earlier layers playing a dominant role in representation learning. This observation underscores the need for the community to address the Curse of Depth to prevent resource waste.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Analysis of the Curse of Depth</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminaries</head><p>This paper primarily focuses on Pre-LN Transformer <ref type="bibr" target="#b2">(Baevski and Auli, 2019;</ref><ref type="bibr" target="#b8">Dai et al., 2019)</ref>. Let x ℓ ∈ R d be the input vector at the ℓ-th layer of Transformer, where d denotes the feature dimension of each layer. For simplicity, we assume all layers to have the same dimension d. The layer output y is calculated as follows:</p><formula xml:id="formula_2">y = x ℓ+1 = x ′ ℓ + FFN(LN(x ′ ℓ )),<label>(2)</label></formula><formula xml:id="formula_3">x ′ ℓ = x ℓ + Attn(LN(x ℓ )),<label>(3)</label></formula><p>where LN denotes the layer normalization function. In addition, the feed-forward network (FFN) and the multihead self-attention (Attn) sub-layers are defined as follows:</p><formula xml:id="formula_4">FFN(x) = W 2 F(W 1 x), Attn(x) = W O (concat(head 1 (x), . . . , head h (x))), head i (x) = softmax (W Qi x) ⊤ (W Ki X) √ d head (W V i X) ⊤ ,<label>(4</label></formula><p>) where F is an activation function, concat concatenates input vectors, softmax applies the softmax function, and</p><formula xml:id="formula_5">W 1 ∈ R d ffn ×d , W 2 ∈ R d×d ffn , W Qi ∈ R d head ×d , W Ki ∈ R d head ×d , W V i ∈ R d head ×d</formula><p>, and W O ∈ R d×d are parameter matrices, and d FFN and d head are the internal dimensions of FFN and multi-head self-attention sub-layers, respectively. X ∈ R d×s , where s is the input sequence length.</p><p>The derivatives of Pre-Ln Transformers are:</p><formula xml:id="formula_6">∂Pre-LN(x) ∂x = I + ∂f (LN(x)) ∂LN(x) ∂LN(x) ∂x ,<label>(5)</label></formula><p>where f here represents either the multi-head attention function or the FFN function. If the term ∂f (LN(x))</p><p>∂LN(x) ∂LN(x) ∂x becomes too small, the Pre-LN layer ∂Pre-LN(x) ∂x behaves like an identity map. Our main objective is to prevent identity map behavior for very deep Transformer networks. The first step in this process is to compute the variance σ 2</p><p>x ℓ of vector x ℓ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Pre-LN Transformers</head><p>Assumption 1. Let x ℓ and x ′ ℓ denote the input and intermediate vectors of the ℓ-th layer. Moreover, let W ℓ denote the model parameter matrix at the ℓ-th layer. We assume that, for all layers, x ℓ , x ′ ℓ , and W ℓ follow normal and independent distributions with mean µ = 0. Lemma 1. Let σ 2</p><p>x ′ ℓ and σ 2</p><p>x ℓ denote the variances of x ′ ℓ and x ℓ , respectively. These two variances exhibit the same overall growth trend, which is:</p><formula xml:id="formula_7">σ 2 x ℓ = σ 2 x1 Θ ℓ-1 k=1 1 + 1 σ x k ,<label>(6)</label></formula><p>where the growth of σ 2 x l is sub-exponential, as shown by the following bounds:</p><formula xml:id="formula_8">Θ(L) ≤ σ 2 x l ≤ Θ(exp(L)).<label>(7)</label></formula><p>Here, the notation</p><formula xml:id="formula_9">Θ means: if f (x) ∈ Θ g(x) , then there exist constants C 1 , C 2 such that C 1 |g(x)| ≤ |f (x)| ≤ C 2 |g(x)| as x → ∞. The lower bound Θ(L) ≤ σ 2 x ℓ indi- cates that σ 2</formula><p>x ℓ grows at least linearly, while the upper bound σ 2</p><p>x ℓ ≤ Θ(exp(L)) implies that its growth does not exceed an exponential function of L.</p><p>Based on Assumption 1 and the work of <ref type="bibr">(Takase et al., 2023b)</ref>, we obtain the following: Theorem 1. For a Pre-LN Transformer with L layers, using Equations (2) and (3), the partial derivative ∂y L ∂x1 can be written as:</p><formula xml:id="formula_10">∂y L ∂x 1 = L-1 ℓ=1 ∂y ℓ ∂x ′ ℓ • ∂x ′ ℓ ∂x ℓ . (<label>8</label></formula><formula xml:id="formula_11">)</formula><p>The Euclidean norm of ∂y L ∂x1 is given by:</p><formula xml:id="formula_12">∂y L ∂x 1 2 ≤ L-1 l=1 1 + 1 σ x ℓ A + 1 σ 2 x ℓ B ,<label>(9)</label></formula><p>where A and B are constants for the Transformer network.</p><p>Then the upper bound for this norm is given as follows: when σ 2 x ℓ grows exponentially, (i.e., at its upper bound), we have:</p><formula xml:id="formula_13">σ 2 x ℓ ∼ exp(ℓ), ∂y L ∂x 1 2 ≤ M,<label>(10)</label></formula><p>where the gradient norm converges to a constant M . Conversely, when σ 2 x ℓ grows linearly (i.e., at its lower bound), we have</p><formula xml:id="formula_14">σ 2 x ℓ ∼ ℓ, ∂y L ∂x 1 2 ≤ Θ(L),<label>(11)</label></formula><p>which means that the gradient norm grows linearly in L.</p><p>The detailed description of A and B, as well as the complete proof, are provided in Appendix A.2.</p><p>From Theorem 1, we observe that when the variance grows exponentially, as the number of layers L → ∞, the norm</p><formula xml:id="formula_15">∂y L ∂x1 2</formula><p>is bounded above by a fixed constant M . This result implies that even an infinitely deep Transformer remains stable, and by the Weierstrass Theorem, the network is guaranteed to converge. Consequently, this implies that for very large L, deeper layers behave nearly as an identity map from x ℓ to y ℓ , thereby limiting the model's expressivity and hindering its ability to learn meaningful transformations. This outcome is undesirable, therefore, we would instead prefer the variance to increase more gradually-e.g., linearly-so that ∂y L ∂x1 2 exhibits linear growth. This observation highlights the necessity of appropriate variance control mechanisms, such as scaling strategies, to prevent excessive identity mappings and enhance network depth utilization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Post-LN Transformers</head><p>For Post-LN Transformers, we continue to adopt Assumption 1. In this setting, each layer is followed by a layer normalization (LN) step, ensuring that the variances σ 2 x ℓ and σ 2</p><p>x ′ ℓ remain fixed at 1 across all layers. Consequently, the norm ∂y ℓ ∂x ℓ 2 exhibits minimal variation from one layer to the next, indicating stable gradient propagation.</p><p>Since the variance is effectively controlled by LN in Post-LN Transformers, an explicit variance-based analysis becomes less critical. Nonetheless, there remain other important aspects to investigate in deeper Post-LN architectures, such as the evolution of feature mappings and the behavior of covariance kernels over deep layers. These directions will be pursued in future work. reducing the effectiveness of deeper layers. To mitigate this issue, we propose LayerNorm Scaling, a simple yet effective normalization strategy. The core idea of LayerNorm Scaling is to control the exponential growth of output variance in Pre-LN by scaling the normalized outputs according to layer depth. Specifically, we apply a scaling factor inversely proportional to the square root of the layer index to scale down the output of LN layers, stabilizing gradient flow and enhancing the contribution of deeper Transformer layers during training. LayerNorm Scaling is illustrated in Figure <ref type="figure" target="#fig_2">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">LayerNorm Scaling</head><p>Formally, for a Transformer model with L layers, the output of Layer Normalization in each layer ℓ is scaled by a factor of 1 √ ℓ . Let h (ℓ) denote the input to Layer Normalization at layer ℓ. The modified output is computed as:</p><formula xml:id="formula_16">h (l) = LayerNorm(h (ℓ) ) × 1 √ ℓ ,<label>(12)</label></formula><p>where ℓ ∈ {1, 2, . . . , L}.</p><p>This scaling prevents excessive variance growth with depth, addressing a key limitation of Pre-LN. Unlike Mix-LN, which stabilizes gradients in deeper layers but suffers from training instability caused by Post-LN (Nguyen and Salazar, 2019; Wang et al., 2024), LayerNorm Scaling preserves the stability advantages of Pre-LN while enhancing the contribution of deeper layers to representation learning. Applying LayerNorm Scaling leads to a notable reduction of layerwise output variance, resulting in lower training loss and faster convergence than vanilla Pre-LN. Moreover, compared with previous LayerNorm variants <ref type="bibr">(Li et al., 2024b;</ref><ref type="bibr" target="#b22">Liu et al., 2020)</ref>, LayerNorm Scaling is hyperparameter-free, easy to implement, and does not introduce additional learnable parameters, making it computationally efficient and readily applicable to existing Transformer architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Theoretical Analysis of LayerNorm Scaling</head><p>Lemma 2. After applying our scaling method, the variances of x ′ ℓ and x ℓ , denoted as σ 2</p><p>x ′ ℓ and σ 2 x ℓ , respectively, exhibit the same growth trend, which is:</p><formula xml:id="formula_17">σ 2 x ℓ+1 = σ 2 x ℓ Θ(1 + 1 √ ℓσ x ℓ ),<label>(13)</label></formula><p>with the following growth rate bounds:</p><formula xml:id="formula_18">Θ(L) ≤ σ 2 x L ≤ Θ(L (2-ϵ) ). (<label>14</label></formula><formula xml:id="formula_19">)</formula><p>where ϵ is a small number with 0 &lt; ϵ ≤ 1/4.</p><p>From Lemma 2, we can conclude that our scaling method effectively slows the growth of the variance upper bound, reducing it from exponential to polynomial growth. Specifically, it limits the upper bound to a quadratic rate instead of an exponential one. Based on Theorem 1, after scaling, we obtain the following:</p><p>Theorem 2. For the scaled Pre-LN Transformers, the Euclidean norm of ∂y L ∂x1 is given by:</p><formula xml:id="formula_20">∂y L ∂x 1 2 ≤ L-1 ℓ=1 1 + 1 ℓσ x ℓ A + 1 ℓ 2 σ 2 x ℓ B ,<label>(15)</label></formula><p>where A and B are dependent on the scaled neural network parameters. Then the upper bound for the norm is given as follows: when σ 2</p><p>x ℓ grows at ℓ (2-ϵ) , (i.e., at its upper bound), we obtain:</p><formula xml:id="formula_21">σ 2 x ℓ ∼ ℓ (2-ϵ) , ∂y L ∂x 1 2 ≤ ω(1),<label>(16)</label></formula><p>where</p><formula xml:id="formula_22">ω denotes that if f (x) = ω(g(x)), then lim x→∞ f (x) g(x) = ∞.</formula><p>Meanwhile, when σ 2 x ℓ grows linearly (i.e., at its lower bound), we obtain:</p><formula xml:id="formula_23">σ 2 x ℓ ∼ ℓ, ∂y L ∂x 1 2 ≤ Θ(L).<label>(17)</label></formula><p>The detailed descriptions of A and B, and ϵ, along with the full proof, are provided in Appendices A.3 and A.4.</p><p>By comparing Theorem 1 (before scaling) with Theorem 2 (after scaling), we observe a substantial reduction in the upper bound of variance. Specifically, it decreases from exponential growth Θ(exp(L)) to at most quadratic growth Θ(L 2 ). In fact, this growth is even slower than quadratic expansion, as it follows Θ(L (2-ϵ) ) for some small ϵ &gt; 0.</p><p>When we select a reasonable upper bound for this expansion, we find that  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">LayerNorm Scaling Reduces Output Variance</head><p>As LayerNorm Scaling aims to reduce output variance, we validate this by comparing it with two scaling approaches: LayerScale <ref type="bibr" target="#b34">(Touvron et al., 2021)</ref> and Scaled Initialization <ref type="bibr" target="#b30">(Shoeybi et al., 2020)</ref>. LayerScale applies per-channel weighting using a diagonal matrix, diag(λ 1 , . . . , λ d ), where each weight λ i is initialized to a small value (e.g., λ i = ϵ). Unlike LayerNorm Scaling, LayerScale learns the scaling factors automatically, which does not necessarily induce a down-scaling effect. Scaled Initialization scales the initialization of W 0 and W 2 to small values by 1 √ 2L where L is the total number of transformer layers. Since scaling is applied only at initialization, we argue that Scaled Initialization may not effectively reduce variance throughout training. We further verify this in Figure <ref type="figure" target="#fig_0">1</ref>, where we can see the output variance of Scaled Initialization is as large as Pre-LN. Table <ref type="table" target="#tab_4">4</ref> presents the results of LLaMA-130M and LLaMA-250M. First, we can see that LayerScale degrades performance. While Scaled Initialization slightly improves over Pre-LN, it falls short of LayerNorm Scaling and the gap becomes larger for the larger model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Enhancing Deep Layers with LayerNorm Scaling</head><p>To evaluate how LayerNorm Scaling improves deep layer effectiveness, we conduct a layer pruning experiment on LLaMA-130M, systematically removing individual layers and measuring the performance drop (∆P (ℓ) ) on the ARCe benchmark <ref type="bibr" target="#b7">(Clark et al., 2018)</ref>. Figure <ref type="figure" target="#fig_3">4</ref> compares the pruning effects between standard Pre-LN and LayerNorm Scaling. In the Pre-LN, removing deep layers results in minimal performance degradation, indicating their limited   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Related Work</head><p>Layer Normalization in Language Models. <ref type="bibr">LN (Ba, 2016)</ref> was initially applied after the residual connection in the original Transformer <ref type="bibr" target="#b36">(Vaswani, 2017)</ref>, which is known as Post-LN. Later on, Pre-LN <ref type="bibr" target="#b2">(Baevski and Auli, 2019;</ref><ref type="bibr" target="#b8">Dai et al., 2019;</ref><ref type="bibr" target="#b27">Nguyen and Salazar, 2019)</ref> dominated LLMs, due to its compelling performance and stability <ref type="bibr" target="#b6">(Brown et al., 2020;</ref><ref type="bibr" target="#b35">Touvron et al., 2023;</ref><ref type="bibr" target="#b16">Jiang et al., 2023;</ref><ref type="bibr" target="#b5">Bi et al., 2024)</ref>.</p><p>Prior works have studied the effect of Pre-LN and Post-LN. <ref type="bibr" target="#b42">Xiong et al. (2020)</ref> proves that Post-LN tends to have larger gradients near the output layer, which necessitates smaller learning rates to stabilize training, whereas Pre-LN scales down gradients with the depth of the model, working better for deep Transformers. <ref type="bibr" target="#b39">Wang et al. (2019)</ref> empirically confirmed that Pre-LN facilitates stacking more layers and Post-LN suffers from gradient vanishing. The idea of connecting multiple layers was proposed in previous works <ref type="bibr" target="#b4">(Bapna et al., 2018;</ref><ref type="bibr" target="#b11">Dou et al., 2018;</ref><ref type="bibr" target="#b39">Wang et al., 2019)</ref>. Adaptive Model Initialization (Admin) was introduced to use additional parameters to control residual dependencies, stabilizing Post-LN. DeepNorm <ref type="bibr" target="#b38">(Wang et al., 2024)</ref> enables stacking 1000-layer Transformer by upscaling the residual connection before applying LN. Additionally, Ding et al. (2021) proposed Sandwich LayerNorm, normalizing both the input and output of each transformer sub-layer. Takase et al. (2023a) introduced B2T to bypass all LN except the final one in each layer. Li et al. (2024b) recently combines Post-LN and Pre-LN to enhance the middle layers. The Curse of Depth in Large Language Models</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we introduce the concept of the Curse of Depth in LLMs, highlighting an urgent yet often overlooked phenomenon: nearly half of the deep layers in modern LLMs are less effective than expected. We discover the root cause of this phenomenon is Pre-LN which is widely used in almost all modern LLMs. To tackle this issue, we introduce LayerNorm Scaling. By scaling the output variance inversely with the layer depth, LayerNorm Scaling ensures that all layers, including deeper ones, contribute meaningfully to training. Our experiments show that this simple modification improves performance, reduces resource usage, and stabilizes training across various model sizes. Layer-Norm Scaling is easy to implement, hyperparameter-free, and provides a robust solution to enhance the efficiency and effectiveness of LLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Impact Statement</head><p>This paper introduces the Curse of Depth in LLMs to call attention to the AI community an urgent but often overlooked phenomenon that nearly half of layers in modern LLMs are not as effective as we expect. The impact of this phenomenon is large that a significant amount of resources used to train LLMs are somehow wasted. We further introduce LayerNorm Scaling to ensure that all layers contribute meaningfully to model training. The result is a significant improvement in model efficiency, enabling better performance with fewer computational resources and training tokens. This innovation not only enhances LLM effectiveness across a variety of tasks but also reduces the environmental and financial costs of training large-scale models, making LLM development more sustainable and accessible. Layer-Norm Scaling presents a simple, hyperparameter-free solution that can be easily adopted, offering immediate practical benefits to the AI research community.</p><p>The Curse of Depth in Large Language Models</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Proofs of the Theorems</head><p>A.1. Proof of Lemma 1</p><p>Proof. Given Equation ( <ref type="formula" target="#formula_2">2</ref>) from <ref type="bibr">(Takase et al., 2023b)</ref>, we have:</p><formula xml:id="formula_24">y = x ℓ+1 = x ′ ℓ + FFN(LN(x ′ ℓ )), x ′ ℓ = x ℓ + Attn(LN(x ℓ )).<label>(18)</label></formula><p>Based on our Assumption 1, let Var(Attn(LN(x ℓ ))) = σ 2 Attn . Then we can write:</p><formula xml:id="formula_25">Var(x ′ ℓ ) = Var(x ℓ ) + Var(Attn(LN(x ℓ ))) + Cov(Attn(LN(x ℓ )), Var(x ℓ )) = σ 2 x ℓ + σ 2 Attn + ρ 1 • σ x ℓ • σ Attn ,<label>(19)</label></formula><p>where ρ 1 is the correlation factor. Similarly, let Var(FFN(LN(x ′ ℓ ))) = σ 2 FFN . Then we have:</p><formula xml:id="formula_26">σ 2 x ℓ+1 = σ ( x ′ ℓ ) 2 + σ 2 FFN + ρ 2 • σ x ′ ℓ • σ FFN ,<label>(20)</label></formula><p>where ρ 2 is the correlation factor. Thus, the relationship between Var(x ℓ+1 ) and Var(x ℓ ) becomes:</p><formula xml:id="formula_27">σ 2 x ℓ+1 = σ 2 x ℓ + σ 2 Attn + σ 2 FFN + ρ 1 • σ x ℓ • σ Attn + ρ 2 • σ x ′ ℓ • σ FFN .<label>(21)</label></formula><p>A.1.1. VARIANCE OF THE ATTENTION</p><p>The scaled dot-product attention mechanism is defined as:</p><formula xml:id="formula_28">Attn(Q, K, V ) = softmax QK T √ d k V.</formula><p>The softmax function outputs a probability distribution over the keys. Let the softmax output be</p><formula xml:id="formula_29">A = softmax QK T √ d k</formula><p>, where A is a matrix with each row summing to 1. The final attention output is obtained by multiplying the softmax output A with the value matrix V :</p><formula xml:id="formula_30">Attn(Q, K, V ) = AV.</formula><p>To simplify the analysis, we make the following additional assumptions: The softmax output A is approximately uniform, meaning each element of A is roughly 1/n, where n is the number of keys/values. Given this assumption, the variance of the attention is:</p><formula xml:id="formula_31">Var(Attn(Q, K, V )) ∼ Var(AV ) = 1 n n i=1 Var(V i ) = 1 n • n • σ 2 V = σ 2 V = σ 2 W . (<label>22</label></formula><formula xml:id="formula_32">)</formula><p>where W is the universal weight matrix defined as before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.2. VARIANCE OF THE FEED-FORWARD NETWORK</head><p>The feed-forward network (FFN) in transformers typically consists of two linear transformations with a ReLU activation in between. The FFN can be written as:</p><formula xml:id="formula_33">FFN(x) = W 2 • ReLU(W 1 • x + b 1 ) + b 2 . (<label>23</label></formula><formula xml:id="formula_34">)</formula><p>where W 1 and W 2 are weight matrices, and b 1 and b 2 are bias vectors.</p><p>Using the result obtained by <ref type="bibr" target="#b38">Wang et al. (2024)</ref>, we get:</p><formula xml:id="formula_35">σ 2 FFN ∼ σ 2 W1 • σ 2 W2 = σ 4 W .<label>(24)</label></formula><p>In conclusion:</p><formula xml:id="formula_36">σ 2 x ′ ℓ = σ 2 x ℓ + σ 2 W + ρ 2 • σ x ℓ • σ W = σ 2 x ℓ (1 + σ W σ x ℓ + σ 2 W σ 2 x ℓ ) = σ 2 x ℓ Θ(1 + 1 σ x ℓ ).<label>(25)</label></formula><p>For simplicity, we set the numerator part to 1. Substitute</p><formula xml:id="formula_37">σ x ′ ℓ = σ x ℓ 1 + σ 2 W σ 2 x ℓ + ρ 2 • σ W σx ℓ</formula><p>. into Equation ( <ref type="formula" target="#formula_27">21</ref>) we get:</p><formula xml:id="formula_38">σ 2 x ℓ+1 = σ 2 x ℓ + σ 2 W + σ 4 W + ρ 1 • σ x ℓ • σ W + ρ 2 • σ x ′ ℓ • σ 2 W = σ 2 x ℓ + σ 2 W + σ 4 W + ρ 1 • σ x ℓ • σ W + ρ 2 • σ x ℓ • σ 2 W + ρ 2 σ 4 W 2σ x ℓ + ρ 2 2 σ 3 W σ x ℓ 2 = σ 2 x ℓ Θ(1 + 1 σ x ℓ ).<label>(26)</label></formula><p>From the result we can generally infer that the variance accumulates layer by layer. The variance with regard to σ x1 :</p><formula xml:id="formula_39">σ 2 x ℓ = σ 2 x1 Θ ℓ-1 k=1 1 + 1 σ x k .<label>(27)</label></formula><p>We can also obtain a similar result for σ 2</p><p>x ′ ℓ .</p><p>We observe that for any σ 2 x k ≥ 1, the sequence is increasing, meaning each term in the product is bounded. Consequently, the entire product is bounded above by:</p><formula xml:id="formula_40">σ 2 x ℓ ≤ σ 2 x1 ℓ-1 k=1 1 + 1 σ x1 = σ 2 x1 1 + 1 σ x1 ℓ-1 = exp Θ(L). (<label>28</label></formula><formula xml:id="formula_41">)</formula><p>Taking the natural logarithm of both sides:</p><formula xml:id="formula_42">log(σ 2 x ℓ ) = log σ 2 x1 ℓ-1 k=1 1 + 1 σ 2 x k = ℓ-1 k=1 log 1 + 1 σ 2 x k + log(σ 2 x1 ) ≥ ℓ-1 k=1 1 σ 2 x k - 1 2 1 σ 2 x k 2 + log(σ 2 x1 ).<label>(29)</label></formula><p>Exponentiating both sides to find the lower bound for σ 2 x ℓ , we obtain:</p><formula xml:id="formula_43">σ 2 x ℓ ≥ σ 2 x1 exp ℓ-1 k=1 1 σ 2 x k - 1 2σ 2 x k .</formula><p>This provides a tighter lower bound for σ 2</p><p>x ℓ compared to the upper bound of Equation ( <ref type="formula" target="#formula_40">28</ref>). Since we know the upper bound of variance grows exponentially, the lower bound must be sub-exponential. Therefore, for σ 2</p><p>x ℓ = ℓ, we must have:</p><formula xml:id="formula_44">σ 2 x ℓ ≥ σ 2 x1 exp ℓ-1 k=1 1 k - 1 2k = Θ(exp( √ L)) ≥ Θ(L).</formula><p>Therefore, the increasing lower bound for σ 2 x ℓ must grows faster than a linear function. So, the increasing of variance is sub-exponential.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Proof of Theorem 1</head><p>In this proof, we will divide the argument into two parts: first, the calculation of the Lemma 3, and second, the analysis of ∂y ℓ ∂x1 . Lemma 3. For an L-layered Pre-LN Transformer, ∂y L ∂x1 using Equations (2) and (3) is given by:</p><formula xml:id="formula_45">∂y L ∂x 1 = L-1 n=1 ∂y ℓ ∂x ′ ℓ • ∂x ′ ℓ ∂x ℓ . (<label>30</label></formula><formula xml:id="formula_46">)</formula><p>The upper bound for the norm of ∂y L ∂x1 is:</p><formula xml:id="formula_47">∂y L ∂x 1 2 ≤ L-1 l=1 1 + σ 2 σ x ′ ℓ ( √ d + √ d FFN ) 2 × 1 + 2dh √ s + 2 + 1 √ s σ 2 σ x ℓ σ 2 d d head + 1 + d head /d .<label>(31)</label></formula><p>Here, h denotes the number of heads, s is the sequence length, and d, d FFN , and d head are the dimension of the embedding, FFN layer and multi-head attention layer, respectively. The standard deviation of W Q , W K , W V , and W FFN at layer ℓ is σ based on Assumption 1.</p><p>A.2.1. PROOF OF LEMMA 3</p><p>Proof. Our derivation follows results in <ref type="bibr">(Takase et al., 2023b)</ref>, specifically Equation ( <ref type="formula" target="#formula_8">7</ref>), which provides an upper bound on the norm of ∂y ℓ ∂x1 as:</p><formula xml:id="formula_48">∂y ℓ ∂x 1 2 = L-1 l=1 ∂y ℓ ∂x ′ ℓ ∂x ′ ℓ ∂x ℓ 2 . (<label>32</label></formula><formula xml:id="formula_49">)</formula><p>Thus, we can estimate the upper bound of the gradient norm of ∂y ℓ ∂x1 by analyzing the spectral norms of the Jacobian matrices for the FFN layer and the self-attention layer, namely, FFN:</p><formula xml:id="formula_50">∂y ℓ ∂x ′ ℓ 2 Attention: ∂x ′ ℓ ∂x ℓ 2 . (<label>33</label></formula><formula xml:id="formula_51">)</formula><p>We now derive an upper bound of ∥ ∂y ℓ ∂x ′ ℓ ∥ 2 as follows:</p><formula xml:id="formula_52">∂y ℓ ∂x ′ ℓ 2 ≤ 1 + ∂FFN(LN(x ′ ℓ )) ∂LN(x ′ ℓ ) 2 ∂LN(x ′ ℓ ) ∂x ′ ℓ 2 . (<label>34</label></formula><formula xml:id="formula_53">)</formula><p>Let σ w1 ℓ and σ w2 ℓ be the standard deviations of W 1 ℓ and W 2 ℓ , respectively. From Assumption 1, the spectral norms of W 1 ℓ and W 2 ℓ are given by their standard deviations and dimensions <ref type="bibr" target="#b37">(Vershynin, 2018)</ref>, so wo have:</p><formula xml:id="formula_54">∥W 1 ∥ 2 ∼ σ 1 d + d FFN .</formula><p>. For simplicity, we assume that d, and d FFN are equal, thus,</p><formula xml:id="formula_55">∂FFN(LN(x ′ ℓ )) ∂LN(x ′ ℓ ) 2 = ∥W 1 ℓ W 2 ℓ ∥ 2 ≤ σ 1 σ 2 ( √ d + d ffn ) 2 . (<label>35</label></formula><formula xml:id="formula_56">)</formula><p>Finally, we have the following bound:</p><formula xml:id="formula_57">∂y ℓ ∂x ′ ℓ 2 ≤ 1 + σ w1 ℓ σ w2 ℓ σ x ′ ℓ ( √ d + √ d FFN ) 2 = 1 + σ 2 ℓ σ x ′ ℓ ( √ d + √ d FFN ) 2 . (<label>36</label></formula><formula xml:id="formula_58">)</formula><p>Following a similar procedure for the FFN, we rewrite ∥ ∂x ′ ∂x ∥ 2 in Equation ( <ref type="formula" target="#formula_50">33</ref>) as:</p><formula xml:id="formula_59">∂x ′ ∂x 2 ≤ 1 + ∂Attn(LN(x)) ∂LN(x) 2 ∂LN(x) ∂x 2 . (<label>37</label></formula><formula xml:id="formula_60">)</formula><p>Let Z(•) = concat(head 1 (•), . . . , head h (•)) and J Z denote the Jacobian of the Z(•). We can now express the spectral norm of the Jacobian matrix of attntion as:</p><formula xml:id="formula_61">∂Attn(LN(x ℓ )) ∂LN(x ℓ ) 2 = W O ℓ Z(LN(x ℓ )) ∂Z(LN(x ℓ )) ∂LN(x ℓ ) 2 = ∥W O ℓ J Z ℓ ∥ 2 . (<label>38</label></formula><formula xml:id="formula_62">)</formula><p>From <ref type="bibr" target="#b37">(Vershynin, 2018)</ref>, we know that:</p><formula xml:id="formula_63">∥J Z ℓ ∥ 2 ≤ h √ s + 2 + 1 √ s σ 3 d 3 d head + σ ℓ x √ d + d head . (<label>39</label></formula><formula xml:id="formula_64">)</formula><p>Here h is the number of heads, s is the sequence length, and the standard deviation of W Q , W K , and W V is σ.</p><p>By combining the inequalities ( <ref type="formula" target="#formula_57">36</ref>), ( <ref type="formula" target="#formula_63">39</ref>) and ( <ref type="formula" target="#formula_59">37</ref>), and assuming that all σ values are the same for simplicity. we obtain:</p><formula xml:id="formula_65">∂y L ∂x 1 2 ≤ L-1 l=1 1 + σ 2 σ x ′ ℓ ( √ d + √ d FFN ) 2 × 1 + 2dh √ s + 2 + 1 √ s σ 2 σ x ℓ σ 2 d d head + 1 + d head /d .<label>(40)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.2. ANALYSIS OF THE UPPER BOUND</head><p>As discussed in <ref type="bibr">(Takase et al., 2023b)</ref>, σ should be sufficiently small, and the standard deviation, σ x ′ ℓ or σ x ℓ should satisfy the condition σ 2 ≪ σ x ′ ℓ to maintain the lazy training scheme. Thus, we obtain the following bound for the product over ℓ from 1 to L:</p><p>To find the bound for ∂y ℓ ∂x1 2 with respect to ℓ, we simplify the given inequality by approximating σ x ℓ and σ x ′ ℓ . Based on Equation (25), σ x ℓ is only one layer ahead of σ x ′ ℓ , and this layer does not significantly affect the overall performance of deep Transformer networks. Furthermore, based on Lemma 1, we assume that σ x ′ ℓ = σ x ℓ . Equation (3) can be expressed in a traditional product form <ref type="bibr" target="#b40">(Whittaker and Watson, 1996)</ref> for σ x ℓ :</p><formula xml:id="formula_66">∂y L ∂x 1 2 ≤ L-1 l=1 1 + 1 σ x ℓ A + 1 σ 2 x ℓ B ,<label>(41)</label></formula><p>where</p><formula xml:id="formula_67">A = σ 2 ( √ d + √ d FFN ) 2 + 2dh √ s + 2 + 1 √ s σ 2 d d head + 1 + d head /d ,<label>(42)</label></formula><p>and</p><formula xml:id="formula_68">B = 2dh √ s + 2 + 1 √ s σ 4 d d head ,<label>(43)</label></formula><p>where A and B are independent of σ x ℓ , and under our assumption, are treated as constants.</p><p>From classical infinite series analysis, it is known that as σ x ℓ grows at a faster rate, the upper bound of the product decreases. The proof is omitted here for brevity. For the upper bound on the convergence rate of σ 2 x ℓ , we assume σ 2 x ℓ = exp(ℓ) without loss of generality. Under this condition, we can derive the following result:</p><p>Taking the natural logarithm of the product:</p><formula xml:id="formula_69">log L-1 k=1 1 + A e k + B e 2k = L-1 k=1 log 1 + A e k + B e 2k .</formula><p>Using the Taylor series expansion for log(1 + x), and applying this to our sum, we get:</p><formula xml:id="formula_70">∞ k=1 log 1 + A e k + B e 2k = ∞ k=1 A e k + B e 2k - 1 2 A e k + B e 2k 2 + 1 3 A e k + B e 2k 3 -• • • .</formula><p>By evaluating the sums for each order of terms, we find that the result is a constant. Carrying this out for each term, we obtain:</p><formula xml:id="formula_71">log L-1 k=1 1 + A e k + B e 2k ∼ A e -1 + B e 2 -1 - 1 2 A 2 e 2 -1 + 2 A • B e 3 -1 + B 2 e 4 -1<label>.</label></formula><p>Thus, the product is approximately:</p><formula xml:id="formula_72">∂y L ∂x 1 2 ≤ exp A e -1 + B e 2 -1 - 1 2 A 2 e 2 -1 + 2 A • B e 3 -1 + B 2 e 4 -1 = M,<label>(44)</label></formula><p>where M is a constant.</p><p>For the lower bound on the convergence rate of σ 2 x ℓ , we assume σ 2 x ℓ = ℓ without loss of generality. Under this condition, we derive the following result. Taking the logarithm of the product, applying the Taylor series expansion for log(1 + x), and applying this to our sum:</p><formula xml:id="formula_73">∞ k=1 log 1 + A k + B e k 2 = ∞ k=1 A k + B e k 2 - 1 2 A k + B e k 2 2 + 1 3 A k + B e k 2 3 -• • • .</formula><p>For the first-order terms:</p><formula xml:id="formula_74">∞ k=1 A k + B e k 2 = A ∞ k=1 1 k + B ∞ k=1 1 e k 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The series</head><p>∞ k=1 1 k is the harmonic series, which diverges. However, we approximate it using the Euler-Mascheroni constant γ and the fact recognize that the harmonic series grows logarithmically:</p><formula xml:id="formula_75">∞ k=1 1 k ∼ log n + γ (for large n).</formula><p>The other series such as ∞ k=1 1 e k 2 converge because e k 2 grows very rapidly.</p><p>For higher-order terms, they converge to constant, involving the series ∞ k=1 1 k 2 converges to π 2 6 , so they contribute a constant. Exponentiating both sides, we get:</p><formula xml:id="formula_76">∞ k=1 1 + A k + B e k 2 ∼ exp (A(log n + γ) + const) .</formula><p>Thus, the growth rate of the upper bound for ∂y L ∂x1 2 is:</p><formula xml:id="formula_77">∂y L ∂x 1 2 ≤ Θ(L).<label>(45)</label></formula><p>A.3. Proof of Lemma 2</p><p>Proof. After scaling, the equation becomes:</p><formula xml:id="formula_78">y = x ℓ+1 = x ′ ℓ + FFN( 1 √ ℓ LN(x ′ ℓ )), x ′ ℓ = x ℓ + Attn( 1 √ ℓ LN(x ℓ )).<label>(46)</label></formula><p>Folloing the same analysis as before, we scale the Attention and FFN sub-layers, yielding:</p><formula xml:id="formula_79">σ 2 Attn = 1 nℓ • n • σ 2 V = 1 ℓ σ 2 V = σ 2 W ℓ , σ 2 FFN ∼ σ 2 W1 ℓ • σ 2 W2 ℓ = σ 4 W ℓ 2 . (<label>47</label></formula><formula xml:id="formula_80">)</formula><p>In conclusion:</p><formula xml:id="formula_81">σ 2 x ′ ℓ = σ 2 x ℓ + σ 2 W + ρ 2 • σ x ℓ • σ W √ ℓ = σ 2 x ℓ Θ(1 + 1 √ ℓσ x ℓ ).<label>(48)</label></formula><p>Similarly, we obtain:</p><formula xml:id="formula_82">σ 2 x ℓ+1 = σ 2 x ℓ Θ(1 + 1 √ ℓσ x ℓ ).<label>(49)</label></formula><p>Taking the natural logarithm of both sides:</p><formula xml:id="formula_83">log(σ 2 x ℓ ) = log σ 2 x1 ℓ-1 k=1 1 + 1 ℓσ 2 x k = ℓ-1 k=1 log 1 + 1 ℓσ 2 x k + log(σ 2 x1 ) ≥ ℓ-1 k=1 1 ℓσ 2 x k - 1 2 1 ℓσ 2 x k 2 + log(σ 2 x1 ).<label>(50)</label></formula><p>To establish a lower bound for σ 2 x ℓ , we exponentiate both sides. Setting σ 2 x ℓ = ℓ, we must have:</p><formula xml:id="formula_84">σ 2 x ℓ ≥ σ 2 x1 exp ℓ-1 k=1 1 k - 1 2k = Θ(exp(log L)) ≥ Θ(L).<label>(51)</label></formula><p>Therefore, the increasing lower bound σ 2 x ℓ is greater than a linear function.</p><p>Similarly, assuming σ 2 x ℓ = ℓ (2-ϵ) , we have:</p><formula xml:id="formula_85">σ 2 x ℓ = σ 2 x1 ℓ-1 k=1 1 + 1 ℓ 2-ϵ/2 ∼ exp ℓ-1 k=1 1 k 2-ϵ/2 ∼ exp ℓ ϵ/2-1 -1 ϵ/2 -1 ≤ Θ(ℓ (2-ϵ) ) ≤ Θ(ℓ 2 ). (<label>52</label></formula><formula xml:id="formula_86">)</formula><p>Here ϵ is a small constant with 0 &lt; ϵ ≤ 1/4. Therefore, the increasing upper bound of σ 2 x ℓ is slower than the ℓ 3 function, leading to: σ 2 x ℓ ≤ Θ(L 2 ) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Proof of Theorem 2</head><p>Proof. Similarly, after applying the scaling transformation, we derive an upper bound for ∥ ∂y ℓ ∂x ′ ℓ ∥ 2 as follows:</p><formula xml:id="formula_87">∂y ℓ ∂x ′ ℓ 2 ≤ 1 + ∂FFN(LN(x ′ ℓ )) ∂LN(x ′ ℓ ) 2 1 √ ℓ 2 ∂LN(x ′ ℓ ) ∂x ′ ℓ 2 = 1 + σ 2 ℓ ℓσ x ′ ℓ ( √ d + √ d FFN ) 2 . (<label>53</label></formula><formula xml:id="formula_88">)</formula><p>Similarly, rewriting Equation (33) after scaling, we have</p><formula xml:id="formula_89">∂x ′ ∂x 2 ≤ 1 + ∂Attn(LN(x)) ∂LN(x) 2 1 √ ℓ 2 ∂LN(x) ∂x 2 . (<label>54</label></formula><formula xml:id="formula_90">)</formula><p>By combining the bound (53), and inequality (54), and assuming all σ are equal for simplicity, we obtain:</p><formula xml:id="formula_91">∂y L ∂x 1 2 ≤ L-1 l=1 1 + σ 2 ℓσ x ′ ℓ ( √ d + √ d FFN ) 2 × 1 + 2dh √ s + 2 + 1 √ s σ 2 ℓσ x ℓ σ 2 d d head + 1 + d head /d .<label>(55)</label></formula><p>Equation ( <ref type="formula" target="#formula_91">55</ref>) is a traditional product form <ref type="bibr" target="#b40">(Whittaker and Watson, 1996)</ref> for σ x ℓ . After scaling, it becomes:</p><formula xml:id="formula_92">∂y L ∂x 1 2 ≤ L-1 l=1 1 + 1 ℓσ x ℓ A + 1 ℓ 2 σ 2 x ℓ B ,<label>(56)</label></formula><p>where A and B retain their forms from Equation (42) and Equation ( <ref type="formula" target="#formula_68">43</ref>) and are treated as constants.</p><p>Regarding the upper bound on the convergence rate of σ 2 x ℓ , we assume σ 2 x ℓ = ℓ (2-ϵ) without loss of generality. For large L, the product can be approximated using the properties of infinite products:</p><formula xml:id="formula_93">L-1 ℓ=1 1 + A ℓ 2-ϵ/2 + B ℓ 4-ϵ ∼ exp L-1 ℓ=1 A ℓ 2-ϵ/2 + B ℓ 4-ϵ . (<label>57</label></formula><formula xml:id="formula_94">)</formula><p>Then, by evaluating the sum in the exponent, we obtain:</p><formula xml:id="formula_95">L-1 ℓ=1 1 + A ℓ 2-ϵ/2 + B ℓ 4-ϵ ∼ exp A • ℓ ϵ/2-1 -1 ϵ/2 -1 + B • ℓ ϵ-3 -1 ϵ -3 . (<label>58</label></formula><formula xml:id="formula_96">)</formula><p>Therefore, we establish the upper bound:</p><formula xml:id="formula_97">∂y L ∂x 1 2 ≤ Θ exp A • ℓ ϵ/2-1 -1 ϵ/2 -1 + B • ℓ ϵ-3 -1 ϵ -3 = ω(1),<label>(59)</label></formula><p>where ω(1) denotes a growth strictly greater than a constant as defined before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training Loss Curve</head><p>We report the training loss curve of Pre-LN and LayerNorm Scaling in Figure <ref type="figure" target="#fig_4">5</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Variance Growth in Pre-LN Training</head><p>To analyze the impact of Pre-LN on variance propagation, we track the variance of layer outputs across different depths during training.</p><p>Figure <ref type="figure" target="#fig_5">6</ref> illustrates the layer-wise variance in LLaMA-130M with Pre-LN at 1000, 3000, and 6000 epochs. Across all stages, variance remains low in shallow layers but grows exponentially in deeper layers, confirming that this issue persists throughout training rather than being a temporary effect. This highlights the necessity of stabilization techniques like LayerNorm Scaling to control variance and ensure effective deep-layer learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Layerwise output variance. This figure compares the output variance across various layers for different setups: (1) Pre-LN; (2) Pre-LN with Scaled Initialization; and (3) LayerNorm Scaling. The experiments are conducted on the LLaM-130M model trained for 10,000 steps. The proposed LayerNorm Scaling effectively controls the variance across layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Performance drop of layer pruning across different LLMs. (a) BERT-Large (Post-LN), (b) Mistral-7B (Pre-LN), (c) Qwen-7B (Pre-LN), (d) DeepSeek-7B (Pre-LN), (e) LLaMA2-7B (Pre-LN), and (f) LLaMA2-13B (Pre-LN). The results show that Pre-LN models exhibit significant inefficiency in deeper layers, while Post-LN models maintain strong deep-layer contributions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Comparison between Pre-LN (a) and LayerNorm Scaling (b). LayerNorm Scaling applies a scaling factor inversely proportional to the square root of the layer index l, preventing excessive variance growth and stabilizing training dynamics across layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Performance drop of layer pruning on LLaMA-130M. LayerNorm Scaling enables deep layers to make a meaningful contribution to the model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Training loss of LLaMA-1B with Pre-LN and LayerNorm Scaling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Variance growth across layers in LLaMA-130M with Pre-LN. Each subplot shows the variance at different training stages (1000, 3000, and 6000 epochs). In all cases, the variance follows an exponential growth pattern as depth increases, indicating that deeper layers experience uncontrolled variance amplification regardless of training progress.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Consequently, fewer layers act as identity mappings compared to the original Pre-LN where nearly all deep layers collapsed into identity transformations. Instead, the after-scaled network effectively utilizes more layers, even as the depth approaches infinity, leading to improved expressivity and trainability.To evaluate the effectiveness of LayerNorm Scaling, we follow the experimental setup ofLi et al. (2024b), using the same model configurations and training conditions to compare it with widely used normalization techniques, including Post-LN (Nguyen and Salazar, 2019), DeepNorm<ref type="bibr" target="#b38">(Wang et al., 2024)</ref>, and Pre-LN<ref type="bibr" target="#b8">(Dai et al., 2019)</ref>. In line with Lialin et al. (2023) and Zhao et al. (2024), we conduct experiments using LLaMA-based architectures with model sizes of 130M, 250M, 350M, and 1B parameters, ensuring consistency in architecture and training settings. Perplexity (↓) comparison of various layer normalization methods across various LLaMA sizes.</figDesc><table><row><cell>∂y L ∂x1 2</cell><cell cols="2">no longer possesses a strict upper</cell></row><row><cell cols="2">bound. That is, as the depth increases, ∂y L ∂x1 2</cell><cell>continues to</cell></row></table><note><p>The architecture incorporates RMSNorm<ref type="bibr" target="#b29">(Shazeer, 2020)</ref> </p><p>and SwiGLU activations<ref type="bibr" target="#b44">(Zhang and Sennrich, 2019)</ref></p><p>, which are applied consistently across all model sizes and normalization methods. For optimization, we use the Adam optimizer<ref type="bibr" target="#b17">(Kingma, 2015)</ref> </p><p>and adopt size-specific learning rates: 1 × 10 -3 for models up to 350M parameters, and 5 × 10 -4 for the 1B parameter model. All models share Sandwich-LN<ref type="bibr" target="#b10">(Ding et al., 2021)</ref></p><p>, and Group-LN<ref type="bibr" target="#b41">(Wu and He, 2018;</ref><ref type="bibr" target="#b24">Ma et al., 2024)</ref></p><p>. Table2</p><p>shows that Admin and Group-LN degrade performance. Sandwich-LN slightly outperforms Pre-LN. Both Mix-LN and LayerNorm Scaling improve over Pre-LN by good margins. However, Mix-LN fails to reduce perplexity under 26, falling short of Layer-Norm Scaling.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison against other normalization methods on LLaMA-130M. Perplexity (↓) is reported.</figDesc><table><row><cell cols="6">Pre-LN Admin Group-LN Sandwich-LN Mix-LN LayerNorm Scaling</cell></row><row><cell>26.73</cell><cell>27.91</cell><cell>28.01</cell><cell>26.51</cell><cell>26.07</cell><cell>25.76</cell></row><row><cell cols="4">5.2. Supervised Fine-tuning</cell><cell></cell><cell></cell></row><row><cell cols="6">We believe that LayerNorm Scaling allows deeper layers</cell></row><row><cell cols="6">in LLMs to contribute more effectively during supervised</cell></row><row><cell cols="6">fine-tuning by alleviating gradient vanishing associated with</cell></row><row><cell cols="6">increasing depth. Compared to models trained with Pre-</cell></row><row><cell cols="6">LN, the deeper layers with LayerNorm Scaling maintain</cell></row><row><cell cols="6">stable output variance, preventing uncontrolled growth and</cell></row><row><cell cols="6">ensuring effective feature representation. As a result, deeper</cell></row><row><cell cols="6">layers contribute more effectively to feature transformation,</cell></row><row><cell cols="6">enhancing representation learning and improving general-</cell></row><row><cell cols="5">ization on complex downstream tasks.</cell><cell></cell></row><row><cell cols="6">To verify this, we follow the fine-tuning methodologies in Li</cell></row><row><cell>et al.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>(2024b) andLi et al. (2024a)</p><p>, applying the same optimization settings as pre-training. We fine-tune models from Section 5.1 on the Commonsense170K dataset<ref type="bibr" target="#b15">(Hu et al., (Hu et al., 2023)</ref></p><p>) across eight downstream tasks. The results, presented in Table3</p><p>, demonstrate that LayerNorm Scaling</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Fine-tuning performance (↑) of LLaMA with various normalizations.</figDesc><table><row><cell>Method</cell><cell></cell><cell></cell><cell cols="7">MMLU BoolQ ARC-e PIQA Hellaswag OBQA Winogrande Average</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">LLaMA-250M</cell><cell></cell></row><row><cell>Post-LN (Ba, 2016)</cell><cell></cell><cell></cell><cell>22.95</cell><cell></cell><cell>37.83</cell><cell>26.94</cell><cell>52.72</cell><cell>26.17</cell><cell>11.60</cell><cell>49.56</cell><cell>32.54</cell></row><row><cell cols="3">DeepNorm (Wang et al., 2024)</cell><cell>23.60</cell><cell></cell><cell>37.86</cell><cell>36.62</cell><cell>61.10</cell><cell>25.69</cell><cell>15.00</cell><cell>49.57</cell><cell>35.63</cell></row><row><cell cols="2">Mix-LN (Li et al., 2024b)</cell><cell></cell><cell>26.53</cell><cell></cell><cell>56.12</cell><cell>41.68</cell><cell>66.34</cell><cell>30.16</cell><cell>18.00</cell><cell>50.56</cell><cell>41.34</cell></row><row><cell cols="3">Pre-LN (Baevski and Auli, 2019)</cell><cell>24.93</cell><cell></cell><cell>38.35</cell><cell>40.15</cell><cell>63.55</cell><cell>26.34</cell><cell>16.20</cell><cell>49.01</cell><cell>36.93</cell></row><row><cell cols="2">Pre-LN + LayerNorm Scaling</cell><cell></cell><cell>27.08</cell><cell></cell><cell>58.17</cell><cell>45.24</cell><cell>67.38</cell><cell>32.81</cell><cell>18.80</cell><cell>52.49</cell><cell>43.14</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">LLaMA-1B</cell><cell></cell></row><row><cell>Post-LN (Ba, 2016)</cell><cell></cell><cell></cell><cell>22.95</cell><cell></cell><cell>37.82</cell><cell>25.08</cell><cell>49.51</cell><cell>25.04</cell><cell>13.80</cell><cell>49.57</cell><cell>31.96</cell></row><row><cell cols="3">DeepNorm (Wang et al., 2024)</cell><cell>23.35</cell><cell></cell><cell>37.83</cell><cell>27.06</cell><cell>52.94</cell><cell>26.19</cell><cell>11.80</cell><cell>49.49</cell><cell>32.67</cell></row><row><cell cols="2">Mix-LN (Li et al., 2024b)</cell><cell></cell><cell>23.19</cell><cell></cell><cell>37.83</cell><cell>25.08</cell><cell>49.51</cell><cell>25.04</cell><cell>11.80</cell><cell>49.57</cell><cell>31.72</cell></row><row><cell cols="3">Pre-LN (Baevski and Auli, 2019)</cell><cell>26.54</cell><cell></cell><cell>62.20</cell><cell>45.70</cell><cell>67.79</cell><cell>30.96</cell><cell>17.40</cell><cell>50.51</cell><cell>43.01</cell></row><row><cell cols="2">Pre-LN + LayerNorm Scaling</cell><cell></cell><cell>28.69</cell><cell></cell><cell>61.80</cell><cell>48.85</cell><cell>67.92</cell><cell>33.94</cell><cell>18.60</cell><cell>54.30</cell><cell>44.87</cell></row><row><cell>0</cell><cell>2</cell><cell cols="2">4 Layer Index 6</cell><cell>8</cell><cell>10</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Comparison against other scaling methods.</figDesc><table><row><cell>Perplexity (↓)</cell><cell cols="2">LLaMA-130M LLaMA-250M</cell></row><row><cell>Training Tokens</cell><cell>2.2B</cell><cell>3.9B</cell></row><row><cell>Pre-LN</cell><cell>26.73</cell><cell>21.92</cell></row><row><cell>+ LayerScale</cell><cell>27.93</cell><cell>23.45</cell></row><row><cell>+ Scaled Initialization</cell><cell>26.04</cell><cell>20.98</cell></row><row><cell>+ LayerNorm Scaling</cell><cell>25.76</cell><cell>20.35</cell></row><row><cell cols="3">role in representation learning. In contrast, with LayerNorm</cell></row><row><cell cols="3">Scaling, pruning deeper layers leads to a more pronounced</cell></row><row><cell cols="3">drop, suggesting they now play a more active role in learn-</cell></row><row><cell cols="3">ing. While early layers remain critical in both models, the</cell></row><row><cell cols="3">performance degradation in the LayerNorm Scaling is more</cell></row><row><cell cols="3">evenly distributed across layers, reflecting a more balanced</cell></row><row><cell cols="3">learning process. These findings confirm that LayerNorm</cell></row><row><cell cols="3">Scaling mitigates the Curse of Depth by ensuring deeper</cell></row><row><cell cols="2">layers contribute effectively to training.</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_0"><p>University of Oxford, UK. Correspondence to: Shiwei Liu &lt;shiwei.liu@maths.ox.ac.uk&gt;. earlier ones<ref type="bibr" target="#b43">(Yin et al., 2024;</ref><ref type="bibr" target="#b13">Gromov et al., 2024;</ref><ref type="bibr" target="#b25">Men et al., 2024)</ref>. On the one hand, this interesting observation provides an effective indicator for LLM compression. For instance, we can compress deeper layers significantly more<ref type="bibr" target="#b43">(Yin et al., 2024;</ref><ref type="bibr" target="#b43">Lu et al., 2024;</ref><ref type="bibr" target="#b12">Dumitru et al., 2024)</ref> to achieve high compression ratios. Even more aggressively, entire deep layers can be pruned completely without compromising performance for the sake of more affordable LLMs<ref type="bibr" target="#b26">(Muralidharan et al., 2024;</ref><ref type="bibr" target="#b31">Siddiqui et al., 2024)</ref>.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Lama Ahmad</title>
		<author>
			<persName><forename type="first">Josh</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal ; Ilge Akkaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florencia</forename><surname>Leoni Aleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janko</forename><surname>Altenschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Altman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.08774</idno>
	</analytic>
	<monogr>
		<title level="m">Shyamal Anadkat, et al. Gpt-4 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ba</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adaptive input representations for neural language modeling</title>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Jinze</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfei</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbin</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.16609</idno>
		<title level="m">Qwen technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Training deeper neural machine translation models with transparent attention</title>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mia</forename><surname>Xu Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deepseek llm: Scaling opensource language models with longtermism</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deli</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanhuang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damai</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honghui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiushi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.02954</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isaac</forename><surname>Cowhey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carissa</forename><surname>Schoenick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05457</idno>
		<title level="m">Think you have solved question answering? try arc, the ai2 reasoning challenge</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NAACL</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cogview: Mastering text-to-image generation via transformers</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyi</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wendi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">19822-19835, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Exploiting deep representations for neural machine translation</title>
		<author>
			<persName><forename type="first">Zi-Yi</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Layer-wise quantization: A pragmatic and effective method for quantizing llms beyond integer bit-levels</title>
		<author>
			<persName><forename type="first">Razvan-Gabriel</forename><surname>Dumitru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishabh</forename><surname>Maheshwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul-Ioan</forename><surname>Clotan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sathwik</forename><surname>Tejaswi Madhusudhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.17415</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Gromov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kushal</forename><surname>Tirumala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hassan</forename><surname>Shapourian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Glorioso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">A</forename><surname>Roberts</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.17887</idno>
		<title level="m">The unreasonable ineffectiveness of the deeper layers</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Measuring massive multitask language understanding</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Collin</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Llm-adapters: An adapter family for parameterefficient fine-tuning of large language models</title>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yihuai</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ee-Peng</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy Ka-Wei</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Albert Q Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devendra</forename><surname>Bamford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Singh Chaplot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>De Las Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gianna</forename><surname>Bressand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lengyel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucile</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><surname>Saulnier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.06825</idno>
		<title level="m">Mistral 7b</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Vedang</forename><surname>Lad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wes</forename><surname>Gurnee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Tegmark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.19384</idno>
		<title level="m">The remarkable robustness of llms: Stages of inference? arXiv preprint</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Owlore: Outlier-weighed layerwise sampled low-rank projection for memory-efficient llm fine-tuning</title>
		<author>
			<persName><forename type="first">Pengxiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiwei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.18380</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Mix-ln: Unleashing the power of deeper layers by combining pre-ln and postln</title>
		<author>
			<persName><forename type="first">Pengxiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiwei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.13795</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Relora: High-rank training through low-rank updates</title>
		<author>
			<persName><forename type="first">Sherin</forename><surname>Vladislav Lialin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Namrata</forename><surname>Muckatira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Shivagunde</surname></persName>
		</author>
		<author>
			<persName><surname>Rumshisky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training transformers</title>
		<author>
			<persName><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Alphapruning: Using heavy-tailed self regularization theory for improved layer-wise pruning of large language models</title>
		<author>
			<persName><forename type="first">Haiquan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yefan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaoqing</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Megalodon: Efficient llm pretraining and inference with unlimited context length</title>
		<author>
			<persName><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaomeng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beidi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lili</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Xin</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaojie</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weipeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Shortgpt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.03853</idno>
		<title level="m">Layers in large language models are more redundant than you expect</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Compact language models via pruning and knowledge distillation</title>
		<author>
			<persName><surname>Saurav Muralidharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Turuvekere</forename><surname>Sharath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raviraj Bhuminand</forename><surname>Sreenivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Chochowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavlo</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName><surname>Molchanov</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Transformers without tears: Improving the normalization of self-attention</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Toan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><surname>Salazar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IWSLT</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName><surname>Rajpurkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05202</idno>
		<title level="m">Glu variants improve transformer</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Megatronlm: Training multi-billion parameter language models using model parallelism</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A deeper look at depth pruning of llms</title>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Shoaib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavlo</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><surname>Molchanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">B2t connection: Serving stability and performance in deep transformers</title>
		<author>
			<persName><forename type="first">Sho</forename><surname>Takase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shun</forename><surname>Kiyono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sosuke</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Spike no more: Stabilizing the pre-training of large language models</title>
		<author>
			<persName><forename type="first">Sho</forename><surname>Takase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shun</forename><surname>Kiyono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sosuke</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.16903</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Going deeper with image transformers</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="32" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothée</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Rozière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Azhar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13971</idno>
		<title level="m">Open and efficient foundation language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">High-Dimensional Probability: An Introduction with Applications in Data Science</title>
		<author>
			<persName><forename type="first">Roman</forename><surname>Vershynin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cambridge Series in Statistical and Probabilistic Mathematics</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deepnet: Scaling transformers to 1,000 layers</title>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning deep transformer models for machine translation</title>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><forename type="middle">F</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidia</forename><forename type="middle">S</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">T</forename><surname>Whittaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">N</forename><surname>Watson</surname></persName>
		</author>
		<title level="m">A Course of Modern Analysis. Cambridge Mathematical Library</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
	<note>4 edition</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">On layer normalization in the transformer architecture</title>
		<author>
			<persName><forename type="first">Ruibin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunchang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tieyan</forename><surname>Liu</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10524" to="10533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Outlier weighed layerwise sparsity (owl): A missing secret sauce for pruning llms to high sparsity</title>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">You</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng-Yu</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiling</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mykola</forename><surname>Pechenizkiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiwei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Root mean square layer normalization</title>
		<author>
			<persName><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Galore: Memory-efficient llm training by gradient low-rank projection</title>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beidi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
